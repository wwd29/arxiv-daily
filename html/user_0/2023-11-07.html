<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Cooperative Network Learning for Large-Scale and Decentralized Graphs. (arXiv:2311.02117v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.02117">http://arxiv.org/abs/2311.02117</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.02117]] Cooperative Network Learning for Large-Scale and Decentralized Graphs(http://arxiv.org/abs/2311.02117)</code></li>
<li>Summary: <p>Graph research, the systematic study of interconnected data points
represented as graphs, plays a vital role in capturing intricate relationships
within networked systems. However, in the real world, as graphs scale up,
concerns about data security among different data-owning agencies arise,
hindering information sharing and, ultimately, the utilization of graph data.
Therefore, establishing a mutual trust mechanism among graph agencies is
crucial for unlocking the full potential of graphs. Here, we introduce a
Cooperative Network Learning (CNL) framework to ensure secure graph computing
for various graph tasks. Essentially, this CNL framework unifies the local and
global perspectives of GNN computing with distributed data for an agency by
virtually connecting all participating agencies as a global graph without a
fixed central coordinator. Inter-agency computing is protected by various
technologies inherent in our framework, including homomorphic encryption and
secure transmission. Moreover, each agency has a fair right to design or employ
various graph learning models from its local or global perspective. Thus, CNL
can collaboratively train GNN models based on decentralized graphs inferred
from local and global graphs. Experiments on contagion dynamics prediction and
traditional graph tasks (i.e., node classification and link prediction)
demonstrate that our CNL architecture outperforms state-of-the-art GNNs
developed at individual sites, revealing that CNL can provide a reliable, fair,
secure, privacy-preserving, and global perspective to build effective and
personalized models for network applications. We hope this framework will
address privacy concerns in graph-related research and integrate decentralized
graph data structures to benefit the network research community in cooperation
and innovation.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Thermal Face Image Classification using Deep Learning Techniques. (arXiv:2311.02314v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.02314">http://arxiv.org/abs/2311.02314</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.02314]] Thermal Face Image Classification using Deep Learning Techniques(http://arxiv.org/abs/2311.02314)</code></li>
<li>Summary: <p>Thermal images have various applications in security, medical and industrial
domains. This paper proposes a practical deep-learning approach for thermal
image classification. Accurate and efficient classification of thermal images
poses a significant challenge across various fields due to the complex image
content and the scarcity of annotated datasets. This work uses a convolutional
neural network (CNN) architecture, specifically ResNet-50 and VGGNet-19, to
extract features from thermal images. This work also applied Kalman filter on
thermal input images for image denoising. The experimental results demonstrate
the effectiveness of the proposed approach in terms of accuracy and efficiency.
</p></li>
</ul>

<h3>Title: Hierarchical Reinforcement Learning for Power Network Topology Control. (arXiv:2311.02129v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.02129">http://arxiv.org/abs/2311.02129</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.02129]] Hierarchical Reinforcement Learning for Power Network Topology Control(http://arxiv.org/abs/2311.02129)</code></li>
<li>Summary: <p>Learning in high-dimensional action spaces is a key challenge in applying
reinforcement learning (RL) to real-world systems. In this paper, we study the
possibility of controlling power networks using RL methods. Power networks are
critical infrastructures that are complex to control. In particular, the
combinatorial nature of the action space poses a challenge to both conventional
optimizers and learned controllers. Hierarchical reinforcement learning (HRL)
represents one approach to address this challenge. More precisely, a HRL
framework for power network topology control is proposed. The HRL framework
consists of three levels of action abstraction. At the highest level, there is
the overall long-term task of power network operation, namely, keeping the
power grid state within security constraints at all times, which is decomposed
into two temporally extended actions: 'do nothing' versus 'propose a topology
change'. At the intermediate level, the action space consists of all
controllable substations. Finally, at the lowest level, the action space
consists of all configurations of the chosen substation. By employing this HRL
framework, several hierarchical power network agents are trained for the IEEE
14-bus network. Whereas at the highest level a purely rule-based policy is
still chosen for all agents in this study, at the intermediate level the policy
is trained using different state-of-the-art RL algorithms. At the lowest level,
either an RL algorithm or a greedy algorithm is used. The performance of the
different 3-level agents is compared with standard baseline (RL or greedy)
approaches. A key finding is that the 3-level agent that employs RL both at the
intermediate and the lowest level outperforms all other agents on the most
difficult task. Our code is publicly available.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Towards Machine Unlearning Benchmarks: Forgetting the Personal Identities in Facial Recognition Systems. (arXiv:2311.02240v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.02240">http://arxiv.org/abs/2311.02240</a></li>
<li>Code URL: https://github.com/ndb796/machineunlearning</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.02240]] Towards Machine Unlearning Benchmarks: Forgetting the Personal Identities in Facial Recognition Systems(http://arxiv.org/abs/2311.02240)</code></li>
<li>Summary: <p>Machine unlearning is a crucial tool for enabling a classification model to
forget specific data that are used in the training time. Recently, various
studies have presented machine unlearning algorithms and evaluated their
methods on several datasets. However, most of the current machine unlearning
algorithms have been evaluated solely on traditional computer vision datasets
such as CIFAR-10, MNIST, and SVHN. Furthermore, previous studies generally
evaluate the unlearning methods in the class-unlearning setup. Most previous
work first trains the classification models and then evaluates the machine
unlearning performance of machine unlearning algorithms by forgetting selected
image classes (categories) in the experiments. Unfortunately, these
class-unlearning settings might not generalize to real-world scenarios. In this
work, we propose a machine unlearning setting that aims to unlearn specific
instance that contains personal privacy (identity) while maintaining the
original task of a given model. Specifically, we propose two machine unlearning
benchmark datasets, MUFAC and MUCAC, that are greatly useful to evaluate the
performance and robustness of a machine unlearning algorithm. In our benchmark
datasets, the original model performs facial feature recognition tasks: face
age estimation (multi-class classification) and facial attribute classification
(binary class classification), where a class does not depend on any single
target subject (personal identity), which can be a realistic setting. Moreover,
we also report the performance of the state-of-the-art machine unlearning
methods on our proposed benchmark datasets. All the datasets, source codes, and
trained models are publicly available at
https://github.com/ndb796/MachineUnlearning.
</p></li>
</ul>

<h3>Title: Bounded and Unbiased Composite Differential Privacy. (arXiv:2311.02324v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.02324">http://arxiv.org/abs/2311.02324</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.02324]] Bounded and Unbiased Composite Differential Privacy(http://arxiv.org/abs/2311.02324)</code></li>
<li>Summary: <p>The objective of differential privacy (DP) is to protect privacy by producing
an output distribution that is indistinguishable between any two neighboring
databases. However, traditional differentially private mechanisms tend to
produce unbounded outputs in order to achieve maximum disturbance range, which
is not always in line with real-world applications. Existing solutions attempt
to address this issue by employing post-processing or truncation techniques to
restrict the output results, but at the cost of introducing bias issues. In
this paper, we propose a novel differentially private mechanism which uses a
composite probability density function to generate bounded and unbiased outputs
for any numerical input data. The composition consists of an activation
function and a base function, providing users with the flexibility to define
the functions according to the DP constraints. We also develop an optimization
algorithm that enables the iterative search for the optimal hyper-parameter
setting without the need for repeated experiments, which prevents additional
privacy overhead. Furthermore, we evaluate the utility of the proposed
mechanism by assessing the variance of the composite probability density
function and introducing two alternative metrics that are simpler to compute
than variance estimation. Our extensive evaluation on three benchmark datasets
demonstrates consistent and significant improvement over the traditional
Laplace and Gaussian mechanisms. The proposed bounded and unbiased composite
differentially private mechanism will underpin the broader DP arsenal and
foster future privacy-preserving studies.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h2>attack</h2>
<h3>Title: Resist Label Noise with PGM for Graph Neural Networks. (arXiv:2311.02116v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.02116">http://arxiv.org/abs/2311.02116</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.02116]] Resist Label Noise with PGM for Graph Neural Networks(http://arxiv.org/abs/2311.02116)</code></li>
<li>Summary: <p>While robust graph neural networks (GNNs) have been widely studied for graph
perturbation and attack, those for label noise have received significantly less
attention. Most existing methods heavily rely on the label smoothness
assumption to correct noisy labels, which adversely affects their performance
on heterophilous graphs. Further, they generally perform poorly in high
noise-rate scenarios. To address these problems, in this paper, we propose a
novel probabilistic graphical model (PGM) based framework LNP. Given a noisy
label set and a clean label set, our goal is to maximize the likelihood of
labels in the clean set. We first present LNP-v1, which generates clean labels
based on graphs only in the Bayesian network. To further leverage the
information of clean labels in the noisy label set, we put forward LNP-v2,
which incorporates the noisy label set into the Bayesian network to generate
clean labels. The generative process can then be used to predict labels for
unlabeled nodes. We conduct extensive experiments to show the robustness of LNP
on varying noise types and rates, and also on graphs with different
heterophilies. In particular, we show that LNP can lead to inspiring
performance in high noise-rate situations.
</p></li>
</ul>

<h3>Title: The Alignment Problem in Context. (arXiv:2311.02147v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.02147">http://arxiv.org/abs/2311.02147</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.02147]] The Alignment Problem in Context(http://arxiv.org/abs/2311.02147)</code></li>
<li>Summary: <p>A core challenge in the development of increasingly capable AI systems is to
make them safe and reliable by ensuring their behaviour is consistent with
human values. This challenge, known as the alignment problem, does not merely
apply to hypothetical future AI systems that may pose catastrophic risks; it
already applies to current systems, such as large language models, whose
potential for harm is rapidly increasing. In this paper, I assess whether we
are on track to solve the alignment problem for large language models, and what
that means for the safety of future AI systems. I argue that existing
strategies for alignment are insufficient, because large language models remain
vulnerable to adversarial attacks that can reliably elicit unsafe behaviour. I
offer an explanation of this lingering vulnerability on which it is not simply
a contingent limitation of current language models, but has deep technical ties
to a crucial aspect of what makes these models useful and versatile in the
first place -- namely, their remarkable aptitude to learn "in context" directly
from user instructions. It follows that the alignment problem is not only
unsolved for current AI systems, but may be intrinsically difficult to solve
without severely undermining their capabilities. Furthermore, this assessment
raises concerns about the prospect of ensuring the safety of future and more
capable AI systems.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Towards objective and systematic evaluation of bias in medical imaging AI. (arXiv:2311.02115v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.02115">http://arxiv.org/abs/2311.02115</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.02115]] Towards objective and systematic evaluation of bias in medical imaging AI(http://arxiv.org/abs/2311.02115)</code></li>
<li>Summary: <p>Artificial intelligence (AI) models trained using medical images for clinical
tasks often exhibit bias in the form of disparities in performance between
subgroups. Since not all sources of biases in real-world medical imaging data
are easily identifiable, it is challenging to comprehensively assess how those
biases are encoded in models, and how capable bias mitigation methods are at
ameliorating performance disparities. In this article, we introduce a novel
analysis framework for systematically and objectively investigating the impact
of biases in medical images on AI models. We developed and tested this
framework for conducting controlled in silico trials to assess bias in medical
imaging AI using a tool for generating synthetic magnetic resonance images with
known disease effects and sources of bias. The feasibility is showcased by
using three counterfactual bias scenarios to measure the impact of simulated
bias effects on a convolutional neural network (CNN) classifier and the
efficacy of three bias mitigation strategies. The analysis revealed that the
simulated biases resulted in expected subgroup performance disparities when the
CNN was trained on the synthetic datasets. Moreover, reweighing was identified
as the most successful bias mitigation strategy for this setup, and we
demonstrated how explainable AI methods can aid in investigating the
manifestation of bias in the model using this framework. Developing fair AI
models is a considerable challenge given that many and often unknown sources of
biases can be present in medical imaging datasets. In this work, we present a
novel methodology to objectively study the impact of biases and mitigation
strategies on deep learning pipelines, which can support the development of
clinical AI that is robust and responsible.
</p></li>
</ul>

<h3>Title: Enhancing Monocular Height Estimation from Aerial Images with Street-view Images. (arXiv:2311.02121v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.02121">http://arxiv.org/abs/2311.02121</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.02121]] Enhancing Monocular Height Estimation from Aerial Images with Street-view Images(http://arxiv.org/abs/2311.02121)</code></li>
<li>Summary: <p>Accurate height estimation from monocular aerial imagery presents a
significant challenge due to its inherently ill-posed nature. This limitation
is rooted in the absence of adequate geometric constraints available to the
model when training with monocular imagery. Without additional geometric
information to supplement the monocular image data, the model's ability to
provide reliable estimations is compromised.
</p>
<p>In this paper, we propose a method that enhances monocular height estimation
by incorporating street-view images. Our insight is that street-view images
provide a distinct viewing perspective and rich structural details of the
scene, serving as geometric constraints to enhance the performance of monocular
height estimation. Specifically, we aim to optimize an implicit 3D scene
representation, density field, with geometry constraints from street-view
images, thereby improving the accuracy and robustness of height estimation. Our
experimental results demonstrate the effectiveness of our proposed method,
outperforming the baseline and offering significant improvements in terms of
accuracy and structural consistency.
</p></li>
</ul>

<h3>Title: Robust Fine-Tuning of Vision-Language Models for Domain Generalization. (arXiv:2311.02236v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.02236">http://arxiv.org/abs/2311.02236</a></li>
<li>Code URL: https://github.com/mit-ll/robust-vision-language-finetuning</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.02236]] Robust Fine-Tuning of Vision-Language Models for Domain Generalization(http://arxiv.org/abs/2311.02236)</code></li>
<li>Summary: <p>Transfer learning enables the sharing of common knowledge among models for a
variety of downstream tasks, but traditional methods suffer in limited training
data settings and produce narrow models incapable of effectively generalizing
under distribution shifts. Foundation models have recently demonstrated
impressive zero-shot inference capabilities and robustness under distribution
shifts. However, zero-shot evaluation for these models has been predominantly
confined to benchmarks with simple distribution shifts, limiting our
understanding of their effectiveness under the more realistic shifts found in
practice. Moreover, common fine-tuning methods for these models have yet to be
evaluated against vision models in few-shot scenarios where training data is
limited. To address these gaps, we present a new recipe for few-shot
fine-tuning of the popular vision-language foundation model CLIP and evaluate
its performance on challenging benchmark datasets with realistic distribution
shifts from the WILDS collection. Our experimentation demonstrates that, while
zero-shot CLIP fails to match performance of trained vision models on more
complex benchmarks, few-shot CLIP fine-tuning outperforms its vision-only
counterparts in terms of in-distribution and out-of-distribution accuracy at
all levels of training data availability. This provides a strong incentive for
adoption of foundation models within few-shot learning applications operating
with real-world data. Code is available at
https://github.com/mit-ll/robust-vision-language-finetuning
</p></li>
</ul>

<h3>Title: RigLSTM: Recurrent Independent Grid LSTM for Generalizable Sequence Learning. (arXiv:2311.02123v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.02123">http://arxiv.org/abs/2311.02123</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.02123]] RigLSTM: Recurrent Independent Grid LSTM for Generalizable Sequence Learning(http://arxiv.org/abs/2311.02123)</code></li>
<li>Summary: <p>Sequential processes in real-world often carry a combination of simple
subsystems that interact with each other in certain forms. Learning such a
modular structure can often improve the robustness against environmental
changes. In this paper, we propose recurrent independent Grid LSTM (RigLSTM),
composed of a group of independent LSTM cells that cooperate with each other,
for exploiting the underlying modular structure of the target task. Our model
adopts cell selection, input feature selection, hidden state selection, and
soft state updating to achieve a better generalization ability on the basis of
the recent Grid LSTM for the tasks where some factors differ between training
and evaluation. Specifically, at each time step, only a fraction of cells are
activated, and the activated cells select relevant inputs and cells to
communicate with. At the end of one time step, the hidden states of the
activated cells are updated by considering the relevance between the inputs and
the hidden states from the last and current time steps. Extensive experiments
on diversified sequential modeling tasks are conducted to show the superior
generalization ability when there exist changes in the testing environment.
Source code is available at \url{https://github.com/ziyuwwang/rig-lstm}.
</p></li>
</ul>

<h3>Title: Contrastive Multi-Modal Representation Learning for Spark Plug Fault Diagnosis. (arXiv:2311.02282v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.02282">http://arxiv.org/abs/2311.02282</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.02282]] Contrastive Multi-Modal Representation Learning for Spark Plug Fault Diagnosis(http://arxiv.org/abs/2311.02282)</code></li>
<li>Summary: <p>Due to the incapability of one sensory measurement to provide enough
information for condition monitoring of some complex engineered industrial
mechanisms and also for overcoming the misleading noise of a single sensor,
multiple sensors are installed to improve the condition monitoring of some
industrial equipment. Therefore, an efficient data fusion strategy is demanded.
In this research, we presented a Denoising Multi-Modal Autoencoder with a
unique training strategy based on contrastive learning paradigm, both being
utilized for the first time in the machine health monitoring realm. The
presented approach, which leverages the merits of both supervised and
unsupervised learning, not only achieves excellent performance in fusing
multiple modalities (or views) of data into an enriched common representation
but also takes data fusion to the next level wherein one of the views can be
omitted during inference time with very slight performance reduction, or even
without any reduction at all. The presented methodology enables multi-modal
fault diagnosis systems to perform more robustly in case of sensor failure
occurrence, and one can also intentionally omit one of the sensors (the more
expensive one) in order to build a more cost-effective condition monitoring
system without sacrificing performance for practical purposes. The
effectiveness of the presented methodology is examined on a real-world private
multi-modal dataset gathered under non-laboratory conditions from a complex
engineered mechanism, an inline four-stroke spark-ignition engine, aiming for
spark plug fault diagnosis. This dataset, which contains the accelerometer and
acoustic signals as two modalities, has a very slight amount of fault, and
achieving good performance on such a dataset promises that the presented method
can perform well on other equipment as well.
</p></li>
</ul>

<h3>Title: Successive Model-Agnostic Meta-Learning for Few-Shot Fault Time Series Prognosis. (arXiv:2311.02300v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.02300">http://arxiv.org/abs/2311.02300</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.02300]] Successive Model-Agnostic Meta-Learning for Few-Shot Fault Time Series Prognosis(http://arxiv.org/abs/2311.02300)</code></li>
<li>Summary: <p>Meta learning is a promising technique for solving few-shot fault prediction
problems, which have attracted the attention of many researchers in recent
years. Existing meta-learning methods for time series prediction, which
predominantly rely on random and similarity matching-based task partitioning,
face three major limitations: (1) feature exploitation inefficiency; (2)
suboptimal task data allocation; and (3) limited robustness with small samples.
To overcome these limitations, we introduce a novel 'pseudo meta-task'
partitioning scheme that treats a continuous time period of a time series as a
meta-task, composed of multiple successive short time periods. Employing
continuous time series as pseudo meta-tasks allows our method to extract more
comprehensive features and relationships from the data, resulting in more
accurate predictions. Moreover, we introduce a differential algorithm to
enhance the robustness of our method across different datasets. Through
extensive experiments on several fault and time series prediction datasets, we
demonstrate that our approach substantially enhances prediction performance and
generalization capability under both few-shot and general conditions.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: An Introduction to Natural Language Processing Techniques and Framework for Clinical Implementation in Radiation Oncology. (arXiv:2311.02205v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.02205">http://arxiv.org/abs/2311.02205</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.02205]] An Introduction to Natural Language Processing Techniques and Framework for Clinical Implementation in Radiation Oncology(http://arxiv.org/abs/2311.02205)</code></li>
<li>Summary: <p>Natural Language Processing (NLP) is a key technique for developing Medical
Artificial Intelligence (AI) systems that leverage Electronic Health Record
(EHR) data to build diagnostic and prognostic models. NLP enables the
conversion of unstructured clinical text into structured data that can be fed
into AI algorithms. The emergence of the transformer architecture and large
language models (LLMs) has led to remarkable advances in NLP for various
healthcare tasks, such as entity recognition, relation extraction, sentence
similarity, text summarization, and question answering. In this article, we
review the major technical innovations that underpin modern NLP models and
present state-of-the-art NLP applications that employ LLMs in radiation
oncology research. However, these LLMs are prone to many errors such as
hallucinations, biases, and ethical violations, which necessitate rigorous
evaluation and validation before clinical deployment. As such, we propose a
comprehensive framework for assessing the NLP models based on their purpose and
clinical fit, technical performance, bias and trust, legal and ethical
implications, and quality assurance, prior to implementation in clinical
radiation oncology. Our article aims to provide guidance and insights for
researchers and clinicians who are interested in developing and using NLP
models in clinical radiation oncology.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: A Comprehensive Study on Model Initialization Techniques Ensuring Efficient Federated Learning. (arXiv:2311.02100v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.02100">http://arxiv.org/abs/2311.02100</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.02100]] A Comprehensive Study on Model Initialization Techniques Ensuring Efficient Federated Learning(http://arxiv.org/abs/2311.02100)</code></li>
<li>Summary: <p>Advancement in the field of machine learning is unavoidable, but something of
major concern is preserving the privacy of the users whose data is being used
for training these machine learning algorithms. Federated learning(FL) has
emerged as a promising paradigm for training machine learning models in a
distributed and privacy-preserving manner which enables one to collaborate and
train a global model without sharing local data. But starting this learning
process on each device in the right way, called ``model initialization" is
critical. The choice of initialization methods used for models plays a crucial
role in the performance, convergence speed, communication efficiency, privacy
guarantees of federated learning systems, etc. In this survey, we dive deeper
into a comprehensive study of various ways of model initialization techniques
in FL.Unlike other studies, our research meticulously compares, categorizes,
and delineates the merits and demerits of each technique, examining their
applicability across diverse FL scenarios. We highlight how factors like client
variability, data non-IIDness, model caliber, security considerations, and
network restrictions influence FL model outcomes and propose how strategic
initialization can address and potentially rectify many such challenges. The
motivation behind this survey is to highlight that the right start can help
overcome challenges like varying data quality, security issues, and network
problems. Our insights provide a foundational base for experts looking to fully
utilize FL, also while understanding the complexities of model initialization.
</p></li>
</ul>

<h3>Title: Client Orchestration and Cost-Efficient Joint Optimization for NOMA-Enabled Hierarchical Federated Learning. (arXiv:2311.02130v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.02130">http://arxiv.org/abs/2311.02130</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.02130]] Client Orchestration and Cost-Efficient Joint Optimization for NOMA-Enabled Hierarchical Federated Learning(http://arxiv.org/abs/2311.02130)</code></li>
<li>Summary: <p>Hierarchical federated learning (HFL) shows great advantages over
conventional two-layer federated learning (FL) in reducing network overhead and
interaction latency while still retaining the data privacy of distributed FL
clients. However, the communication and energy overhead still pose a bottleneck
for HFL performance, especially as the number of clients raises dramatically.
To tackle this issue, we propose a non-orthogonal multiple access (NOMA)
enabled HFL system under semi-synchronous cloud model aggregation in this
paper, aiming to minimize the total cost of time and energy at each HFL global
round. Specifically, we first propose a novel fuzzy logic based client
orchestration policy considering client heterogenerity in multiple aspects,
including channel quality, data quantity and model staleness. Subsequently,
given the fuzzy based client-edge association, a joint edge server scheduling
and resource allocation problem is formulated. Utilizing problem decomposition,
we firstly derive the closed-form solution for the edge server scheduling
subproblem via the penalty dual decomposition (PDD) method. Next, a deep
deterministic policy gradient (DDPG) based algorithm is proposed to tackle the
resource allocation subproblem considering time-varying environments. Finally,
extensive simulations demonstrate that the proposed scheme outperforms the
considered benchmarks regarding HFL performance improvement and total cost
reduction.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: FairSeg: A Large-scale Medical Image Segmentation Dataset for Fairness Learning with Fair Error-Bound Scaling. (arXiv:2311.02189v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.02189">http://arxiv.org/abs/2311.02189</a></li>
<li>Code URL: https://github.com/harvard-ophthalmology-ai-lab/fairseg</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.02189]] FairSeg: A Large-scale Medical Image Segmentation Dataset for Fairness Learning with Fair Error-Bound Scaling(http://arxiv.org/abs/2311.02189)</code></li>
<li>Summary: <p>Fairness in artificial intelligence models has gained significantly more
attention in recent years, especially in the area of medicine, as fairness in
medical models is critical to people's well-being and lives. High-quality
medical fairness datasets are needed to promote fairness learning research.
Existing medical fairness datasets are all for classification tasks, and no
fairness datasets are available for medical segmentation, while medical
segmentation is an equally important clinical task as classifications, which
can provide detailed spatial information on organ abnormalities ready to be
assessed by clinicians. In this paper, we propose the first fairness dataset
for medical segmentation named FairSeg with 10,000 subject samples. In
addition, we propose a fair error-bound scaling approach to reweight the loss
function with the upper error-bound in each identity group. We anticipate that
the segmentation performance equity can be improved by explicitly tackling the
hard cases with high training errors in each identity group. To facilitate fair
comparisons, we propose new equity-scaled segmentation performance metrics,
such as the equity-scaled Dice coefficient, which is calculated as the overall
Dice coefficient divided by one plus the standard deviation of group Dice
coefficients. Through comprehensive experiments, we demonstrate that our fair
error-bound scaling approach either has superior or comparable fairness
performance to the state-of-the-art fairness learning models. The dataset and
code are publicly accessible via
\url{https://github.com/Harvard-Ophthalmology-AI-Lab/FairSeg}.
</p></li>
</ul>

<h3>Title: LLMs grasp morality in concept. (arXiv:2311.02294v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.02294">http://arxiv.org/abs/2311.02294</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.02294]] LLMs grasp morality in concept(http://arxiv.org/abs/2311.02294)</code></li>
<li>Summary: <p>Work in AI ethics and fairness has made much progress in regulating LLMs to
reflect certain values, such as fairness, truth, and diversity. However, it has
taken the problem of how LLMs might 'mean' anything at all for granted. Without
addressing this, it is not clear what imbuing LLMs with such values even means.
In response, we provide a general theory of meaning that extends beyond humans.
We use this theory to explicate the precise nature of LLMs as meaning-agents.
We suggest that the LLM, by virtue of its position as a meaning-agent, already
grasps the constructions of human society (e.g. morality, gender, and race) in
concept. Consequently, under certain ethical frameworks, currently popular
methods for model alignment are limited at best and counterproductive at worst.
Moreover, unaligned models may help us better develop our moral and social
philosophy.
</p></li>
</ul>

<h3>Title: Equal Opportunity of Coverage in Fair Regression. (arXiv:2311.02243v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.02243">http://arxiv.org/abs/2311.02243</a></li>
<li>Code URL: https://github.com/fangxin-wang/bfqr</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.02243]] Equal Opportunity of Coverage in Fair Regression(http://arxiv.org/abs/2311.02243)</code></li>
<li>Summary: <p>We study fair machine learning (ML) under predictive uncertainty to enable
reliable and trustworthy decision-making. The seminal work of ``equalized
coverage'' proposed an uncertainty-aware fairness notion. However, it does not
guarantee equal coverage rates across more fine-grained groups (e.g.,
low-income females) conditioning on the true label and is biased in the
assessment of uncertainty. To tackle these limitations, we propose a new
uncertainty-aware fairness -- Equal Opportunity of Coverage (EOC) -- that aims
to achieve two properties: (1) coverage rates for different groups with similar
outcomes are close, and (2) the coverage rate for the entire population remains
at a predetermined level. Further, the prediction intervals should be narrow to
be informative. We propose Binned Fair Quantile Regression (BFQR), a
distribution-free post-processing method to improve EOC with reasonable width
for any trained ML models. It first calibrates a hold-out set to bound
deviation from EOC, then leverages conformal prediction to maintain EOC on a
test set, meanwhile optimizing prediction interval width. Experimental results
demonstrate the effectiveness of our method in improving EOC. Our code is
publicly available at https://github.com/fangxin-wang/bfqr .
</p></li>
</ul>

<h3>Title: The Potential of Wearable Sensors for Assessing Patient Acuity in Intensive Care Unit (ICU). (arXiv:2311.02251v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.02251">http://arxiv.org/abs/2311.02251</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.02251]] The Potential of Wearable Sensors for Assessing Patient Acuity in Intensive Care Unit (ICU)(http://arxiv.org/abs/2311.02251)</code></li>
<li>Summary: <p>Acuity assessments are vital in critical care settings to provide timely
interventions and fair resource allocation. Traditional acuity scores rely on
manual assessments and documentation of physiological states, which can be
time-consuming, intermittent, and difficult to use for healthcare providers.
Furthermore, such scores do not incorporate granular information such as
patients' mobility level, which can indicate recovery or deterioration in the
ICU. We hypothesized that existing acuity scores could be potentially improved
by employing Artificial Intelligence (AI) techniques in conjunction with
Electronic Health Records (EHR) and wearable sensor data. In this study, we
evaluated the impact of integrating mobility data collected from wrist-worn
accelerometers with clinical data obtained from EHR for developing an AI-driven
acuity assessment score. Accelerometry data were collected from 86 patients
wearing accelerometers on their wrists in an academic hospital setting. The
data was analyzed using five deep neural network models: VGG, ResNet,
MobileNet, SqueezeNet, and a custom Transformer network. These models
outperformed a rule-based clinical score (SOFA= Sequential Organ Failure
Assessment) used as a baseline, particularly regarding the precision,
sensitivity, and F1 score. The results showed that while a model relying solely
on accelerometer data achieved limited performance (AUC 0.50, Precision 0.61,
and F1-score 0.68), including demographic information with the accelerometer
data led to a notable enhancement in performance (AUC 0.69, Precision 0.75, and
F1-score 0.67). This work shows that the combination of mobility and patient
information can successfully differentiate between stable and unstable states
in critically ill patients.
</p></li>
</ul>

<h2>interpretability</h2>
<h2>explainability</h2>
<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Patch-based Selection and Refinement for Early Object Detection. (arXiv:2311.02274v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.02274">http://arxiv.org/abs/2311.02274</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.02274]] Patch-based Selection and Refinement for Early Object Detection(http://arxiv.org/abs/2311.02274)</code></li>
<li>Summary: <p>Early object detection (OD) is a crucial task for the safety of many dynamic
systems. Current OD algorithms have limited success for small objects at a long
distance. To improve the accuracy and efficiency of such a task, we propose a
novel set of algorithms that divide the image into patches, select patches with
objects at various scales, elaborate the details of a small object, and detect
it as early as possible. Our approach is built upon a transformer-based network
and integrates the diffusion model to improve the detection accuracy. As
demonstrated on BDD100K, our algorithms enhance the mAP for small objects from
1.03 to 8.93, and reduce the data volume in computation by more than 77\%. The
source code is available at
\href{https://github.com/destiny301/dpr}{https://github.com/destiny301/dpr}
</p></li>
</ul>

<h3>Title: Sparse Training of Discrete Diffusion Models for Graph Generation. (arXiv:2311.02142v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.02142">http://arxiv.org/abs/2311.02142</a></li>
<li>Code URL: https://github.com/qym7/sparsediff</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.02142]] Sparse Training of Discrete Diffusion Models for Graph Generation(http://arxiv.org/abs/2311.02142)</code></li>
<li>Summary: <p>Generative models for graphs often encounter scalability challenges due to
the inherent need to predict interactions for every node pair. Despite the
sparsity often exhibited by real-world graphs, the unpredictable sparsity
patterns of their adjacency matrices, stemming from their unordered nature,
leads to quadratic computational complexity. In this work, we introduce
SparseDiff, a denoising diffusion model for graph generation that is able to
exploit sparsity during its training phase. At the core of SparseDiff is a
message-passing neural network tailored to predict only a subset of edges
during each forward pass. When combined with a sparsity-preserving noise model,
this model can efficiently work with edge lists representations of graphs,
paving the way for scalability to much larger structures. During the sampling
phase, SparseDiff iteratively populates the adjacency matrix from its prior
state, ensuring prediction of the full graph while controlling memory
utilization. Experimental results show that SparseDiff simultaneously matches
state-of-the-art in generation performance on both small and large graphs,
highlighting the versatility of our method.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Not all layers are equally as important: Every Layer Counts BERT. (arXiv:2311.02265v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.02265">http://arxiv.org/abs/2311.02265</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.02265]] Not all layers are equally as important: Every Layer Counts BERT(http://arxiv.org/abs/2311.02265)</code></li>
<li>Summary: <p>This paper introduces a novel modification of the transformer architecture,
tailored for the data-efficient pretraining of language models. This aspect is
evaluated by participating in the BabyLM challenge, where our solution won both
the \textsc{strict} and \textsc{strict-small} tracks. Our approach allows each
transformer layer to select which outputs of previous layers to process. The
empirical results verify the potential of this simple modification and show
that not all layers are equally as important.
</p></li>
</ul>

<h3>Title: Emergence of Abstract State Representations in Embodied Sequence Modeling. (arXiv:2311.02171v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.02171">http://arxiv.org/abs/2311.02171</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.02171]] Emergence of Abstract State Representations in Embodied Sequence Modeling(http://arxiv.org/abs/2311.02171)</code></li>
<li>Summary: <p>Decision making via sequence modeling aims to mimic the success of language
models, where actions taken by an embodied agent are modeled as tokens to
predict. Despite their promising performance, it remains unclear if embodied
sequence modeling leads to the emergence of internal representations that
represent the environmental state information. A model that lacks abstract
state representations would be liable to make decisions based on surface
statistics which fail to generalize. We take the BabyAI environment, a grid
world in which language-conditioned navigation tasks are performed, and build a
sequence modeling Transformer, which takes a language instruction, a sequence
of actions, and environmental observations as its inputs. In order to
investigate the emergence of abstract state representations, we design a
"blindfolded" navigation task, where only the initial environmental layout, the
language instruction, and the action sequence to complete the task are
available for training. Our probing results show that intermediate
environmental layouts can be reasonably reconstructed from the internal
activations of a trained model, and that language instructions play a role in
the reconstruction accuracy. Our results suggest that many key features of
state representations can emerge via embodied sequence modeling, supporting an
optimistic outlook for applications of sequence modeling objectives to more
complex embodied decision-making domains.
</p></li>
</ul>

<h3>Title: Multi-scale Time-stepping of Partial Differential Equations with Transformers. (arXiv:2311.02225v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.02225">http://arxiv.org/abs/2311.02225</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.02225]] Multi-scale Time-stepping of Partial Differential Equations with Transformers(http://arxiv.org/abs/2311.02225)</code></li>
<li>Summary: <p>Developing fast surrogates for Partial Differential Equations (PDEs) will
accelerate design and optimization in almost all scientific and engineering
applications. Neural networks have been receiving ever-increasing attention and
demonstrated remarkable success in computational modeling of PDEs, however;
their prediction accuracy is not at the level of full deployment. In this work,
we utilize the transformer architecture, the backbone of numerous
state-of-the-art AI models, to learn the dynamics of physical systems as the
mixing of spatial patterns learned by a convolutional autoencoder. Moreover, we
incorporate the idea of multi-scale hierarchical time-stepping to increase the
prediction speed and decrease accumulated error over time. Our model achieves
similar or better results in predicting the time-evolution of Navier-Stokes
equations compared to the powerful Fourier Neural Operator (FNO) and two
transformer-based neural operators OFormer and Galerkin Transformer.
</p></li>
</ul>

<h3>Title: FragXsiteDTI: Revealing Responsible Segments in Drug-Target Interaction with Transformer-Driven Interpretation. (arXiv:2311.02326v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.02326">http://arxiv.org/abs/2311.02326</a></li>
<li>Code URL: https://github.com/yazdanimehdi/fragxsitedti</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.02326]] FragXsiteDTI: Revealing Responsible Segments in Drug-Target Interaction with Transformer-Driven Interpretation(http://arxiv.org/abs/2311.02326)</code></li>
<li>Summary: <p>Drug-Target Interaction (DTI) prediction is vital for drug discovery, yet
challenges persist in achieving model interpretability and optimizing
performance. We propose a novel transformer-based model, FragXsiteDTI, that
aims to address these challenges in DTI prediction. Notably, FragXsiteDTI is
the first DTI model to simultaneously leverage drug molecule fragments and
protein pockets. Our information-rich representations for both proteins and
drugs offer a detailed perspective on their interaction. Inspired by the
Perceiver IO framework, our model features a learnable latent array, initially
interacting with protein binding site embeddings using cross-attention and
later refined through self-attention and used as a query to the drug fragments
in the drug's cross-attention transformer block. This learnable query array
serves as a mediator and enables seamless information translation, preserving
critical nuances in drug-protein interactions. Our computational results on
three benchmarking datasets demonstrate the superior predictive power of our
model over several state-of-the-art models. We also show the interpretability
of our model in terms of the critical components of both target proteins and
drug molecules within drug-target pairs.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Generative Artificial Intelligence in Healthcare: Ethical Considerations and Assessment Checklist. (arXiv:2311.02107v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.02107">http://arxiv.org/abs/2311.02107</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.02107]] Generative Artificial Intelligence in Healthcare: Ethical Considerations and Assessment Checklist(http://arxiv.org/abs/2311.02107)</code></li>
<li>Summary: <p>The widespread use of ChatGPT and other emerging technology powered by
generative artificial intelligence (AI) has drawn much attention to potential
ethical issues, especially in high-stakes applications such as healthcare.
However, less clear is how to resolve such issues beyond following guidelines
and regulations that are still under discussion and development. On the other
hand, other types of generative AI have been used to synthesize images and
other types of data for research and practical purposes, which have resolved
some ethical issues and exposed other ethical issues, but such technology is
less often the focus of ongoing ethical discussions. Here we highlight gaps in
current ethical discussions of generative AI via a systematic scoping review of
relevant existing research in healthcare, and reduce the gaps by proposing an
ethics checklist for comprehensive assessment and transparent documentation of
ethical discussions in generative AI development. While the checklist can be
readily integrated into the current peer review and publication system to
enhance generative AI research, it may also be used in broader settings to
disclose ethics-related considerations in generative AI-powered products (or
real-life applications of such products) to help users establish reasonable
trust in their capabilities.
</p></li>
</ul>

<h3>Title: Joint Composite Latent Space Bayesian Optimization. (arXiv:2311.02213v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.02213">http://arxiv.org/abs/2311.02213</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.02213]] Joint Composite Latent Space Bayesian Optimization(http://arxiv.org/abs/2311.02213)</code></li>
<li>Summary: <p>Bayesian Optimization (BO) is a technique for sample-efficient black-box
optimization that employs probabilistic models to identify promising input
locations for evaluation. When dealing with composite-structured functions,
such as f=g o h, evaluating a specific location x yields observations of both
the final outcome f(x) = g(h(x)) as well as the intermediate output(s) h(x).
Previous research has shown that integrating information from these
intermediate outputs can enhance BO performance substantially. However,
existing methods struggle if the outputs h(x) are high-dimensional. Many
relevant problems fall into this setting, including in the context of
generative AI, molecular design, or robotics. To effectively tackle these
challenges, we introduce Joint Composite Latent Space Bayesian Optimization
(JoCo), a novel framework that jointly trains neural network encoders and
probabilistic models to adaptively compress high-dimensional input and output
spaces into manageable latent representations. This enables viable BO on these
compressed representations, allowing JoCo to outperform other state-of-the-art
methods in high-dimensional BO on a wide variety of simulated and real-world
problems.
</p></li>
</ul>

<h3>Title: Structured Neural Networks for Density Estimation and Causal Inference. (arXiv:2311.02221v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.02221">http://arxiv.org/abs/2311.02221</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.02221]] Structured Neural Networks for Density Estimation and Causal Inference(http://arxiv.org/abs/2311.02221)</code></li>
<li>Summary: <p>Injecting structure into neural networks enables learning functions that
satisfy invariances with respect to subsets of inputs. For instance, when
learning generative models using neural networks, it is advantageous to encode
the conditional independence structure of observed variables, often in the form
of Bayesian networks. We propose the Structured Neural Network (StrNN), which
injects structure through masking pathways in a neural network. The masks are
designed via a novel relationship we explore between neural network
architectures and binary matrix factorization, to ensure that the desired
independencies are respected. We devise and study practical algorithms for this
otherwise NP-hard design problem based on novel objectives that control the
model architecture. We demonstrate the utility of StrNN in three applications:
(1) binary and Gaussian density estimation with StrNN, (2) real-valued density
estimation with Structured Autoregressive Flows (StrAFs) and Structured
Continuous Normalizing Flows (StrCNF), and (3) interventional and
counterfactual analysis with StrAFs for causal inference. Our work opens up new
avenues for learning neural networks that enable data-efficient generative
modeling and the use of normalizing flows for causal effect estimation.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: PILL: Plug Into LLM with Adapter Expert and Attention Gate. (arXiv:2311.02126v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.02126">http://arxiv.org/abs/2311.02126</a></li>
<li>Code URL: https://github.com/dsaltyfish/pill</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.02126]] PILL: Plug Into LLM with Adapter Expert and Attention Gate(http://arxiv.org/abs/2311.02126)</code></li>
<li>Summary: <p>Due to the remarkable capabilities of powerful Large Language Models (LLMs)
in effectively following instructions, there has been a growing number of
assistants in the community to assist humans. Recently, significant progress
has been made in the development of Vision Language Models (VLMs), expanding
the capabilities of LLMs and enabling them to execute more diverse
instructions. However, it is foreseeable that models will likely need to handle
tasks involving additional modalities such as speech, video, and others. This
poses a particularly prominent challenge of dealing with the complexity of
mixed modalities. To address this, we introduce a novel architecture called
PILL: Plug Into LLM with adapter expert and attention gate to better decouple
these complex modalities and leverage efficient fine-tuning. We introduce two
modules: Firstly, utilizing Mixture-of-Modality-Adapter-Expert to independently
handle different modalities, enabling better adaptation to downstream tasks
while preserving the expressive capability of the original model. Secondly, by
introducing Modality-Attention-Gating, which enables adaptive control of the
contribution of modality tokens to the overall representation. In addition, we
have made improvements to the Adapter to enhance its learning and expressive
capabilities. Experimental results demonstrate that our approach exhibits
competitive performance compared to other mainstream methods for modality
fusion. For researchers interested in our work, we provide free access to the
code and models at https://github.com/DsaltYfish/PILL.
</p></li>
</ul>

<h3>Title: COSMIC: Data Efficient Instruction-tuning For Speech In-Context Learning. (arXiv:2311.02248v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.02248">http://arxiv.org/abs/2311.02248</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.02248]] COSMIC: Data Efficient Instruction-tuning For Speech In-Context Learning(http://arxiv.org/abs/2311.02248)</code></li>
<li>Summary: <p>We present a data and cost efficient way of incorporating the speech modality
into a large language model (LLM). The resulting multi-modal LLM is a
COntextual Speech Model with Instruction-following/in-context-learning
Capabilities - COSMIC. Speech comprehension test question-answer (SQA) pairs
are generated using GPT-3.5 based on the speech transcriptions as a part of the
supervision for the instruction tuning. With fewer than 20M trainable
parameters and as little as 450 hours of English speech data for SQA
generation, COSMIC exhibits emergent instruction-following and in-context
learning capabilities in speech-to-text tasks. The model is able to follow the
given text instructions to generate text response even on the unseen EN$\to$X
speech-to-text translation (S2TT) task with zero-shot setting. We evaluate the
model's in-context learning via various tasks such as EN$\to$X S2TT and
few-shot domain adaptation. And instruction-following capabilities are
evaluated through a contextual biasing benchmark. Our results demonstrate the
efficacy of the proposed low cost recipe for building a speech LLM and that
with the new instruction-tuning data.
</p></li>
</ul>

<h3>Title: Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs. (arXiv:2311.02262v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.02262">http://arxiv.org/abs/2311.02262</a></li>
<li>Code URL: https://github.com/qingruzhang/pasta</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.02262]] Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs(http://arxiv.org/abs/2311.02262)</code></li>
<li>Summary: <p>In human-written articles, we often leverage the subtleties of text style,
such as bold and italics, to guide the attention of readers. These textual
emphases are vital for the readers to grasp the conveyed information. When
interacting with large language models (LLMs), we have a similar need -
steering the model to pay closer attention to user-specified information, e.g.,
an instruction. Existing methods, however, are constrained to process plain
text and do not support such a mechanism. This motivates us to introduce PASTA
- Post-hoc Attention STeering Approach, a method that allows LLMs to read text
with user-specified emphasis marks. To this end, PASTA identifies a small
subset of attention heads and applies precise attention reweighting on them,
directing the model attention to user-specified parts. Like prompting, PASTA is
applied at inference time and does not require changing any model parameters.
Experiments demonstrate that PASTA can substantially enhance an LLM's ability
to follow user instructions or integrate new knowledge from user inputs,
leading to a significant performance improvement on a variety of tasks, e.g.,
an average accuracy improvement of 22% for LLAMA-7B. Our code is publicly
available at https://github.com/QingruZhang/PASTA .
</p></li>
</ul>

<h3>Title: Narrowing the Gap between Zero- and Few-shot Machine Translation by Matching Styles. (arXiv:2311.02310v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.02310">http://arxiv.org/abs/2311.02310</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.02310]] Narrowing the Gap between Zero- and Few-shot Machine Translation by Matching Styles(http://arxiv.org/abs/2311.02310)</code></li>
<li>Summary: <p>Large language models trained primarily in a monolingual setting have
demonstrated their ability to generalize to machine translation using zero- and
few-shot examples with in-context learning. However, even though zero-shot
translations are relatively good, there remains a discernible gap comparing
their performance with the few-shot setting. In this paper, we investigate the
factors contributing to this gap and find that this gap can largely be closed
(for about 70%) by matching the writing styles of the target corpus.
Additionally, we explore potential approaches to enhance zero-shot baselines
without the need for parallel demonstration examples, providing valuable
insights into how these methods contribute to improving translation metrics.
</p></li>
</ul>

<h3>Title: Relax: Composable Abstractions for End-to-End Dynamic Machine Learning. (arXiv:2311.02103v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.02103">http://arxiv.org/abs/2311.02103</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.02103]] Relax: Composable Abstractions for End-to-End Dynamic Machine Learning(http://arxiv.org/abs/2311.02103)</code></li>
<li>Summary: <p>Dynamic shape computations have become critical in modern machine learning
workloads, especially in emerging large language models. The success of these
models has driven demand for deploying them to a diverse set of backend
environments. In this paper, we present Relax, a compiler abstraction for
optimizing end-to-end dynamic machine learning workloads. Relax introduces
first-class symbolic shape annotations to track dynamic shape computations
globally across the program. It also introduces a cross-level abstraction that
encapsulates computational graphs, loop-level tensor programs, and library
calls in a single representation to enable cross-level optimizations. We build
an end-to-end compilation framework using the proposed approach to optimize
dynamic shape models. Experimental results on large language models show that
Relax delivers performance competitive with state-of-the-art hand-optimized
systems across platforms and enables deployment of emerging dynamic models to a
broader set of environments, including mobile phones, embedded devices, and web
browsers.
</p></li>
</ul>

<h3>Title: Making Harmful Behaviors Unlearnable for Large Language Models. (arXiv:2311.02105v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.02105">http://arxiv.org/abs/2311.02105</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.02105]] Making Harmful Behaviors Unlearnable for Large Language Models(http://arxiv.org/abs/2311.02105)</code></li>
<li>Summary: <p>Large language models (LLMs) have shown great potential as general-purpose AI
assistants in various domains. To meet the requirements of different
applications, LLMs are often customized by further fine-tuning. However, the
powerful learning ability of LLMs not only enables them to acquire new tasks
but also makes them susceptible to learning undesired behaviors. For example,
even safety-aligned LLMs can be easily fine-tuned into harmful assistants as
the fine-tuning data often contains implicit or explicit harmful content. Can
we train LLMs on harmful data without learning harmful behaviors? This paper
proposes a controllable training framework that makes harmful behaviors
unlearnable during the fine-tuning process. Specifically, we introduce
``security vectors'', a few new parameters that can be separated from the LLM,
to ensure LLM's responses are consistent with the harmful behavior. Security
vectors are activated during fine-tuning, the consistent behavior makes LLM
believe that such behavior has already been learned, there is no need to
further optimize for harmful data. During inference, we can deactivate security
vectors to restore the LLM's normal behavior. The experimental results show
that the security vectors generated by 100 harmful samples are enough to
prevent LLM from learning 1000 harmful samples, while preserving the ability to
learn other useful information.
</p></li>
</ul>

<h3>Title: LLMs-augmented Contextual Bandit. (arXiv:2311.02268v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.02268">http://arxiv.org/abs/2311.02268</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.02268]] LLMs-augmented Contextual Bandit(http://arxiv.org/abs/2311.02268)</code></li>
<li>Summary: <p>Contextual bandits have emerged as a cornerstone in reinforcement learning,
enabling systems to make decisions with partial feedback. However, as contexts
grow in complexity, traditional bandit algorithms can face challenges in
adequately capturing and utilizing such contexts. In this paper, we propose a
novel integration of large language models (LLMs) with the contextual bandit
framework. By leveraging LLMs as an encoder, we enrich the representation of
the context, providing the bandit with a denser and more informative view.
Preliminary results on synthetic datasets demonstrate the potential of this
approach, showing notable improvements in cumulative rewards and reductions in
regret compared to traditional bandit algorithms. This integration not only
showcases the capabilities of LLMs in reinforcement learning but also opens the
door to a new era of contextually-aware decision systems.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Using DUCK-Net for Polyp Image Segmentation. (arXiv:2311.02239v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.02239">http://arxiv.org/abs/2311.02239</a></li>
<li>Code URL: https://github.com/RazvanDu/DUCK-Net</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.02239]] Using DUCK-Net for Polyp Image Segmentation(http://arxiv.org/abs/2311.02239)</code></li>
<li>Summary: <p>This paper presents a novel supervised convolutional neural network
architecture, "DUCK-Net", capable of effectively learning and generalizing from
small amounts of medical images to perform accurate segmentation tasks. Our
model utilizes an encoder-decoder structure with a residual downsampling
mechanism and a custom convolutional block to capture and process image
information at multiple resolutions in the encoder segment. We employ data
augmentation techniques to enrich the training set, thus increasing our model's
performance. While our architecture is versatile and applicable to various
segmentation tasks, in this study, we demonstrate its capabilities specifically
for polyp segmentation in colonoscopy images. We evaluate the performance of
our method on several popular benchmark datasets for polyp segmentation,
Kvasir-SEG, CVC-ClinicDB, CVC-ColonDB, and ETIS-LARIBPOLYPDB showing that it
achieves state-of-the-art results in terms of mean Dice coefficient, Jaccard
index, Precision, Recall, and Accuracy. Our approach demonstrates strong
generalization capabilities, achieving excellent performance even with limited
training data. The code is publicly available on GitHub:
https://github.com/RazvanDu/DUCK-Net
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
