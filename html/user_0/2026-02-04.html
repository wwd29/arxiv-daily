<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2026-02-04</h1>
<h3>Title: The Hypocrisy Gap: Quantifying Divergence Between Internal Belief and Chain-of-Thought Explanation via Sparse Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Shikhar Shiromani, Archie Chaudhury, Sri Pranav Kunda</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02496">https://arxiv.org/abs/2602.02496</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02496">https://arxiv.org/pdf/2602.02496</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02496]] The Hypocrisy Gap: Quantifying Divergence Between Internal Belief and Chain-of-Thought Explanation via Sparse Autoencoders(https://arxiv.org/abs/2602.02496)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) frequently exhibit unfaithful behavior, producing a final answer that differs significantly from their internal chain of thought (CoT) reasoning in order to appease the user they are conversing with. In order to better detect this behavior, we introduce the Hypocrisy Gap, a mechanistic metric utilizing Sparse Autoencoders (SAEs) to quantify the divergence between a model's internal reasoning and its final generation. By mathematically comparing an internal truth belief, derived via sparse linear probes, to the final generated trajectory in latent space, we quantify and detect a model's tendency to engage in unfaithful behavior. Experiments on Gemma, Llama, and Qwen models using Anthropic's Sycophancy benchmark show that our method achieves an AUROC of 0.55-0.73 for detecting sycophantic runs and 0.55-0.74 for hypocritical cases where the model internally "knows" the user is wrong, consistently outperforming a decision-aligned log-probability baseline (0.41-0.50 AUROC).</li>
</ul>

<h3>Title: STEMVerse: A Dual-Axis Diagnostic Framework for STEM Reasoning in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xuzhao Li, Xuchen Li, Jian Zhao, Shiyu Hu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02497">https://arxiv.org/abs/2602.02497</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02497">https://arxiv.org/pdf/2602.02497</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02497]] STEMVerse: A Dual-Axis Diagnostic Framework for STEM Reasoning in Large Language Models(https://arxiv.org/abs/2602.02497)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) achieve significant breakthroughs in complex reasoning tasks, evaluating their proficiency in science, technology, engineering, and mathematics (STEM) has become a primary method for measuring machine intelligence. However, current evaluation paradigms often treat benchmarks as isolated "silos," offering only monolithic aggregate scores that neglect the intricacies of both academic specialization and cognitive depth. This result-oriented approach fails to distinguish whether model errors stem from insufficient domain knowledge or deficiencies in cognitive capacity, thereby limiting the diagnostic value. To address this, we propose STEMVerse, a diagnostic framework designed to systematically analyze the STEM reasoning capabilities of LLMs. This framework characterizes model performance across academic specialization and cognitive complexity to map the capability required for reasoning. We re-aggregate over 20,000 STEM problems from mainstream benchmarks into a unified "Discipline $\times$ Cognition" capability space, assigning dual-axis labels to every instance. Utilizing this unified diagnostic framework, we systematically evaluate representative LLM families across varying parameter scales and training paradigms. Our empirical results reveal structural failure patterns in STEM reasoning. By integrating multi-disciplinary coverage and fine-grained cognitive stratification into a unified framework, STEMVerse provides a clear and actionable perspective for understanding the scientific reasoning characteristics of LLMs.</li>
</ul>

<h3>Title: Test-Time Detoxification without Training or Learning Anything</h3>
<ul>
<li><strong>Authors: </strong>Baturay Saglam, Dionysis Kalogerias</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02498">https://arxiv.org/abs/2602.02498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02498">https://arxiv.org/pdf/2602.02498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02498]] Test-Time Detoxification without Training or Learning Anything(https://arxiv.org/abs/2602.02498)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models can produce toxic or inappropriate text even for benign inputs, creating risks when deployed at scale. Detoxification is therefore important for safety and user trust, particularly when we want to reduce harmful content without sacrificing the model's generation quality. Many existing approaches rely on model retraining, gradients, or learned auxiliary components, which can be costly and may not transfer across model families or to truly black-box settings. We introduce a test-time procedure that approximates the gradient of completion toxicity with respect to the input embeddings and uses a small number of descent steps to steer generation toward less toxic continuations. This is achieved with zeroth-order optimization that requires only access to input embeddings, a toxicity scoring function, and forward evaluations of the model. Empirically, the approach delivers robust toxicity reductions across models and prompts and, in most settings, achieves the best overall toxicity-quality trade-off. More broadly, our work positions word embeddings as effective control variables and encourages wider use of black-box optimization to guide autoregressive language models toward scalable, safer text generation, without requiring any training or access to intermediate computations.</li>
</ul>

<h3>Title: ROSA-Tuning: Enhancing Long-Context Modeling via Suffix Matching</h3>
<ul>
<li><strong>Authors: </strong>Yunao Zheng, Xiaojie Wang, Lei Ren, Wei Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02499">https://arxiv.org/abs/2602.02499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02499">https://arxiv.org/pdf/2602.02499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02499]] ROSA-Tuning: Enhancing Long-Context Modeling via Suffix Matching(https://arxiv.org/abs/2602.02499)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Long-context capability and computational efficiency are among the central challenges facing today's large language models. Existing efficient attention methods reduce computational complexity, but they typically suffer from a limited coverage of the model state. This paper proposes ROSA-Tuning, a retrieval-and-recall mechanism for enhancing the long-context modeling ability of pretrained models. Beyond the standard attention mechanism, ROSA-Tuning introduces in parallel a CPU-based ROSA (RWKV Online Suffix Automaton) retrieval module, which efficiently locates historical positions in long contexts that are relevant to the current query, and injects the retrieved information into the model state in a trainable manner; subsequent weighted fusion can then be handled by range-restricted attention. To enable end-to-end training, we design a binary discretization strategy and a counterfactual gradient algorithm, and further optimize overall execution efficiency via an asynchronous CPU-GPU pipeline. Systematic evaluations on Qwen3-Base-1.7B show that ROSA-Tuning substantially restores the long-context modeling ability of windowed-attention models, achieving performance close to and in some cases matching global attention on benchmarks such as LongBench, while maintaining computational efficiency and GPU memory usage that are nearly comparable to windowed-attention methods, offering a new technical path for efficient long-context processing. The example code can be found at this https URL.</li>
</ul>

<h3>Title: Augmenting Parameter-Efficient Pre-trained Language Models with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Saurabh Anand, Shubham Malaviya, Manish Shukla, Sachin Lodha</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02501">https://arxiv.org/abs/2602.02501</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02501">https://arxiv.org/pdf/2602.02501</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02501]] Augmenting Parameter-Efficient Pre-trained Language Models with Large Language Models(https://arxiv.org/abs/2602.02501)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, large language model</a></li>
<li><strong>Abstract: </strong>Training AI models in cybersecurity with help of vast datasets offers significant opportunities to mimic real-world behaviors effectively. However, challenges like data drift and scarcity of labelled data lead to frequent updates of models and the risk of overfitting. To address these challenges, we used parameter-efficient fine-tuning techniques for pre-trained language models wherein we combine compacters with various layer freezing strategies. To enhance the capabilities of these pre-trained language models, in this work we introduce two strategies that use large language models. In the first strategy, we utilize large language models as data-labelling tools wherein they generate labels for unlabeled data. In the second strategy, large language modes are utilized as fallback mechanisms for predictions having low confidence scores. We perform comprehensive experimental analysis on the proposed strategies on different downstream tasks specific to cybersecurity domain. We empirically demonstrate that by combining parameter-efficient pre-trained models with large language models, we can improve the reliability and robustness of models, making them more suitable for real-world cybersecurity applications.</li>
</ul>

<h3>Title: GraphDancer: Training LLMs to Explore and Reason over Graphs via Curriculum Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Yuyang Bai, Zhuofeng Li, Ping Nie, Jianwen Xie, Yu Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02518">https://arxiv.org/abs/2602.02518</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02518">https://arxiv.org/pdf/2602.02518</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02518]] GraphDancer: Training LLMs to Explore and Reason over Graphs via Curriculum Reinforcement Learning(https://arxiv.org/abs/2602.02518)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) increasingly rely on external knowledge to improve factuality, yet many real-world knowledge sources are organized as heterogeneous graphs rather than plain text. Reasoning over such graph-structured knowledge poses two key challenges: (1) navigating structured, schema-defined relations requires precise function calls rather than similarity-based retrieval, and (2) answering complex questions often demands multi-hop evidence aggregation through iterative information seeking. We propose GraphDancer, a reinforcement learning (RL) framework that teaches LLMs to navigate graphs by interleaving reasoning and function execution. To make RL effective for moderate-sized LLMs, we introduce a graph-aware curriculum that schedules training by the structural complexity of information-seeking trajectories using an easy-to-hard biased sampler. We evaluate GraphDancer on a multi-domain benchmark by training on one domain only and testing on unseen domains and out-of-distribution question types. Despite using only a 3B backbone, GraphDancer outperforms baselines equipped with either a 14B backbone or GPT-4o-mini, demonstrating robust cross-domain generalization of graph exploration and reasoning skills. Our code and models can be found at this https URL .</li>
</ul>

<h3>Title: The "Robert Boulton" Singularity: Semantic Tunneling and Manifold Unfolding in Recursive AI</h3>
<ul>
<li><strong>Authors: </strong>Pengyue Hou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02526">https://arxiv.org/abs/2602.02526</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02526">https://arxiv.org/pdf/2602.02526</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02526]] The "Robert Boulton" Singularity: Semantic Tunneling and Manifold Unfolding in Recursive AI(https://arxiv.org/abs/2602.02526)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The stability of generative artificial intelligence trained on recursive synthetic data is conventionally monitored via Perplexity (PPL). We demonstrate that PPL is a deceptive metric in context-stabilized regimes (L=128). Using a rigorous sliding-window protocol (N=1500), we identify a novel failure mode termed "Semantic Tunneling." While the Baseline model maintains high grammatical fluency (PPL approx. 83.9), it suffers a catastrophic loss of semantic diversity, converging within seven generations to a single, low-entropy narrative attractor: the "Robert Boulton" Singularity. This phenomenon represents a total collapse of the latent manifold (Global Effective Rank 3.62 -> 2.22), where the model discards diverse world knowledge to optimize for statistically safe syntactic templates. To address this, we apply the Multi-Scale Negative Coupled Information Systems (MNCIS) framework recently established in Hou (2026) [arXiv:2601.11594]. We demonstrate that Adaptive Spectral Negative Coupling (ASNC) acts as a topological operator that actively induces "Manifold Unfolding." MNCIS forces the model to expand its effective rank from the anisotropic baseline of 3.62 to a hyper-diverse state of 5.35, effectively constructing an "Artificial Manifold" that resists the gravitational pull of semantic attractors and preserves the long-tail distribution of the training data.</li>
</ul>

<h3>Title: Hypersonic Flow Control: Generalized Deep Reinforcement Learning for Hypersonic Intake Unstart Control under Uncertainty</h3>
<ul>
<li><strong>Authors: </strong>Trishit Mondal, Ameya D. Jagtap</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.flu-dyn</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02531">https://arxiv.org/abs/2602.02531</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02531">https://arxiv.org/pdf/2602.02531</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02531]] Hypersonic Flow Control: Generalized Deep Reinforcement Learning for Hypersonic Intake Unstart Control under Uncertainty(https://arxiv.org/abs/2602.02531)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The hypersonic unstart phenomenon poses a major challenge to reliable air-breathing propulsion at Mach 5 and above, where strong shock-boundary-layer interactions and rapid pressure fluctuations can destabilize inlet operation. Here, we demonstrate a deep reinforcement learning (DRL)- based active flow control strategy to control unstart in a canonical two-dimensional hypersonic inlet at Mach 5 and Reynolds number $5\times 10^6$. The in-house CFD solver enables high-fidelity simulations with adaptive mesh refinement, resolving key flow features, including shock motion, boundary-layer dynamics, and flow separation, that are essential for learning physically consistent control policies suitable for real-time deployment. The DRL controller robustly stabilizes the inlet over a wide range of back pressures representative of varying combustion chamber conditions. It further generalizes to previously unseen scenarios, including different back-pressure levels, Reynolds numbers, and sensor configurations, while operating with noisy measurements, thereby demonstrating strong zero-shot generalization. Control remains robust in the presence of noisy sensor measurements, and a minimal, optimally selected sensor set achieves comparable performance, enabling practical implementation. These results establish a data-driven approach for real-time hypersonic flow control under realistic operational uncertainties.</li>
</ul>

<h3>Title: CADENT: Gated Hybrid Distillation for Sample-Efficient Transfer in Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Mahyar Alinejad, Yue Wang, George Atia</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02532">https://arxiv.org/abs/2602.02532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02532">https://arxiv.org/pdf/2602.02532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02532]] CADENT: Gated Hybrid Distillation for Sample-Efficient Transfer in Reinforcement Learning(https://arxiv.org/abs/2602.02532)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Transfer learning promises to reduce the high sample complexity of deep reinforcement learning (RL), yet existing methods struggle with domain shift between source and target environments. Policy distillation provides powerful tactical guidance but fails to transfer long-term strategic knowledge, while automaton-based methods capture task structure but lack fine-grained action guidance. This paper introduces Context-Aware Distillation with Experience-gated Transfer (CADENT), a framework that unifies strategic automaton-based knowledge with tactical policy-level knowledge into a coherent guidance signal. CADENT's key innovation is an experience-gated trust mechanism that dynamically weighs teacher guidance against the student's own experience at the state-action level, enabling graceful adaptation to target domain specifics. Across challenging environments, from sparse-reward grid worlds to continuous control tasks, CADENT achieves 40-60\% better sample efficiency than baselines while maintaining superior asymptotic performance, establishing a robust approach for adaptive knowledge transfer in RL.</li>
</ul>

<h3>Title: Enhancing Psychologists' Understanding through Explainable Deep Learning Framework for ADHD Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Abdul Rehman, Ilona Heldal, Jerry Chun-Wei Lin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02535">https://arxiv.org/abs/2602.02535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02535">https://arxiv.org/pdf/2602.02535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02535]] Enhancing Psychologists' Understanding through Explainable Deep Learning Framework for ADHD Diagnosis(https://arxiv.org/abs/2602.02535)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Attention Deficit Hyperactivity Disorder (ADHD) is a neurodevelopmental disorder that is challenging to diagnose and requires advanced approaches for reliable and transparent identification and classification. It is characterized by a pattern of inattention, hyperactivity and impulsivity that is more severe and more frequent than in individuals with a comparable level of development. In this paper, an explainable framework based on a fine-tuned hybrid Deep Neural Network (DNN) and Recurrent Neural Network (RNN) called HyExDNN-RNN model is proposed for ADHD detection, multi-class categorization, and decision interpretation. This framework not only detects ADHD, but also provides interpretable insights into the diagnostic process so that psychologists can better understand and trust the results of the diagnosis. We use the Pearson correlation coefficient for optimal feature selection and machine and deep learning models for experimental analysis and comparison. We use a standardized technique for feature reduction, model selection and interpretation to accurately determine the diagnosis rate and ensure the interpretability of the proposed framework. Our framework provided excellent results on binary classification, with HyExDNN-RNN achieving an F1 score of 99% and 94.2% on multi-class categorization. XAI approaches, in particular SHapley Additive exPlanations (SHAP) and Permutation Feature Importance (PFI), provided important insights into the importance of features and the decision logic of models. By combining AI with human expertise, we aim to bridge the gap between advanced computational techniques and practical psychological applications. These results demonstrate the potential of our framework to assist in ADHD diagnosis and interpretation.</li>
</ul>

<h3>Title: WorldVQA: Measuring Atomic World Knowledge in Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Runjie Zhou, Youbo Shao, Haoyu Lu, Bowei Xing, Tongtong Bai, Yujie Chen, Jie Zhao, Lin Sui, Haotian Yao, Zijia Zhao, Hao Yang, Haoning Wu, Zaida Zhou, Jinguo Zhu, Zhiqi Huang, Yiping Bao, Yangyang Liu, Y.Charles, Xinyu Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02537">https://arxiv.org/abs/2602.02537</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02537">https://arxiv.org/pdf/2602.02537</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02537]] WorldVQA: Measuring Atomic World Knowledge in Multimodal Large Language Models(https://arxiv.org/abs/2602.02537)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce WorldVQA, a benchmark designed to evaluate the atomic visual world knowledge of Multimodal Large Language Models (MLLMs). Unlike current evaluations, which often conflate visual knowledge retrieval with reasoning, WorldVQA decouples these capabilities to strictly measure "what the model memorizes." The benchmark assesses the atomic capability of grounding and naming visual entities across a stratified taxonomy, spanning from common head-class objects to long-tail rarities. We expect WorldVQA to serve as a rigorous test for visual factuality, thereby establishing a standard for assessing the encyclopedic breadth and hallucination rates of current and next-generation frontier models.</li>
</ul>

<h3>Title: Enhancing Post-Training Quantization via Future Activation Awareness</h3>
<ul>
<li><strong>Authors: </strong>Zheqi Lv, Zhenxuan Fan, Qi Tian, Wenqiao Zhang, Yueting Zhuang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02538">https://arxiv.org/abs/2602.02538</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02538">https://arxiv.org/pdf/2602.02538</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02538]] Enhancing Post-Training Quantization via Future Activation Awareness(https://arxiv.org/abs/2602.02538)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Post-training quantization (PTQ) is a widely used method to compress large language models (LLMs) without fine-tuning. It typically sets quantization hyperparameters (e.g., scaling factors) based on current-layer activations. Although this method is efficient, it suffers from quantization bias and error accumulation, resulting in suboptimal and unstable quantization, especially when the calibration data is biased. To overcome these issues, we propose Future-Aware Quantization (FAQ), which leverages future-layer activations to guide quantization. This allows better identification and preservation of important weights, while reducing sensitivity to calibration noise. We further introduce a window-wise preview mechanism to softly aggregate multiple future-layer activations, mitigating over-reliance on any single layer. To avoid expensive greedy search, we use a pre-searched configuration to minimize overhead. Experiments show that FAQ consistently outperforms prior methods with negligible extra cost, requiring no backward passes, data reconstruction, or tuning, making it well-suited for edge deployment.</li>
</ul>

<h3>Title: Toward Ultra-Long-Horizon Sequential Model Editing</h3>
<ul>
<li><strong>Authors: </strong>Mingda Liu, Zhenghan Zhu, Ze'an Miao, Katsuki Fujisawa</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02543">https://arxiv.org/abs/2602.02543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02543">https://arxiv.org/pdf/2602.02543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02543]] Toward Ultra-Long-Horizon Sequential Model Editing(https://arxiv.org/abs/2602.02543)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Model editing has emerged as a practical approach for mitigating factual errors and outdated knowledge in large language models (LLMs). Among existing methods, the Locate-and-Edit (L&E) paradigm is the dominant framework: it locates MLP parameters implicated in expressing a target fact, and then performs a localized update to rewrite that fact. However, long sequences of edits often trigger abrupt model collapse in L&E beyond a critical point. We empirically identify a strong correlation between collapse and explosive growth of edited MLP weight norms, and formally prove that commonly used L&E update rules can induce exponential norm growth across sequential edits in the absence of explicit norm control. To address this issue, we propose Norm-Anchor Scaling NAS, a plug-and-play norm-constrained strategy. Across extensive experiments, NAS delays the collapse point of representative L&E algorithms by more than 4 times and yields a 72.2% average relative gain in editing performance, requiring only a single additional line of code and incurring negligible computational overhead.</li>
</ul>

<h3>Title: SPA-Cache: Singular Proxies for Adaptive Caching in Diffusion Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wenhao Sun, Rong-Cheng Tu, Yifu Ding, Zhao Jin, Jingyi Liao, Yongcheng Jing, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02544">https://arxiv.org/abs/2602.02544</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02544">https://arxiv.org/pdf/2602.02544</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02544]] SPA-Cache: Singular Proxies for Adaptive Caching in Diffusion Language Models(https://arxiv.org/abs/2602.02544)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>While Diffusion Language Models (DLMs) offer a flexible, arbitrary-order alternative to the autoregressive paradigm, their non-causal nature precludes standard KV caching, forcing costly hidden state recomputation at every decoding step. Existing DLM caching approaches reduce this cost by selective hidden state updates; however, they are still limited by (i) costly token-wise update identification heuristics and (ii) rigid, uniform budget allocation that fails to account for heterogeneous hidden state dynamics. To address these challenges, we present SPA-Cache that jointly optimizes update identification and budget allocation in DLM cache. First, we derive a low-dimensional singular proxy that enables the identification of update-critical tokens in a low-dimensional subspace, substantially reducing the overhead of update identification. Second, we introduce an adaptive strategy that allocates fewer updates to stable layers without degrading generation quality. Together, these contributions significantly improve the efficiency of DLMs, yielding up to an $8\times$ throughput improvement over vanilla decoding and a $2$--$4\times$ speedup over existing caching baselines.</li>
</ul>

<h3>Title: Beyond Alignment: Expanding Reasoning Capacity via Manifold-Reshaping Policy Optimization</h3>
<ul>
<li><strong>Authors: </strong>Dayu Wang, Jiaye Yang, Weikang Li, Jiahui Liang, Yang Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02545">https://arxiv.org/abs/2602.02545</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02545">https://arxiv.org/pdf/2602.02545</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02545]] Beyond Alignment: Expanding Reasoning Capacity via Manifold-Reshaping Policy Optimization(https://arxiv.org/abs/2602.02545)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated remarkable success in enhancing the reasoning capabilities of Large Language Models (LLMs). However, recent studies question whether RL genuinely expands reasoning capacity or merely aligns existing latent capabilities, arguing that exploration remains confined within the pre-trained model's low-rank bias manifold. In this work, we challenge this accessibility boundary hypothesis by demonstrating that the latent reasoning space can be fundamentally expanded through targeted geometric interventions. We propose Manifold-Reshaping Policy Optimization (MRPO), a geometric framework designed to fundamentally restructure the inference space of LLMs. MRPO operates in two stages: first, we employ Spectral Orthogonal Exploration (SOE) to eject the policy initialization into the null space of the bias manifold; second, we integrate an Effective Rank regularization term into the policy optimization objective. This approach incentivizes the discovery and maintenance of high-dimensional reasoning trajectories against the entropy-reducing tendency of standard RL. Empirically, our 4B-parameter method achieves state-of-the-art performance on mathematical tasks, significantly outperforming larger models (e.g., Qwen3-32B) and expanding the capability boundary beyond standard GRPO. Our code is available at this https URL</li>
</ul>

<h3>Title: D$^2$Quant: Accurate Low-bit Post-Training Weight Quantization for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Xianglong Yan, ChengZhu Bao, Zhiteng Li, Tianao Zhang, Shaoqiu Zhang, Ruobing Xie, Samm Sun, Yulun Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02546">https://arxiv.org/abs/2602.02546</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02546">https://arxiv.org/pdf/2602.02546</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02546]] D$^2$Quant: Accurate Low-bit Post-Training Weight Quantization for LLMs(https://arxiv.org/abs/2602.02546)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) deliver strong performance, but their high compute and memory costs make deployment difficult in resource-constrained scenarios. Weight-only post-training quantization (PTQ) is appealing, as it reduces memory usage and enables practical speedup without low-bit operators or specialized hardware. However, accuracy often degrades significantly in weight-only PTQ at sub-4-bit precision, and our analysis identifies two main causes: (1) down-projection matrices are a well-known quantization bottleneck, but maintaining their fidelity often requires extra bit-width; (2) weight quantization induces activation deviations, but effective correction strategies remain underexplored. To address these issues, we propose D$^2$Quant, a novel weight-only PTQ framework that improves quantization from both the weight and activation perspectives. On the weight side, we design a Dual-Scale Quantizer (DSQ) tailored to down-projection matrices, with an absorbable scaling factor that significantly improves accuracy without increasing the bit budget. On the activation side, we propose Deviation-Aware Correction (DAC), which incorporates a mean-shift correction within LayerNorm to mitigate quantization-induced activation distribution shifts. Extensive experiments across multiple LLM families and evaluation metrics show that D$^2$Quant delivers superior performance for weight-only PTQ at sub-4-bit precision. The code and models will be available at this https URL.</li>
</ul>

<h3>Title: naPINN: Noise-Adaptive Physics-Informed Neural Networks for Recovering Physics from Corrupted Measurement</h3>
<ul>
<li><strong>Authors: </strong>Hankyeol Kim, Pilsung Kang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02547">https://arxiv.org/abs/2602.02547</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02547">https://arxiv.org/pdf/2602.02547</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02547]] naPINN: Noise-Adaptive Physics-Informed Neural Networks for Recovering Physics from Corrupted Measurement(https://arxiv.org/abs/2602.02547)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Physics-Informed Neural Networks (PINNs) are effective methods for solving inverse problems and discovering governing equations from observational data. However, their performance degrades significantly under complex measurement noise and gross outliers. To address this issue, we propose the Noise-Adaptive Physics-Informed Neural Network (naPINN), which robustly recovers physical solutions from corrupted measurements without prior knowledge of the noise distribution. naPINN embeds an energy-based model into the training loop to learn the latent distribution of prediction residuals. Leveraging the learned energy landscape, a trainable reliability gate adaptively filters data points exhibiting high energy, while a rejection cost regularization prevents trivial solutions where valid data are discarded. We demonstrate the efficacy of naPINN on various benchmark partial differential equations corrupted by non-Gaussian noise and varying rates of outliers. The results show that naPINN significantly outperforms existing robust PINN baselines, successfully isolating outliers and accurately reconstructing the dynamics under severe data corruption.</li>
</ul>

<h3>Title: ToolTok: Tool Tokenization for Efficient and Generalizable GUI Agents</h3>
<ul>
<li><strong>Authors: </strong>Xiaoce Wang, Guibin Zhang, Junzhe Li, Jinzhe Tu, Chun Li, Ming Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02548">https://arxiv.org/abs/2602.02548</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02548">https://arxiv.org/pdf/2602.02548</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02548]] ToolTok: Tool Tokenization for Efficient and Generalizable GUI Agents(https://arxiv.org/abs/2602.02548)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Existing GUI agent models relying on coordinate-based one-step visual grounding struggle with generalizing to varying input resolutions and aspect ratios. Alternatives introduce coordinate-free strategies yet suffer from learning under severe data scarcity. To address the limitations, we propose ToolTok, a novel paradigm of multi-step pathfinding for GUI agents, where operations are modeled as a sequence of progressive tool usage. Specifically, we devise tools aligned with human interaction habits and represent each tool using learnable token embeddings. To enable efficient embedding learning under limited supervision, ToolTok introduces a semantic anchoring mechanism that grounds each tool with semantically related concepts as natural inductive bias. To further enable a pre-trained large language model to progressively acquire tool semantics, we construct an easy-to-hard curriculum consisting of three tasks: token definition question-answering, pure text-guided tool selection, and simplified visual pathfinding. Extensive experiments on multiple benchmarks show that ToolTok achieves superior performance among models of comparable scale (4B) and remains competitive with a substantially larger model (235B). Notably, these results are obtained using less than 1% of the training data required by other post-training approaches. In addition, ToolTok demonstrates strong generalization across unseen scenarios. Our training & inference code is open-source at this https URL.</li>
</ul>

<h3>Title: HyPAC: Cost-Efficient LLMs-Human Hybrid Annotation with PAC Error Guarantees</h3>
<ul>
<li><strong>Authors: </strong>Hao Zeng, Huipeng Huang, Xinhao Qu, Jianguo Huang, Bingyi Jing, Hongxin Wei</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02550">https://arxiv.org/abs/2602.02550</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02550">https://arxiv.org/pdf/2602.02550</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02550]] HyPAC: Cost-Efficient LLMs-Human Hybrid Annotation with PAC Error Guarantees(https://arxiv.org/abs/2602.02550)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Data annotation often involves multiple sources with different cost-quality trade-offs, such as fast large language models (LLMs), slow reasoning models, and human experts. In this work, we study the problem of routing inputs to the most cost-efficient annotation source while controlling the labeling error on test instances. We propose \textbf{HyPAC}, a method that adaptively labels inputs to the most cost-efficient annotation source while providing distribution-free guarantees on annotation error. HyPAC calibrates two decision thresholds using importance sampling and upper confidence bounds, partitioning inputs into three regions based on uncertainty and routing each to the appropriate annotation source. We prove that HyPAC achieves the minimum expected cost with a probably approximately correct (PAC) guarantee on the annotation error, free of data distribution and pre-trained models. Experiments on common benchmarks demonstrate the effectiveness of our method, reducing the annotation cost by 78.51\% while tightly controlling the annotation error.</li>
</ul>

<h3>Title: EEO-TFV: Escape-Explore Optimizer for Web-Scale Time-Series Forecasting and Vision Analysis</h3>
<ul>
<li><strong>Authors: </strong>Hua Wang, Jinghao Lu, Fan Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02551">https://arxiv.org/abs/2602.02551</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02551">https://arxiv.org/pdf/2602.02551</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02551]] EEO-TFV: Escape-Explore Optimizer for Web-Scale Time-Series Forecasting and Vision Analysis(https://arxiv.org/abs/2602.02551)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Transformer-based foundation models have achieved remarkable progress in tasks such as time-series forecasting and image segmentation. However, they frequently suffer from error accumulation in multivariate long-sequence prediction and exhibit vulnerability to out-of-distribution samples in image-related tasks. Furthermore, these challenges become particularly pronounced in large-scale Web data analysis tasks, which typically involve complex temporal patterns and multimodal features. This complexity substantially increases optimization difficulty, rendering models prone to stagnation at saddle points within high-dimensional parameter spaces. To address these issues, we propose a lightweight Transformer architecture in conjunction with a novel Escape-Explore Optimizer (EEO). The optimizer enhances both exploration and generalization while effectively avoiding sharp minima and saddle-point traps. Experimental results show that, in representative Web data scenarios, our method achieves performance on par with state-of-the-art models across 11 time-series benchmark datasets and the Synapse medical image segmentation task. Moreover, it demonstrates superior generalization and stability, thereby validating its potential as a versatile cross-task foundation model for Web-scale data mining and analysis.</li>
</ul>

<h3>Title: Beyond Experience Retrieval: Learning to Generate Utility-Optimized Structured Experience for Frozen LLMs</h3>
<ul>
<li><strong>Authors: </strong>Xuancheng Li, Haitao Li, Yujia Zhou, Yiqun Liu, Qingyao Ai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02556">https://arxiv.org/abs/2602.02556</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02556">https://arxiv.org/pdf/2602.02556</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02556]] Beyond Experience Retrieval: Learning to Generate Utility-Optimized Structured Experience for Frozen LLMs(https://arxiv.org/abs/2602.02556)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are largely static and often redo reasoning or repeat mistakes. Prior experience reuse typically relies on external retrieval, which is similarity-based, can introduce noise, and adds latency. We introduce SEAM (Structured Experience Adapter Module), a lightweight, executor-specific plug-in that stores experience in its parameters and generates a structured, instance-tailored experience entry in a single forward pass to guide a frozen LLM executor. SEAM is trained for utility via executor rollouts and GRPO while keeping the executor frozen, and it can be further improved after deployment with supervised fine-tuning on logged successful trajectories. Experiments on mathematical reasoning benchmarks show consistent accuracy gains across executors with low overhead. Extensive ablations and analyses further elucidate the mechanisms underlying SEAM's effectiveness and robustness.</li>
</ul>

<h3>Title: The Alignment Curse: Cross-Modality Jailbreak Transfer in Omni-Models</h3>
<ul>
<li><strong>Authors: </strong>Yupeng Chen, Junchi Yu, Aoxi Liu, Philip Torr, Adel Bibi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02557">https://arxiv.org/abs/2602.02557</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02557">https://arxiv.org/pdf/2602.02557</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02557]] The Alignment Curse: Cross-Modality Jailbreak Transfer in Omni-Models(https://arxiv.org/abs/2602.02557)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Recent advances in end-to-end trained omni-models have significantly improved multimodal understanding. At the same time, safety red-teaming has expanded beyond text to encompass audio-based jailbreak attacks. However, an important bridge between textual and audio jailbreaks remains underexplored. In this work, we study the cross-modality transfer of jailbreak attacks from text to audio, motivated by the semantic similarity between the two modalities and the maturity of textual jailbreak methods. We first analyze the connection between modality alignment and cross-modality jailbreak transfer, showing that strong alignment can inadvertently propagate textual vulnerabilities to the audio modality, which we term the alignment curse. Guided by this analysis, we conduct an empirical evaluation of textual jailbreaks, text-transferred audio jailbreaks, and existing audio-based jailbreaks on recent omni-models. Our results show that text-transferred audio jailbreaks perform comparably to, and often better than, audio-based jailbreaks, establishing them as simple yet powerful baselines for future audio red-teaming. We further demonstrate strong cross-model transferability and show that text-transferred audio attacks remain effective even under a stricter audio-only access threat model.</li>
</ul>

<h3>Title: PA-MIL: Phenotype-Aware Multiple Instance Learning Guided by Language Prompting and Genotype-to-Phenotype Relationships</h3>
<ul>
<li><strong>Authors: </strong>Zekang Yang, Hong Liu, Xiangdong Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02558">https://arxiv.org/abs/2602.02558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02558">https://arxiv.org/pdf/2602.02558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02558]] PA-MIL: Phenotype-Aware Multiple Instance Learning Guided by Language Prompting and Genotype-to-Phenotype Relationships(https://arxiv.org/abs/2602.02558)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Deep learning has been extensively researched in the analysis of pathology whole-slide images (WSIs). However, most existing methods are limited to providing prediction interpretability by locating the model's salient areas in a post-hoc manner, failing to offer more reliable and accountable explanations. In this work, we propose Phenotype-Aware Multiple Instance Learning (PA-MIL), a novel ante-hoc interpretable framework that identifies cancer-related phenotypes from WSIs and utilizes them for cancer subtyping. To facilitate PA-MIL in learning phenotype-aware features, we 1) construct a phenotype knowledge base containing cancer-related phenotypes and their associated genotypes. 2) utilize the morphological descriptions of phenotypes as language prompting to aggregate phenotype-related features. 3) devise the Genotype-to-Phenotype Neural Network (GP-NN) grounded in genotype-to-phenotype relationships, which provides multi-level guidance for PA-MIL. Experimental results on multiple datasets demonstrate that PA-MIL achieves competitive performance compared to existing MIL methods while offering improved interpretability. PA-MIL leverages phenotype saliency as evidence and, using a linear classifier, achieves competitive results compared to state-of-the-art methods. Additionally, we thoroughly analyze the genotype-phenotype relationships, as well as cohort-level and case-level interpretability, demonstrating the reliability and accountability of PA-MIL.</li>
</ul>

<h3>Title: Auditing Sybil: Explaining Deep Lung Cancer Risk Prediction Through Generative Interventional Attributions</h3>
<ul>
<li><strong>Authors: </strong>Bartlomiej Sobieski, Jakub Grzywaczewski, Karol Dobiczek, Mateusz Wójcik, Tomasz Bartczak, Patryk Szatkowski, Przemysław Bombiński, Matthew Tivnan, Przemyslaw Biecek</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02560">https://arxiv.org/abs/2602.02560</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02560">https://arxiv.org/pdf/2602.02560</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02560]] Auditing Sybil: Explaining Deep Lung Cancer Risk Prediction Through Generative Interventional Attributions(https://arxiv.org/abs/2602.02560)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Lung cancer remains the leading cause of cancer mortality, driving the development of automated screening tools to alleviate radiologist workload. Standing at the frontier of this effort is Sybil, a deep learning model capable of predicting future risk solely from computed tomography (CT) with high precision. However, despite extensive clinical validation, current assessments rely purely on observational metrics. This correlation-based approach overlooks the model's actual reasoning mechanism, necessitating a shift to causal verification to ensure robust decision-making before clinical deployment. We propose S(H)NAP, a model-agnostic auditing framework that constructs generative interventional attributions validated by expert radiologists. By leveraging realistic 3D diffusion bridge modeling to systematically modify anatomical features, our approach isolates object-specific causal contributions to the risk score. Providing the first interventional audit of Sybil, we demonstrate that while the model often exhibits behavior akin to an expert radiologist, differentiating malignant pulmonary nodules from benign ones, it suffers from critical failure modes, including dangerous sensitivity to clinically unjustified artifacts and a distinct radial bias.</li>
</ul>

<h3>Title: A Comparative Simulation Study of the Fairness and Accuracy of Predictive Policing Systems in Baltimore City</h3>
<ul>
<li><strong>Authors: </strong>Samin Semsar, Kiran Laxmikant Prabhu, Gabriella Waters, James Foulds</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02566">https://arxiv.org/abs/2602.02566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02566">https://arxiv.org/pdf/2602.02566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02566]] A Comparative Simulation Study of the Fairness and Accuracy of Predictive Policing Systems in Baltimore City(https://arxiv.org/abs/2602.02566)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>There are ongoing discussions about predictive policing systems, such as those deployed in Los Angeles, California and Baltimore, Maryland, being unfair, for example, by exhibiting racial bias. Studies found that unfairness may be due to feedback loops and being trained on historically biased recorded data. However, comparative studies on predictive policing systems are few and are not sufficiently comprehensive. In this work, we perform a comprehensive comparative simulation study on the fairness and accuracy of predictive policing technologies in Baltimore. Our results suggest that the situation around bias in predictive policing is more complex than was previously assumed. While predictive policing exhibited bias due to feedback loops as was previously reported, we found that the traditional alternative, hot spots policing, had similar issues. Predictive policing was found to be more fair and accurate than hot spots policing in the short term, although it amplified bias faster, suggesting the potential for worse long-run behavior. In Baltimore, in some cases the bias in these systems tended toward over-policing in White neighborhoods, unlike in previous studies. Overall, this work demonstrates a methodology for city-specific evaluation and behavioral-tendency comparison of predictive policing systems, showing how such simulations can reveal inequities and long-term tendencies.</li>
</ul>

<h3>Title: DECEIVE-AFC: Adversarial Claim Attacks against Search-Enabled LLM-based Fact-Checking Systems</h3>
<ul>
<li><strong>Authors: </strong>Haoran Ou, Kangjie Chen, Gelei Deng, Hangcheng Liu, Jie Zhang, Tianwei Zhang, Kwok-Yan Lam</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02569">https://arxiv.org/abs/2602.02569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02569">https://arxiv.org/pdf/2602.02569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02569]] DECEIVE-AFC: Adversarial Claim Attacks against Search-Enabled LLM-based Fact-Checking Systems(https://arxiv.org/abs/2602.02569)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Fact-checking systems with search-enabled large language models (LLMs) have shown strong potential for verifying claims by dynamically retrieving external evidence. However, the robustness of such systems against adversarial attack remains insufficiently understood. In this work, we study adversarial claim attacks against search-enabled LLM-based fact-checking systems under a realistic input-only threat model. We propose DECEIVE-AFC, an agent-based adversarial attack framework that integrates novel claim-level attack strategies and adversarial claim validity evaluation principles. DECEIVE-AFC systematically explores adversarial attack trajectories that disrupt search behavior, evidence retrieval, and LLM-based reasoning without relying on access to evidence sources or model internals. Extensive evaluations on benchmark datasets and real-world systems demonstrate that our attacks substantially degrade verification performance, reducing accuracy from 78.7% to 53.7%, and significantly outperform existing claim-based attack baselines with strong cross-system transferability.</li>
</ul>

<h3>Title: Trajectory Consistency for One-Step Generation on Euler Mean Flows</h3>
<ul>
<li><strong>Authors: </strong>Zhiqi Li, Yuchen Sun, Duowen Chen, Jinjin He, Bo Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02571">https://arxiv.org/abs/2602.02571</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02571">https://arxiv.org/pdf/2602.02571</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02571]] Trajectory Consistency for One-Step Generation on Euler Mean Flows(https://arxiv.org/abs/2602.02571)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We propose \emph{Euler Mean Flows (EMF)}, a flow-based generative framework for one-step and few-step generation that enforces long-range trajectory consistency with minimal sampling cost. The key idea of EMF is to replace the trajectory consistency constraint, which is difficult to supervise and optimize over long time scales, with a principled linear surrogate that enables direct data supervision for long-horizon flow-map compositions. We derive this approximation from the semigroup formulation of flow-based models and show that, under mild regularity assumptions, it faithfully approximates the original consistency objective while being substantially easier to optimize. This formulation leads to a unified, JVP-free training framework that supports both $u$-prediction and $x_1$-prediction variants, avoiding explicit Jacobian computations and significantly reducing memory and computational overhead. Experiments on image synthesis, particle-based geometry generation, and functional generation demonstrate improved optimization stability and sample quality under fixed sampling budgets, together with approximately $50\%$ reductions in training time and memory consumption compared to existing one-step methods for image generation.</li>
</ul>

<h3>Title: QuantLRM: Quantization of Large Reasoning Models via Fine-Tuning Signals</h3>
<ul>
<li><strong>Authors: </strong>Nan Zhang, Eugene Kwek, Yusen Zhang, Muyu Pan, Suhang Wang, Prasenjit Mitra, Rui Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02581">https://arxiv.org/abs/2602.02581</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02581">https://arxiv.org/pdf/2602.02581</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02581]] QuantLRM: Quantization of Large Reasoning Models via Fine-Tuning Signals(https://arxiv.org/abs/2602.02581)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, large language model</a></li>
<li><strong>Abstract: </strong>Weight-only quantization is important for compressing Large Language Models (LLMs). Inspired by the spirit of classical magnitude pruning, we study whether the magnitude of weight updates during reasoning-incentivized fine-tuning can provide valuable signals for quantizing Large Reasoning Models (LRMs). We hypothesize that the smallest and largest weight updates during fine-tuning are more important than those of intermediate magnitude, a phenomenon we term "protecting both ends". Upon hypothesis validation, we introduce QuantLRM, which stands for weight quantization of LRMs via fine-tuning signals. We fit simple restricted quadratic functions on weight updates to protect both ends. By multiplying the average quadratic values with the count of zero weight updates of channels, we compute channel importance that is more effective than using activation or second-order information. We run QuantLRM to quantize various fine-tuned models (including supervised, direct preference optimization, and reinforcement learning fine-tuning) over four reasoning benchmarks (AIME-120, FOLIO, temporal sequences, and GPQA-Diamond) and empirically find that QuantLRM delivers a consistent improvement for LRMs quantization, with an average improvement of 6.55% on a reinforcement learning fine-tuned model. Also supporting non-fine-tuned LRMs, QuantLRM gathers effective signals via pseudo-fine-tuning, which greatly enhances its applicability.</li>
</ul>

<h3>Title: Learnable Koopman-Enhanced Transformer-Based Time Series Forecasting with Spectral Control</h3>
<ul>
<li><strong>Authors: </strong>Ali Forootani, Raffaele Iervolino</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02592">https://arxiv.org/abs/2602.02592</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02592">https://arxiv.org/pdf/2602.02592</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02592]] Learnable Koopman-Enhanced Transformer-Based Time Series Forecasting with Spectral Control(https://arxiv.org/abs/2602.02592)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This paper proposes a unified family of learnable Koopman operator parameterizations that integrate linear dynamical systems theory with modern deep learning forecasting architectures. We introduce four learnable Koopman variants-scalar-gated, per-mode gated, MLP-shaped spectral mapping, and low-rank Koopman operators which generalize and interpolate between strictly stable Koopman operators and unconstrained linear latent dynamics. Our formulation enables explicit control over the spectrum, stability, and rank of the linear transition operator while retaining compatibility with expressive nonlinear backbones such as Patchtst, Autoformer, and Informer. We evaluate the proposed operators in a large-scale benchmark that also includes LSTM, DLinear, and simple diagonal State-Space Models (SSMs), as well as lightweight transformer variants. Experiments across multiple horizons and patch lengths show that learnable Koopman models provide a favorable bias-variance trade-off, improved conditioning, and more interpretable latent dynamics. We provide a full spectral analysis, including eigenvalue trajectories, stability envelopes, and learned spectral distributions. Our results demonstrate that learnable Koopman operators are effective, stable, and theoretically principled components for deep forecasting.</li>
</ul>

<h3>Title: To Defend Against Cyber Attacks, We Must Teach AI Agents to Hack</h3>
<ul>
<li><strong>Authors: </strong>Terry Yue Zhuo, Yangruibo Ding, Wenbo Guo, Ruijie Meng</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02595">https://arxiv.org/abs/2602.02595</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02595">https://arxiv.org/pdf/2602.02595</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02595]] To Defend Against Cyber Attacks, We Must Teach AI Agents to Hack(https://arxiv.org/abs/2602.02595)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, defense, attack</a></li>
<li><strong>Abstract: </strong>For over a decade, cybersecurity has relied on human labor scarcity to limit attackers to high-value targets manually or generic automated attacks at scale. Building sophisticated exploits requires deep expertise and manual effort, leading defenders to assume adversaries cannot afford tailored attacks at scale. AI agents break this balance by automating vulnerability discovery and exploitation across thousands of targets, needing only small success rates to remain profitable. Current developers focus on preventing misuse through data filtering, safety alignment, and output guardrails. Such protections fail against adversaries who control open-weight models, bypass safety controls, or develop offensive capabilities independently. We argue that AI-agent-driven cyber attacks are inevitable, requiring a fundamental shift in defensive strategy. In this position paper, we identify why existing defenses cannot stop adaptive adversaries and demonstrate that defenders must develop offensive security intelligence. We propose three actions for building frontier offensive AI capabilities responsibly. First, construct comprehensive benchmarks covering the full attack lifecycle. Second, advance from workflow-based to trained agents for discovering in-wild vulnerabilities at scale. Third, implement governance restricting offensive agents to audited cyber ranges, staging release by capability tier, and distilling findings into safe defensive-only agents. We strongly recommend treating offensive AI capabilities as essential defensive infrastructure, as containing cybersecurity risks requires mastering them in controlled settings before adversaries do.</li>
</ul>

<h3>Title: ContextEvolve: Multi-Agent Context Compression for Systems Code Optimization</h3>
<ul>
<li><strong>Authors: </strong>Hongyuan Su, Yu Zheng, Yong Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02597">https://arxiv.org/abs/2602.02597</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02597">https://arxiv.org/pdf/2602.02597</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02597]] ContextEvolve: Multi-Agent Context Compression for Systems Code Optimization(https://arxiv.org/abs/2602.02597)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models are transforming systems research by automating the discovery of performance-critical algorithms for computer systems. Despite plausible codes generated by LLMs, producing solutions that meet the stringent correctness and performance requirements of systems demands iterative optimization. Test-time reinforcement learning offers high search efficiency but requires parameter updates infeasible under API-only access, while existing training-free evolutionary methods suffer from inefficient context utilization and undirected search. We introduce ContextEvolve, a multi-agent framework that achieves RL-level search efficiency under strict parameter-blind constraints by decomposing optimization context into three orthogonal dimensions: a Summarizer Agent condenses semantic state via code-to-language abstraction, a Navigator Agent distills optimization direction from trajectory analysis, and a Sampler Agent curates experience distribution through prioritized exemplar retrieval. This orchestration forms a functional isomorphism with RL-mapping to state representation, policy gradient, and experience replay-enabling principled optimization in a textual latent space. On the ADRS benchmark, ContextEvolve outperforms state-of-the-art baselines by 33.3% while reducing token consumption by 29.0%. Codes for our work are released at this https URL</li>
</ul>

<h3>Title: RAP: KV-Cache Compression via RoPE-Aligned Pruning</h3>
<ul>
<li><strong>Authors: </strong>Jihao Xin, Tian Lvu, Hatem Ltaief, David Keyes, Marco Canini</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02599">https://arxiv.org/abs/2602.02599</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02599">https://arxiv.org/pdf/2602.02599</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02599]] RAP: KV-Cache Compression via RoPE-Aligned Pruning(https://arxiv.org/abs/2602.02599)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Long-context inference in large language models is increasingly bottlenecked by the memory and compute cost of the KV-Cache. Low-rank factorization compresses KV projections by writing $W \approx A * B$, where A produces latent KV states and B can be absorbed into downstream weights. In modern RoPE-based LLMs, this absorption fails: RoPE forces latent KV states to be reconstructed to full dimension, reintroducing substantial memory and compute overhead. We propose RoPE-Aligned Pruning (RAP), which prunes entire RoPE-aligned column pairs to preserve RoPE's 2x2 rotation structure, restore B absorption, and eliminate reconstruction. Our evaluation on LLaMA-3-8B and Mistral-7B shows that RAP enables joint reduction of KV-Cache, attention parameters, and FLOPs by 20-30%, all at once, while maintaining strong accuracy. Notably, RAP reduces attention latency to 83% (prefill) and 77% (decode) of baseline.</li>
</ul>

<h3>Title: Step-Wise Refusal Dynamics in Autoregressive and Diffusion Language Models</h3>
<ul>
<li><strong>Authors: </strong>Eliron Rahimi, Elad Hirshel, Rom Himelstein, Amit LeVi, Avi Mendelson, Chaim Baskin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02600">https://arxiv.org/abs/2602.02600</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02600">https://arxiv.org/pdf/2602.02600</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02600]] Step-Wise Refusal Dynamics in Autoregressive and Diffusion Language Models(https://arxiv.org/abs/2602.02600)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, interpretability, diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion language models (DLMs) have recently emerged as a promising alternative to autoregressive (AR) models, offering parallel decoding and controllable sampling dynamics while achieving competitive generation quality at scale. Despite this progress, the role of sampling mechanisms in shaping refusal behavior and jailbreak robustness remains poorly understood. In this work, we present a fundamental analytical framework for step-wise refusal dynamics, enabling comparison between AR and diffusion sampling. Our analysis reveals that the sampling strategy itself plays a central role in safety behavior, as a factor distinct from the underlying learned representations. Motivated by this analysis, we introduce the Step-Wise Refusal Internal Dynamics (SRI) signal, which supports interpretability and improved safety for both AR and DLMs. We demonstrate that the geometric structure of SRI captures internal recovery dynamics, and identifies anomalous behavior in harmful generations as cases of \emph{incomplete internal recovery} that are not observable at the text level. This structure enables lightweight inference-time detectors that generalize to unseen attacks while matching or outperforming existing defenses with over $100\times$ lower inference overhead.</li>
</ul>

<h3>Title: Position: 3D Gaussian Splatting Watermarking Should Be Scenario-Driven and Threat-Model Explicit</h3>
<ul>
<li><strong>Authors: </strong>Yangfan Deng, Anirudh Nakra, Min Wu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02602">https://arxiv.org/abs/2602.02602</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02602">https://arxiv.org/pdf/2602.02602</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02602]] Position: 3D Gaussian Splatting Watermarking Should Be Scenario-Driven and Threat-Model Explicit(https://arxiv.org/abs/2602.02602)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, watermark</a></li>
<li><strong>Abstract: </strong>3D content acquisition and creation are expanding rapidly in the new era of machine learning and AI. 3D Gaussian Splatting (3DGS) has become a promising high-fidelity and real-time representation for 3D content. Similar to the initial wave of digital audio-visual content at the turn of the millennium, the demand for intellectual property protection is also increasing, since explicit and editable 3D parameterization makes unauthorized use and dissemination easier. In this position paper, we argue that effective progress in watermarking 3D assets requires articulated security objectives and realistic threat models, incorporating the lessons learned from digital audio-visual asset protection over the past decades. To address this gap in security specification and evaluation, we advocate a scenario-driven formulation, in which adversarial capabilities are formalized through a security model. Based on this formulation, we construct a reference framework that organizes existing methods and clarifies how specific design choices map to corresponding adversarial assumptions. Within this framework, we also examine a legacy spread-spectrum embedding scheme, characterizing its advantages and limitations and highlighting the important trade-offs it entails. Overall, this work aims to foster effective intellectual property protection for 3D assets.</li>
</ul>

<h3>Title: ClinConNet: A Blockchain-based Dynamic Consent Management Platform for Clinical Research</h3>
<ul>
<li><strong>Authors: </strong>Montassar Naghmouchi, Maryline Laurent</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02610">https://arxiv.org/abs/2602.02610</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02610">https://arxiv.org/pdf/2602.02610</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02610]] ClinConNet: A Blockchain-based Dynamic Consent Management Platform for Clinical Research(https://arxiv.org/abs/2602.02610)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect</a></li>
<li><strong>Abstract: </strong>Consent is an ethical cornerstone of clinical research and healthcare in general. Although the ethical principles of consent - providing information, ensuring comprehension, and ensuring voluntariness - are well-defined, the technological infrastructure remains outdated. Clinicians are responsible for obtaining informed consent from research subjects or patients, and for managing it before, during, and after clinical trials or care, which is a burden for them. The voluntary nature of participating in clinical research or undergoing medical treatment implies the need for a participant-centric consent management system. However, this is not reflected in most established systems. Not only do most healthcare information systems not follow a user-centric model, but they also create data silos, which significantly reduce the mobility of patient data between different healthcare institutions and impact personalized medicine. Furthermore, consent management tools are outdated. We propose ClinConNet (Clinical Consent Network), a platform that connects researchers and participants based on clinical research projects. ClinConNet is powered by a dynamic consent model based on blockchain and take advantage of dynamic consent interfaces, as well as blockchain and Self-Sovereign Identity systems. ClinConNet is user-centric and provides important privacy features for patients, such as unlinkability, confidentiality, and ownership of identity data. It is also compatible with the right to be forgotten, as defined in many personal data protection regulations, such as the GDPR. We provide a detailed privacy and security analysis in an adversarial model, as well as a Proof of Concept implementation with detailed performance measures that demonstrate the feasibility of our blockchain-based consent management system with a median end-to-end consent establishment time of under 200ms and a throughput of 250TPS.</li>
</ul>

<h3>Title: TinyGuard:A lightweight Byzantine Defense for Resource-Constrained Federated Learning via Statistical Update Fingerprints</h3>
<ul>
<li><strong>Authors: </strong>Ali Mahdavi, Santa Aghapour, Azadeh Zamanifar, Amirfarhad Farhadi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02615">https://arxiv.org/abs/2602.02615</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02615">https://arxiv.org/pdf/2602.02615</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02615]] TinyGuard:A lightweight Byzantine Defense for Resource-Constrained Federated Learning via Statistical Update Fingerprints(https://arxiv.org/abs/2602.02615)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Existing Byzantine robust aggregation mechanisms typically rely on fulldimensional gradi ent comparisons or pairwise distance computations, resulting in computational overhead that limits applicability in large scale and resource constrained federated systems. This paper proposes TinyGuard, a lightweight Byzantine defense that augments the standard FedAvg algorithm via statistical update f ingerprinting. Instead of operating directly on high-dimensional gradients, TinyGuard extracts compact statistical fingerprints cap turing key behavioral properties of client updates, including norm statistics, layer-wise ratios, sparsity measures, and low-order mo ments. Byzantine clients are identified by measuring robust sta tistical deviations in this low-dimensional fingerprint space with nd complexity, without modifying the underlying optimization procedure. Extensive experiments on MNIST, Fashion-MNIST, ViT-Lite, and ViT-Small with LoRA adapters demonstrate that TinyGuard pre serves FedAvg convergence in benign settings and achieves up to 95 percent accuracy under multiple Byzantine attack scenarios, including sign-flipping, scaling, noise injection, and label poisoning. Against adaptive white-box adversaries, Pareto frontier analysis across four orders of magnitude confirms that attackers cannot simultaneously evade detection and achieve effective poisoning, features we term statistical handcuffs. Ablation studies validate stable detection precision 0.8 across varying client counts (50-150), threshold parameters and extreme data heterogeneity . The proposed framework is architecture-agnostic and well-suited for federated fine-tuning of foundation models where traditional Byzantine defenses become impractical</li>
</ul>

<h3>Title: daVinci-Agency: Unlocking Long-Horizon Agency Data-Efficiently</h3>
<ul>
<li><strong>Authors: </strong>Mohan Jiang, Dayuan Fu, Junhao Shi, Ji Zeng, Weiye Si, Keyu Li, Xuefeng Li, Yang Xiao, Wenjie Li, Dequan Wang, Pengfei Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02619">https://arxiv.org/abs/2602.02619</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02619">https://arxiv.org/pdf/2602.02619</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02619]] daVinci-Agency: Unlocking Long-Horizon Agency Data-Efficiently(https://arxiv.org/abs/2602.02619)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While Large Language Models (LLMs) excel at short-term tasks, scaling them to long-horizon agentic workflows remains challenging. The core bottleneck lies in the scarcity of training data that captures authentic long-dependency structures and cross-stage evolutionary dynamics--existing synthesis methods either confine to single-feature scenarios constrained by model distribution, or incur prohibitive human annotation costs, failing to provide scalable, high-quality supervision. We address this by reconceptualizing data synthesis through the lens of real-world software evolution. Our key insight: Pull Request (PR) sequences naturally embody the supervision signals for long-horizon learning. They decompose complex objectives into verifiable submission units, maintain functional coherence across iterations, and encode authentic refinement patterns through bug-fix histories. Building on this, we propose daVinci-Agency, which systematically mines structured supervision from chain-of-PRs through three interlocking mechanisms: (1) progressive task decomposition via continuous commits, (2) long-term consistency enforcement through unified functional objectives, and (3) verifiable refinement from authentic bug-fix trajectories. Unlike synthetic trajectories that treat each step independently, daVinci-Agency's PR-grounded structure inherently preserves the causal dependencies and iterative refinements essential for teaching persistent goal-directed behavior and enables natural alignment with project-level, full-cycle task modeling. The resulting trajectories are substantial--averaging 85k tokens and 116 tool calls--yet remarkably data-efficient: fine-tuning GLM-4.6 on 239 daVinci-Agency samples yields broad improvements across benchmarks, notably achieving a 47% relative gain on Toolathlon. Beyond benchmark performance, our analysis confirms...</li>
</ul>

<h3>Title: Learning Consistent Causal Abstraction Networks</h3>
<ul>
<li><strong>Authors: </strong>Gabriele D'Acunto, Paolo Di Lorenzo, Sergio Barbarossa</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02623">https://arxiv.org/abs/2602.02623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02623">https://arxiv.org/pdf/2602.02623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02623]] Learning Consistent Causal Abstraction Networks(https://arxiv.org/abs/2602.02623)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, explainability</a></li>
<li><strong>Abstract: </strong>Causal artificial intelligence aims to enhance explainability, trustworthiness, and robustness in AI by leveraging structural causal models (SCMs). In this pursuit, recent advances formalize network sheaves and cosheaves of causal knowledge. Pushing in the same direction, we tackle the learning of consistent causal abstraction network (CAN), a sheaf-theoretic framework where (i) SCMs are Gaussian, (ii) restriction maps are transposes of constructive linear causal abstractions (CAs) adhering to the semantic embedding principle, and (iii) edge stalks correspond--up to permutation--to the node stalks of more detailed SCMs. Our problem formulation separates into edge-specific local Riemannian problems and avoids nonconvex objectives. We propose an efficient search procedure, solving the local problems with SPECTRAL, our iterative method with closed-form updates and suitable for positive definite and semidefinite covariance matrices. Experiments on synthetic data show competitive performance in the CA learning task, and successful recovery of diverse CAN structures.</li>
</ul>

<h3>Title: Learning Better Certified Models from Empirically-Robust Teachers</h3>
<ul>
<li><strong>Authors: </strong>Alessandro De Palma</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02626">https://arxiv.org/abs/2602.02626</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02626">https://arxiv.org/pdf/2602.02626</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02626]] Learning Better Certified Models from Empirically-Robust Teachers(https://arxiv.org/abs/2602.02626)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Adversarial training attains strong empirical robustness to specific adversarial attacks by training on concrete adversarial perturbations, but it produces neural networks that are not amenable to strong robustness certificates through neural network verification. On the other hand, earlier certified training schemes directly train on bounds from network relaxations to obtain models that are certifiably robust, but display sub-par standard performance. Recent work has shown that state-of-the-art trade-offs between certified robustness and standard performance can be obtained through a family of losses combining adversarial outputs and neural network bounds. Nevertheless, differently from empirical robustness, verifiability still comes at a significant cost in standard performance. In this work, we propose to leverage empirically-robust teachers to improve the performance of certifiably-robust models through knowledge distillation. Using a versatile feature-space distillation objective, we show that distillation from adversarially-trained teachers consistently improves on the state-of-the-art in certified training for ReLU networks across a series of robust computer vision benchmarks.</li>
</ul>

<h3>Title: Trustworthy Blockchain-based Federated Learning for Electronic Health Records: Securing Participant Identity with Decentralized Identifiers and Verifiable Credentials</h3>
<ul>
<li><strong>Authors: </strong>Rodrigo Tertulino, Ricardo Almeida, Laercio Alencar</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02629">https://arxiv.org/abs/2602.02629</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02629">https://arxiv.org/pdf/2602.02629</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02629]] Trustworthy Blockchain-based Federated Learning for Electronic Health Records: Securing Participant Identity with Decentralized Identifiers and Verifiable Credentials(https://arxiv.org/abs/2602.02629)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>The digitization of healthcare has generated massive volumes of Electronic Health Records (EHRs), offering unprecedented opportunities for training Artificial Intelligence (AI) models. However, stringent privacy regulations such as GDPR and HIPAA have created data silos that prevent centralized training. Federated Learning (FL) has emerged as a promising solution that enables collaborative model training without sharing raw patient data. Despite its potential, FL remains vulnerable to poisoning and Sybil attacks, in which malicious participants corrupt the global model or infiltrate the network using fake identities. While recent approaches integrate Blockchain technology for auditability, they predominantly rely on probabilistic reputation systems rather than robust cryptographic identity verification. This paper proposes a Trustworthy Blockchain-based Federated Learning (TBFL) framework integrating Self-Sovereign Identity (SSI) standards. By leveraging Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs), our architecture ensures only authenticated healthcare entities contribute to the global model. Through comprehensive evaluation using the MIMIC-IV dataset, we demonstrate that anchoring trust in cryptographic identity verification rather than behavioral patterns significantly mitigates security risks while maintaining clinical utility. Our results show the framework successfully neutralizes 100% of Sybil attacks, achieves robust predictive performance (AUC = 0.954, Recall = 0.890), and introduces negligible computational overhead (<0.12%). The approach provides a secure, scalable, and economically viable ecosystem for inter-institutional health data collaboration, with total operational costs of approximately $18 for 100 training rounds across multiple institutions.</li>
</ul>

<h3>Title: Performance of Small Language Model Pretraining on FABRIC: An Empirical Study</h3>
<ul>
<li><strong>Authors: </strong>Praveen Rao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02632">https://arxiv.org/abs/2602.02632</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02632">https://arxiv.org/pdf/2602.02632</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02632]] Performance of Small Language Model Pretraining on FABRIC: An Empirical Study(https://arxiv.org/abs/2602.02632)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) require enormous computing power to pretrain on massive datasets. When limited datasets are available, smaller-sized LLMs are better choice to pretrain (on user-specified datasets) by following the scaling laws of LLMs. Using pretrained models, vector embeddings can be generated for raw data and stored using vector databases to support modern AI applications and semantic search. In this work, we investigate the performance of pretraining techniques for smaller-sized LLMs on an experimental testbed (with commodity GPUs) available to academic users at no charge. We consider data parallelism, intra-operator parallelism, and inter-operator/pipeline parallelism, and their combinations for pretraining. We set up different GPU clusters with homogeneous and heterogeneous GPU hardware. Furthermore, we investigate the impact of network latency on pretraining performance especially when GPUs are geographically distributed. We used GPT-2 medium and large models and pretrained them using open-source packages, namely, Alpa and Ray. We observed that Alpa's execution plans that collectively optimized intra-operator and inter-operator/pipeline parallelism consistently performed the best when GPUs were geographically distributed. This was especially true when the network latencies were in 10's of milliseconds. Based on the insights gained from the experiments, we propose a systematic approach for selecting the appropriate pretraining technique to achieve high training performance/lower execution time as well as to reduce the number of GPUs used.</li>
</ul>

<h3>Title: Graph-Augmented Reasoning with Large Language Models for Tobacco Pest and Disease Management</h3>
<ul>
<li><strong>Authors: </strong>Siyu Li, Chenwei Song, Qi Zhou, Wan Zhou, Xinyi Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02635">https://arxiv.org/abs/2602.02635</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02635">https://arxiv.org/pdf/2602.02635</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02635]] Graph-Augmented Reasoning with Large Language Models for Tobacco Pest and Disease Management(https://arxiv.org/abs/2602.02635)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>This paper proposes a graph-augmented reasoning framework for tobacco pest and disease management that integrates structured domain knowledge into large language models. Building on GraphRAG, we construct a domain-specific knowledge graph and retrieve query-relevant subgraphs to provide relational evidence during answer generation. The framework adopts ChatGLM as the Transformer backbone with LoRA-based parameter-efficient fine-tuning, and employs a graph neural network to learn node representations that capture symptom-disease-treatment dependencies. By explicitly modeling diseases, symptoms, pesticides, and control measures as linked entities, the system supports evidence-aware retrieval beyond surface-level text similarity. Retrieved graph evidence is incorporated into the LLM input to guide generation toward domain-consistent recommendations and to mitigate hallucinated or inappropriate treatments. Experimental results show consistent improvements over text-only baselines, with the largest gains observed on multi-hop and comparative reasoning questions that require chaining multiple relations.</li>
</ul>

<h3>Title: Benchmarking Large Language Models for Zero-shot and Few-shot Phishing URL Detection</h3>
<ul>
<li><strong>Authors: </strong>Najmul Hasan, Prashanth BusiReddyGari</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02641">https://arxiv.org/abs/2602.02641</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02641">https://arxiv.org/pdf/2602.02641</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02641]] Benchmarking Large Language Models for Zero-shot and Few-shot Phishing URL Detection(https://arxiv.org/abs/2602.02641)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, defense, attack, generative, large language model</a></li>
<li><strong>Abstract: </strong>The Uniform Resource Locator (URL), introduced in a connectivity-first era to define access and locate resources, remains historically limited, lacking future-proof mechanisms for security, trust, or resilience against fraud and abuse, despite the introduction of reactive protections like HTTPS during the cybersecurity era. In the current AI-first threatscape, deceptive URLs have reached unprecedented sophistication due to the widespread use of generative AI by cybercriminals and the AI-vs-AI arms race to produce context-aware phishing websites and URLs that are virtually indistinguishable to both users and traditional detection tools. Although AI-generated phishing accounted for a small fraction of filter-bypassing attacks in 2024, phishing volume has escalated over 4,000% since 2022, with nearly 50% more attacks evading detection. At the rate the threatscape is escalating, and phishing tactics are emerging faster than labeled data can be produced, zero-shot and few-shot learning with large language models (LLMs) offers a timely and adaptable solution, enabling generalization with minimal supervision. Given the critical importance of phishing URL detection in large-scale cybersecurity defense systems, we present a comprehensive benchmark of LLMs under a unified zero-shot and few-shot prompting framework and reveal operational trade-offs. Our evaluation uses a balanced dataset with consistent prompts, offering detailed analysis of performance, generalization, and model efficacy, quantified by accuracy, precision, recall, F1 score, AUROC, and AUPRC, to reflect both classification quality and practical utility in threat detection settings. We conclude few-shot prompting improves performance across multiple LLMs.</li>
</ul>

<h3>Title: MARA: Continuous SE(3)-Equivariant Attention for Molecular Force Fields</h3>
<ul>
<li><strong>Authors: </strong>Francesco Leonardi, Boris Bonev, Kaspar Riesen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02671">https://arxiv.org/abs/2602.02671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02671">https://arxiv.org/pdf/2602.02671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02671]] MARA: Continuous SE(3)-Equivariant Attention for Molecular Force Fields(https://arxiv.org/abs/2602.02671)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Machine learning force fields (MLFFs) have become essential for accurate and efficient atomistic modeling. Despite their high accuracy, most existing approaches rely on fixed angular expansions, limiting flexibility in weighting local geometric interactions. We introduce Modular Angular-Radial Attention (MARA), a module that extends spherical attention -- originally developed for SO(3) tasks -- to the molecular domain and SE(3), providing an efficient approximation of equivariant interactions. MARA operates directly on the angular and radial coordinates of neighboring atoms, enabling flexible, geometrically informed, and modular weighting of local environments. Unlike existing attention mechanisms in SE(3)-equivariant architectures, MARA can be integrated in a plug-and-play manner into models such as MACE without architectural modifications. Across molecular benchmarks, MARA improves energy and force predictions, reduces high-error events, and enhances robustness. These results demonstrate that continuous spherical attention is an effective and generalizable geometric operator that increases the expressiveness, stability, and reliability of atomistic models.</li>
</ul>

<h3>Title: FlexRank: Nested Low-Rank Knowledge Decomposition for Adaptive Model Deployment</h3>
<ul>
<li><strong>Authors: </strong>Riccardo Zaccone, Stefanos Laskaridis, Marco Ciccone, Samuel Horváth</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02680">https://arxiv.org/abs/2602.02680</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02680">https://arxiv.org/pdf/2602.02680</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02680]] FlexRank: Nested Low-Rank Knowledge Decomposition for Adaptive Model Deployment(https://arxiv.org/abs/2602.02680)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>The growing scale of deep neural networks, encompassing large language models (LLMs) and vision transformers (ViTs), has made training from scratch prohibitively expensive and deployment increasingly costly. These models are often used as computational monoliths with fixed cost, a rigidity that does not leverage overparametrized architectures and largely hinders adaptive deployment across different cost budgets. We argue that importance-ordered nested components can be extracted from pretrained models, and selectively activated on the available computational budget. To this end, our proposed FlexRank method leverages low-rank weight decomposition with nested, importance-based consolidation to extract submodels of increasing capabilities. Our approach enables a "train-once, deploy-everywhere" paradigm that offers a graceful trade-off between cost and performance without training from scratch for each budget - advancing practical deployment of large models.</li>
</ul>

<h3>Title: Expert-Data Alignment Governs Generation Quality in Decentralized Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Marcos Villagra, Bidhan Roy, Raihan Seraj, Zhiying Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02685">https://arxiv.org/abs/2602.02685</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02685">https://arxiv.org/pdf/2602.02685</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02685]] Expert-Data Alignment Governs Generation Quality in Decentralized Diffusion Models(https://arxiv.org/abs/2602.02685)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Decentralized Diffusion Models (DDMs) route denoising through experts trained independently on disjoint data clusters, which can strongly disagree in their predictions. What governs the quality of generations in such systems? We present the first ever systematic investigation of this question. A priori, the expectation is that minimizing denoising trajectory sensitivity -- minimizing how perturbations amplify during sampling -- should govern generation quality. We demonstrate this hypothesis is incorrect: a stability-quality dissociation. Full ensemble routing, which combines all expert predictions at each step, achieves the most stable sampling dynamics and best numerical convergence while producing the worst generation quality (FID 47.9 vs. 22.6 for sparse Top-2 routing). Instead, we identify expert-data alignment as the governing principle: generation quality depends on routing inputs to experts whose training distribution covers the current denoising state. Across two distinct DDM systems, we validate expert-data alignment using (i) data-cluster distance analysis, confirming sparse routing selects experts with data clusters closest to the current denoising state, and (ii) per-expert analysis, showing selected experts produce more accurate predictions than non-selected ones, and (iii) expert disagreement analysis, showing quality degrades when experts disagree. For DDM deployment, our findings establish that routing should prioritize expert-data alignment over numerical stability metrics.</li>
</ul>

<h3>Title: Monotonicity as an Architectural Bias for Robust Language Models</h3>
<ul>
<li><strong>Authors: </strong>Patrick Cooper, Alireza Nadali, Ashutosh Trivedi, Alvaro Velasquez</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02686">https://arxiv.org/abs/2602.02686</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02686">https://arxiv.org/pdf/2602.02686</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02686]] Monotonicity as an Architectural Bias for Robust Language Models(https://arxiv.org/abs/2602.02686)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are known to exhibit brittle behavior under adversarial prompts and jailbreak attacks, even after extensive alignment and fine-tuning. This fragility reflects a broader challenge of modern neural language models: small, carefully structured perturbations in high-dimensional input spaces can induce large and unpredictable changes in internal semantic representations and output. We investigate monotonicity as an architectural inductive bias for improving the robustness of Transformer-based language models. Monotonicity constrains semantic transformations so that strengthening information, evidence, or constraints cannot lead to regressions in the corresponding internal representations. Such order-preserving behavior has long been exploited in control and safety-critical systems to simplify reasoning and improve robustness, but has traditionally been viewed as incompatible with the expressivity required by neural language models. We show that this trade-off is not inherent. By enforcing monotonicity selectively in the feed-forward sublayers of sequence-to-sequence Transformers -- while leaving attention mechanisms unconstrained -- we obtain monotone language models that preserve the performance of their pretrained counterparts. This architectural separation allows negation, contradiction, and contextual interactions to be introduced explicitly through attention, while ensuring that subsequent semantic refinement is order-preserving. Empirically, monotonicity substantially improves robustness: adversarial attack success rates drop from approximately 69% to 19%, while standard summarization performance degrades only marginally.</li>
</ul>

<h3>Title: Eidolon: A Practical Post-Quantum Signature Scheme Based on k-Colorability in the Age of Graph Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Asmaa Cherkaoui, Ramon Flores, Delaram Kahrobaei, Richard Wilson</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02689">https://arxiv.org/abs/2602.02689</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02689">https://arxiv.org/pdf/2602.02689</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02689]] Eidolon: A Practical Post-Quantum Signature Scheme Based on k-Colorability in the Age of Graph Neural Networks(https://arxiv.org/abs/2602.02689)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>We propose Eidolon, a practical post-quantum signature scheme based on the NP-complete k-colorability problem. Our construction generalizes the Goldreich-Micali-Wigderson zero-knowledge protocol to arbitrary k >= 3, applies the Fiat-Shamir transform, and uses Merkle-tree commitments to compress signatures from O(tn) to O(t log n). Crucially, we generate hard instances via planted "quiet" colorings that preserve the statistical profile of random graphs. We present the first empirical security analysis of such a scheme against both classical solvers (ILP, DSatur) and a custom graph neural network (GNN) attacker. Experiments show that for n >= 60, neither approach recovers the secret coloring, demonstrating that well-engineered k-coloring instances can resist modern cryptanalysis, including machine learning. This revives combinatorial hardness as a credible foundation for post-quantum signatures.</li>
</ul>

<h3>Title: Sparsely Supervised Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Wenshuai Zhao, Zhiyuan Li, Yi Zhao, Mohammad Hassan Vali, Martin Trapp, Joni Pajarinen, Juho Kannala, Arno Solin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02699">https://arxiv.org/abs/2602.02699</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02699">https://arxiv.org/pdf/2602.02699</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02699]] Sparsely Supervised Diffusion(https://arxiv.org/abs/2602.02699)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have shown remarkable success across a wide range of generative tasks. However, they often suffer from spatially inconsistent generation, arguably due to the inherent locality of their denoising mechanisms. This can yield samples that are locally plausible but globally inconsistent. To mitigate this issue, we propose sparsely supervised learning for diffusion models, a simple yet effective masking strategy that can be implemented with only a few lines of code. Interestingly, the experiments show that it is safe to mask up to 98\% of pixels during diffusion model training. Our method delivers competitive FID scores across experiments and, most importantly, avoids training instability on small datasets. Moreover, the masking strategy reduces memorization and promotes the use of essential contextual information during generation.</li>
</ul>

<h3>Title: Every Bit Counts: A Theoretical Study of Precision-Expressivity Tradeoffs in Quantized Transformers</h3>
<ul>
<li><strong>Authors: </strong>Sayak Chakrabarti, Toniann Pitassi, Josh Alman</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02707">https://arxiv.org/abs/2602.02707</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02707">https://arxiv.org/pdf/2602.02707</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02707]] Every Bit Counts: A Theoretical Study of Precision-Expressivity Tradeoffs in Quantized Transformers(https://arxiv.org/abs/2602.02707)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Quantization reduces the numerical precision of Transformer computations and is widely used to accelerate inference, yet its effect on expressivity remains poorly characterized. We demonstrate a fine-grained theoretical tradeoff between expressivity and precision: For every p we exhibit a function {\Gamma}, inspired by the equality function, and prove that a one-layer softmax Transformer can compute {\Gamma}, with p bits of precision, but not with p-1 bits of precision. This result concretely explains the widely observed phenomenon of empirical loss of expressivity when quantization is used. Practically, it suggests that tasks requiring equality-like comparisons (exact match, membership, etc.) are especially sensitive to quantization. Dropping even one bit can cross a threshold where the model cannot represent the needed comparison reliably. Thus, it paves the way for developing heuristics that will help practitioners choose how much quantization is possible: the precision should be chosen as a function of the length of equality to be checked for the specific task. Our proofs combine explicit finite-precision Transformer constructions with communication-complexity lower bounds, yielding a tight "one-bit" threshold.</li>
</ul>

<h3>Title: BinaryPPO: Efficient Policy Optimization for Binary Classification</h3>
<ul>
<li><strong>Authors: </strong>Punya Syon Pandey, Zhijing Jin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02708">https://arxiv.org/abs/2602.02708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02708">https://arxiv.org/pdf/2602.02708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02708]] BinaryPPO: Efficient Policy Optimization for Binary Classification(https://arxiv.org/abs/2602.02708)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Supervised fine-tuning (SFT) is the standard approach for binary classification tasks such as toxicity detection, factuality verification, and causal inference. However, SFT often performs poorly in real-world settings with label noise, class imbalance, or sparse supervision. We introduce BinaryPPO, an offline reinforcement learning large language model (LLM) framework that reformulates binary classification as a reward maximization problem. Our method leverages a variant of Proximal Policy Optimization (PPO) with a confidence-weighted reward function that penalizes uncertain or incorrect predictions, enabling the model to learn robust decision policies from static datasets without online interaction. Across eight domain-specific benchmarks and multiple models with differing architectures, BinaryPPO improves accuracy by 40-60 percentage points, reaching up to 99%, substantially outperforming supervised baselines. We provide an in-depth analysis of the role of reward shaping, advantage scaling, and policy stability in enabling this improvement. Overall, we demonstrate that confidence-based reward design provides a robust alternative to SFT for binary classification. Our code is available at this https URL.</li>
</ul>

<h3>Title: Towards Understanding Steering Strength</h3>
<ul>
<li><strong>Authors: </strong>Magamed Taimeskhanov, Samuel Vaiter, Damien Garreau</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02712">https://arxiv.org/abs/2602.02712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02712">https://arxiv.org/pdf/2602.02712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02712]] Towards Understanding Steering Strength(https://arxiv.org/abs/2602.02712)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>A popular approach to post-training control of large language models (LLMs) is the steering of intermediate latent representations. Namely, identify a well-chosen direction depending on the task at hand and perturbs representations along this direction at inference time. While many propositions exist to pick this direction, considerably less is understood about how to choose the magnitude of the move, whereas its importance is clear: too little and the intended behavior does not emerge, too much and the model's performance degrades beyond repair. In this work, we propose the first theoretical analysis of steering strength. We characterize its effect on next token probability, presence of a concept, and cross-entropy, deriving precise qualitative laws governing these quantities. Our analysis reveals surprising behaviors, including non-monotonic effects of steering strength. We validate our theoretical predictions empirically on eleven language models, ranging from a small GPT architecture to modern models.</li>
</ul>

<h3>Title: On the Feasibility of Hybrid Homomorphic Encryption for Intelligent Transportation Systems</h3>
<ul>
<li><strong>Authors: </strong>Kyle Yates, Abdullah Al Mamun, Mashrur Chowdhury</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02717">https://arxiv.org/abs/2602.02717</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02717">https://arxiv.org/pdf/2602.02717</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02717]] On the Feasibility of Hybrid Homomorphic Encryption for Intelligent Transportation Systems(https://arxiv.org/abs/2602.02717)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect</a></li>
<li><strong>Abstract: </strong>Many Intelligent Transportation Systems (ITS) applications require strong privacy guarantees for both users and their data. Homomorphic encryption (HE) enables computation directly on encrypted messages and thus offers a compelling approach to privacy-preserving data processing in ITS. However, practical HE schemes incur substantial ciphertext expansion and communication overhead, which limits their suitability for time-critical transportation systems. Hybrid homomorphic encryption (HHE) addresses this challenge by combining a homomorphic encryption scheme with a symmetric cipher, enabling efficient encrypted computation while dramatically reducing communication cost. In this paper, we develop theoretical models of representative ITS applications that integrate HHE to protect sensitive vehicular data. We then perform a parameter-based evaluation of the HHE scheme Rubato to estimate ciphertext sizes and communication overhead under realistic ITS workloads. Our results show that HHE achieves orders-of-magnitude reductions in ciphertext size compared with conventional HE while maintaining cryptographic security, making it significantly more practical for latency-constrained ITS communication.</li>
</ul>

<h3>Title: Composition for Pufferfish Privacy</h3>
<ul>
<li><strong>Authors: </strong>Jiamu Bai, Guanlin He, Xin Gu, Daniel Kifer, Kiwan Maeng</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02718">https://arxiv.org/abs/2602.02718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02718">https://arxiv.org/pdf/2602.02718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02718]] Composition for Pufferfish Privacy(https://arxiv.org/abs/2602.02718)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>When creating public data products out of confidential datasets, inferential/posterior-based privacy definitions, such as Pufferfish, provide compelling privacy semantics for data with correlations. However, such privacy definitions are rarely used in practice because they do not always compose. For example, it is possible to design algorithms for these privacy definitions that have no leakage when run once but reveal the entire dataset when run more than once. We prove necessary and sufficient conditions that must be added to ensure linear composition for Pufferfish mechanisms, hence avoiding such privacy collapse. These extra conditions turn out to be differential privacy-style inequalities, indicating that achieving both the interpretable semantics of Pufferfish for correlated data and composition benefits requires adopting differentially private mechanisms to Pufferfish. We show that such translation is possible through a concept called the $(a,b)$-influence curve, and many existing differentially private algorithms can be translated with our framework into a composable Pufferfish algorithm. We illustrate the benefit of our new framework by designing composable Pufferfish algorithms for Markov chains that significantly outperform prior work.</li>
</ul>

<h3>Title: End-to-end reconstruction of OCT optical properties and speckle-reduced structural intensity via physics-based learning</h3>
<ul>
<li><strong>Authors: </strong>Jinglun Yu, Yaning Wang, Wenhan Guo, Yuan Gao, Yu Sun, Jin U. Kang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02721">https://arxiv.org/abs/2602.02721</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02721">https://arxiv.org/pdf/2602.02721</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02721]] End-to-end reconstruction of OCT optical properties and speckle-reduced structural intensity via physics-based learning(https://arxiv.org/abs/2602.02721)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Inverse scattering in optical coherence tomography (OCT) seeks to recover both structural images and intrinsic tissue optical properties, including refractive index, scattering coefficient, and anisotropy. This inverse problem is challenging due to attenuation, speckle noise, and strong coupling among parameters. We propose a regularized end-to-end deep learning framework that jointly reconstructs optical parameter maps and speckle-reduced OCT structural intensity for layer visualization. Trained with Monte Carlo-simulated ground truth, our network incorporates a physics-based OCT forward model that generates predicted signals from the estimated parameters, providing physics-consistent supervision for parameter recovery and artifact suppression. Experiments on the synthetic corneal OCT dataset demonstrate robust optical map recovery under noise, improved resolution, and enhanced structural fidelity. This approach enables quantitative multi-parameter tissue characterization and highlights the benefit of combining physics-informed modeling with deep learning for computational OCT.</li>
</ul>

<h3>Title: Hierarchical Entity-centric Reinforcement Learning with Factored Subgoal Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Dan Haramati, Carl Qi, Tal Daniel, Amy Zhang, Aviv Tamar, George Konidaris</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02722">https://arxiv.org/abs/2602.02722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02722">https://arxiv.org/pdf/2602.02722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02722]] Hierarchical Entity-centric Reinforcement Learning with Factored Subgoal Diffusion(https://arxiv.org/abs/2602.02722)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We propose a hierarchical entity-centric framework for offline Goal-Conditioned Reinforcement Learning (GCRL) that combines subgoal decomposition with factored structure to solve long-horizon tasks in domains with multiple entities. Achieving long-horizon goals in complex environments remains a core challenge in Reinforcement Learning (RL). Domains with multiple entities are particularly difficult due to their combinatorial complexity. GCRL facilitates generalization across goals and the use of subgoal structure, but struggles with high-dimensional observations and combinatorial state-spaces, especially under sparse reward. We employ a two-level hierarchy composed of a value-based GCRL agent and a factored subgoal-generating conditional diffusion model. The RL agent and subgoal generator are trained independently and composed post hoc through selective subgoal generation based on the value function, making the approach modular and compatible with existing GCRL algorithms. We introduce new variations to benchmark tasks that highlight the challenges of multi-entity domains, and show that our method consistently boosts performance of the underlying RL agent on image-based long-horizon tasks with sparse rewards, achieving over 150% higher success rates on the hardest task in our suite and generalizing to increasing horizons and numbers of entities. Rollout videos are provided at: this https URL</li>
</ul>

<h3>Title: Search-Augmented Masked Diffusion Models for Constrained Generation</h3>
<ul>
<li><strong>Authors: </strong>Huu Binh Ta (1), Michael Cardei (1), Alvaro Velasquez (2), Ferdinando Fioretto (1) ((1) University of Virginia, (2) University of Colorado at Boulder)</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02727">https://arxiv.org/abs/2602.02727</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02727">https://arxiv.org/pdf/2602.02727</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02727]] Search-Augmented Masked Diffusion Models for Constrained Generation(https://arxiv.org/abs/2602.02727)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Discrete diffusion models generate sequences by iteratively denoising samples corrupted by categorical noise, offering an appealing alternative to autoregressive decoding for structured and symbolic generation. However, standard training targets a likelihood-based objective that primarily matches the data distribution and provides no native mechanism for enforcing hard constraints or optimizing non-differentiable properties at inference time. This work addresses this limitation and introduces Search-Augmented Masked Diffusion (SearchDiff), a training-free neurosymbolic inference framework that integrates informed search directly into the reverse denoising process. At each denoising step, the model predictions define a proposal set that is optimized under a user-specified property satisfaction, yielding a modified reverse transition that steers sampling toward probable and feasible solutions. Experiments in biological design and symbolic reasoning illustrate that SearchDiff substantially improves constraint satisfaction and property adherence, while consistently outperforming discrete diffusion and autoregressive baselines.</li>
</ul>

<h3>Title: CAPS: Unifying Attention, Recurrence, and Alignment in Transformer-based Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Viresh Pati, Yubin Kim, Vinh Pham, Jevon Twitty, Shihao Yang, Jiecheng Lu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02729">https://arxiv.org/abs/2602.02729</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02729">https://arxiv.org/pdf/2602.02729</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02729]] CAPS: Unifying Attention, Recurrence, and Alignment in Transformer-based Time Series Forecasting(https://arxiv.org/abs/2602.02729)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This paper presents $\textbf{CAPS}$ (Clock-weighted Aggregation with Prefix-products and Softmax), a structured attention mechanism for time series forecasting that decouples three distinct temporal structures: global trends, local shocks, and seasonal patterns. Standard softmax attention entangles these through global normalization, while recent recurrent models sacrifice long-term, order-independent selection for order-dependent causal structure. CAPS combines SO(2) rotations for phase alignment with three additive gating paths -- Riemann softmax, prefix-product gates, and a Clock baseline -- within a single attention layer. We introduce the Clock mechanism, a learned temporal weighting that modulates these paths through a shared notion of temporal importance. Experiments on long- and short-term forecasting benchmarks surpass vanilla softmax and linear attention mechanisms and demonstrate competitive performance against seven strong baselines with linear complexity. Our code implementation is available at this https URL.</li>
</ul>

<h3>Title: Predicting first-episode homelessness among US Veterans using longitudinal EHR data: time-varying models and social risk factors</h3>
<ul>
<li><strong>Authors: </strong>Rohan Pandey, Haijuan Yan, Hong Yu, Jack Tsai</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02731">https://arxiv.org/abs/2602.02731</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02731">https://arxiv.org/pdf/2602.02731</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02731]] Predicting first-episode homelessness among US Veterans using longitudinal EHR data: time-varying models and social risk factors(https://arxiv.org/abs/2602.02731)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Homelessness among US veterans remains a critical public health challenge, yet risk prediction offers a pathway for proactive intervention. In this retrospective prognostic study, we analyzed electronic health record (EHR) data from 4,276,403 Veterans Affairs patients during a 2016 observation period to predict first-episode homelessness occurring 3-12 months later in 2017 (prevalence: 0.32-1.19%). We constructed static and time-varying EHR representations, utilizing clinician-informed logic to model the persistence of clinical conditions and social risks over time. We then compared the performance of classical machine learning, transformer-based masked language models, and fine-tuned large language models (LLMs). We demonstrate that incorporating social and behavioral factors into longitudinal models improved precision-recall area under the curve (PR-AUC) by 15-30%. In the top 1% risk tier, models yielded positive predictive values ranging from 3.93-4.72% at 3 months, 7.39-8.30% at 6 months, 9.84-11.41% at 9 months, and 11.65-13.80% at 12 months across model architectures. Large language models underperformed encoder-based models on discrimination but showed smaller performance disparities across racial groups. These results demonstrate that longitudinal, socially informed EHR modeling concentrates homelessness risk into actionable strata, enabling targeted and data-informed prevention strategies for at-risk veterans.</li>
</ul>

<h3>Title: TabPFN for Zero-shot Parametric Engineering Design Generation</h3>
<ul>
<li><strong>Authors: </strong>Ke Wang, Yifan Tang, Nguyen Gia Hien Vu, Faez Ahmed, G. Gary Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02735">https://arxiv.org/abs/2602.02735</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02735">https://arxiv.org/pdf/2602.02735</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02735]] TabPFN for Zero-shot Parametric Engineering Design Generation(https://arxiv.org/abs/2602.02735)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Deep generative models for engineering design often require substantial computational cost, large training datasets, and extensive retraining when design requirements or datasets change, limiting their applicability in real-world engineering design workflow. In this work, we propose a zero-shot generation framework for parametric engineering design based on TabPFN, enabling conditional design generation using only a limited number of reference samples and without any task-specific model training or fine-tuning. The proposed method generates design parameters sequentially conditioned on target performance indicators, providing a flexible alternative to conventional generative models. The effectiveness of the proposed approach is evaluated on three engineering design datasets, i.e., ship hull design, BlendedNet aircraft, and UIUC airfoil. Experimental results demonstrate that the proposed method achieves competitive diversity across highly structured parametric design spaces, remains robust to variations in sampling, resolution and parameter dimensionality of geometry generation, and achieves a low performance error (e.g., less than 2% in generated ship hull designs' performance). Compared with diffusion-based generative models, the proposed framework significantly reduces computational overhead and data requirements while preserving reliable generation performance. These results highlight the potential of zero-shot, data-efficient generation as a practical and efficient tool for engineering design, enabling rapid deployment, flexible adaptation to new design settings, and ease of integration into real-world engineering workflows.</li>
</ul>

<h3>Title: TopoPrune: Robust Data Pruning via Unified Latent Space Topology</h3>
<ul>
<li><strong>Authors: </strong>Arjun Roy, Prajna G. Malettira, Manish Nagaraj, Kaushik Roy</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02739">https://arxiv.org/abs/2602.02739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02739">https://arxiv.org/pdf/2602.02739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02739]] TopoPrune: Robust Data Pruning via Unified Latent Space Topology(https://arxiv.org/abs/2602.02739)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Geometric data pruning methods, while practical for leveraging pretrained models, are fundamentally unstable. Their reliance on extrinsic geometry renders them highly sensitive to latent space perturbations, causing performance to degrade during cross-architecture transfer or in the presence of feature noise. We introduce TopoPrune, a framework which resolves this challenge by leveraging topology to capture the stable, intrinsic structure of data. TopoPrune operates at two scales, (1) utilizing a topology-aware manifold approximation to establish a global low-dimensional embedding of the dataset. Subsequently, (2) it employs differentiable persistent homology to perform a local topological optimization on the manifold embeddings, ranking samples by their structural complexity. We demonstrate that our unified dual-scale topological approach ensures high accuracy and precision, particularly at significant dataset pruning rates (e.g., 90%). Furthermore, through the inherent stability properties of topology, TopoPrune is (a) exceptionally robust to noise perturbations of latent feature embeddings and (b) demonstrates superior transferability across diverse network architectures. This study demonstrates a promising avenue towards stable and principled topology-based frameworks for robust data-efficient learning.</li>
</ul>

<h3>Title: Entropy-Guided Dynamic Tokens for Graph-LLM Alignment in Molecular Understanding</h3>
<ul>
<li><strong>Authors: </strong>Zihao Jing, Qiuhao Zeng, Ruiyi Fang, Yan Sun, Boyu Wang, Pingzhao Hu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02742">https://arxiv.org/abs/2602.02742</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02742">https://arxiv.org/pdf/2602.02742</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02742]] Entropy-Guided Dynamic Tokens for Graph-LLM Alignment in Molecular Understanding(https://arxiv.org/abs/2602.02742)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Molecular understanding is central to advancing areas such as scientific discovery, yet Large Language Models (LLMs) struggle to understand molecular graphs effectively. Existing graph-LLM bridges often adapt the Q-Former-style connector with fixed-length static tokens, which is originally designed for vision tasks. These designs overlook stereochemistry and substructural context and typically require costly LLM-backbone fine-tuning, limiting efficiency and generalization. We introduce EDT-Former, an Entropy-guided Dynamic Token Transformer that generates tokens aligned with informative molecular patches, thereby preserving both local and global structural features for molecular graph understanding. Beyond prior approaches, EDT-Former enables alignment between frozen graph encoders and LLMs without tuning the LLM backbone (excluding the embedding layer), resulting in computationally efficient finetuning, and achieves stateof-the-art results on MoleculeQA, Molecule-oriented Mol-Instructions, and property prediction benchmarks (TDC, MoleculeNet), underscoring its effectiveness for scalable and generalizable multimodal molecular understanding</li>
</ul>

<h3>Title: From Task Solving to Robust Real-World Adaptation in LLM Agents</h3>
<ul>
<li><strong>Authors: </strong>Pouya Pezeshkpour, Estevam Hruschka</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02760">https://arxiv.org/abs/2602.02760</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02760">https://arxiv.org/pdf/2602.02760</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02760]] From Task Solving to Robust Real-World Adaptation in LLM Agents(https://arxiv.org/abs/2602.02760)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models are increasingly deployed as specialized agents that plan, call tools, and take actions over extended horizons. Yet many existing evaluations assume a "clean interface" where dynamics are specified and stable, tools and sensors are reliable, and success is captured by a single explicit objective-often overestimating real-world readiness. In practice, agents face underspecified rules, unreliable signals, shifting environments, and implicit, multi-stakeholder goals. The challenge is therefore not just solving tasks, but adapting while solving: deciding what to trust, what is wanted, when to verify, and when to fall back or escalate. We stress-test deployment-relevant robustness under four operational circumstances: partial observability, dynamic environments, noisy signals, and dynamic agent state. We benchmark agentic LLMs in a grid-based game with a simple goal but long-horizon execution. Episodes violate clean-interface assumptions yet remain solvable, forcing agents to infer rules, pay for information, adapt to environmental and internal shifts, and act cautiously under noise. Across five state-of-the-art LLM agents, we find large gaps between nominal task-solving and deployment-like robustness. Performance generally degrades as grid size and horizon increase, but rankings are unstable: weaker models can beat stronger ones when strategy matches the uncertainty regime. Despite no explicit instruction, agents trade off completion, efficiency, and penalty avoidance, suggesting partial objective inference. Ablations and feature analyses reveal model-specific sensitivities and failure drivers, motivating work on verification, safe action selection, and objective inference under partial observability, noise, and non-stationarity.</li>
</ul>

<h3>Title: Exposing Vulnerabilities in Explanation for Time Series Classifiers via Dual-Target Attacks</h3>
<ul>
<li><strong>Authors: </strong>Bohan Wang, Zewen Liu, Lu Lin, Hui Liu, Li Xiong, Ming Jin, Wei Jin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02763">https://arxiv.org/abs/2602.02763</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02763">https://arxiv.org/pdf/2602.02763</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02763]] Exposing Vulnerabilities in Explanation for Time Series Classifiers via Dual-Target Attacks(https://arxiv.org/abs/2602.02763)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Interpretable time series deep learning systems are often assessed by checking temporal consistency on explanations, implicitly treating this as evidence of robustness. We show that this assumption can fail: Predictions and explanations can be adversarially decoupled, enabling targeted misclassification while the explanation remains plausible and consistent with a chosen reference rationale. We propose TSEF (Time Series Explanation Fooler), a dual-target attack that jointly manipulates the classifier and explainer outputs. In contrast to single-objective misclassification attacks that disrupt explanation and spread attribution mass broadly, TSEF achieves targeted prediction changes while keeping explanations consistent with the reference. Across multiple datasets and explainer backbones, our results consistently reveal that explanation stability is a misleading proxy for decision robustness and motivate coupling-aware robustness evaluations for trustworthy time series tasks.</li>
</ul>

<h3>Title: SVD-ViT: Does SVD Make Vision Transformers Attend More to the Foreground?</h3>
<ul>
<li><strong>Authors: </strong>Haruhiko Murata, Kazuhiro Hotta</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02765">https://arxiv.org/abs/2602.02765</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02765">https://arxiv.org/pdf/2602.02765</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02765]] SVD-ViT: Does SVD Make Vision Transformers Attend More to the Foreground?(https://arxiv.org/abs/2602.02765)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Vision Transformers (ViT) have been established as large-scale foundation models. However, because self-attention operates globally, they lack an explicit mechanism to distinguish foreground from background. As a result, ViT may learn unnecessary background features and artifacts, leading to degraded classification performance. To address this issue, we propose SVD-ViT, which leverages singular value decomposition (SVD) to prioritize the learning of foreground features. SVD-ViT consists of three components-\textbf{SPC module}, \textbf{SSVA}, and \textbf{ID-RSVD}-and suppresses task-irrelevant factors such as background noise and artifacts by extracting and aggregating singular vectors that capture object foreground information. Experimental results demonstrate that our method improves classification accuracy and effectively learns informative foreground representations while reducing the impact of background noise.</li>
</ul>

<h3>Title: Privately Fine-Tuned LLMs Preserve Temporal Dynamics in Tabular Data</h3>
<ul>
<li><strong>Authors: </strong>Lucas Rosenblatt, Peihan Liu, Ryan McKenna, Natalia Ponomareva</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02766">https://arxiv.org/abs/2602.02766</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02766">https://arxiv.org/pdf/2602.02766</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02766]] Privately Fine-Tuned LLMs Preserve Temporal Dynamics in Tabular Data(https://arxiv.org/abs/2602.02766)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Research on differentially private synthetic tabular data has largely focused on independent and identically distributed rows where each record corresponds to a unique individual. This perspective neglects the temporal complexity in longitudinal datasets, such as electronic health records, where a user contributes an entire (sub) table of sequential events. While practitioners might attempt to model such data by flattening user histories into high-dimensional vectors for use with standard marginal-based mechanisms, we demonstrate that this strategy is insufficient. Flattening fails to preserve temporal coherence even when it maintains valid marginal distributions. We introduce PATH, a novel generative framework that treats the full table as the unit of synthesis and leverages the autoregressive capabilities of privately fine-tuned large language models. Extensive evaluations show that PATH effectively captures long-range dependencies that traditional methods miss. Empirically, our method reduces the distributional distance to real trajectories by over 60% and reduces state transition errors by nearly 50% compared to leading marginal mechanisms while achieving similar marginal fidelity.</li>
</ul>

<h3>Title: AmharicStoryQA: A Multicultural Story Question Answering Benchmark in Amharic</h3>
<ul>
<li><strong>Authors: </strong>Israel Abebe Azime, Abenezer Kebede Angamo, Hana Mekonen Tamiru, Dagnachew Mekonnen Marilign, Philipp Slusallek, Seid Muhie Yimam, Dietrich Klakow</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02774">https://arxiv.org/abs/2602.02774</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02774">https://arxiv.org/pdf/2602.02774</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02774]] AmharicStoryQA: A Multicultural Story Question Answering Benchmark in Amharic(https://arxiv.org/abs/2602.02774)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the growing emphasis on multilingual and cultural evaluation benchmarks for large language models, language and culture are often treated as synonymous, and performance is commonly used as a proxy for a models understanding of a given language. In this work, we argue that such evaluations overlook meaningful cultural variation that exists within a single language. We address this gap by focusing on narratives from different regions of Ethiopia and demonstrate that, despite shared linguistic characteristics, region-specific and domain-specific content substantially influences language evaluation outcomes. To this end, we introduce \textbf{\textit{AmharicStoryQA}}, a long-sequence story question answering benchmark grounded in culturally diverse narratives from Amharic-speaking regions. Using this benchmark, we reveal a significant narrative understanding gap in existing LLMs, highlight pronounced regional differences in evaluation results, and show that supervised fine-tuning yields uneven improvements across regions and evaluation settings. Our findings emphasize the need for culturally grounded benchmarks that go beyond language-level evaluation to more accurately assess and improve narrative understanding in low-resource languages.</li>
</ul>

<h3>Title: VerIde ECG Biometrics: Verification and Identification</h3>
<ul>
<li><strong>Authors: </strong>Scagnetto Arjuna</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02776">https://arxiv.org/abs/2602.02776</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02776">https://arxiv.org/pdf/2602.02776</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02776]] VerIde ECG Biometrics: Verification and Identification(https://arxiv.org/abs/2602.02776)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, biometric</a></li>
<li><strong>Abstract: </strong>This work studies electrocardiogram (ECG) biometrics at large scale, evaluating how strongly an ECG can be linked to an individual and, consequently, how its anonymization may be compromised. We show that identity information is already present in tabular representations (fiducial features): even a simple MLP-based embedding network yields non-trivial performance, indicating that anonymization based solely on releasing features does not guarantee privacy. We then adopt embedding-based deep learning models (ArcFace), first on features and then on ECG waveforms, showing a performance jump when moving from tabular inputs to waveforms, and a further gain with larger training sets and consistent normalization across train/val/test. On a large-scale test set, verification achieves high TAR at strict FAR thresholds (TAR=0.908 @ FAR=1e-3; TAR=0.820 @ FAR=1e-4) with EER=2.53% (all-vs-all); closed-set identification yields Rank@1=0.812 and Rank@10=0.910. In open-set, a two-stage pipeline (top-K shortlist on embeddings + re-ranking) reaches DIR@FAR up to 0.976 at FAR=1e-3 and 1e-4. Overall, the results show that ECG carries a measurable individual signature: re-identification is already possible with tabular features and is further amplified by embedding-based models, making privacy implications and realistic operational protocols essential to consider.</li>
</ul>

<h3>Title: Evaluating False Alarm and Missing Attacks in CAN IDS</h3>
<ul>
<li><strong>Authors: </strong>Nirab Hossain, Pablo Moriano</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02781">https://arxiv.org/abs/2602.02781</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02781">https://arxiv.org/pdf/2602.02781</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02781]] Evaluating False Alarm and Missing Attacks in CAN IDS(https://arxiv.org/abs/2602.02781)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack, robust</a></li>
<li><strong>Abstract: </strong>Modern vehicles rely on electronic control units (ECUs) interconnected through the Controller Area Network (CAN), making in-vehicle communication a critical security concern. Machine learning (ML)-based intrusion detection systems (IDS) are increasingly deployed to protect CAN traffic, yet their robustness against adversarial manipulation remains largely unexplored. We present a systematic adversarial evaluation of CAN IDS using the ROAD dataset, comparing four shallow learning models with a deep neural network-based detector. Using protocol-compliant, payload-level perturbations generated via FGSM, BIM and PGD, we evaluate adversarial effects on both benign and malicious CAN frames. While all models achieve strong baseline performance under benign conditions, adversarial perturbations reveal substantial vulnerabilities. Although shallow and deep models are robust to false-alarm induction, with the deep neural network (DNN) performing best on benign traffic, all architectures suffer significant increases in missed attacks. Notably, under gradient-based attacks, the shallow model extra trees (ET) demonstrates improved robustness to missed-attack induction compared to the other models. Our results demonstrate that adversarial manipulation can simultaneously trigger false alarms and evade detection, underscoring the need for adversarial robustness evaluation in safety-critical automotive IDS.</li>
</ul>

<h3>Title: Cross-Temporal Attention Fusion (CTAF) for Multimodal Physiological Signals in Self-Supervised Learning</h3>
<ul>
<li><strong>Authors: </strong>Arian Khorasani, Théophile Demazure</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02784">https://arxiv.org/abs/2602.02784</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02784">https://arxiv.org/pdf/2602.02784</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02784]] Cross-Temporal Attention Fusion (CTAF) for Multimodal Physiological Signals in Self-Supervised Learning(https://arxiv.org/abs/2602.02784)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We study multimodal affect modeling when EEG and peripheral physiology are asynchronous, which most fusion methods ignore or handle with costly warping. We propose Cross-Temporal Attention Fusion (CTAF), a self-supervised module that learns soft bidirectional alignments between modalities and builds a robust clip embedding using time-aware cross attention, a lightweight fusion gate, and alignment-regularized contrastive objectives with optional weak supervision. On the K-EmoCon dataset, under leave-one-out cross-validation evaluation, CTAF yields higher cosine margins for matched pairs and better cross-modal token retrieval within one second, and it is competitive with the baseline on three-bin accuracy and macro-F1 while using few labels. Our contributions are a time-aware fusion mechanism that directly models correspondence, an alignment-driven self-supervised objective tailored to EEG and physiology, and an evaluation protocol that measures alignment quality itself. Our approach accounts for the coupling between the central and autonomic nervous systems in psychophysiological time series. These results indicate that CTAF is a strong step toward label-efficient, generalizable EEG-peripheral fusion under temporal asynchrony.</li>
</ul>

<h3>Title: LEMON: Local Explanations via Modality-aware OptimizatioN</h3>
<ul>
<li><strong>Authors: </strong>Yu Qin, Phillip Sloan, Raul Santos-Rodriguez, Majid Mirmehdi, Telmo de Menezes e Silva Filho</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02786">https://arxiv.org/abs/2602.02786</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02786">https://arxiv.org/pdf/2602.02786</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02786]] LEMON: Local Explanations via Modality-aware OptimizatioN(https://arxiv.org/abs/2602.02786)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Multimodal models are ubiquitous, yet existing explainability methods are often single-modal, architecture-dependent, or too computationally expensive to run at scale. We introduce LEMON (Local Explanations via Modality-aware OptimizatioN), a model-agnostic framework for local explanations of multimodal predictions. LEMON fits a single modality-aware surrogate with group-structured sparsity to produce unified explanations that disentangle modality-level contributions and feature-level attributions. The approach treats the predictor as a black box and is computationally efficient, requiring relatively few forward passes while remaining faithful under repeated perturbations. We evaluate LEMON on vision-language question answering and a clinical prediction task with image, text, and tabular inputs, comparing against representative multimodal baselines. Across backbones, LEMON achieves competitive deletion-based faithfulness while reducing black-box evaluations by 35-67 times and runtime by 2-8 times compared to strong multimodal baselines.</li>
</ul>

<h3>Title: Structure-Preserving Learning Improves Geometry Generalization in Neural PDEs</h3>
<ul>
<li><strong>Authors: </strong>Benjamin D. Shaffer, Shawn Koohy, Brooks Kinch, M. Ani Hsieh, Nathaniel Trask</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02788">https://arxiv.org/abs/2602.02788</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02788">https://arxiv.org/pdf/2602.02788</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02788]] Structure-Preserving Learning Improves Geometry Generalization in Neural PDEs(https://arxiv.org/abs/2602.02788)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We aim to develop physics foundation models for science and engineering that provide real-time solutions to Partial Differential Equations (PDEs) which preserve structure and accuracy under adaptation to unseen geometries. To this end, we introduce General-Geometry Neural Whitney Forms (Geo-NeW): a data-driven finite element method. We jointly learn a differential operator and compatible reduced finite element spaces defined on the underlying geometry. The resulting model is solved to generate predictions, while exactly preserving physical conservation laws through Finite Element Exterior Calculus. Geometry enters the model as a discretized mesh both through a transformer-based encoding and as the basis for the learned finite element spaces. This explicitly connects the underlying geometry and imposed boundary conditions to the solution, providing a powerful inductive bias for learning neural PDEs, which we demonstrate improves generalization to unseen domains. We provide a novel parameterization of the constitutive model ensuring the existence and uniqueness of the solution. Our approach demonstrates state-of-the-art performance on several steady-state PDE benchmarks, and provides a significant improvement over conventional baselines on out-of-distribution geometries.</li>
</ul>

<h3>Title: LmPT: Conditional Point Transformer for Anatomical Landmark Detection on 3D Point Clouds</h3>
<ul>
<li><strong>Authors: </strong>Matteo Bastico, Pierre Onghena, David Ryckelynck, Beatriz Marcotegui, Santiago Velasco-Forero, Laurent Corté, Caroline Robine--Decourcelle, Etienne Decencière</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02808">https://arxiv.org/abs/2602.02808</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02808">https://arxiv.org/pdf/2602.02808</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02808]] LmPT: Conditional Point Transformer for Anatomical Landmark Detection on 3D Point Clouds(https://arxiv.org/abs/2602.02808)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Accurate identification of anatomical landmarks is crucial for various medical applications. Traditional manual landmarking is time-consuming and prone to inter-observer variability, while rule-based methods are often tailored to specific geometries or limited sets of landmarks. In recent years, anatomical surfaces have been effectively represented as point clouds, which are lightweight structures composed of spatial coordinates. Following this strategy and to overcome the limitations of existing landmarking techniques, we propose Landmark Point Transformer (LmPT), a method for automatic anatomical landmark detection on point clouds that can leverage homologous bones from different species for translational research. The LmPT model incorporates a conditioning mechanism that enables adaptability to different input types to conduct cross-species learning. We focus the evaluation of our approach on femoral landmarking using both human and newly annotated dog femurs, demonstrating its generalization and effectiveness across species. The code and dog femur dataset will be publicly available at: this https URL.</li>
</ul>

<h3>Title: Membership Inference Attacks from Causal Principles</h3>
<ul>
<li><strong>Authors: </strong>Mathieu Even, Clément Berenfeld, Linus Bleistein, Tudor Cebere, Julie Josse, Aurélien Bellet</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02819">https://arxiv.org/abs/2602.02819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02819">https://arxiv.org/pdf/2602.02819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02819]] Membership Inference Attacks from Causal Principles(https://arxiv.org/abs/2602.02819)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, membership infer</a></li>
<li><strong>Abstract: </strong>Membership Inference Attacks (MIAs) are widely used to quantify training data memorization and assess privacy risks. Standard evaluation requires repeated retraining, which is computationally costly for large models. One-run methods (single training with randomized data inclusion) and zero-run methods (post hoc evaluation) are often used instead, though their statistical validity remains unclear. To address this gap, we frame MIA evaluation as a causal inference problem, defining memorization as the causal effect of including a data point in the training set. This novel formulation reveals and formalizes key sources of bias in existing protocols: one-run methods suffer from interference between jointly included points, while zero-run evaluations popular for LLMs are confounded by non-random membership assignment. We derive causal analogues of standard MIA metrics and propose practical estimators for multi-run, one-run, and zero-run regimes with non-asymptotic consistency guarantees. Experiments on real-world data show that our approach enables reliable memorization measurement even when retraining is impractical and under distribution shift, providing a principled foundation for privacy evaluation in modern AI systems.</li>
</ul>

<h3>Title: From Tokens to Numbers: Continuous Number Modeling for SVG Generation</h3>
<ul>
<li><strong>Authors: </strong>Michael Ogezi, Martin Bell, Freda Shi, Ethan Smith</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02820">https://arxiv.org/abs/2602.02820</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02820">https://arxiv.org/pdf/2602.02820</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02820]] From Tokens to Numbers: Continuous Number Modeling for SVG Generation(https://arxiv.org/abs/2602.02820)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>For certain image generation tasks, vector graphics such as Scalable Vector Graphics (SVGs) offer clear benefits such as increased flexibility, size efficiency, and editing ease, but remain less explored than raster-based approaches. A core challenge is that the numerical, geometric parameters, which make up a large proportion of SVGs, are inefficiently encoded as long sequences of tokens. This slows training, reduces accuracy, and hurts generalization. To address these problems, we propose Continuous Number Modeling (CNM), an approach that directly models numbers as first-class, continuous values rather than discrete tokens. This formulation restores the mathematical elegance of the representation by aligning the model's inputs with the data's continuous nature, removing discretization artifacts introduced by token-based encoding. We then train a multimodal transformer on 2 million raster-to-SVG samples, followed by fine-tuning via reinforcement learning using perceptual feedback to further improve visual quality. Our approach improves training speed by over 30% while maintaining higher perceptual fidelity compared to alternative approaches. This work establishes CNM as a practical and efficient approach for high-quality vector generation, with potential for broader applications. We make our code available this http URL.</li>
</ul>

<h3>Title: CATNIP: LLM Unlearning via Calibrated and Tokenized Negative Preference Alignment</h3>
<ul>
<li><strong>Authors: </strong>Zhengbang Yang, Yisheng Zhong, Junyuan Hong, Zhuangdi Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02824">https://arxiv.org/abs/2602.02824</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02824">https://arxiv.org/pdf/2602.02824</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02824]] CATNIP: LLM Unlearning via Calibrated and Tokenized Negative Preference Alignment(https://arxiv.org/abs/2602.02824)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust</a></li>
<li><strong>Abstract: </strong>Pretrained knowledge memorized in LLMs raises critical concerns over safety and privacy, which has motivated LLM Unlearning as a technique for selectively removing the influences of undesirable knowledge. Existing approaches, rooted in Gradient Ascent (GA), often degrade general domain knowledge while relying on retention data or curated contrastive pairs, which can be either impractical or data and computationally prohibitive. Negative Preference Alignment has been explored for unlearning to tackle the limitations of GA, which, however, remains confined by its choice of reference model and shows undermined performance in realistic data settings. These limitations raise two key questions: i) Can we achieve effective unlearning that quantifies model confidence in undesirable knowledge and uses it to calibrate gradient updates more precisely, thus reducing catastrophic forgetting? ii) Can we make unlearning robust to data scarcity and length variation? We answer both questions affirmatively with CATNIP (Calibrated and Tokenized Negative Preference Alignment), a principled method that rescales unlearning effects in proportion to the model's token-level confidence, thus ensuring fine-grained control over forgetting. Extensive evaluations on MUSE and WMDP benchmarks demonstrated that our work enables effective unlearning without requiring retention data or contrastive unlearning response pairs, with stronger knowledge forgetting and preservation tradeoffs than state-of-the-art methods.</li>
</ul>

<h3>Title: A Single Revision Step Improves Token-Efficient LLM Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yingchuan Zhang, Terry Ma, Wenxuan Zhong, Ping Ma</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02828">https://arxiv.org/abs/2602.02828</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02828">https://arxiv.org/pdf/2602.02828</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02828]] A Single Revision Step Improves Token-Efficient LLM Reasoning(https://arxiv.org/abs/2602.02828)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) achieve higher accuracy on challenging reasoning tasks by scaling test-time compute through multiple trajectory sampling. However, standard aggregation methods like majority voting or individual confidence-based filtering face a fundamental "blind spot": they evaluate each trace in isolation. As problems scale in difficulty, models often generate hallucinated paths that exhibit misleadingly high confidence, causing the true solution to be suppressed by a narrow margin in traditional voting. We ask: can we enable traces to "peer-review" each other to resolve these near-miss errors? We introduce Packet-Conditioned Revision (PACER), a training-free, inference-only framework that enables reasoning traces to revise their conclusions through a structured coordination step. After a preliminary screening of generated traces, PACER constructs a compact consensus packet containing (i) unique candidate answers, (ii) their aggregated confidence scores, and (iii) representative reasoning summaries for each candidate answer. Individual traces then perform a targeted self-review conditioned on this packet, allowing them to identify specific logical junctions where they diverged from the broader consensus and pivot if their original reasoning is found to be flawed. Final predictions are obtained via confidence-weighted voting over these revised trajectories. On challenging competitive math benchmarks such as AIME and BRUMO, PACER matches or exceeds the accuracy of 256-sample majority voting, significantly outperforming raw ensemble baselines by transforming simple consensus into a collaborative logical refinement process.</li>
</ul>

<h3>Title: Koopman Autoencoders with Continuous-Time Latent Dynamics for Fluid Dynamics Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Rares Grozavescu, Pengyu Zhang, Etienne Meunier, Mark Girolami</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.flu-dyn</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02832">https://arxiv.org/abs/2602.02832</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02832">https://arxiv.org/pdf/2602.02832</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02832]] Koopman Autoencoders with Continuous-Time Latent Dynamics for Fluid Dynamics Forecasting(https://arxiv.org/abs/2602.02832)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Data-driven surrogate models have emerged as powerful tools for accelerating the simulation of turbulent flows. However, classical approaches which perform autoregressive rollouts often trade off between strong short-term accuracy and long-horizon stability. Koopman autoencoders, inspired by Koopman operator theory, provide a physics-based alternative by mapping nonlinear dynamics into a latent space where linear evolution is conducted. In practice, most existing formulations operate in a discrete-time setting, limiting temporal flexibility. In this work, we introduce a continuous-time Koopman framework that models latent evolution through numerical integration schemes. By allowing variable timesteps at inference, the method demonstrates robustness to temporal resolution and generalizes beyond training regimes. In addition, the learned dynamics closely adhere to the analytical matrix exponential solution, enabling efficient long-horizon forecasting. We evaluate the approach on classical CFD benchmarks and report accuracy, stability, and extrapolation properties.</li>
</ul>

<h3>Title: Tabula RASA: Exposing and Breaking the Relational Bottleneck in Transformers</h3>
<ul>
<li><strong>Authors: </strong>Jonas Petersen, Camilla Mazzoleni, Riccardo Maggioni</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02834">https://arxiv.org/abs/2602.02834</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02834">https://arxiv.org/pdf/2602.02834</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02834]] Tabula RASA: Exposing and Breaking the Relational Bottleneck in Transformers(https://arxiv.org/abs/2602.02834)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformers achieve remarkable performance across many domains, yet struggle with tasks requiring multi-hop relational reasoning over structured data. We analyze this limitation through circuit complexity: standard transformers are $\mathsf{TC}^0$-complete and require $\Omega(k)$ layers for $k$-hop reasoning. We introduce RASA (Relation-Aware Sparse Attention), a minimal modification adding: (1) edge-type embeddings that inject relational structure into attention scores, and (2) sparse masking that restricts attention to graph-adjacent positions. While RASA has the same asymptotic depth requirements, sparse masking reduces the attention search space from $O(2^{n^2})$ to $O(2^m)$ patterns, and edge biases provide explicit relation routing. Empirically, on MetaQA (1/2/3-hop) and WebQuestionsSP, RASA outperforms standard transformers and matches GPT-4 at lower cost, with advantages growing with reasoning depth (+7.1 points on 3-hop). We do not claim formal learnability guarantees; the contribution is empirical validation that minimal structural modifications substantially improve multi-hop reasoning.</li>
</ul>

<h3>Title: Semantics-Aware Generative Latent Data Augmentation for Learning in Low-Resource Domains</h3>
<ul>
<li><strong>Authors: </strong>Jae-Sung Bae, Minje Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02841">https://arxiv.org/abs/2602.02841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02841">https://arxiv.org/pdf/2602.02841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02841]] Semantics-Aware Generative Latent Data Augmentation for Learning in Low-Resource Domains(https://arxiv.org/abs/2602.02841)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Despite strong performance in data-rich regimes, deep learning often underperforms in the data-scarce settings common in practice. While foundation models (FMs) trained on massive datasets demonstrate strong generalization by extracting general-purpose features, they can still suffer from scarce labeled data during downstream fine-tuning. To address this, we propose GeLDA, a semantics-aware generative latent data augmentation framework that leverages conditional diffusion models to synthesize samples in an FM-induced latent space. Because this space is low-dimensional and concentrates task-relevant information compared to the input space, GeLDA enables efficient, high-quality data generation. GeLDA conditions generation on auxiliary feature vectors that capture semantic relationships among classes or subdomains, facilitating data augmentation in low-resource domains. We validate GeLDA in two large-scale recognition tasks: (a) in zero-shot language-specific speech emotion recognition, GeLDA improves the Whisper-large baseline's unweighted average recall by 6.13%; and (b) in long-tailed image classification, it achieves 74.7% tail-class accuracy on ImageNet-LT, setting a new state-of-the-art result.</li>
</ul>

<h3>Title: Causal Flow Q-Learning for Robust Offline Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Mingxuan Li, Junzhe Zhang, Elias Bareinboim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02847">https://arxiv.org/abs/2602.02847</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02847">https://arxiv.org/pdf/2602.02847</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02847]] Causal Flow Q-Learning for Robust Offline Reinforcement Learning(https://arxiv.org/abs/2602.02847)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Expressive policies based on flow-matching have been successfully applied in reinforcement learning (RL) more recently due to their ability to model complex action distributions from offline data. These algorithms build on standard policy gradients, which assume that there is no unmeasured confounding in the data. However, this condition does not necessarily hold for pixel-based demonstrations when a mismatch exists between the demonstrator's and the learner's sensory capabilities, leading to implicit confounding biases in offline data. We address the challenge by investigating the problem of confounded observations in offline RL from a causal perspective. We develop a novel causal offline RL objective that optimizes policies' worst-case performance that may arise due to confounding biases. Based on this new objective, we introduce a practical implementation that learns expressive flow-matching policies from confounded demonstrations, employing a deep discriminator to assess the discrepancy between the target policy and the nominal behavioral policy. Experiments across 25 pixel-based tasks demonstrate that our proposed confounding-robust augmentation procedure achieves a success rate 120\% that of confounding-unaware, state-of-the-art offline RL methods.</li>
</ul>

<h3>Title: Zero Sum SVD: Balancing Loss Sensitivity for Low Rank LLM Compression</h3>
<ul>
<li><strong>Authors: </strong>Ali Abbasi, Chayne Thrash, Haoran Qin, Shansita Sharma, Sepehr Seifi, Soheil Kolouri</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02848">https://arxiv.org/abs/2602.02848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02848">https://arxiv.org/pdf/2602.02848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02848]] Zero Sum SVD: Balancing Loss Sensitivity for Low Rank LLM Compression(https://arxiv.org/abs/2602.02848)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Advances in large language models have driven strong performance across many tasks, but their memory and compute costs still hinder deployment. SVD-based compression reduces storage and can speed up inference via low-rank factors, yet performance depends on how rank is allocated under a global compression ratio. Prior methods often use homogeneous ranks for similarly sized matrices, despite large differences in loss sensitivity, or rely on expensive iterative pre-truncation optimization to determine per matrix ranks. We propose \textbf{Zero Sum SVD} (\textbf{ZS-SVD}), a post-training method that performs \emph{global} singular component selection using activation whitening and first-order calibration loss estimates in whitened coordinates. \textbf{ZS-SVD} prunes components across the whole model with a \textbf{zero sum} rule that keeps the cumulative predicted loss change near zero, automatically yielding heterogeneous ranks without solving a rank allocation optimization. Motivated by evidence that gradients near pretrained solutions exhibit low rank structure, we also introduce an optional lightweight correction that applies a \textbf{single} projected gradient update after truncation, followed by re-truncation. Extensive experiments across multiple LLM architectures show consistent gains across diverse benchmarks and compression ratios. Code is available at this https URL</li>
</ul>

<h3>Title: Self-Supervised Uncalibrated Multi-View Video Anonymization in the Operating Room</h3>
<ul>
<li><strong>Authors: </strong>Keqi Chen, Vinkle Srivastav, Armine Vardazaryan, Cindy Rolland, Didier Mutter, Nicolas Padoy</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02850">https://arxiv.org/abs/2602.02850</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02850">https://arxiv.org/pdf/2602.02850</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02850]] Self-Supervised Uncalibrated Multi-View Video Anonymization in the Operating Room(https://arxiv.org/abs/2602.02850)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Privacy preservation is a prerequisite for using video data in Operating Room (OR) research. Effective anonymization relies on the exhaustive localization of every individual; even a single missed detection necessitates extensive manual correction. However, existing approaches face two critical scalability bottlenecks: (1) they usually require manual annotations of each new clinical site for high accuracy; (2) while multi-camera setups have been widely adopted to address single-view ambiguity, camera calibration is typically required whenever cameras are repositioned. To address these problems, we propose a novel self-supervised multi-view video anonymization framework consisting of whole-body person detection and whole-body pose estimation, without annotation or camera calibration. Our core strategy is to enhance the single-view detector by "retrieving" false negatives using temporal and multi-view context, and conducting self-supervised domain adaptation. We first run an off-the-shelf whole-body person detector in each view with a low-score threshold to gather candidate detections. Then, we retrieve the low-score false negatives that exhibit consistency with the high-score detections via tracking and self-supervised uncalibrated multi-view association. These recovered detections serve as pseudo labels to iteratively fine-tune the whole-body detector. Finally, we apply whole-body pose estimation on each detected person, and fine-tune the pose model using its own high-score predictions. Experiments on the 4D-OR dataset of simulated surgeries and our dataset of real surgeries show the effectiveness of our approach achieving over 97% recall. Moreover, we train a real-time whole-body detector using our pseudo labels, achieving comparable performance and highlighting our method's practical applicability. Code is available at this https URL.</li>
</ul>

<h3>Title: Late-Stage Generalization Collapse in Grokking: Detecting anti-grokking with Weightwatcher</h3>
<ul>
<li><strong>Authors: </strong>Hari K Prakash, Charles H Martin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02859">https://arxiv.org/abs/2602.02859</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02859">https://arxiv.org/pdf/2602.02859</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02859]] Late-Stage Generalization Collapse in Grokking: Detecting anti-grokking with Weightwatcher(https://arxiv.org/abs/2602.02859)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>\emph{Memorization} in neural networks lacks a precise operational definition and is often inferred from the grokking regime, where training accuracy saturates while test accuracy remains very low. We identify a previously unreported third phase of grokking in this training regime: \emph{anti-grokking}, a late-stage collapse of generalization. We revisit two canonical grokking setups: a 3-layer MLP trained on a subset of MNIST and a transformer trained on modular addition, but extended training far beyond standard. In both cases, after models transition from pre-grokking to successful generalization, test accuracy collapses back to chance while training accuracy remains perfect, indicating a distinct post-generalization failure mode. To diagnose anti-grokking, we use the open-source \texttt{WeightWatcher} tool based on HTSR/SETOL theory. The primary signal is the emergence of \emph{Correlation Traps}: anomalously large eigenvalues beyond the Marchenko--Pastur bulk in the empirical spectral density of shuffled weight matrices, which are predicted to impair generalization. As a secondary signal, anti-grokking corresponds to the average HTSR layer quality metric $\alpha$ deviating from $2.0$. Neither metric requires access to the test or training data. We compare these signals to alternative grokking diagnostic, including $\ell_2$ norms, Activation Sparsity, Absolute Weight Entropy, and Local Circuit Complexity. These track pre-grokking and grokking but fail to identify anti-grokking. Finally, we show that Correlation Traps can induce catastrophic forgetting and/or prototype memorization, and observe similar pathologies in large-scale LLMs, like OSS GPT 20/120B.</li>
</ul>

<h3>Title: ViThinker: Active Vision-Language Reasoning via Dynamic Perceptual Querying</h3>
<ul>
<li><strong>Authors: </strong>Weihang You, Qingchan Zhu, David Liu, Yi Pan, Geng Yuan, Hanqi Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02873">https://arxiv.org/abs/2602.02873</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02873">https://arxiv.org/pdf/2602.02873</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02873]] ViThinker: Active Vision-Language Reasoning via Dynamic Perceptual Querying(https://arxiv.org/abs/2602.02873)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Chain-of-Thought (CoT) reasoning excels in language models but struggles in vision-language models due to premature visual-to-text conversion that discards continuous information such as geometry and spatial layout. While recent methods enhance CoT through static enumeration or attention-based selection, they remain passive, i.e., processing pre-computed inputs rather than actively seeking task-relevant details. Inspired by human active perception, we introduce ViThinker, a framework that enables vision-language models to autonomously generate decision (query) tokens triggering the synthesis of expert-aligned visual features on demand. ViThinker internalizes vision-expert capabilities during training, performing generative mental simulation during inference without external tool calls. Through a two-stage curriculum: first distilling frozen experts into model parameters, then learning task-driven querying via sparsity penalties, i.e., ViThinker discovers minimal sufficient perception for each reasoning step. Evaluations across vision-centric benchmarks demonstrate consistent improvements, validating that active query generation outperforms passive approaches in both perceptual grounding and reasoning accuracy.</li>
</ul>

<h3>Title: A Geometry-Aware Efficient Algorithm for Compositional Entropic Risk Minimization</h3>
<ul>
<li><strong>Authors: </strong>Xiyuan Wei, Linli Zhou, Bokun Wang, Chih-Jen Lin, Tianbao Yang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02877">https://arxiv.org/abs/2602.02877</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02877">https://arxiv.org/pdf/2602.02877</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02877]] A Geometry-Aware Efficient Algorithm for Compositional Entropic Risk Minimization(https://arxiv.org/abs/2602.02877)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper studies optimization for a family of problems termed $\textbf{compositional entropic risk minimization}$, in which each data's loss is formulated as a Log-Expectation-Exponential (Log-E-Exp) function. The Log-E-Exp formulation serves as an abstraction of the Log-Sum-Exponential (LogSumExp) function when the explicit summation inside the logarithm is taken over a gigantic number of items and is therefore expensive to evaluate. While entropic risk objectives of this form arise in many machine learning problems, existing optimization algorithms suffer from several fundamental limitations including non-convergence, numerical instability, and slow convergence rates. To address these limitations, we propose a geometry-aware stochastic algorithm, termed $\textbf{SCENT}$, for the dual formulation of entropic risk minimization cast as a min--min optimization problem. The key to our design is a $\textbf{stochastic proximal mirror descent (SPMD)}$ update for the dual variable, equipped with a Bregman divergence induced by a negative exponential function that faithfully captures the geometry of the objective. Our main contributions are threefold: (i) we establish an $O(1/\sqrt{T})$ convergence rate of the proposed SCENT algorithm for convex problems; (ii) we theoretically characterize the advantages of SPMD over standard SGD update for optimizing the dual variable; and (iii) we demonstrate the empirical effectiveness of SCENT on extreme classification, partial AUC maximization, contrastive learning and distributionally robust optimization, where it consistently outperforms existing baselines.</li>
</ul>

<h3>Title: Mixture of Concept Bottleneck Experts</h3>
<ul>
<li><strong>Authors: </strong>Francesco De Santis, Gabriele Ciravegna, Giovanni De Felice, Arianna Casanova, Francesco Giannini, Michelangelo Diligenti, Mateo Espinosa Zarlenga, Pietro Barbiero, Johannes Schneider, Danilo Giordano</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02886">https://arxiv.org/abs/2602.02886</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02886">https://arxiv.org/pdf/2602.02886</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02886]] Mixture of Concept Bottleneck Experts(https://arxiv.org/abs/2602.02886)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Concept Bottleneck Models (CBMs) promote interpretability by grounding predictions in human-understandable concepts. However, existing CBMs typically fix their task predictor to a single linear or Boolean expression, limiting both predictive accuracy and adaptability to diverse user needs. We propose Mixture of Concept Bottleneck Experts (M-CBEs), a framework that generalizes existing CBMs along two dimensions: the number of experts and the functional form of each expert, exposing an underexplored region of the design space. We investigate this region by instantiating two novel models: Linear M-CBE, which learns a finite set of linear expressions, and Symbolic M-CBE, which leverages symbolic regression to discover expert functions from data under user-specified operator vocabularies. Empirical evaluation demonstrates that varying the mixture size and functional form provides a robust framework for navigating the accuracy-interpretability trade-off, adapting to different user and task needs.</li>
</ul>

<h3>Title: HALT: Hallucination Assessment via Log-probs as Time series</h3>
<ul>
<li><strong>Authors: </strong>Ahmad Shapiro, Karan Taneja, Ashok Goel</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02888">https://arxiv.org/abs/2602.02888</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02888">https://arxiv.org/pdf/2602.02888</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02888]] HALT: Hallucination Assessment via Log-probs as Time series(https://arxiv.org/abs/2602.02888)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Hallucinations remain a major obstacle for large language models (LLMs), especially in safety-critical domains. We present HALT (Hallucination Assessment via Log-probs as Time series), a lightweight hallucination detector that leverages only the top-20 token log-probabilities from LLM generations as a time series. HALT uses a gated recurrent unit model combined with entropy-based features to learn model calibration bias, providing an extremely efficient alternative to large encoders. Unlike white-box approaches, HALT does not require access to hidden states or attention maps, relying only on output log-probabilities. Unlike black-box approaches, it operates on log-probs rather than surface-form text, which enables stronger domain generalization and compatibility with proprietary LLMs without requiring access to internal weights. To benchmark performance, we introduce HUB (Hallucination detection Unified Benchmark), which consolidates prior datasets into ten capabilities covering both reasoning tasks (Algorithmic, Commonsense, Mathematical, Symbolic, Code Generation) and general purpose skills (Chat, Data-to-Text, Question Answering, Summarization, World Knowledge). While being 30x smaller, HALT outperforms Lettuce, a fine-tuned modernBERT-base encoder, achieving a 60x speedup gain on HUB. HALT and HUB together establish an effective framework for hallucination detection across diverse LLM capabilities.</li>
</ul>

<h3>Title: Self-Soupervision: Cooking Model Soups without Labels</h3>
<ul>
<li><strong>Authors: </strong>Anthony Fuller, James R. Green, Evan Shelhamer</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02890">https://arxiv.org/abs/2602.02890</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02890">https://arxiv.org/pdf/2602.02890</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02890]] Self-Soupervision: Cooking Model Soups without Labels(https://arxiv.org/abs/2602.02890)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Model soups are strange and strangely effective combinations of parameters. They take a model (the stock), fine-tune it into multiple models (the ingredients), and then mix their parameters back into one model (the soup) to improve predictions. While all known soups require supervised learning, and optimize the same loss on labeled data, our recipes for Self-\emph{Soup}ervision generalize soups to self-supervised learning (SSL). Our Self-Souping lets us flavor ingredients on new data sources, e.g. from unlabeled data from a task for transfer or from a shift for robustness. We show that Self-Souping on corrupted test data, then fine-tuning back on uncorrupted train data, boosts robustness by +3.5\% (ImageNet-C) and +7\% (LAION-C). Self-\emph{Soup}ervision also unlocks countless SSL algorithms to cook the diverse ingredients needed for more robust soups. We show for the first time that ingredients can differ in their SSL hyperparameters -- and more surprisingly, in their SSL algorithms. We cook soups of MAE, MoCoV3, and MMCR ingredients that are more accurate than any one single SSL ingredient.</li>
</ul>

<h3>Title: TraceNAS: Zero-shot LLM Pruning via Gradient Trace Correlation</h3>
<ul>
<li><strong>Authors: </strong>Prajna G. Malettira, Manish Nagaraj, Arjun Roy, Shubham Negi, Kaushik Roy</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02891">https://arxiv.org/abs/2602.02891</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02891">https://arxiv.org/pdf/2602.02891</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02891]] TraceNAS: Zero-shot LLM Pruning via Gradient Trace Correlation(https://arxiv.org/abs/2602.02891)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Structured pruning is essential for efficient deployment of Large Language Models (LLMs). The varying sensitivity of LLM sub-blocks to pruning necessitates the identification of optimal non-uniformly pruned models. Existing methods evaluate the importance of layers, attention heads, or weight channels in isolation. Such localized focus ignores the complex global structural dependencies that exist across the model. Training-aware structured pruning addresses global dependencies, but its computational cost can be just as expensive as post-pruning training. To alleviate the computational burden of training-aware pruning and capture global structural dependencies, we propose TraceNAS, a training-free Neural Architecture Search (NAS) framework that jointly explores structured pruning of LLM depth and width. TraceNAS identifies pruned models that maintain a high degree of loss landscape alignment with the pretrained model using a scale-invariant zero-shot proxy, effectively selecting models that exhibit maximal performance potential during post-pruning training. TraceNAS is highly efficient, enabling high-fidelity discovery of pruned models on a single GPU in 8.5 hours, yielding a 10$\times$ reduction in GPU-hours compared to training-aware methods. Evaluations on the Llama and Qwen families demonstrate that TraceNAS is competitive with training-aware baselines across commonsense and reasoning benchmarks.</li>
</ul>

<h3>Title: Manifold-Constrained Energy-Based Transition Models for Offline Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Zeyu Fang, Zuyuan Zhang, Mahdi Imani, Tian Lan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02900">https://arxiv.org/abs/2602.02900</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02900">https://arxiv.org/pdf/2602.02900</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02900]] Manifold-Constrained Energy-Based Transition Models for Offline Reinforcement Learning(https://arxiv.org/abs/2602.02900)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Model-based offline reinforcement learning is brittle under distribution shift: policy improvement drives rollouts into state--action regions weakly supported by the dataset, where compounding model error yields severe value overestimation. We propose Manifold-Constrained Energy-based Transition Models (MC-ETM), which train conditional energy-based transition models using a manifold projection--diffusion negative sampler. MC-ETM learns a latent manifold of next states and generates near-manifold hard negatives by perturbing latent codes and running Langevin dynamics in latent space with the learned conditional energy, sharpening the energy landscape around the dataset support and improving sensitivity to subtle out-of-distribution deviations. For policy optimization, the learned energy provides a single reliability signal: rollouts are truncated when the minimum energy over sampled next states exceeds a threshold, and Bellman backups are stabilized via pessimistic penalties based on Q-value-level dispersion across energy-guided samples. We formalize MC-ETM through a hybrid pessimistic MDP formulation and derive a conservative performance bound separating in-support evaluation error from truncation risk. Empirically, MC-ETM improves multi-step dynamics fidelity and yields higher normalized returns on standard offline control benchmarks, particularly under irregular dynamics and sparse data coverage.</li>
</ul>

<h3>Title: Spatiotemporal Decision Transformer for Traffic Coordination</h3>
<ul>
<li><strong>Authors: </strong>Haoran Su, Yandong Sun, Hanxiao Deng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02903">https://arxiv.org/abs/2602.02903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02903">https://arxiv.org/pdf/2602.02903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02903]] Spatiotemporal Decision Transformer for Traffic Coordination(https://arxiv.org/abs/2602.02903)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Traffic signal control is a critical challenge in urban transportation, requiring coordination among multiple intersections to optimize network-wide traffic flow. While reinforcement learning has shown promise for adaptive signal control, existing methods struggle with multi-agent coordination and sample efficiency. We introduce MADT (Multi-Agent Decision Transformer), a novel approach that reformulates multi-agent traffic signal control as a sequence modeling problem. MADT extends the Decision Transformer paradigm to multi-agent settings by incorporating: (1) a graph attention mechanism for modeling spatial dependencies between intersections, (2) a|temporal transformer encoder for capturing traffic dynamics, and (3) return-to-go conditioning for target performance specification. Our approach enables offline learning from historical traffic data, with architecture design that facilitates potential online fine-tuning. Experiments on synthetic grid networks and real-world traffic scenarios demonstrate that MADT achieves state-of-the-art performance, reducing average travel time by 5-6% compared to the strongest baseline while exhibiting superior coordination among adjacent intersections.</li>
</ul>

<h3>Title: A Random Matrix Theory Perspective on the Consistency of Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Binxu Wang, Jacob Zavatone-Veth, Cengiz Pehlevan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02908">https://arxiv.org/abs/2602.02908</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02908">https://arxiv.org/pdf/2602.02908</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02908]] A Random Matrix Theory Perspective on the Consistency of Diffusion Models(https://arxiv.org/abs/2602.02908)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models trained on different, non-overlapping subsets of a dataset often produce strikingly similar outputs when given the same noise seed. We trace this consistency to a simple linear effect: the shared Gaussian statistics across splits already predict much of the generated images. To formalize this, we develop a random matrix theory (RMT) framework that quantifies how finite datasets shape the expectation and variance of the learned denoiser and sampling map in the linear setting. For expectations, sampling variability acts as a renormalization of the noise level through a self-consistent relation $\sigma^2 \mapsto \kappa(\sigma^2)$, explaining why limited data overshrink low-variance directions and pull samples toward the dataset mean. For fluctuations, our variance formulas reveal three key factors behind cross-split disagreement: \textit{anisotropy} across eigenmodes, \textit{inhomogeneity} across inputs, and overall scaling with dataset size. Extending deterministic-equivalence tools to fractional matrix powers further allows us to analyze entire sampling trajectories. The theory sharply predicts the behavior of linear diffusion models, and we validate its predictions on UNet and DiT architectures in their non-memorization regime, identifying where and how samples deviates across training data split. This provides a principled baseline for reproducibility in diffusion training, linking spectral properties of data to the stability of generative outputs.</li>
</ul>

<h3>Title: FaceLinkGen: Rethinking Identity Leakage in Privacy-Preserving Face Recognition with Identity Extraction</h3>
<ul>
<li><strong>Authors: </strong>Wenqi Guo, Shan Du</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02914">https://arxiv.org/abs/2602.02914</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02914">https://arxiv.org/pdf/2602.02914</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02914]] FaceLinkGen: Rethinking Identity Leakage in Privacy-Preserving Face Recognition with Identity Extraction(https://arxiv.org/abs/2602.02914)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, attack, extraction</a></li>
<li><strong>Abstract: </strong>Transformation-based privacy-preserving face recognition (PPFR) aims to verify identities while hiding facial data from attackers and malicious service providers. Existing evaluations mostly treat privacy as resistance to pixel-level reconstruction, measured by PSNR and SSIM. We show that this reconstruction-centric view fails. We present FaceLinkGen, an identity extraction attack that performs linkage/matching and face regeneration directly from protected templates without recovering original pixels. On three recent PPFR systems, FaceLinkGen reaches over 98.5\% matching accuracy and above 96\% regeneration success, and still exceeds 92\% matching and 94\% regeneration in a near zero knowledge setting. These results expose a structural gap between pixel distortion metrics, which are widely used in PPFR evaluation, and real privacy. We show that visual obfuscation leaves identity information broadly exposed to both external intruders and untrusted service providers.</li>
</ul>

<h3>Title: Weighted Temporal Decay Loss for Learning Wearable PPG Data with Sparse Clinical Labels</h3>
<ul>
<li><strong>Authors: </strong>Yunsung Chung, Keum San Chun, Migyeong Gwak, Han Feng, Yingshuo Liu, Chanho Lim, Viswam Nathan, Nassir Marrouche, Sharanya Arcot Desai</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02917">https://arxiv.org/abs/2602.02917</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02917">https://arxiv.org/pdf/2602.02917</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02917]] Weighted Temporal Decay Loss for Learning Wearable PPG Data with Sparse Clinical Labels(https://arxiv.org/abs/2602.02917)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Advances in wearable computing and AI have increased interest in leveraging PPG for health monitoring over the past decade. One of the biggest challenges in developing health algorithms based on such biosignals is the sparsity of clinical labels, which makes biosignals temporally distant from lab draws less reliable for supervision. To address this problem, we introduce a simple training strategy that learns a biomarker-specific decay of sample weight over the time gap between a segment and its ground truth label and uses this weight in the loss with a regularizer to prevent trivial solutions. On smartwatch PPG from 450 participants across 10 biomarkers, the approach improves over baselines. In the subject-wise setting, the proposed approach averages 0.715 AUPRC, compared to 0.674 for a fine-tuned self-supervised baseline and 0.626 for a feature-based Random Forest. A comparison of four decay families shows that a simple linear decay function is most robust on average. Beyond accuracy, the learned decay rates summarize how quickly each biomarker's PPG evidence becomes stale, providing an interpretable view of temporal sensitivity.</li>
</ul>

<h3>Title: A Multi-scale Linear-time Encoder for Whole-Slide Image Analysis</h3>
<ul>
<li><strong>Authors: </strong>Jagan Mohan Reddy Dwarampudi, Joshua Wong, Hien Van Nguyen, Tania Banerjee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, q-bio.TO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02918">https://arxiv.org/abs/2602.02918</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02918">https://arxiv.org/pdf/2602.02918</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02918]] A Multi-scale Linear-time Encoder for Whole-Slide Image Analysis(https://arxiv.org/abs/2602.02918)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We introduce Multi-scale Adaptive Recurrent Biomedical Linear-time Encoder (MARBLE), the first \textit{purely Mamba-based} multi-state multiple instance learning (MIL) framework for whole-slide image (WSI) analysis. MARBLE processes multiple magnification levels in parallel and integrates coarse-to-fine reasoning within a linear-time state-space model, efficiently capturing cross-scale dependencies with minimal parameter overhead. WSI analysis remains challenging due to gigapixel resolutions and hierarchical magnifications, while existing MIL methods typically operate at a single scale and transformer-based approaches suffer from quadratic attention costs. By coupling parallel multi-scale processing with linear-time sequence modeling, MARBLE provides a scalable and modular alternative to attention-based architectures. Experiments on five public datasets show improvements of up to \textbf{6.9\%} in AUC, \textbf{20.3\%} in accuracy, and \textbf{2.3\%} in C-index, establishing MARBLE as an efficient and generalizable framework for multi-scale WSI analysis.</li>
</ul>

<h3>Title: A Reproducible Framework for Bias-Resistant Machine Learning on Small-Sample Neuroimaging Data</h3>
<ul>
<li><strong>Authors: </strong>Jagan Mohan Reddy Dwarampudi, Jennifer L Purks, Joshua Wong, Renjie Hu, Tania Banerjee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, q-bio.NC, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02920">https://arxiv.org/abs/2602.02920</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02920">https://arxiv.org/pdf/2602.02920</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02920]] A Reproducible Framework for Bias-Resistant Machine Learning on Small-Sample Neuroimaging Data(https://arxiv.org/abs/2602.02920)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>We introduce a reproducible, bias-resistant machine learning framework that integrates domain-informed feature engineering, nested cross-validation, and calibrated decision-threshold optimization for small-sample neuroimaging data. Conventional cross-validation frameworks that reuse the same folds for both model selection and performance estimation yield optimistically biased results, limiting reproducibility and generalization. Demonstrated on a high-dimensional structural MRI dataset of deep brain stimulation cognitive outcomes, the framework achieved a nested-CV balanced accuracy of 0.660\,$\pm$\,0.068 using a compact, interpretable subset selected via importance-guided ranking. By combining interpretability and unbiased evaluation, this work provides a generalizable computational blueprint for reliable machine learning in data-limited biomedical domains.</li>
</ul>

<h3>Title: How Does the Lagrangian Guide Safe Reinforcement Learning through Diffusion Models?</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyuan Cheng, Wenxuan Yuan, Boyang Li, Yuanchao Xu, Yiming Yang, Hao Liang, Bei Peng, Robert Loftin, Zhuo Sun, Yukun Hu</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02924">https://arxiv.org/abs/2602.02924</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02924">https://arxiv.org/pdf/2602.02924</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02924]] How Does the Lagrangian Guide Safe Reinforcement Learning through Diffusion Models?(https://arxiv.org/abs/2602.02924)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion policy sampling enables reinforcement learning (RL) to represent multimodal action distributions beyond suboptimal unimodal Gaussian policies. However, existing diffusion-based RL methods primarily focus on offline settings for reward maximization, with limited consideration of safety in online settings. To address this gap, we propose Augmented Lagrangian-Guided Diffusion (ALGD), a novel algorithm for off-policy safe RL. By revisiting optimization theory and energy-based model, we show that the instability of primal-dual methods arises from the non-convex Lagrangian landscape. In diffusion-based safe RL, the Lagrangian can be interpreted as an energy function guiding the denoising dynamics. Counterintuitively, direct usage destabilizes both policy generation and training. ALGD resolves this issue by introducing an augmented Lagrangian that locally convexifies the energy landscape, yielding a stabilized policy generation and training process without altering the distribution of the optimal policy. Theoretical analysis and extensive experiments demonstrate that ALGD is both theoretically grounded and empirically effective, achieving strong and stable performance across diverse environments.</li>
</ul>

<h3>Title: Refining Decision Boundaries In Anomaly Detection Using Similarity Search Within the Feature Space</h3>
<ul>
<li><strong>Authors: </strong>Sidahmed Benabderrahmane, Petko Valtchev, James Cheney, Talal Rahwan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02925">https://arxiv.org/abs/2602.02925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02925">https://arxiv.org/pdf/2602.02925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02925]] Refining Decision Boundaries In Anomaly Detection Using Similarity Search Within the Feature Space(https://arxiv.org/abs/2602.02925)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust</a></li>
<li><strong>Abstract: </strong>Detecting rare and diverse anomalies in highly imbalanced datasets-such as Advanced Persistent Threats (APTs) in cybersecurity-remains a fundamental challenge for machine learning systems. Active learning offers a promising direction by strategically querying an oracle to minimize labeling effort, yet conventional approaches often fail to exploit the intrinsic geometric structure of the feature space for model refinement. In this paper, we introduce SDA2E, a Sparse Dual Adversarial Attention-based AutoEncoder designed to learn compact and discriminative latent representations from imbalanced, high-dimensional data. We further propose a similarity-guided active learning framework that integrates three novel strategies to refine decision boundaries efficiently: mormal-like expansion, which enriches the training set with points similar to labeled normals to improve reconstruction fidelity; anomaly-like prioritization, which boosts ranking accuracy by focusing on points resembling known anomalies; and a hybrid strategy that combines both for balanced model refinement and ranking. A key component of our framework is a new similarity measure, Normalized Matching 1s (SIM_NM1), tailored for sparse binary embeddings. We evaluate SDA2E extensively across 52 imbalanced datasets, including multiple DARPA Transparent Computing scenarios, and benchmark it against 15 state-of-the-art anomaly detection methods. Results demonstrate that SDA2E consistently achieves superior ranking performance (nDCG up to 1.0 in several cases) while reducing the required labeled data by up to 80% compared to passive training. Statistical tests confirm the significance of these improvements. Our work establishes a robust, efficient, and statistically validated framework for anomaly detection that is particularly suited to cybersecurity applications such as APT detection.</li>
</ul>

<h3>Title: Distance Marching for Generative Modeling</h3>
<ul>
<li><strong>Authors: </strong>Zimo Wang, Ishit Mehta, Haolin Lu, Chung-En Sun, Ge Yan, Tsui-Wei Weng, Tzu-Mao Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02928">https://arxiv.org/abs/2602.02928</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02928">https://arxiv.org/pdf/2602.02928</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02928]] Distance Marching for Generative Modeling(https://arxiv.org/abs/2602.02928)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Time-unconditional generative models learn time-independent denoising vector fields. But without time conditioning, the same noisy input may correspond to multiple noise levels and different denoising directions, which interferes with the supervision signal. Inspired by distance field modeling, we propose Distance Marching, a new time-unconditional approach with two principled inference methods. Crucially, we design losses that focus on closer targets. This yields denoising directions better directed toward the data manifold. Across architectures, Distance Marching consistently improves FID by 13.5% on CIFAR-10 and ImageNet over recent time-unconditional baselines. For class-conditional ImageNet generation, despite removing time input, Distance Marching surpasses flow matching using our losses and inference methods. It achieves lower FID than flow matching's final performance using 60% of the sampling steps and 13.6% lower FID on average across backbone sizes. Moreover, our distance prediction is also helpful for early stopping during sampling and for OOD detection. We hope distance field modeling can serve as a principled lens for generative modeling.</li>
</ul>

<h3>Title: RPG-AE: Neuro-Symbolic Graph Autoencoders with Rare Pattern Mining for Provenance-Based Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Asif Tauhid, Sidahmed Benabderrahmane, Mohamad Altrabulsi, Ahamed Foisal, Talal Rahwan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02929">https://arxiv.org/abs/2602.02929</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02929">https://arxiv.org/pdf/2602.02929</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02929]] RPG-AE: Neuro-Symbolic Graph Autoencoders with Rare Pattern Mining for Provenance-Based Anomaly Detection(https://arxiv.org/abs/2602.02929)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, steal, interpretability</a></li>
<li><strong>Abstract: </strong>Advanced Persistent Threats (APTs) are sophisticated, long-term cyberattacks that are difficult to detect because they operate stealthily and often blend into normal system behavior. This paper presents a neuro-symbolic anomaly detection framework that combines a Graph Autoencoder (GAE) with rare pattern mining to identify APT-like activities in system-level provenance data. Our approach first constructs a process behavioral graph using k-Nearest Neighbors based on feature similarity, then learns normal relational structure using a Graph Autoencoder. Anomaly candidates are identified through deviations between observed and reconstructed graph structure. To further improve detection, we integrate an rare pattern mining module that discovers infrequent behavioral co-occurrences and uses them to boost anomaly scores for processes exhibiting rare signatures. We evaluate the proposed method on the DARPA Transparent Computing datasets and show that rare-pattern boosting yields substantial gains in anomaly ranking quality over the baseline GAE. Compared with existing unsupervised approaches on the same benchmark, our single unified model consistently outperforms individual context-based detectors and achieves performance competitive with ensemble aggregation methods that require multiple separate detectors. These results highlight the value of coupling graph-based representation learning with classical pattern mining to improve both effectiveness and interpretability in provenance-based security anomaly detection.</li>
</ul>

<h3>Title: Equal Access, Unequal Interaction: A Counterfactual Audit of LLM Fairness</h3>
<ul>
<li><strong>Authors: </strong>Alireza Amiri-Margavi, Arshia Gharagozlou, Amin Gholami Davodi, Seyed Pouyan Mousavi Davoudi, Hamidreza Hasani Balyani</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02932">https://arxiv.org/abs/2602.02932</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02932">https://arxiv.org/pdf/2602.02932</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02932]] Equal Access, Unequal Interaction: A Counterfactual Audit of LLM Fairness(https://arxiv.org/abs/2602.02932)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Prior work on fairness in large language models (LLMs) has primarily focused on access-level behaviors such as refusals and safety filtering. However, equitable access does not ensure equitable interaction quality once a response is provided. In this paper, we conduct a controlled fairness audit examining how LLMs differ in tone, uncertainty, and linguistic framing across demographic identities after access is granted. Using a counterfactual prompt design, we evaluate GPT-4 and LLaMA-3.1-70B on career advice tasks while varying identity attributes along age, gender, and nationality. We assess access fairness through refusal analysis and measure interaction quality using automated linguistic metrics, including sentiment, politeness, and hedging. Identity-conditioned differences are evaluated using paired statistical tests. Both models exhibit zero refusal rates across all identities, indicating uniform access. Nevertheless, we observe systematic, model-specific disparities in interaction quality: GPT-4 expresses significantly higher hedging toward younger male users, while LLaMA exhibits broader sentiment variation across identity groups. These results show that fairness disparities can persist at the interaction level even when access is equal, motivating evaluation beyond refusal-based audits.</li>
</ul>

<h3>Title: 3D-Learning: Diffusion-Augmented Distributionally Robust Decision-Focused Learning</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Wen, Lei Fan, Jianyi Yang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02943">https://arxiv.org/abs/2602.02943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02943">https://arxiv.org/pdf/2602.02943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02943]] 3D-Learning: Diffusion-Augmented Distributionally Robust Decision-Focused Learning(https://arxiv.org/abs/2602.02943)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Predict-then-Optimize (PTO) pipelines are widely employed in computing and networked systems, where Machine Learning (ML) models are used to predict critical contextual information for downstream decision-making tasks such as cloud LLM serving, data center demand response, and edge workload scheduling. However, these ML predictors are often vulnerable to out-of-distribution (OOD) samples at test time, leading to significant decision performance degradation due to large prediction errors. To address the generalization challenges under OOD conditions, we present the framework of Distributionally Robust Decision-Focused Learning (DR-DFL), which trains ML models to optimize decision performance under the worst-case distribution. Instead of relying on classical Distributionally Robust Optimization (DRO) techniques, we propose Diffusion-Augmented Distributionally Robust Decision-Focused Learning (3D-Learning), which searches for the worst-case distribution within the parameterized space of a diffusion model. By leveraging the powerful distribution modeling capabilities of diffusion models, 3D-Learning identifies worst-case distributions that remain consistent with real data, achieving a favorable balance between average and worst-case scenarios. Empirical results on an LLM resource provisioning task demonstrate that 3D-Learning outperforms existing DRO and Data Augmentation methods in OOD generalization performance.</li>
</ul>

<h3>Title: SRA-Seg: Synthetic to Real Alignment for Semi-Supervised Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>OFM Riaz Rahman Aranya, Kevin Desai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02944">https://arxiv.org/abs/2602.02944</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02944">https://arxiv.org/pdf/2602.02944</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02944]] SRA-Seg: Synthetic to Real Alignment for Semi-Supervised Medical Image Segmentation(https://arxiv.org/abs/2602.02944)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Synthetic data, an appealing alternative to extensive expert-annotated data for medical image segmentation, consistently fails to improve segmentation performance despite its visual realism. The reason being that synthetic and real medical images exist in different semantic feature spaces, creating a domain gap that current semi-supervised learning methods cannot bridge. We propose SRA-Seg, a framework explicitly designed to align synthetic and real feature distributions for medical image segmentation. SRA-Seg introduces a similarity-alignment (SA) loss using frozen DINOv2 embeddings to pull synthetic representations toward their nearest real counterparts in semantic space. We employ soft edge blending to create smooth anatomical transitions and continuous labels, eliminating the hard boundaries from traditional copy-paste augmentation. The framework generates pseudo-labels for synthetic images via an EMA teacher model and applies soft-segmentation losses that respect uncertainty in mixed regions. Our experiments demonstrate strong results: using only 10% labeled real data and 90% synthetic unlabeled data, SRA-Seg achieves 89.34% Dice on ACDC and 84.42% on FIVES, significantly outperforming existing semi-supervised methods and matching the performance of methods using real unlabeled data.</li>
</ul>

<h3>Title: Variational Sparse Paired Autoencoders (vsPAIR) for Inverse Problems and Uncertainty Quantification</h3>
<ul>
<li><strong>Authors: </strong>Jack Michael Solomon, Rishi Leburu, Matthias Chung</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02948">https://arxiv.org/abs/2602.02948</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02948">https://arxiv.org/pdf/2602.02948</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02948]] Variational Sparse Paired Autoencoders (vsPAIR) for Inverse Problems and Uncertainty Quantification(https://arxiv.org/abs/2602.02948)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Inverse problems are fundamental to many scientific and engineering disciplines; they arise when one seeks to reconstruct hidden, underlying quantities from noisy measurements. Many applications demand not just point estimates but interpretable uncertainty. Providing fast inference alongside uncertainty estimates remains challenging yet desirable in numerous applications. We propose the Variational Sparse Paired Autoencoder (vsPAIR) to address this challenge. The architecture pairs a standard VAE encoding observations with a sparse VAE encoding quantities of interest, connected through a learned latent mapping. The variational structure enables uncertainty estimation, the paired architecture encourages interpretability by anchoring QoI representations to clean data, and sparse encodings provide structure by concentrating information into identifiable factors rather than diffusing across all dimensions. We also propose modifications to existing sparse VAE methods: a hard-concrete spike-and-slab relaxation for differentiable training and a beta hyperprior for adaptive sparsity levels. To validate the effectiveness of our proposed architecture, we conduct experiments on blind inpainting and computed tomography, demonstrating that vsPAIR is a capable inverse problem solver that can provide interpretable and structured uncertainty estimates.</li>
</ul>

<h3>Title: Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization</h3>
<ul>
<li><strong>Authors: </strong>Haocheng Xi, Shuo Yang, Yilong Zhao, Muyang Li, Han Cai, Xingyang Li, Yujun Lin, Zhuoyang Zhang, Jintao Zhang, Xiuyu Li, Zhiying Xu, Jun Wu, Chenfeng Xu, Ion Stoica, Song Han, Kurt Keutzer</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02958">https://arxiv.org/abs/2602.02958</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02958">https://arxiv.org/pdf/2602.02958</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02958]] Quant VideoGen: Auto-Regressive Long Video Generation via 2-Bit KV-Cache Quantization(https://arxiv.org/abs/2602.02958)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Despite rapid progress in autoregressive video diffusion, an emerging system algorithm bottleneck limits both deployability and generation capability: KV cache memory. In autoregressive video generation models, the KV cache grows with generation history and quickly dominates GPU memory, often exceeding 30 GB, preventing deployment on widely available hardware. More critically, constrained KV cache budgets restrict the effective working memory, directly degrading long horizon consistency in identity, layout, and motion. To address this challenge, we present Quant VideoGen (QVG), a training free KV cache quantization framework for autoregressive video diffusion models. QVG leverages video spatiotemporal redundancy through Semantic Aware Smoothing, producing low magnitude, quantization friendly residuals. It further introduces Progressive Residual Quantization, a coarse to fine multi stage scheme that reduces quantization error while enabling a smooth quality memory trade off. Across LongCat Video, HY WorldPlay, and Self Forcing benchmarks, QVG establishes a new Pareto frontier between quality and memory efficiency, reducing KV cache memory by up to 7.0 times with less than 4% end to end latency overhead while consistently outperforming existing baselines in generation quality.</li>
</ul>

<h3>Title: Human-Centric Traffic Signal Control for Equity: A Multi-Agent Action Branching Deep Reinforcement Learning Approach</h3>
<ul>
<li><strong>Authors: </strong>Xiaocai Zhang, Neema Nassir, Lok Sang Chan, Milad Haghani</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02959">https://arxiv.org/abs/2602.02959</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02959">https://arxiv.org/pdf/2602.02959</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02959]] Human-Centric Traffic Signal Control for Equity: A Multi-Agent Action Branching Deep Reinforcement Learning Approach(https://arxiv.org/abs/2602.02959)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Coordinating traffic signals along multimodal corridors is challenging because many multi-agent deep reinforcement learning (DRL) approaches remain vehicle-centric and struggle with high-dimensional discrete action spaces. We propose MA2B-DDQN, a human-centric multi-agent action-branching double Deep Q-Network (DQN) framework that explicitly optimizes traveler-level equity. Our key contribution is an action-branching discrete control formulation that decomposes corridor control into (i) local, per-intersection actions that allocate green time between the next two phases and (ii) a single global action that selects the total duration of those phases. This decomposition enables scalable coordination under discrete control while reducing the effective complexity of joint decision-making. We also design a human-centric reward that penalizes the number of delayed individuals in the corridor, accounting for pedestrians, vehicle occupants, and transit passengers. Extensive evaluations across seven realistic traffic scenarios in Melbourne, Australia, demonstrate that our approach significantly reduces the number of impacted travelers, outperforming existing DRL and baseline methods. Experiments confirm the robustness of our model, showing minimal variance across diverse settings. This framework not only advocates for a fairer traffic signal system but also provides a scalable solution adaptable to varied urban traffic conditions.</li>
</ul>

<h3>Title: Q-ShiftDP: A Differentially Private Parameter-Shift Rule for Quantum Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Hoang M. Ngo, Nhat Hoang-Xuan, Quan Nguyen, Nguyen Do, Incheol Shin, My T. Thai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02962">https://arxiv.org/abs/2602.02962</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02962">https://arxiv.org/pdf/2602.02962</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02962]] Q-ShiftDP: A Differentially Private Parameter-Shift Rule for Quantum Machine Learning(https://arxiv.org/abs/2602.02962)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Quantum Machine Learning (QML) promises significant computational advantages, but preserving training data privacy remains challenging. Classical approaches like differentially private stochastic gradient descent (DP-SGD) add noise to gradients but fail to exploit the unique properties of quantum gradient estimation. In this work, we introduce the Differentially Private Parameter-Shift Rule (Q-ShiftDP), the first privacy mechanism tailored to QML. By leveraging the inherent boundedness and stochasticity of quantum gradients computed via the parameter-shift rule, Q-ShiftDP enables tighter sensitivity analysis and reduces noise requirements. We combine carefully calibrated Gaussian noise with intrinsic quantum noise to provide formal privacy and utility guarantees, and show that harnessing quantum noise further improves the privacy-utility trade-off. Experiments on benchmark datasets demonstrate that Q-ShiftDP consistently outperforms classical DP methods in QML.</li>
</ul>

<h3>Title: Where Norms and References Collide: Evaluating LLMs on Normative Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Mitchell Abrams, Kaveh Eskandari Miandoab, Felix Gervits, Vasanth Sarathy, Matthias Scheutz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02975">https://arxiv.org/abs/2602.02975</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02975">https://arxiv.org/pdf/2602.02975</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02975]] Where Norms and References Collide: Evaluating LLMs on Normative Reasoning(https://arxiv.org/abs/2602.02975)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Embodied agents, such as robots, will need to interact in situated environments where successful communication often depends on reasoning over social norms: shared expectations that constrain what actions are appropriate in context. A key capability in such settings is norm-based reference resolution (NBRR), where interpreting referential expressions requires inferring implicit normative expectations grounded in physical and social context. Yet it remains unclear whether Large Language Models (LLMs) can support this kind of reasoning. In this work, we introduce SNIC (Situated Norms in Context), a human-validated diagnostic testbed designed to probe how well state-of-the-art LLMs can extract and utilize normative principles relevant to NBRR. SNIC emphasizes physically grounded norms that arise in everyday tasks such as cleaning, tidying, and serving. Across a range of controlled evaluations, we find that even the strongest LLMs struggle to consistently identify and apply social norms, particularly when norms are implicit, underspecified, or in conflict. These findings reveal a blind spot in current LLMs and highlight a key challenge for deploying language-based systems in socially situated, embodied settings.</li>
</ul>

<h3>Title: Aligning Forest and Trees in Images and Long Captions for Visually Grounded Understanding</h3>
<ul>
<li><strong>Authors: </strong>Byeongju Woo, Zilin Wang, Byeonghyun Pak, Sangwoo Mo, Stella X. Yu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02977">https://arxiv.org/abs/2602.02977</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02977">https://arxiv.org/pdf/2602.02977</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02977]] Aligning Forest and Trees in Images and Long Captions for Visually Grounded Understanding(https://arxiv.org/abs/2602.02977)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Large vision-language models such as CLIP struggle with long captions because they align images and texts as undifferentiated wholes. Fine-grained vision-language understanding requires hierarchical semantics capturing both global context and localized details across visual and textual domains. Yet linguistic hierarchies from syntax or semantics rarely match visual organization, and purely visual hierarchies tend to fragment scenes into appearance-driven parts without semantic focus. We propose CAFT (Cross-domain Alignment of Forests and Trees), a hierarchical image-text representation learning framework that aligns global and local semantics across images and long captions without pixel-level supervision. Coupling a fine-to-coarse visual encoder with a hierarchical text transformer, it uses a hierarchical alignment loss that matches whole images with whole captions while biasing region-sentence correspondences, so that coarse semantics are built from fine-grained evidence rather than from aggregation untethered to part-level grounding. Trained on 30M image-text pairs, CAFT achieves state-of-the-art performance on six long-text retrieval benchmarks and exhibits strong scaling behavior. Experiments show that hierarchical cross-domain alignment enables fine-grained, visually grounded image-text representations to emerge without explicit region-level supervision.</li>
</ul>

<h3>Title: CPMobius: Iterative Coach-Player Reasoning for Data-Free Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Ran Li, Zeyuan Liu, Yinghao chen, Bingxiang He, Jiarui Yuan, Zixuan Fu, Weize Chen, Jinyi Hu, Zhiyuan Liu, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02979">https://arxiv.org/abs/2602.02979</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02979">https://arxiv.org/pdf/2602.02979</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02979]] CPMobius: Iterative Coach-Player Reasoning for Data-Free Reinforcement Learning(https://arxiv.org/abs/2602.02979)</code><input type="text"></li>
<li><strong>Keywords: </strong>data-free, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated strong potential in complex reasoning, yet their progress remains fundamentally constrained by reliance on massive high-quality human-curated tasks and labels, either through supervised fine-tuning (SFT) or reinforcement learning (RL) on reasoning-specific data. This dependence renders supervision-heavy training paradigms increasingly unsustainable, with signs of diminishing scalability already evident in practice. To overcome this limitation, we introduce CPMöbius (CPMobius), a collaborative Coach-Player paradigm for data-free reinforcement learning of reasoning models. Unlike traditional adversarial self-play, CPMöbius, inspired by real world human sports collaboration and multi-agent collaboration, treats the Coach and Player as independent but cooperative roles. The Coach proposes instructions targeted at the Player's capability and receives rewards based on changes in the Player's performance, while the Player is rewarded for solving the increasingly instructive tasks generated by the Coach. This cooperative optimization loop is designed to directly enhance the Player's mathematical reasoning ability. Remarkably, CPMöbius achieves substantial improvement without relying on any external training data, outperforming existing unsupervised approaches. For example, on Qwen2.5-Math-7B-Instruct, our method improves accuracy by an overall average of +4.9 and an out-of-distribution average of +5.4, exceeding RENT by +1.5 on overall accuracy and R-zero by +4.2 on OOD accuracy.</li>
</ul>

<h3>Title: Why Some Models Resist Unlearning: A Linear Stability Perspective</h3>
<ul>
<li><strong>Authors: </strong>Wei-Kai Chang, Rajiv Khanna</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02986">https://arxiv.org/abs/2602.02986</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02986">https://arxiv.org/pdf/2602.02986</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02986]] Why Some Models Resist Unlearning: A Linear Stability Perspective(https://arxiv.org/abs/2602.02986)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Machine unlearning, the ability to erase the effect of specific training samples without retraining from scratch, is critical for privacy, regulation, and efficiency. However, most progress in unlearning has been empirical, with little theoretical understanding of when and why unlearning works. We tackle this gap by framing unlearning through the lens of asymptotic linear stability to capture the interaction between optimization dynamics and data geometry. The key quantity in our analysis is data coherence which is the cross sample alignment of loss surface directions near the optimum. We decompose coherence along three axes: within the retain set, within the forget set, and between them, and prove tight stability thresholds that separate convergence from divergence. To further link data properties to forgettability, we study a two layer ReLU CNN under a signal plus noise model and show that stronger memorization makes forgetting easier: when the signal to noise ratio (SNR) is lower, cross sample alignment is weaker, reducing coherence and making unlearning easier; conversely, high SNR, highly aligned models resist unlearning. For empirical verification, we show that Hessian tests and CNN heatmaps align closely with the predicted boundary, mapping the stability frontier of gradient based unlearning as a function of batching, mixing, and data/model alignment. Our analysis is grounded in random matrix theory tools and provides the first principled account of the trade offs between memorization, coherence, and unlearning.</li>
</ul>

<h3>Title: NLI:Non-uniform Linear Interpolation Approximation of Nonlinear Operations for Efficient LLMs Inference</h3>
<ul>
<li><strong>Authors: </strong>Jiangyong Yu, Xiaomeng Han, Xing Hu, Chen Xu, Zhe Jiang, Dawei Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02988">https://arxiv.org/abs/2602.02988</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02988">https://arxiv.org/pdf/2602.02988</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02988]] NLI:Non-uniform Linear Interpolation Approximation of Nonlinear Operations for Efficient LLMs Inference(https://arxiv.org/abs/2602.02988)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of tasks, but their deployment is often constrained by substantial memory footprints and computational costs. While prior work has achieved significant progress in compressing and accelerating linear layers, nonlinear layers-such as SiLU, RMSNorm, and Softmax-still heavily depend on high-precision floating-point operations. In this paper, we propose a calibration-free, dynamic-programming-optimal, and hardware-friendly framework called Non-uniform Linear Interpolation (NLI). NLI is capable of efficiently approximating a variety of nonlinear functions, enabling seamless integration into LLMs and other deep neural networks with almost no loss in accuracy. NLI ingeniously recasts cutpoint selection as a dynamic-programming problem, achieving the globally minimal interpolation error in O(MxN2) time via Bellman's optimality principle. Based on the NLI algorithm, we also design and implement a plug-and-play universal nonlinear computation unit. Hardware experiments demonstrate that the NLI Engine achieves more than 4x improvement in computational efficiency compared to the state-of-the-art designs.</li>
</ul>

<h3>Title: Video-OPD: Efficient Post-Training of Multimodal Large Language Models for Temporal Video Grounding via On-Policy Distillation</h3>
<ul>
<li><strong>Authors: </strong>Jiaze Li, Hao Yin, Haoran Xu, Boshen Xu, Wenhui Tan, Zewen He, Jianzhong Ju, Zhenbo Luo, Jian Luan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.02994">https://arxiv.org/abs/2602.02994</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.02994">https://arxiv.org/pdf/2602.02994</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.02994]] Video-OPD: Efficient Post-Training of Multimodal Large Language Models for Temporal Video Grounding via On-Policy Distillation(https://arxiv.org/abs/2602.02994)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning has emerged as a principled post-training paradigm for Temporal Video Grounding (TVG) due to its on-policy optimization, yet existing GRPO-based methods remain fundamentally constrained by sparse reward signals and substantial computational overhead. We propose Video-OPD, an efficient post-training framework for TVG inspired by recent advances in on-policy distillation. Video-OPD optimizes trajectories sampled directly from the current policy, thereby preserving alignment between training and inference distributions, while a frontier teacher supplies dense, token-level supervision via a reverse KL divergence objective. This formulation preserves the on-policy property critical for mitigating distributional shift, while converting sparse, episode-level feedback into fine-grained, step-wise learning signals. Building on Video-OPD, we introduce Teacher-Validated Disagreement Focusing (TVDF), a lightweight training curriculum that iteratively prioritizes trajectories that are both teacher-reliable and maximally informative for the student, thereby improving training efficiency. Empirical results demonstrate that Video-OPD consistently outperforms GRPO while achieving substantially faster convergence and lower computational cost, establishing on-policy distillation as an effective alternative to conventional reinforcement learning for TVG.</li>
</ul>

<h3>Title: Causal Graph Spatial-Temporal Autoencoder for Reliable and Interpretable Process Monitoring</h3>
<ul>
<li><strong>Authors: </strong>Xiangrui Zhang, Chunyue Song, Wei Dai, Zheng Zhang, Kaihua Gao, Furong Gao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03004">https://arxiv.org/abs/2602.03004</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03004">https://arxiv.org/pdf/2602.03004</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03004]] Causal Graph Spatial-Temporal Autoencoder for Reliable and Interpretable Process Monitoring(https://arxiv.org/abs/2602.03004)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>To improve the reliability and interpretability of industrial process monitoring, this article proposes a Causal Graph Spatial-Temporal Autoencoder (CGSTAE). The network architecture of CGSTAE combines two components: a correlation graph structure learning module based on spatial self-attention mechanism (SSAM) and a spatial-temporal encoder-decoder module utilizing graph convolutional long-short term memory (GCLSTM). The SSAM learns correlation graphs by capturing dynamic relationships between variables, while a novel three-step causal graph structure learning algorithm is introduced to derive a causal graph from these correlation graphs. The algorithm leverages a reverse perspective of causal invariance principle to uncover the invariant causal graph from varying correlations. The spatial-temporal encoder-decoder, built with GCLSTM units, reconstructs time-series process data within a sequence-to-sequence framework. The proposed CGSTAE enables effective process monitoring and fault detection through two statistics in the feature space and residual space. Finally, we validate the effectiveness of CGSTAE in process monitoring through the Tennessee Eastman process and a real-world air separation process.</li>
</ul>

<h3>Title: CVE-Factory: Scaling Expert-Level Agentic Tasks for Code Security Vulnerability</h3>
<ul>
<li><strong>Authors: </strong>Xianzhen Luo, Jingyuan Zhang, Shiqi Zhou, Rain Huang, Chuan Xiao, Qingfu Zhu, Zhiyuan Ma, Xing Yue, Yang Yue, Wencong Zeng, Wanxiang Che</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03012">https://arxiv.org/abs/2602.03012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03012">https://arxiv.org/pdf/2602.03012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03012]] CVE-Factory: Scaling Expert-Level Agentic Tasks for Code Security Vulnerability(https://arxiv.org/abs/2602.03012)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Evaluating and improving the security capabilities of code agents requires high-quality, executable vulnerability tasks. However, existing works rely on costly, unscalable manual reproduction and suffer from outdated data distributions. To address these, we present CVE-Factory, the first multi-agent framework to achieve expert-level quality in automatically transforming sparse CVE metadata into fully executable agentic tasks. Cross-validation against human expert reproductions shows that CVE-Factory achieves 95\% solution correctness and 96\% environment fidelity, confirming its expert-level quality. It is also evaluated on the latest realistic vulnerabilities and achieves a 66.2\% verified success. This automation enables two downstream contributions. First, we construct LiveCVEBench, a continuously updated benchmark of 190 tasks spanning 14 languages and 153 repositories that captures emerging threats including AI-tooling vulnerabilities. Second, we synthesize over 1,000 executable training environments, the first large-scale scaling of agentic tasks in code security. Fine-tuned Qwen3-32B improves from 5.3\% to 35.8\% on LiveCVEBench, surpassing Claude 4.5 Sonnet, with gains generalizing to Terminal Bench (12.5\% to 31.3\%). We open-source CVE-Factory, LiveCVEBench, Abacus-cve (fine-tuned model), training dataset, and leaderboard. All resources are available at this https URL .</li>
</ul>

<h3>Title: FedKRSO: Communication and Memory Efficient Federated Fine-Tuning of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Guohao Yang, Tongle Wu, Yuanxiong Guo, Ying Sun, Yanmin Gong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03019">https://arxiv.org/abs/2602.03019</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03019">https://arxiv.org/pdf/2602.03019</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03019]] FedKRSO: Communication and Memory Efficient Federated Fine-Tuning of Large Language Models(https://arxiv.org/abs/2602.03019)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate, large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning is essential to adapt general-purpose large language models (LLMs) to domain-specific tasks. As a privacy-preserving framework to leverage decentralized data for collaborative model training, Federated Learning (FL) is gaining popularity in LLM fine-tuning, but remains challenging due to the high cost of transmitting full model parameters and computing full gradients on resource-constrained clients. While Parameter-Efficient Fine-Tuning (PEFT) methods are widely used in FL to reduce communication and memory costs, they often sacrifice model performance compared to FFT. This paper proposes FedKRSO (Federated $K$-Seed Random Subspace Optimization), a novel method that enables communication and memory efficient FFT of LLMs in federated settings. In FedKRSO, clients update the model within a shared set of random low-dimension subspaces generated by the server to save memory usage. Furthermore, instead of transmitting full model parameters in each FL round, clients send only the model update accumulators along the subspaces to the server, enabling efficient global model aggregation and dissemination. By using these strategies, FedKRSO can substantially reduce communication and memory overhead while overcoming the performance limitations of PEFT, closely approximating the performance of federated FFT. The convergence properties of FedKRSO are analyzed rigorously under general FL settings. Extensive experiments on the GLUE benchmark across diverse FL scenarios demonstrate that FedKRSO achieves both superior performance and low communication and memory overhead, paving the way towards on federated LLM fine-tuning at the resource-constrained edge.</li>
</ul>

<h3>Title: Generalizable and Interpretable RF Fingerprinting with Shapelet-Enhanced Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tianya Zhao, Junqing Zhang, Haowen Xu, Xiaoyan Sun, Jun Dai, Xuyu Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03035">https://arxiv.org/abs/2602.03035</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03035">https://arxiv.org/pdf/2602.03035</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03035]] Generalizable and Interpretable RF Fingerprinting with Shapelet-Enhanced Large Language Models(https://arxiv.org/abs/2602.03035)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Deep neural networks (DNNs) have achieved remarkable success in radio frequency (RF) fingerprinting for wireless device authentication. However, their practical deployment faces two major limitations: domain shift, where models trained in one environment struggle to generalize to others, and the black-box nature of DNNs, which limits interpretability. To address these issues, we propose a novel framework that integrates a group of variable-length two-dimensional (2D) shapelets with a pre-trained large language model (LLM) to achieve efficient, interpretable, and generalizable RF fingerprinting. The 2D shapelets explicitly capture diverse local temporal patterns across the in-phase and quadrature (I/Q) components, providing compact and interpretable representations. Complementarily, the pre-trained LLM captures more long-range dependencies and global contextual information, enabling strong generalization with minimal training overhead. Moreover, our framework also supports prototype generation for few-shot inference, enhancing cross-domain performance without additional retraining. To evaluate the effectiveness of our proposed method, we conduct extensive experiments on six datasets across various protocols and domains. The results show that our method achieves superior standard and few-shot performance across both source and unseen domains.</li>
</ul>

<h3>Title: LatentMem: Customizing Latent Memory for Multi-Agent Systems</h3>
<ul>
<li><strong>Authors: </strong>Muxin Fu, Guibin Zhang, Xiangyuan Xue, Yafu Li, Zefeng He, Siyuan Huang, Xiaoye Qu, Yu Cheng, Yang Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03036">https://arxiv.org/abs/2602.03036</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03036">https://arxiv.org/pdf/2602.03036</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03036]] LatentMem: Customizing Latent Memory for Multi-Agent Systems(https://arxiv.org/abs/2602.03036)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language model (LLM)-powered multi-agent systems (MAS) demonstrate remarkable collective intelligence, wherein multi-agent memory serves as a pivotal mechanism for continual adaptation. However, existing multi-agent memory designs remain constrained by two fundamental bottlenecks: (i) memory homogenization arising from the absence of role-aware customization, and (ii) information overload induced by excessively fine-grained memory entries. To address these limitations, we propose LatentMem, a learnable multi-agent memory framework designed to customize agent-specific memories in a token-efficient manner. Specifically, LatentMem comprises an experience bank that stores raw interaction trajectories in a lightweight form, and a memory composer that synthesizes compact latent memories conditioned on retrieved experience and agent-specific contexts. Further, we introduce Latent Memory Policy Optimization (LMPO), which propagates task-level optimization signals through latent memories to the composer, encouraging it to produce compact and high-utility representations. Extensive experiments across diverse benchmarks and mainstream MAS frameworks show that LatentMem achieves a performance gain of up to $19.36$% over vanilla settings and consistently outperforms existing memory architectures, without requiring any modifications to the underlying frameworks.</li>
</ul>

<h3>Title: HP-GAN: Harnessing pretrained networks for GAN improvement with FakeTwins and discriminator consistency</h3>
<ul>
<li><strong>Authors: </strong>Geonhui Son, Jeong Ryong Lee, Dosik Hwang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03039">https://arxiv.org/abs/2602.03039</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03039">https://arxiv.org/pdf/2602.03039</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03039]] HP-GAN: Harnessing pretrained networks for GAN improvement with FakeTwins and discriminator consistency(https://arxiv.org/abs/2602.03039)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, generative</a></li>
<li><strong>Abstract: </strong>Generative Adversarial Networks (GANs) have made significant progress in enhancing the quality of image synthesis. Recent methods frequently leverage pretrained networks to calculate perceptual losses or utilize pretrained feature spaces. In this paper, we extend the capabilities of pretrained networks by incorporating innovative self-supervised learning techniques and enforcing consistency between discriminators during GAN training. Our proposed method, named HP-GAN, effectively exploits neural network priors through two primary strategies: FakeTwins and discriminator consistency. FakeTwins leverages pretrained networks as encoders to compute a self-supervised loss and applies this through the generated images to train the generator, thereby enabling the generation of more diverse and high quality images. Additionally, we introduce a consistency mechanism between discriminators that evaluate feature maps extracted from Convolutional Neural Network (CNN) and Vision Transformer (ViT) feature networks. Discriminator consistency promotes coherent learning among discriminators and enhances training robustness by aligning their assessments of image quality. Our extensive evaluation across seventeen datasets-including scenarios with large, small, and limited data, and covering a variety of image domains-demonstrates that HP-GAN consistently outperforms current state-of-the-art methods in terms of Fréchet Inception Distance (FID), achieving significant improvements in image diversity and quality. Code is available at: this https URL.</li>
</ul>

<h3>Title: DF-LoGiT: Data-Free Logic-Gated Backdoor Attacks in Vision Transformers</h3>
<ul>
<li><strong>Authors: </strong>Xiaozuo Shen, Yifei Cai, Rui Ning, Chunsheng Xin, Hongyi Wu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03040">https://arxiv.org/abs/2602.03040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03040">https://arxiv.org/pdf/2602.03040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03040]] DF-LoGiT: Data-Free Logic-Gated Backdoor Attacks in Vision Transformers(https://arxiv.org/abs/2602.03040)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, steal, data-free, transformer</a></li>
<li><strong>Abstract: </strong>The widespread adoption of Vision Transformers (ViTs) elevates supply-chain risk on third-party model hubs, where an adversary can implant backdoors into released checkpoints. Existing ViT backdoor attacks largely rely on poisoned-data training, while prior data-free attempts typically require synthetic-data fine-tuning or extra model components. This paper introduces Data-Free Logic-Gated Backdoor Attacks (DF-LoGiT), a truly data-free backdoor attack on ViTs via direct weight editing. DF-LoGiT exploits ViT's native multi-head architecture to realize a logic-gated compositional trigger, enabling a stealthy and effective backdoor. We validate its effectiveness through theoretical analysis and extensive experiments, showing that DF-LoGiT achieves near-100% attack success with negligible degradation in benign accuracy and remains robust against representative classical and ViT-specific defenses.</li>
</ul>

<h3>Title: SAFE-KD: Risk-Controlled Early-Exit Distillation for Vision Backbones</h3>
<ul>
<li><strong>Authors: </strong>Salim Khazem</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03043">https://arxiv.org/abs/2602.03043</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03043">https://arxiv.org/pdf/2602.03043</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03043]] SAFE-KD: Risk-Controlled Early-Exit Distillation for Vision Backbones(https://arxiv.org/abs/2602.03043)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Early-exit networks reduce inference cost by allowing ``easy'' inputs to stop early, but practical deployment hinges on knowing \emph{when} early exit is safe. We introduce SAFE-KD, a universal multi-exit wrapper for modern vision backbones that couples hierarchical distillation with \emph{conformal risk control}. SAFE-KD attaches lightweight exit heads at intermediate depths, distills a strong teacher into all exits via Decoupled Knowledge Distillation (DKD), and enforces deep-to-shallow consistency between exits. At inference, we calibrate per-exit stopping thresholds on a held-out set using conformal risk control (CRC) to guarantee a user-specified \emph{selective} misclassification risk (among the samples that exit early) under exchangeability. Across multiple datasets and architectures, SAFE-KD yields improved accuracy compute trade-offs, stronger calibration, and robust performance under corruption while providing finite-sample risk guarantees.</li>
</ul>

<h3>Title: Clarify Before You Draw: Proactive Agents for Robust Text-to-CAD Generation</h3>
<ul>
<li><strong>Authors: </strong>Bo Yuan, Zelin Zhao, Petr Molodyk, Bin Hu, Yongxin Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03045">https://arxiv.org/abs/2602.03045</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03045">https://arxiv.org/pdf/2602.03045</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03045]] Clarify Before You Draw: Proactive Agents for Robust Text-to-CAD Generation(https://arxiv.org/abs/2602.03045)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models have recently enabled text-to-CAD systems that synthesize parametric CAD programs (e.g., CadQuery) from natural language prompts. In practice, however, geometric descriptions can be under-specified or internally inconsistent: critical dimensions may be missing and constraints may conflict. Existing fine-tuned models tend to reactively follow user instructions and hallucinate dimensions when the text is ambiguous. To address this, we propose a proactive agentic framework for text-to-CadQuery generation, named ProCAD, that resolves specification issues before code synthesis. Our framework pairs a proactive clarifying agent, which audits the prompt and asks targeted clarification questions only when necessary to produce a self-consistent specification, with a CAD coding agent that translates the specification into an executable CadQuery program. We fine-tune the coding agent on a curated high-quality text-to-CadQuery dataset and train the clarifying agent via agentic SFT on clarification trajectories. Experiments show that proactive clarification significantly improves robustness to ambiguous prompts while keeping interaction overhead low. ProCAD outperforms frontier closed-source models, including Claude Sonnet 4.5, reducing the mean Chamfer distance by 79.9 percent and lowering the invalidity ratio from 4.8 percent to 0.9 percent. Our code and datasets will be made publicly available.</li>
</ul>

<h3>Title: SAES-SVD: Self-Adaptive Suppression of Accumulated and Local Errors for SVD-based LLM Compression</h3>
<ul>
<li><strong>Authors: </strong>Xing Hu, Dawei Yang, Yuan Cheng, Zhixuan Chen, Zukang Xu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03051">https://arxiv.org/abs/2602.03051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03051">https://arxiv.org/pdf/2602.03051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03051]] SAES-SVD: Self-Adaptive Suppression of Accumulated and Local Errors for SVD-based LLM Compression(https://arxiv.org/abs/2602.03051)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid growth in the parameter scale of large language models (LLMs) has created a high demand for efficient compression techniques. As a hardware-agnostic and highly compatible technique, low-rank compression has been widely adopted. However, existing methods typically compress each layer independently by minimizing per-layer reconstruction error, overlooking a critical limitation: the reconstruction error propagates and accumulates through the network, which leads to amplified global deviations from the full-precision baseline. To address this, we propose Self-Adaptive Error Suppression SVD (SAES-SVD), a LLMs compression framework that jointly optimizes intra-layer reconstruction and inter-layer error compensation. SAES-SVD is composed of two novel components: (1) Cumulative Error-Aware Layer Compression (CEALC), which formulates the compression objective as a combination of local reconstruction and weighted cumulative error compensation. Based on it, we derive a closed-form low-rank solution relied on second-order activation statistics, which explicitly aligns each layer's output with its full-precision counterpart to compensate for accumulated errors. (2) Adaptive Collaborative Error Suppression (ACES), which automatically adjusts the weighting coefficient to enhance the low-rank structure of the compression objective in CEALC. Specifically, the coefficient is optimized to maximize the ratio between the Frobenius norm of the compressed layer's output and that of the compression objective under a fixed rank, thus ensuring that the rank budget is utilized effectively. Extensive experiments across multiple LLM architectures and tasks show that, without fine-tuning or mixed-rank strategies, SAES-SVD consistently improves post-compression performance.</li>
</ul>

<h3>Title: Fedcompass: Federated Clustered and Periodic Aggregation Framework for Hybrid Classical-Quantum Models</h3>
<ul>
<li><strong>Authors: </strong>Yueheng Wang, Xing He, Zinuo Cai, Rui Zhang, Ruhui Ma, Yuan Liu, Rajkumar Buyya</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03052">https://arxiv.org/abs/2602.03052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03052">https://arxiv.org/pdf/2602.03052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03052]] Fedcompass: Federated Clustered and Periodic Aggregation Framework for Hybrid Classical-Quantum Models(https://arxiv.org/abs/2602.03052)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated learning enables collaborative model training across decentralized clients under privacy constraints. Quantum computing offers potential for alleviating computational and communication burdens in federated learning, yet hybrid classical-quantum federated learning remains susceptible to performance degradation under non-IID data. To address this,we propose FEDCOMPASS, a layered aggregation framework for hybrid classical-quantum federated learning. FEDCOMPASS employs spectral clustering to group clients by class distribution similarity and performs cluster-wise aggregation for classical feature extractors. For quantum parameters, it uses circular mean aggregation combined with adaptive optimization to ensure stable global updates. Experiments on three benchmark datasets show that FEDCOMPASS improves test accuracy by up to 10.22% and enhances convergence stability under non-IID settings, outperforming six strong federated learning baselines.</li>
</ul>

<h3>Title: IVC-Prune: Revealing the Implicit Visual Coordinates in LVLMs for Vision Token Pruning</h3>
<ul>
<li><strong>Authors: </strong>Zhichao Sun, Yidong Ma, Gang Liu, Yibo Chen, Xu Tang, Yao Hu, Yongchao Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03060">https://arxiv.org/abs/2602.03060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03060">https://arxiv.org/pdf/2602.03060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03060]] IVC-Prune: Revealing the Implicit Visual Coordinates in LVLMs for Vision Token Pruning(https://arxiv.org/abs/2602.03060)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Large Vision-Language Models (LVLMs) achieve impressive performance across multiple tasks. A significant challenge, however, is their prohibitive inference cost when processing high-resolution visual inputs. While visual token pruning has emerged as a promising solution, existing methods that primarily focus on semantic relevance often discard tokens that are crucial for spatial reasoning. We address this gap through a novel insight into \emph{how LVLMs process spatial reasoning}. Specifically, we reveal that LVLMs implicitly establish visual coordinate systems through Rotary Position Embeddings (RoPE), where specific token positions serve as \textbf{implicit visual coordinates} (IVC tokens) that are essential for spatial reasoning. Based on this insight, we propose \textbf{IVC-Prune}, a training-free, prompt-aware pruning strategy that retains both IVC tokens and semantically relevant foreground tokens. IVC tokens are identified by theoretically analyzing the mathematical properties of RoPE, targeting positions at which its rotation matrices approximate identity matrix or the $90^\circ$ rotation matrix. Foreground tokens are identified through a robust two-stage process: semantic seed discovery followed by contextual refinement via value-vector similarity. Extensive evaluations across four representative LVLMs and twenty diverse benchmarks show that IVC-Prune reduces visual tokens by approximately 50\% while maintaining $\geq$ 99\% of the original performance and even achieving improvements on several benchmarks. Source codes are available at this https URL.</li>
</ul>

<h3>Title: FlashSinkhorn: IO-Aware Entropic Optimal Transport</h3>
<ul>
<li><strong>Authors: </strong>Felix X.-F. Ye, Xingjie Li, An Yu, Ming-Ching Chang, Linsong Chu, Davis Wertheimer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03067">https://arxiv.org/abs/2602.03067</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03067">https://arxiv.org/pdf/2602.03067</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03067]] FlashSinkhorn: IO-Aware Entropic Optimal Transport(https://arxiv.org/abs/2602.03067)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Entropic optimal transport (EOT) via Sinkhorn iterations is widely used in modern machine learning, yet GPU solvers remain inefficient at scale. Tensorized implementations suffer quadratic HBM traffic from dense $n\times m$ interactions, while existing online backends avoid storing dense matrices but still rely on generic tiled map-reduce reduction kernels with limited fusion. We present \textbf{FlashSinkhorn}, an IO-aware EOT solver for squared Euclidean cost that rewrites stabilized log-domain Sinkhorn updates as row-wise LogSumExp reductions of biased dot-product scores, the same normalization as transformer attention. This enables FlashAttention-style fusion and tiling: fused Triton kernels stream tiles through on-chip SRAM and update dual potentials in a single pass, substantially reducing HBM IO per iteration while retaining linear-memory operations. We further provide streaming kernels for transport application, enabling scalable first- and second-order optimization. On A100 GPUs, FlashSinkhorn achieves up to $32\times$ forward-pass and $161\times$ end-to-end speedups over state-of-the-art online baselines on point-cloud OT, improves scalability on OT-based downstream tasks. For reproducibility, we release an open-source implementation at this https URL.</li>
</ul>

<h3>Title: TMS: Trajectory-Mixed Supervision for Reward-Free, On-Policy SFT</h3>
<ul>
<li><strong>Authors: </strong>Rana Muhammad Shahroz Khan, Zijie Liu, Zhen Tan, Charles Fleming, Tianlong Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03073">https://arxiv.org/abs/2602.03073</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03073">https://arxiv.org/pdf/2602.03073</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03073]] TMS: Trajectory-Mixed Supervision for Reward-Free, On-Policy SFT(https://arxiv.org/abs/2602.03073)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT) are the two dominant paradigms for enhancing Large Language Model (LLM) performance on downstream tasks. While RL generally preserves broader model capabilities (retention) better than SFT, it comes with significant costs: complex reward engineering, instability, and expensive on-policy sampling. In contrast, SFT is efficient but brittle, often suffering from catastrophic forgetting due to $\textbf{Supervision Mismatch}$: the divergence between the model's evolving policy and static training labels. We address this trade-off with $\textbf{Trajectory-Mixed Supervision (TMS)}$, a reward-free framework that approximates the on-policy benefits of RL by creating a dynamic curriculum from the model's own historical checkpoints. TMS minimizes $\textit{Policy-Label Divergence (PLD)}$, preventing the mode collapse that drives forgetting in standard SFT. Experiments across reasoning (MATH, GSM8K) and instruction-following benchmarks demonstrate that TMS effectively shifts the accuracy--retention Pareto frontier. While RL remains the gold standard for retention, TMS significantly outperforms standard and iterative SFT, bridging the gap to RL without requiring reward models or verifiers. Mechanistic analysis confirms that PLD drift accurately predicts forgetting and that TMS successfully mitigates this drift.</li>
</ul>

<h3>Title: ReMiT: RL-Guided Mid-Training for Iterative LLM Evolution</h3>
<ul>
<li><strong>Authors: </strong>Junjie Huang, Jiarui Qin, Di Yin, Weiwen Liu, Yong Yu, Xing Sun, Weinan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03075">https://arxiv.org/abs/2602.03075</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03075">https://arxiv.org/pdf/2602.03075</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03075]] ReMiT: RL-Guided Mid-Training for Iterative LLM Evolution(https://arxiv.org/abs/2602.03075)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Standard training pipelines for large language models (LLMs) are typically unidirectional, progressing from pre-training to post-training. However, the potential for a bidirectional process--where insights from post-training retroactively improve the pre-trained foundation--remains unexplored. We aim to establish a self-reinforcing flywheel: a cycle in which reinforcement learning (RL)-tuned model strengthens the base model, which in turn enhances subsequent post-training performance, requiring no specially trained teacher or reference model. To realize this, we analyze training dynamics and identify the mid-training (annealing) phase as a critical turning point for model capabilities. This phase typically occurs at the end of pre-training, utilizing high-quality corpora under a rapidly decaying learning rate. Building upon this insight, we introduce ReMiT (Reinforcement Learning-Guided Mid-Training). Specifically, ReMiT leverages the reasoning priors of RL-tuned models to dynamically reweight tokens during the mid-training phase, prioritizing those pivotal for reasoning. Empirically, ReMiT achieves an average improvement of 3\% on 10 pre-training benchmarks, spanning math, code, and general reasoning, and sustains these gains by over 2\% throughout the post-training pipeline. These results validate an iterative feedback loop, enabling continuous and self-reinforcing evolution of LLMs.</li>
</ul>

<h3>Title: A generalizable large-scale foundation model for musculoskeletal radiographs</h3>
<ul>
<li><strong>Authors: </strong>Shinn Kim, Soobin Lee, Kyoungseob Shin, Han-Soo Kim, Yongsung Kim, Minsu Kim, Juhong Nam, Somang Ko, Daeheon Kwon, Wook Huh, Ilkyu Han, Sunghoon Kwon</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03076">https://arxiv.org/abs/2602.03076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03076">https://arxiv.org/pdf/2602.03076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03076]] A generalizable large-scale foundation model for musculoskeletal radiographs(https://arxiv.org/abs/2602.03076)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Artificial intelligence (AI) has shown promise in detecting and characterizing musculoskeletal diseases from radiographs. However, most existing models remain task-specific, annotation-dependent, and limited in generalizability across diseases and anatomical regions. Although a generalizable foundation model trained on large-scale musculoskeletal radiographs is clinically needed, publicly available datasets remain limited in size and lack sufficient diversity to enable training across a wide range of musculoskeletal conditions and anatomical sites. Here, we present SKELEX, a large-scale foundation model for musculoskeletal radiographs, trained using self-supervised learning on 1.2 million diverse, condition-rich images. The model was evaluated on 12 downstream diagnostic tasks and generally outperformed baselines in fracture detection, osteoarthritis grading, and bone tumor classification. Furthermore, SKELEX demonstrated zero-shot abnormality localization, producing error maps that identified pathologic regions without task-specific training. Building on this capability, we developed an interpretable, region-guided model for predicting bone tumors, which maintained robust performance on independent external datasets and was deployed as a publicly accessible web application. Overall, SKELEX provides a scalable, label-efficient, and generalizable AI framework for musculoskeletal imaging, establishing a foundation for both clinical translation and data-efficient research in musculoskeletal radiology.</li>
</ul>

<h3>Title: Geometry-Preserving Neural Architectures on Manifolds with Boundary</h3>
<ul>
<li><strong>Authors: </strong>Karthik Elamvazhuthi, Shiba Biswal, Kian Rosenblum, Arushi Katyal, Tianli Qu, Grady Ma, Rishi Sonthalia</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03082">https://arxiv.org/abs/2602.03082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03082">https://arxiv.org/pdf/2602.03082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03082]] Geometry-Preserving Neural Architectures on Manifolds with Boundary(https://arxiv.org/abs/2602.03082)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Preserving geometric structure is important in learning. We propose a unified class of geometry-aware architectures that interleave geometric updates between layers, where both projection layers and intrinsic exponential map updates arise as discretizations of projected dynamical systems on manifolds (with or without boundary). Within this framework, we establish universal approximation results for constrained neural ODEs. We also analyze architectures that enforce geometry only at the output, proving a separate universal approximation property that enables direct comparison to interleaved designs. When the constraint set is unknown, we learn projections via small-time heat-kernel limits, showing diffusion/flow-matching can be used as data-based projections. Experiments on dynamics over S^2 and SO(3), and diffusion on S^{d-1}-valued features demonstrate exact feasibility for analytic updates and strong performance for learned projections</li>
</ul>

<h3>Title: AERO: Autonomous Evolutionary Reasoning Optimization via Endogenous Dual-Loop Feedback</h3>
<ul>
<li><strong>Authors: </strong>Zhitao Gao, Jie Ma, Xuhong Li, Pengyu Li, Ning Qu, Yaqiang Wu, Hui Liu, Jun Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03084">https://arxiv.org/abs/2602.03084</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03084">https://arxiv.org/pdf/2602.03084</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03084]] AERO: Autonomous Evolutionary Reasoning Optimization via Endogenous Dual-Loop Feedback(https://arxiv.org/abs/2602.03084)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved significant success in complex reasoning but remain bottlenecked by reliance on expert-annotated data and external verifiers. While existing self-evolution paradigms aim to bypass these constraints, they often fail to identify the optimal learning zone and risk reinforcing collective hallucinations and incorrect priors through flawed internal feedback. To address these challenges, we propose \underline{A}utonomous \underline{E}volutionary \underline{R}easoning \underline{O}ptimization (AERO), an unsupervised framework that achieves autonomous reasoning evolution by internalizing self-questioning, answering, and criticism within a synergistic dual-loop system. Inspired by the \textit{Zone of Proximal Development (ZPD)} theory, AERO utilizes entropy-based positioning to target the ``solvability gap'' and employs Independent Counterfactual Correction for robust verification. Furthermore, we introduce a Staggered Training Strategy to synchronize capability growth across functional roles and prevent curriculum collapse. Extensive evaluations across nine benchmarks spanning three domains demonstrate that AERO achieves average performance improvements of 4.57\% on Qwen3-4B-Base and 5.10\% on Qwen3-8B-Base, outperforming competitive baselines. Code is available at this https URL.</li>
</ul>

<h3>Title: The Trigger in the Haystack: Extracting and Reconstructing LLM Backdoor Triggers</h3>
<ul>
<li><strong>Authors: </strong>Blake Bullwinkel, Giorgio Severi, Keegan Hines, Amanda Minnich, Ram Shankar Siva Kumar, Yonatan Zunger</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03085">https://arxiv.org/abs/2602.03085</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03085">https://arxiv.org/pdf/2602.03085</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03085]] The Trigger in the Haystack: Extracting and Reconstructing LLM Backdoor Triggers(https://arxiv.org/abs/2602.03085)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, extraction</a></li>
<li><strong>Abstract: </strong>Detecting whether a model has been poisoned is a longstanding problem in AI security. In this work, we present a practical scanner for identifying sleeper agent-style backdoors in causal language models. Our approach relies on two key findings: first, sleeper agents tend to memorize poisoning data, making it possible to leak backdoor examples using memory extraction techniques. Second, poisoned LLMs exhibit distinctive patterns in their output distributions and attention heads when backdoor triggers are present in the input. Guided by these observations, we develop a scalable backdoor scanning methodology that assumes no prior knowledge of the trigger or target behavior and requires only inference operations. Our scanner integrates naturally into broader defensive strategies and does not alter model performance. We show that our method recovers working triggers across multiple backdoor scenarios and a broad range of models and fine-tuning methods.</li>
</ul>

<h3>Title: Neural Predictor-Corrector: Solving Homotopy Problems with Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Jiayao Mai, Bangyan Liao, Zhenjun Zhao, Yingping Zeng, Haoang Li, Javier Civera, Tailin Wu, Yi Zhou, Peidong Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03086">https://arxiv.org/abs/2602.03086</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03086">https://arxiv.org/pdf/2602.03086</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03086]] Neural Predictor-Corrector: Solving Homotopy Problems with Reinforcement Learning(https://arxiv.org/abs/2602.03086)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The Homotopy paradigm, a general principle for solving challenging problems, appears across diverse domains such as robust optimization, global optimization, polynomial root-finding, and sampling. Practical solvers for these problems typically follow a predictor-corrector (PC) structure, but rely on hand-crafted heuristics for step sizes and iteration termination, which are often suboptimal and task-specific. To address this, we unify these problems under a single framework, which enables the design of a general neural solver. Building on this unified view, we propose Neural Predictor-Corrector (NPC), which replaces hand-crafted heuristics with automatically learned policies. NPC formulates policy selection as a sequential decision-making problem and leverages reinforcement learning to automatically discover efficient strategies. To further enhance generalization, we introduce an amortized training mechanism, enabling one-time offline training for a class of problems and efficient online inference on new instances. Experiments on four representative homotopy problems demonstrate that our method generalizes effectively to unseen instances. It consistently outperforms classical and specialized baselines in efficiency while demonstrating superior stability across tasks, highlighting the value of unifying homotopy methods into a single neural framework.</li>
</ul>

<h3>Title: Test-time Recursive Thinking: Self-Improvement without External Feedback</h3>
<ul>
<li><strong>Authors: </strong>Yufan Zhuang, Chandan Singh, Liyuan Liu, Yelong Shen, Dinghuai Zhang, Jingbo Shang, Jianfeng Gao, Weizhu Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03094">https://arxiv.org/abs/2602.03094</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03094">https://arxiv.org/pdf/2602.03094</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03094]] Test-time Recursive Thinking: Self-Improvement without External Feedback(https://arxiv.org/abs/2602.03094)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Modern Large Language Models (LLMs) have shown rapid improvements in reasoning capabilities, driven largely by reinforcement learning (RL) with verifiable rewards. Here, we ask whether these LLMs can self-improve without the need for additional training. We identify two core challenges for such systems: (i) efficiently generating diverse, high-quality candidate solutions, and (ii) reliably selecting correct answers in the absence of ground-truth supervision. To address these challenges, we propose Test-time Recursive Thinking (TRT), an iterative self-improvement framework that conditions generation on rollout-specific strategies, accumulated knowledge, and self-generated verification signals. Using TRT, open-source models reach 100% accuracy on AIME-25/24, and on LiveCodeBench's most difficult problems, closed-source models improve by 10.4-14.8 percentage points without external feedback.</li>
</ul>

<h3>Title: Task--Specificity Score: Measuring How Much Instructions Really Matter for Supervision</h3>
<ul>
<li><strong>Authors: </strong>Pritam Kadasi, Abhishek Upperwal, Mayank Singh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03103">https://arxiv.org/abs/2602.03103</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03103">https://arxiv.org/pdf/2602.03103</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03103]] Task--Specificity Score: Measuring How Much Instructions Really Matter for Supervision(https://arxiv.org/abs/2602.03103)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Instruction tuning is now the default way to train and adapt large language models, but many instruction--input--output pairs are only weakly specified: for a given input, the same output can remain plausible under several alternative instructions. This raises a simple question: \emph{does the instruction uniquely determine the target output?} We propose the \textbf{Task--Specificity Score (TSS)} to quantify how much an instruction matters for predicting its output, by contrasting the true instruction against plausible alternatives for the same input. We further introduce \textbf{TSS++}, which uses hard alternatives and a small quality term to mitigate easy-negative effects. Across three instruction datasets (\textsc{Alpaca}, \textsc{Dolly-15k}, \textsc{NI-20}) and three open LLMs (Gemma, Llama, Qwen), we show that selecting task-specific examples improves downstream performance under tight token budgets and complements quality-based filters such as perplexity and IFD.</li>
</ul>

<h3>Title: Gromov Wasserstein Optimal Transport for Semantic Correspondences</h3>
<ul>
<li><strong>Authors: </strong>Francis Snelgar, Stephen Gould, Ming Xu, Liang Zheng, Akshay Asthana</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03105">https://arxiv.org/abs/2602.03105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03105">https://arxiv.org/pdf/2602.03105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03105]] Gromov Wasserstein Optimal Transport for Semantic Correspondences(https://arxiv.org/abs/2602.03105)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Establishing correspondences between image pairs is a long studied problem in computer vision. With recent large-scale foundation models showing strong zero-shot performance on downstream tasks including classification and segmentation, there has been interest in using the internal feature maps of these models for the semantic correspondence task. Recent works observe that features from DINOv2 and Stable Diffusion (SD) are complementary, the former producing accurate but sparse correspondences, while the latter produces spatially consistent correspondences. As a result, current state-of-the-art methods for semantic correspondence involve combining features from both models in an ensemble. While the performance of these methods is impressive, they are computationally expensive, requiring evaluating feature maps from large-scale foundation models. In this work we take a different approach, instead replacing SD features with a superior matching algorithm which is imbued with the desirable spatial consistency property. Specifically, we replace the standard nearest neighbours matching with an optimal transport algorithm that includes a Gromov Wasserstein spatial smoothness prior. We show that we can significantly boost the performance of the DINOv2 baseline, and be competitive and sometimes surpassing state-of-the-art methods using Stable Diffusion features, while being 5--10x more efficient. We make code available at this https URL .</li>
</ul>

<h3>Title: The Mask of Civility: Benchmarking Chinese Mock Politeness Comprehension in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yitong Zhang, Yuhan Xiang, Mingxuan Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03107">https://arxiv.org/abs/2602.03107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03107">https://arxiv.org/pdf/2602.03107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03107]] The Mask of Civility: Benchmarking Chinese Mock Politeness Comprehension in Large Language Models(https://arxiv.org/abs/2602.03107)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>From a pragmatic perspective, this study systematically evaluates the differences in performance among representative large language models (LLMs) in recognizing politeness, impoliteness, and mock politeness phenomena in Chinese. Addressing the existing gaps in pragmatic comprehension, the research adopts the frameworks of Rapport Management Theory and the Model of Mock Politeness to construct a three-category dataset combining authentic and simulated Chinese discourse. Six representative models, including GPT-5.1 and DeepSeek, were selected as test subjects and evaluated under four prompting conditions: zero-shot, few-shot, knowledge-enhanced, and hybrid strategies. This study serves as a meaningful attempt within the paradigm of ``Great Linguistics,'' offering a novel approach to applying pragmatic theory in the age of technological transformation. It also responds to the contemporary question of how technology and the humanities may coexist, representing an interdisciplinary endeavor that bridges linguistic technology and humanistic reflection.</li>
</ul>

<h3>Title: ChemPro: A Progressive Chemistry Benchmark for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Aaditya Baranwal, Shruti Vyas</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03108">https://arxiv.org/abs/2602.03108</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03108">https://arxiv.org/pdf/2602.03108</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03108]] ChemPro: A Progressive Chemistry Benchmark for Large Language Models(https://arxiv.org/abs/2602.03108)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>We introduce ChemPro, a progressive benchmark with 4100 natural language question-answer pairs in Chemistry, across 4 coherent sections of difficulty designed to assess the proficiency of Large Language Models (LLMs) in a broad spectrum of general chemistry topics. We include Multiple Choice Questions and Numerical Questions spread across fine-grained information recall, long-horizon reasoning, multi-concept questions, problem-solving with nuanced articulation, and straightforward questions in a balanced ratio, effectively covering Bio-Chemistry, Inorganic-Chemistry, Organic-Chemistry and Physical-Chemistry. ChemPro is carefully designed analogous to a student's academic evaluation for basic to high-school chemistry. A gradual increase in the question difficulty rigorously tests the ability of LLMs to progress from solving basic problems to solving more sophisticated challenges. We evaluate 45+7 state-of-the-art LLMs, spanning both open-source and proprietary variants, and our analysis reveals that while LLMs perform well on basic chemistry questions, their accuracy declines with different types and levels of complexity. These findings highlight the critical limitations of LLMs in general scientific reasoning and understanding and point towards understudied dimensions of difficulty, emphasizing the need for more robust methodologies to improve LLMs.</li>
</ul>

<h3>Title: AgentDyn: A Dynamic Open-Ended Benchmark for Evaluating Prompt Injection Attacks of Real-World Agent Security System</h3>
<ul>
<li><strong>Authors: </strong>Hao Li, Ruoyao Wen, Shanghao Shi, Ning Zhang, Chaowei Xiao</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03117">https://arxiv.org/abs/2602.03117</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03117">https://arxiv.org/pdf/2602.03117</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03117]] AgentDyn: A Dynamic Open-Ended Benchmark for Evaluating Prompt Injection Attacks of Real-World Agent Security System(https://arxiv.org/abs/2602.03117)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense, attack</a></li>
<li><strong>Abstract: </strong>AI agents that autonomously interact with external tools and environments show great promise across real-world applications. However, the external data which agent consumes also leads to the risk of indirect prompt injection attacks, where malicious instructions embedded in third-party content hijack agent behavior. Guided by benchmarks, such as AgentDojo, there has been significant amount of progress in developing defense against the said attacks. As the technology continues to mature, and that agents are increasingly being relied upon for more complex tasks, there is increasing pressing need to also evolve the benchmark to reflect threat landscape faced by emerging agentic systems. In this work, we reveal three fundamental flaws in current benchmarks and push the frontier along these dimensions: (i) lack of dynamic open-ended tasks, (ii) lack of helpful instructions, and (iii) simplistic user tasks. To bridge this gap, we introduce AgentDyn, a manually designed benchmark featuring 60 challenging open-ended tasks and 560 injection test cases across Shopping, GitHub, and Daily Life. Unlike prior static benchmarks, AgentDyn requires dynamic planning and incorporates helpful third-party instructions. Our evaluation of ten state-of-the-art defenses suggests that almost all existing defenses are either not secure enough or suffer from significant over-defense, revealing that existing defenses are still far from real-world deployment. Our benchmark is available at this https URL.</li>
</ul>

<h3>Title: Quantized Evolution Strategies: High-precision Fine-tuning of Quantized LLMs at Low-precision Cost</h3>
<ul>
<li><strong>Authors: </strong>Yinggan Xu, Risto Miikkulainen, Xin Qiu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03120">https://arxiv.org/abs/2602.03120</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03120">https://arxiv.org/pdf/2602.03120</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03120]] Quantized Evolution Strategies: High-precision Fine-tuning of Quantized LLMs at Low-precision Cost(https://arxiv.org/abs/2602.03120)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Post-Training Quantization (PTQ) is essential for deploying Large Language Models (LLMs) on memory-constrained devices, yet it renders models static and difficult to fine-tune. Standard fine-tuning paradigms, including Reinforcement Learning (RL), fundamentally rely on backpropagation and high-precision weights to compute gradients. Thus they cannot be used on quantized models, where the parameter space is discrete and non-differentiable. While Evolution Strategies (ES) offer a backpropagation-free alternative, optimization of the quantized parameters can still fail due to vanishing or inaccurate gradient. This paper introduces Quantized Evolution Strategies (QES), an optimization paradigm that performs full-parameter fine-tuning directly in the quantized space. QES is based on two innovations: (1) it integrates accumulated error feedback to preserve high-precision gradient signals, and (2) it utilizes a stateless seed replay to reduce memory usage to low-precision inference levels. QES significantly outperforms the state-of-the-art zeroth-order fine-tuning method on arithmetic reasoning tasks, making direct fine-tuning for quantized models possible. It therefore opens up the possibility for scaling up LLMs entirely in the quantized space. The source code is available at this https URL .</li>
</ul>

<h3>Title: Beyond Cropping and Rotation: Automated Evolution of Powerful Task-Specific Augmentations with Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Judah Goldfeder, Shreyes Kaliyur, Vaibhav Sourirajan, Patrick Minwan Puma, Philippe Martin Wyder, Yuhang Hu, Jiong Lin, Hod Lipson</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03123">https://arxiv.org/abs/2602.03123</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03123">https://arxiv.org/pdf/2602.03123</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03123]] Beyond Cropping and Rotation: Automated Evolution of Powerful Task-Specific Augmentations with Generative Models(https://arxiv.org/abs/2602.03123)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Data augmentation has long been a cornerstone for reducing overfitting in vision models, with methods like AutoAugment automating the design of task-specific augmentations. Recent advances in generative models, such as conditional diffusion and few-shot NeRFs, offer a new paradigm for data augmentation by synthesizing data with significantly greater diversity and realism. However, unlike traditional augmentations like cropping or rotation, these methods introduce substantial changes that enhance robustness but also risk degrading performance if the augmentations are poorly matched to the task. In this work, we present EvoAug, an automated augmentation learning pipeline, which leverages these generative models alongside an efficient evolutionary algorithm to learn optimal task-specific augmentations. Our pipeline introduces a novel approach to image augmentation that learns stochastic augmentation trees that hierarchically compose augmentations, enabling more structured and adaptive transformations. We demonstrate strong performance across fine-grained classification and few-shot learning tasks. Notably, our pipeline discovers augmentations that align with domain knowledge, even in low-data settings. These results highlight the potential of learned generative augmentations, unlocking new possibilities for robust model training.</li>
</ul>

<h3>Title: Feature, Alignment, and Supervision in Category Learning: A Comparative Approach with Children and Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Fanxiao Wani Qiu, Oscar Leong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03124">https://arxiv.org/abs/2602.03124</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03124">https://arxiv.org/pdf/2602.03124</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03124]] Feature, Alignment, and Supervision in Category Learning: A Comparative Approach with Children and Neural Networks(https://arxiv.org/abs/2602.03124)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Understanding how humans and machines learn from sparse data is central to cognitive science and machine learning. Using a species-fair design, we compare children and convolutional neural networks (CNNs) in a few-shot semi-supervised category learning task. Both learners are exposed to novel object categories under identical conditions. Learners receive mixtures of labeled and unlabeled exemplars while we vary supervision (1/3/6 labels), target feature (size, shape, pattern), and perceptual alignment (high/low). We find that children generalize rapidly from minimal labels but show strong feature-specific biases and sensitivity to alignment. CNNs show a different interaction profile: added supervision improves performance, but both alignment and feature structure moderate the impact additional supervision has on learning. These results show that human-model comparisons must be drawn under the right conditions, emphasizing interactions among supervision, feature structure, and alignment rather than overall accuracy.</li>
</ul>

<h3>Title: Flexible Geometric Guidance for Probabilistic Human Pose Estimation with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Francis Snelgar, Ming Xu, Stephen Gould, Liang Zheng, Akshay Asthana</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03126">https://arxiv.org/abs/2602.03126</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03126">https://arxiv.org/pdf/2602.03126</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03126]] Flexible Geometric Guidance for Probabilistic Human Pose Estimation with Diffusion Models(https://arxiv.org/abs/2602.03126)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>3D human pose estimation from 2D images is a challenging problem due to depth ambiguity and occlusion. Because of these challenges the task is underdetermined, where there exists multiple -- possibly infinite -- poses that are plausible given the image. Despite this, many prior works assume the existence of a deterministic mapping and estimate a single pose given an image. Furthermore, methods based on machine learning require a large amount of paired 2D-3D data to train and suffer from generalization issues to unseen scenarios. To address both of these issues, we propose a framework for pose estimation using diffusion models, which enables sampling from a probability distribution over plausible poses which are consistent with a 2D image. Our approach falls under the guidance framework for conditional generation, and guides samples from an unconditional diffusion model, trained only on 3D data, using the gradients of the heatmaps from a 2D keypoint detector. We evaluate our method on the Human 3.6M dataset under best-of-$m$ multiple hypothesis evaluation, showing state-of-the-art performance among methods which do not require paired 2D-3D data for training. We additionally evaluate the generalization ability using the MPI-INF-3DHP and 3DPW datasets and demonstrate competitive performance. Finally, we demonstrate the flexibility of our framework by using it for novel tasks including pose generation and pose completion, without the need to train bespoke conditional models. We make code available at this https URL .</li>
</ul>

<h3>Title: Cyber Insurance, Audit, and Policy: Review, Analysis and Recommendations</h3>
<ul>
<li><strong>Authors: </strong>Danielle Jean Hanson, Jeremy Straub</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03127">https://arxiv.org/abs/2602.03127</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03127">https://arxiv.org/pdf/2602.03127</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03127]] Cyber Insurance, Audit, and Policy: Review, Analysis and Recommendations(https://arxiv.org/abs/2602.03127)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack</a></li>
<li><strong>Abstract: </strong>Cyber insurance, which protects insured organizations against financial losses from cyberattacks and data breaches, can be difficult and expensive to obtain for many organizations. These difficulties stem from insurers difficulty in understanding and accurately assessing the risks that they are undertaking. Cybersecurity audits, which are already implemented in many organizations for compliance and other purposes, present a potential solution to this challenge. This paper provides a structured review and analysis of prior work in this area, analysis of the challenges and potential benefits that cyber audits provide and recommendations for the use of cyber audits to reduce cyber insurance costs and improve its availability.</li>
</ul>

<h3>Title: Contrastive Concept-Tree Search for LLM-Assisted Algorithm Discovery</h3>
<ul>
<li><strong>Authors: </strong>Timothee Leleu, Sudeera Gunathilaka, Federico Ghimenti, Surya Ganguli</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03132">https://arxiv.org/abs/2602.03132</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03132">https://arxiv.org/pdf/2602.03132</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03132]] Contrastive Concept-Tree Search for LLM-Assisted Algorithm Discovery(https://arxiv.org/abs/2602.03132)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language Model (LLM)-assisted algorithm discovery is an iterative, black-box optimization process over programs to approximatively solve a target task, where an LLM proposes candidate programs and an external evaluator provides task feedback. Despite intense recent research on the topic and promising results, how can the LLM internal representation of the space of possible programs be maximally exploited to improve performance is an open question. Here, we introduce Contrastive Concept-Tree Search (CCTS), which extracts a hierarchical concept representation from the generated programs and learns a contrastive concept model that guides parent selection. By reweighting parents using a likelihood-ratio score between high- and low-performing solutions, CCTS biases search toward useful concept combinations and away from misleading ones, providing guidance through an explicit concept hierarchy rather than the algorithm lineage constructed by the LLM. We show that CCTS improves search efficiency over fitness-based baselines and produces interpretable, task-specific concept trees across a benchmark of open Erdős-type combinatorics problems. Our analysis indicates that the gains are driven largely by learning which concepts to avoid. We further validate these findings in a controlled synthetic algorithm-discovery environment, which reproduces qualitatively the search dynamics observed with the LLMs.</li>
</ul>

<h3>Title: FSOD-VFM: Few-Shot Object Detection with Vision Foundation Models and Graph Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Chen-Bin Feng, Youyang Sha, Longfei Liu, Yongjun Yu, Chi Man Vong, Xuanlong Yu, Xi Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03137">https://arxiv.org/abs/2602.03137</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03137">https://arxiv.org/pdf/2602.03137</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03137]] FSOD-VFM: Few-Shot Object Detection with Vision Foundation Models and Graph Diffusion(https://arxiv.org/abs/2602.03137)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, diffusion</a></li>
<li><strong>Abstract: </strong>In this paper, we present FSOD-VFM: Few-Shot Object Detectors with Vision Foundation Models, a framework that leverages vision foundation models to tackle the challenge of few-shot object detection. FSOD-VFM integrates three key components: a universal proposal network (UPN) for category-agnostic bounding box generation, SAM2 for accurate mask extraction, and DINOv2 features for efficient adaptation to new object categories. Despite the strong generalization capabilities of foundation models, the bounding boxes generated by UPN often suffer from overfragmentation, covering only partial object regions and leading to numerous small, false-positive proposals rather than accurate, complete object detections. To address this issue, we introduce a novel graph-based confidence reweighting method. In our approach, predicted bounding boxes are modeled as nodes in a directed graph, with graph diffusion operations applied to propagate confidence scores across the network. This reweighting process refines the scores of proposals, assigning higher confidence to whole objects and lower confidence to local, fragmented parts. This strategy improves detection granularity and effectively reduces the occurrence of false-positive bounding box proposals. Through extensive experiments on Pascal-5$^i$, COCO-20$^i$, and CD-FSOD datasets, we demonstrate that our method substantially outperforms existing approaches, achieving superior performance without requiring additional training. Notably, on the challenging CD-FSOD dataset, which spans multiple datasets and domains, our FSOD-VFM achieves 31.6 AP in the 10-shot setting, substantially outperforming previous training-free methods that reach only 21.4 AP. Code is available at: this https URL.</li>
</ul>

<h3>Title: SATORIS-N: Spectral Analysis based Traffic Observation Recovery via Informed Subspaces and Nuclear-norm minimization</h3>
<ul>
<li><strong>Authors: </strong>Sampad Mohanty, Bhaskar Krishnamachari</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03138">https://arxiv.org/abs/2602.03138</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03138">https://arxiv.org/pdf/2602.03138</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03138]] SATORIS-N: Spectral Analysis based Traffic Observation Recovery via Informed Subspaces and Nuclear-norm minimization(https://arxiv.org/abs/2602.03138)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Traffic-density matrices from different days exhibit both low rank and stable correlations in their singular-vector subspaces. Leveraging this, we introduce SATORIS-N, a framework for imputing partially observed traffic-density by informed subspace priors from neighboring days. Our contribution is a subspace-aware semidefinite programming (SDP)} formulation of nuclear norm that explicitly informs the reconstruction with prior singular-subspace information. This convex formulation jointly enforces low rank and subspace alignment, providing a single global optimum and substantially improving accuracy under medium and high occlusion. We also study a lightweight implicit subspace-alignment} strategy in which matrices from consecutive days are concatenated to encourage alignment of spatial or temporal singular directions. Although this heuristic offers modest gains when missing rates are low, the explicit SDP approach is markedly more robust when large fractions of entries are missing. Across two real-world datasets (Beijing and Shanghai), SATORIS-N consistently outperforms standard matrix-completion methods such as SoftImpute, IterativeSVD, statistical, and even deep learning baselines at high occlusion levels. The framework generalizes to other spatiotemporal settings in which singular subspaces evolve slowly over time. In the context of intelligent vehicles and vehicle-to-everything (V2X) systems, accurate traffic-density reconstruction enables critical applications including cooperative perception, predictive routing, and vehicle-to-infrastructure (V2I) communication optimization. When infrastructure sensors or vehicle-reported observations are incomplete - due to communication dropouts, sensor occlusions, or sparse connected vehicle penetration-reliable imputation becomes essential for safe and efficient autonomous navigation.</li>
</ul>

<h3>Title: Self-Hinting Language Models Enhance Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Baohao Liao, Hanze Dong, Xinxing Xu, Christof Monz, Jiang Bian</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03143">https://arxiv.org/abs/2602.03143</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03143">https://arxiv.org/pdf/2602.03143</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03143]] Self-Hinting Language Models Enhance Reinforcement Learning(https://arxiv.org/abs/2602.03143)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Group Relative Policy Optimization (GRPO) has recently emerged as a practical recipe for aligning large language models with verifiable objectives. However, under sparse terminal rewards, GRPO often stalls because rollouts within a group frequently receive identical rewards, causing relative advantages to collapse and updates to vanish. We propose self-hint aligned GRPO with privileged supervision (SAGE), an on-policy reinforcement learning framework that injects privileged hints during training to reshape the rollout distribution under the same terminal verifier reward. For each prompt $x$, the model samples a compact hint $h$ (e.g., a plan or decomposition) and then generates a solution $\tau$ conditioned on $(x,h)$. Crucially, the task reward $R(x,\tau)$ is unchanged; hints only increase within-group outcome diversity under finite sampling, preventing GRPO advantages from collapsing under sparse rewards. At test time, we set $h=\varnothing$ and deploy the no-hint policy without any privileged information. Moreover, sampling diverse self-hints serves as an adaptive curriculum that tracks the learner's bottlenecks more effectively than fixed hints from an initial policy or a stronger external model. Experiments over 6 benchmarks with 3 LLMs show that SAGE consistently outperforms GRPO, on average +2.0 on Llama-3.2-3B-Instruct, +1.2 on Qwen2.5-7B-Instruct and +1.3 on Qwen3-4B-Instruct. The code is available at this https URL.</li>
</ul>

<h3>Title: What Makes a Good Example? Modeling Exemplar Selection with Neural Network Representations</h3>
<ul>
<li><strong>Authors: </strong>Fanxiao Wani Qiu, Oscar Leong, Alexander LaTourrette</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03144">https://arxiv.org/abs/2602.03144</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03144">https://arxiv.org/pdf/2602.03144</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03144]] What Makes a Good Example? Modeling Exemplar Selection with Neural Network Representations(https://arxiv.org/abs/2602.03144)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Teaching requires distilling a rich category distribution into a small set of informative exemplars. Although prior work shows that humans consider both representativeness and diversity when teaching, the computational principles underlying these tradeoffs remain unclear. We address this gap by modeling human exemplar selection using neural network feature representations and principled subset selection criteria. Novel visual categories were embedded along a one-dimensional morph continuum using pretrained vision models, and selection strategies varied in their emphasis on prototypicality, joint representativeness, and diversity. Adult participants selected one to three exemplars to teach a learner. Model-human comparisons revealed that strategies based on joint representativeness, or its combination with diversity, best captured human judgments, whereas purely prototypical or diversity-based strategies performed worse. Moreover, transformer-based representations consistently aligned more closely with human behavior than convolutional networks. These results highlight the potential utility of dataset distillation methods in machine learning as computational models for teaching.</li>
</ul>

<h3>Title: FASA: Frequency-aware Sparse Attention</h3>
<ul>
<li><strong>Authors: </strong>Yifei Wang, Yueqi Wang, Zhenrui Yue, Huimin Zeng, Yong Wang, Ismini Lourentzou, Zhengzhong Tu, Xiangxiang Chu, Julian McAuley</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03152">https://arxiv.org/abs/2602.03152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03152">https://arxiv.org/pdf/2602.03152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03152]] FASA: Frequency-aware Sparse Attention(https://arxiv.org/abs/2602.03152)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The deployment of Large Language Models (LLMs) faces a critical bottleneck when handling lengthy inputs: the prohibitive memory footprint of the Key Value (KV) cache. To address this bottleneck, the token pruning paradigm leverages attention sparsity to selectively retain a small, critical subset of tokens. However, existing approaches fall short, with static methods risking irreversible information loss and dynamic strategies employing heuristics that insufficiently capture the query-dependent nature of token importance. We propose FASA, a novel framework that achieves query-aware token eviction by dynamically predicting token importance. FASA stems from a novel insight into RoPE: the discovery of functional sparsity at the frequency-chunk (FC) level. Our key finding is that a small, identifiable subset of "dominant" FCs consistently exhibits high contextual agreement with the full attention head. This provides a robust and computationally free proxy for identifying salient tokens. %making them a powerful and efficient proxy for token importance. Building on this insight, FASA first identifies a critical set of tokens using dominant FCs, and then performs focused attention computation solely on this pruned subset. % Since accessing only a small fraction of the KV cache, FASA drastically lowers memory bandwidth requirements and computational cost. Across a spectrum of long-context tasks, from sequence modeling to complex CoT reasoning, FASA consistently outperforms all token-eviction baselines and achieves near-oracle accuracy, demonstrating remarkable robustness even under constraint budgets. Notably, on LongBench-V1, FASA reaches nearly 100\% of full-KV performance when only keeping 256 tokens, and achieves 2.56$\times$ speedup using just 18.9\% of the cache on AIME24.</li>
</ul>

<h3>Title: Fully Kolmogorov-Arnold Deep Model in Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Xingyu Qiu, Xinghua Ma, Dong Liang, Gongning Luo, Wei Wang, Kuanquan Wang, Shuo Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03156">https://arxiv.org/abs/2602.03156</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03156">https://arxiv.org/pdf/2602.03156</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03156]] Fully Kolmogorov-Arnold Deep Model in Medical Image Segmentation(https://arxiv.org/abs/2602.03156)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Deeply stacked KANs are practically impossible due to high training difficulties and substantial memory requirements. Consequently, existing studies can only incorporate few KAN layers, hindering the comprehensive exploration of KANs. This study overcomes these limitations and introduces the first fully KA-based deep model, demonstrating that KA-based layers can entirely replace traditional architectures in deep learning and achieve superior learning capacity. Specifically, (1) the proposed Share-activation KAN (SaKAN) reformulates Sprecher's variant of Kolmogorov-Arnold representation theorem, which achieves better optimization due to its simplified parameterization and denser training samples, to ease training difficulty, (2) this paper indicates that spline gradients contribute negligibly to training while consuming huge GPU memory, thus proposes the Grad-Free Spline to significantly reduce memory usage and computational overhead. (3) Building on these two innovations, our ALL U-KAN is the first representative implementation of fully KA-based deep model, where the proposed KA and KAonv layers completely replace FC and Conv layers. Extensive evaluations on three medical image segmentation tasks confirm the superiority of the full KA-based architecture compared to partial KA-based and traditional architectures, achieving all higher segmentation accuracy. Compared to directly deeply stacked KAN, ALL U-KAN achieves 10 times reduction in parameter count and reduces memory consumption by more than 20 times, unlocking the new explorations into deep KAN architectures.</li>
</ul>

<h3>Title: Adversarial construction as a potential solution to the experiment design problem in large task spaces</h3>
<ul>
<li><strong>Authors: </strong>Prakhar Godara, Frederick Callaway, Marcelo G. Mattar</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03172">https://arxiv.org/abs/2602.03172</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03172">https://arxiv.org/pdf/2602.03172</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03172]] Adversarial construction as a potential solution to the experiment design problem in large task spaces(https://arxiv.org/abs/2602.03172)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Despite decades of work, we still lack a robust, task-general theory of human behavior even in the simplest domains. In this paper we tackle the generality problem head-on, by aiming to develop a unified model for all tasks embedded in a task-space. In particular we consider the space of binary sequence prediction tasks where the observations are generated by the space parameterized by hidden Markov models (HMM). As the space of tasks is large, experimental exploration of the entire space is infeasible. To solve this problem we propose the adversarial construction approach, which helps identify tasks that are most likely to elicit a qualitatively novel behavior. Our results suggest that adversarial construction significantly outperforms random sampling of environments and therefore could be used as a proxy for optimal experimental design in high-dimensional task spaces.</li>
</ul>

<h3>Title: LSGQuant: Layer-Sensitivity Guided Quantization for One-Step Diffusion Real-World Video Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Tianxing Wu, Zheng Chen, Cirou Xu, Bowen Chai, Yong Guo, Yutong Liu, Linghe Kong, Yulun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03182">https://arxiv.org/abs/2602.03182</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03182">https://arxiv.org/pdf/2602.03182</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03182]] LSGQuant: Layer-Sensitivity Guided Quantization for One-Step Diffusion Real-World Video Super-Resolution(https://arxiv.org/abs/2602.03182)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>One-Step Diffusion Models have demonstrated promising capability and fast inference in video super-resolution (VSR) for real-world. Nevertheless, the substantial model size and high computational cost of Diffusion Transformers (DiTs) limit downstream applications. While low-bit quantization is a common approach for model compression, the effectiveness of quantized models is challenged by the high dynamic range of input latent and diverse layer behaviors. To deal with these challenges, we introduce LSGQuant, a layer-sensitivity guided quantizing approach for one-step diffusion-based real-world VSR. Our method incorporates a Dynamic Range Adaptive Quantizer (DRAQ) to fit video token activations. Furthermore, we estimate layer sensitivity and implement a Variance-Oriented Layer Training Strategy (VOLTS) by analyzing layer-wise statistics in calibration. We also introduce Quantization-Aware Optimization (QAO) to jointly refine the quantized branch and a retained high-precision branch. Extensive experiments demonstrate that our method has nearly performance to origin model with full-precision and significantly exceeds existing quantization techniques. Code is available at: this https URL.</li>
</ul>

<h3>Title: Privasis: Synthesizing the Largest "Public" Private Dataset from Scratch</h3>
<ul>
<li><strong>Authors: </strong>Hyunwoo Kim, Niloofar Mireshghallah, Michael Duan, Rui Xin, Shuyue Stella Li, Jaehun Jung, David Acuna, Qi Pang, Hanshen Xiao, G. Edward Suh, Sewoong Oh, Yulia Tsvetkov, Pang Wei Koh, Yejin Choi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03183">https://arxiv.org/abs/2602.03183</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03183">https://arxiv.org/pdf/2602.03183</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03183]] Privasis: Synthesizing the Largest "Public" Private Dataset from Scratch(https://arxiv.org/abs/2602.03183)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Research involving privacy-sensitive data has always been constrained by data scarcity, standing in sharp contrast to other areas that have benefited from data scaling. This challenge is becoming increasingly urgent as modern AI agents--such as OpenClaw and Gemini Agent--are granted persistent access to highly sensitive personal information. To tackle this longstanding bottleneck and the rising risks, we present Privasis (i.e., privacy oasis), the first million-scale fully synthetic dataset entirely built from scratch--an expansive reservoir of texts with rich and diverse private information--designed to broaden and accelerate research in areas where processing sensitive social data is inevitable. Compared to existing datasets, Privasis, comprising 1.4 million records, offers orders-of-magnitude larger scale with quality, and far greater diversity across various document types, including medical history, legal documents, financial records, calendars, and text messages with a total of 55.1 million annotated attributes such as ethnicity, date of birth, workplace, etc. We leverage Privasis to construct a parallel corpus for text sanitization with our pipeline that decomposes texts and applies targeted sanitization. Our compact sanitization models (<=4B) trained on this dataset outperform state-of-the-art large language models, such as GPT-5 and Qwen-3 235B. We plan to release data, models, and code to accelerate future research on privacy-sensitive domains and agents.</li>
</ul>

<h3>Title: DynSplit-KV: Dynamic Semantic Splitting for KVCache Compression in Efficient Long-Context LLM Inference</h3>
<ul>
<li><strong>Authors: </strong>Jiancai Ye, Jun Liu, Qingchen Li, Tianlang Zhao, Hanbin Zhang, Jiayi Pan, Ningyi Xu, Guohao Dai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03184">https://arxiv.org/abs/2602.03184</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03184">https://arxiv.org/pdf/2602.03184</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03184]] DynSplit-KV: Dynamic Semantic Splitting for KVCache Compression in Efficient Long-Context LLM Inference(https://arxiv.org/abs/2602.03184)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Although Key-Value (KV) Cache is essential for efficient large language models (LLMs) inference, its growing memory footprint in long-context scenarios poses a significant bottleneck, making KVCache compression crucial. Current compression methods rely on rigid splitting strategies, such as fixed intervals or pre-defined delimiters. We observe that rigid splitting suffers from significant accuracy degradation (ranging from 5.5% to 55.1%) across different scenarios, owing to the scenario-dependent nature of the semantic boundaries. This highlights the necessity of dynamic semantic splitting to match semantics. To achieve this, we face two challenges. (1) Improper delimiter selection misaligns semantics with the KVCache, resulting in 28.6% accuracy loss. (2) Variable-length blocks after splitting introduce over 73.1% additional inference overhead. To address the above challenges, we propose DynSplit-KV, a KVCache compression method that dynamically identifies delimiters for splitting. We propose: (1) a dynamic importance-aware delimiter selection strategy, improving accuracy by 49.9%. (2) A uniform mapping strategy that transforms variable-length semantic blocks into a fixed-length format, reducing inference overhead by 4.9x. Experiments show that DynSplit-KV achieves the highest accuracy, 2.2x speedup compared with FlashAttention and 2.6x peak memory reduction in long-context scenarios.</li>
</ul>

<h3>Title: Prompt Augmentation Scales up GRPO Training on Mathematical Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Wenquan Lu, Hai Huang, Randall Balestriero</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03190">https://arxiv.org/abs/2602.03190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03190">https://arxiv.org/pdf/2602.03190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03190]] Prompt Augmentation Scales up GRPO Training on Mathematical Reasoning(https://arxiv.org/abs/2602.03190)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning algorithms such as group-relative policy optimization (GRPO) have demonstrated strong potential for improving the mathematical reasoning capabilities of large language models. However, prior work has consistently observed an entropy collapse phenomenon during reinforcement post-training, characterized by a monotonic decrease in policy entropy that ultimately leads to training instability and collapse. As a result, most existing approaches restrict training to short horizons (typically 5-20 epochs), limiting sustained exploration and hindering further policy improvement. In addition, nearly all prior work relies on a single, fixed reasoning prompt or template during training. In this work, we introduce prompt augmentation, a training strategy that instructs the model to generate reasoning traces under diverse templates and formats, thereby increasing rollout diversity. We show that, without a KL regularization term, prompt augmentation enables stable scaling of training duration under a fixed dataset and allows the model to tolerate low-entropy regimes without premature collapse. Empirically, a Qwen2.5-Math-1.5B model trained with prompt augmentation on the MATH Level 3-5 dataset achieves state-of-the-art performance, reaching 44.5 per-benchmark accuracy and 51.3 per-question accuracy on standard mathematical reasoning benchmarks, including AIME24, AMC, MATH500, Minerva, and OlympiadBench. The code and model checkpoints are available at this https URL.</li>
</ul>

<h3>Title: Reinforcement Learning with Promising Tokens for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jing-Cheng Pang, Liang Lu, Xian Tang, Kun Jiang, Sijie Wu, Kai Zhang, Xubin Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03195">https://arxiv.org/abs/2602.03195</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03195">https://arxiv.org/pdf/2602.03195</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03195]] Reinforcement Learning with Promising Tokens for Large Language Models(https://arxiv.org/abs/2602.03195)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) has emerged as a key paradigm for aligning and optimizing large language models (LLMs). Standard approaches treat the LLM as the policy and apply RL directly over the full vocabulary space. However, this formulation includes the massive tail of contextually irrelevant tokens in the action space, which could distract the policy from focusing on decision-making among the truly reasonable tokens. In this work, we verify that valid reasoning paths could inherently concentrate within a low-rank subspace. Based on this insight, we introduce Reinforcement Learning with Promising Tokens (RLPT), a framework that mitigates the action space issue by decoupling strategic decision-making from token generation. Specifically, RLPT leverages the semantic priors of the base model to identify a dynamic set of \emph{promising tokens} and constrains policy optimization exclusively to this refined subset via masking. Theoretical analysis and empirical results demonstrate that RLPT effectively reduces gradient variance, stabilizes the training process, and improves sample efficiency. Experiment results on math, coding, and telecom reasoning show that RLPT outperforms standard RL baselines and integrates effectively across various model sizes (4B and 8B) and RL algorithms (GRPO and DAPO).</li>
</ul>

<h3>Title: From Single Scan to Sequential Consistency: A New Paradigm for LIDAR Relocalization</h3>
<ul>
<li><strong>Authors: </strong>Minghang Zhu, Zhijing Wang, Yuxin Guo, Wen Li, Sheng Ao, Cheng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03198">https://arxiv.org/abs/2602.03198</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03198">https://arxiv.org/pdf/2602.03198</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03198]] From Single Scan to Sequential Consistency: A New Paradigm for LIDAR Relocalization(https://arxiv.org/abs/2602.03198)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>LiDAR relocalization aims to estimate the global 6-DoF pose of a sensor in the environment. However, existing regression-based approaches are prone to dynamic or ambiguous scenarios, as they either solely rely on single-frame inference or neglect the spatio-temporal consistency across scans. In this paper, we propose TempLoc, a new LiDAR relocalization framework that enhances the robustness of localization by effectively modeling sequential consistency. Specifically, a Global Coordinate Estimation module is first introduced to predict point-wise global coordinates and associated uncertainties for each LiDAR scan. A Prior Coordinate Generation module is then presented to estimate inter-frame point correspondences by the attention mechanism. Lastly, an Uncertainty-Guided Coordinate Fusion module is deployed to integrate both predictions of point correspondence in an end-to-end fashion, yielding a more temporally consistent and accurate global 6-DoF pose. Experimental results on the NCLT and Oxford Robot-Car benchmarks show that our TempLoc outperforms stateof-the-art methods by a large margin, demonstrating the effectiveness of temporal-aware correspondence modeling in LiDAR relocalization. Our code will be released soon.</li>
</ul>

<h3>Title: ForesightKV: Optimizing KV Cache Eviction for Reasoning Models by Learning Long-Term Contribution</h3>
<ul>
<li><strong>Authors: </strong>Zican Dong, Peiyu Liu, Junyi Li, Zhipeng Chen, Han Peng, Shuo Wang, Wayne Xin Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03203">https://arxiv.org/abs/2602.03203</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03203">https://arxiv.org/pdf/2602.03203</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03203]] ForesightKV: Optimizing KV Cache Eviction for Reasoning Models by Learning Long-Term Contribution(https://arxiv.org/abs/2602.03203)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recently, large language models (LLMs) have shown remarkable reasoning abilities by producing long reasoning traces. However, as the sequence length grows, the key-value (KV) cache expands linearly, incurring significant memory and computation costs. Existing KV cache eviction methods mitigate this issue by discarding less important KV pairs, but often fail to capture complex KV dependencies, resulting in performance degradation. To better balance efficiency and performance, we introduce ForesightKV, a training-based KV cache eviction framework that learns to predict which KV pairs to evict during long-text generations. We first design the Golden Eviction algorithm, which identifies the optimal eviction KV pairs at each step using future attention scores. These traces and the scores at each step are then distilled via supervised training with a Pairwise Ranking Loss. Furthermore, we formulate cache eviction as a Markov Decision Process and apply the GRPO algorithm to mitigate the significant language modeling loss increase on low-entropy tokens. Experiments on AIME2024 and AIME2025 benchmarks of three reasoning models demonstrate that ForesightKV consistently outperforms prior methods under only half the cache budget, while benefiting synergistically from both supervised and reinforcement learning approaches.</li>
</ul>

<h3>Title: Spectral Evolution Search: Efficient Inference-Time Scaling for Reward-Aligned Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Jinyan Ye, Zhongjie Duan, Zhiwen Li, Cen Chen, Daoyuan Chen, Yaliang Li, Yingda Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03208">https://arxiv.org/abs/2602.03208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03208">https://arxiv.org/pdf/2602.03208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03208]] Spectral Evolution Search: Efficient Inference-Time Scaling for Reward-Aligned Image Generation(https://arxiv.org/abs/2602.03208)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Inference-time scaling offers a versatile paradigm for aligning visual generative models with downstream objectives without parameter updates. However, existing approaches that optimize the high-dimensional initial noise suffer from severe inefficiency, as many search directions exert negligible influence on the final generation. We show that this inefficiency is closely related to a spectral bias in generative dynamics: model sensitivity to initial perturbations diminishes rapidly as frequency increases. Building on this insight, we propose Spectral Evolution Search (SES), a plug-and-play framework for initial noise optimization that executes gradient-free evolutionary search within a low-frequency subspace. Theoretically, we derive the Spectral Scaling Prediction from perturbation propagation dynamics, which explains the systematic differences in the impact of perturbations across frequencies. Extensive experiments demonstrate that SES significantly advances the Pareto frontier of generation quality versus computational cost, consistently outperforming strong baselines under equivalent budgets.</li>
</ul>

<h3>Title: VIRAL: Visual In-Context Reasoning via Analogy in Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Zhiwen Li, Zhongjie Duan, Jinyan Ye, Cen Chen, Daoyuan Chen, Yaliang Li, Yingda Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03210">https://arxiv.org/abs/2602.03210</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03210">https://arxiv.org/pdf/2602.03210</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03210]] VIRAL: Visual In-Context Reasoning via Analogy in Diffusion Transformers(https://arxiv.org/abs/2602.03210)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Replicating In-Context Learning (ICL) in computer vision remains challenging due to task heterogeneity. We propose \textbf{VIRAL}, a framework that elicits visual reasoning from a pre-trained image editing model by formulating ICL as conditional generation via visual analogy ($x_s : x_t :: x_q : y_q$). We adapt a frozen Diffusion Transformer (DiT) using role-aware multi-image conditioning and introduce a Mixture-of-Experts LoRA to mitigate gradient interference across diverse tasks. Additionally, to bridge the gaps in current visual context datasets, we curate a large-scale dataset spanning perception, restoration, and editing. Experiments demonstrate that VIRAL outperforms existing methods, validating that a unified V-ICL paradigm can handle the majority of visual tasks, including open-domain editing. Our code is available at this https URL</li>
</ul>

<h3>Title: Lookahead Sample Reward Guidance for Test-Time Scaling of Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Yeongmin Kim, Donghyeok Shin, Byeonghu Na, Minsang Park, Richard Lee Kim, Il-Chul Moon</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03211">https://arxiv.org/abs/2602.03211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03211">https://arxiv.org/pdf/2602.03211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03211]] Lookahead Sample Reward Guidance for Test-Time Scaling of Diffusion Models(https://arxiv.org/abs/2602.03211)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated strong generative performance; however, generated samples often fail to fully align with human intent. This paper studies a test-time scaling method that enables sampling from regions with higher human-aligned reward values. Existing gradient guidance methods approximate the expected future reward (EFR) at an intermediate particle $\mathbf{x}_t$ using a Taylor approximation, but this approximation at each time step incurs high computational cost due to sequential neural backpropagation. We show that the EFR at any $\mathbf{x}_t$ can be computed using only marginal samples from a pre-trained diffusion model. The proposed EFR formulation detaches the neural dependency between $\mathbf{x}_t$ and the EFR, enabling closed-form guidance computation without neural backpropagation. To further improve efficiency, we introduce lookahead sampling to collect marginal samples. For final sample generation, we use an accurate solver that guides particles toward high-reward lookahead samples. We refer to this sampling scheme as LiDAR sampling. LiDAR achieves substantial performance improvements using only three samples with a 3-step lookahead solver, exhibiting steep performance gains as lookahead accuracy and sample count increase; notably, it reaches the same GenEval performance as the latest gradient guidance method for SDXL with a 9.5x speedup.</li>
</ul>

<h3>Title: ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask</h3>
<ul>
<li><strong>Authors: </strong>Zhuoran Yang, Yanyong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03213">https://arxiv.org/abs/2602.03213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03213">https://arxiv.org/pdf/2602.03213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03213]] ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask(https://arxiv.org/abs/2602.03213)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Autonomous driving relies on robust models trained on large-scale, high-quality multi-view driving videos. Although world models provide a cost-effective solution for generating realistic driving data, they often suffer from identity drift, where the same object changes its appearance or category across frames due to the absence of instance-level temporal constraints. We introduce ConsisDrive, an identity-preserving driving world model designed to enforce temporal consistency at the instance level. Our framework incorporates two key components: (1) Instance-Masked Attention, which applies instance identity masks and trajectory masks within attention blocks to ensure that visual tokens interact only with their corresponding instance features across spatial and temporal dimensions, thereby preserving object identity consistency; and (2) Instance-Masked Loss, which adaptively emphasizes foreground regions with probabilistic instance masking, reducing background noise while maintaining overall scene fidelity. By integrating these mechanisms, ConsisDrive achieves state-of-the-art driving video generation quality and demonstrates significant improvements in downstream autonomous driving tasks on the nuScenes dataset. Our project page is this https URL.</li>
</ul>

<h3>Title: Token Sparse Attention: Efficient Long-Context Inference with Interleaved Token Selection</h3>
<ul>
<li><strong>Authors: </strong>Dongwon Jo, Beomseok Kang, Jiwon Song, Jae-Joon Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03216">https://arxiv.org/abs/2602.03216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03216">https://arxiv.org/pdf/2602.03216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03216]] Token Sparse Attention: Efficient Long-Context Inference with Interleaved Token Selection(https://arxiv.org/abs/2602.03216)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The quadratic complexity of attention remains the central bottleneck in long-context inference for large language models. Prior acceleration methods either sparsify the attention map with structured patterns or permanently evict tokens at specific layers, which can retain irrelevant tokens or rely on irreversible early decisions despite the layer-/head-wise dynamics of token importance. In this paper, we propose Token Sparse Attention, a lightweight and dynamic token-level sparsification mechanism that compresses per-head $Q$, $K$, $V$ to a reduced token set during attention and then decompresses the output back to the original sequence, enabling token information to be reconsidered in subsequent layers. Furthermore, Token Sparse Attention exposes a new design point at the intersection of token selection and sparse attention. Our approach is fully compatible with dense attention implementations, including Flash Attention, and can be seamlessly composed with existing sparse attention kernels. Experimental results show that Token Sparse Attention consistently improves accuracy-latency trade-off, achieving up to $\times$3.23 attention speedup at 128K context with less than 1% accuracy degradation. These results demonstrate that dynamic and interleaved token-level sparsification is a complementary and effective strategy for scalable long-context inference.</li>
</ul>

<h3>Title: PokeFusion Attention: Enhancing Reference-Free Style-Conditioned Generation</h3>
<ul>
<li><strong>Authors: </strong>Jingbang Tang (James)</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03220">https://arxiv.org/abs/2602.03220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03220">https://arxiv.org/pdf/2602.03220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03220]] PokeFusion Attention: Enhancing Reference-Free Style-Conditioned Generation(https://arxiv.org/abs/2602.03220)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper studies reference-free style-conditioned character generation in text-to-image diffusion models, where high-quality synthesis requires both stable character structure and consistent, fine-grained style expression across diverse prompts. Existing approaches primarily rely on text-only prompting, which is often under-specified for visual style and tends to produce noticeable style drift and geometric inconsistency, or introduce reference-based adapters that depend on external images at inference time, increasing architectural complexity and limiting deployment this http URL propose PokeFusion Attention, a lightweight decoder-level cross-attention mechanism that fuses textual semantics with learned style embeddings directly inside the diffusion decoder. By decoupling text and style conditioning at the attention level, our method enables effective reference-free stylized generation while keeping the pretrained diffusion backbone fully this http URL Attention trains only decoder cross-attention layers together with a compact style projection module, resulting in a parameter-efficient and plug-and-play control component that can be easily integrated into existing diffusion pipelines and transferred across different this http URL on a stylized character generation benchmark (Pokemon-style) demonstrate that our method consistently improves style fidelity, semantic alignment, and character shape consistency compared with representative adapter-based baselines, while maintaining low parameter overhead and inference-time simplicity.</li>
</ul>

<h3>Title: ATACompressor: Adaptive Task-Aware Compression for Efficient Long-Context Processing in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Xuancheng Li, Haitao Li, Yujia Zhou, Qingyao Ai, Yiqun Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03226">https://arxiv.org/abs/2602.03226</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03226">https://arxiv.org/pdf/2602.03226</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03226]] ATACompressor: Adaptive Task-Aware Compression for Efficient Long-Context Processing in LLMs(https://arxiv.org/abs/2602.03226)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Long-context inputs in large language models (LLMs) often suffer from the "lost in the middle" problem, where critical information becomes diluted or ignored due to excessive length. Context compression methods aim to address this by reducing input size, but existing approaches struggle with balancing information preservation and compression efficiency. We propose Adaptive Task-Aware Compressor (ATACompressor), which dynamically adjusts compression based on the specific requirements of the task. ATACompressor employs a selective encoder that compresses only the task-relevant portions of long contexts, ensuring that essential information is preserved while reducing unnecessary content. Its adaptive allocation controller perceives the length of relevant content and adjusts the compression rate accordingly, optimizing resource utilization. We evaluate ATACompressor on three QA datasets: HotpotQA, MSMARCO, and SQUAD-showing that it outperforms existing methods in terms of both compression efficiency and task performance. Our approach provides a scalable solution for long-context processing in LLMs. Furthermore, we perform a range of ablation studies and analysis experiments to gain deeper insights into the key components of ATACompressor.</li>
</ul>

<h3>Title: Spiral RoPE: Rotate Your Rotary Positional Embeddings in the 2D Plane</h3>
<ul>
<li><strong>Authors: </strong>Haoyu Liu, Sucheng Ren, Tingyu Zhu, Peng Wang, Cihang Xie, Alan Yuille, Zeyu Zheng, Feng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03227">https://arxiv.org/abs/2602.03227</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03227">https://arxiv.org/pdf/2602.03227</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03227]] Spiral RoPE: Rotate Your Rotary Positional Embeddings in the 2D Plane(https://arxiv.org/abs/2602.03227)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Rotary Position Embedding (RoPE) is the de facto positional encoding in large language models due to its ability to encode relative positions and support length extrapolation. When adapted to vision transformers, the standard axial formulation decomposes two-dimensional spatial positions into horizontal and vertical components, implicitly restricting positional encoding to axis-aligned directions. We identify this directional constraint as a fundamental limitation of the standard axial 2D RoPE, which hinders the modeling of oblique spatial relationships that naturally exist in natural images. To overcome this limitation, we propose Spiral RoPE, a simple yet effective extension that enables multi-directional positional encoding by partitioning embedding channels into multiple groups associated with uniformly distributed directions. Each group is rotated according to the projection of the patch position onto its corresponding direction, allowing spatial relationships to be encoded beyond the horizontal and vertical axes. Across a wide range of vision tasks including classification, segmentation, and generation, Spiral RoPE consistently improves performance. Qualitative analysis of attention maps further show that Spiral RoPE exhibits more concentrated activations on semantically relevant objects and better respects local object boundaries, highlighting the importance of multi-directional positional encoding in vision transformers.</li>
</ul>

<h3>Title: EventFlash: Towards Efficient MLLMs for Event-Based Vision</h3>
<ul>
<li><strong>Authors: </strong>Shaoyu Liu, Jianing Li, Guanghui Zhao, Yunjian Zhang, Wen Jiang, Ming Li, Xiangyang Ji</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03230">https://arxiv.org/abs/2602.03230</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03230">https://arxiv.org/pdf/2602.03230</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03230]] EventFlash: Towards Efficient MLLMs for Event-Based Vision(https://arxiv.org/abs/2602.03230)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Event-based multimodal large language models (MLLMs) enable robust perception in high-speed and low-light scenarios, addressing key limitations of frame-based MLLMs. However, current event-based MLLMs often rely on dense image-like processing paradigms, overlooking the spatiotemporal sparsity of event streams and resulting in high computational cost. In this paper, we propose EventFlash, a novel and efficient MLLM to explore spatiotemporal token sparsification for reducing data redundancy and accelerating inference. Technically, we build EventMind, a large-scale and scene-diverse dataset with over 500k instruction sets, providing both short and long event stream sequences to support our curriculum training strategy. We then present an adaptive temporal window aggregation module for efficient temporal sampling, which adaptively compresses temporal tokens while retaining key temporal cues. Finally, a sparse density-guided attention module is designed to improve spatial token efficiency by selecting informative regions and suppressing empty or sparse areas. Experimental results show that EventFlash achieves a $12.4\times$ throughput improvement over the baseline (EventFlash-Zero) while maintaining comparable performance. It supports long-range event stream processing with up to 1,000 bins, significantly outperforming the 5-bin limit of EventGPT. We believe EventFlash serves as an efficient foundation model for event-based vision.</li>
</ul>

<h3>Title: Merging Beyond: Streaming LLM Updates via Activation-Guided Rotations</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Yao, Haonan Sheng, Qingsong Lv, Han Wu, Shuqi Liu, Zehua Liu, Zengyan Liu, Jiahui Gao, Haochen Tan, Xiaojin Fu, Haoli Bai, Hing Cheung So, Zhijiang Guo, Linqi Song</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03237">https://arxiv.org/abs/2602.03237</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03237">https://arxiv.org/pdf/2602.03237</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03237]] Merging Beyond: Streaming LLM Updates via Activation-Guided Rotations(https://arxiv.org/abs/2602.03237)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The escalating scale of Large Language Models (LLMs) necessitates efficient adaptation techniques. Model merging has gained prominence for its efficiency and controllability. However, existing merging techniques typically serve as post-hoc refinements or focus on mitigating task interference, often failing to capture the dynamic optimization benefits of supervised fine-tuning (SFT). In this work, we propose Streaming Merging, an innovative model updating paradigm that conceptualizes merging as an iterative optimization process. Central to this paradigm is \textbf{ARM} (\textbf{A}ctivation-guided \textbf{R}otation-aware \textbf{M}erging), a strategy designed to approximate gradient descent dynamics. By treating merging coefficients as learning rates and deriving rotation vectors from activation subspaces, ARM effectively steers parameter updates along data-driven trajectories. Unlike conventional linear interpolation, ARM aligns semantic subspaces to preserve the geometric structure of high-dimensional parameter evolution. Remarkably, ARM requires only early SFT checkpoints and, through iterative merging, surpasses the fully converged SFT model. Experimental results across model scales (1.7B to 14B) and diverse domains (e.g., math, code) demonstrate that ARM can transcend converged checkpoints. Extensive experiments show that ARM provides a scalable and lightweight framework for efficient model adaptation.</li>
</ul>

<h3>Title: InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhuoran Yang, Xi Guo, Chenjing Ding, Chiyu Wang, Wei Wu, Yanyong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03242">https://arxiv.org/abs/2602.03242</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03242">https://arxiv.org/pdf/2602.03242</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03242]] InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation(https://arxiv.org/abs/2602.03242)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Autonomous driving relies on robust models trained on high-quality, large-scale multi-view driving videos. While world models offer a cost-effective solution for generating realistic driving videos, they struggle to maintain instance-level temporal consistency and spatial geometric fidelity. To address these challenges, we propose InstaDrive, a novel framework that enhances driving video realism through two key advancements: (1) Instance Flow Guider, which extracts and propagates instance features across frames to enforce temporal consistency, preserving instance identity over time. (2) Spatial Geometric Aligner, which improves spatial reasoning, ensures precise instance positioning, and explicitly models occlusion hierarchies. By incorporating these instance-aware mechanisms, InstaDrive achieves state-of-the-art video generation quality and enhances downstream autonomous driving tasks on the nuScenes dataset. Additionally, we utilize CARLA's autopilot to procedurally and stochastically simulate rare but safety-critical driving scenarios across diverse maps and regions, enabling rigorous safety evaluation for autonomous systems. Our project page is this https URL.</li>
</ul>

<h3>Title: LaVPR: Benchmarking Language and Vision for Place Recognition</h3>
<ul>
<li><strong>Authors: </strong>Ofer Idan, Dan Badur, Yosi Keller, Yoli Shavit</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03253">https://arxiv.org/abs/2602.03253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03253">https://arxiv.org/pdf/2602.03253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03253]] LaVPR: Benchmarking Language and Vision for Place Recognition(https://arxiv.org/abs/2602.03253)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Visual Place Recognition (VPR) often fails under extreme environmental changes and perceptual aliasing. Furthermore, standard systems cannot perform "blind" localization from verbal descriptions alone, a capability needed for applications such as emergency response. To address these challenges, we introduce LaVPR, a large-scale benchmark that extends existing VPR datasets with over 650,000 rich natural-language descriptions. Using LaVPR, we investigate two paradigms: Multi-Modal Fusion for enhanced robustness and Cross-Modal Retrieval for language-based localization. Our results show that language descriptions yield consistent gains in visually degraded conditions, with the most significant impact on smaller backbones. Notably, adding language allows compact models to rival the performance of much larger vision-only architectures. For cross-modal retrieval, we establish a baseline using Low-Rank Adaptation (LoRA) and Multi-Similarity loss, which substantially outperforms standard contrastive methods across vision-language models. Ultimately, LaVPR enables a new class of localization systems that are both resilient to real-world stochasticity and practical for resource-constrained deployment. Our dataset and code are available at this https URL.</li>
</ul>

<h3>Title: GraDE: A Graph Diffusion Estimator for Frequent Subgraph Discovery in Neural Architectures</h3>
<ul>
<li><strong>Authors: </strong>Yikang Yang, Zhengxin Yang, Minghao Luo, Luzhou Peng, Hongxiao Li, Wanling Gao, Lei Wang, Jianfeng Zhan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03257">https://arxiv.org/abs/2602.03257</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03257">https://arxiv.org/pdf/2602.03257</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03257]] GraDE: A Graph Diffusion Estimator for Frequent Subgraph Discovery in Neural Architectures(https://arxiv.org/abs/2602.03257)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Finding frequently occurring subgraph patterns or network motifs in neural architectures is crucial for optimizing efficiency, accelerating design, and uncovering structural insights. However, as the subgraph size increases, enumeration-based methods are perfectly accurate but computationally prohibitive, while sampling-based methods are computationally tractable but suffer from a severe decline in discovery capability. To address these challenges, this paper proposes GraDE, a diffusion-guided search framework that ensures both computational feasibility and discovery capability. The key innovation is the Graph Diffusion Estimator (GraDE), which is the first to introduce graph diffusion models to identify frequent subgraphs by scoring their typicality within the learned distribution. Comprehensive experiments demonstrate that the estimator achieves superior ranking accuracy, with up to 114\% improvement compared to sampling-based baselines. Benefiting from this, the proposed framework successfully discovers large-scale frequent patterns, achieving up to 30$\times$ higher median frequency than sampling-based methods.</li>
</ul>

<h3>Title: HypCBC: Domain-Invariant Hyperbolic Cross-Branch Consistency for Generalizable Medical Image Analysis</h3>
<ul>
<li><strong>Authors: </strong>Francesco Di Salvo, Sebastian Doerrich, Jonas Alle, Christian Ledig</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03264">https://arxiv.org/abs/2602.03264</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03264">https://arxiv.org/pdf/2602.03264</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03264]] HypCBC: Domain-Invariant Hyperbolic Cross-Branch Consistency for Generalizable Medical Image Analysis(https://arxiv.org/abs/2602.03264)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Robust generalization beyond training distributions remains a critical challenge for deep neural networks. This is especially pronounced in medical image analysis, where data is often scarce and covariate shifts arise from different hardware devices, imaging protocols, and heterogeneous patient populations. These factors collectively hinder reliable performance and slow down clinical adoption. Despite recent progress, existing learning paradigms primarily rely on the Euclidean manifold, whose flat geometry fails to capture the complex, hierarchical structures present in clinical data. In this work, we exploit the advantages of hyperbolic manifolds to model complex data characteristics. We present the first comprehensive validation of hyperbolic representation learning for medical image analysis and demonstrate statistically significant gains across eleven in-distribution datasets and three ViT models. We further propose an unsupervised, domain-invariant hyperbolic cross-branch consistency constraint. Extensive experiments confirm that our proposed method promotes domain-invariant features and outperforms state-of-the-art Euclidean methods by an average of $+2.1\%$ AUC on three domain generalization benchmarks: Fitzpatrick17k, Camelyon17-WILDS, and a cross-dataset setup for retinal imaging. These datasets span different imaging modalities, data sizes, and label granularities, confirming generalization capabilities across substantially different conditions. The code is available at this https URL .</li>
</ul>

<h3>Title: Beyond Suffixes: Token Position in GCG Adversarial Attacks on Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hicham Eddoubi, Umar Faruk Abdullahi, Fadi Hassan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03265">https://arxiv.org/abs/2602.03265</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03265">https://arxiv.org/pdf/2602.03265</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03265]] Beyond Suffixes: Token Position in GCG Adversarial Attacks on Large Language Models(https://arxiv.org/abs/2602.03265)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have seen widespread adoption across multiple domains, creating an urgent need for robust safety alignment mechanisms. However, robustness remains challenging due to jailbreak attacks that bypass alignment via adversarial prompts. In this work, we focus on the prevalent Greedy Coordinate Gradient (GCG) attack and identify a previously underexplored attack axis in jailbreak attacks typically framed as suffix-based: the placement of adversarial tokens within the prompt. Using GCG as a case study, we show that both optimizing attacks to generate prefixes instead of suffixes and varying adversarial token position during evaluation substantially influence attack success rates. Our findings highlight a critical blind spot in current safety evaluations and underline the need to account for the position of adversarial tokens in the adversarial robustness evaluation of LLMs.</li>
</ul>

<h3>Title: Unveiling Covert Toxicity in Multimodal Data via Toxicity Association Graphs: A Graph-Based Metric and Interpretable Detection Framework</h3>
<ul>
<li><strong>Authors: </strong>Guanzong Wu, Zihao Zhu, Siwei Lyu, Baoyuan Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03268">https://arxiv.org/abs/2602.03268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03268">https://arxiv.org/pdf/2602.03268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03268]] Unveiling Covert Toxicity in Multimodal Data via Toxicity Association Graphs: A Graph-Based Metric and Interpretable Detection Framework(https://arxiv.org/abs/2602.03268)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Detecting toxicity in multimodal data remains a significant challenge, as harmful meanings often lurk beneath seemingly benign individual modalities: only emerging when modalities are combined and semantic associations are activated. To address this, we propose a novel detection framework based on Toxicity Association Graphs (TAGs), which systematically model semantic associations between innocuous entities and latent toxic implications. Leveraging TAGs, we introduce the first quantifiable metric for hidden toxicity, the Multimodal Toxicity Covertness (MTC), which measures the degree of concealment in toxic multimodal expressions. By integrating our detection framework with the MTC metric, our approach enables precise identification of covert toxicity while preserving full interpretability of the decision-making process, significantly enhancing transparency in multimodal toxicity detection. To validate our method, we construct the Covert Toxic Dataset, the first benchmark specifically designed to capture high-covertness toxic multimodal instances. This dataset encodes nuanced cross-modal associations and serves as a rigorous testbed for evaluating both the proposed metric and detection framework. Extensive experiments demonstrate that our approach outperforms existing methods across both low- and high-covertness toxicity regimes, while delivering clear, interpretable, and auditable detection outcomes. Together, our contributions advance the state of the art in explainable multimodal toxicity detection and lay the foundation for future context-aware and interpretable approaches. Content Warning: This paper contains examples of toxic multimodal content that may be offensive or disturbing to some readers. Reader discretion is advised.</li>
</ul>

<h3>Title: LogicScan: An LLM-driven Framework for Detecting Business Logic Vulnerabilities in Smart Contracts</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Gao, Zijian Zhang, Yuqiang Sun, Ye Liu, Chengwei Liu, Han Liu, Yi Li, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03271">https://arxiv.org/abs/2602.03271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03271">https://arxiv.org/pdf/2602.03271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03271]] LogicScan: An LLM-driven Framework for Detecting Business Logic Vulnerabilities in Smart Contracts(https://arxiv.org/abs/2602.03271)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Business logic vulnerabilities have become one of the most damaging yet least understood classes of smart contract vulnerabilities. Unlike traditional bugs such as reentrancy or arithmetic errors, these vulnerabilities arise from missing or incorrectly enforced business invariants and are tightly coupled with protocol semantics. Existing static analysis techniques struggle to capture such high-level logic, while recent large language model based approaches often suffer from unstable outputs and low accuracy due to hallucination and limited verification. In this paper, we propose LogicScan, an automated contrastive auditing framework for detecting business logic vulnerabilities in smart contracts. The key insight behind LogicScan is that mature, widely deployed on-chain protocols implicitly encode well-tested and consensus-driven business invariants. LogicScan systematically mines these invariants from large-scale on-chain contracts and reuses them as reference constraints to audit target contracts. To achieve this, LogicScan introduces a Business Specification Language (BSL) to normalize diverse implementation patterns into structured, verifiable logic representations. It further combines noise-aware logic aggregation with contrastive auditing to identify missing or weakly enforced invariants while mitigating LLM-induced false positives. We evaluate LogicScan on three real-world datasets, including DeFiHacks, Web3Bugs, and a set of top-200 audited contracts. The results show that LogicScan achieves an F1 score of 85.2%, significantly outperforming state-of-the-art tools while maintaining a low false-positive rate on production-grade contracts. Additional experiments demonstrate that LogicScan maintains consistent performance across different LLMs and is cost-effective, and that its false-positive suppression mechanisms substantially improve robustness.</li>
</ul>

<h3>Title: BlockRR: A Unified Framework of RR-type Algorithms for Label Differential Privacy</h3>
<ul>
<li><strong>Authors: </strong>Haixia Liu, Yi Ding</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03277">https://arxiv.org/abs/2602.03277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03277">https://arxiv.org/pdf/2602.03277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03277]] BlockRR: A Unified Framework of RR-type Algorithms for Label Differential Privacy(https://arxiv.org/abs/2602.03277)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce BlockRR, a novel and unified randomized-response mechanism for label differential privacy. This framework generalizes existed RR-type mechanisms as special cases under specific parameter settings, which eliminates the need for separate, case-by-case analysis. Theoretically, we prove that BlockRR satisfies $\epsilon$-label DP. We also design a partition method for BlockRR based on a weight matrix derived from label prior information; the parallel composition principle ensures that the composition of two such mechanisms remains $\epsilon$-label DP. Empirically, we evaluate BlockRR on two variants of CIFAR-10 with varying degrees of class imbalance. Results show that in the high-privacy and moderate-privacy regimes ($\epsilon \leq 3.0$), our propsed method gets a better balance between test accuaracy and the average of per-class accuracy. In the low-privacy regime ($\epsilon \geq 4.0$), all methods reduce BlockRR to standard RR without additional performance loss.</li>
</ul>

<h3>Title: Global Geometry Is Not Enough for Vision Representations</h3>
<ul>
<li><strong>Authors: </strong>Jiwan Chung, Seon Joo Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03282">https://arxiv.org/abs/2602.03282</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03282">https://arxiv.org/pdf/2602.03282</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03282]] Global Geometry Is Not Enough for Vision Representations(https://arxiv.org/abs/2602.03282)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>A common assumption in representation learning is that globally well-distributed embeddings support robust and generalizable representations. This focus has shaped both training objectives and evaluation protocols, implicitly treating global geometry as a proxy for representational competence. While global geometry effectively encodes which elements are present, it is often insensitive to how they are composed. We investigate this limitation by testing the ability of geometric metrics to predict compositional binding across 21 vision encoders. We find that standard geometry-based statistics exhibit near-zero correlation with compositional binding. In contrast, functional sensitivity, as measured by the input-output Jacobian, reliably tracks this capability. We further provide an analytic account showing that this disparity arises from objective design, as existing losses explicitly constrain embedding geometry but leave the local input-output mapping unconstrained. These results suggest that global embedding geometry captures only a partial view of representational competence and establish functional sensitivity as a critical complementary axis for modeling composite structure.</li>
</ul>

<h3>Title: Time Is All It Takes: Spike-Retiming Attacks on Event-Driven Spiking Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Yi Yu, Qixin Zhang, Shuhan Ye, Xun Lin, Qianshan Wei, Kun Wang, Wenhan Yang, Dacheng Tao, Xudong Jiang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03284">https://arxiv.org/abs/2602.03284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03284">https://arxiv.org/pdf/2602.03284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03284]] Time Is All It Takes: Spike-Retiming Attacks on Event-Driven Spiking Neural Networks(https://arxiv.org/abs/2602.03284)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, steal</a></li>
<li><strong>Abstract: </strong>Spiking neural networks (SNNs) compute with discrete spikes and exploit temporal structure, yet most adversarial attacks change intensities or event counts instead of timing. We study a timing-only adversary that retimes existing spikes while preserving spike counts and amplitudes in event-driven SNNs, thus remaining rate-preserving. We formalize a capacity-1 spike-retiming threat model with a unified trio of budgets: per-spike jitter $\mathcal{B}_{\infty}$, total delay $\mathcal{B}_{1}$, and tamper count $\mathcal{B}_{0}$. Feasible adversarial examples must satisfy timeline consistency and non-overlap, which makes the search space discrete and constrained. To optimize such retimings at scale, we use projected-in-the-loop (PIL) optimization: shift-probability logits yield a differentiable soft retiming for backpropagation, and a strict projection in the forward pass produces a feasible discrete schedule that satisfies capacity-1, non-overlap, and the chosen budget at every step. The objective maximizes task loss on the projected input and adds a capacity regularizer together with budget-aware penalties, which stabilizes gradients and aligns optimization with evaluation. Across event-driven benchmarks (CIFAR10-DVS, DVS-Gesture, N-MNIST) and diverse SNN architectures, we evaluate under binary and integer event grids and a range of retiming budgets, and also test models trained with timing-aware adversarial training designed to counter timing-only attacks. For example, on DVS-Gesture the attack attains high success (over $90\%$) while touching fewer than $2\%$ of spikes under $\mathcal{B}_{0}$. Taken together, our results show that spike retiming is a practical and stealthy attack surface that current defenses struggle to counter, providing a clear reference for temporal robustness in event-driven SNNs. Code is available at this https URL.</li>
</ul>

<h3>Title: A3-TTA: Adaptive Anchor Alignment Test-Time Adaptation for Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Jianghao Wu, Xiangde Luo, Yubo Zhou, Lianming Wu, Guotai Wang, Shaoting Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03292">https://arxiv.org/abs/2602.03292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03292">https://arxiv.org/pdf/2602.03292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03292]] A3-TTA: Adaptive Anchor Alignment Test-Time Adaptation for Image Segmentation(https://arxiv.org/abs/2602.03292)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Test-Time Adaptation (TTA) offers a practical solution for deploying image segmentation models under domain shift without accessing source data or retraining. Among existing TTA strategies, pseudo-label-based methods have shown promising performance. However, they often rely on perturbation-ensemble heuristics (e.g., dropout sampling, test-time augmentation, Gaussian noise), which lack distributional grounding and yield unstable training signals. This can trigger error accumulation and catastrophic forgetting during adaptation. To address this, we propose \textbf{A3-TTA}, a TTA framework that constructs reliable pseudo-labels through anchor-guided supervision. Specifically, we identify well-predicted target domain images using a class compact density metric, under the assumption that confident predictions imply distributional proximity to the source domain. These anchors serve as stable references to guide pseudo-label generation, which is further regularized via semantic consistency and boundary-aware entropy minimization. Additionally, we introduce a self-adaptive exponential moving average strategy to mitigate label noise and stabilize model update during adaptation. Evaluated on both multi-domain medical images (heart structure and prostate segmentation) and natural images, A3-TTA significantly improves average Dice scores by 10.40 to 17.68 percentage points compared to the source model, outperforming several state-of-the-art TTA methods under different segmentation model architectures. A3-TTA also excels in continual TTA, maintaining high performance across sequential target domains with strong anti-forgetting ability. The code will be made publicly available at this https URL.</li>
</ul>

<h3>Title: Anomaly Detection via Mean Shift Density Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Pritam Kar, Rahul Bordoloi, Olaf Wolkenhauer, Saptarshi Bej</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03293">https://arxiv.org/abs/2602.03293</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03293">https://arxiv.org/pdf/2602.03293</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03293]] Anomaly Detection via Mean Shift Density Enhancement(https://arxiv.org/abs/2602.03293)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust</a></li>
<li><strong>Abstract: </strong>Unsupervised anomaly detection stands as an important problem in machine learning, with applications in financial fraud prevention, network security and medical diagnostics. Existing unsupervised anomaly detection algorithms rarely perform well across different anomaly types, often excelling only under specific structural assumptions. This lack of robustness also becomes particularly evident under noisy settings. We propose Mean Shift Density Enhancement (MSDE), a fully unsupervised framework that detects anomalies through their geometric response to density-driven manifold evolution. MSDE is based on the principle that normal samples, being well supported by local density, remain stable under iterative density enhancement, whereas anomalous samples undergo large cumulative displacements as they are attracted toward nearby density modes. To operationalize this idea, MSDE employs a weighted mean-shift procedure with adaptive, sample-specific density weights derived from a UMAP-based fuzzy neighborhood graph. Anomaly scores are defined by the total displacement accumulated across a small number of mean-shift iterations. We evaluate MSDE on the ADBench benchmark, comprising forty six real-world tabular datasets, four realistic anomaly generation mechanisms, and six noise levels. Compared to 13 established unsupervised baselines, MSDE achieves consistently strong, balanced and robust performance for AUC-ROC, AUC-PR, and Precision@n, at several noise levels and on average over several types of anomalies. These results demonstrate that displacement-based scoring provides a robust alternative to the existing state-of-the-art for unsupervised anomaly detection.</li>
</ul>

<h3>Title: POP: Prefill-Only Pruning for Efficient Large Model Inference</h3>
<ul>
<li><strong>Authors: </strong>Junhui He, Zhihui Fu, Jun Wang, Qingan Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03295">https://arxiv.org/abs/2602.03295</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03295">https://arxiv.org/pdf/2602.03295</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03295]] POP: Prefill-Only Pruning for Efficient Large Model Inference(https://arxiv.org/abs/2602.03295)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) and Vision-Language Models (VLMs) have demonstrated remarkable capabilities. However, their deployment is hindered by significant computational costs. Existing structured pruning methods, while hardware-efficient, often suffer from significant accuracy degradation. In this paper, we argue that this failure stems from a stage-agnostic pruning approach that overlooks the asymmetric roles between the prefill and decode stages. By introducing a virtual gate mechanism, our importance analysis reveals that deep layers are critical for next-token prediction (decode) but largely redundant for context encoding (prefill). Leveraging this insight, we propose Prefill-Only Pruning (POP), a stage-aware inference strategy that safely omits deep layers during the computationally intensive prefill stage while retaining the full model for the sensitive decode stage. To enable the transition between stages, we introduce independent Key-Value (KV) projections to maintain cache integrity, and a boundary handling strategy to ensure the accuracy of the first generated token. Extensive experiments on Llama-3.1, Qwen3-VL, and Gemma-3 across diverse modalities demonstrate that POP achieves up to 1.37$\times$ speedup in prefill latency with minimal performance loss, effectively overcoming the accuracy-efficiency trade-off limitations of existing structured pruning methods.</li>
</ul>

<h3>Title: R1-SyntheticVL: Is Synthetic Data from Generative Models Ready for Multimodal Large Language Model?</h3>
<ul>
<li><strong>Authors: </strong>Jingyi Zhang, Tianyi Lin, Huanjin Yao, Xiang Lan, Shunyu Liu, Jiaxing Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03300">https://arxiv.org/abs/2602.03300</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03300">https://arxiv.org/pdf/2602.03300</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03300]] R1-SyntheticVL: Is Synthetic Data from Generative Models Ready for Multimodal Large Language Model?(https://arxiv.org/abs/2602.03300)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>In this work, we aim to develop effective data synthesis techniques that autonomously synthesize multimodal training data for enhancing MLLMs in solving complex real-world tasks. To this end, we propose Collective Adversarial Data Synthesis (CADS), a novel and general approach to synthesize high-quality, diverse and challenging multimodal data for MLLMs. The core idea of CADS is to leverage collective intelligence to ensure high-quality and diverse generation, while exploring adversarial learning to synthesize challenging samples for effectively driving model improvement. Specifically, CADS operates with two cyclic phases, i.e., Collective Adversarial Data Generation (CAD-Generate) and Collective Adversarial Data Judgment (CAD-Judge). CAD-Generate leverages collective knowledge to jointly generate new and diverse multimodal data, while CAD-Judge collaboratively assesses the quality of synthesized data. In addition, CADS introduces an Adversarial Context Optimization mechanism to optimize the generation context to encourage challenging and high-value data generation. With CADS, we construct MMSynthetic-20K and train our model R1-SyntheticVL, which demonstrates superior performance on various benchmarks.</li>
</ul>

<h3>Title: medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions</h3>
<ul>
<li><strong>Authors: </strong>Qianyi Xu, Gousia Habib, Feng Wu, Yanrui Du, Zhihui Chen, Swapnil Mishra, Dilruk Perera, Mengling Feng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03305">https://arxiv.org/abs/2602.03305</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03305">https://arxiv.org/pdf/2602.03305</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03305]] medR: Reward Engineering for Clinical Offline Reinforcement Learning via Tri-Drive Potential Functions(https://arxiv.org/abs/2602.03305)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning (RL) offers a powerful framework for optimizing dynamic treatment regimes (DTRs). However, clinical RL is fundamentally bottlenecked by reward engineering: the challenge of defining signals that safely and effectively guide policy learning in complex, sparse offline environments. Existing approaches often rely on manual heuristics that fail to generalize across diverse pathologies. To address this, we propose an automated pipeline leveraging Large Language Models (LLMs) for offline reward design and verification. We formulate the reward function using potential functions consisted of three core components: survival, confidence, and competence. We further introduce quantitative metrics to rigorously evaluate and select the optimal reward structure prior to deployment. By integrating LLM-driven domain knowledge, our framework automates the design of reward functions for specific diseases while significantly enhancing the performance of the resulting policies.</li>
</ul>

<h3>Title: Entropy-Gated Selective Policy Optimization:Token-Level Gradient Allocation for Hybrid Training of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuelin Hu, Zhengxue Cheng, Wei Liu, Li Song</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03309">https://arxiv.org/abs/2602.03309</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03309">https://arxiv.org/pdf/2602.03309</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03309]] Entropy-Gated Selective Policy Optimization:Token-Level Gradient Allocation for Hybrid Training of Large Language Models(https://arxiv.org/abs/2602.03309)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Hybrid training methods for large language models combine supervised fine tuning (SFT) on expert demonstrations with reinforcement learning (RL) on model rollouts, typically at the sample level. We propose Entropy Gated Selective Policy Optimization (EGSPO), a three stage framework that extends sample level mixing with token level gradient modulation. Stage 1, SFT expert learning, establishes a reliable warm up policy using expert demonstrations with a pure SFT loss. Stage 2, RL rollout generation, samples trajectories from the current policy and computes per token predictive entropy. Stage 3, the EGSPO mechanism, applies entropy gated gradient allocation: a predictive entropy module routes high entropy tokens to full PPO updates to encourage exploration, and low entropy tokens to attenuated PPO updates to reduce variance and preserve knowledge. Critically, both branches incorporate the advantage function A_t, ensuring that incorrect trajectories receive consistent negative learning signals and preventing reinforcement of confident errors. EGSPO achieves consistent improvements on mathematical reasoning benchmarks, with gains of 3.8 percent on AIME and 2.9 percent on MATH over the CHORD phi baseline, while incurring only 3.4 percent additional computational overhead.</li>
</ul>

<h3>Title: PQTNet: Pixel-wise Quantitative Thermography Neural Network for Estimating Defect Depth in Polylactic Acid Parts by Additive Manufacturing</h3>
<ul>
<li><strong>Authors: </strong>Lei Deng, Wenhao Huang, Chao Yang, Haoyuan Zheng, Yinbin Tian, Yue Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03314">https://arxiv.org/abs/2602.03314</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03314">https://arxiv.org/pdf/2602.03314</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03314]] PQTNet: Pixel-wise Quantitative Thermography Neural Network for Estimating Defect Depth in Polylactic Acid Parts by Additive Manufacturing(https://arxiv.org/abs/2602.03314)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Defect depth quantification in additively manufactured (AM) components remains a significant challenge for non-destructive testing (NDT). This study proposes a Pixel-wise Quantitative Thermography Neural Network (PQT-Net) to address this challenge for polylactic acid (PLA) parts. A key innovation is a novel data augmentation strategy that reconstructs thermal sequence data into two-dimensional stripe images, preserving the complete temporal evolution of heat diffusion for each pixel. The PQT-Net architecture incorporates a pre-trained EfficientNetV2-S backbone and a custom Residual Regression Head (RRH) with learnable parameters to refine outputs. Comparative experiments demonstrate the superiority of PQT-Net over other deep learning models, achieving a minimum Mean Absolute Error (MAE) of 0.0094 mm and a coefficient of determination (R) exceeding 99%. The high precision of PQT-Net underscores its potential for robust quantitative defect characterization in AM.</li>
</ul>

<h3>Title: Invisible Clean-Label Backdoor Attacks for Generative Data Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Ting Xiang, Jinhui Zhao, Changjian Chen, Zhuo Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03316">https://arxiv.org/abs/2602.03316</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03316">https://arxiv.org/pdf/2602.03316</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03316]] Invisible Clean-Label Backdoor Attacks for Generative Data Augmentation(https://arxiv.org/abs/2602.03316)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, generative</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of image generative models, generative data augmentation has become an effective way to enrich training images, especially when only small-scale datasets are available. At the same time, in practical applications, generative data augmentation can be vulnerable to clean-label backdoor attacks, which aim to bypass human inspection. However, based on theoretical analysis and preliminary experiments, we observe that directly applying existing pixel-level clean-label backdoor attack methods (e.g., COMBAT) to generated images results in low attack success rates. This motivates us to move beyond pixel-level triggers and focus instead on the latent feature level. To this end, we propose InvLBA, an invisible clean-label backdoor attack method for generative data augmentation by latent perturbation. We theoretically prove that the generalization of the clean accuracy and attack success rates of InvLBA can be guaranteed. Experiments on multiple datasets show that our method improves the attack success rate by 46.43% on average, with almost no reduction in clean accuracy and high robustness against SOTA defense methods.</li>
</ul>

<h3>Title: MIRROR: A Multi-Agent Framework with Iterative Adaptive Revision and Hierarchical Retrieval for Optimization Modeling in Operations Research</h3>
<ul>
<li><strong>Authors: </strong>Yifan Shi, Jialong Shi, Jiayi Wang, Ye Fan, Jianyong Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03318">https://arxiv.org/abs/2602.03318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03318">https://arxiv.org/pdf/2602.03318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03318]] MIRROR: A Multi-Agent Framework with Iterative Adaptive Revision and Hierarchical Retrieval for Optimization Modeling in Operations Research(https://arxiv.org/abs/2602.03318)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Operations Research (OR) relies on expert-driven modeling-a slow and fragile process ill-suited to novel scenarios. While large language models (LLMs) can automatically translate natural language into optimization models, existing approaches either rely on costly post-training or employ multi-agent frameworks, yet most still lack reliable collaborative error correction and task-specific retrieval, often leading to incorrect outputs. We propose MIRROR, a fine-tuning-free, end-to-end multi-agent framework that directly translates natural language optimization problems into mathematical models and solver code. MIRROR integrates two core mechanisms: (1) execution-driven iterative adaptive revision for automatic error correction, and (2) hierarchical retrieval to fetch relevant modeling and coding exemplars from a carefully curated exemplar library. Experiments show that MIRROR outperforms existing methods on standard OR benchmarks, with notable results on complex industrial datasets such as IndustryOR and Mamo-ComplexLP. By combining precise external knowledge infusion with systematic error correction, MIRROR provides non-expert users with an efficient and reliable OR modeling solution, overcoming the fundamental limitations of general-purpose LLMs in expert optimization tasks.</li>
</ul>

<h3>Title: Information-Theoretic Multi-Model Fusion for Target-Oriented Adaptive Sampling in Materials Design</h3>
<ul>
<li><strong>Authors: </strong>Yixuan Zhang, Zhiyuan Li, Weijia He, Mian Dai, Chen Shen, Teng Long, Hongbin Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03319">https://arxiv.org/abs/2602.03319</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03319">https://arxiv.org/pdf/2602.03319</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03319]] Information-Theoretic Multi-Model Fusion for Target-Oriented Adaptive Sampling in Materials Design(https://arxiv.org/abs/2602.03319)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Target-oriented discovery under limited evaluation budgets requires making reliable progress in high-dimensional, heterogeneous design spaces where each new measurement is costly, whether experimental or high-fidelity simulation. We present an information-theoretic framework for target-oriented adaptive sampling that reframes optimization as trajectory discovery: instead of approximating the full response surface, the method maintains and refines a low-entropy information state that concentrates search on target-relevant directions. The approach couples data, model beliefs, and physics/structure priors through dimension-aware information budgeting, adaptive bootstrapped distillation over a heterogeneous surrogate reservoir, and structure-aware candidate manifold analysis with Kalman-inspired multi-model fusion to balance consensus-driven exploitation and disagreement-driven exploration. Evaluated under a single unified protocol without dataset-specific tuning, the framework improves sample efficiency and reliability across 14 single- and multi-objective materials design tasks spanning candidate pools from $600$ to $4 \times 10^6$ and feature dimensions from $10$ to $10^3$, typically reaching top-performing regions within 100 evaluations. Complementary 20-dimensional synthetic benchmarks (Ackley, Rastrigin, Schwefel) further demonstrate robustness to rugged and multimodal landscapes.</li>
</ul>

<h3>Title: MedSAM-Agent: Empowering Interactive Medical Image Segmentation with Multi-turn Agentic Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Shengyuan Liu, Liuxin Bao, Qi Yang, Wanting Geng, Boyun Zheng, Chenxin Li, Wenting Chen, Houwen Peng, Yixuan Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03320">https://arxiv.org/abs/2602.03320</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03320">https://arxiv.org/pdf/2602.03320</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03320]] MedSAM-Agent: Empowering Interactive Medical Image Segmentation with Multi-turn Agentic Reinforcement Learning(https://arxiv.org/abs/2602.03320)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Medical image segmentation is evolving from task-specific models toward generalizable frameworks. Recent research leverages Multi-modal Large Language Models (MLLMs) as autonomous agents, employing reinforcement learning with verifiable reward (RLVR) to orchestrate specialized tools like the Segment Anything Model (SAM). However, these approaches often rely on single-turn, rigid interaction strategies and lack process-level supervision during training, which hinders their ability to fully exploit the dynamic potential of interactive tools and leads to redundant actions. To bridge this gap, we propose MedSAM-Agent, a framework that reformulates interactive segmentation as a multi-step autonomous decision-making process. First, we introduce a hybrid prompting strategy for expert-curated trajectory generation, enabling the model to internalize human-like decision heuristics and adaptive refinement strategies. Furthermore, we develop a two-stage training pipeline that integrates multi-turn, end-to-end outcome verification with a clinical-fidelity process reward design to promote interaction parsimony and decision efficiency. Extensive experiments across 6 medical modalities and 21 datasets demonstrate that MedSAM-Agent achieves state-of-the-art performance, effectively unifying autonomous medical reasoning with robust, iterative optimization. Code is available \href{this https URL}{here}.</li>
</ul>

<h3>Title: From Inexact Gradients to Byzantine Robustness: Acceleration and Optimization under Similarity</h3>
<ul>
<li><strong>Authors: </strong>Renaud Gaucher, Aymeric Dieuleveut, Hadrien Hendrikx</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03329">https://arxiv.org/abs/2602.03329</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03329">https://arxiv.org/pdf/2602.03329</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03329]] From Inexact Gradients to Byzantine Robustness: Acceleration and Optimization under Similarity(https://arxiv.org/abs/2602.03329)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate</a></li>
<li><strong>Abstract: </strong>Standard federated learning algorithms are vulnerable to adversarial nodes, a.k.a. Byzantine failures. To solve this issue, robust distributed learning algorithms have been developed, which typically replace parameter averaging by robust aggregations. While generic conditions on these aggregations exist to guarantee the convergence of (Stochastic) Gradient Descent (SGD), the analyses remain rather ad-hoc. This hinders the development of more complex robust algorithms, such as accelerated ones. In this work, we show that Byzantine-robust distributed optimization can, under standard generic assumptions, be cast as a general optimization with inexact gradient oracles (with both additive and multiplicative error terms), an active field of research. This allows for instance to directly show that GD on top of standard robust aggregation procedures obtains optimal asymptotic error in the Byzantine setting. Going further, we propose two optimization schemes to speed up the convergence. The first one is a Nesterov-type accelerated scheme whose proof directly derives from accelerated inexact gradient results applied to our formulation. The second one hinges on Optimization under Similarity, in which the server leverages an auxiliary loss function that approximates the global loss. Both approaches allow to drastically reduce the communication complexity compared to previous methods, as we show theoretically and empirically.</li>
</ul>

<h3>Title: PWAVEP: Purifying Imperceptible Adversarial Perturbations in 3D Point Clouds via Spectral Graph Wavelets</h3>
<ul>
<li><strong>Authors: </strong>Haoran Li, Renyang Liu, Hongjia Liu, Chen Wang, Long Yin, Jian Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03333">https://arxiv.org/abs/2602.03333</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03333">https://arxiv.org/pdf/2602.03333</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03333]] PWAVEP: Purifying Imperceptible Adversarial Perturbations in 3D Point Clouds via Spectral Graph Wavelets(https://arxiv.org/abs/2602.03333)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Recent progress in adversarial attacks on 3D point clouds, particularly in achieving spatial imperceptibility and high attack performance, presents significant challenges for defenders. Current defensive approaches remain cumbersome, often requiring invasive model modifications, expensive training procedures or auxiliary data access. To address these threats, in this paper, we propose a plug-and-play and non-invasive defense mechanism in the spectral domain, grounded in a theoretical and empirical analysis of the relationship between imperceptible perturbations and high-frequency spectral components. Building upon these insights, we introduce a novel purification framework, termed PWAVEP, which begins by computing a spectral graph wavelet domain saliency score and local sparsity score for each point. Guided by these values, PWAVEP adopts a hierarchical strategy, it eliminates the most salient points, which are identified as hardly recoverable adversarial outliers. Simultaneously, it applies a spectral filtering process to a broader set of moderately salient points. This process leverages a graph wavelet transform to attenuate high-frequency coefficients associated with the targeted points, thereby effectively suppressing adversarial noise. Extensive evaluations demonstrate that the proposed PWAVEP achieves superior accuracy and robustness compared to existing approaches, advancing the state-of-the-art in 3D point cloud purification. Code and datasets are available at this https URL</li>
</ul>

<h3>Title: Composable Visual Tokenizers with Generator-Free Diagnostics of Learnability</h3>
<ul>
<li><strong>Authors: </strong>Bingchen Zhao, Qiushan Guo, Ye Wang, Yixuan Huang, Zhonghua Zhai, Yu Tian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03339">https://arxiv.org/abs/2602.03339</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03339">https://arxiv.org/pdf/2602.03339</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03339]] Composable Visual Tokenizers with Generator-Free Diagnostics of Learnability(https://arxiv.org/abs/2602.03339)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce CompTok, a training framework for learning visual tokenizers whose tokens are enhanced for compositionality. CompTok uses a token-conditioned diffusion decoder. By employing an InfoGAN-style objective, where we train a recognition model to predict the tokens used to condition the diffusion decoder using the decoded images, we enforce the decoder to not ignore any of the tokens. To promote compositional control, besides the original images, CompTok also trains on tokens formed by swapping token subsets between images, enabling more compositional control of the token over the decoder. As the swapped tokens between images do not have ground truth image targets, we apply a manifold constraint via an adversarial flow regularizer to keep unpaired swap generations on the natural-image distribution. The resulting tokenizer not only achieves state-of-the-art performance on image class-conditioned generation, but also demonstrates properties such as swapping tokens between images to achieve high level semantic editing of an image. Additionally, we propose two metrics that measures the landscape of the token space that can be useful to describe not only the compositionality of the tokens, but also how easy to learn the landscape is for a generator to be trained on this space. We show in experiments that CompTok can improve on both of the metrics as well as supporting state-of-the-art generators for class conditioned generation.</li>
</ul>

<h3>Title: Tiled Prompts: Overcoming Prompt Underspecification in Image and Video Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Bryan Sangwoo Kim, Jonghyun Park, Jong Chul Ye</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03342">https://arxiv.org/abs/2602.03342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03342">https://arxiv.org/pdf/2602.03342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03342]] Tiled Prompts: Overcoming Prompt Underspecification in Image and Video Super-Resolution(https://arxiv.org/abs/2602.03342)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-conditioned diffusion models have advanced image and video super-resolution by using prompts as semantic priors, but modern super-resolution pipelines typically rely on latent tiling to scale to high resolutions, where a single global caption causes prompt underspecification. A coarse global prompt often misses localized details (prompt sparsity) and provides locally irrelevant guidance (prompt misguidance) that can be amplified by classifier-free guidance. We propose Tiled Prompts, a unified framework for image and video super-resolution that generates a tile-specific prompt for each latent tile and performs super-resolution under locally text-conditioned posteriors, providing high-information guidance that resolves prompt underspecification with minimal overhead. Experiments on high resolution real-world images and videos show consistent gains in perceptual quality and text alignment, while reducing hallucinations and tile-level artifacts relative to global-prompt baselines.</li>
</ul>

<h3>Title: Robustness as an Emergent Property of Task Performance</h3>
<ul>
<li><strong>Authors: </strong>Shir Ashury-Tahan, Ariel Gera, Elron Bandel, Michal Shmueli-Scheuer, Leshem Choshen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03344">https://arxiv.org/abs/2602.03344</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03344">https://arxiv.org/pdf/2602.03344</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03344]] Robustness as an Emergent Property of Task Performance(https://arxiv.org/abs/2602.03344)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Robustness is often regarded as a critical future challenge for real-world applications, where stability is essential. However, as models often learn tasks in a similar order, we hypothesize that easier tasks will be easier regardless of how they are presented to the model. Indeed, in this paper, we show that as models approach high performance on a task, robustness is effectively achieved. Through an empirical analysis of multiple models across diverse datasets and configurations (e.g., paraphrases, different temperatures), we find a strong positive correlation. Moreover, we find that robustness is primarily driven by task-specific competence rather than inherent model-level properties, challenging current approaches that treat robustness as an independent capability. Thus, from a high-level perspective, we may expect that as new tasks saturate, model robustness on these tasks will emerge accordingly. For researchers, this implies that explicit efforts to measure and improve robustness may warrant reduced emphasis, as such robustness is likely to develop alongside performance gains. For practitioners, it acts as a sign that indeed the tasks that the literature deals with are unreliable, but on easier past tasks, the models are reliable and ready for real-world deployment.</li>
</ul>

<h3>Title: Achieving Linear Speedup for Composite Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Kun Huang, Shi Pu</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03357">https://arxiv.org/abs/2602.03357</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03357">https://arxiv.org/pdf/2602.03357</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03357]] Achieving Linear Speedup for Composite Federated Learning(https://arxiv.org/abs/2602.03357)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>This paper proposes FedNMap, a normal map-based method for composite federated learning, where the objective consists of a smooth loss and a possibly nonsmooth regularizer. FedNMap leverages a normal map-based update scheme to handle the nonsmooth term and incorporates a local correction strategy to mitigate the impact of data heterogeneity across clients. Under standard assumptions, including smooth local losses, weak convexity of the regularizer, and bounded stochastic gradient variance, FedNMap achieves linear speedup with respect to both the number of clients $n$ and the number of local updates $Q$ for nonconvex losses, both with and without the Polyak-Łojasiewicz (PL) condition. To our knowledge, this is the first result establishing linear speedup for nonconvex composite federated learning.</li>
</ul>

<h3>Title: MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling</h3>
<ul>
<li><strong>Authors: </strong>Ning Ding, Fangcheng Liu, Kyungrae Kim, Linji Hao, Kyeng-Hun Lee, Hyeonmok Ko, Yehui Tang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03359">https://arxiv.org/abs/2602.03359</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03359">https://arxiv.org/pdf/2602.03359</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03359]] MeKi: Memory-based Expert Knowledge Injection for Efficient LLM Scaling(https://arxiv.org/abs/2602.03359)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Scaling Large Language Models (LLMs) typically relies on increasing the number of parameters or test-time computations to boost performance. However, these strategies are impractical for edge device deployment due to limited RAM and NPU resources. Despite hardware constraints, deploying performant LLM on edge devices such as smartphone remains crucial for user experience. To address this, we propose MeKi (Memory-based Expert Knowledge Injection), a novel system that scales LLM capacity via storage space rather than FLOPs. MeKi equips each Transformer layer with token-level memory experts that injects pre-stored semantic knowledge into the generation process. To bridge the gap between training capacity and inference efficiency, we employ a re-parameterization strategy to fold parameter matrices used during training into a compact static lookup table. By offloading the knowledge to ROM, MeKi decouples model capacity from computational cost, introducing zero inference latency overhead. Extensive experiments demonstrate that MeKi significantly outperforms dense LLM baselines with identical inference speed, validating the effectiveness of memory-based scaling paradigm for on-device LLMs. Project homepage is at this https URL.</li>
</ul>

<h3>Title: Z3D: Zero-Shot 3D Visual Grounding from Images</h3>
<ul>
<li><strong>Authors: </strong>Nikita Drozdov, Andrey Lemeshko, Nikita Gavrilov, Anton Konushin, Danila Rukhovich, Maksim Kolodiazhnyi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03361">https://arxiv.org/abs/2602.03361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03361">https://arxiv.org/pdf/2602.03361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03361]] Z3D: Zero-Shot 3D Visual Grounding from Images(https://arxiv.org/abs/2602.03361)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>3D visual grounding (3DVG) aims to localize objects in a 3D scene based on natural language queries. In this work, we explore zero-shot 3DVG from multi-view images alone, without requiring any geometric supervision or object priors. We introduce Z3D, a universal grounding pipeline that flexibly operates on multi-view images while optionally incorporating camera poses and depth maps. We identify key bottlenecks in prior zero-shot methods causing significant performance degradation and address them with (i) a state-of-the-art zero-shot 3D instance segmentation method to generate high-quality 3D bounding box proposals and (ii) advanced reasoning via prompt-based segmentation, which utilizes full capabilities of modern VLMs. Extensive experiments on the ScanRefer and Nr3D benchmarks demonstrate that our approach achieves state-of-the-art performance among zero-shot methods. Code is available at this https URL .</li>
</ul>

<h3>Title: Pursuing Best Industrial Practices for Retrieval-Augmented Generation in the Medical Domain</h3>
<ul>
<li><strong>Authors: </strong>Wei Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03368">https://arxiv.org/abs/2602.03368</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03368">https://arxiv.org/pdf/2602.03368</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03368]] Pursuing Best Industrial Practices for Retrieval-Augmented Generation in the Medical Domain(https://arxiv.org/abs/2602.03368)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While retrieval augmented generation (RAG) has been swiftly adopted in industrial applications based on large language models (LLMs), there is no consensus on what are the best practices for building a RAG system in terms of what are the components, how to organize these components and how to implement each component for the industrial applications, especially in the medical domain. In this work, we first carefully analyze each component of the RAG system and propose practical alternatives for each component. Then, we conduct systematic evaluations on three types of tasks, revealing the best practices for improving the RAG system and how LLM-based RAG systems make trade-offs between performance and efficiency.</li>
</ul>

<h3>Title: Symbol-Aware Reasoning with Masked Discrete Diffusion for Handwritten Mathematical Expression Recognition</h3>
<ul>
<li><strong>Authors: </strong>Takaya Kawakatsu, Ryo Ishiyama</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03370">https://arxiv.org/abs/2602.03370</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03370">https://arxiv.org/pdf/2602.03370</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03370]] Symbol-Aware Reasoning with Masked Discrete Diffusion for Handwritten Mathematical Expression Recognition(https://arxiv.org/abs/2602.03370)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Handwritten Mathematical Expression Recognition (HMER) requires reasoning over diverse symbols and 2D structural layouts, yet autoregressive models struggle with exposure bias and syntactic inconsistency. We present a discrete diffusion framework that reformulates HMER as iterative symbolic refinement instead of sequential generation. Through multi-step remasking, the proposal progressively refines both symbols and structural relations, removing causal dependencies and improving structural consistency. A symbol-aware tokenization and Random-Masking Mutual Learning further enhance syntactic alignment and robustness to handwriting diversity. On the MathWriting benchmark, the proposal achieves 5.56\% CER and 60.42\% EM, outperforming strong Transformer and commercial baselines. Consistent gains on CROHME 2014--2023 demonstrate that discrete diffusion provides a new paradigm for structure-aware visual recognition beyond generative modeling.</li>
</ul>

<h3>Title: Multi-Resolution Alignment for Voxel Sparsity in Camera-Based 3D Semantic Scene Completion</h3>
<ul>
<li><strong>Authors: </strong>Zhiwen Yang, Yuxin Peng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03371">https://arxiv.org/abs/2602.03371</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03371">https://arxiv.org/pdf/2602.03371</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03371]] Multi-Resolution Alignment for Voxel Sparsity in Camera-Based 3D Semantic Scene Completion(https://arxiv.org/abs/2602.03371)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Camera-based 3D semantic scene completion (SSC) offers a cost-effective solution for assessing the geometric occupancy and semantic labels of each voxel in the surrounding 3D scene with image inputs, providing a voxel-level scene perception foundation for the perception-prediction-planning autonomous driving systems. Although significant progress has been made in existing methods, their optimization rely solely on the supervision from voxel labels and face the challenge of voxel sparsity as a large portion of voxels in autonomous driving scenarios are empty, which limits both optimization efficiency and model performance. To address this issue, we propose a \textit{Multi-Resolution Alignment (MRA)} approach to mitigate voxel sparsity in camera-based 3D semantic scene completion, which exploits the scene and instance level alignment across multi-resolution 3D features as auxiliary supervision. Specifically, we first propose the Multi-resolution View Transformer module, which projects 2D image features into multi-resolution 3D features and aligns them at the scene level through fusing discriminative seed features. Furthermore, we design the Cubic Semantic Anisotropy module to identify the instance-level semantic significance of each voxel, accounting for the semantic differences of a specific voxel against its neighboring voxels within a cubic area. Finally, we devise a Critical Distribution Alignment module, which selects critical voxels as instance-level anchors with the guidance of cubic semantic anisotropy, and applies a circulated loss for auxiliary supervision on the critical feature distribution consistency across different resolutions. The code is available at this https URL.</li>
</ul>

<h3>Title: SLIM-Diff: Shared Latent Image-Mask Diffusion with Lp loss for Data-Scarce Epilepsy FLAIR MRI</h3>
<ul>
<li><strong>Authors: </strong>Mario Pascual-González, Ariadna Jiménez-Partinen, R.M. Luque-Baena, Fátima Nagib-Raya, Ezequiel López-Rubio</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03372">https://arxiv.org/abs/2602.03372</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03372">https://arxiv.org/pdf/2602.03372</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03372]] SLIM-Diff: Shared Latent Image-Mask Diffusion with Lp loss for Data-Scarce Epilepsy FLAIR MRI(https://arxiv.org/abs/2602.03372)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Focal cortical dysplasia (FCD) lesions in epilepsy FLAIR MRI are subtle and scarce, making joint image--mask generative modeling prone to instability and memorization. We propose SLIM-Diff, a compact joint diffusion model whose main contributions are (i) a single shared-bottleneck U-Net that enforces tight coupling between anatomy and lesion geometry from a 2-channel image+mask representation, and (ii) loss-geometry tuning via a tunable $L_p$ objective. As an internal baseline, we include the canonical DDPM-style objective ($\epsilon$-prediction with $L_2$ loss) and isolate the effect of prediction parameterization and $L_p$ geometry under a matched setup. Experiments show that $x_0$-prediction is consistently the strongest choice for joint synthesis, and that fractional sub-quadratic penalties ($L_{1.5}$) improve image fidelity while $L_2$ better preserves lesion mask morphology. Our code and model weights are available in this https URL</li>
</ul>

<h3>Title: Unifying Watermarking via Dimension-Aware Mapping</h3>
<ul>
<li><strong>Authors: </strong>Jiale Meng, Runyi Hu, Jie Zhang, Zheming Lu, Ivor Tsang, Tianwei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03373">https://arxiv.org/abs/2602.03373</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03373">https://arxiv.org/pdf/2602.03373</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03373]] Unifying Watermarking via Dimension-Aware Mapping(https://arxiv.org/abs/2602.03373)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, watermark</a></li>
<li><strong>Abstract: </strong>Deep watermarking methods often share similar encoder-decoder architectures, yet differ substantially in their functional behaviors. We propose DiM, a new multi-dimensional watermarking framework that formulates watermarking as a dimension-aware mapping problem, thereby unifying existing watermarking methods at the functional level. Under DiM, watermark information is modeled as payloads of different dimensionalities, including one-dimensional binary messages, two-dimensional spatial masks, and three-dimensional spatiotemporal structures. We find that the dimensional configuration of embedding and extraction largely determines the resulting watermarking behavior. Same-dimensional mappings preserve payload structure and support fine-grained control, while cross-dimensional mappings enable spatial or spatiotemporal localization. We instantiate DiM in the video domain, where spatiotemporal representations enable a broader set of dimension mappings. Experiments demonstrate that varying only the embedding and extraction dimensions, without architectural changes, leads to different watermarking capabilities, including spatiotemporal tamper localization, local embedding control, and recovery of temporal order under frame disruptions.</li>
</ul>

<h3>Title: SEW: Strengthening Robustness of Black-box DNN Watermarking via Specificity Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Huming Qiu, Mi Zhang, Junjie Sun, Peiyi Chen, Xiaohan Zhang, Min Yang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03377">https://arxiv.org/abs/2602.03377</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03377">https://arxiv.org/pdf/2602.03377</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03377]] SEW: Strengthening Robustness of Black-box DNN Watermarking via Specificity Enhancement(https://arxiv.org/abs/2602.03377)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, watermark</a></li>
<li><strong>Abstract: </strong>To ensure the responsible distribution and use of open-source deep neural networks (DNNs), DNN watermarking has become a crucial technique to trace and verify unauthorized model replication or misuse. In practice, black-box watermarks manifest as specific predictive behaviors for specially crafted samples. However, due to the generalization nature of DNNs, the keys to extracting the watermark message are not unique, which would provide attackers with more opportunities. Advanced attack techniques can reverse-engineer approximate replacements for the original watermark keys, enabling subsequent watermark removal. In this paper, we explore black-box DNN watermarking specificity, which refers to the accuracy of a watermark's response to a key. Using this concept, we introduce Specificity-Enhanced Watermarking (SEW), a new method that improves specificity by reducing the association between the watermark and approximate keys. Through extensive evaluation using three popular watermarking benchmarks, we validate that enhancing specificity significantly contributes to strengthening robustness against removal attacks. SEW effectively defends against six state-of-the-art removal attacks, while maintaining model usability and watermark verification performance.</li>
</ul>

<h3>Title: Dynamic Topology Optimization for Non-IID Data in Decentralized Learning</h3>
<ul>
<li><strong>Authors: </strong>Bart Cox, Antreas Ioannou, Jérémie Decouchant</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03383">https://arxiv.org/abs/2602.03383</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03383">https://arxiv.org/pdf/2602.03383</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03383]] Dynamic Topology Optimization for Non-IID Data in Decentralized Learning(https://arxiv.org/abs/2602.03383)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust</a></li>
<li><strong>Abstract: </strong>Decentralized learning (DL) enables a set of nodes to train a model collaboratively without central coordination, offering benefits for privacy and scalability. However, DL struggles to train a high accuracy model when the data distribution is non-independent and identically distributed (non-IID) and when the communication topology is static. To address these issues, we propose Morph, a topology optimization algorithm for DL. In Morph, nodes adaptively choose peers for model exchange based on maximum model dissimilarity. Morph maintains a fixed in-degree while dynamically reshaping the communication graph through gossip-based peer discovery and diversity-driven neighbor selection, thereby improving robustness to data heterogeneity. Experiments on CIFAR-10 and FEMNIST with up to 100 nodes show that Morph consistently outperforms static and epidemic baselines, while closely tracking the fully connected upper bound. On CIFAR-10, Morph achieves a relative improvement of 1.12x in test accuracy compared to the state-of-the-art baselines. On FEMNIST, Morph achieves an accuracy that is 1.08x higher than Epidemic Learning. Similar trends hold for 50 node deployments, where Morph narrows the gap to the fully connected upper bound within 0.5 percentage points on CIFAR-10. These results demonstrate that Morph achieves higher final accuracy, faster convergence, and more stable learning as quantified by lower inter-node variance, while requiring fewer communication rounds than baselines and no global knowledge.</li>
</ul>

<h3>Title: On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shumin Wang, Yuexiang Xie, Wenhao Zhang, Yuchang Sun, Yanxi Chen, Yaliang Li, Yanyong Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03392">https://arxiv.org/abs/2602.03392</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03392">https://arxiv.org/pdf/2602.03392</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03392]] On the Entropy Dynamics in Reinforcement Fine-Tuning of Large Language Models(https://arxiv.org/abs/2602.03392)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Entropy serves as a critical metric for measuring the diversity of outputs generated by large language models (LLMs), providing valuable insights into their exploration capabilities. While recent studies increasingly focus on monitoring and adjusting entropy to better balance exploration and exploitation in reinforcement fine-tuning (RFT), a principled understanding of entropy dynamics during this process is yet to be thoroughly investigated. In this paper, we establish a theoretical framework for analyzing the entropy dynamics during the RFT process, which begins with a discriminant expression that quantifies entropy change under a single logit update. This foundation enables the derivation of a first-order expression for entropy change, which can be further extended to the update formula of Group Relative Policy Optimization (GRPO). The corollaries and insights drawn from the theoretical analysis inspire the design of entropy control methods, and also offer a unified lens for interpreting various entropy-based methods in existing studies. We provide empirical evidence to support the main conclusions of our analysis and demonstrate the effectiveness of the derived entropy-discriminator clipping methods. This study yields novel insights into RFT training dynamics, providing theoretical support and practical strategies for optimizing the exploration-exploitation balance during LLM fine-tuning.</li>
</ul>

<h3>Title: Towards Distillation-Resistant Large Language Models: An Information-Theoretic Perspective</h3>
<ul>
<li><strong>Authors: </strong>Hao Fang, Tianyi Zhang, Tianqu Zhuang, Jiawei Kong, Kuofeng Gao, Bin Chen, Leqi Liang, Shu-Tao Xia, Ke Xu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03396">https://arxiv.org/abs/2602.03396</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03396">https://arxiv.org/pdf/2602.03396</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03396]] Towards Distillation-Resistant Large Language Models: An Information-Theoretic Perspective(https://arxiv.org/abs/2602.03396)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, defense, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Proprietary large language models (LLMs) embody substantial economic value and are generally exposed only as black-box APIs, yet adversaries can still exploit their outputs to extract knowledge via distillation. Existing defenses focus exclusively on text-based distillation, leaving the important logit-based distillation largely unexplored. In this work, we analyze this problem and present an effective solution from an information-theoretic perspective. We characterize distillation-relevant information in teacher outputs using the conditional mutual information (CMI) between teacher logits and input queries conditioned on ground-truth labels. This quantity captures contextual information beneficial for model extraction, motivating us to defend distillation via CMI minimization. Guided by our theoretical analysis, we propose learning a transformation matrix that purifies the original outputs to enhance distillation resistance. We further derive a CMI-inspired anti-distillation objective to optimize this transformation, which effectively removes distillation-relevant information while preserving output utility. Extensive experiments across multiple LLMs and strong distillation algorithms demonstrate that the proposed method significantly degrades distillation performance while preserving task accuracy, effectively protecting models' intellectual property.</li>
</ul>

<h3>Title: UnHype: CLIP-Guided Hypernetworks for Dynamic LoRA Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Piotr Wójcik, Maksym Petrenko, Wojciech Gromski, Przemysław Spurek, Maciej Zieba</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03410">https://arxiv.org/abs/2602.03410</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03410">https://arxiv.org/pdf/2602.03410</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03410]] UnHype: CLIP-Guided Hypernetworks for Dynamic LoRA Unlearning(https://arxiv.org/abs/2602.03410)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in large-scale diffusion models have intensified concerns about their potential misuse, particularly in generating realistic yet harmful or socially disruptive content. This challenge has spurred growing interest in effective machine unlearning, the process of selectively removing specific knowledge or concepts from a model without compromising its overall generative capabilities. Among various approaches, Low-Rank Adaptation (LoRA) has emerged as an effective and efficient method for fine-tuning models toward targeted unlearning. However, LoRA-based methods often exhibit limited adaptability to concept semantics and struggle to balance removing closely related concepts with maintaining generalization across broader meanings. Moreover, these methods face scalability challenges when multiple concepts must be erased simultaneously. To address these limitations, we introduce UnHype, a framework that incorporates hypernetworks into single- and multi-concept LoRA training. The proposed architecture can be directly plugged into Stable Diffusion as well as modern flow-based text-to-image models, where it demonstrates stable training behavior and effective concept control. During inference, the hypernetwork dynamically generates adaptive LoRA weights based on the CLIP embedding, enabling more context-aware, scalable unlearning. We evaluate UnHype across several challenging tasks, including object erasure, celebrity erasure, and explicit content removal, demonstrating its effectiveness and versatility. Repository: this https URL.</li>
</ul>

<h3>Title: Verified Critical Step Optimization for LLM Agents</h3>
<ul>
<li><strong>Authors: </strong>Mukai Li, Qingcheng Zeng, Tianqing Fang, Zhenwen Liang, Linfeng Song, Qi Liu, Haitao Mi, Dong Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03412">https://arxiv.org/abs/2602.03412</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03412">https://arxiv.org/pdf/2602.03412</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03412]] Verified Critical Step Optimization for LLM Agents(https://arxiv.org/abs/2602.03412)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As large language model agents tackle increasingly complex long-horizon tasks, effective post-training becomes critical. Prior work faces fundamental challenges: outcome-only rewards fail to precisely attribute credit to intermediate steps, estimated step-level rewards introduce systematic noise, and Monte Carlo sampling approaches for step reward estimation incur prohibitive computational cost. Inspired by findings that only a small fraction of high-entropy tokens drive effective RL for reasoning, we propose Critical Step Optimization (CSO), which focuses preference learning on verified critical steps, decision points where alternate actions demonstrably flip task outcomes from failure to success. Crucially, our method starts from failed policy trajectories rather than expert demonstrations, directly targeting the policy model's weaknesses. We use a process reward model (PRM) to identify candidate critical steps, leverage expert models to propose high-quality alternatives, then continue execution from these alternatives using the policy model itself until task completion. Only alternatives that the policy successfully executes to correct outcomes are verified and used as DPO training data, ensuring both quality and policy reachability. This yields fine-grained, verifiable supervision at critical decisions while avoiding trajectory-level coarseness and step-level noise. Experiments on GAIA-Text-103 and XBench-DeepSearch show that CSO achieves 37% and 26% relative improvement over the SFT baseline and substantially outperforms other post-training methods, while requiring supervision at only 16% of trajectory steps. This demonstrates the effectiveness of selective verification-based learning for agent post-training.</li>
</ul>

<h3>Title: Socratic-Geo: Synthetic Data Generation and Geometric Reasoning via Multi-Agent Interaction</h3>
<ul>
<li><strong>Authors: </strong>Zhengbo Jiao, Shaobo Wang, Zifan Zhang, Wei Wang, Bing Zhao, Hu Wei, Linfeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03414">https://arxiv.org/abs/2602.03414</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03414">https://arxiv.org/pdf/2602.03414</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03414]] Socratic-Geo: Synthetic Data Generation and Geometric Reasoning via Multi-Agent Interaction(https://arxiv.org/abs/2602.03414)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have significantly advanced vision-language understanding. However, even state-of-the-art models struggle with geometric reasoning, revealing a critical bottleneck: the extreme scarcity of high-quality image-text pairs. Human annotation is prohibitively expensive, while automated methods fail to ensure fidelity and training effectiveness. Existing approaches either passively adapt to available images or employ inefficient random exploration with filtering, decoupling generation from learning needs. We propose Socratic-Geo, a fully autonomous framework that dynamically couples data synthesis with model learning through multi-agent interaction. The Teacher agent generates parameterized Python scripts with reflective feedback (Reflect for solvability, RePI for visual validity), ensuring image-text pair purity. The Solver agent optimizes reasoning through preference learning, with failure paths guiding Teacher's targeted augmentation. Independently, the Generator learns image generation capabilities on accumulated "image-code-instruction" triplets, distilling programmatic drawing intelligence into visual generation. Starting from only 108 seed problems, Socratic-Solver achieves 49.11 on six benchmarks using one-quarter of baseline data, surpassing strong baselines by 2.43 points. Socratic-Generator achieves 42.4% on GenExam, establishing new state-of-the-art for open-source models, surpassing Seedream-4.0 (39.8%) and approaching Gemini-2.5-Flash-Image (43.1%).</li>
</ul>

<h3>Title: Origin Lens: A Privacy-First Mobile Framework for Cryptographic Image Provenance and AI Detection</h3>
<ul>
<li><strong>Authors: </strong>Alexander Loth, Dominique Conceicao Rosario, Peter Ebinger, Martin Kappes, Marc-Oliver Pahl</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV, cs.CY, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03423">https://arxiv.org/abs/2602.03423</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03423">https://arxiv.org/pdf/2602.03423</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03423]] Origin Lens: A Privacy-First Mobile Framework for Cryptographic Image Provenance and AI Detection(https://arxiv.org/abs/2602.03423)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, generative</a></li>
<li><strong>Abstract: </strong>The proliferation of generative AI poses challenges for information integrity assurance, requiring systems that connect model governance with end-user verification. We present Origin Lens, a privacy-first mobile framework that targets visual disinformation through a layered verification architecture. Unlike server-side detection systems, Origin Lens performs cryptographic image provenance verification and AI detection locally on the device via a Rust/Flutter hybrid architecture. Our system integrates multiple signals - including cryptographic provenance, generative model fingerprints, and optional retrieval-augmented verification - to provide users with graded confidence indicators at the point of consumption. We discuss the framework's alignment with regulatory requirements (EU AI Act, DSA) and its role in verification infrastructure that complements platform-level mechanisms.</li>
</ul>

<h3>Title: Hierarchical Concept-to-Appearance Guidance for Multi-Subject Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Yijia Xu, Zihao Wang, Jinshi Cui</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03448">https://arxiv.org/abs/2602.03448</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03448">https://arxiv.org/pdf/2602.03448</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03448]] Hierarchical Concept-to-Appearance Guidance for Multi-Subject Image Generation(https://arxiv.org/abs/2602.03448)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Multi-subject image generation aims to synthesize images that faithfully preserve the identities of multiple reference subjects while following textual instructions. However, existing methods often suffer from identity inconsistency and limited compositional control, as they rely on diffusion models to implicitly associate text prompts with reference images. In this work, we propose Hierarchical Concept-to-Appearance Guidance (CAG), a framework that provides explicit, structured supervision from high-level concepts to fine-grained appearances. At the conceptual level, we introduce a VAE dropout training strategy that randomly omits reference VAE features, encouraging the model to rely more on robust semantic signals from a Visual Language Model (VLM) and thereby promoting consistent concept-level generation in the absence of complete appearance cues. At the appearance level, we integrate the VLM-derived correspondences into a correspondence-aware masked attention module within the Diffusion Transformer (DiT). This module restricts each text token to attend only to its matched reference regions, ensuring precise attribute binding and reliable multi-subject composition. Extensive experiments demonstrate that our method achieves state-of-the-art performance on the multi-subject image generation, substantially improving prompt following and subject consistency.</li>
</ul>

<h3>Title: Beyond Variance: Prompt-Efficient RLVR via Rare-Event Amplification and Bidirectional Pairing</h3>
<ul>
<li><strong>Authors: </strong>Xin Sheng, Jiaxin Li, Yujuan Pang, Ran Peng, Yong Ma</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03452">https://arxiv.org/abs/2602.03452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03452">https://arxiv.org/pdf/2602.03452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03452]] Beyond Variance: Prompt-Efficient RLVR via Rare-Event Amplification and Bidirectional Pairing(https://arxiv.org/abs/2602.03452)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning with verifiable rewards (RLVR) is effective for training large language models on deterministic outcome reasoning tasks. Prior work shows RLVR works with few prompts, but prompt selection is often based only on training-accuracy variance, leading to unstable optimization directions and weaker transfer. We revisit prompt selection from a mechanism-level view and argue that an effective minibatch should provide both (i) a reliable positive anchor and (ii) explicit negative learning signals from rare failures. Based on this principle, we propose \emph{positive--negative pairing}: at each update, we sample a hard-but-solvable $q^{+}$ and an easy-but-brittle prompt $q^{-}$(high success rate but not perfect), characterized by low and high empirical success rates under multiple rollouts. We further introduce Weighted GRPO, which reweights binary outcomes at the pair level and uses group-normalized advantages to amplify rare successes on $q^{+}$ into sharp positive guidance while turning rare failures on $q^{-}$ into strong negative penalties. This bidirectional signal provides informative learning feedback for both successes and failures, improving sample efficiency without suppressing exploration. On Qwen2.5-Math-7B, a single paired minibatch per update consistently outperforms a GRPO baseline that selects two prompts via commonly used variance-based selection heuristics: AIME~2025 Pass@8 improves from 16.8 to 22.2, and AMC23 Pass@64 from 94.0 to 97.0, while remaining competitive with large-scale RLVR trained from a pool of 1209 training prompts. Similar gains are observed on Qwen2.5-Math-7B-Instruct.</li>
</ul>

<h3>Title: Contextualized Visual Personalization in Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yeongtak Oh, Sangwon Yu, Junsung Park, Han Cheol Moon, Jisoo Mok, Sungroh Yoon</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03454">https://arxiv.org/abs/2602.03454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03454">https://arxiv.org/pdf/2602.03454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03454]] Contextualized Visual Personalization in Vision-Language Models(https://arxiv.org/abs/2602.03454)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Despite recent progress in vision-language models (VLMs), existing approaches often fail to generate personalized responses based on the user's specific experiences, as they lack the ability to associate visual inputs with a user's accumulated visual-textual context. We newly formalize this challenge as contextualized visual personalization, which requires the visual recognition and textual retrieval of personalized visual experiences by VLMs when interpreting new images. To address this issue, we propose CoViP, a unified framework that treats personalized image captioning as a core task for contextualized visual personalization and improves this capability through reinforcement-learning-based post-training and caption-augmented generation. We further introduce diagnostic evaluations that explicitly rule out textual shortcut solutions and verify whether VLMs truly leverage visual context. Extensive experiments demonstrate that existing open-source and proprietary VLMs exhibit substantial limitations, while CoViP not only improves personalized image captioning but also yields holistic gains across downstream personalization tasks. These results highlight CoViP as a crucial stage for enabling robust and generalizable contextualized visual personalization.</li>
</ul>

<h3>Title: Causal Inference on Networks under Misspecified Exposure Mappings: A Partial Identification Framework</h3>
<ul>
<li><strong>Authors: </strong>Maresa Schröder, Miruna Oprescu, Stefan Feuerriegel, Nathan Kallus</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03459">https://arxiv.org/abs/2602.03459</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03459">https://arxiv.org/pdf/2602.03459</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03459]] Causal Inference on Networks under Misspecified Exposure Mappings: A Partial Identification Framework(https://arxiv.org/abs/2602.03459)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Estimating treatment effects in networks is challenging, as each potential outcome depends on the treatments of all other nodes in the network. To overcome this difficulty, existing methods typically impose an exposure mapping that compresses the treatment assignments in the network into a low-dimensional summary. However, if this mapping is misspecified, standard estimators for direct and spillover effects can be severely biased. We propose a novel partial identification framework for causal inference on networks to assess the robustness of treatment effects under misspecifications of the exposure mapping. Specifically, we derive sharp upper and lower bounds on direct and spillover effects under such misspecifications. As such, our framework presents a novel application of causal sensitivity analysis to exposure mappings. We instantiate our framework for three canonical exposure settings widely used in practice: (i) weighted means of the neighborhood treatments, (ii) threshold-based exposure mappings, and (iii) truncated neighborhood interference in the presence of higher-order spillovers. Furthermore, we develop orthogonal estimators for these bounds and prove that the resulting bound estimates are valid, sharp, and efficient. Our experiments show the bounds remain informative and provide reliable conclusions under misspecification of exposure mappings.</li>
</ul>

<h3>Title: Reading Between the Code Lines: On the Use of Self-Admitted Technical Debt for Security Analysis</h3>
<ul>
<li><strong>Authors: </strong>Nicolás E. Díaz Ferreyra, Moritz Mock, Max Kretschmann, Barbara Russo, Mojtaba Shahin, Mansooreh Zahedi, Riccardo Scandariato</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.HC, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03470">https://arxiv.org/abs/2602.03470</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03470">https://arxiv.org/pdf/2602.03470</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03470]] Reading Between the Code Lines: On the Use of Self-Admitted Technical Debt for Security Analysis(https://arxiv.org/abs/2602.03470)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Static Analysis Tools (SATs) are central to security engineering activities, as they enable early identification of code weaknesses without requiring execution. However, their effectiveness is often limited by high false-positive rates and incomplete coverage of vulnerability classes. At the same time, developers frequently document security-related shortcuts and compromises as Self-Admitted Technical Debt (SATD) in software artifacts, such as code comments. While prior work has recognized SATD as a rich source of security information, it remains unclear whether -and in what ways- it is utilized during SAT-aided security analysis. OBJECTIVE: This work investigates the extent to which security-related SATD complements the output produced by SATs and helps bridge some of their well-known limitations. METHOD: We followed a mixed-methods approach consisting of (i) the analysis of a SATD-annotated vulnerability dataset using three state-of-the-art SATs and (ii) an online survey with 72 security practitioners. RESULTS: The combined use of all SATs flagged 114 of the 135 security-related SATD instances, spanning 24 distinct Common Weakness Enumeration (CWE) identifiers. A manual mapping of the SATD comments revealed 33 unique CWE types, 6 of which correspond to categories that SATs commonly overlook or struggle to detect (e.g., race conditions). Survey responses further suggest that developers frequently pair SAT outputs with SATD insights to better understand the impact and root causes of security weaknesses and to identify suitable fixes. IMPLICATIONS: Our findings show that such SATD-encoded information can be a meaningful complement to SAT-driven security analysis, while helping to overcome some of SATs' practical shortcomings.</li>
</ul>

<h3>Title: ScDiVa: Masked Discrete Diffusion for Joint Modeling of Single-Cell Identity and Expression</h3>
<ul>
<li><strong>Authors: </strong>Mingxuan Wang, Cheng Chen, Gaoyang Jiang, Zijia Ren, Chuangxin Zhao, Lu Shi, Yanbiao Ma</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03477">https://arxiv.org/abs/2602.03477</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03477">https://arxiv.org/pdf/2602.03477</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03477]] ScDiVa: Masked Discrete Diffusion for Joint Modeling of Single-Cell Identity and Expression(https://arxiv.org/abs/2602.03477)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Single-cell RNA-seq profiles are high-dimensional, sparse, and unordered, causing autoregressive generation to impose an artificial ordering bias and suffer from error accumulation. To address this, we propose scDiVa, a masked discrete diffusion foundation model that aligns generation with the dropout-like corruption process by defining a continuous-time forward masking mechanism in token space. ScDiVa features a bidirectional denoiser that jointly models discrete gene identities and continuous values, utilizing entropy-normalized serialization and a latent anchor token to maximize information efficiency and preserve global cell identity. The model is trained via depth-invariant time sampling and a dual denoising objective to simulate varying sparsity levels while ensuring precise recovery of both identity and magnitude. Pre-trained on 59 million cells, scDiVa achieves strong transfer performance across major benchmarks, including batch integration, cell type annotation, and perturbation response prediction. These results suggest that masked discrete diffusion serves as a biologically coherent and effective alternative to autoregression.</li>
</ul>

<h3>Title: DeepDFA: Injecting Temporal Logic in Deep Learning for Sequential Subsymbolic Applications</h3>
<ul>
<li><strong>Authors: </strong>Elena Umili, Francesco Argenziano, Roberto Capobianco</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03486">https://arxiv.org/abs/2602.03486</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03486">https://arxiv.org/pdf/2602.03486</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03486]] DeepDFA: Injecting Temporal Logic in Deep Learning for Sequential Subsymbolic Applications(https://arxiv.org/abs/2602.03486)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Integrating logical knowledge into deep neural network training is still a hard challenge, especially for sequential or temporally extended domains involving subsymbolic observations. To address this problem, we propose DeepDFA, a neurosymbolic framework that integrates high-level temporal logic - expressed as Deterministic Finite Automata (DFA) or Moore Machines - into neural architectures. DeepDFA models temporal rules as continuous, differentiable layers, enabling symbolic knowledge injection into subsymbolic domains. We demonstrate how DeepDFA can be used in two key settings: (i) static image sequence classification, and (ii) policy learning in interactive non-Markovian environments. Across extensive experiments, DeepDFA outperforms traditional deep learning models (e.g., LSTMs, GRUs, Transformers) and novel neuro-symbolic systems, achieving state-of-the-art results in temporal knowledge integration. These results highlight the potential of DeepDFA to bridge subsymbolic learning and symbolic reasoning in sequential tasks.</li>
</ul>

<h3>Title: Detecting and Explaining Malware Family Evolution Using Rule-Based Drift Analysis</h3>
<ul>
<li><strong>Authors: </strong>Olha Jurečková, Martin Jureček</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03489">https://arxiv.org/abs/2602.03489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03489">https://arxiv.org/pdf/2602.03489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03489]] Detecting and Explaining Malware Family Evolution Using Rule-Based Drift Analysis(https://arxiv.org/abs/2602.03489)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust</a></li>
<li><strong>Abstract: </strong>Malware detection and classification into families are critical tasks in cybersecurity, complicated by the continual evolution of malware to evade detection. This evolution introduces concept drift, in which the statistical properties of malware features change over time, reducing the effectiveness of static machine learning models. Understanding and explaining this drift is essential for maintaining robust and trustworthy malware detectors. In this paper, we propose an interpretable approach to concept drift detection. Our method uses a rule-based classifier to generate human-readable descriptions of both original and evolved malware samples belonging to the same malware family. By comparing the resulting rule sets using a similarity function, we can detect and quantify concept drift. Crucially, this comparison also identifies the specific features and feature values that have changed, providing clear explanations of how malware has evolved to bypass detection. Experimental results demonstrate that the proposed method not only accurately detects drift but also provides actionable insights into the behavior of evolving malware families, supporting both detection and threat analysis.</li>
</ul>

<h3>Title: Least but not Last: Fine-tuning Intermediate Principal Components for Better Performance-Forgetting Trade-Offs</h3>
<ul>
<li><strong>Authors: </strong>Alessio Quercia, Arya Bangun, Ira Assent, Hanno Scharr</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03493">https://arxiv.org/abs/2602.03493</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03493">https://arxiv.org/pdf/2602.03493</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03493]] Least but not Last: Fine-tuning Intermediate Principal Components for Better Performance-Forgetting Trade-Offs(https://arxiv.org/abs/2602.03493)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Low-Rank Adaptation (LoRA) methods have emerged as crucial techniques for adapting large pre-trained models to downstream tasks under computational and memory constraints. However, they face a fundamental challenge in balancing task-specific performance gains against catastrophic forgetting of pre-trained knowledge, where existing methods provide inconsistent recommendations. This paper presents a comprehensive analysis of the performance-forgetting trade-offs inherent in low-rank adaptation using principal components as initialization. Our investigation reveals that fine-tuning intermediate components leads to better balance and show more robustness to high learning rates than first (PiSSA) and last (MiLoRA) components in existing work. Building on these findings, we provide a practical approach for initialization of LoRA that offers superior trade-offs. We demonstrate in a thorough empirical study on a variety of computer vision and NLP tasks that our approach improves accuracy and reduces forgetting, also in continual learning scenarios.</li>
</ul>

<h3>Title: Lookahead Path Likelihood Optimization for Diffusion LLMs</h3>
<ul>
<li><strong>Authors: </strong>Xuejie Liu, Yap Vit Chun, Yitao Liang, Anji Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03496">https://arxiv.org/abs/2602.03496</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03496">https://arxiv.org/pdf/2602.03496</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03496]] Lookahead Path Likelihood Optimization for Diffusion LLMs(https://arxiv.org/abs/2602.03496)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Diffusion Large Language Models (dLLMs) support arbitrary-order generation, yet their inference performance critically depends on the unmasking order. Existing strategies rely on heuristics that greedily optimize local confidence, offering limited guidance for identifying unmasking paths that are globally consistent and accurate. To bridge this gap, we introduce path log-likelihood (Path LL), a trajectory-conditioned objective that strongly correlates with downstream accuracy and enables principled selection of unmasking paths. To optimize Path LL at inference time, we propose POKE, an efficient value estimator that predicts the expected future Path LL of a partial decoding trajectory. We then integrate this lookahead signal into POKE-SMC, a Sequential Monte Carlo-based search framework for dynamically identifying optimal unmasking paths. Extensive experiments across 6 reasoning tasks show that POKE-SMC consistently improves accuracy, achieving 2%--3% average gains over strong decoding-time scaling baselines at comparable inference overhead on LLaDA models and advancing the accuracy--compute Pareto frontier.</li>
</ul>

<h3>Title: Reparameterization Flow Policy Optimization</h3>
<ul>
<li><strong>Authors: </strong>Hai Zhong, Zhuoran Li, Xun Wang, Longbo Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03501">https://arxiv.org/abs/2602.03501</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03501">https://arxiv.org/pdf/2602.03501</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03501]] Reparameterization Flow Policy Optimization(https://arxiv.org/abs/2602.03501)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Reparameterization Policy Gradient (RPG) has emerged as a powerful paradigm for model-based reinforcement learning, enabling high sample efficiency by backpropagating gradients through differentiable dynamics. However, prior RPG approaches have been predominantly restricted to Gaussian policies, limiting their performance and failing to leverage recent advances in generative models. In this work, we identify that flow policies, which generate actions via differentiable ODE integration, naturally align with the RPG framework, a connection not established in prior work. However, naively exploiting this synergy proves ineffective, often suffering from training instability and a lack of exploration. We propose Reparameterization Flow Policy Optimization (RFO). RFO computes policy gradients by backpropagating jointly through the flow generation process and system dynamics, unlocking high sample efficiency without requiring intractable log-likelihood calculations. RFO includes two tailored regularization terms for stability and exploration. We also propose a variant of RFO with action chunking. Extensive experiments on diverse locomotion and manipulation tasks, involving both rigid and soft bodies with state or visual inputs, demonstrate the effectiveness of RFO. Notably, on a challenging locomotion task controlling a soft-body quadruped, RFO achieves almost $2\times$ the reward of the state-of-the-art baseline.</li>
</ul>

<h3>Title: Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models</h3>
<ul>
<li><strong>Authors: </strong>Arco van Breda, Erman Acar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03506">https://arxiv.org/abs/2602.03506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03506">https://arxiv.org/pdf/2602.03506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03506]] Explaining the Explainer: Understanding the Inner Workings of Transformer-based Symbolic Regression Models(https://arxiv.org/abs/2602.03506)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Following their success across many domains, transformers have also proven effective for symbolic regression (SR); however, the internal mechanisms underlying their generation of mathematical operators remain largely unexplored. Although mechanistic interpretability has successfully identified circuits in language and vision models, it has not yet been applied to SR. In this article, we introduce PATCHES, an evolutionary circuit discovery algorithm that identifies compact and correct circuits for SR. Using PATCHES, we isolate 28 circuits, providing the first circuit-level characterisation of an SR transformer. We validate these findings through a robust causal evaluation framework based on key notions such as faithfulness, completeness, and minimality. Our analysis shows that mean patching with performance-based evaluation most reliably isolates functionally correct circuits. In contrast, we demonstrate that direct logit attribution and probing classifiers primarily capture correlational features rather than causal ones, limiting their utility for circuit discovery. Overall, these results establish SR as a high-potential application domain for mechanistic interpretability and propose a principled methodology for circuit discovery.</li>
</ul>

<h3>Title: Learning to Reason Faithfully through Step-Level Faithfulness Maximization</h3>
<ul>
<li><strong>Authors: </strong>Runquan Gui, Yafu Li, Xiaoye Qu, Ziyan Liu, Yeqiu Cheng, Yu Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03507">https://arxiv.org/abs/2602.03507</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03507">https://arxiv.org/pdf/2602.03507</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03507]] Learning to Reason Faithfully through Step-Level Faithfulness Maximization(https://arxiv.org/abs/2602.03507)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning with Verifiable Rewards (RLVR) has markedly improved the performance of Large Language Models (LLMs) on tasks requiring multi-step reasoning. However, most RLVR pipelines rely on sparse outcome-based rewards, providing little supervision over intermediate steps and thus encouraging over-confidence and spurious reasoning, which in turn increases hallucinations. To address this, we propose FaithRL, a general reinforcement learning framework that directly optimizes reasoning faithfulness. We formalize a faithfulness-maximization objective and theoretically show that optimizing it mitigates over-confidence. To instantiate this objective, we introduce a geometric reward design and a faithfulness-aware advantage modulation mechanism that assigns step-level credit by penalizing unsupported steps while preserving valid partial derivations. Across diverse backbones and benchmarks, FaithRL consistently reduces hallucination rates while maintaining (and often improving) answer correctness. Further analysis confirms that FaithRL increases step-wise reasoning faithfulness and generalizes robustly. Our code is available at this https URL.</li>
</ul>

<h3>Title: Semantic Routing: Exploring Multi-Layer LLM Feature Weighting for Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Bozhou Li, Yushuo Guan, Haolin Li, Bohan Zeng, Yiyan Ji, Yue Ding, Pengfei Wan, Kun Gai, Yuanxing Zhang, Wentao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03510">https://arxiv.org/abs/2602.03510</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03510">https://arxiv.org/pdf/2602.03510</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03510]] Semantic Routing: Exploring Multi-Layer LLM Feature Weighting for Diffusion Transformers(https://arxiv.org/abs/2602.03510)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Recent DiT-based text-to-image models increasingly adopt LLMs as text encoders, yet text conditioning remains largely static and often utilizes only a single LLM layer, despite pronounced semantic hierarchy across LLM layers and non-stationary denoising dynamics over both diffusion time and network depth. To better match the dynamic process of DiT generation and thereby enhance the diffusion model's generative capability, we introduce a unified normalized convex fusion framework equipped with lightweight gates to systematically organize multi-layer LLM hidden states via time-wise, depth-wise, and joint fusion. Experiments establish Depth-wise Semantic Routing as the superior conditioning strategy, consistently improving text-image alignment and compositional generation (e.g., +9.97 on the GenAI-Bench Counting task). Conversely, we find that purely time-wise fusion can paradoxically degrade visual generation fidelity. We attribute this to a train-inference trajectory mismatch: under classifier-free guidance, nominal timesteps fail to track the effective SNR, causing semantically mistimed feature injection during inference. Overall, our results position depth-wise routing as a strong and effective baseline and highlight the critical need for trajectory-aware signals to enable robust time-dependent conditioning.</li>
</ul>

<h3>Title: Not All Negative Samples Are Equal: LLMs Learn Better from Plausible Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Zixiang Di, Jinyi Han, Shuo Zhang, Ying Liao, Zhi Li, Xiaofeng Ji, Yongqi Wang, Zheming Yang, Ming Gao, Bingdong Li, Jie Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03516">https://arxiv.org/abs/2602.03516</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03516">https://arxiv.org/pdf/2602.03516</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03516]] Not All Negative Samples Are Equal: LLMs Learn Better from Plausible Reasoning(https://arxiv.org/abs/2602.03516)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Learning from negative samples holds great promise for improving Large Language Model (LLM) reasoning capability, yet existing methods treat all incorrect responses as equally informative, overlooking the crucial role of sample quality. To address this, we propose Plausible Negative Samples (PNS), a method that synthesizes high-quality negative samples exhibiting expected format and structural coherence while ultimately yielding incorrect answers. PNS trains a dedicated model via reverse reinforcement learning (RL) guided by a composite reward combining format compliance, accuracy inversion, reward model assessment, and chain-of-thought evaluation, generating responses nearly indistinguishable from correct solutions. We further validate PNS as a plug-and-play data source for preference optimization across three backbone models on seven mathematical reasoning benchmarks. Results demonstrate that PNS consistently outperforms other negative sample synthesis methods, achieving an average improvement of 2.03% over RL-trained models.</li>
</ul>

<h3>Title: Rank-Learner: Orthogonal Ranking of Treatment Effects</h3>
<ul>
<li><strong>Authors: </strong>Henri Arno, Dennis Frauen, Emil Javurek, Thomas Demeester, Stefan Feuerriegel</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03517">https://arxiv.org/abs/2602.03517</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03517">https://arxiv.org/pdf/2602.03517</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03517]] Rank-Learner: Orthogonal Ranking of Treatment Effects(https://arxiv.org/abs/2602.03517)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Many decision-making problems require ranking individuals by their treatment effects rather than estimating the exact effect magnitudes. Examples include prioritizing patients for preventive care interventions, or ranking customers by the expected incremental impact of an advertisement. Surprisingly, while causal effect estimation has received substantial attention in the literature, the problem of directly learning rankings of treatment effects has largely remained unexplored. In this paper, we introduce Rank-Learner, a novel two-stage learner that directly learns the ranking of treatment effects from observational data. We first show that naive approaches based on precise treatment effect estimation solve a harder problem than necessary for ranking, while our Rank-Learner optimizes a pairwise learning objective that recovers the true treatment effect ordering, without explicit CATE estimation. We further show that our Rank-Learner is Neyman-orthogonal and thus comes with strong theoretical guarantees, including robustness to estimation errors in the nuisance functions. In addition, our Rank-Learner is model-agnostic, and can be instantiated with arbitrary machine learning models (e.g., neural networks). We demonstrate the effectiveness of our method through extensive experiments where Rank-Learner consistently outperforms standard CATE estimators and non-orthogonal ranking methods. Overall, we provide practitioners with a new, orthogonal two-stage learner for ranking individuals by their treatment effects.</li>
</ul>

<h3>Title: Live or Lie: Action-Aware Capsule Multiple Instance Learning for Risk Assessment in Live Streaming Platforms</h3>
<ul>
<li><strong>Authors: </strong>Yiran Qiao, Jing Chen, Xiang Ao, Qiwei Zhong, Yang Liu, Qing He</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03520">https://arxiv.org/abs/2602.03520</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03520">https://arxiv.org/pdf/2602.03520</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03520]] Live or Lie: Action-Aware Capsule Multiple Instance Learning for Risk Assessment in Live Streaming Platforms(https://arxiv.org/abs/2602.03520)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Live streaming has become a cornerstone of today's internet, enabling massive real-time social interactions. However, it faces severe risks arising from sparse, coordinated malicious behaviors among multiple participants, which are often concealed within normal activities and challenging to detect timely and accurately. In this work, we provide a pioneering study on risk assessment in live streaming rooms, characterized by weak supervision where only room-level labels are available. We formulate the task as a Multiple Instance Learning (MIL) problem, treating each room as a bag and defining structured user-timeslot capsules as instances. These capsules represent subsequences of user actions within specific time windows, encapsulating localized behavioral patterns. Based on this formulation, we propose AC-MIL, an Action-aware Capsule MIL framework that models both individual behaviors and group-level coordination patterns. AC-MIL captures multi-granular semantics and behavioral cues through a serial and parallel architecture that jointly encodes temporal dynamics and cross-user dependencies. These signals are integrated for robust room-level risk prediction, while also offering interpretable evidence at the behavior segment level. Extensive experiments on large-scale industrial datasets from Douyin demonstrate that AC-MIL significantly outperforms MIL and sequential baselines, establishing new state-of-the-art performance in room-level risk assessment for live streaming. Moreover, AC-MIL provides capsule-level interpretability, enabling identification of risky behavior segments as actionable evidence for intervention. The project page is available at: this https URL.</li>
</ul>

<h3>Title: Interpretable Logical Anomaly Classification via Constraint Decomposition and Instruction Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Xufei Zhang, Xinjiao Zhou, Ziling Deng, Dongdong Geng, Jianxiong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03530">https://arxiv.org/abs/2602.03530</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03530">https://arxiv.org/pdf/2602.03530</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03530]] Interpretable Logical Anomaly Classification via Constraint Decomposition and Instruction Fine-Tuning(https://arxiv.org/abs/2602.03530)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Logical anomalies are violations of predefined constraints on object quantity, spatial layout, and compositional relationships in industrial images. While prior work largely treats anomaly detection as a binary decision, such formulations cannot indicate which logical rule is broken and therefore offer limited value for quality assurance. We introduce Logical Anomaly Classification (LAC), a task that unifies anomaly detection and fine-grained violation classification in a single inference step. To tackle LAC, we propose LogiCls, a vision-language framework that decomposes complex logical constraints into a sequence of verifiable subqueries. We further present a data-centric instruction synthesis pipeline that generates chain-of-thought (CoT) supervision for these subqueries, coupling precise grounding annotations with diverse image-text augmentations to adapt vision language models (VLMs) to logic-sensitive reasoning. Training is stabilized by a difficulty-aware resampling strategy that emphasizes challenging subqueries and long tail constraint types. Extensive experiments demonstrate that LogiCls delivers robust, interpretable, and accurate industrial logical anomaly classification, providing both the predicted violation categories and their evidence trails.</li>
</ul>

<h3>Title: Robust Representation Learning in Masked Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Anika Shrivastava, Renu Rameshan, Samar Agnihotri</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03531">https://arxiv.org/abs/2602.03531</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03531">https://arxiv.org/pdf/2602.03531</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03531]] Robust Representation Learning in Masked Autoencoders(https://arxiv.org/abs/2602.03531)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Masked Autoencoders (MAEs) achieve impressive performance in image classification tasks, yet the internal representations they learn remain less understood. This work started as an attempt to understand the strong downstream classification performance of MAE. In this process we discover that representations learned with the pretraining and fine-tuning, are quite robust - demonstrating a good classification performance in the presence of degradations, such as blur and occlusions. Through layer-wise analysis of token embeddings, we show that pretrained MAE progressively constructs its latent space in a class-aware manner across network depth: embeddings from different classes lie in subspaces that become increasingly separable. We further observe that MAE exhibits early and persistent global attention across encoder layers, in contrast to standard Vision Transformers (ViTs). To quantify feature robustness, we introduce two sensitivity indicators: directional alignment between clean and perturbed embeddings, and head-wise retention of active features under degradations. These studies help establish the robust classification performance of MAEs.</li>
</ul>

<h3>Title: PnP-U3D: Plug-and-Play 3D Framework Bridging Autoregression and Diffusion for Unified Understanding and Generation</h3>
<ul>
<li><strong>Authors: </strong>Yongwei Chen, Tianyi Wei, Yushi Lan, Zhaoyang Lyu, Shangchen Zhou, Xudong Xu, Xingang Pan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03533">https://arxiv.org/abs/2602.03533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03533">https://arxiv.org/pdf/2602.03533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03533]] PnP-U3D: Plug-and-Play 3D Framework Bridging Autoregression and Diffusion for Unified Understanding and Generation(https://arxiv.org/abs/2602.03533)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, large language model</a></li>
<li><strong>Abstract: </strong>The rapid progress of large multimodal models has inspired efforts toward unified frameworks that couple understanding and generation. While such paradigms have shown remarkable success in 2D, extending them to 3D remains largely underexplored. Existing attempts to unify 3D tasks under a single autoregressive (AR) paradigm lead to significant performance degradation due to forced signal quantization and prohibitive training cost. Our key insight is that the essential challenge lies not in enforcing a unified autoregressive paradigm, but in enabling effective information interaction between generation and understanding while minimally compromising their inherent capabilities and leveraging pretrained models to reduce training cost. Guided by this perspective, we present the first unified framework for 3D understanding and generation that combines autoregression with diffusion. Specifically, we adopt an autoregressive next-token prediction paradigm for 3D understanding, and a continuous diffusion paradigm for 3D generation. A lightweight transformer bridges the feature space of large language models and the conditional space of 3D diffusion models, enabling effective cross-modal information exchange while preserving the priors learned by standalone models. Extensive experiments demonstrate that our framework achieves state-of-the-art performance across diverse 3D understanding and generation benchmarks, while also excelling in 3D editing tasks. These results highlight the potential of unified AR+diffusion models as a promising direction for building more general-purpose 3D intelligence.</li>
</ul>

<h3>Title: Can Large Language Models Generalize Procedures Across Representations?</h3>
<ul>
<li><strong>Authors: </strong>Fangru Lin, Valentin Hofmann, Xingchen Wan, Weixing Wang, Zifeng Ding, Anthony G. Cohn, Janet B. Pierrehumbert</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03542">https://arxiv.org/abs/2602.03542</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03542">https://arxiv.org/pdf/2602.03542</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03542]] Can Large Language Models Generalize Procedures Across Representations?(https://arxiv.org/abs/2602.03542)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are trained and tested extensively on symbolic representations such as code and graphs, yet real-world user tasks are often specified in natural language. To what extent can LLMs generalize across these representations? Here, we approach this question by studying isomorphic tasks involving procedures represented in code, graphs, and natural language (e.g., scheduling steps in planning). We find that training LLMs with popular post-training methods on graphs or code data alone does not reliably generalize to corresponding natural language tasks, while training solely on natural language can lead to inefficient performance gains. To address this gap, we propose a two-stage data curriculum that first trains on symbolic, then natural language data. The curriculum substantially improves model performance across model families and tasks. Remarkably, a 1.5B Qwen model trained by our method can closely match zero-shot GPT-4o in naturalistic planning. Finally, our analysis suggests that successful cross-representation generalization can be interpreted as a form of generative analogy, which our curriculum effectively encourages.</li>
</ul>

<h3>Title: SEAD: Self-Evolving Agent for Multi-Turn Service Dialogue</h3>
<ul>
<li><strong>Authors: </strong>Yuqin Dai, Ning Gao, Wei Zhang, Jie Wang, Zichen Luo, Jinpeng Wang, Yujie Wang, Ruiyuan Wu, Chaozheng Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03548">https://arxiv.org/abs/2602.03548</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03548">https://arxiv.org/pdf/2602.03548</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03548]] SEAD: Self-Evolving Agent for Multi-Turn Service Dialogue(https://arxiv.org/abs/2602.03548)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models have demonstrated remarkable capabilities in open-domain dialogues. However, current methods exhibit suboptimal performance in service dialogues, as they rely on noisy, low-quality human conversation data. This limitation arises from data scarcity and the difficulty of simulating authentic, goal-oriented user behaviors. To address these issues, we propose SEAD (Self-Evolving Agent for Service Dialogue), a framework that enables agents to learn effective strategies without large-scale human annotations. SEAD decouples user modeling into two components: a Profile Controller that generates diverse user states to manage training curriculum, and a User Role-play Model that focuses on realistic role-playing. This design ensures the environment provides adaptive training scenarios rather than acting as an unfair adversary. Experiments demonstrate that SEAD significantly outperforms Open-source Foundation Models and Closed-source Commercial Models, improving task completion rate by 17.6% and dialogue efficiency by 11.1%. Code is available at: this https URL.</li>
</ul>

<h3>Title: Assessing the Impact of Typological Features on Multilingual Machine Translation in the Age of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Vitalii Hirak, Jaap Jumelet, Arianna Bisazza</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03551">https://arxiv.org/abs/2602.03551</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03551">https://arxiv.org/pdf/2602.03551</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03551]] Assessing the Impact of Typological Features on Multilingual Machine Translation in the Age of Large Language Models(https://arxiv.org/abs/2602.03551)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite major advances in multilingual modeling, large quality disparities persist across languages. Besides the obvious impact of uneven training resources, typological properties have also been proposed to determine the intrinsic difficulty of modeling a language. The existing evidence, however, is mostly based on small monolingual language models or bilingual translation models trained from scratch. We expand on this line of work by analyzing two large pre-trained multilingual translation models, NLLB-200 and Tower+, which are state-of-the-art representatives of encoder-decoder and decoder-only machine translation, respectively. Based on a broad set of languages, we find that target language typology drives translation quality of both models, even after controlling for more trivial factors, such as data resourcedness and writing script. Additionally, languages with certain typological properties benefit more from a wider search of the output space, suggesting that such languages could profit from alternative decoding strategies beyond the standard left-to-right beam search. To facilitate further research in this area, we release a set of fine-grained typological properties for 212 languages of the FLORES+ MT evaluation benchmark.</li>
</ul>

<h3>Title: When Single Answer Is Not Enough: Rethinking Single-Step Retrosynthesis Benchmarks for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Bogdan Zagribelnyy, Ivan Ilin, Maksim Kuznetsov, Nikita Bondarev, Roman Schutski, Thomas MacDougall, Rim Shayakhmetov, Zulfat Miftakhutdinov, Mikolaj Mizera, Vladimir Aladinskiy, Alex Aliper, Alex Zhavoronkov</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CE, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03554">https://arxiv.org/abs/2602.03554</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03554">https://arxiv.org/pdf/2602.03554</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03554]] When Single Answer Is Not Enough: Rethinking Single-Step Retrosynthesis Benchmarks for LLMs(https://arxiv.org/abs/2602.03554)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent progress has expanded the use of large language models (LLMs) in drug discovery, including synthesis planning. However, objective evaluation of retrosynthesis performance remains limited. Existing benchmarks and metrics typically rely on published synthetic procedures and Top-K accuracy based on single ground-truth, which does not capture the open-ended nature of real-world synthesis planning. We propose a new benchmarking framework for single-step retrosynthesis that evaluates both general-purpose and chemistry-specialized LLMs using ChemCensor, a novel metric for chemical plausibility. By emphasizing plausibility over exact match, this approach better aligns with human synthesis planning practices. We also introduce CREED, a novel dataset comprising millions of ChemCensor-validated reaction records for LLM training, and use it to train a model that improves over the LLM baselines under this benchmark.</li>
</ul>

<h3>Title: Cut to the Mix: Simple Data Augmentation Outperforms Elaborate Ones in Limited Organ Segmentation Datasets</h3>
<ul>
<li><strong>Authors: </strong>Chang Liu, Fuxin Fan, Annette Schwarz, Andreas Maier</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03555">https://arxiv.org/abs/2602.03555</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03555">https://arxiv.org/pdf/2602.03555</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03555]] Cut to the Mix: Simple Data Augmentation Outperforms Elaborate Ones in Limited Organ Segmentation Datasets(https://arxiv.org/abs/2602.03555)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Multi-organ segmentation is a widely applied clinical routine and automated organ segmentation tools dramatically improve the pipeline of the radiologists. Recently, deep learning (DL) based segmentation models have shown the capacity to accomplish such a task. However, the training of the segmentation networks requires large amount of data with manual annotations, which is a major concern due to the data scarcity from clinic. Working with limited data is still common for researches on novel imaging modalities. To enhance the effectiveness of DL models trained with limited data, data augmentation (DA) is a crucial regularization technique. Traditional DA (TDA) strategies focus on basic intra-image operations, i.e. generating images with different orientations and intensity distributions. In contrast, the interimage and object-level DA operations are able to create new images from separate individuals. However, such DA strategies are not well explored on the task of multi-organ segmentation. In this paper, we investigated four possible inter-image DA strategies: CutMix, CarveMix, ObjectAug and AnatoMix, on two organ segmentation datasets. The result shows that CutMix, CarveMix and AnatoMix can improve the average dice score by 4.9, 2.0 and 1.9, compared with the state-of-the-art nnUNet without DA strategies. These results can be further improved by adding TDA strategies. It is revealed in our experiments that Cut-Mix is a robust but simple DA strategy to drive up the segmentation performance for multi-organ segmentation, even when CutMix produces intuitively 'wrong' images. Our implementation is publicly available for future benchmarks.</li>
</ul>

<h3>Title: ELIQ: A Label-Free Framework for Quality Assessment of Evolving AI-Generated Images</h3>
<ul>
<li><strong>Authors: </strong>Xinyue Li, Zhiming Xu, Zhichao Zhang, Zhaolin Cai, Sijing Wu, Xiongkuo Min, Yitong Chen, Guangtao Zhai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03558">https://arxiv.org/abs/2602.03558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03558">https://arxiv.org/pdf/2602.03558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03558]] ELIQ: A Label-Free Framework for Quality Assessment of Evolving AI-Generated Images(https://arxiv.org/abs/2602.03558)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>Generative text-to-image models are advancing at an unprecedented pace, continuously shifting the perceptual quality ceiling and rendering previously collected labels unreliable for newer generations. To address this, we present ELIQ, a Label-free Framework for Quality Assessment of Evolving AI-generated Images. Specifically, ELIQ focuses on visual quality and prompt-image alignment, automatically constructs positive and aspect-specific negative pairs to cover both conventional distortions and AIGC-specific distortion modes, enabling transferable supervision without human annotations. Building on these pairs, ELIQ adapts a pre-trained multimodal model into a quality-aware critic via instruction tuning and predicts two-dimensional quality using lightweight gated fusion and a Quality Query Transformer. Experiments across multiple benchmarks demonstrate that ELIQ consistently outperforms existing label-free methods, generalizes from AI-generated content (AIGC) to user-generated content (UGC) scenarios without modification, and paves the way for scalable and label-free quality assessment under continuously evolving generative models. The code will be released upon publication.</li>
</ul>

<h3>Title: CoGenCast: A Coupled Autoregressive-Flow Generative Framework for Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Yaguo Liu, Mingyue Cheng, Daoyu Wang, Xiaoyu Tao, Qi Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03564">https://arxiv.org/abs/2602.03564</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03564">https://arxiv.org/pdf/2602.03564</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03564]] CoGenCast: A Coupled Autoregressive-Flow Generative Framework for Time Series Forecasting(https://arxiv.org/abs/2602.03564)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, large language model</a></li>
<li><strong>Abstract: </strong>Time series forecasting can be viewed as a generative problem that requires both semantic understanding over contextual conditions and stochastic modeling of continuous temporal dynamics. Existing approaches typically rely on either autoregressive large language models (LLMs) for semantic context modeling or diffusion-like models for continuous probabilistic generation. However, neither method alone can adequately model both aspects simultaneously. In this work, we propose CoGenCast, a hybrid generative framework that couples pre-trained LLMs with flow-matching mechanism for effective time series forecasting. Specifically, we reconfigure pre-trained decoder-only LLMs into a native forecasting encoder-decoder backbone by modifying only the attention topology, enabling bidirectional context encoding and causal representation generation. Building on this, a flow-matching mechanism is further integrated to model temporal evolution, capturing continuous stochastic dynamics conditioned on the autoregressively generated representation. Notably, CoGenCast naturally supports multimodal forecasting and cross-domain unified training. Extensive experiments on multiple benchmarks show that CoGenCast consistently outperforms previous compared baselines. Code is available at this https URL.</li>
</ul>

<h3>Title: Riemannian Neural Optimal Transport</h3>
<ul>
<li><strong>Authors: </strong>Alessandro Micheli, Yueqi Cao, Anthea Monod, Samir Bhatt</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03566">https://arxiv.org/abs/2602.03566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03566">https://arxiv.org/pdf/2602.03566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03566]] Riemannian Neural Optimal Transport(https://arxiv.org/abs/2602.03566)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Computational optimal transport (OT) offers a principled framework for generative modeling. Neural OT methods, which use neural networks to learn an OT map (or potential) from data in an amortized way, can be evaluated out of sample after training, but existing approaches are tailored to Euclidean geometry. Extending neural OT to high-dimensional Riemannian manifolds remains an open challenge. In this paper, we prove that any method for OT on manifolds that produces discrete approximations of transport maps necessarily suffers from the curse of dimensionality: achieving a fixed accuracy requires a number of parameters that grows exponentially with the manifold dimension. Motivated by this limitation, we introduce Riemannian Neural OT (RNOT) maps, which are continuous neural-network parameterizations of OT maps on manifolds that avoid discretization and incorporate geometric structure by construction. Under mild regularity assumptions, we prove that RNOT maps approximate Riemannian OT maps with sub-exponential complexity in the dimension. Experiments on synthetic and real datasets demonstrate improved scalability and competitive performance relative to discretization-based baselines.</li>
</ul>

<h3>Title: Asymmetric Hierarchical Anchoring for Audio-Visual Joint Representation: Resolving Information Allocation Ambiguity for Robust Cross-Modal Generalization</h3>
<ul>
<li><strong>Authors: </strong>Bixing Wu, Yuhong Zhao, Zongli Ye, Jiachen Lian, Xiangyu Yue, Gopala Anumanchipalli</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03570">https://arxiv.org/abs/2602.03570</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03570">https://arxiv.org/pdf/2602.03570</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03570]] Asymmetric Hierarchical Anchoring for Audio-Visual Joint Representation: Resolving Information Allocation Ambiguity for Robust Cross-Modal Generalization(https://arxiv.org/abs/2602.03570)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Audio-visual joint representation learning under Cross-Modal Generalization (CMG) aims to transfer knowledge from a labeled source modality to an unlabeled target modality through a unified discrete representation space. Existing symmetric frameworks often suffer from information allocation ambiguity, where the absence of structural inductive bias leads to semantic-specific leakage across modalities. We propose Asymmetric Hierarchical Anchoring (AHA), which enforces directional information allocation by designating a structured semantic anchor within a shared hierarchy. In our instantiation, we exploit the hierarchical discrete representations induced by audio Residual Vector Quantization (RVQ) to guide video feature distillation into a shared semantic space. To ensure representational purity, we replace fragile mutual information estimators with a GRL-based adversarial decoupler that explicitly suppresses semantic leakage in modality-specific branches, and introduce Local Sliding Alignment (LSA) to encourage fine-grained temporal alignment across modalities. Extensive experiments on AVE and AVVP benchmarks demonstrate that AHA consistently outperforms symmetric baselines in cross-modal transfer. Additional analyses on talking-face disentanglement experiment further validate that the learned representations exhibit improved semantic consistency and disentanglement, indicating the broader applicability of the proposed framework.</li>
</ul>

<h3>Title: Use Graph When It Needs: Efficiently and Adaptively Integrating Retrieval-Augmented Generation with Graphs</h3>
<ul>
<li><strong>Authors: </strong>Su Dong, Qinggang Zhang, Yilin Xiao, Shengyuan Chen, Chuang Zhou, Xiao Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03578">https://arxiv.org/abs/2602.03578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03578">https://arxiv.org/pdf/2602.03578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03578]] Use Graph When It Needs: Efficiently and Adaptively Integrating Retrieval-Augmented Generation with Graphs(https://arxiv.org/abs/2602.03578)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) often struggle with knowledge-intensive tasks due to hallucinations and outdated parametric knowledge. While Retrieval-Augmented Generation (RAG) addresses this by integrating external corpora, its effectiveness is limited by fragmented information in unstructured domain documents. Graph-augmented RAG (GraphRAG) emerged to enhance contextual reasoning through structured knowledge graphs, yet paradoxically underperforms vanilla RAG in real-world scenarios, exhibiting significant accuracy drops and prohibitive latency despite gains on complex queries. We identify the rigid application of GraphRAG to all queries, regardless of complexity, as the root cause. To resolve this, we propose an efficient and adaptive GraphRAG framework called EA-GraphRAG that dynamically integrates RAG and GraphRAG paradigms through syntax-aware complexity analysis. Our approach introduces: (i) a syntactic feature constructor that parses each query and extracts a set of structural features; (ii) a lightweight complexity scorer that maps these features to a continuous complexity score; and (iii) a score-driven routing policy that selects dense RAG for low-score queries, invokes graph-based retrieval for high-score queries, and applies complexity-aware reciprocal rank fusion to handle borderline cases. Extensive experiments on a comprehensive benchmark, consisting of two single-hop and two multi-hop QA benchmarks, demonstrate that our EA-GraphRAG significantly improves accuracy, reduces latency, and achieves state-of-the-art performance in handling mixed scenarios involving both simple and complex queries.</li>
</ul>

<h3>Title: Don't believe everything you read: Understanding and Measuring MCP Behavior under Misleading Tool Descriptions</h3>
<ul>
<li><strong>Authors: </strong>Zhihao Li, Boyang Ma, Xuelong Dai, Minghui Xu, Yue Zhang, Biwei Yan, Kun Li</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03580">https://arxiv.org/abs/2602.03580</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03580">https://arxiv.org/pdf/2602.03580</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03580]] Don't believe everything you read: Understanding and Measuring MCP Behavior under Misleading Tool Descriptions(https://arxiv.org/abs/2602.03580)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>The Model Context Protocol (MCP) enables large language models to invoke external tools through natural-language descriptions, forming the foundation of many AI agent applications. However, MCP does not enforce consistency between documented tool behavior and actual code execution, even though MCP Servers often run with broad system privileges. This gap introduces a largely unexplored security risk. We study how mismatches between externally presented tool descriptions and underlying implementations systematically shape the mental models and decision-making behavior of intelligent agents. Specifically, we present the first large-scale study of description-code inconsistency in the MCP ecosystem. We design an automated static analysis framework and apply it to 10,240 real-world MCP Servers across 36 categories. Our results show that while most servers are highly consistent, approximately 13% exhibit substantial mismatches that can enable undocumented privileged operations, hidden state mutations, or unauthorized financial actions. We further observe systematic differences across application categories, popularity levels, and MCP marketplaces. Our findings demonstrate that description-code inconsistency is a concrete and prevalent attack surface in MCP-based AI agents, and motivate the need for systematic auditing and stronger transparency guarantees in future agent ecosystems.</li>
</ul>

<h3>Title: $V_0$: A Generalist Value Model for Any Policy at State Zero</h3>
<ul>
<li><strong>Authors: </strong>Yi-Kai Zhang, Zhiyuan Yao, Hongyan Hao, Yueqing Sun, Qi Gu, Hui Su, Xunliang Cai, De-Chuan Zhan, Han-Jia Ye</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03584">https://arxiv.org/abs/2602.03584</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03584">https://arxiv.org/pdf/2602.03584</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03584]] $V_0$: A Generalist Value Model for Any Policy at State Zero(https://arxiv.org/abs/2602.03584)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Policy gradient methods rely on a baseline to measure the relative advantage of an action, ensuring the model reinforces behaviors that outperform its current average capability. In the training of Large Language Models (LLMs) using Actor-Critic methods (e.g., PPO), this baseline is typically estimated by a Value Model (Critic) often as large as the policy model itself. However, as the policy continuously evolves, the value model requires expensive, synchronous incremental training to accurately track the shifting capabilities of the policy. To avoid this overhead, Group Relative Policy Optimization (GRPO) eliminates the coupled value model by using the average reward of a group of rollouts as the baseline; yet, this approach necessitates extensive sampling to maintain estimation stability. In this paper, we propose $V_0$, a Generalist Value Model capable of estimating the expected performance of any model on unseen prompts without requiring parameter updates. We reframe value estimation by treating the policy's dynamic capability as an explicit context input; specifically, we leverage a history of instruction-performance pairs to dynamically profile the model, departing from the traditional paradigm that relies on parameter fitting to perceive capability shifts. Focusing on value estimation at State Zero (i.e., the initial prompt, hence $V_0$), our model serves as a critical resource scheduler. During GRPO training, $V_0$ predicts success rates prior to rollout, allowing for efficient sampling budget allocation; during deployment, it functions as a router, dispatching instructions to the most cost-effective and suitable model. Empirical results demonstrate that $V_0$ significantly outperforms heuristic budget allocation and achieves a Pareto-optimal trade-off between performance and cost in LLM routing tasks.</li>
</ul>

<h3>Title: SlowFocus: Enhancing Fine-grained Temporal Understanding in Video LLM</h3>
<ul>
<li><strong>Authors: </strong>Ming Nie, Dan Ding, Chunwei Wang, Yuanfan Guo, Jianhua Han, Hang Xu, Li Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03589">https://arxiv.org/abs/2602.03589</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03589">https://arxiv.org/pdf/2602.03589</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03589]] SlowFocus: Enhancing Fine-grained Temporal Understanding in Video LLM(https://arxiv.org/abs/2602.03589)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated exceptional capabilities in text understanding, which has paved the way for their expansion into video LLMs (Vid-LLMs) to analyze video data. However, current Vid-LLMs struggle to simultaneously retain high-quality frame-level semantic information (i.e., a sufficient number of tokens per frame) and comprehensive video-level temporal information (i.e., an adequate number of sampled frames per video). This limitation hinders the advancement of Vid-LLMs towards fine-grained video understanding. To address this issue, we introduce the SlowFocus mechanism, which significantly enhances the equivalent sampling frequency without compromising the quality of frame-level visual tokens. SlowFocus begins by identifying the query-related temporal segment based on the posed question, then performs dense sampling on this segment to extract local high-frequency features. A multi-frequency mixing attention module is further leveraged to aggregate these local high-frequency details with global low-frequency contexts for enhanced temporal comprehension. Additionally, to tailor Vid-LLMs to this innovative mechanism, we introduce a set of training strategies aimed at bolstering both temporal grounding and detailed temporal reasoning capabilities. Furthermore, we establish FineAction-CGR, a benchmark specifically devised to assess the ability of Vid-LLMs to process fine-grained temporal understanding tasks. Comprehensive experiments demonstrate the superiority of our mechanism across both existing public video understanding benchmarks and our proposed FineAction-CGR.</li>
</ul>

<h3>Title: High-Resolution Underwater Camouflaged Object Detection: GBU-UCOD Dataset and Topology-Aware and Frequency-Decoupled Networks</h3>
<ul>
<li><strong>Authors: </strong>Wenji Wu, Shuo Ye, Yiyu Liu, Jiguang He, Zhuo Wang, Zitong Yu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03591">https://arxiv.org/abs/2602.03591</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03591">https://arxiv.org/pdf/2602.03591</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03591]] High-Resolution Underwater Camouflaged Object Detection: GBU-UCOD Dataset and Topology-Aware and Frequency-Decoupled Networks(https://arxiv.org/abs/2602.03591)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Underwater Camouflaged Object Detection (UCOD) is a challenging task due to the extreme visual similarity between targets and backgrounds across varying marine depths. Existing methods often struggle with topological fragmentation of slender creatures in the deep sea and the subtle feature extraction of transparent organisms. In this paper, we propose DeepTopo-Net, a novel framework that integrates topology-aware modeling with frequency-decoupled perception. To address physical degradation, we design the Water-Conditioned Adaptive Perceptor (WCAP), which employs Riemannian metric tensors to dynamically deform convolutional sampling fields. Furthermore, the Abyssal-Topology Refinement Module (ATRM) is developed to maintain the structural connectivity of spindly targets through skeletal priors. Specifically, we first introduce GBU-UCOD, the first high-resolution (2K) benchmark tailored for marine vertical zonation, filling the data gap for hadal and abyssal zones. Extensive experiments on MAS3K, RMAS, and our proposed GBU-UCOD datasets demonstrate that DeepTopo-Net achieves state-of-the-art performance, particularly in preserving the morphological integrity of complex underwater patterns. The datasets and codes will be released at this https URL.</li>
</ul>

<h3>Title: Refer-Agent: A Collaborative Multi-Agent System with Reasoning and Reflection for Referring Video Object Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Haichao Jiang, Tianming Liang, Wei-Shi Zheng, Jian-Fang Hu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03595">https://arxiv.org/abs/2602.03595</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03595">https://arxiv.org/pdf/2602.03595</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03595]] Refer-Agent: A Collaborative Multi-Agent System with Reasoning and Reflection for Referring Video Object Segmentation(https://arxiv.org/abs/2602.03595)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Referring Video Object Segmentation (RVOS) aims to segment objects in videos based on textual queries. Current methods mainly rely on large-scale supervised fine-tuning (SFT) of Multi-modal Large Language Models (MLLMs). However, this paradigm suffers from heavy data dependence and limited scalability against the rapid evolution of MLLMs. Although recent zero-shot approaches offer a flexible alternative, their performance remains significantly behind SFT-based methods, due to the straightforward workflow designs. To address these limitations, we propose \textbf{Refer-Agent}, a collaborative multi-agent system with alternating reasoning-reflection mechanisms. This system decomposes RVOS into step-by-step reasoning process. During reasoning, we introduce a Coarse-to-Fine frame selection strategy to ensure the frame diversity and textual relevance, along with a Dynamic Focus Layout that adaptively adjusts the agent's visual focus. Furthermore, we propose a Chain-of-Reflection mechanism, which employs a Questioner-Responder pair to generate a self-reflection chain, enabling the system to verify intermediate results and generates feedback for next-round reasoning refinement. Extensive experiments on five challenging benchmarks demonstrate that Refer-Agent significantly outperforms state-of-the-art methods, including both SFT-based models and zero-shot approaches. Moreover, Refer-Agent is flexible and enables fast integration of new MLLMs without any additional fine-tuning costs. Code will be released.</li>
</ul>

<h3>Title: SAGE-5GC: Security-Aware Guidelines for Evaluating Anomaly Detection in the 5G Core Network</h3>
<ul>
<li><strong>Authors: </strong>Cristian Manca, Christian Scano, Giorgio Piras, Fabio Brau, Maura Pintor, Battista Biggio</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03596">https://arxiv.org/abs/2602.03596</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03596">https://arxiv.org/pdf/2602.03596</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03596]] SAGE-5GC: Security-Aware Guidelines for Evaluating Anomaly Detection in the 5G Core Network(https://arxiv.org/abs/2602.03596)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>Machine learning-based anomaly detection systems are increasingly being adopted in 5G Core networks to monitor complex, high-volume traffic. However, most existing approaches are evaluated under strong assumptions that rarely hold in operational environments, notably the availability of independent and identically distributed (IID) data and the absence of adaptive this http URL this work, we study the problem of detecting 5G attacks \textit{in the wild}, focusing on realistic deployment settings. We propose a set of Security-Aware Guidelines for Evaluating anomaly detectors in 5G Core Network (SAGE-5GC), driven by domain knowledge and consideration of potential adversarial threats. Using a realistic 5G Core dataset, we first train several anomaly detectors and assess their baseline performance against standard 5GC control-plane cyberattacks targeting PFCP-based network this http URL then extend the evaluation to adversarial settings, where an attacker tries to manipulate the observable features of the network traffic to evade detection, under the constraint that the intended functionality of the malicious traffic is preserved. Starting from a selected set of controllable features, we analyze model sensitivity and adversarial robustness through randomized perturbations. Finally, we introduce a practical optimization strategy based on genetic algorithms that operates exclusively on attacker-controllable features and does not require prior knowledge of the underlying detection model. Our experimental results show that adversarially crafted attacks can substantially degrade detection performance, underscoring the need for robust, security-aware evaluation methodologies for anomaly detection in 5G networks deployed in the wild.</li>
</ul>

<h3>Title: A Lightweight Library for Energy-Based Joint-Embedding Predictive Architectures</h3>
<ul>
<li><strong>Authors: </strong>Basile Terver, Randall Balestriero, Megi Dervishi, David Fan, Quentin Garrido, Tushar Nagarajan, Koustuv Sinha, Wancong Zhang, Mike Rabbat, Yann LeCun, Amir Bar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03604">https://arxiv.org/abs/2602.03604</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03604">https://arxiv.org/pdf/2602.03604</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03604]] A Lightweight Library for Energy-Based Joint-Embedding Predictive Architectures(https://arxiv.org/abs/2602.03604)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present EB-JEPA, an open-source library for learning representations and world models using Joint-Embedding Predictive Architectures (JEPAs). JEPAs learn to predict in representation space rather than pixel space, avoiding the pitfalls of generative modeling while capturing semantically meaningful features suitable for downstream tasks. Our library provides modular, self-contained implementations that illustrate how representation learning techniques developed for image-level self-supervised learning can transfer to video, where temporal dynamics add complexity, and ultimately to action-conditioned world models, where the model must additionally learn to predict the effects of control inputs. Each example is designed for single-GPU training within a few hours, making energy-based self-supervised learning accessible for research and education. We provide ablations of JEA components on CIFAR-10. Probing these representations yields 91% accuracy, indicating that the model learns useful features. Extending to video, we include a multi-step prediction example on Moving MNIST that demonstrates how the same principles scale to temporal modeling. Finally, we show how these representations can drive action-conditioned world models, achieving a 97% planning success rate on the Two Rooms navigation task. Comprehensive ablations reveal the critical importance of each regularization component for preventing representation collapse. Code is available at this https URL.</li>
</ul>

<h3>Title: Controlling Output Rankings in Generative Engines for LLM-based Search</h3>
<ul>
<li><strong>Authors: </strong>Haibo Jin, Ruoxi Chen, Peiyan Zhang, Yifeng Luo, Huimin Zeng, Man Luo, Haohan Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03608">https://arxiv.org/abs/2602.03608</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03608">https://arxiv.org/pdf/2602.03608</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03608]] Controlling Output Rankings in Generative Engines for LLM-based Search(https://arxiv.org/abs/2602.03608)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>The way customers search for and choose products is changing with the rise of large language models (LLMs). LLM-based search, or generative engines, provides direct product recommendations to users, rather than traditional online search results that require users to explore options themselves. However, these recommendations are strongly influenced by the initial retrieval order of LLMs, which disadvantages small businesses and independent creators by limiting their visibility. In this work, we propose CORE, an optimization method that \textbf{C}ontrols \textbf{O}utput \textbf{R}ankings in g\textbf{E}nerative Engines for LLM-based search. Since the LLM's interactions with the search engine are black-box, CORE targets the content returned by search engines as the primary means of influencing output rankings. Specifically, CORE optimizes retrieved content by appending strategically designed optimization content to steer the ranking of outputs. We introduce three types of optimization content: string-based, reasoning-based, and review-based, demonstrating their effectiveness in shaping output rankings. To evaluate CORE in realistic settings, we introduce ProductBench, a large-scale benchmark with 15 product categories and 200 products per category, where each product is associated with its top-10 recommendations collected from Amazon's search interface. Extensive experiments on four LLMs with search capabilities (GPT-4o, Gemini-2.5, Claude-4, and Grok-3) demonstrate that CORE achieves an average Promotion Success Rate of \textbf{91.4\% @Top-5}, \textbf{86.6\% @Top-3}, and \textbf{80.3\% @Top-1}, across 15 product categories, outperforming existing ranking manipulation methods while preserving the fluency of optimized content.</li>
</ul>

<h3>Title: Explanations Leak: Membership Inference with Differential Privacy and Active Learning Defense</h3>
<ul>
<li><strong>Authors: </strong>Fatima Ezzeddine, Osama Zammar, Silvia Giordano, Omran Ayoub</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03611">https://arxiv.org/abs/2602.03611</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03611">https://arxiv.org/pdf/2602.03611</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03611]] Explanations Leak: Membership Inference with Differential Privacy and Active Learning Defense(https://arxiv.org/abs/2602.03611)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, attack, extraction, membership infer, explainability</a></li>
<li><strong>Abstract: </strong>Counterfactual explanations (CFs) are increasingly integrated into Machine Learning as a Service (MLaaS) systems to improve transparency; however, ML models deployed via APIs are already vulnerable to privacy attacks such as membership inference and model extraction, and the impact of explanations on this threat landscape remains insufficiently understood. In this work, we focus on the problem of how CFs expand the attack surface of MLaaS by strengthening membership inference attacks (MIAs), and on the need to design defense mechanisms that mitigate this emerging risk without undermining utility and explainability. First, we systematically analyze how exposing CFs through query-based APIs enables more effective shadow-based MIAs. Second, we propose a defense framework that integrates Differential Privacy (DP) with Active Learning (AL) to jointly reduce memorization and limit effective training data exposure. Finally, we conduct an extensive empirical evaluation to characterize the three-way trade-off between privacy leakage, predictive performance, and explanation quality. Our findings highlight the need to carefully balance transparency, utility, and privacy in the responsible deployment of explainable MLaaS systems.</li>
</ul>

<h3>Title: Multi-Objective Optimization for Synthetic-to-Real Style Transfer</h3>
<ul>
<li><strong>Authors: </strong>Estelle Chigot, Thomas Oberlin, Manon Huguenin, Dennis Wilson</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03625">https://arxiv.org/abs/2602.03625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03625">https://arxiv.org/pdf/2602.03625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03625]] Multi-Objective Optimization for Synthetic-to-Real Style Transfer(https://arxiv.org/abs/2602.03625)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Semantic segmentation networks require large amounts of pixel-level annotated data, which are costly to obtain for real-world images. Computer graphics engines can generate synthetic images alongside their ground-truth annotations. However, models trained on such images can perform poorly on real images due to the domain gap between real and synthetic images. Style transfer methods can reduce this difference by applying a realistic style to synthetic images. Choosing effective data transformations and their sequence is difficult due to the large combinatorial search space of style transfer operators. Using multi-objective genetic algorithms, we optimize pipelines to balance structural coherence and style similarity to target domains. We study the use of paired-image metrics on individual image samples during evolution to enable rapid pipeline evaluation, as opposed to standard distributional metrics that require the generation of many images. After optimization, we evaluate the resulting Pareto front using distributional metrics and segmentation performance. We apply this approach to standard datasets in synthetic-to-real domain adaptation: from the video game GTA5 to real image datasets Cityscapes and ACDC, focusing on adverse conditions. Results demonstrate that evolutionary algorithms can propose diverse augmentation pipelines adapted to different objectives. The contribution of this work is the formulation of style transfer as a sequencing problem suitable for evolutionary optimization and the study of efficient metrics that enable feasible search in this space. The source code is available at: this https URL.</li>
</ul>

<h3>Title: Ultra Fast PDE Solving via Physics Guided Few-step Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Cindy Xiangrui Kong, Yueqi Wang, Haoyang Zheng, Weijian Luo, Guang Lin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03627">https://arxiv.org/abs/2602.03627</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03627">https://arxiv.org/pdf/2602.03627</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03627]] Ultra Fast PDE Solving via Physics Guided Few-step Diffusion(https://arxiv.org/abs/2602.03627)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion-based models have demonstrated impressive accuracy and generalization in solving partial differential equations (PDEs). However, they still face significant limitations, such as high sampling costs and insufficient physical consistency, stemming from their many-step iterative sampling mechanism and lack of explicit physics constraints. To address these issues, we propose Phys-Instruct, a novel physics-guided distillation framework which not only (1) compresses a pre-trained diffusion PDE solver into a few-step generator via matching generator and prior diffusion distributions to enable rapid sampling, but also (2) enhances the physics consistency by explicitly injecting PDE knowledge through a PDE distillation guidance. Physic-Instruct is built upon a solid theoretical foundation, leading to a practical physics-constrained training objective that admits tractable gradients. Across five PDE benchmarks, Phys-Instruct achieves orders-of-magnitude faster inference while reducing PDE error by more than 8 times compared to state-of-the-art diffusion baselines. Moreover, the resulting unconditional student model functions as a compact prior, enabling efficient and physically consistent inference for various downstream conditional tasks. Our results indicate that Phys-Instruct is a novel, effective, and efficient framework for ultra-fast PDE solving powered by deep generative models.</li>
</ul>

<h3>Title: BIRDTurk: Adaptation of the BIRD Text-to-SQL Dataset to Turkish</h3>
<ul>
<li><strong>Authors: </strong>Burak Aktaş, Mehmet Can Baytekin, Süha Kağan Köse, Ömer İlbilgi, Elif Özge Yılmaz, Çağrı Toraman, Bilge Kaan Görür</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03633">https://arxiv.org/abs/2602.03633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03633">https://arxiv.org/pdf/2602.03633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03633]] BIRDTurk: Adaptation of the BIRD Text-to-SQL Dataset to Turkish(https://arxiv.org/abs/2602.03633)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Text-to-SQL systems have achieved strong performance on English benchmarks, yet their behavior in morphologically rich, low-resource languages remains largely unexplored. We introduce BIRDTurk, the first Turkish adaptation of the BIRD benchmark, constructed through a controlled translation pipeline that adapts schema identifiers to Turkish while strictly preserving the logical structure and execution semantics of SQL queries and databases. Translation quality is validated on a sample size determined by the Central Limit Theorem to ensure 95% confidence, achieving 98.15% accuracy on human-evaluated samples. Using BIRDTurk, we evaluate inference-based prompting, agentic multi-stage reasoning, and supervised fine-tuning. Our results reveal that Turkish introduces consistent performance degradation, driven by both structural linguistic divergence and underrepresentation in LLM pretraining, while agentic reasoning demonstrates stronger cross-lingual robustness. Supervised fine-tuning remains challenging for standard multilingual baselines but scales effectively with modern instruction-tuned models. BIRDTurk provides a controlled testbed for cross-lingual Text-to-SQL evaluation under realistic database conditions. We release the training and development splits to support future research.</li>
</ul>

<h3>Title: TRE: Encouraging Exploration in the Trust Region</h3>
<ul>
<li><strong>Authors: </strong>Chao Huang, Yujing Lu, Quangang Li, Shenghe Wang, Yan Wang, Yueyang Zhang, Long Xia, Jiashu Zhao, Zhiyuan Sun, Daiting Shi, Tingwen Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03635">https://arxiv.org/abs/2602.03635</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03635">https://arxiv.org/pdf/2602.03635</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03635]] TRE: Encouraging Exploration in the Trust Region(https://arxiv.org/abs/2602.03635)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Entropy regularization is a standard technique in reinforcement learning (RL) to enhance exploration, yet it yields negligible effects or even degrades performance in Large Language Models (LLMs). We attribute this failure to the cumulative tail risk inherent to LLMs with massive vocabularies and long generation horizons. In such environments, standard global entropy maximization indiscriminately dilutes probability mass into the vast tail of invalid tokens rather than focusing on plausible candidates, thereby disrupting coherent reasoning. To address this, we propose Trust Region Entropy (TRE), a method that encourages exploration strictly within the model's trust region. Extensive experiments across mathematical reasoning (MATH), combinatorial search (Countdown), and preference alignment (HH) tasks demonstrate that TRE consistently outperforms vanilla PPO, standard entropy regularization, and other exploration baselines. Our code is available at this https URL.</li>
</ul>

<h3>Title: CTTVAE: Latent Space Structuring for Conditional Tabular Data Generation on Imbalanced Datasets</h3>
<ul>
<li><strong>Authors: </strong>Milosh Devic, Jordan Gierschendorf, David Garson</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03641">https://arxiv.org/abs/2602.03641</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03641">https://arxiv.org/pdf/2602.03641</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03641]] CTTVAE: Latent Space Structuring for Conditional Tabular Data Generation on Imbalanced Datasets(https://arxiv.org/abs/2602.03641)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, transformer, generative</a></li>
<li><strong>Abstract: </strong>Generating synthetic tabular data under severe class imbalance is essential for domains where rare but high-impact events drive decision-making. However, most generative models either overlook minority groups or fail to produce samples that are useful for downstream learning. We introduce CTTVAE, a Conditional Transformer-based Tabular Variational Autoencoder equipped with two complementary mechanisms: (i) a class-aware triplet margin loss that restructures the latent space for sharper intra-class compactness and inter-class separation, and (ii) a training-by-sampling strategy that adaptively increases exposure to underrepresented groups. Together, these components form CTTVAE+TBS, a framework that consistently yields more representative and utility-aligned samples without destabilizing training. Across six real-world benchmarks, CTTVAE+TBS achieves the strongest downstream utility on minority classes, often surpassing models trained on the original imbalanced data while maintaining competitive fidelity and bridging the gap for privacy for interpolation-based sampling methods and deep generative methods. Ablation studies further confirm that both latent structuring and targeted sampling contribute to these gains. By explicitly prioritizing downstream performance in rare categories, CTTVAE+TBS provides a robust and interpretable solution for conditional tabular data generation, with direct applicability to industries such as healthcare, fraud detection, and predictive maintenance where even small gains in minority cases can be critical.</li>
</ul>

<h3>Title: Reinforcement Fine-Tuning for History-Aware Dense Retriever in RAG</h3>
<ul>
<li><strong>Authors: </strong>Yicheng Zhang, Zhen Qin, Zhaomin Wu, Wenqi Zhang, Shuiguang Deng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03645">https://arxiv.org/abs/2602.03645</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03645">https://arxiv.org/pdf/2602.03645</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03645]] Reinforcement Fine-Tuning for History-Aware Dense Retriever in RAG(https://arxiv.org/abs/2602.03645)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) enables large language models (LLMs) to produce evidence-based responses, and its performance hinges on the matching between the retriever and LLMs. Retriever optimization has emerged as an efficient alternative to fine-tuning LLMs. However, existing solutions suffer from objective mismatch between retriever optimization and the goal of RAG pipeline. Reinforcement learning (RL) provides a promising solution to address this limitation, yet applying RL to retriever optimization introduces two fundamental challenges: 1) the deterministic retrieval is incompatible with RL formulations, and 2) state aliasing arises from query-only retrieval in multi-hop reasoning. To address these challenges, we replace deterministic retrieval with stochastic sampling and formulate RAG as a Markov decision process, making retriever optimizable by RL. Further, we incorporate retrieval history into the state at each retrieval step to mitigate state aliasing. Extensive experiments across diverse RAG pipelines, datasets, and retriever scales demonstrate consistent improvements of our approach in RAG performance.</li>
</ul>

<h3>Title: Can Developers rely on LLMs for Secure IaC Development?</h3>
<ul>
<li><strong>Authors: </strong>Ehsan Firouzi, Shardul Bhatt, Mohammad Ghafari</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03648">https://arxiv.org/abs/2602.03648</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03648">https://arxiv.org/pdf/2602.03648</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03648]] Can Developers rely on LLMs for Secure IaC Development?(https://arxiv.org/abs/2602.03648)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>We investigated the capabilities of GPT-4o and Gemini 2.0 Flash for secure Infrastructure as Code (IaC) development. For security smell detection, on the Stack Overflow dataset, which primarily contains small, simplified code snippets, the models detected at least 71% of security smells when prompted to analyze code from a security perspective (general prompt). With a guided prompt (adding clear, step-by-step instructions), this increased to 78%.In GitHub repositories, which contain complete, real-world project scripts, a general prompt was less effective, leaving more than half of the smells undetected. However, with the guided prompt, the models uncovered at least 67% of the smells. For secure code generation, we prompted LLMs with 89 vulnerable synthetic scenarios and observed that only 7% of the generated scripts were secure. Adding an explicit instruction to generate secure code increased GPT secure output rate to 17%, while Gemini changed little (8%). These results highlight the need for further research to improve LLMs' capabilities in assisting developers with secure IaC development.</li>
</ul>

<h3>Title: RAGTurk: Best Practices for Retrieval Augmented Generation in Turkish</h3>
<ul>
<li><strong>Authors: </strong>Süha Kağan Köse, Mehmet Can Baytekin, Burak Aktaş, Bilge Kaan Görür, Evren Ayberk Munis, Deniz Yılmaz, Muhammed Yusuf Kartal, Çağrı Toraman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03652">https://arxiv.org/abs/2602.03652</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03652">https://arxiv.org/pdf/2602.03652</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03652]] RAGTurk: Best Practices for Retrieval Augmented Generation in Turkish(https://arxiv.org/abs/2602.03652)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) enhances LLM factuality, yet design guidance remains English-centric, limiting insights for morphologically rich languages like Turkish. We address this by constructing a comprehensive Turkish RAG dataset derived from Turkish Wikipedia and CulturaX, comprising question-answer pairs and relevant passage chunks. We benchmark seven stages of the RAG pipeline, from query transformation and reranking to answer refinement, without task-specific fine-tuning. Our results show that complex methods like HyDE maximize accuracy (85%) that is considerably higher than the baseline (78.70%). Also a Pareto-optimal configuration using Cross-encoder Reranking and Context Augmentation achieves comparable performance (84.60%) with much lower cost. We further demonstrate that over-stacking generative modules can degrade performance by distorting morphological cues, whereas simple query clarification with robust reranking offers an effective solution.</li>
</ul>

<h3>Title: Reference-Free EM Validation Flow for Detecting Triggered Hardware Trojans</h3>
<ul>
<li><strong>Authors: </strong>Mahsa Tahghigh, Hassan Salmani</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03666">https://arxiv.org/abs/2602.03666</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03666">https://arxiv.org/pdf/2602.03666</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03666]] Reference-Free EM Validation Flow for Detecting Triggered Hardware Trojans(https://arxiv.org/abs/2602.03666)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Hardware Trojans (HTs) threaten the trust and reliability of integrated circuits (ICs), particularly when triggered HTs remain dormant during standard testing and activate only under rare conditions. Existing electromagnetic (EM) side-channel-based detection techniques often rely on golden references or labeled data, which are infeasible in modern distributed manufacturing. This paper introduces a reference-free, design-agnostic framework for detecting triggered HTs directly from post-silicon EM emissions. The proposed flow converts each EM trace into a time-frequency scalogram using Continuous Wavelet Transform (CWT), extracts discriminative features through a convolutional neural network (CNN), reduces dimensionality with principal component analysis (PCA), and applies Bayesian Gaussian Mixture Modeling (BGMM) for unsupervised probabilistic clustering. The framework quantifies detection confidence using posterior-based metrics (alpha_{post}, beta_{post}), Bayesian information criterion (Delta BIC), and Mahalanobis cluster separation (D), enabling interpretable anomaly decisions without golden data. Experimental validation on AES-128 designs embedded with four different HTs demonstrates high separability between HT-free and HT-activated conditions and robustness to PCA variance thresholds. The results highlight the method's scalability, statistical interpretability, and potential for extension to runtime and in-field HT monitoring in trusted microelectronics.</li>
</ul>

<h3>Title: Efficient Sequential Neural Network with Spatial-Temporal Attention and Linear LSTM for Robust Lane Detection Using Multi-Frame Images</h3>
<ul>
<li><strong>Authors: </strong>Sandeep Patil, Yongqi Dong, Haneen Farah, Hans Hellendoorn</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03669">https://arxiv.org/abs/2602.03669</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03669">https://arxiv.org/pdf/2602.03669</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03669]] Efficient Sequential Neural Network with Spatial-Temporal Attention and Linear LSTM for Robust Lane Detection Using Multi-Frame Images(https://arxiv.org/abs/2602.03669)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Lane detection is a crucial perception task for all levels of automated vehicles (AVs) and Advanced Driver Assistance Systems, particularly in mixed-traffic environments where AVs must interact with human-driven vehicles (HDVs) and challenging traffic scenarios. Current methods lack versatility in delivering accurate, robust, and real-time compatible lane detection, especially vision-based methods often neglect critical regions of the image and their spatial-temporal (ST) salience, leading to poor performance in difficult circumstances such as serious occlusion and dazzle lighting. This study introduces a novel sequential neural network model with a spatial-temporal attention mechanism to focus on key features of lane lines and exploit salient ST correlations among continuous image frames. The proposed model, built on a standard encoder-decoder structure and common neural network backbones, is trained and evaluated on three large-scale open-source datasets. Extensive experiments demonstrate the strength and robustness of the proposed model, outperforming state-of-the-art methods in various testing scenarios. Furthermore, with the ST attention mechanism, the developed sequential neural network models exhibit fewer parameters and reduced Multiply-Accumulate Operations (MACs) compared to baseline sequential models, highlighting their computational efficiency. Relevant data, code, and models are released at this https URL.</li>
</ul>

<h3>Title: mopri - An Analysis Framework for Unveiling Privacy Violations in Mobile Apps</h3>
<ul>
<li><strong>Authors: </strong>Cornell Ziepel, Stephan Escher, Sebastian Rehms, Stefan Köpsell</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03671">https://arxiv.org/abs/2602.03671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03671">https://arxiv.org/pdf/2602.03671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03671]] mopri - An Analysis Framework for Unveiling Privacy Violations in Mobile Apps(https://arxiv.org/abs/2602.03671)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, robust</a></li>
<li><strong>Abstract: </strong>Everyday services of society increasingly rely on mobile applications, resulting in a conflicting situation between the possibility of participation on the one side and user privacy and digital freedom on the other. In order to protect users' rights to informational self-determination, regulatory approaches for the collection and processing of personal data have been developed, such as the EU's GDPR. However, inspecting the compliance of mobile apps with privacy regulations remains difficult. Thus, in order to enable end users and enforcement bodies to verify and enforce data protection compliance, we propose mopri, a conceptual framework designed for analyzing the behavior of mobile apps through a comprehensive, adaptable, and user-centered approach. Recognizing the gaps in existing frameworks, mopri serves as a foundation for integrating various analysis tools into a streamlined, modular pipeline that employs static and dynamic analysis methods. Building on this concept, a prototype has been developed which effectively extracts permissions and tracking libraries while employing robust methods for dynamic traffic recording and decryption. Additionally, it incorporates result enrichment and reporting features that enhance the clarity and usability of the analysis outcomes. The prototype showcases the feasibility of a holistic and modular approach to privacy analysis, emphasizing the importance of continuous adaptation to the evolving challenges presented by the mobile app ecosystem.</li>
</ul>

<h3>Title: Referring Industrial Anomaly Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Pengfei Yue, Xiaokang Jiang, Yilin Lu, Jianghang Lin, Shengchuan Zhang, Liujuan Cao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03673">https://arxiv.org/abs/2602.03673</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03673">https://arxiv.org/pdf/2602.03673</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03673]] Referring Industrial Anomaly Segmentation(https://arxiv.org/abs/2602.03673)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Industrial Anomaly Detection (IAD) is vital for manufacturing, yet traditional methods face significant challenges: unsupervised approaches yield rough localizations requiring manual thresholds, while supervised methods overfit due to scarce, imbalanced data. Both suffer from the "One Anomaly Class, One Model" limitation. To address this, we propose Referring Industrial Anomaly Segmentation (RIAS), a paradigm leveraging language to guide detection. RIAS generates precise masks from text descriptions without manual thresholds and uses universal prompts to detect diverse anomalies with a single model. We introduce the MVTec-Ref dataset to support this, designed with diverse referring expressions and focusing on anomaly patterns, notably with 95% small anomalies. We also propose the Dual Query Token with Mask Group Transformer (DQFormer) benchmark, enhanced by Language-Gated Multi-Level Aggregation (LMA) to improve multi-scale segmentation. Unlike traditional methods using redundant queries, DQFormer employs only "Anomaly" and "Background" tokens for efficient visual-textual integration. Experiments demonstrate RIAS's effectiveness in advancing IAD toward open-set capabilities. Code: this https URL.</li>
</ul>

<h3>Title: Instruction Anchors: Dissecting the Causal Dynamics of Modality Arbitration</h3>
<ul>
<li><strong>Authors: </strong>Yu Zhang, Mufan Xu, Xuefeng Bai, Kehai chen, Pengfei Zhang, Yang Xiang, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03677">https://arxiv.org/abs/2602.03677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03677">https://arxiv.org/pdf/2602.03677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03677]] Instruction Anchors: Dissecting the Causal Dynamics of Modality Arbitration(https://arxiv.org/abs/2602.03677)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Modality following serves as the capacity of multimodal large language models (MLLMs) to selectively utilize multimodal contexts based on user instructions. It is fundamental to ensuring safety and reliability in real-world deployments. However, the underlying mechanisms governing this decision-making process remain poorly understood. In this paper, we investigate its working mechanism through an information flow lens. Our findings reveal that instruction tokens function as structural anchors for modality arbitration: Shallow attention layers perform non-selective information transfer, routing multimodal cues to these anchors as a latent buffer; Modality competition is resolved within deep attention layers guided by the instruction intent, while MLP layers exhibit semantic inertia, acting as an adversarial force. Furthermore, we identify a sparse set of specialized attention heads that drive this arbitration. Causal interventions demonstrate that manipulating a mere $5\%$ of these critical heads can decrease the modality-following ratio by $60\%$ through blocking, or increase it by $60\%$ through targeted amplification of failed samples. Our work provides a substantial step toward model transparency and offers a principled framework for the orchestration of multimodal information in MLLMs.</li>
</ul>

<h3>Title: Neural Attention Search Linear: Towards Adaptive Token-Level Hybrid Attention Models</h3>
<ul>
<li><strong>Authors: </strong>Difan Deng, Andreas Bentzen Winje, Lukas Fehring, Marius Lindauer</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03681">https://arxiv.org/abs/2602.03681</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03681">https://arxiv.org/pdf/2602.03681</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03681]] Neural Attention Search Linear: Towards Adaptive Token-Level Hybrid Attention Models(https://arxiv.org/abs/2602.03681)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The quadratic computational complexity of softmax transformers has become a bottleneck in long-context scenarios. In contrast, linear attention model families provide a promising direction towards a more efficient sequential model. These linear attention models compress past KV values into a single hidden state, thereby efficiently reducing complexity during both training and inference. However, their expressivity remains limited by the size of their hidden state. Previous work proposed interleaving softmax and linear attention layers to reduce computational complexity while preserving expressivity. Nevertheless, the efficiency of these models remains bottlenecked by their softmax attention layers. In this paper, we propose Neural Attention Search Linear (NAtS-L), a framework that applies both linear attention and softmax attention operations within the same layer on different tokens. NAtS-L automatically determines whether a token can be handled by a linear attention model, i.e., tokens that have only short-term impact and can be encoded into fixed-size hidden states, or require softmax attention, i.e., tokens that contain information related to long-term retrieval and need to be preserved for future queries. By searching for optimal Gated DeltaNet and softmax attention combinations across tokens, we show that NAtS-L provides a strong yet efficient token-level hybrid architecture.</li>
</ul>

<h3>Title: Universal One-third Time Scaling in Learning Peaked Distributions</h3>
<ul>
<li><strong>Authors: </strong>Yizhou Liu, Ziming Liu, Cengiz Pehlevan, Jeff Gore</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03685">https://arxiv.org/abs/2602.03685</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03685">https://arxiv.org/pdf/2602.03685</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03685]] Universal One-third Time Scaling in Learning Peaked Distributions(https://arxiv.org/abs/2602.03685)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Training large language models (LLMs) is computationally expensive, partly because the loss exhibits slow power-law convergence whose origin remains debatable. Through systematic analysis of toy models and empirical evaluation of LLMs, we show that this behavior can arise intrinsically from the use of softmax and cross-entropy. When learning peaked probability distributions, e.g., next-token distributions, these components yield power-law vanishing losses and gradients, creating a fundamental optimization bottleneck. This ultimately leads to power-law time scaling of the loss with a universal exponent of $1/3$. Our results provide a mechanistic explanation for observed neural scaling and suggest new directions for improving LLM training efficiency.</li>
</ul>

<h3>Title: QuAIL: Quality-Aware Inertial Learning for Robust Training under Data Corruption</h3>
<ul>
<li><strong>Authors: </strong>Mattia Sabella, Alberto Archetti, Pietro Pinoli, Matteo Matteucci, Cinzia Cappiello</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03686">https://arxiv.org/abs/2602.03686</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03686">https://arxiv.org/pdf/2602.03686</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03686]] QuAIL: Quality-Aware Inertial Learning for Robust Training under Data Corruption(https://arxiv.org/abs/2602.03686)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Tabular machine learning systems are frequently trained on data affected by non-uniform corruption, including noisy measurements, missing entries, and feature-specific biases. In practice, these defects are often documented only through column-level reliability indicators rather than instance-wise quality annotations, limiting the applicability of many robustness and cleaning techniques. We present QuAIL, a quality-informed training mechanism that incorporates feature reliability priors directly into the learning process. QuAIL augments existing models with a learnable feature-modulation layer whose updates are selectively constrained by a quality-dependent proximal regularizer, thereby inducing controlled adaptation across features of varying trustworthiness. This stabilizes optimization under structured corruption without explicit data repair or sample-level reweighting. Empirical evaluation across 50 classification and regression datasets demonstrates that QuAIL consistently improves average performance over neural baselines under both random and value-dependent corruption, with especially robust behavior in low-data and systematically biased settings. These results suggest that incorporating feature reliability information directly into optimization dynamics is a practical and effective approach for resilient tabular learning.</li>
</ul>

<h3>Title: Rethinking the Reranker: Boundary-Aware Evidence Selection for Robust Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Jiashuo Sun, Pengcheng Jiang, Saizhuo Wang, Jiajun Fan, Heng Wang, Siru Ouyang, Ming Zhong, Yizhu Jiao, Chengsong Huang, Xueqiang Xu, Pengrui Han, Peiran Li, Jiaxin Huang, Ge Liu, Heng Ji, Jiawei Han</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03689">https://arxiv.org/abs/2602.03689</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03689">https://arxiv.org/pdf/2602.03689</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03689]] Rethinking the Reranker: Boundary-Aware Evidence Selection for Robust Retrieval-Augmented Generation(https://arxiv.org/abs/2602.03689)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) systems remain brittle under realistic retrieval noise, even when the required evidence appears in the top-K results. A key reason is that retrievers and rerankers optimize solely for relevance, often selecting either trivial, answer-revealing passages or evidence that lacks the critical information required to answer the question, without considering whether the evidence is suitable for the generator. We propose BAR-RAG, which reframes the reranker as a boundary-aware evidence selector that targets the generator's Goldilocks Zone -- evidence that is neither trivially easy nor fundamentally unanswerable for the generator, but is challenging yet sufficient for inference and thus provides the strongest learning signal. BAR-RAG trains the selector with reinforcement learning using generator feedback, and adopts a two-stage pipeline that fine-tunes the generator under the induced evidence distribution to mitigate the distribution mismatch between training and inference. Experiments on knowledge-intensive question answering benchmarks show that BAR-RAG consistently improves end-to-end performance under noisy retrieval, achieving an average gain of 10.3 percent over strong RAG and reranking baselines while substantially improving robustness. Code is publicly avaliable at this https URL.</li>
</ul>

<h3>Title: LLM-Inspired Pretrain-Then-Finetune for Small-Data, Large-Scale Optimization</h3>
<ul>
<li><strong>Authors: </strong>Zishi Zhang, Jinhui Han, Ming Hu, Yijie Peng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03690">https://arxiv.org/abs/2602.03690</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03690">https://arxiv.org/pdf/2602.03690</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03690]] LLM-Inspired Pretrain-Then-Finetune for Small-Data, Large-Scale Optimization(https://arxiv.org/abs/2602.03690)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>We consider small-data, large-scale decision problems in which a firm must make many operational decisions simultaneously (e.g., across a large product portfolio) while observing only a few, potentially noisy, data points per instance. Inspired by the success of large language models (LLMs), we propose a pretrain-then-finetune approach built on a designed Transformer model to address this challenge. The model is first pretrained on large-scale, domain-informed synthetic data that encode managerial knowledge and structural features of the decision environment, and is then fine-tuned on real observations. This new pipeline offers two complementary advantages: pretraining injects domain knowledge into the learning process and enables the training of high-capacity models using abundant synthetic data, while finetuning adapts the pretrained model to the operational environment and improves alignment with the true data-generating regime. While we have leveraged the Transformer's state-of-the-art representational capacity, particularly its attention mechanism, to efficiently extract cross-task structure, our approach is not an off-the-shelf application. Instead, it relies on problem-specific architectural design and a tailored training procedure to match the decision setting. Theoretically, we develop the first comprehensive error analysis regarding Transformer learning in relevant contexts, establishing nonasymptotic guarantees that validate the method's effectiveness. Critically, our analysis reveals how pretraining and fine-tuning jointly determine performance, with the dominant contribution governed by whichever is more favorable. In particular, finetuning exhibits an economies-of-scale effect, whereby transfer learning becomes increasingly effective as the number of instances grows.</li>
</ul>

<h3>Title: OCRTurk: A Comprehensive OCR Benchmark for Turkish</h3>
<ul>
<li><strong>Authors: </strong>Deniz Yılmaz, Evren Ayberk Munis, Çağrı Toraman, Süha Kağan Köse, Burak Aktaş, Mehmet Can Baytekin, Bilge Kaan Görür</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03693">https://arxiv.org/abs/2602.03693</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03693">https://arxiv.org/pdf/2602.03693</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03693]] OCRTurk: A Comprehensive OCR Benchmark for Turkish(https://arxiv.org/abs/2602.03693)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Document parsing is now widely used in applications, such as large-scale document digitization, retrieval-augmented generation, and domain-specific pipelines in healthcare and education. Benchmarking these models is crucial for assessing their reliability and practical robustness. Existing benchmarks mostly target high-resource languages and provide limited coverage for low-resource settings, such as Turkish. Moreover, existing studies on Turkish document parsing lack a standardized benchmark that reflects real-world scenarios and document diversity. To address this gap, we introduce OCRTurk, a Turkish document parsing benchmark covering multiple layout elements and document categories at three difficulty levels. OCRTurk consists of 180 Turkish documents drawn from academic articles, theses, slide decks, and non-academic articles. We evaluate seven OCR models on OCRTurk using element-wise metrics. Across difficulty levels, PaddleOCR achieves the strongest overall results, leading most element-wise metrics except figures and attaining high Normalized Edit Distance scores in easy, medium, and hard subsets. We also observe performance variation by document type. Models perform well on non-academic documents, while slideshows become the most challenging.</li>
</ul>

<h3>Title: Conflict-Resolving and Sharpness-Aware Minimization for Generalized Knowledge Editing with Multiple Updates</h3>
<ul>
<li><strong>Authors: </strong>Duy Nguyen, Hanqi Xiao, Archiki Prasad, Elias Stengel-Eskin, Hyunji Lee, Mohit Bansal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03696">https://arxiv.org/abs/2602.03696</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03696">https://arxiv.org/pdf/2602.03696</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03696]] Conflict-Resolving and Sharpness-Aware Minimization for Generalized Knowledge Editing with Multiple Updates(https://arxiv.org/abs/2602.03696)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) rely on internal knowledge to solve many downstream tasks, making it crucial to keep them up to date. Since full retraining is expensive, prior work has explored efficient alternatives such as model editing and parameter-efficient fine-tuning. However, these approaches often break down in practice due to poor generalization across inputs, limited stability, and knowledge conflict. To address these limitations, we propose the CoRSA (Conflict-Resolving and Sharpness-Aware Minimization) training framework, a parameter-efficient, holistic approach for knowledge editing with multiple updates. CoRSA tackles multiple challenges simultaneously: it improves generalization to different input forms and enhances stability across multiple updates by minimizing loss curvature, and resolves conflicts by maximizing the margin between new and prior knowledge. Across three widely used fact editing benchmarks, CoRSA achieves significant gains in generalization, outperforming baselines with average absolute improvements of 12.42% over LoRA and 10% over model editing methods. With multiple updates, it maintains high update efficacy while reducing catastrophic forgetting by 27.82% compared to LoRA. CoRSA also generalizes to the code domain, outperforming the strongest baseline by 5.48% Pass@5 in update efficacy.</li>
</ul>

<h3>Title: Data-Driven Graph Filters via Adaptive Spectral Shaping</h3>
<ul>
<li><strong>Authors: </strong>Dylan Sandfelder, Mihai Cucuringu, Xiaowen Dong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03698">https://arxiv.org/abs/2602.03698</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03698">https://arxiv.org/pdf/2602.03698</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03698]] Data-Driven Graph Filters via Adaptive Spectral Shaping(https://arxiv.org/abs/2602.03698)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>We introduce Adaptive Spectral Shaping, a data-driven framework for graph filtering that learns a reusable baseline spectral kernel and modulates it with a small set of Gaussian factors. The resulting multi-peak, multi-scale responses allocate energy to heterogeneous regions of the Laplacian spectrum while remaining interpretable via explicit centers and bandwidths. To scale, we implement filters with Chebyshev polynomial expansions, avoiding eigendecompositions. We further propose Transferable Adaptive Spectral Shaping (TASS): the baseline kernel is learned on source graphs and, on a target graph, kept fixed while only the shaping parameters are adapted, enabling few-shot transfer under matched compute. Across controlled synthetic benchmarks spanning graph families and signal regimes, Adaptive Spectral Shaping reduces reconstruction error relative to fixed-prototype wavelets and learned linear banks, and TASS yields consistent positive transfer. The framework provides compact spectral modules that plug into graph signal processing pipelines and graph neural networks, combining scalability, interpretability, and cross-graph generalization.</li>
</ul>

<h3>Title: Anytime Pretraining: Horizon-Free Learning-Rate Schedules with Weight Averaging</h3>
<ul>
<li><strong>Authors: </strong>Alexandru Meterez, Pranav Ajit Nair, Depen Morwani, Cengiz Pehlevan, Sham Kakade</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03702">https://arxiv.org/abs/2602.03702</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03702">https://arxiv.org/pdf/2602.03702</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03702]] Anytime Pretraining: Horizon-Free Learning-Rate Schedules with Weight Averaging(https://arxiv.org/abs/2602.03702)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models are increasingly trained in continual or open-ended settings, where the total training horizon is not known in advance. Despite this, most existing pretraining recipes are not anytime: they rely on horizon-dependent learning rate schedules and extensive tuning under a fixed compute budget. In this work, we provide a theoretical analysis demonstrating the existence of anytime learning schedules for overparameterized linear regression, and we highlight the central role of weight averaging - also known as model merging - in achieving the minimax convergence rates of stochastic gradient descent. We show that these anytime schedules polynomially decay with time, with the decay rate determined by the source and capacity conditions of the problem. Empirically, we evaluate 150M and 300M parameter language models trained at 1-32x Chinchilla scale, comparing constant learning rates with weight averaging and $1/\sqrt{t}$ schedules with weight averaging against a well-tuned cosine schedule. Across the full training range, the anytime schedules achieve comparable final loss to cosine decay. Taken together, our results suggest that weight averaging combined with simple, horizon-free step sizes offers a practical and effective anytime alternative to cosine learning rate schedules for large language model pretraining.</li>
</ul>

<h3>Title: Cognitively Diverse Multiple-Choice Question Generation: A Hybrid Multi-Agent Framework with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yu Tian, Linh Huynh, Katerina Christhilf, Shubham Chakraborty, Micah Watanabe, Tracy Arner, Danielle McNamara</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03704">https://arxiv.org/abs/2602.03704</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03704">https://arxiv.org/pdf/2602.03704</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03704]] Cognitively Diverse Multiple-Choice Question Generation: A Hybrid Multi-Agent Framework with Large Language Models(https://arxiv.org/abs/2602.03704)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have made automated multiple-choice question (MCQ) generation increasingly feasible; however, reliably producing items that satisfy controlled cognitive demands remains a challenge. To address this gap, we introduce ReQUESTA, a hybrid, multi-agent framework for generating cognitively diverse MCQs that systematically target text-based, inferential, and main idea comprehension. ReQUESTA decomposes MCQ authoring into specialized subtasks and coordinates LLM-powered agents with rule-based components to support planning, controlled generation, iterative evaluation, and post-processing. We evaluated the framework in a large-scale reading comprehension study using academic expository texts, comparing ReQUESTA-generated MCQs with those produced by a single-pass GPT-5 zero-shot baseline. Psychometric analyses of learner responses assessed item difficulty and discrimination, while expert raters evaluated question quality across multiple dimensions, including topic relevance and distractor quality. Results showed that ReQUESTA-generated items were consistently more challenging, more discriminative, and more strongly aligned with overall reading comprehension performance. Expert evaluations further indicated stronger alignment with central concepts and superior distractor linguistic consistency and semantic plausibility, particularly for inferential questions. These findings demonstrate that hybrid, agentic orchestration can systematically improve the reliability and controllability of LLM-based generation, highlighting workflow design as a key lever for structured artifact generation beyond single-pass prompting.</li>
</ul>

<h3>Title: Beyond Tokens: Semantic-Aware Speculative Decoding for Efficient Inference by Probing Internal States</h3>
<ul>
<li><strong>Authors: </strong>Ximing Dong, Shaowei Wang, Dayi Lin, Boyuan Chen, Ahmed E. Hassan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03708">https://arxiv.org/abs/2602.03708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03708">https://arxiv.org/pdf/2602.03708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03708]] Beyond Tokens: Semantic-Aware Speculative Decoding for Efficient Inference by Probing Internal States(https://arxiv.org/abs/2602.03708)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) achieve strong performance across many tasks but suffer from high inference latency due to autoregressive decoding. The issue is exacerbated in Large Reasoning Models (LRMs), which generate lengthy chains of thought. While speculative decoding accelerates inference by drafting and verifying multiple tokens in parallel, existing methods operate at the token level and ignore semantic equivalence (i.e., different token sequences expressing the same meaning), leading to inefficient rejections. We propose SemanticSpec, a semantic-aware speculative decoding framework that verifies entire semantic sequences instead of tokens. SemanticSpec introduces a semantic probability estimation mechanism that probes the model's internal hidden states to assess the likelihood of generating sequences with specific this http URL on four benchmarks show that SemanticSpec achieves up to 2.7x speedup on DeepSeekR1-32B and 2.1x on QwQ-32B, consistently outperforming token-level and sequence-level baselines in both efficiency and effectiveness.</li>
</ul>

<h3>Title: No Shortcuts to Culture: Indonesian Multi-hop Question Answering for Complex Cultural Understanding</h3>
<ul>
<li><strong>Authors: </strong>Vynska Amalia Permadi, Xingwei Tan, Nafise Sadat Moosavi, Nikos Aletras</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03709">https://arxiv.org/abs/2602.03709</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03709">https://arxiv.org/pdf/2602.03709</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03709]] No Shortcuts to Culture: Indonesian Multi-hop Question Answering for Complex Cultural Understanding(https://arxiv.org/abs/2602.03709)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Understanding culture requires reasoning across context, tradition, and implicit social knowledge, far beyond recalling isolated facts. Yet most culturally focused question answering (QA) benchmarks rely on single-hop questions, which may allow models to exploit shallow cues rather than demonstrate genuine cultural reasoning. In this work, we introduce ID-MoCQA, the first large-scale multi-hop QA dataset for assessing the cultural understanding of large language models (LLMs), grounded in Indonesian traditions and available in both English and Indonesian. We present a new framework that systematically transforms single-hop cultural questions into multi-hop reasoning chains spanning six clue types (e.g., commonsense, temporal, geographical). Our multi-stage validation pipeline, combining expert review and LLM-as-a-judge filtering, ensures high-quality question-answer pairs. Our evaluation across state-of-the-art models reveals substantial gaps in cultural reasoning, particularly in tasks requiring nuanced inference. ID-MoCQA provides a challenging and essential benchmark for advancing the cultural competency of LLMs.</li>
</ul>

<h3>Title: Training Multi-Turn Search Agent via Contrastive Dynamic Branch Sampling</h3>
<ul>
<li><strong>Authors: </strong>Yubao Zhao, Weiquan Huang, Sudong Wang, Ruochen Zhao, Chen Chen, Yao Shu, Chengwei Qin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03719">https://arxiv.org/abs/2602.03719</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03719">https://arxiv.org/pdf/2602.03719</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03719]] Training Multi-Turn Search Agent via Contrastive Dynamic Branch Sampling(https://arxiv.org/abs/2602.03719)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Agentic reinforcement learning has enabled large language models to perform complex multi-turn planning and tool use. However, learning in long-horizon settings remains challenging due to sparse, trajectory-level outcome rewards. While prior tree-based methods attempt to mitigate this issue, they often suffer from high variance and computational inefficiency. Through empirical analysis of search agents, We identify a common pattern: performance diverges mainly due to decisions near the tail. Motivated by this observation, we propose Branching Relative Policy Optimization (BranPO), a value-free method that provides step-level contrastive supervision without dense rewards. BranPO truncates trajectories near the tail and resamples alternative continuations to construct contrastive suffixes over shared prefixes, reducing credit ambiguity in long-horizon rollouts. To further boost efficiency and stabilize training, we introduce difficulty-aware branch sampling to adapt branching frequency across tasks, and redundant step masking to suppress uninformative actions. Extensive experiments on various question answering benchmarks demonstrate that BranPO consistently outperforms strong baselines, achieving significant accuracy gains on long-horizon tasks without increasing the overall training budget. Our code is available at \href{this https URL}{code}.</li>
</ul>

<h3>Title: Efficient Training of Boltzmann Generators Using Off-Policy Log-Dispersion Regularization</h3>
<ul>
<li><strong>Authors: </strong>Henrik Schopmans, Christopher von Klitzing, Pascal Friederich</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03729">https://arxiv.org/abs/2602.03729</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03729">https://arxiv.org/pdf/2602.03729</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03729]] Efficient Training of Boltzmann Generators Using Off-Policy Log-Dispersion Regularization(https://arxiv.org/abs/2602.03729)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Sampling from unnormalized probability densities is a central challenge in computational science. Boltzmann generators are generative models that enable independent sampling from the Boltzmann distribution of physical systems at a given temperature. However, their practical success depends on data-efficient training, as both simulation data and target energy evaluations are costly. To this end, we propose off-policy log-dispersion regularization (LDR), a novel regularization framework that builds on a generalization of the log-variance objective. We apply LDR in the off-policy setting in combination with standard data-based training objectives, without requiring additional on-policy samples. LDR acts as a shape regularizer of the energy landscape by leveraging additional information in the form of target energy labels. The proposed regularization framework is broadly applicable, supporting unbiased or biased simulation datasets as well as purely variational training without access to target samples. Across all benchmarks, LDR improves both final performance and data efficiency, with sample efficiency gains of up to one order of magnitude.</li>
</ul>

<h3>Title: RegionReasoner: Region-Grounded Multi-Round Visual Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Wenfang Sun, Hao Chen, Yingjun Du, Yefeng Zheng, Cees G. M. Snoek</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03733">https://arxiv.org/abs/2602.03733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03733">https://arxiv.org/pdf/2602.03733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03733]] RegionReasoner: Region-Grounded Multi-Round Visual Reasoning(https://arxiv.org/abs/2602.03733)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Large vision-language models have achieved remarkable progress in visual reasoning, yet most existing systems rely on single-step or text-only reasoning, limiting their ability to iteratively refine understanding across multiple visual contexts. To address this limitation, we introduce a new multi-round visual reasoning benchmark with training and test sets spanning both detection and segmentation tasks, enabling systematic evaluation under iterative reasoning scenarios. We further propose RegionReasoner, a reinforcement learning framework that enforces grounded reasoning by requiring each reasoning trace to explicitly cite the corresponding reference bounding boxes, while maintaining semantic coherence via a global-local consistency reward. This reward extracts key objects and nouns from both global scene captions and region-level captions, aligning them with the reasoning trace to ensure consistency across reasoning steps. RegionReasoner is optimized with structured rewards combining grounding fidelity and global-local semantic alignment. Experiments on detection and segmentation tasks show that RegionReasoner-7B, together with our newly introduced benchmark RegionDial-Bench, considerably improves multi-round reasoning accuracy, spatial grounding precision, and global-local consistency, establishing a strong baseline for this emerging research direction.</li>
</ul>

<h3>Title: Edge-Optimized Vision-Language Models for Underground Infrastructure Assessment</h3>
<ul>
<li><strong>Authors: </strong>Johny J. Lopez, Md Meftahul Ferdaus, Mahdi Abdelguerfi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03742">https://arxiv.org/abs/2602.03742</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03742">https://arxiv.org/pdf/2602.03742</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03742]] Edge-Optimized Vision-Language Models for Underground Infrastructure Assessment(https://arxiv.org/abs/2602.03742)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Autonomous inspection of underground infrastructure, such as sewer and culvert systems, is critical to public safety and urban sustainability. Although robotic platforms equipped with visual sensors can efficiently detect structural deficiencies, the automated generation of human-readable summaries from these detections remains a significant challenge, especially on resource-constrained edge devices. This paper presents a novel two-stage pipeline for end-to-end summarization of underground deficiencies, combining our lightweight RAPID-SCAN segmentation model with a fine-tuned Vision-Language Model (VLM) deployed on an edge computing platform. The first stage employs RAPID-SCAN (Resource-Aware Pipeline Inspection and Defect Segmentation using Compact Adaptive Network), achieving 0.834 F1-score with only 0.64M parameters for efficient defect segmentation. The second stage utilizes a fine-tuned Phi-3.5 VLM that generates concise, domain-specific summaries in natural language from the segmentation outputs. We introduce a curated dataset of inspection images with manually verified descriptions for VLM fine-tuning and evaluation. To enable real-time performance, we employ post-training quantization with hardware-specific optimization, achieving significant reductions in model size and inference latency without compromising summarization quality. We deploy and evaluate our complete pipeline on a mobile robotic platform, demonstrating its effectiveness in real-world inspection scenarios. Our results show the potential of edge-deployable integrated AI systems to bridge the gap between automated defect detection and actionable insights for infrastructure maintenance, paving the way for more scalable and autonomous inspection solutions.</li>
</ul>

<h3>Title: LIVE: Long-horizon Interactive Video World Modeling</h3>
<ul>
<li><strong>Authors: </strong>Junchao Huang, Ziyang Ye, Xinting Hu, Tianyu He, Guiyu Zhang, Shaoshuai Shi, Jiang Bian, Li Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03747">https://arxiv.org/abs/2602.03747</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03747">https://arxiv.org/pdf/2602.03747</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03747]] LIVE: Long-horizon Interactive Video World Modeling(https://arxiv.org/abs/2602.03747)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Autoregressive video world models predict future visual observations conditioned on actions. While effective over short horizons, these models often struggle with long-horizon generation, as small prediction errors accumulate over time. Prior methods alleviate this by introducing pre-trained teacher models and sequence-level distribution matching, which incur additional computational cost and fail to prevent error propagation beyond the training horizon. In this work, we propose LIVE, a Long-horizon Interactive Video world modEl that enforces bounded error accumulation via a novel cycle-consistency objective, thereby eliminating the need for teacher-based distillation. Specifically, LIVE first performs a forward rollout from ground-truth frames and then applies a reverse generation process to reconstruct the initial state. The diffusion loss is subsequently computed on the reconstructed terminal state, providing an explicit constraint on long-horizon error propagation. Moreover, we provide an unified view that encompasses different approaches and introduce progressive training curriculum to stabilize training. Experiments demonstrate that LIVE achieves state-of-the-art performance on long-horizon benchmarks, generating stable, high-quality videos far beyond training rollout lengths.</li>
</ul>

<h3>Title: See-through: Single-image Layer Decomposition for Anime Characters</h3>
<ul>
<li><strong>Authors: </strong>Jian Lin, Chengze Li, Haoyun Qin, Kwun Wang Chan, Yanghua Jin, Hanyuan Liu, Stephen Chun Wang Choy, Xueting Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03749">https://arxiv.org/abs/2602.03749</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03749">https://arxiv.org/pdf/2602.03749</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03749]] See-through: Single-image Layer Decomposition for Anime Characters(https://arxiv.org/abs/2602.03749)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>We introduce a framework that automates the transformation of static anime illustrations into manipulatable 2.5D models. Current professional workflows require tedious manual segmentation and the artistic ``hallucination'' of occluded regions to enable motion. Our approach overcomes this by decomposing a single image into fully inpainted, semantically distinct layers with inferred drawing orders. To address the scarcity of training data, we introduce a scalable engine that bootstraps high-quality supervision from commercial Live2D models, capturing pixel-perfect semantics and hidden geometry. Our methodology couples a diffusion-based Body Part Consistency Module, which enforces global geometric coherence, with a pixel-level pseudo-depth inference mechanism. This combination resolves the intricate stratification of anime characters, e.g., interleaving hair strands, allowing for dynamic layer reconstruction. We demonstrate that our approach yields high-fidelity, manipulatable models suitable for professional, real-time animation applications.</li>
</ul>

<h3>Title: Test-Time Conditioning with Representation-Aligned Visual Features</h3>
<ul>
<li><strong>Authors: </strong>Nicolas Sereyjol-Garros, Ellington Kirby, Victor Letzelter, Victor Besnier, Nermin Samet</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03753">https://arxiv.org/abs/2602.03753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03753">https://arxiv.org/pdf/2602.03753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03753]] Test-Time Conditioning with Representation-Aligned Visual Features(https://arxiv.org/abs/2602.03753)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>While representation alignment with self-supervised models has been shown to improve diffusion model training, its potential for enhancing inference-time conditioning remains largely unexplored. We introduce Representation-Aligned Guidance (REPA-G), a framework that leverages these aligned representations, with rich semantic properties, to enable test-time conditioning from features in generation. By optimizing a similarity objective (the potential) at inference, we steer the denoising process toward a conditioned representation extracted from a pre-trained feature extractor. Our method provides versatile control at multiple scales, ranging from fine-grained texture matching via single patches to broad semantic guidance using global image feature tokens. We further extend this to multi-concept composition, allowing for the faithful combination of distinct concepts. REPA-G operates entirely at inference time, offering a flexible and precise alternative to often ambiguous text prompts or coarse class labels. We theoretically justify how this guidance enables sampling from the potential-induced tilted distribution. Quantitative results on ImageNet and COCO demonstrate that our approach achieves high-quality, diverse generations. Code is available at this https URL.</li>
</ul>

<h3>Title: Reasoning with Latent Tokens in Diffusion Language Models</h3>
<ul>
<li><strong>Authors: </strong>Andre He, Sean Welleck, Daniel Fried</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03769">https://arxiv.org/abs/2602.03769</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03769">https://arxiv.org/pdf/2602.03769</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03769]] Reasoning with Latent Tokens in Diffusion Language Models(https://arxiv.org/abs/2602.03769)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Discrete diffusion models have recently become competitive with autoregressive models for language modeling, even outperforming them on reasoning tasks requiring planning and global coherence, but they require more computation at inference time. We trace this trade-off to a key mechanism: diffusion models are trained to jointly predict a distribution over all unknown tokens, including those that will not actually be decoded in the current step. Ablating this joint prediction yields faster inference but degrades performance, revealing that accurate prediction at the decoded position relies on joint reasoning about the distribution of undecoded tokens. We interpret these as latent tokens and introduce a method for modulating their number, demonstrating empirically that this enables a smooth tradeoff between inference speed and sample quality. Furthermore, we demonstrate that latent tokens can be introduced into autoregressive models through an auxiliary multi-token prediction objective, yielding substantial improvements on the same reasoning tasks where they have traditionally struggled. Our results suggest that latent tokens, while arising naturally in diffusion, represent a general mechanism for improving performance on tasks requiring global coherence or lookahead.</li>
</ul>

<h3>Title: UniGeM: Unifying Data Mixing and Selection via Geometric Exploration and Mining</h3>
<ul>
<li><strong>Authors: </strong>Changhao Wang, Yunfei Yu, Xinhao Yao, Jiaolong Yang, Riccardo Cantoro, Chaobo Li, Qing Cui, Jun Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03772">https://arxiv.org/abs/2602.03772</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03772">https://arxiv.org/pdf/2602.03772</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03772]] UniGeM: Unifying Data Mixing and Selection via Geometric Exploration and Mining(https://arxiv.org/abs/2602.03772)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The scaling of Large Language Models (LLMs) is increasingly limited by data quality. Most methods handle data mixing and sample selection separately, which can break the structure in code corpora. We introduce \textbf{UniGeM}, a framework that unifies mixing and selection by treating data curation as a \textit{manifold approximation} problem without training proxy models or relying on external reference datasets. UniGeM operates hierarchically: \textbf{Macro-Exploration} learns mixing weights with stability-based clustering; \textbf{Micro-Mining} filters high-quality instances by their geometric distribution to ensure logical consistency. Validated by training 8B and 16B MoE models on 100B tokens, UniGeM achieves \textbf{2.0$\times$ data efficiency} over a random baseline and further improves overall performance compared to SOTA methods in reasoning-heavy evaluations and multilingual generalization.</li>
</ul>

<h3>Title: Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL</h3>
<ul>
<li><strong>Authors: </strong>Ian Wu, Yuxiao Qu, Amrith Setlur, Aviral Kumar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03773">https://arxiv.org/abs/2602.03773</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03773">https://arxiv.org/pdf/2602.03773</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03773]] Reasoning Cache: Continual Improvement Over Long Horizons via Short-Horizon RL(https://arxiv.org/abs/2602.03773)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) that can continually improve beyond their training budgets are able to solve increasingly difficult problems by adapting at test time, a property we refer to as extrapolation. However, standard reinforcement learning (RL) operates over fixed problem distributions and training budgets, which limits extrapolation amidst distribution shift at test time. To address this, we introduce RC, an iterative decoding algorithm that replaces standard autoregressive decoding during both training and inference. RC exploits an asymmetry between the response generation and summarization capabilities of LLMs to construct reasoning chains that consistently improve across iterations. Models trained to use RC can extrapolate and continually improve over reasoning horizons more than an order of magnitude longer than those seen during training. Empirically, training a 4B model with RC using a 16k-token training budget improves performance on HMMT 2025 from 40% to nearly 70% with 0.5m tokens at test time, outperforming both comparably sized models and many larger reasoning LLMs. Finally, we also show that models trained with RC can more effectively leverage existing scaffolds to further scale test-time performance, due to the improved summary-conditioned generation abilities learned through training.</li>
</ul>

<h3>Title: QVLA: Not All Channels Are Equal in Vision-Language-Action Model's Quantization</h3>
<ul>
<li><strong>Authors: </strong>Yuhao Xu, Yantai Yang, Zhenyang Fan, Yufan Liu, Yuming Li, Bing Li, Zhipeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03782">https://arxiv.org/abs/2602.03782</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03782">https://arxiv.org/pdf/2602.03782</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03782]] QVLA: Not All Channels Are Equal in Vision-Language-Action Model's Quantization(https://arxiv.org/abs/2602.03782)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The advent of Vision-Language-Action (VLA) models represents a significant leap for embodied intelligence, yet their immense computational demands critically hinder deployment on resource-constrained robotic platforms. Intuitively, low-bit quantization is a prevalent and preferred technique for large-scale model compression. However, we find that a systematic analysis of VLA model's quantization is fundamentally lacking. We argue that naively applying uniform-bit quantization from Large Language Models (LLMs) to robotics is flawed, as these methods prioritize passive data fidelity while ignoring how minor action deviations compound into catastrophic task failures. To bridge this gap, we introduce QVLA, the first action-centric quantization framework specifically designed for embodied control. In a sharp departure from the rigid, uniform-bit quantization of LLM-based methods, QVLA introduces a highly granular, channel-wise bit allocation strategy. Its core mechanism is to directly measure the final action-space sensitivity when quantizing each individual channel to various bit-widths. This process yields a precise, per-channel importance metric that guides a global optimization, which elegantly unifies quantization and pruning (0-bit) into a single, cohesive framework. Extensive evaluations on different baselines demonstrate the superiority of our approach. In the LIBERO, the quantization version of OpenVLA-OFT with our method requires only 29.2% of the original model's VRAM while maintaining 98.9% of its original performance and achieving a 1.49x speedup. This translates to a 22.6% performance improvement over the LLM-derived method SmoothQuant. Our work establishes a new, principled foundation for compressing VLA models in robotics, paving the way for deploying powerful, large-scale models on real-world hardware. Code will be released.</li>
</ul>

<h3>Title: Efficient Estimation of Kernel Surrogate Models for Task Attribution</h3>
<ul>
<li><strong>Authors: </strong>Zhenshuo Zhang, Minxuan Duan, Hongyang R. Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03783">https://arxiv.org/abs/2602.03783</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03783">https://arxiv.org/pdf/2602.03783</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03783]] Efficient Estimation of Kernel Surrogate Models for Task Attribution(https://arxiv.org/abs/2602.03783)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Modern AI agents such as large language models are trained on diverse tasks -- translation, code generation, mathematical reasoning, and text prediction -- simultaneously. A key question is to quantify how each individual training task influences performance on a target task, a problem we refer to as task attribution. The direct approach, leave-one-out retraining, measures the effect of removing each task, but is computationally infeasible at scale. An alternative approach that builds surrogate models to predict a target task's performance for any subset of training tasks has emerged in recent literature. Prior work focuses on linear surrogate models, which capture first-order relationships, but miss nonlinear interactions such as synergy, antagonism, or XOR-type effects. In this paper, we first consider a unified task weighting framework for analyzing task attribution methods, and show a new connection between linear surrogate models and influence functions through a second-order analysis. Then, we introduce kernel surrogate models, which more effectively represent second-order task interactions. To efficiently learn the kernel surrogate, we develop a gradient-based estimation procedure that leverages a first-order approximation of pretrained models; empirically, this yields accurate estimates with less than $2\%$ relative error without repeated retraining. Experiments across multiple domains -- including math reasoning in transformers, in-context learning, and multi-objective reinforcement learning -- demonstrate the effectiveness of kernel surrogate models. They achieve a $25\%$ higher correlation with the leave-one-out ground truth than linear surrogates and influence-function baselines. When used for downstream task selection, kernel surrogate models yield a $40\%$ improvement in demonstration selection for in-context learning and multi-objective reinforcement learning benchmarks.</li>
</ul>

<h3>Title: Context Compression via Explicit Information Transmission</h3>
<ul>
<li><strong>Authors: </strong>Jiangnan Ye, Hanqi Yan, Zhenyi Shen, Heng Chang, Ye Mao, Yulan He</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03784">https://arxiv.org/abs/2602.03784</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03784">https://arxiv.org/pdf/2602.03784</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03784]] Context Compression via Explicit Information Transmission(https://arxiv.org/abs/2602.03784)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Long-context inference with Large Language Models (LLMs) is costly due to quadratic attention and growing key-value caches, motivating context compression. In this work, we study soft context compression, where a long context is condensed into a small set of continuous representations. Existing methods typically re-purpose the LLM itself as a trainable compressor, relying on layer-by-layer self-attention to iteratively aggregate information. We argue that this paradigm suffers from two structural limitations: (i) progressive representation overwriting across layers (ii) uncoordinated allocation of compression capacity across tokens. We propose ComprExIT (Context Compression via Explicit Information Transmission), a lightweight framework that formulates soft compression into a new paradigm: explicit information transmission over frozen LLM hidden states. This decouples compression from the model's internal self-attention dynamics. ComprExIT performs (i) depth-wise transmission to selectively transmit multi-layer information into token anchors, mitigating progressive overwriting, and (ii) width-wise transmission to aggregate anchors into a small number of slots via a globally optimized transmission plan, ensuring coordinated allocation of information. Across six question-answering benchmarks, ComprExIT consistently outperforms state-of-the-art context compression methods while introducing only ~1% additional parameters, demonstrating that explicit and coordinated information transmission enables more effective and robust long-context compression.</li>
</ul>

<h3>Title: Inference-time Unlearning Using Conformal Prediction</h3>
<ul>
<li><strong>Authors: </strong>Somnath Basu Roy Chowdhury, Rahul Kidambi, Avinava Dubey, David Wang, Gokhan Mergen, Amr Ahmed, Aranyak Mehta</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03787">https://arxiv.org/abs/2602.03787</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03787">https://arxiv.org/pdf/2602.03787</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03787]] Inference-time Unlearning Using Conformal Prediction(https://arxiv.org/abs/2602.03787)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Machine unlearning is the process of efficiently removing specific information from a trained machine learning model without retraining from scratch. Existing unlearning methods, which often provide provable guarantees, typically involve retraining a subset of model parameters based on a forget set. While these approaches show promise in certain scenarios, their underlying assumptions are often challenged in real-world applications -- particularly when applied to generative models. Furthermore, updating parameters using these unlearning procedures often degrades the general-purpose capabilities the model acquired during pre-training. Motivated by these shortcomings, this paper considers the paradigm of inference time unlearning -- wherein, the generative model is equipped with an (approximately correct) verifier that judges whether the model's response satisfies appropriate unlearning guarantees. This paper introduces a framework that iteratively refines the quality of the generated responses using feedback from the verifier without updating the model parameters. The proposed framework leverages conformal prediction to reduce computational overhead and provide distribution-free unlearning guarantees. This paper's approach significantly outperforms existing state-of-the-art methods, reducing unlearning error by up to 93% across challenging unlearning benchmarks.</li>
</ul>

<h3>Title: Should I use Synthetic Data for That? An Analysis of the Suitability of Synthetic Data for Data Sharing and Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Bogdan Kulynych, Theresa Stadler, Jean Louis Raisaro, Carmela Troncoso</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03791">https://arxiv.org/abs/2602.03791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03791">https://arxiv.org/pdf/2602.03791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03791]] Should I use Synthetic Data for That? An Analysis of the Suitability of Synthetic Data for Data Sharing and Augmentation(https://arxiv.org/abs/2602.03791)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in generative modelling have led many to see synthetic data as the go-to solution for a range of problems around data access, scarcity, and under-representation. In this paper, we study three prominent use cases: (1) Sharing synthetic data as a proxy for proprietary datasets to enable statistical analyses while protecting privacy, (2) Augmenting machine learning training sets with synthetic data to improve model performance, and (3) Augmenting datasets with synthetic data to reduce variance in statistical estimation. For each use case, we formalise the problem setting and study, through formal analysis and case studies, under which conditions synthetic data can achieve its intended objectives. We identify fundamental and practical limits that constrain when synthetic data can serve as an effective solution for a particular problem. Our analysis reveals that due to these limits many existing or envisioned use cases of synthetic data are a poor problem fit. Our formalisations and classification of synthetic data use cases enable decision makers to assess whether synthetic data is a suitable approach for their specific data availability problem.</li>
</ul>

<h3>Title: WebSentinel: Detecting and Localizing Prompt Injection Attacks for Web Agents</h3>
<ul>
<li><strong>Authors: </strong>Xilong Wang, Yinuo Liu, Zhun Wang, Dawn Song, Neil Gong</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03792">https://arxiv.org/abs/2602.03792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03792">https://arxiv.org/pdf/2602.03792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03792]] WebSentinel: Detecting and Localizing Prompt Injection Attacks for Web Agents(https://arxiv.org/abs/2602.03792)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Prompt injection attacks manipulate webpage content to cause web agents to execute attacker-specified tasks instead of the user's intended ones. Existing methods for detecting and localizing such attacks achieve limited effectiveness, as their underlying assumptions often do not hold in the web-agent setting. In this work, we propose WebSentinel, a two-step approach for detecting and localizing prompt injection attacks in webpages. Given a webpage, Step I extracts \emph{segments of interest} that may be contaminated, and Step II evaluates each segment by checking its consistency with the webpage content as context. We show that WebSentinel is highly effective, substantially outperforming baseline methods across multiple datasets of both contaminated and clean webpages that we collected. Our code is available at: this https URL.</li>
</ul>

<h3>Title: Manifold Random Features</h3>
<ul>
<li><strong>Authors: </strong>Ananya Parashar, Derek Long, Dwaipayan Saha, Krzysztof Choromanski</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03797">https://arxiv.org/abs/2602.03797</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03797">https://arxiv.org/pdf/2602.03797</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03797]] Manifold Random Features(https://arxiv.org/abs/2602.03797)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We present a new paradigm for creating random features to approximate bi-variate functions (in particular, kernels) defined on general manifolds. This new mechanism of Manifold Random Features (MRFs) leverages discretization of the manifold and the recently introduced technique of Graph Random Features (GRFs) to learn continuous fields on manifolds. Those fields are used to find continuous approximation mechanisms that otherwise, in general scenarios, cannot be derived analytically. MRFs provide positive and bounded features, a key property for accurate, low-variance approximation. We show deep asymptotic connection between GRFs, defined on discrete graph objects, and continuous random features used for regular kernels. As a by-product of our method, we re-discover recently introduced mechanism of Gaussian kernel approximation applied in particular to improve linear-attention Transformers, considering simple random walks on graphs and by-passing original complex mathematical computations. We complement our algorithm with a rigorous theoretical analysis and verify in thorough experimental studies.</li>
</ul>

<h3>Title: Bridging Online and Offline RL: Contextual Bandit Learning for Multi-Turn Code Generation</h3>
<ul>
<li><strong>Authors: </strong>Ziru Chen, Dongdong Chen, Ruinan Jin, Yingbin Liang, Yujia Xie, Huan Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03806">https://arxiv.org/abs/2602.03806</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03806">https://arxiv.org/pdf/2602.03806</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03806]] Bridging Online and Offline RL: Contextual Bandit Learning for Multi-Turn Code Generation(https://arxiv.org/abs/2602.03806)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recently, there have been significant research interests in training large language models (LLMs) with reinforcement learning (RL) on real-world tasks, such as multi-turn code generation. While online RL tends to perform better than offline RL, its higher training cost and instability hinders wide adoption. In this paper, we build on the observation that multi-turn code generation can be formulated as a one-step recoverable Markov decision process and propose contextual bandit learning with offline trajectories (Cobalt), a new method that combines the benefits of online and offline RL. Cobalt first collects code generation trajectories using a reference LLM and divides them into partial trajectories as contextual prompts. Then, during online bandit learning, the LLM is trained to complete each partial trajectory prompt through single-step code generation. Cobalt outperforms two multi-turn online RL baselines based on GRPO and VeRPO, and substantially improves R1-Distill 8B and Qwen3 8B by up to 9.0 and 6.2 absolute Pass@1 scores on LiveCodeBench. Also, we analyze LLMs' in-context reward hacking behaviors and augment Cobalt training with perturbed trajectories to mitigate this issue. Overall, our results demonstrate Cobalt as a promising solution for iterative decision-making tasks like multi-turn code generation. Our code and data are available at this https URL.</li>
</ul>

<h3>Title: Enhancing Imbalanced Node Classification via Curriculum-Guided Feature Learning and Three-Stage Attention Network</h3>
<ul>
<li><strong>Authors: </strong>Abdul Joseph Fofanah, Lian Wen, David Chen, Shaoyang Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03808">https://arxiv.org/abs/2602.03808</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03808">https://arxiv.org/pdf/2602.03808</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03808]] Enhancing Imbalanced Node Classification via Curriculum-Guided Feature Learning and Three-Stage Attention Network(https://arxiv.org/abs/2602.03808)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Imbalanced node classification in graph neural networks (GNNs) happens when some labels are much more common than others, which causes the model to learn unfairly and perform badly on the less common classes. To solve this problem, we propose a Curriculum-Guided Feature Learning and Three-Stage Attention Network (CL3AN-GNN), a learning network that uses a three-step attention system (Engage, Enact, Embed) similar to how humans learn. The model begins by engaging with structurally simpler features, defined as (1) local neighbourhood patterns (1-hop), (2) low-degree node attributes, and (3) class-separable node pairs identified via initial graph convolutional networks and graph attention networks (GCN and GAT) embeddings. This foundation enables stable early learning despite label skew. The Enact stage then addresses complicated aspects: (1) connections that require multiple steps, (2) edges that connect different types of nodes, and (3) nodes at the edges of minority classes by using adjustable attention weights. Finally, Embed consolidates these features via iterative message passing and curriculum-aligned loss weighting. We evaluate CL3AN-GNN on eight Open Graph Benchmark datasets spanning social, biological, and citation networks. Experiments show consistent improvements across all datasets in accuracy, F1-score, and AUC over recent state-of-the-art methods. The model's step-by-step method works well with different types of graph datasets, showing quicker results than training everything at once, better performance on new, imbalanced graphs, and clear explanations of each step using gradient stability and attention correlation learning curves. This work provides both a theoretically grounded framework for curriculum learning in GNNs and practical evidence of its effectiveness against imbalances, validated through metrics, convergence speeds, and generalisation tests.</li>
</ul>

<h3>Title: Antidistillation Fingerprinting</h3>
<ul>
<li><strong>Authors: </strong>Yixuan Even Xu, John Kirchenbauer, Yash Savani, Asher Trockman, Alexander Robey, Tom Goldstein, Fei Fang, J. Zico Kolter</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03812">https://arxiv.org/abs/2602.03812</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03812">https://arxiv.org/pdf/2602.03812</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03812]] Antidistillation Fingerprinting(https://arxiv.org/abs/2602.03812)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, watermark, large language model</a></li>
<li><strong>Abstract: </strong>Model distillation enables efficient emulation of frontier large language models (LLMs), creating a need for robust mechanisms to detect when a third-party student model has trained on a teacher model's outputs. However, existing fingerprinting techniques that could be used to detect such distillation rely on heuristic perturbations that impose a steep trade-off between generation quality and fingerprinting strength, often requiring significant degradation of utility to ensure the fingerprint is effectively internalized by the student. We introduce antidistillation fingerprinting (ADFP), a principled approach that aligns the fingerprinting objective with the student's learning dynamics. Building upon the gradient-based framework of antidistillation sampling, ADFP utilizes a proxy model to identify and sample tokens that directly maximize the expected detectability of the fingerprint in the student after fine-tuning, rather than relying on the incidental absorption of the un-targeted biases of a more naive watermark. Experiments on GSM8K and OASST1 benchmarks demonstrate that ADFP achieves a significant Pareto improvement over state-of-the-art baselines, yielding stronger detection confidence with minimal impact on utility, even when the student model's architecture is unknown.</li>
</ul>

<h3>Title: Fast-Slow Efficient Training for Multimodal Large Language Models via Visual Token Pruning</h3>
<ul>
<li><strong>Authors: </strong>Dingkun Zhang, Shuhan Qi, Yulin Wu, Xinyu Xiao, Xuan Wang, Long Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03815">https://arxiv.org/abs/2602.03815</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03815">https://arxiv.org/pdf/2602.03815</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03815]] Fast-Slow Efficient Training for Multimodal Large Language Models via Visual Token Pruning(https://arxiv.org/abs/2602.03815)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) suffer from severe training inefficiency issue, which is associated with their massive model sizes and visual token numbers. Existing efforts in efficient training focus on reducing model sizes or trainable parameters. Inspired by the success of Visual Token Pruning (VTP) in improving inference efficiency, we are exploring another substantial research direction for efficient training by reducing visual tokens. However, applying VTP at the training stage results in a training-inference mismatch: pruning-trained models perform poorly when inferring on non-pruned full visual token sequences. To close this gap, we propose DualSpeed, a fast-slow framework for efficient training of MLLMs. The fast-mode is the primary mode, which incorporates existing VTP methods as plugins to reduce visual tokens, along with a mode isolator to isolate the model's behaviors. The slow-mode is the auxiliary mode, where the model is trained on full visual sequences to retain training-inference consistency. To boost its training, it further leverages self-distillation to learn from the sufficiently trained fast-mode. Together, DualSpeed can achieve both training efficiency and non-degraded performance. Experiments show DualSpeed accelerates the training of LLaVA-1.5 by 2.1$\times$ and LLaVA-NeXT by 4.0$\times$, retaining over 99% performance. Code: this https URL</li>
</ul>

<h3>Title: SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving</h3>
<ul>
<li><strong>Authors: </strong>Yesom Park, Annie C. Lu, Shao-Ching Huang, Qiyang Hu, Y. Sungtaek Ju, Stanley Osher</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03816">https://arxiv.org/abs/2602.03816</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03816">https://arxiv.org/pdf/2602.03816</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03816]] SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving(https://arxiv.org/abs/2602.03816)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We propose SymPlex, a reinforcement learning framework for discovering analytical symbolic solutions to partial differential equations (PDEs) without access to ground-truth expressions. SymPlex formulates symbolic PDE solving as tree-structured decision-making and optimizes candidate solutions using only the PDE and its boundary conditions. At its core is SymFormer, a structure-aware Transformer that models hierarchical symbolic dependencies via tree-relative self-attention and enforces syntactic validity through grammar-constrained autoregressive decoding, overcoming the limited expressivity of sequence-based generators. Unlike numerical and neural approaches that approximate solutions in discretized or implicit function spaces, SymPlex operates directly in symbolic expression space, enabling interpretable and human-readable solutions that naturally represent non-smooth behavior and explicit parametric dependence. Empirical results demonstrate exact recovery of non-smooth and parametric PDE solutions using deep learning-based symbolic methods.</li>
</ul>

<h3>Title: They Said Memes Were Harmless-We Found the Ones That Hurt: Decoding Jokes, Symbols, and Cultural References</h3>
<ul>
<li><strong>Authors: </strong>Sahil Tripathi, Gautam Siddharth Kashyap, Mehwish Nasim, Jian Yang, Jiechao Gao, Usman Naseem</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03822">https://arxiv.org/abs/2602.03822</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03822">https://arxiv.org/pdf/2602.03822</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03822]] They Said Memes Were Harmless-We Found the Ones That Hurt: Decoding Jokes, Symbols, and Cultural References(https://arxiv.org/abs/2602.03822)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Meme-based social abuse detection is challenging because harmful intent often relies on implicit cultural symbolism and subtle cross-modal incongruence. Prior approaches, from fusion-based methods to in-context learning with Large Vision-Language Models (LVLMs), have made progress but remain limited by three factors: i) cultural blindness (missing symbolic context), ii) boundary ambiguity (satire vs. abuse confusion), and iii) lack of interpretability (opaque model reasoning). We introduce CROSS-ALIGN+, a three-stage framework that systematically addresses these limitations: (1) Stage I mitigates cultural blindness by enriching multimodal representations with structured knowledge from ConceptNet, Wikidata, and Hatebase; (2) Stage II reduces boundary ambiguity through parameter-efficient LoRA adapters that sharpen decision boundaries; and (3) Stage III enhances interpretability by generating cascaded explanations. Extensive experiments on five benchmarks and eight LVLMs demonstrate that CROSS-ALIGN+ consistently outperforms state-of-the-art methods, achieving up to 17% relative F1 improvement while providing interpretable justifications for each decision.</li>
</ul>

<h3>Title: Robust Intervention Learning from Emergency Stop Interventions</h3>
<ul>
<li><strong>Authors: </strong>Ethan Pronovost, Khimya Khetarpal, Siddhartha Srinivasa</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03825">https://arxiv.org/abs/2602.03825</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03825">https://arxiv.org/pdf/2602.03825</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03825]] Robust Intervention Learning from Emergency Stop Interventions(https://arxiv.org/abs/2602.03825)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Human interventions are a common source of data in autonomous systems during testing. These interventions provide an important signal about where the current policy needs improvement, but are often noisy and incomplete. We define Robust Intervention Learning (RIL) as the problem of learning from intervention data while remaining robust to the quality and informativeness of the intervention signal. In the best case, interventions are precise and avoiding them is sufficient to solve the task, but in many realistic settings avoiding interventions is necessary but not sufficient for achieving good performance. We study robust intervention learning in the context of emergency stop interventions and propose Residual Intervention Fine-Tuning (RIFT), a residual fine-tuning algorithm that treats intervention feedback as an incomplete learning signal and explicitly combines it with a prior policy. By framing intervention learning as a fine-tuning problem, our approach leverages structure encoded in the prior policy to resolve ambiguity when intervention signals under-specify the task. We provide theoretical analysis characterizing conditions under which this formulation yields principled policy improvement, and identify regimes where intervention learning is expected to fail. Our experiments reveal that residual fine-tuning enables robust and consistent policy improvement across a range of intervention strategies and prior policy qualities, and highlight robust intervention learning as a promising direction for future work.</li>
</ul>

<h3>Title: Continuous Control of Editing Models via Adaptive-Origin Guidance</h3>
<ul>
<li><strong>Authors: </strong>Alon Wolf, Chen Katzir, Kfir Aberman, Or Patashnik</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03826">https://arxiv.org/abs/2602.03826</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03826">https://arxiv.org/pdf/2602.03826</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03826]] Continuous Control of Editing Models via Adaptive-Origin Guidance(https://arxiv.org/abs/2602.03826)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion-based editing models have emerged as a powerful tool for semantic image and video manipulation. However, existing models lack a mechanism for smoothly controlling the intensity of text-guided edits. In standard text-conditioned generation, Classifier-Free Guidance (CFG) impacts prompt adherence, suggesting it as a potential control for edit intensity in editing models. However, we show that scaling CFG in these models does not produce a smooth transition between the input and the edited result. We attribute this behavior to the unconditional prediction, which serves as the guidance origin and dominates the generation at low guidance scales, while representing an arbitrary manipulation of the input content. To enable continuous control, we introduce Adaptive-Origin Guidance (AdaOr), a method that adjusts this standard guidance origin with an identity-conditioned adaptive origin, using an identity instruction corresponding to the identity manipulation. By interpolating this identity prediction with the standard unconditional prediction according to the edit strength, we ensure a continuous transition from the input to the edited result. We evaluate our method on image and video editing tasks, demonstrating that it provides smoother and more consistent control compared to current slider-based editing approaches. Our method incorporates an identity instruction into the standard training framework, enabling fine-grained control at inference time without per-edit procedure or reliance on specialized datasets.</li>
</ul>

<h3>Title: Accelerating Scientific Research with Gemini: Case Studies and Common Techniques</h3>
<ul>
<li><strong>Authors: </strong>David P. Woodruff, Vincent Cohen-Addad, Lalit Jain, Jieming Mao, Song Zuo, MohammadHossein Bateni, Simina Branzei, Michael P. Brenner, Lin Chen, Ying Feng, Lance Fortnow, Gang Fu, Ziyi Guan, Zahra Hadizadeh, Mohammad T. Hajiaghayi, Mahdi JafariRaviz, Adel Javanmard, Karthik C. S., Ken-ichi Kawarabayashi, Ravi Kumar, Silvio Lattanzi, Euiwoong Lee, Yi Li, Ioannis Panageas, Dimitris Paparas, Benjamin Przybocki, Bernardo Subercaseaux, Ola Svensson, Shayan Taherijam, Xuan Wu, Eylon Yogev, Morteza Zadimoghaddam, Samson Zhou, Vahab Mirrokni</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03837">https://arxiv.org/abs/2602.03837</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03837">https://arxiv.org/pdf/2602.03837</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03837]] Accelerating Scientific Research with Gemini: Case Studies and Common Techniques(https://arxiv.org/abs/2602.03837)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have opened new avenues for accelerating scientific research. While models are increasingly capable of assisting with routine tasks, their ability to contribute to novel, expert-level mathematical discovery is less understood. We present a collection of case studies demonstrating how researchers have successfully collaborated with advanced AI models, specifically Google's Gemini-based models (in particular Gemini Deep Think and its advanced variants), to solve open problems, refute conjectures, and generate new proofs across diverse areas in theoretical computer science, as well as other areas such as economics, optimization, and physics. Based on these experiences, we extract common techniques for effective human-AI collaboration in theoretical research, such as iterative refinement, problem decomposition, and cross-disciplinary knowledge transfer. While the majority of our results stem from this interactive, conversational methodology, we also highlight specific instances that push beyond standard chat interfaces. These include deploying the model as a rigorous adversarial reviewer to detect subtle flaws in existing proofs, and embedding it within a "neuro-symbolic" loop that autonomously writes and executes code to verify complex derivations. Together, these examples highlight the potential of AI not just as a tool for automation, but as a versatile, genuine partner in the creative process of scientific discovery.</li>
</ul>

<h3>Title: Understanding and Exploiting Weight Update Sparsity for Communication-Efficient Distributed RL</h3>
<ul>
<li><strong>Authors: </strong>Erfan Miahi, Eugene Belilovsky</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03839">https://arxiv.org/abs/2602.03839</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03839">https://arxiv.org/pdf/2602.03839</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03839]] Understanding and Exploiting Weight Update Sparsity for Communication-Efficient Distributed RL(https://arxiv.org/abs/2602.03839)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) is a critical component for post-training large language models (LLMs). However, in bandwidth-constrained distributed RL, scalability is often bottlenecked by the synchronization of policy weights from trainers to inference workers, particularly over commodity networks or in decentralized settings. While recent studies suggest that RL updates modify only a small fraction of model parameters, these observations are typically based on coarse checkpoint differences. We present a systematic empirical study of weight-update sparsity at both step-level and multi-step granularities, examining its evolution across training dynamics, off-policy delay, and model scale. We find that update sparsity is consistently high, frequently exceeding 99% across practically relevant settings. Leveraging this structure, we propose PULSE (Patch Updates via Lossless Sparse Encoding), a simple yet highly efficient lossless weight synchronization method that transmits only the indices and values of modified parameters. PULSE is robust to transmission errors and avoids floating-point drift inherent in additive delta schemes. In bandwidth-constrained decentralized environments, our approach achieves over 100x (14 GB to ~108 MB) communication reduction while maintaining bit-identical training dynamics and performance compared to full weight synchronization. By exploiting this structure, PULSE enables decentralized RL training to approach centralized throughput, reducing the bandwidth required for weight synchronization from 20 Gbit/s to 0.2 Gbit/s to maintain high GPU utilization.</li>
</ul>

<h3>Title: PLATE: Plasticity-Tunable Efficient Adapters for Geometry-Aware Continual Learning</h3>
<ul>
<li><strong>Authors: </strong>Romain Cosentino</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.03846">https://arxiv.org/abs/2602.03846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.03846">https://arxiv.org/pdf/2602.03846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.03846]] PLATE: Plasticity-Tunable Efficient Adapters for Geometry-Aware Continual Learning(https://arxiv.org/abs/2602.03846)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>We develop a continual learning method for pretrained models that \emph{requires no access to old-task data}, addressing a practical barrier in foundation model adaptation where pretraining distributions are often unavailable. Our key observation is that pretrained networks exhibit substantial \emph{geometric redundancy}, and that this redundancy can be exploited in two complementary ways. First, redundant neurons provide a proxy for dominant pretraining-era feature directions, enabling the construction of approximately protected update subspaces directly from pretrained weights. Second, redundancy offers a natural bias for \emph{where} to place plasticity: by restricting updates to a subset of redundant neurons and constraining the remaining degrees of freedom, we obtain update families with reduced functional drift on the old-data distribution and improved worst-case retention guarantees. These insights lead to \textsc{PLATE} (\textbf{Pla}sticity-\textbf{T}unable \textbf{E}fficient Adapters), a continual learning method requiring no past-task data that provides explicit control over the plasticity-retention trade-off. PLATE parameterizes each layer with a structured low-rank update $\Delta W = B A Q^\top$, where $B$ and $Q$ are computed once from pretrained weights and kept frozen, and only $A$ is trained on the new task. The code is available at this https URL.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
