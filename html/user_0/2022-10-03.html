<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: The More Secure, The Less Equally Usable: Gender and Ethnicity (Un)fairness of Deep Face Recognition along Security Thresholds. (arXiv:2209.15550v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15550">http://arxiv.org/abs/2209.15550</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15550] The More Secure, The Less Equally Usable: Gender and Ethnicity (Un)fairness of Deep Face Recognition along Security Thresholds](http://arxiv.org/abs/2209.15550)</code></li>
<li>Summary: <p>Face biometrics are playing a key role in making modern smart city
applications more secure and usable. Commonly, the recognition threshold of a
face recognition system is adjusted based on the degree of security for the
considered use case. The likelihood of a match can be for instance decreased by
setting a high threshold in case of a payment transaction verification. Prior
work in face recognition has unfortunately showed that error rates are usually
higher for certain demographic groups. These disparities have hence brought
into question the fairness of systems empowered with face biometrics. In this
paper, we investigate the extent to which disparities among demographic groups
change under different security levels. Our analysis includes ten face
recognition models, three security thresholds, and six demographic groups based
on gender and ethnicity. Experiments show that the higher the security of the
system is, the higher the disparities in usability among demographic groups
are. Compelling unfairness issues hence exist and urge countermeasures in
real-world high-stakes environments requiring severe security levels.
</p></li>
</ul>

<h3>Title: Hidden in Plain Sight: Exploring Encrypted Channels in Android apps. (arXiv:2209.15107v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15107">http://arxiv.org/abs/2209.15107</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15107] Hidden in Plain Sight: Exploring Encrypted Channels in Android apps](http://arxiv.org/abs/2209.15107)</code></li>
<li>Summary: <p>As privacy features in Android operating system improve, privacy-invasive
apps may gradually shift their focus to non-standard and covert channels for
leaking private user/device information. Such leaks also remain largely
undetected by state-of-the-art privacy analysis tools, which are very effective
in uncovering privacy exposures via regular HTTP and HTTPS channels. In this
study, we design and implement, ThirdEye, to significantly extend the
visibility of current privacy analysis tools, in terms of the exposures that
happen across various non-standard and covert channels, i.e., via any protocol
over TCP/UDP (beyond HTTP/S), and using multi-layer custom encryption over
HTTP/S and non-HTTP protocols. Besides network exposures, we also consider
covert channels via storage media that also leverage custom encryption layers.
Using ThirdEye, we analyzed 12,598 top-apps in various categories from
Androidrank, and found that 2887/12,598 (22.92%) apps used custom
encryption/decryption for network transmission and storing content in shared
device storage, and 2465/2887 (85.38%) of those apps sent device information
(e.g., advertising ID, list of installed apps) over the network that can
fingerprint users. Besides, 299 apps transmitted insecure encrypted content
over HTTP/non-HTTP protocols; 22 apps that used authentication tokens over
HTTPS, happen to expose them over insecure (albeit custom encrypted)
HTTP/non-HTTP channels. We found non-standard and covert channels with multiple
levels of obfuscation (e.g., encrypted data over HTTPS, encryption at nested
levels), and the use of vulnerable keys and cryptographic algorithms. Our
findings can provide valuable insights into the evolving field of non-standard
and covert channels, and help spur new countermeasures against such privacy
leakage and security issues.
</p></li>
</ul>

<h3>Title: Securing Large-Scale D2D Networks Using Covert Communication and Friendly Jamming. (arXiv:2209.15170v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15170">http://arxiv.org/abs/2209.15170</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15170] Securing Large-Scale D2D Networks Using Covert Communication and Friendly Jamming](http://arxiv.org/abs/2209.15170)</code></li>
<li>Summary: <p>We exploit both covert communication and friendly jamming to propose a
friendly jamming-assisted covert communication and use it to doubly secure a
large-scale device-to-device (D2D) network against eavesdroppers (i.e.,
wardens). The D2D transmitters defend against the wardens by: 1) hiding their
transmissions with enhanced covert communication, and 2) leveraging friendly
jamming to ensure information secrecy even if the D2D transmissions are
detected. We model the combat between the wardens and the D2D network (the
transmitters and the friendly jammers) as a two-stage Stackelberg game.
Therein, the wardens are the followers at the lower stage aiming to minimize
their detection errors, and the D2D network is the leader at the upper stage
aiming to maximize its utility (in terms of link reliability and communication
security) subject to the constraint on communication covertness. We apply
stochastic geometry to model the network spatial configuration so as to conduct
a system-level study. We develop a bi-level optimization algorithm to search
for the equilibrium of the proposed Stackelberg game based on the successive
convex approximation (SCA) method and Rosenbrock method. Numerical results
reveal interesting insights. We observe that without the assistance from the
jammers, it is difficult to achieve covert communication on D2D transmission.
Moreover, we illustrate the advantages of the proposed friendly
jamming-assisted covert communication by comparing it with the
information-theoretical secrecy approach in terms of the secure communication
probability and network utility.
</p></li>
</ul>

<h3>Title: Cerberus: A Formal Approach to Secure and Efficient Enclave Memory Sharing. (arXiv:2209.15253v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15253">http://arxiv.org/abs/2209.15253</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15253] Cerberus: A Formal Approach to Secure and Efficient Enclave Memory Sharing](http://arxiv.org/abs/2209.15253)</code></li>
<li>Summary: <p>Hardware enclaves rely on a disjoint memory model, which maps each physical
address to an enclave to achieve strong memory isolation. However, this
severely limits the performance and programmability of enclave programs. While
some prior work proposes enclave memory sharing, it does not provide a formal
model or verification of their designs. This paper presents Cerberus, a formal
approach to secure and efficient enclave memory sharing. To reduce the burden
of formal verification, we compare different sharing models and choose a simple
yet powerful sharing model. Based on the sharing model, Cerberus extends an
enclave platform such that enclave memory can be made immutable and shareable
across multiple enclaves via additional operations. We use incremental
verification starting with an existing formal model called the Trusted Abstract
Platform (TAP). Using our extended TAP model, we formally verify that Cerberus
does not break or weaken the security guarantees of the enclaves despite
allowing memory sharing. More specifically, we prove the Secure Remote
Execution (SRE) property on our formal model. Finally, the paper shows the
feasibility of Cerberus by implementing it in an existing enclave platform,
RISC-V Keystone.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Data Querying with Ciphertext Policy Attribute Based Encryption. (arXiv:2209.15103v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15103">http://arxiv.org/abs/2209.15103</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15103] Data Querying with Ciphertext Policy Attribute Based Encryption](http://arxiv.org/abs/2209.15103)</code></li>
<li>Summary: <p>Data encryption limits the power and efficiency of queries. Direct processing
of encrypted data should ideally be possible to avoid the need for data
decryption, processing, and re-encryption. It is vital to keep the data
searchable and sortable. That is, some information is intentionally leaked.
This intentional leakage technology is known as "querying over encrypted data
schemes", which offer confidentiality as well as querying over encrypted data,
but it is not meant to provide flexible access control. This paper suggests the
use of Ciphertext Policy Attributes Based Encryption (CP-ABE) to address three
security requirements, namely: confidentiality, queries over encrypted data,
and flexible access control. By combining flexible access control and data
confidentiality, CP-ABE can authenticate who can access data and possess the
secret key. Thus, this paper identifies how much data leakage there is in order
to figure out what kinds of operations are allowed when data is encrypted by
CP-ABE.
</p></li>
</ul>

<h3>Title: SoK: On the Impossible Security of Very Large Foundation Models. (arXiv:2209.15259v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15259">http://arxiv.org/abs/2209.15259</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15259] SoK: On the Impossible Security of Very Large Foundation Models](http://arxiv.org/abs/2209.15259)</code></li>
<li>Summary: <p>Large machine learning models, or so-called foundation models, aim to serve
as base-models for application-oriented machine learning. Although these models
showcase impressive performance, they have been empirically found to pose
serious security and privacy issues. We may however wonder if this is a
limitation of the current models, or if these issues stem from a fundamental
intrinsic impossibility of the foundation model learning problem itself. This
paper aims to systematize our knowledge supporting the latter. More precisely,
we identify several key features of today's foundation model learning problem
which, given the current understanding in adversarial machine learning, suggest
incompatibility of high accuracy with both security and privacy. We begin by
observing that high accuracy seems to require (1) very high-dimensional models
and (2) huge amounts of data that can only be procured through user-generated
datasets. Moreover, such data is fundamentally heterogeneous, as users
generally have very specific (easily identifiable) data-generating habits. More
importantly, users' data is filled with highly sensitive information, and maybe
heavily polluted by fake users. We then survey lower bounds on accuracy in
privacy-preserving and Byzantine-resilient heterogeneous learning that, we
argue, constitute a compelling case against the possibility of designing a
secure and privacy-preserving high-accuracy foundation model. We further stress
that our analysis also applies to other high-stake machine learning
applications, including content recommendation. We conclude by calling for
measures to prioritize security and privacy, and to slow down the race for ever
larger models.
</p></li>
</ul>

<h3>Title: Family-Based Fingerprint Analysis: A Position Paper. (arXiv:2209.15620v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15620">http://arxiv.org/abs/2209.15620</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15620] Family-Based Fingerprint Analysis: A Position Paper](http://arxiv.org/abs/2209.15620)</code></li>
<li>Summary: <p>Thousands of vulnerabilities are reported on a monthly basis to security
repositories, such as the National Vulnerability Database. Among these
vulnerabilities, software misconfiguration is one of the top 10 security risks
for web applications. With this large influx of vulnerability reports, software
fingerprinting has become a highly desired capability to discover distinctive
and efficient signatures and recognize reportedly vulnerable software
implementations. Due to the exponential worst-case complexity of fingerprint
matching, designing more efficient methods for fingerprinting becomes highly
desirable, especially for variability-intensive systems where optional features
add another exponential factor to its analysis. This position paper presents
our vision of a framework that lifts model learning and family-based analysis
principles to software fingerprinting. In this framework, we propose unifying
databases of signatures into a featured finite state machine and using presence
conditions to specify whether and in which circumstances a given input-output
trace is observed. We believe feature-based signatures can aid performance
improvements by reducing the size of fingerprints under analysis.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Visual Privacy Protection Based on Type-I Adversarial Attack. (arXiv:2209.15304v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15304">http://arxiv.org/abs/2209.15304</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15304] Visual Privacy Protection Based on Type-I Adversarial Attack](http://arxiv.org/abs/2209.15304)</code></li>
<li>Summary: <p>With the development of online artificial intelligence systems, many deep
neural networks (DNNs) have been deployed in cloud environments. In practical
applications, developers or users need to provide their private data to DNNs,
such as faces. However, data transmitted and stored in the cloud is insecure
and at risk of privacy leakage. In this work, inspired by Type-I adversarial
attack, we propose an adversarial attack-based method to protect visual privacy
of data. Specifically, the method encrypts the visual information of private
data while maintaining them correctly predicted by DNNs, without modifying the
model parameters. The empirical results on face recognition tasks show that the
proposed method can deeply hide the visual information in face images and
hardly affect the accuracy of the recognition models. In addition, we further
extend the method to classification tasks and also achieve state-of-the-art
performance.
</p></li>
</ul>

<h3>Title: L-SRR: Local Differential Privacy for Location-Based Services with Staircase Randomized Response. (arXiv:2209.15091v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15091">http://arxiv.org/abs/2209.15091</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15091] L-SRR: Local Differential Privacy for Location-Based Services with Staircase Randomized Response](http://arxiv.org/abs/2209.15091)</code></li>
<li>Summary: <p>Location-based services (LBS) have been significantly developed and widely
deployed in mobile devices. It is also well-known that LBS applications may
result in severe privacy concerns by collecting sensitive locations. A strong
privacy model ''local differential privacy'' (LDP) has been recently deployed
in many different applications (e.g., Google RAPPOR, iOS, and Microsoft
Telemetry) but not effective for LBS applications due to the low utility of
existing LDP mechanisms. To address such deficiency, we propose the first LDP
framework for a variety of location-based services (namely ''L-SRR''), which
privately collects and analyzes user locations with high utility. Specifically,
we design a novel randomization mechanism ''Staircase Randomized Response''
(SRR) and extend the empirical estimation to significantly boost the utility
for SRR in different LBS applications (e.g., traffic density estimation, and
k-nearest neighbors). We have conducted extensive experiments on four real LBS
datasets by benchmarking with other LDP schemes in practical applications. The
experimental results demonstrate that L-SRR significantly outperforms them.
</p></li>
</ul>

<h3>Title: Individual Privacy Accounting with Gaussian Differential Privacy. (arXiv:2209.15596v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15596">http://arxiv.org/abs/2209.15596</a></li>
<li>Code URL: <a href="https://github.com/dpbayes/individual-accounting-gdp">https://github.com/dpbayes/individual-accounting-gdp</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15596] Individual Privacy Accounting with Gaussian Differential Privacy](http://arxiv.org/abs/2209.15596)</code></li>
<li>Summary: <p>Individual privacy accounting enables bounding differential privacy (DP) loss
individually for each participant involved in the analysis. This can be
informative as often the individual privacy losses are considerably smaller
than those indicated by the DP bounds that are based on considering worst-case
bounds at each data access. In order to account for the individual privacy
losses in a principled manner, we need a privacy accountant for adaptive
compositions of randomised mechanisms, where the loss incurred at a given data
access is allowed to be smaller than the worst-case loss. This kind of analysis
has been carried out for the R\'enyi differential privacy (RDP) by Feldman and
Zrnic (2021), however not yet for the so-called optimal privacy accountants. We
make first steps in this direction by providing a careful analysis using the
Gaussian differential privacy which gives optimal bounds for the Gaussian
mechanism, one of the most versatile DP mechanisms. This approach is based on
determining a certain supermartingale for the hockey-stick divergence and on
extending the R\'enyi divergence-based fully adaptive composition results by
Feldman and Zrnic (2021). We also consider measuring the individual
$(\varepsilon,\delta)$-privacy losses using the so-called privacy loss
distributions. With the help of the Blackwell theorem, we can then make use of
the RDP analysis to construct an approximative individual
$(\varepsilon,\delta)$-accountant.
</p></li>
</ul>

<h3>Title: Machine Unlearning Method Based On Projection Residual. (arXiv:2209.15276v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15276">http://arxiv.org/abs/2209.15276</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15276] Machine Unlearning Method Based On Projection Residual](http://arxiv.org/abs/2209.15276)</code></li>
<li>Summary: <p>Machine learning models (mainly neural networks) are used more and more in
real life. Users feed their data to the model for training. But these processes
are often one-way. Once trained, the model remembers the data. Even when data
is removed from the dataset, the effects of these data persist in the model.
With more and more laws and regulations around the world protecting data
privacy, it becomes even more important to make models forget this data
completely through machine unlearning.
</p></li>
</ul>

<p>This paper adopts the projection residual method based on Newton iteration
method. The main purpose is to implement machine unlearning tasks in the
context of linear regression models and neural network models. This method
mainly uses the iterative weighting method to completely forget the data and
its corresponding influence, and its computational cost is linear in the
feature dimension of the data. This method can improve the current machine
learning method. At the same time, it is independent of the size of the
training set. Results were evaluated by feature injection testing (FIT).
Experiments show that this method is more thorough in deleting data, which is
close to model retraining.
</p>

<h3>Title: TabDDPM: Modelling Tabular Data with Diffusion Models. (arXiv:2209.15421v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15421">http://arxiv.org/abs/2209.15421</a></li>
<li>Code URL: <a href="https://github.com/rotot0/tab-ddpm">https://github.com/rotot0/tab-ddpm</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15421] TabDDPM: Modelling Tabular Data with Diffusion Models](http://arxiv.org/abs/2209.15421)</code></li>
<li>Summary: <p>Denoising diffusion probabilistic models are currently becoming the leading
paradigm of generative modeling for many important data modalities. Being the
most prevalent in the computer vision community, diffusion models have also
recently gained some attention in other domains, including speech, NLP, and
graph-like data. In this work, we investigate if the framework of diffusion
models can be advantageous for general tabular problems, where datapoints are
typically represented by vectors of heterogeneous features. The inherent
heterogeneity of tabular data makes it quite challenging for accurate modeling,
since the individual features can be of completely different nature, i.e., some
of them can be continuous and some of them can be discrete. To address such
data types, we introduce TabDDPM -- a diffusion model that can be universally
applied to any tabular dataset and handles any type of feature. We extensively
evaluate TabDDPM on a wide set of benchmarks and demonstrate its superiority
over existing GAN/VAE alternatives, which is consistent with the advantage of
diffusion models in other fields. Additionally, we show that TabDDPM is
eligible for privacy-oriented setups, where the original datapoints cannot be
publicly shared.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Anomaly localization for copy detection patterns through print estimations. (arXiv:2209.15625v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15625">http://arxiv.org/abs/2209.15625</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15625] Anomaly localization for copy detection patterns through print estimations](http://arxiv.org/abs/2209.15625)</code></li>
<li>Summary: <p>Copy detection patterns (CDP) are recent technologies for protecting products
from counterfeiting. However, in contrast to traditional copy fakes, deep
learning-based fakes have shown to be hardly distinguishable from originals by
traditional authentication systems. Systems based on classical supervised
learning and digital templates assume knowledge of fake CDP at training time
and cannot generalize to unseen types of fakes. Authentication based on printed
copies of originals is an alternative that yields better results even for
unseen fakes and simple authentication metrics but comes at the impractical
cost of acquisition and storage of printed copies. In this work, to overcome
these shortcomings, we design a machine learning (ML) based authentication
system that only requires digital templates and printed original CDP for
training, whereas authentication is based solely on digital templates, which
are used to estimate original printed codes. The obtained results show that the
proposed system can efficiently authenticate original and detect fake CDP by
accurately locating the anomalies in the fake CDP. The empirical evaluation of
the authentication system under investigation is performed on the original and
ML-based fakes CDP printed on two industrial printers.
</p></li>
</ul>

<h3>Title: A Survey: Implementations of Non-fungible Token System in Different Fields. (arXiv:2209.15288v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15288">http://arxiv.org/abs/2209.15288</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15288] A Survey: Implementations of Non-fungible Token System in Different Fields](http://arxiv.org/abs/2209.15288)</code></li>
<li>Summary: <p>In the realm of digital art and collectibles, NFTs are sweeping the board.
Because of the massive sales to a new crypto audience, the livelihoods of
digital artists are being transformed. It is no surprise that celebs are
jumping on the bandwagon. It is a fact that NFTs can be used in multiple ways,
including digital artwork such as animation, character design, digital
painting, collection of selfies or vlogs, and many more digital entities. As a
result, they may be used to signify the possession of any specific object,
whether it be digital or physical. NFTs are digital tokens that may be used to
indicate ownership of one of a-kind goods. For example, I can buy a shoe or T
shirt from any store, and then if the store provides me the same 3D model of
that T-Shirt or shoe of the exact same design and color, it would be more
connected with my feelings. They enable us to tokenize items such as artwork,
valuables, and even real estate. NFTs can only be owned by one person at a
time, and they are protected by the Ethereum blockchain no one can alter the
ownership record or create a new NFT. The word non-fungible can be used to
describe items like your furniture, a song file, or your computer. It is
impossible to substitute these goods with anything else because they each have
their own distinct characteristics. The goal was to find all the existing
implementations of Non-fungible Tokens in different fields of recent
technology, so that an overall overview of future implementations of NFT can be
found and how it can be used to enrich user experiences.
</p></li>
</ul>

<h2>defense</h2>
<h2>attack</h2>
<h3>Title: Physical Adversarial Attack meets Computer Vision: A Decade Survey. (arXiv:2209.15179v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15179">http://arxiv.org/abs/2209.15179</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15179] Physical Adversarial Attack meets Computer Vision: A Decade Survey](http://arxiv.org/abs/2209.15179)</code></li>
<li>Summary: <p>Although Deep Neural Networks (DNNs) have achieved impressive results in
computer vision, their exposed vulnerability to adversarial attacks remains a
serious concern. A series of works has shown that by adding elaborate
perturbations to images, DNNs could have catastrophic degradation in
performance metrics. And this phenomenon does not only exist in the digital
space but also in the physical space. Therefore, estimating the security of
these DNNs-based systems is critical for safely deploying them in the real
world, especially for security-critical applications, e.g., autonomous cars,
video surveillance, and medical diagnosis. In this paper, we focus on physical
adversarial attacks and provide a comprehensive survey of over 150 existing
papers. We first clarify the concept of the physical adversarial attack and
analyze its characteristics. Then, we define the adversarial medium, essential
to perform attacks in the physical world. Next, we present the physical
adversarial attack methods in task order: classification, detection, and
re-identification, and introduce their performance in solving the trilemma:
effectiveness, stealthiness, and robustness. In the end, we discuss the current
challenges and potential future directions.
</p></li>
</ul>

<h3>Title: Reliable Face Morphing Attack Detection in On-The-Fly Border Control Scenario with Variation in Image Resolution and Capture Distance. (arXiv:2209.15474v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15474">http://arxiv.org/abs/2209.15474</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15474] Reliable Face Morphing Attack Detection in On-The-Fly Border Control Scenario with Variation in Image Resolution and Capture Distance](http://arxiv.org/abs/2209.15474)</code></li>
<li>Summary: <p>Face Recognition Systems (FRS) are vulnerable to various attacks performed
directly and indirectly. Among these attacks, face morphing attacks are highly
potential in deceiving automatic FRS and human observers and indicate a severe
security threat, especially in the border control scenario. This work presents
a face morphing attack detection, especially in the On-The-Fly (OTF) Automatic
Border Control (ABC) scenario. We present a novel Differential-MAD (D-MAD)
algorithm based on the spherical interpolation and hierarchical fusion of deep
features computed from six different pre-trained deep Convolutional Neural
Networks (CNNs). Extensive experiments are carried out on the newly generated
face morphing dataset (SCFace-Morph) based on the publicly available SCFace
dataset by considering the real-life scenario of Automatic Border Control (ABC)
gates. Experimental protocols are designed to benchmark the proposed and
state-of-the-art (SOTA) D-MAD techniques for different camera resolutions and
capture distances. Obtained results have indicated the superior performance of
the proposed D-MAD method compared to the existing methods.
</p></li>
</ul>

<h3>Title: Impact of Face Image Quality Estimation on Presentation Attack Detection. (arXiv:2209.15489v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15489">http://arxiv.org/abs/2209.15489</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15489] Impact of Face Image Quality Estimation on Presentation Attack Detection](http://arxiv.org/abs/2209.15489)</code></li>
<li>Summary: <p>Non-referential face image quality assessment methods have gained popularity
as a pre-filtering step on face recognition systems. In most of them, the
quality score is usually designed with face matching in mind. However, a small
amount of work has been done on measuring their impact and usefulness on
Presentation Attack Detection (PAD). In this paper, we study the effect of
quality assessment methods on filtering bona fide and attack samples, their
impact on PAD systems, and how the performance of such systems is improved when
training on a filtered (by quality) dataset. On a Vision Transformer PAD
algorithm, a reduction of 20% of the training dataset by removing lower quality
samples allowed us to improve the BPCER by 3% in a cross-dataset test.
</p></li>
</ul>

<h3>Title: Augmentation Backdoors. (arXiv:2209.15139v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15139">http://arxiv.org/abs/2209.15139</a></li>
<li>Code URL: <a href="https://github.com/slkdfjslkjfd/augmentation_backdoors">https://github.com/slkdfjslkjfd/augmentation_backdoors</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15139] Augmentation Backdoors](http://arxiv.org/abs/2209.15139)</code></li>
<li>Summary: <p>Data augmentation is used extensively to improve model generalisation.
However, reliance on external libraries to implement augmentation methods
introduces a vulnerability into the machine learning pipeline. It is well known
that backdoors can be inserted into machine learning models through serving a
modified dataset to train on. Augmentation therefore presents a perfect
opportunity to perform this modification without requiring an initially
backdoored dataset. In this paper we present three backdoor attacks that can be
covertly inserted into data augmentation. Our attacks each insert a backdoor
using a different type of computer vision augmentation transform, covering
simple image transforms, GAN-based augmentation, and composition-based
augmentation. By inserting the backdoor using these augmentation transforms, we
make our backdoors difficult to detect, while still supporting arbitrary
backdoor functionality. We evaluate our attacks on a range of computer vision
benchmarks and demonstrate that an attacker is able to introduce backdoors
through just a malicious augmentation routine.
</p></li>
</ul>

<h3>Title: Data Poisoning Attacks Against Multimodal Encoders. (arXiv:2209.15266v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15266">http://arxiv.org/abs/2209.15266</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15266] Data Poisoning Attacks Against Multimodal Encoders](http://arxiv.org/abs/2209.15266)</code></li>
<li>Summary: <p>Traditional machine learning (ML) models usually rely on large-scale labeled
datasets to achieve strong performance. However, such labeled datasets are
often challenging and expensive to obtain. Also, the predefined categories
limit the model's ability to generalize to other visual concepts as additional
labeled data is required. On the contrary, the newly emerged multimodal model,
which contains both visual and linguistic modalities, learns the concept of
images from the raw text. It is a promising way to solve the above problems as
it can use easy-to-collect image-text pairs to construct the training dataset
and the raw texts contain almost unlimited categories according to their
semantics. However, learning from a large-scale unlabeled dataset also exposes
the model to the risk of potential poisoning attacks, whereby the adversary
aims to perturb the model's training dataset to trigger malicious behaviors in
it. Previous work mainly focuses on the visual modality. In this paper, we
instead focus on answering two questions: (1) Is the linguistic modality also
vulnerable to poisoning attacks? and (2) Which modality is most vulnerable? To
answer the two questions, we conduct three types of poisoning attacks against
CLIP, the most representative multimodal contrastive learning framework.
Extensive evaluations on different datasets and model architectures show that
all three attacks can perform well on the linguistic modality with only a
relatively low poisoning rate and limited epochs. Also, we observe that the
poisoning effect differs between different modalities, i.e., with lower MinRank
in the visual modality and with higher Hit@K when K is small in the linguistic
modality. To mitigate the attacks, we propose both pre-training and
post-training defenses. We empirically show that both defenses can
significantly reduce the attack performance while preserving the model's
utility.
</p></li>
</ul>

<h3>Title: Wi-attack: Cross-technology Impersonation Attack against iBeacon Services. (arXiv:2209.15322v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15322">http://arxiv.org/abs/2209.15322</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15322] Wi-attack: Cross-technology Impersonation Attack against iBeacon Services](http://arxiv.org/abs/2209.15322)</code></li>
<li>Summary: <p>iBeacon protocol is widely deployed to provide location-based services. By
receiving its BLE advertisements, nearby devices can estimate the proximity to
the iBeacon or calculate indoor positions. However, the open nature of these
advertisements brings vulnerability to impersonation attacks. Such attacks
could lead to spam, unreliable positioning, and even security breaches. In this
paper, we propose Wi-attack, revealing the feasibility of using WiFi devices to
conduct impersonation attacks on iBeacon services. Different from impersonation
attacks using BLE compatible hardware, Wi-attack is not restricted by
broadcasting intervals and is able to impersonate multiple iBeacons at the same
time. Effective attacks can be launched on iBeacon services without
modifications to WiFi hardware or firmware. To enable direct communication from
WiFi to BLE, we use the digital emulation technique of cross technology
communication. To enhance the packet reception along with its stability, we add
redundant packets to eliminate cyclic prefix error entirely. The emulation
provides an iBeacon packet reception rate up to 66.2%. We conduct attacks on
three iBeacon services scenarios, point deployment, multilateration, and
fingerprint-based localization. The evaluation results show that Wi-attack can
bring an average distance error of more than 20 meters on fingerprint-based
localization using only 3 APs.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Generalizability of Adversarial Robustness Under Distribution Shifts. (arXiv:2209.15042v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15042">http://arxiv.org/abs/2209.15042</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15042] Generalizability of Adversarial Robustness Under Distribution Shifts](http://arxiv.org/abs/2209.15042)</code></li>
<li>Summary: <p>Recent progress in empirical and certified robustness promises to deliver
reliable and deployable Deep Neural Networks (DNNs). Despite that success, most
existing evaluations of DNN robustness have been done on images sampled from
the same distribution that the model was trained on. Yet, in the real world,
DNNs may be deployed in dynamic environments that exhibit significant
distribution shifts. In this work, we take a first step towards thoroughly
investigating the interplay between empirical and certified adversarial
robustness on one hand and domain generalization on another. To do so, we train
robust models on multiple domains and evaluate their accuracy and robustness on
an unseen domain. We observe that: (1) both empirical and certified robustness
generalize to unseen domains, and (2) the level of generalizability does not
correlate well with input visual similarity, measured by the FID between source
and target domains. We also extend our study to cover a real-world medical
application, in which adversarial augmentation enhances both the robustness and
generalization accuracy in unseen domains.
</p></li>
</ul>

<h3>Title: 3D UX-Net: A Large Kernel Volumetric ConvNet Modernizing Hierarchical Transformer for Medical Image Segmentation. (arXiv:2209.15076v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15076">http://arxiv.org/abs/2209.15076</a></li>
<li>Code URL: <a href="https://github.com/masilab/3dux-net">https://github.com/masilab/3dux-net</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15076] 3D UX-Net: A Large Kernel Volumetric ConvNet Modernizing Hierarchical Transformer for Medical Image Segmentation](http://arxiv.org/abs/2209.15076)</code></li>
<li>Summary: <p>Vision transformers (ViTs) have quickly superseded convolutional networks
(ConvNets) as the current state-of-the-art (SOTA) models for medical image
segmentation. Hierarchical transformers (e.g., Swin Transformers) reintroduced
several ConvNet priors and further enhanced the practical viability of adapting
volumetric segmentation in 3D medical datasets. The effectiveness of hybrid
approaches is largely credited to the large receptive field for non-local
self-attention and the large number of model parameters. In this work, we
propose a lightweight volumetric ConvNet, termed 3D UX-Net, which adapts the
hierarchical transformer using ConvNet modules for robust volumetric
segmentation. Specifically, we revisit volumetric depth-wise convolutions with
large kernel size (e.g. starting from $7\times7\times7$) to enable the larger
global receptive fields, inspired by Swin Transformer. We further substitute
the multi-layer perceptron (MLP) in Swin Transformer blocks with pointwise
depth convolutions and enhance model performances with fewer normalization and
activation layers, thus reducing the number of model parameters. 3D UX-Net
competes favorably with current SOTA transformers (e.g. SwinUNETR) using three
challenging public datasets on volumetric brain and abdominal imaging: 1)
MICCAI Challenge 2021 FLARE, 2) MICCAI Challenge 2021 FeTA, and 3) MICCAI
Challenge 2022 AMOS. 3D UX-Net consistently outperforms SwinUNETR with
improvement from 0.929 to 0.938 Dice (FLARE2021) and 0.867 to 0.874 Dice
(Feta2021). We further evaluate the transfer learning capability of 3D UX-Net
with AMOS2022 and demonstrates another improvement of $2.27\%$ Dice (from 0.880
to 0.900). The source code with our proposed model are available at
https://github.com/MASILab/3DUX-Net.
</p></li>
</ul>

<h3>Title: Your Out-of-Distribution Detection Method is Not Robust!. (arXiv:2209.15246v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15246">http://arxiv.org/abs/2209.15246</a></li>
<li>Code URL: <a href="https://github.com/rohban-lab/atd">https://github.com/rohban-lab/atd</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15246] Your Out-of-Distribution Detection Method is Not Robust!](http://arxiv.org/abs/2209.15246)</code></li>
<li>Summary: <p>Out-of-distribution (OOD) detection has recently gained substantial attention
due to the importance of identifying out-of-domain samples in reliability and
safety. Although OOD detection methods have advanced by a great deal, they are
still susceptible to adversarial examples, which is a violation of their
purpose. To mitigate this issue, several defenses have recently been proposed.
Nevertheless, these efforts remained ineffective, as their evaluations are
based on either small perturbation sizes, or weak attacks. In this work, we
re-examine these defenses against an end-to-end PGD attack on in/out data with
larger perturbation sizes, e.g. up to commonly used $\epsilon=8/255$ for the
CIFAR-10 dataset. Surprisingly, almost all of these defenses perform worse than
a random detection under the adversarial setting. Next, we aim to provide a
robust OOD detection method. In an ideal defense, the training should expose
the model to almost all possible adversarial perturbations, which can be
achieved through adversarial training. That is, such training perturbations
should based on both in- and out-of-distribution samples. Therefore, unlike OOD
detection in the standard setting, access to OOD, as well as in-distribution,
samples sounds necessary in the adversarial training setup. These tips lead us
to adopt generative OOD detection methods, such as OpenGAN, as a baseline. We
subsequently propose the Adversarially Trained Discriminator (ATD), which
utilizes a pre-trained robust model to extract robust features, and a generator
model to create OOD samples. Using ATD with CIFAR-10 and CIFAR-100 as the
in-distribution data, we could significantly outperform all previous methods in
the robust AUROC while maintaining high standard AUROC and classification
accuracy. The code repository is available at https://github.com/rohban-lab/ATD .
</p></li>
</ul>

<h3>Title: ERNIE-ViL 2.0: Multi-view Contrastive Learning for Image-Text Pre-training. (arXiv:2209.15270v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15270">http://arxiv.org/abs/2209.15270</a></li>
<li>Code URL: <a href="https://github.com/PaddlePaddle/ERNIE">https://github.com/PaddlePaddle/ERNIE</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15270] ERNIE-ViL 2](http://arxiv.org/abs/2209.15270)</code></li>
<li>Summary: <p>Recent Vision-Language Pre-trained (VLP) models based on dual encoder have
attracted extensive attention from academia and industry due to their superior
performance on various cross-modal tasks and high computational efficiency.
They attempt to learn cross-modal representation using contrastive learning on
image-text pairs, however, the built inter-modal correlations only rely on a
single view for each modality. Actually, an image or a text contains various
potential views, just as humans could capture a real-world scene via diverse
descriptions or photos. In this paper, we propose ERNIE-ViL 2.0, a Multi-View
Contrastive learning framework to build intra-modal and inter-modal
correlations between diverse views simultaneously, aiming at learning a more
robust cross-modal representation. Specifically, we construct multiple views
within each modality to learn the intra-modal correlation for enhancing the
single-modal representation. Besides the inherent visual/textual views, we
construct sequences of object tags as a special textual view to narrow the
cross-modal semantic gap on noisy image-text pairs. Pre-trained with 29M
publicly available datasets, ERNIE-ViL 2.0 achieves competitive results on
English cross-modal retrieval. Additionally, to generalize our method to
Chinese cross-modal tasks, we train ERNIE-ViL 2.0 through scaling up the
pre-training datasets to 1.5B Chinese image-text pairs, resulting in
significant improvements compared to previous SOTA results on Chinese
cross-modal retrieval. We release our pre-trained models in
https://github.com/PaddlePaddle/ERNIE.
</p></li>
</ul>

<h3>Title: Towards General-Purpose Representation Learning of Polygonal Geometries. (arXiv:2209.15458v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15458">http://arxiv.org/abs/2209.15458</a></li>
<li>Code URL: <a href="https://github.com/gengchenmai/polygon_encoder">https://github.com/gengchenmai/polygon_encoder</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15458] Towards General-Purpose Representation Learning of Polygonal Geometries](http://arxiv.org/abs/2209.15458)</code></li>
<li>Summary: <p>Neural network representation learning for spatial data is a common need for
geographic artificial intelligence (GeoAI) problems. In recent years, many
advancements have been made in representation learning for points, polylines,
and networks, whereas little progress has been made for polygons, especially
complex polygonal geometries. In this work, we focus on developing a
general-purpose polygon encoding model, which can encode a polygonal geometry
(with or without holes, single or multipolygons) into an embedding space. The
result embeddings can be leveraged directly (or finetuned) for downstream tasks
such as shape classification, spatial relation prediction, and so on. To
achieve model generalizability guarantees, we identify a few desirable
properties: loop origin invariance, trivial vertex invariance, part permutation
invariance, and topology awareness. We explore two different designs for the
encoder: one derives all representations in the spatial domain; the other
leverages spectral domain representations. For the spatial domain approach, we
propose ResNet1D, a 1D CNN-based polygon encoder, which uses circular padding
to achieve loop origin invariance on simple polygons. For the spectral domain
approach, we develop NUFTspec based on Non-Uniform Fourier Transformation
(NUFT), which naturally satisfies all the desired properties. We conduct
experiments on two tasks: 1) shape classification based on MNIST; 2) spatial
relation prediction based on two new datasets - DBSR-46K and DBSR-cplx46K. Our
results show that NUFTspec and ResNet1D outperform multiple existing baselines
with significant margins. While ResNet1D suffers from model performance
degradation after shape-invariance geometry modifications, NUFTspec is very
robust to these modifications due to the nature of the NUFT.
</p></li>
</ul>

<h3>Title: Few-shot Text Classification with Dual Contrastive Consistency. (arXiv:2209.15069v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15069">http://arxiv.org/abs/2209.15069</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15069] Few-shot Text Classification with Dual Contrastive Consistency](http://arxiv.org/abs/2209.15069)</code></li>
<li>Summary: <p>In this paper, we explore how to utilize pre-trained language model to
perform few-shot text classification where only a few annotated examples are
given for each class. Since using traditional cross-entropy loss to fine-tune
language model under this scenario causes serious overfitting and leads to
sub-optimal generalization of model, we adopt supervised contrastive learning
on few labeled data and consistency-regularization on vast unlabeled data.
Moreover, we propose a novel contrastive consistency to further boost model
performance and refine sentence representation. After conducting extensive
experiments on four datasets, we demonstrate that our model (FTCC) can
outperform state-of-the-art methods and has better robustness.
</p></li>
</ul>

<h3>Title: On The Robustness of Self-Supervised Representations for Spoken Language Modeling. (arXiv:2209.15483v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15483">http://arxiv.org/abs/2209.15483</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15483] On The Robustness of Self-Supervised Representations for Spoken Language Modeling](http://arxiv.org/abs/2209.15483)</code></li>
<li>Summary: <p>Self-supervised representations have been extensively studied for
discriminative and generative tasks. However, their robustness capabilities
have not been extensively investigated. This work focuses on self-supervised
representations for spoken generative language models. First, we empirically
demonstrate how current state-of-the-art speech representation models lack
robustness to basic signal variations that do not alter the spoken information.
To overcome this, we propose an effective and efficient method to learn robust
self-supervised speech representation for generative spoken language modeling.
The proposed approach is based on applying a set of signal transformations to
the speech signal and optimizing the model using an iterative pseudo-labeling
scheme. Our method significantly improves over the evaluated baselines when
considering encoding metrics. We additionally evaluate our method on the
speech-to-speech translation task. We consider Spanish-English and
French-English conversions and empirically demonstrate the benefits of
following the proposed approach.
</p></li>
</ul>

<h3>Title: OAK4XAI: Model towards Out-Of-Box eXplainable Artificial Intelligence for Digital Agriculture. (arXiv:2209.15104v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15104">http://arxiv.org/abs/2209.15104</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15104] OAK4XAI: Model towards Out-Of-Box eXplainable Artificial Intelligence for Digital Agriculture](http://arxiv.org/abs/2209.15104)</code></li>
<li>Summary: <p>Recent machine learning approaches have been effective in Artificial
Intelligence (AI) applications. They produce robust results with a high level
of accuracy. However, most of these techniques do not provide
human-understandable explanations for supporting their results and decisions.
They usually act as black boxes, and it is not easy to understand how decisions
have been made. Explainable Artificial Intelligence (XAI), which has received
much interest recently, tries to provide human-understandable explanations for
decision-making and trained AI models. For instance, in digital agriculture,
related domains often present peculiar or input features with no link to
background knowledge. The application of the data mining process on
agricultural data leads to results (knowledge), which are difficult to explain.
In this paper, we propose a knowledge map model and an ontology design as an
XAI framework (OAK4XAI) to deal with this issue. The framework does not only
consider the data analysis part of the process, but it takes into account the
semantics aspect of the domain knowledge via an ontology and a knowledge map
model, provided as modules of the framework. Many ongoing XAI studies aim to
provide accurate and verbalizable accounts for how given feature values
contribute to model decisions. The proposed approach, however, focuses on
providing consistent information and definitions of concepts, algorithms, and
values involved in the data mining models. We built an Agriculture Computing
Ontology (AgriComO) to explain the knowledge mined in agriculture. AgriComO has
a well-designed structure and includes a wide range of concepts and
transformations suitable for agriculture and computing domains.
</p></li>
</ul>

<h3>Title: Ensemble Machine Learning Model Trained on a New Synthesized Dataset Generalizes Well for Stress Prediction Using Wearable Devices. (arXiv:2209.15146v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15146">http://arxiv.org/abs/2209.15146</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15146] Ensemble Machine Learning Model Trained on a New Synthesized Dataset Generalizes Well for Stress Prediction Using Wearable Devices](http://arxiv.org/abs/2209.15146)</code></li>
<li>Summary: <p>Introduction. We investigate the generalization ability of models built on
datasets containing a small number of subjects, recorded in single study
protocols. Next, we propose and evaluate methods combining these datasets into
a single, large dataset. Finally, we propose and evaluate the use of ensemble
techniques by combining gradient boosting with an artificial neural network to
measure predictive power on new, unseen data.
</p></li>
</ul>

<p>Methods. Sensor biomarker data from six public datasets were utilized in this
study. To test model generalization, we developed a gradient boosting model
trained on one dataset (SWELL), and tested its predictive power on two datasets
previously used in other studies (WESAD, NEURO). Next, we merged four small
datasets, i.e. (SWELL, NEURO, WESAD, UBFC-Phys), to provide a combined total of
99 subjects,. In addition, we utilized random sampling combined with another
dataset (EXAM) to build a larger training dataset consisting of 200 synthesized
subjects,. Finally, we developed an ensemble model that combines our gradient
boosting model with an artificial neural network, and tested it on two
additional, unseen publicly available stress datasets (WESAD and Toadstool).
</p>
<p>Results. Our method delivers a robust stress measurement system capable of
achieving 85% predictive accuracy on new, unseen validation data, achieving a
25% performance improvement over single models trained on small datasets.
</p>
<p>Conclusion. Models trained on small, single study protocol datasets do not
generalize well for use on new, unseen data and lack statistical power.
Ma-chine learning models trained on a dataset containing a larger number of
varied study subjects capture physiological variance better, resulting in more
robust stress detection.
</p>

<h3>Title: Online Multi-Agent Decentralized Byzantine-robust Gradient Estimation. (arXiv:2209.15274v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15274">http://arxiv.org/abs/2209.15274</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15274] Online Multi-Agent Decentralized Byzantine-robust Gradient Estimation](http://arxiv.org/abs/2209.15274)</code></li>
<li>Summary: <p>In this paper, we propose an iterative scheme for distributed
Byzantineresilient estimation of a gradient associated with a black-box model.
Our algorithm is based on simultaneous perturbation, secure state estimation
and two-timescale stochastic approximations. We also show the performance of
our algorithm through numerical experiments.
</p></li>
</ul>

<h3>Title: Observational Robustness and Invariances in Reinforcement Learning via Lexicographic Objectives. (arXiv:2209.15320v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15320">http://arxiv.org/abs/2209.15320</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15320] Observational Robustness and Invariances in Reinforcement Learning via Lexicographic Objectives](http://arxiv.org/abs/2209.15320)</code></li>
<li>Summary: <p>Policy robustness in Reinforcement Learning (RL) may not be desirable at any
price; the alterations caused by robustness requirements from otherwise optimal
policies should be explainable and quantifiable. Policy gradient algorithms
that have strong convergence guarantees are usually modified to obtain robust
policies in ways that do not preserve algorithm guarantees, which defeats the
purpose of formal robustness requirements. In this work we study a notion of
robustness in partially observable MDPs where state observations are perturbed
by a noise-induced stochastic kernel. We characterise the set of policies that
are maximally robust by analysing how the policies are altered by this kernel.
We then establish a connection between such robust policies and certain
properties of the noise kernel, as well as with structural properties of the
underlying MDPs, constructing sufficient conditions for policy robustness. We
use these notions to propose a robustness-inducing scheme, applicable to any
policy gradient algorithm, to formally trade off the reward achieved by a
policy with its robustness level through lexicographic optimisation, which
preserves convergence properties of the original algorithm. We test the the
proposed approach through numerical experiments on safety-critical RL
environments, and show how the proposed method helps achieve high robustness
when state errors are introduced in the policy roll-out.
</p></li>
</ul>

<h3>Title: Momentum Tracking: Momentum Acceleration for Decentralized Deep Learning on Heterogeneous Data. (arXiv:2209.15505v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15505">http://arxiv.org/abs/2209.15505</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15505] Momentum Tracking: Momentum Acceleration for Decentralized Deep Learning on Heterogeneous Data](http://arxiv.org/abs/2209.15505)</code></li>
<li>Summary: <p>SGD with momentum acceleration is one of the key components for improving the
performance of neural networks. For decentralized learning, a straightforward
approach using momentum acceleration is Distributed SGD (DSGD) with momentum
acceleration (DSGDm). However, DSGDm performs worse than DSGD when the data
distributions are statistically heterogeneous. Recently, several studies have
addressed this issue and proposed methods with momentum acceleration that are
more robust to data heterogeneity than DSGDm, although their convergence rates
remain dependent on data heterogeneity and decrease when the data distributions
are heterogeneous. In this study, we propose Momentum Tracking, which is a
method with momentum acceleration whose convergence rate is proven to be
independent of data heterogeneity. More specifically, we analyze the
convergence rate of Momentum Tracking in the standard deep learning setting,
where the objective function is non-convex and the stochastic gradient is used.
Then, we identify that it is independent of data heterogeneity for any momentum
coefficient $\beta\in [0, 1)$. Through image classification tasks, we
demonstrate that Momentum Tracking is more robust to data heterogeneity than
the existing decentralized learning methods with momentum acceleration and can
consistently outperform these existing methods when the data distributions are
heterogeneous.
</p></li>
</ul>

<h2>biometric</h2>
<h3>Title: Two-headed eye-segmentation approach for biometric identification. (arXiv:2209.15471v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15471">http://arxiv.org/abs/2209.15471</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15471] Two-headed eye-segmentation approach for biometric identification](http://arxiv.org/abs/2209.15471)</code></li>
<li>Summary: <p>Iris-based identification systems are among the most popular approaches for
person identification. Such systems require good-quality segmentation modules
that ideally identify the regions for different eye components. This paper
introduces the new two-headed architecture, where the eye components and
eyelashes are segmented using two separate decoding modules. Moreover, we
investigate various training scenarios by adopting different training losses.
Thanks to the two-headed approach, we were also able to examine the quality of
the model with the convex prior, which enforces the convexity of the segmented
shapes. We conducted an extensive evaluation of various learning scenarios on
real-life conditions high-resolution near-infrared iris images.
</p></li>
</ul>

<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Guided Unsupervised Learning by Subaperture Decomposition for Ocean SAR Image Retrieval. (arXiv:2209.15034v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15034">http://arxiv.org/abs/2209.15034</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15034] Guided Unsupervised Learning by Subaperture Decomposition for Ocean SAR Image Retrieval](http://arxiv.org/abs/2209.15034)</code></li>
<li>Summary: <p>Spaceborne synthetic aperture radar (SAR) can provide accurate images of the
ocean surface roughness day-or-night in nearly all weather conditions, being an
unique asset for many geophysical applications. Considering the huge amount of
data daily acquired by satellites, automated techniques for physical features
extraction are needed. Even if supervised deep learning methods attain
state-of-the-art results, they require great amount of labeled data, which are
difficult and excessively expensive to acquire for ocean SAR imagery. To this
end, we use the subaperture decomposition (SD) algorithm to enhance the
unsupervised learning retrieval on the ocean surface, empowering ocean
researchers to search into large ocean databases. We empirically prove that SD
improve the retrieval precision with over 20% for an unsupervised transformer
auto-encoder network. Moreover, we show that SD brings important performance
boost when Doppler centroid images are used as input data, leading the way to
new unsupervised physics guided retrieval algorithms.
</p></li>
</ul>

<h3>Title: Towards End-to-end Handwritten Document Recognition. (arXiv:2209.15362v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15362">http://arxiv.org/abs/2209.15362</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15362] Towards End-to-end Handwritten Document Recognition](http://arxiv.org/abs/2209.15362)</code></li>
<li>Summary: <p>Handwritten text recognition has been widely studied in the last decades for
its numerous applications. Nowadays, the state-of-the-art approach consists in
a three-step process. The document is segmented into text lines, which are then
ordered and recognized. However, this three-step approach has many drawbacks.
The three steps are treated independently whereas they are closely related.
Errors accumulate from one step to the other. The ordering step is based on
heuristic rules which prevent its use for documents with a complex layouts or
for heterogeneous documents. The need for additional physical segmentation
annotations for training the segmentation stage is inherent to this approach.
In this thesis, we propose to tackle these issues by performing the handwritten
text recognition of whole document in an end-to-end way. To this aim, we
gradually increase the difficulty of the recognition task, moving from isolated
lines to paragraphs, and then to whole documents. We proposed an approach at
the line level, based on a fully convolutional network, in order to design a
first generic feature extraction step for the handwriting recognition task.
Based on this preliminary work, we studied two different approaches to
recognize handwritten paragraphs. We reached state-of-the-art results at
paragraph level on the RIMES 2011, IAM and READ 2016 datasets and outperformed
the line-level state of the art on these datasets. We finally proposed the
first end-to-end approach dedicated to the recognition of both text and layout,
at document level. Characters and layout tokens are sequentially predicted
following a learned reading order. We proposed two new metrics we used to
evaluate this task on the RIMES 2009 and READ 2016 dataset, at page level and
double-page level.
</p></li>
</ul>

<h3>Title: Improving 3D-aware Image Synthesis with A Geometry-aware Discriminator. (arXiv:2209.15637v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15637">http://arxiv.org/abs/2209.15637</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15637] Improving 3D-aware Image Synthesis with A Geometry-aware Discriminator](http://arxiv.org/abs/2209.15637)</code></li>
<li>Summary: <p>3D-aware image synthesis aims at learning a generative model that can render
photo-realistic 2D images while capturing decent underlying 3D shapes. A
popular solution is to adopt the generative adversarial network (GAN) and
replace the generator with a 3D renderer, where volume rendering with neural
radiance field (NeRF) is commonly used. Despite the advancement of synthesis
quality, existing methods fail to obtain moderate 3D shapes. We argue that,
considering the two-player game in the formulation of GANs, only making the
generator 3D-aware is not enough. In other words, displacing the generative
mechanism only offers the capability, but not the guarantee, of producing
3D-aware images, because the supervision of the generator primarily comes from
the discriminator. To address this issue, we propose GeoD through learning a
geometry-aware discriminator to improve 3D-aware GANs. Concretely, besides
differentiating real and fake samples from the 2D image space, the
discriminator is additionally asked to derive the geometry information from the
inputs, which is then applied as the guidance of the generator. Such a simple
yet effective design facilitates learning substantially more accurate 3D
shapes. Extensive experiments on various generator architectures and training
datasets verify the superiority of GeoD over state-of-the-art alternatives.
Moreover, our approach is registered as a general framework such that a more
capable discriminator (i.e., with a third task of novel view synthesis beyond
domain classification and geometry extraction) can further assist the generator
with a better multi-view consistency.
</p></li>
</ul>

<h3>Title: RL-MD: A Novel Reinforcement Learning Approach for DNA Motif Discovery. (arXiv:2209.15181v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15181">http://arxiv.org/abs/2209.15181</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15181] RL-MD: A Novel Reinforcement Learning Approach for DNA Motif Discovery](http://arxiv.org/abs/2209.15181)</code></li>
<li>Summary: <p>The extraction of sequence patterns from a collection of functionally linked
unlabeled DNA sequences is known as DNA motif discovery, and it is a key task
in computational biology. Several deep learning-based techniques have recently
been introduced to address this issue. However, these algorithms can not be
used in real-world situations because of the need for labeled data. Here, we
presented RL-MD, a novel reinforcement learning based approach for DNA motif
discovery task. RL-MD takes unlabelled data as input, employs a relative
information-based method to evaluate each proposed motif, and utilizes these
continuous evaluation results as the reward. The experiments show that RL-MD
can identify high-quality motifs in real-world data.
</p></li>
</ul>

<h3>Title: Designing and Training of Lightweight Neural Networks on Edge Devices using Early Halting in Knowledge Distillation. (arXiv:2209.15560v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15560">http://arxiv.org/abs/2209.15560</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15560] Designing and Training of Lightweight Neural Networks on Edge Devices using Early Halting in Knowledge Distillation](http://arxiv.org/abs/2209.15560)</code></li>
<li>Summary: <p>Automated feature extraction capability and significant performance of Deep
Neural Networks (DNN) make them suitable for Internet of Things (IoT)
applications. However, deploying DNN on edge devices becomes prohibitive due to
the colossal computation, energy, and storage requirements. This paper presents
a novel approach for designing and training lightweight DNN using large-size
DNN. The approach considers the available storage, processing speed, and
maximum allowable processing time to execute the task on edge devices. We
present a knowledge distillation based training procedure to train the
lightweight DNN to achieve adequate accuracy. During the training of
lightweight DNN, we introduce a novel early halting technique, which preserves
network resources; thus, speedups the training procedure. Finally, we present
the empirically and real-world evaluations to verify the effectiveness of the
proposed approach under different constraints using various edge devices.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Rethinking Data Heterogeneity in Federated Learning: Introducing a New Notion and Standard Benchmarks. (arXiv:2209.15595v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15595">http://arxiv.org/abs/2209.15595</a></li>
<li>Code URL: <a href="https://github.com/mmorafah/fl-sc-niid">https://github.com/mmorafah/fl-sc-niid</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15595] Rethinking Data Heterogeneity in Federated Learning: Introducing a New Notion and Standard Benchmarks](http://arxiv.org/abs/2209.15595)</code></li>
<li>Summary: <p>Though successful, federated learning presents new challenges for machine
learning, especially when the issue of data heterogeneity, also known as
Non-IID data, arises. To cope with the statistical heterogeneity, previous
works incorporated a proximal term in local optimization or modified the model
aggregation scheme at the server side or advocated clustered federated learning
approaches where the central server groups agent population into clusters with
jointly trainable data distributions to take the advantage of a certain level
of personalization. While effective, they lack a deep elaboration on what kind
of data heterogeneity and how the data heterogeneity impacts the accuracy
performance of the participating clients. In contrast to many of the prior
federated learning approaches, we demonstrate not only the issue of data
heterogeneity in current setups is not necessarily a problem but also in fact
it can be beneficial for the FL participants. Our observations are intuitive:
(1) Dissimilar labels of clients (label skew) are not necessarily considered
data heterogeneity, and (2) the principal angle between the agents' data
subspaces spanned by their corresponding principal vectors of data is a better
estimate of the data heterogeneity. Our code is available at
https://github.com/MMorafah/FL-SC-NIID.
</p></li>
</ul>

<h3>Title: Fed-CBS: A Heterogeneity-Aware Client Sampling Mechanism for Federated Learning via Class-Imbalance Reduction. (arXiv:2209.15245v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15245">http://arxiv.org/abs/2209.15245</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15245] Fed-CBS: A Heterogeneity-Aware Client Sampling Mechanism for Federated Learning via Class-Imbalance Reduction](http://arxiv.org/abs/2209.15245)</code></li>
<li>Summary: <p>Due to limited communication capacities of edge devices, most existing
federated learning (FL) methods randomly select only a subset of devices to
participate in training for each communication round. Compared with engaging
all the available clients, the random-selection mechanism can lead to
significant performance degradation on non-IID (independent and identically
distributed) data. In this paper, we show our key observation that the
essential reason resulting in such performance degradation is the
class-imbalance of the grouped data from randomly selected clients. Based on
our key observation, we design an efficient heterogeneity-aware client sampling
mechanism, i.e., Federated Class-balanced Sampling (Fed-CBS), which can
effectively reduce class-imbalance of the group dataset from the intentionally
selected clients. In particular, we propose a measure of class-imbalance and
then employ homomorphic encryption to derive this measure in a
privacy-preserving way. Based on this measure, we also design a
computation-efficient client sampling strategy, such that the actively selected
clients will generate a more class-balanced grouped dataset with theoretical
guarantees. Extensive experimental results demonstrate Fed-CBS outperforms the
status quo approaches. Furthermore, it achieves comparable or even better
performance than the ideal setting where all the available clients participate
in the FL training.
</p></li>
</ul>

<h3>Title: Sparse Random Networks for Communication-Efficient Federated Learning. (arXiv:2209.15328v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15328">http://arxiv.org/abs/2209.15328</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15328] Sparse Random Networks for Communication-Efficient Federated Learning](http://arxiv.org/abs/2209.15328)</code></li>
<li>Summary: <p>One main challenge in federated learning is the large communication cost of
exchanging weight updates from clients to the server at each round. While prior
work has made great progress in compressing the weight updates through gradient
compression methods, we propose a radically different approach that does not
update the weights at all. Instead, our method freezes the weights at their
initial \emph{random} values and learns how to sparsify the random network for
the best performance. To this end, the clients collaborate in training a
\emph{stochastic} binary mask to find the optimal sparse random network within
the original one. At the end of the training, the final model is a sparse
network with random weights -- or a subnetwork inside the dense random network.
We show improvements in accuracy, communication (less than $1$ bit per
parameter (bpp)), convergence speed, and final model size (less than $1$ bpp)
over relevant baselines on MNIST, EMNIST, CIFAR-10, and CIFAR-100 datasets, in
the low bitrate regime under various system configurations.
</p></li>
</ul>

<h3>Title: Vertical Semi-Federated Learning for Efficient Online Advertising. (arXiv:2209.15635v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15635">http://arxiv.org/abs/2209.15635</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15635] Vertical Semi-Federated Learning for Efficient Online Advertising](http://arxiv.org/abs/2209.15635)</code></li>
<li>Summary: <p>As an emerging secure learning paradigm in leveraging cross-silo private
data, vertical federated learning (VFL) is expected to improve advertising
models by enabling the joint learning of complementary user attributes
privately owned by the advertiser and the publisher. However, the 1) restricted
applicable scope to overlapped samples and 2) high system challenge of
real-time federated serving have limited its application to advertising
systems.
</p></li>
</ul>

<p>In this paper, we advocate new learning setting Semi-VFL (Vertical
Semi-Federated Learning) as a lightweight solution to utilize all available
data (both the overlapped and non-overlapped data) that is free from federated
serving. Semi-VFL is expected to perform better than single-party models and
maintain a low inference cost. It's notably important to i) alleviate the
absence of the passive party's feature and ii) adapt to the whole sample space
to implement a good solution for Semi-VFL. Thus, we propose a carefully
designed joint privileged learning framework (JPL) as an efficient
implementation of Semi-VFL. Specifically, we build an inference-efficient
single-party student model applicable to the whole sample space and meanwhile
maintain the advantage of the federated feature extension. Novel feature
imitation and ranking consistency restriction methods are proposed to extract
cross-party feature correlations and maintain cross-sample-space consistency
for both the overlapped and non-overlapped data.
</p>
<p>We conducted extensive experiments on real-world advertising datasets. The
results show that our method achieves the best performance over baseline
methods and validate its effectiveness in maintaining cross-view feature
correlation.
</p>

<h2>fair</h2>
<h3>Title: Rethinking and Recomputing the Value of ML Models. (arXiv:2209.15157v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15157">http://arxiv.org/abs/2209.15157</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15157] Rethinking and Recomputing the Value of ML Models](http://arxiv.org/abs/2209.15157)</code></li>
<li>Summary: <p>In this paper, we argue that the way we have been training and evaluating ML
models has largely forgotten the fact that they are applied in an organization
or societal context as they provide value to people. We show that with this
perspective we fundamentally change how we evaluate, select and deploy ML
models - and to some extent even what it means to learn. Specifically, we
stress that the notion of value plays a central role in learning and
evaluating, and different models may require different learning practices and
provide different values based on the application context they are applied. We
also show that this concretely impacts how we select and embed models into
human workflows based on experimental datasets. Nothing of what is presented
here is hard: to a large extent is a series of fairly trivial observations with
massive practical implications.
</p></li>
</ul>

<h3>Title: MEIM: Multi-partition Embedding Interaction Beyond Block Term Format for Efficient and Expressive Link Prediction. (arXiv:2209.15597v1 [cs.AI])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15597">http://arxiv.org/abs/2209.15597</a></li>
<li>Code URL: <a href="https://github.com/tranhungnghiep/meim-kge">https://github.com/tranhungnghiep/meim-kge</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15597] MEIM: Multi-partition Embedding Interaction Beyond Block Term Format for Efficient and Expressive Link Prediction](http://arxiv.org/abs/2209.15597)</code></li>
<li>Summary: <p>Knowledge graph embedding aims to predict the missing relations between
entities in knowledge graphs. Tensor-decomposition-based models, such as
ComplEx, provide a good trade-off between efficiency and expressiveness, that
is crucial because of the large size of real world knowledge graphs. The recent
multi-partition embedding interaction (MEI) model subsumes these models by
using the block term tensor format and provides a systematic solution for the
trade-off. However, MEI has several drawbacks, some of which carried from its
subsumed tensor-decomposition-based models. In this paper, we address these
drawbacks and introduce the Multi-partition Embedding Interaction iMproved
beyond block term format (MEIM) model, with independent core tensor for
ensemble effects and soft orthogonality for max-rank mapping, in addition to
multi-partition embedding. MEIM improves expressiveness while still being
highly efficient, helping it to outperform strong baselines and achieve
state-of-the-art results on difficult link prediction benchmarks using fairly
small embedding sizes. The source code is released at
https://github.com/tranhungnghiep/MEIM-KGE.
</p></li>
</ul>

<h3>Title: Variable-Based Calibration for Machine Learning Classifiers. (arXiv:2209.15154v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15154">http://arxiv.org/abs/2209.15154</a></li>
<li>Code URL: <a href="https://github.com/markellekelly/variable-wise-calibration">https://github.com/markellekelly/variable-wise-calibration</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15154] Variable-Based Calibration for Machine Learning Classifiers](http://arxiv.org/abs/2209.15154)</code></li>
<li>Summary: <p>The deployment of machine learning classifiers in high-stakes domains
requires well-calibrated confidence scores for model predictions. In this paper
we introduce the notion of variable-based calibration to characterize
calibration properties of a model with respect to a variable of interest,
generalizing traditional score-based calibration and metrics such as expected
calibration error (ECE). In particular, we find that models with near-perfect
ECE can exhibit significant variable-based calibration error as a function of
features of the data. We demonstrate this phenomenon both theoretically and in
practice on multiple well-known datasets, and show that it can persist after
the application of existing recalibration methods. To mitigate this issue, we
propose strategies for detection, visualization, and quantification of
variable-based calibration error. We then examine the limitations of current
score-based recalibration methods and explore potential modifications. Finally,
we discuss the implications of these findings, emphasizing that an
understanding of calibration beyond simple aggregate measures is crucial for
endeavors such as fairness and model interpretability.
</p></li>
</ul>

<h3>Title: Higher-order Neural Additive Models: An Interpretable Machine Learning Model with Feature Interactions. (arXiv:2209.15409v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15409">http://arxiv.org/abs/2209.15409</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15409] Higher-order Neural Additive Models: An Interpretable Machine Learning Model with Feature Interactions](http://arxiv.org/abs/2209.15409)</code></li>
<li>Summary: <p>Black-box models, such as deep neural networks, exhibit superior predictive
performances, but understanding their behavior is notoriously difficult. Many
explainable artificial intelligence methods have been proposed to reveal the
decision-making processes of black box models. However, their applications in
high-stakes domains remain limited. Recently proposed neural additive models
(NAM) have achieved state-of-the-art interpretable machine learning. NAM can
provide straightforward interpretations with slight performance sacrifices
compared with multi-layer perceptron. However, NAM can only model
1$^{\text{st}}$-order feature interactions; thus, it cannot capture the
co-relationships between input features. To overcome this problem, we propose a
novel interpretable machine learning method called higher-order neural additive
models (HONAM) and a feature interaction method for high interpretability.
HONAM can model arbitrary orders of feature interactions. Therefore, it can
provide the high predictive performance and interpretability that high-stakes
domains need. In addition, we propose a novel hidden unit to effectively learn
sharp-shape functions. We conducted experiments using various real-world
datasets to examine the effectiveness of HONAM. Furthermore, we demonstrate
that HONAM can achieve fair AI with a slight performance sacrifice. The source
code for HONAM is publicly available.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Evaluation of importance estimators in deep learning classifiers for Computed Tomography. (arXiv:2209.15398v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15398">http://arxiv.org/abs/2209.15398</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15398] Evaluation of importance estimators in deep learning classifiers for Computed Tomography](http://arxiv.org/abs/2209.15398)</code></li>
<li>Summary: <p>Deep learning has shown superb performance in detecting objects and
classifying images, ensuring a great promise for analyzing medical imaging.
Translating the success of deep learning to medical imaging, in which doctors
need to understand the underlying process, requires the capability to interpret
and explain the prediction of neural networks. Interpretability of deep neural
networks often relies on estimating the importance of input features (e.g.,
pixels) with respect to the outcome (e.g., class probability). However, a
number of importance estimators (also known as saliency maps) have been
developed and it is unclear which ones are more relevant for medical imaging
applications. In the present work, we investigated the performance of several
importance estimators in explaining the classification of computed tomography
(CT) images by a convolutional deep network, using three distinct evaluation
metrics. First, the model-centric fidelity measures a decrease in the model
accuracy when certain inputs are perturbed. Second, concordance between
importance scores and the expert-defined segmentation masks is measured on a
pixel level by a receiver operating characteristic (ROC) curves. Third, we
measure a region-wise overlap between a XRAI-based map and the segmentation
mask by Dice Similarity Coefficients (DSC). Overall, two versions of SmoothGrad
topped the fidelity and ROC rankings, whereas both Integrated Gradients and
SmoothGrad excelled in DSC evaluation. Interestingly, there was a critical
discrepancy between model-centric (fidelity) and human-centric (ROC and DSC)
evaluation. Expert expectation and intuition embedded in segmentation maps does
not necessarily align with how the model arrived at its prediction.
Understanding this difference in interpretability would help harnessing the
power of deep learning in medicine.
</p></li>
</ul>

<h3>Title: Neural Integral Equations. (arXiv:2209.15190v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15190">http://arxiv.org/abs/2209.15190</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15190] Neural Integral Equations](http://arxiv.org/abs/2209.15190)</code></li>
<li>Summary: <p>Integral equations (IEs) are functional equations defined through integral
operators, where the unknown function is integrated over a possibly
multidimensional space. Important applications of IEs have been found
throughout theoretical and applied sciences, including in physics, chemistry,
biology, and engineering; often in the form of inverse problems. IEs are
especially useful since differential equations, e.g. ordinary differential
equations (ODEs), and partial differential equations (PDEs) can be formulated
in an integral version which is often more convenient to solve. Moreover,
unlike ODEs and PDEs, IEs can model inherently non-local dynamical systems,
such as ones with long distance spatiotemporal relations. While efficient
algorithms exist for solving given IEs, no method exists that can learn an
integral equation and its associated dynamics from data alone. In this article,
we introduce Neural Integral Equations (NIE), a method that learns an unknown
integral operator from data through a solver. We also introduce an attentional
version of NIE, called Attentional Neural Integral Equations (ANIE), where the
integral is replaced by self-attention, which improves scalability and provides
interpretability. We show that learning dynamics via integral equations is
faster than doing so via other continuous methods, such as Neural ODEs.
Finally, we show that ANIE outperforms other methods on several benchmark tasks
in ODE, PDE, and IE systems of synthetic and real-world data.
</p></li>
</ul>

<h3>Title: Explainable Censored Learning: Finding Critical Features with Long Term Prognostic Values for Survival Prediction. (arXiv:2209.15450v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15450">http://arxiv.org/abs/2209.15450</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15450] Explainable Censored Learning: Finding Critical Features with Long Term Prognostic Values for Survival Prediction](http://arxiv.org/abs/2209.15450)</code></li>
<li>Summary: <p>Interpreting critical variables involved in complex biological processes
related to survival time can help understand prediction from survival models,
evaluate treatment efficacy, and develop new therapies for patients. Currently,
the predictive results of deep learning (DL)-based models are better than or as
good as standard survival methods, they are often disregarded because of their
lack of transparency and little interpretability, which is crucial to their
adoption in clinical applications. In this paper, we introduce a novel, easily
deployable approach, called EXplainable CEnsored Learning (EXCEL), to
iteratively exploit critical variables and simultaneously implement (DL) model
training based on these variables. First, on a toy dataset, we illustrate the
principle of EXCEL; then, we mathematically analyze our proposed method, and we
derive and prove tight generalization error bounds; next, on two semi-synthetic
datasets, we show that EXCEL has good anti-noise ability and stability;
finally, we apply EXCEL to a variety of real-world survival datasets including
clinical data and genetic data, demonstrating that EXCEL can effectively
identify critical features and achieve performance on par with or better than
the original models. It is worth pointing out that EXCEL is flexibly deployed
in existing or emerging models for explainable survival data in the presence of
right censoring.
</p></li>
</ul>

<h2>exlainability</h2>
<h2>watermark</h2>
<h3>Title: Generative Model Watermarking Based on Human Visual System. (arXiv:2209.15268v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.15268">http://arxiv.org/abs/2209.15268</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.15268] Generative Model Watermarking Based on Human Visual System](http://arxiv.org/abs/2209.15268)</code></li>
<li>Summary: <p>Intellectual property protection of deep neural networks is receiving
attention from more and more researchers, and the latest research applies model
watermarking to generative models for image processing. However, the existing
watermarking methods designed for generative models do not take into account
the effects of different channels of sample images on watermarking. As a
result, the watermarking performance is still limited. To tackle this problem,
in this paper, we first analyze the effects of embedding watermark information
on different channels. Then, based on the characteristics of human visual
system (HVS), we introduce two HVS-based generative model watermarking methods,
which are realized in RGB color space and YUV color space respectively. In RGB
color space, the watermark is embedded into the R and B channels based on the
fact that HVS is more sensitive to G channel. In YUV color space, the watermark
is embedded into the DCT domain of U and V channels based on the fact that HVS
is more sensitive to brightness changes. Experimental results demonstrate the
effectiveness of the proposed work, which improves the fidelity of the model to
be protected and has good universality compared with previous methods.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
