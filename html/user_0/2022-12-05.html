<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Inheritance and Blockchain: Thoughts and Open Questions. (arXiv:2212.01194v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.01194">http://arxiv.org/abs/2212.01194</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.01194] Inheritance and Blockchain: Thoughts and Open Questions](http://arxiv.org/abs/2212.01194) #secure</code></li>
<li>Summary: <p>Inheritance is the fundamental building block of civilization. This is the
addition of wealth, knowledge and properties over time that produce the society
in which we are living. Every generation does not have to start from zero and
can capitalize on the efforts of previous generations. Blockchain based assets
are very efficiently and securely transferred between living entities. Yet the
actual way to make heirs inherit crypto-assets is seldom discussed. It appears
that the problems linked with the inheritance of crypto-assets raise a lot of
technical, societal and legal issues. Part of those issues have to be tackled
with at the level of the blockchain infrastructure itself. The aim of this
paper is to open a research field, and to discuss some ideas, with regards to
this overlooked issue. Inheritance is neither a peripheral question nor one
that can be dodged. It comes with its own set of challenges that have to be met
if blockchain based finance, and asset management, is to be taken seriously.
</p></li>
</ul>

<h3>Title: OOG- Optuna Optimized GAN Sampling Technique for Tabular Imbalanced Malware Data. (arXiv:2212.01274v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.01274">http://arxiv.org/abs/2212.01274</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.01274] OOG- Optuna Optimized GAN Sampling Technique for Tabular Imbalanced Malware Data](http://arxiv.org/abs/2212.01274) #secure</code></li>
<li>Summary: <p>Cyberspace occupies a large portion of people's life in the age of modern
technology, and while there are those who utilize it for good, there are also
those who do not. Malware is an application whose construction was not
motivated by a benign goal and it can harm, steal, or even alter personal
information and secure applications and software. Thus, there are numerous
techniques to avoid malware, one of which is to develop samples of malware so
that the system can be updated with the growing number of malwares, allowing it
to recognize when malwares attempt to enter. The Generative Adversarial Network
(GAN) sampling technique has been used in this study to generate new malware
samples. GANs have multiple variants, and in order to determine which variant
is optimal for a given dataset sample, their parameters must be modified. This
study employs Optuna, an autonomous hyperparameter tuning algorithm, to
determine the optimal settings for the dataset under consideration. In this
study, the architecture of the Optuna Optimized GAN (OOG) method is shown,
along with scores of 98.06%, 99.00%, 97.23%, and 98.04% for accuracy,
precision, recall and f1 score respectively. After tweaking the hyperparameters
of five supervised boosting algorithms, XGBoost, LightGBM, CatBoost, Extra
Trees Classifier, and Gradient Boosting Classifier, the methodology of this
paper additionally employs the weighted ensemble technique to acquire this
result. In addition to comparing existing efforts in this domain, the study
demonstrates how promising GAN is in comparison to other sampling techniques
such as SMOTE.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: A Hybrid Deep Learning Anomaly Detection Framework for Intrusion Detection. (arXiv:2212.00966v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.00966">http://arxiv.org/abs/2212.00966</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.00966] A Hybrid Deep Learning Anomaly Detection Framework for Intrusion Detection](http://arxiv.org/abs/2212.00966) #security</code></li>
<li>Summary: <p>Cyber intrusion attacks that compromise the users' critical and sensitive
data are escalating in volume and intensity, especially with the growing
connections between our daily life and the Internet. The large volume and high
complexity of such intrusion attacks have impeded the effectiveness of most
traditional defence techniques. While at the same time, the remarkable
performance of the machine learning methods, especially deep learning, in
computer vision, had garnered research interests from the cyber security
community to further enhance and automate intrusion detections. However, the
expensive data labeling and limitation of anomalous data make it challenging to
train an intrusion detector in a fully supervised manner. Therefore, intrusion
detection based on unsupervised anomaly detection is an important feature too.
In this paper, we propose a three-stage deep learning anomaly detection based
network intrusion attack detection framework. The framework comprises an
integration of unsupervised (K-means clustering), semi-supervised (GANomaly)
and supervised learning (CNN) algorithms. We then evaluated and showed the
performance of our implemented framework on three benchmark datasets: NSL-KDD,
CIC-IDS2018, and TON_IoT.
</p></li>
</ul>

<h3>Title: Deep-Learning-based Vulnerability Detection in Binary Executables. (arXiv:2212.01254v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.01254">http://arxiv.org/abs/2212.01254</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.01254] Deep-Learning-based Vulnerability Detection in Binary Executables](http://arxiv.org/abs/2212.01254) #security</code></li>
<li>Summary: <p>The identification of vulnerabilities is an important element in the software
development life cycle to ensure the security of software. While vulnerability
identification based on the source code is a well studied field, the
identification of vulnerabilities on basis of a binary executable without the
corresponding source code is more challenging. Recent research [1] has shown,
how such detection can be achieved by deep learning methods. However, that
particular approach is limited to the identification of only 4 types of
vulnerabilities. Subsequently, we analyze to what extent we could cover the
identification of a larger variety of vulnerabilities. Therefore, a supervised
deep learning approach using recurrent neural networks for the application of
vulnerability detection based on binary executables is used. The underlying
basis is a dataset with 50,651 samples of vulnerable code in the form of a
standardized LLVM Intermediate Representation. The vectorised features of a
Word2Vec model are used to train different variations of three basic
architectures of recurrent neural networks (GRU, LSTM, SRNN). A binary
classification was established for detecting the presence of an arbitrary
vulnerability, and a multi-class model was trained for the identification of
the exact vulnerability, which achieved an out-of-sample accuracy of 88% and
77%, respectively. Differences in the detection of different vulnerabilities
were also observed, with non-vulnerable samples being detected with a
particularly high precision of over 98%. Thus, the methodology presented allows
an accurate detection of 23 (compared to 4 [1]) vulnerabilities.
</p></li>
</ul>

<h3>Title: 5G-NIDD: A Comprehensive Network Intrusion Detection Dataset Generated over 5G Wireless Network. (arXiv:2212.01298v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.01298">http://arxiv.org/abs/2212.01298</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.01298] 5G-NIDD: A Comprehensive Network Intrusion Detection Dataset Generated over 5G Wireless Network](http://arxiv.org/abs/2212.01298) #security</code></li>
<li>Summary: <p>With a plethora of new connections, features, and services introduced, the
5th generation (5G) wireless technology reflects the development of mobile
communication networks and is here to stay for the next decade. The multitude
of services and technologies that 5G incorporates have made modern
communication networks very complex and sophisticated in nature. This
complexity along with the incorporation of Machine Learning (ML) and Artificial
Intelligence (AI) provides the opportunity for the attackers to launch
intelligent attacks against the network and network devices. These attacks
often traverse undetected due to the lack of intelligent security mechanisms to
counter these threats. Therefore, the implementation of real-time, proactive,
and self-adaptive security mechanisms throughout the network would be an
integral part of 5G as well as future communication systems. Therefore, large
amounts of data collected from real networks will play an important role in the
training of AI/ML models to identify and detect malicious content in network
traffic. This work presents 5G-NIDD, a fully labeled dataset built on a
functional 5G test network that can be used by those who develop and test AI/ML
solutions. The work further analyses the collected data using common ML models
and shows the achieved accuracy levels.
</p></li>
</ul>

<h3>Title: Bitcoin Security-Latency Under Network Delay. (arXiv:2212.01372v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.01372">http://arxiv.org/abs/2212.01372</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.01372] Bitcoin Security-Latency Under Network Delay](http://arxiv.org/abs/2212.01372) #security</code></li>
<li>Summary: <p>We improve security-latency bounds of Nakamoto consensus by analyzing the
race between adversarial and honest chains in three different phases:
pre-mining, confirmation and post-confirmation. We find the probability
distribution of the length of the adversarial chain and the rigged adversarial
chain under jumper models during the confirmation interval. We analyze certain
properties of this race to model pre-mining and post-confirmation phases with
random walks that provide tighter bounds than existing results. Combining all
three phases provides novel upper and lower bounds for blockchains with small
$\lambda\Delta$.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Private Multiparty Perception for Navigation. (arXiv:2212.00912v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.00912">http://arxiv.org/abs/2212.00912</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.00912] Private Multiparty Perception for Navigation](http://arxiv.org/abs/2212.00912) #privacy</code></li>
<li>Summary: <p>We introduce a framework for navigating through cluttered environments by
connecting multiple cameras together while simultaneously preserving privacy.
Occlusions and obstacles in large environments are often challenging situations
for navigation agents because the environment is not fully observable from a
single camera view. Given multiple camera views of an environment, our approach
learns to produce a multiview scene representation that can only be used for
navigation, provably preventing one party from inferring anything beyond the
output task. On a new navigation dataset that we will publicly release,
experiments show that private multiparty representations allow navigation
through complex scenes and around obstacles while jointly preserving privacy.
Our approach scales to an arbitrary number of camera viewpoints. We believe
developing visual representations that preserve privacy is increasingly
important for many applications such as navigation.
</p></li>
</ul>

<h3>Title: Integer Subspace Differential Privacy. (arXiv:2212.00936v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.00936">http://arxiv.org/abs/2212.00936</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.00936] Integer Subspace Differential Privacy](http://arxiv.org/abs/2212.00936) #privacy</code></li>
<li>Summary: <p>We propose new differential privacy solutions for when external
\emph{invariants} and \emph{integer} constraints are simultaneously enforced on
the data product. These requirements arise in real world applications of
private data curation, including the public release of the 2020 U.S. Decennial
Census. They pose a great challenge to the production of provably private data
products with adequate statistical usability. We propose \emph{integer subspace
differential privacy} to rigorously articulate the privacy guarantee when data
products maintain both the invariants and integer characteristics, and
demonstrate the composition and post-processing properties of our proposal. To
address the challenge of sampling from a potentially highly restricted discrete
space, we devise a pair of unbiased additive mechanisms, the generalized
Laplace and the generalized Gaussian mechanisms, by solving the Diophantine
equations as defined by the constraints. The proposed mechanisms have good
accuracy, with errors exhibiting sub-exponential and sub-Gaussian tail
probabilities respectively. To implement our proposal, we design an MCMC
algorithm and supply empirical convergence assessment using estimated upper
bounds on the total variation distance via $L$-lag coupling. We demonstrate the
efficacy of our proposal with applications to a synthetic problem with
intersecting invariants, a sensitive contingency table with known margins, and
the 2010 Census county-level demonstration data with mandated fixed state
population totals.
</p></li>
</ul>

<h3>Title: Assessing Anonymized System Logs Usefulness for Behavioral Analysis in RNN Models. (arXiv:2212.01101v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.01101">http://arxiv.org/abs/2212.01101</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.01101] Assessing Anonymized System Logs Usefulness for Behavioral Analysis in RNN Models](http://arxiv.org/abs/2212.01101) #privacy</code></li>
<li>Summary: <p>System logs are a common source of monitoring data for analyzing computing
systems' behavior. Due to the complexity of modern computing systems and the
large size of collected monitoring data, automated analysis mechanisms are
required. Numerous machine learning and deep learning methods are proposed to
address this challenge. However, due to the existence of sensitive data in
system logs their analysis and storage raise serious privacy concerns.
Anonymization methods could be used to clean the monitoring data before
analysis. However, anonymized system logs, in general, do not provide adequate
usefulness for the majority of behavioral analysis. Content-aware anonymization
mechanisms such as PaRS preserve the correlation of system logs even after
anonymization. This work evaluates the usefulness of anonymized system logs
taken from the Taurus HPC cluster anonymized using PaRS, for behavioral
analysis via recurrent neural network models.
</p></li>
</ul>

<h3>Title: On-device Training: A First Overview on Existing Systems. (arXiv:2212.00824v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.00824">http://arxiv.org/abs/2212.00824</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.00824] On-device Training: A First Overview on Existing Systems](http://arxiv.org/abs/2212.00824) #privacy</code></li>
<li>Summary: <p>The recent breakthroughs in machine learning (ML) and deep learning (DL) have
enabled many new capabilities across plenty of application domains. While most
existing machine learning models require large memory and computing power,
efforts have been made to deploy some models on resource-constrained devices as
well. There are several systems that perform inference on the device, while
direct training on the device still remains a challenge. On-device training,
however, is attracting more and more interest because: (1) it enables training
models on local data without needing to share data over the cloud, thus
enabling privacy preserving computation by design; (2) models can be refined on
devices to provide personalized services and cope with model drift in order to
adapt to the changes of the real-world environment; and (3) it enables the
deployment of models in remote, hardly accessible locations or places without
stable internet connectivity. We summarize and analyze the-state-of-art systems
research to provide the first survey of on-device training from a systems
perspective.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h2>attack</h2>
<h3>Title: Membership Inference Attacks Against Semantic Segmentation Models. (arXiv:2212.01082v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.01082">http://arxiv.org/abs/2212.01082</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.01082] Membership Inference Attacks Against Semantic Segmentation Models](http://arxiv.org/abs/2212.01082) #attack</code></li>
<li>Summary: <p>Membership inference attacks aim to infer whether a data record has been used
to train a target model by observing its predictions. In sensitive domains such
as healthcare, this can constitute a severe privacy violation. In this work we
attempt to address the existing knowledge gap by conducting an exhaustive study
of membership inference attacks and defences in the domain of semantic image
segmentation. Our findings indicate that for certain threat models, these
learning settings can be considerably more vulnerable than the previously
considered classification settings. We additionally investigate a threat model
where a dishonest adversary can perform model poisoning to aid their inference
and evaluate the effects that these adaptations have on the success of
membership inference attacks. We quantitatively evaluate the attacks on a
number of popular model architectures across a variety of semantic segmentation
tasks, demonstrating that membership inference attacks in this domain can
achieve a high success rate and defending against them may result in
unfavourable privacy-utility trade-offs or increased computational costs.
</p></li>
</ul>

<h3>Title: Safe machine learning model release from Trusted Research Environments: The AI-SDC package. (arXiv:2212.01233v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.01233">http://arxiv.org/abs/2212.01233</a></li>
<li>Code URL: <a href="https://github.com/ai-sdc/ai-sdc">https://github.com/ai-sdc/ai-sdc</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2212.01233] Safe machine learning model release from Trusted Research Environments: The AI-SDC package](http://arxiv.org/abs/2212.01233) #attack</code></li>
<li>Summary: <p>We present AI-SDC, an integrated suite of open source Python tools to
facilitate Statistical Disclosure Control (SDC) of Machine Learning (ML) models
trained on confidential data prior to public release. AI-SDC combines (i) a
SafeModel package that extends commonly used ML models to provide ante-hoc SDC
by assessing the vulnerability of disclosure posed by the training regime; and
(ii) an Attacks package that provides post-hoc SDC by rigorously assessing the
empirical disclosure risk of a model through a variety of simulated attacks
after training. The AI-SDC code and documentation are available under an MIT
license at https://github.com/AI-SDC/AI-SDC.
</p></li>
</ul>

<h3>Title: Fast Detection of Burst Jamming for Delay-Sensitive Internet-of-Things Applications. (arXiv:2212.01362v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.01362">http://arxiv.org/abs/2212.01362</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.01362] Fast Detection of Burst Jamming for Delay-Sensitive Internet-of-Things Applications](http://arxiv.org/abs/2212.01362) #attack</code></li>
<li>Summary: <p>In this paper, we investigate the design of a burst jamming detection method
for delay-sensitive Internet-of-Things (IoT) applications. In order to obtain a
timely detection of burst jamming, we propose an online principal direction
anomaly detection (OPDAD) method. We consider the one-ring scatter channel
model, where the base station equipped with a large number of antennas is
elevated at a high altitude. In this case, since the angular spread of the
legitimate IoT transmitter or the jammer is restricted within a narrow region,
there is a distinct difference of the principal direction of the signal space
between the jamming attack and the normal state. Unlike existing statistical
features based batching methods, the proposed OPDAD method adopts an online
iterative processing mode, which can quickly detect the exact attack time block
instance by analyzing the newly coming signal. In addition, our detection
method does not rely on the prior knowledge of the attacker, because it only
cares the abrupt change in the principal direction of the signal space.
Moreover, based on the high spatial resolution and the narrow angular spread,
we provide the convergence rate estimate and derive a nearly optimal finite
sample error bound for the proposed OPDAD method. Numerical results show the
excellent real time capability and detection performance of our proposed
method.
</p></li>
</ul>

<h3>Title: Pareto Regret Analyses in Multi-objective Multi-armed Bandit. (arXiv:2212.00884v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.00884">http://arxiv.org/abs/2212.00884</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.00884] Pareto Regret Analyses in Multi-objective Multi-armed Bandit](http://arxiv.org/abs/2212.00884) #attack</code></li>
<li>Summary: <p>We study Pareto optimality in multi-objective multi-armed bandit by providing
a formulation of adversarial multi-objective multi-armed bandit and properly
defining its Pareto regrets that can be generalized to stochastic settings as
well. The regrets do not rely on any scalarization functions and reflect Pareto
optimality compared to scalarized regrets. We also present new algorithms
assuming both with and without prior information of the multi-objective
multi-armed bandit setting. The algorithms are shown optimal in adversarial
settings and nearly optimal in stochastic settings simultaneously by our
established upper bounds and lower bounds on Pareto regrets. Moreover, the
lower bound analyses show that the new regrets are consistent with the existing
Pareto regret for stochastic settings and extend an adversarial attack
mechanism from bandit to the multi-objective one.
</p></li>
</ul>

<h3>Title: Initial Results for Pairwise Causal Discovery Using Quantitative Information Flow. (arXiv:2212.01279v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.01279">http://arxiv.org/abs/2212.01279</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.01279] Initial Results for Pairwise Causal Discovery Using Quantitative Information Flow](http://arxiv.org/abs/2212.01279) #attack</code></li>
<li>Summary: <p>Pairwise Causal Discovery is the task of determining causal, anticausal,
confounded or independence relationships from pairs of variables. Over the last
few years, this challenging task has promoted not only the discovery of novel
machine learning models aimed at solving the task, but also discussions on how
learning the causal direction of variables may benefit machine learning
overall. In this paper, we show that Quantitative Information Flow (QIF), a
measure usually employed for measuring leakages of information from a system to
an attacker, shows promising results as features for the task. In particular,
experiments with real-world datasets indicate that QIF is statistically tied to
the state of the art. Our initial results motivate further inquiries on how QIF
relates to causality and what are its limitations.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Single-shot ToF sensing with sub-mm precision using conventional CMOS sensors. (arXiv:2212.00928v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.00928">http://arxiv.org/abs/2212.00928</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.00928] Single-shot ToF sensing with sub-mm precision using conventional CMOS sensors](http://arxiv.org/abs/2212.00928) #robust</code></li>
<li>Summary: <p>We present a novel single-shot interferometric ToF camera targeted for
precise 3D measurements of dynamic objects. The camera concept is based on
Synthetic Wavelength Interferometry, a technique that allows retrieval of depth
maps of objects with optically rough surfaces at submillimeter depth precision.
In contrast to conventional ToF cameras, our device uses only off-the-shelf
CCD/CMOS detectors and works at their native chip resolution (as of today,
theoretically up to 20 Mp and beyond). Moreover, we can obtain a full 3D model
of the object in single-shot, meaning that no temporal sequence of exposures or
temporal illumination modulation (such as amplitude or frequency modulation) is
necessary, which makes our camera robust against object motion.
</p></li>
</ul>

<p>In this paper, we introduce the novel camera concept and show first
measurements that demonstrate the capabilities of our system. We present 3D
measurements of small (cm-sized) objects with > 2 Mp point cloud resolution
(the resolution of our used detector) and up to sub-mm depth precision. We also
report a "single-shot 3D video" acquisition and a first single-shot
"Non-Line-of-Sight" measurement. Our technique has great potential for
high-precision applications with dynamic object movement, e.g., in AR/VR,
industrial inspection, medical imaging, and imaging through scattering media
like fog or human tissue.
</p>

<h3>Title: Masked Contrastive Pre-Training for Efficient Video-Text Retrieval. (arXiv:2212.00986v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.00986">http://arxiv.org/abs/2212.00986</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.00986] Masked Contrastive Pre-Training for Efficient Video-Text Retrieval](http://arxiv.org/abs/2212.00986) #robust</code></li>
<li>Summary: <p>We present a simple yet effective end-to-end Video-language Pre-training
(VidLP) framework, Masked Contrastive Video-language Pretraining (MAC), for
video-text retrieval tasks. Our MAC aims to reduce video representation's
spatial and temporal redundancy in the VidLP model by a mask sampling mechanism
to improve pre-training efficiency. Comparing conventional temporal sparse
sampling, we propose to randomly mask a high ratio of spatial regions and only
feed visible regions into the encoder as sparse spatial sampling. Similarly, we
adopt the mask sampling technique for text inputs for consistency. Instead of
blindly applying the mask-then-prediction paradigm from MAE, we propose a
masked-then-alignment paradigm for efficient video-text alignment. The
motivation is that video-text retrieval tasks rely on high-level alignment
rather than low-level reconstruction, and multimodal alignment with masked
modeling encourages the model to learn a robust and general multimodal
representation from incomplete and unstable inputs. Coupling these designs
enables efficient end-to-end pre-training: reduce FLOPs (60% off), accelerate
pre-training (by 3x), and improve performance. Our MAC achieves
state-of-the-art results on various video-text retrieval datasets, including
MSR-VTT, DiDeMo, and ActivityNet. Our approach is omnivorous to input
modalities. With minimal modifications, we achieve competitive results on
image-text retrieval tasks.
</p></li>
</ul>

<h3>Title: Compound Batch Normalization for Long-tailed Image Classification. (arXiv:2212.01007v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.01007">http://arxiv.org/abs/2212.01007</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.01007] Compound Batch Normalization for Long-tailed Image Classification](http://arxiv.org/abs/2212.01007) #robust</code></li>
<li>Summary: <p>Significant progress has been made in learning image classification neural
networks under long-tail data distribution using robust training algorithms
such as data re-sampling, re-weighting, and margin adjustment. Those methods,
however, ignore the impact of data imbalance on feature normalization. The
dominance of majority classes (head classes) in estimating statistics and
affine parameters causes internal covariate shifts within less-frequent
categories to be overlooked. To alleviate this challenge, we propose a compound
batch normalization method based on a Gaussian mixture. It can model the
feature space more comprehensively and reduce the dominance of head classes. In
addition, a moving average-based expectation maximization (EM) algorithm is
employed to estimate the statistical parameters of multiple Gaussian
distributions. However, the EM algorithm is sensitive to initialization and can
easily become stuck in local minima where the multiple Gaussian components
continue to focus on majority classes. To tackle this issue, we developed a
dual-path learning framework that employs class-aware split feature
normalization to diversify the estimated Gaussian distributions, allowing the
Gaussian components to fit with training samples of less-frequent classes more
comprehensively. Extensive experiments on commonly used datasets demonstrated
that the proposed method outperforms existing methods on long-tailed image
classification.
</p></li>
</ul>

<h3>Title: Improving Training and Inference of Face Recognition Models via Random Temperature Scaling. (arXiv:2212.01015v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.01015">http://arxiv.org/abs/2212.01015</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.01015] Improving Training and Inference of Face Recognition Models via Random Temperature Scaling](http://arxiv.org/abs/2212.01015) #robust</code></li>
<li>Summary: <p>Data uncertainty is commonly observed in the images for face recognition
(FR). However, deep learning algorithms often make predictions with high
confidence even for uncertain or irrelevant inputs. Intuitively, FR algorithms
can benefit from both the estimation of uncertainty and the detection of
out-of-distribution (OOD) samples. Taking a probabilistic view of the current
classification model, the temperature scalar is exactly the scale of
uncertainty noise implicitly added in the softmax function. Meanwhile, the
uncertainty of images in a dataset should follow a prior distribution. Based on
the observation, a unified framework for uncertainty modeling and FR, Random
Temperature Scaling (RTS), is proposed to learn a reliable FR algorithm. The
benefits of RTS are two-fold. (1) In the training phase, it can adjust the
learning strength of clean and noisy samples for stability and accuracy. (2) In
the test phase, it can provide a score of confidence to detect uncertain,
low-quality and even OOD samples, without training on extra labels. Extensive
experiments on FR benchmarks demonstrate that the magnitude of variance in RTS,
which serves as an OOD detection metric, is closely related to the uncertainty
of the input image. RTS can achieve top performance on both the FR and OOD
detection tasks. Moreover, the model trained with RTS can perform robustly on
datasets with noise. The proposed module is light-weight and only adds
negligible computation cost to the model.
</p></li>
</ul>

<h3>Title: Model and Data Agreement for Learning with Noisy Labels. (arXiv:2212.01054v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.01054">http://arxiv.org/abs/2212.01054</a></li>
<li>Code URL: <a href="https://github.com/zyh-uaiaaaa/MDA-noisy-label-learning">https://github.com/zyh-uaiaaaa/MDA-noisy-label-learning</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2212.01054] Model and Data Agreement for Learning with Noisy Labels](http://arxiv.org/abs/2212.01054) #robust</code></li>
<li>Summary: <p>Learning with noisy labels is a vital topic for practical deep learning as
models should be robust to noisy open-world datasets in the wild. The
state-of-the-art noisy label learning approach JoCoR fails when faced with a
large ratio of noisy labels. Moreover, selecting small-loss samples can also
cause error accumulation as once the noisy samples are mistakenly selected as
small-loss samples, they are more likely to be selected again. In this paper,
we try to deal with error accumulation in noisy label learning from both model
and data perspectives. We introduce mean point ensemble to utilize a more
robust loss function and more information from unselected samples to reduce
error accumulation from the model perspective. Furthermore, as the flip images
have the same semantic meaning as the original images, we select small-loss
samples according to the loss values of flip images instead of the original
ones to reduce error accumulation from the data perspective. Extensive
experiments on CIFAR-10, CIFAR-100, and large-scale Clothing1M show that our
method outperforms state-of-the-art noisy label learning methods with different
levels of label noise. Our method can also be seamlessly combined with other
noisy label learning methods to further improve their performance and
generalize well to other tasks. The code is available in
https://github.com/zyh-uaiaaaa/MDA-noisy-label-learning.
</p></li>
</ul>

<h3>Title: Generative Reasoning Integrated Label Noise Robust Deep Image Representation Learning in Remote Sensing. (arXiv:2212.01261v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.01261">http://arxiv.org/abs/2212.01261</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.01261] Generative Reasoning Integrated Label Noise Robust Deep Image Representation Learning in Remote Sensing](http://arxiv.org/abs/2212.01261) #robust</code></li>
<li>Summary: <p>The development of deep learning based image representation learning (IRL)
methods has attracted great attention in the context of remote sensing (RS)
image understanding. Most of these methods require the availability of a high
quantity and quality of annotated training images, which can be time-consuming
and costly to gather. To reduce labeling costs, publicly available thematic
maps, automatic labeling procedures or crowdsourced data can be used. However,
such approaches increase the risk of including label noise in training data. It
may result in overfitting on noisy labels when discriminative reasoning is
employed as in most of the existing methods. This leads to sub-optimal learning
procedures, and thus inaccurate characterization of RS images. In this paper,
as a first time in RS, we introduce a generative reasoning integrated label
noise robust representation learning (GRID) approach. GRID aims to model the
complementary characteristics of discriminative and generative reasoning for
IRL under noisy labels. To this end, we first integrate generative reasoning
into discriminative reasoning through a variational autoencoder. This allows
our approach to automatically detect training samples with noisy labels. Then,
through our label noise robust hybrid representation learning strategy, GRID
adjusts the whole learning procedure for IRL of these samples through
generative reasoning and that of the other samples through discriminative
reasoning. Our approach learns discriminative image representations while
preventing interference of noisy labels during training independently from the
IRL method. Thus, unlike the existing methods, GRID does not depend on the type
of annotation, label noise, neural network, loss or learning task, and thus can
be utilized for various RS image understanding problems. Experimental results
show the effectiveness of GRID compared to state-of-the-art methods.
</p></li>
</ul>

<h3>Title: MIC: Masked Image Consistency for Context-Enhanced Domain Adaptation. (arXiv:2212.01322v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.01322">http://arxiv.org/abs/2212.01322</a></li>
<li>Code URL: <a href="https://github.com/lhoyer/mic">https://github.com/lhoyer/mic</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2212.01322] MIC: Masked Image Consistency for Context-Enhanced Domain Adaptation](http://arxiv.org/abs/2212.01322) #robust</code></li>
<li>Summary: <p>In unsupervised domain adaptation (UDA), a model trained on source data (e.g.
synthetic) is adapted to target data (e.g. real-world) without access to target
annotation. Most previous UDA methods struggle with classes that have a similar
visual appearance on the target domain as no ground truth is available to learn
the slight appearance differences. To address this problem, we propose a Masked
Image Consistency (MIC) module to enhance UDA by learning spatial context
relations of the target domain as additional clues for robust visual
recognition. MIC enforces the consistency between predictions of masked target
images, where random patches are withheld, and pseudo-labels that are generated
based on the complete image by an exponential moving average teacher. To
minimize the consistency loss, the network has to learn to infer the
predictions of the masked regions from their context. Due to its simple and
universal concept, MIC can be integrated into various UDA methods across
different visual recognition tasks such as image classification, semantic
segmentation, and object detection. MIC significantly improves the
state-of-the-art performance across the different recognition tasks for
synthetic-to-real, day-to-nighttime, and clear-to-adverse-weather UDA. For
instance, MIC achieves an unprecedented UDA performance of 75.9 mIoU and 92.8%
on GTA-to-Cityscapes and VisDA-2017, respectively, which corresponds to an
improvement of +2.1 and +3.0 percent points over the previous state of the art.
The implementation is available at https://github.com/lhoyer/MIC.
</p></li>
</ul>

<h3>Title: AGRO: Adversarial Discovery of Error-prone groups for Robust Optimization. (arXiv:2212.00921v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.00921">http://arxiv.org/abs/2212.00921</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.00921] AGRO: Adversarial Discovery of Error-prone groups for Robust Optimization](http://arxiv.org/abs/2212.00921) #robust</code></li>
<li>Summary: <p>Models trained via empirical risk minimization (ERM) are known to rely on
spurious correlations between labels and task-independent input features,
resulting in poor generalization to distributional shifts. Group
distributionally robust optimization (G-DRO) can alleviate this problem by
minimizing the worst-case loss over a set of pre-defined groups over training
data. G-DRO successfully improves performance of the worst-group, where the
correlation does not hold. However, G-DRO assumes that the spurious
correlations and associated worst groups are known in advance, making it
challenging to apply it to new tasks with potentially multiple unknown spurious
correlations. We propose AGRO -- Adversarial Group discovery for
Distributionally Robust Optimization -- an end-to-end approach that jointly
identifies error-prone groups and improves accuracy on them. AGRO equips G-DRO
with an adversarial slicing model to find a group assignment for training
examples which maximizes worst-case loss over the discovered groups. On the
WILDS benchmark, AGRO results in 8% higher model performance on average on
known worst-groups, compared to prior group discovery approaches used with
G-DRO. AGRO also improves out-of-distribution performance on SST2, QQP, and
MS-COCO -- datasets where potential spurious correlations are as yet
uncharacterized. Human evaluation of ARGO groups shows that they contain
well-defined, yet previously unstudied spurious correlations that lead to model
errors.
</p></li>
</ul>

<h3>Title: Surrogate Gradient Spiking Neural Networks as Encoders for Large Vocabulary Continuous Speech Recognition. (arXiv:2212.01187v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.01187">http://arxiv.org/abs/2212.01187</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.01187] Surrogate Gradient Spiking Neural Networks as Encoders for Large Vocabulary Continuous Speech Recognition](http://arxiv.org/abs/2212.01187) #robust</code></li>
<li>Summary: <p>Compared to conventional artificial neurons that produce dense and
real-valued responses, biologically-inspired spiking neurons transmit sparse
and binary information, which can also lead to energy-efficient
implementations. Recent research has shown that spiking neural networks can be
trained like standard recurrent neural networks using the surrogate gradient
method. They have shown promising results on speech command recognition tasks.
Using the same technique, we show that they are scalable to large vocabulary
continuous speech recognition, where they are capable of replacing LSTMs in the
encoder with only minor loss of performance. This suggests that they may be
applicable to more involved sequence-to-sequence tasks. Moreover, in contrast
to their recurrent non-spiking counterparts, they show robustness to exploding
gradient problems without the need to use gates.
</p></li>
</ul>

<h3>Title: Improving Iterative Text Revision by Learning Where to Edit from Other Revision Tasks. (arXiv:2212.01350v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.01350">http://arxiv.org/abs/2212.01350</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.01350] Improving Iterative Text Revision by Learning Where to Edit from Other Revision Tasks](http://arxiv.org/abs/2212.01350) #robust</code></li>
<li>Summary: <p>Iterative text revision improves text quality by fixing grammatical errors,
rephrasing for better readability or contextual appropriateness, or
reorganizing sentence structures throughout a document. Most recent research
has focused on understanding and classifying different types of edits in the
iterative revision process from human-written text instead of building accurate
and robust systems for iterative text revision. In this work, we aim to build
an end-to-end text revision system that can iteratively generate helpful edits
by explicitly detecting editable spans (where-to-edit) with their corresponding
edit intents and then instructing a revision model to revise the detected edit
spans. Leveraging datasets from other related text editing NLP tasks, combined
with the specification of editable spans, leads our system to more accurately
model the process of iterative text refinement, as evidenced by empirical
results and human evaluations. Our system significantly outperforms previous
baselines on our text revision tasks and other standard text revision tasks,
including grammatical error correction, text simplification, sentence fusion,
and style transfer. Through extensive qualitative and quantitative analysis, we
make vital connections between edit intentions and writing quality, and better
computational modeling of iterative text revisions.
</p></li>
</ul>

<h3>Title: Navigating causal deep learning. (arXiv:2212.00911v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.00911">http://arxiv.org/abs/2212.00911</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.00911] Navigating causal deep learning](http://arxiv.org/abs/2212.00911) #robust</code></li>
<li>Summary: <p>Causal deep learning (CDL) is a new and important research area in the larger
field of machine learning. With CDL, researchers aim to structure and encode
causal knowledge in the extremely flexible representation space of deep
learning models. Doing so will lead to more informed, robust, and general
predictions and inference -- which is important! However, CDL is still in its
infancy. For example, it is not clear how we ought to compare different methods
as they are so different in their output, the way they encode causal knowledge,
or even how they represent this knowledge. This is a living paper that
categorises methods in causal deep learning beyond Pearl's ladder of causation.
We refine the rungs in Pearl's ladder, while also adding a separate dimension
that categorises the parametric assumptions of both input and representation,
arriving at the map of causal deep learning. Our map covers machine learning
disciplines such as supervised learning, reinforcement learning, generative
modelling and beyond. Our paradigm is a tool which helps researchers to: find
benchmarks, compare methods, and most importantly: identify research gaps. With
this work we aim to structure the avalanche of papers being published on causal
deep learning. While papers on the topic are being published daily, our map
remains fixed. We open-source our map for others to use as they see fit:
perhaps to offer guidance in a related works section, or to better highlight
the contribution of their paper.
</p></li>
</ul>

<h3>Title: Clustering through Feature Space Sequence Discovery and Analysis. (arXiv:2212.00996v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.00996">http://arxiv.org/abs/2212.00996</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.00996] Clustering through Feature Space Sequence Discovery and Analysis](http://arxiv.org/abs/2212.00996) #robust</code></li>
<li>Summary: <p>Identifying high-dimensional data patterns without a priori knowledge is an
important task of data science. This paper proposes a simple and efficient
noparametric algorithm: Data Convert to Sequence Analysis, DCSA, which
dynamically explore each point in the feature space without repetition, and a
Directed Hamilton Path will be found. Based on the change point analysis
theory, The sequence corresponding to the path is cut into several fragments to
achieve clustering. The experiments on real-world datasets from different
fields with dimensions ranging from 4 to 20531 confirm that the method in this
work is robust and has visual interpretability in result analysis.
</p></li>
</ul>

<h3>Title: Robustness in Fatigue Strength Estimation. (arXiv:2212.01136v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.01136">http://arxiv.org/abs/2212.01136</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.01136] Robustness in Fatigue Strength Estimation](http://arxiv.org/abs/2212.01136) #robust</code></li>
<li>Summary: <p>Fatigue strength estimation is a costly manual material characterization
process in which state-of-the-art approaches follow a standardized experiment
and analysis procedure. In this paper, we examine a modular, Machine
Learning-based approach for fatigue strength estimation that is likely to
reduce the number of experiments and, thus, the overall experimental costs.
Despite its high potential, deployment of a new approach in a real-life lab
requires more than the theoretical definition and simulation. Therefore, we
study the robustness of the approach against misspecification of the prior and
discretization of the specified loads. We identify its applicability and its
advantageous behavior over the state-of-the-art methods, potentially reducing
the number of costly experiments.
</p></li>
</ul>

<h3>Title: Denoising after Entropy-based Debiasing A Robust Training Method for Dataset Bias with Noisy Labels. (arXiv:2212.01189v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.01189">http://arxiv.org/abs/2212.01189</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.01189] Denoising after Entropy-based Debiasing A Robust Training Method for Dataset Bias with Noisy Labels](http://arxiv.org/abs/2212.01189) #robust</code></li>
<li>Summary: <p>Improperly constructed datasets can result in inaccurate inferences. For
instance, models trained on biased datasets perform poorly in terms of
generalization (i.e., dataset bias). Recent debiasing techniques have
successfully achieved generalization performance by underestimating
easy-to-learn samples (i.e., bias-aligned samples) and highlighting
difficult-to-learn samples (i.e., bias-conflicting samples). However, these
techniques may fail owing to noisy labels, because the trained model recognizes
noisy labels as difficult-to-learn and thus highlights them. In this study, we
find that earlier approaches that used the provided labels to quantify
difficulty could be affected by the small proportion of noisy labels.
Furthermore, we find that running denoising algorithms before debiasing is
ineffective because denoising algorithms reduce the impact of
difficult-to-learn samples, including valuable bias-conflicting samples.
Therefore, we propose an approach called denoising after entropy-based
debiasing, i.e., DENEB, which has three main stages. (1) The prejudice model is
trained by emphasizing (bias-aligned, clean) samples, which are selected using
a Gaussian Mixture Model. (2) Using the per-sample entropy from the output of
the prejudice model, the sampling probability of each sample that is
proportional to the entropy is computed. (3) The final model is trained using
existing denoising algorithms with the mini-batches constructed by following
the computed sampling probability. Compared to existing debiasing and denoising
algorithms, our method achieves better debiasing performance on multiple
benchmarks.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Focus! Relevant and Sufficient Context Selection for News Image Captioning. (arXiv:2212.00843v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.00843">http://arxiv.org/abs/2212.00843</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.00843] Focus! Relevant and Sufficient Context Selection for News Image Captioning](http://arxiv.org/abs/2212.00843) #extraction</code></li>
<li>Summary: <p>News Image Captioning requires describing an image by leveraging additional
context from a news article. Previous works only coarsely leverage the article
to extract the necessary context, which makes it challenging for models to
identify relevant events and named entities. In our paper, we first demonstrate
that by combining more fine-grained context that captures the key named
entities (obtained via an oracle) and the global context that summarizes the
news, we can dramatically improve the model's ability to generate accurate news
captions. This begs the question, how to automatically extract such key
entities from an image? We propose to use the pre-trained vision and language
retrieval model CLIP to localize the visually grounded entities in the news
article and then capture the non-visual entities via an open relation
extraction model. Our experiments demonstrate that by simply selecting a better
context from the article, we can significantly improve the performance of
existing models and achieve new state-of-the-art performance on multiple
benchmarks.
</p></li>
</ul>

<h3>Title: Dunhuang murals contour generation network based on convolution and self-attention fusion. (arXiv:2212.00935v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.00935">http://arxiv.org/abs/2212.00935</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.00935] Dunhuang murals contour generation network based on convolution and self-attention fusion](http://arxiv.org/abs/2212.00935) #extraction</code></li>
<li>Summary: <p>Dunhuang murals are a collection of Chinese style and national style, forming
a self-contained Chinese-style Buddhist art. It has very high historical and
cultural value and research significance. Among them, the lines of Dunhuang
murals are highly general and expressive. It reflects the character's
distinctive character and complex inner emotions. Therefore, the outline
drawing of murals is of great significance to the research of Dunhuang Culture.
The contour generation of Dunhuang murals belongs to image edge detection,
which is an important branch of computer vision, aims to extract salient
contour information in images. Although convolution-based deep learning
networks have achieved good results in image edge extraction by exploring the
contextual and semantic features of images. However, with the enlargement of
the receptive field, some local detail information is lost. This makes it
impossible for them to generate reasonable outline drawings of murals. In this
paper, we propose a novel edge detector based on self-attention combined with
convolution to generate line drawings of Dunhuang murals. Compared with
existing edge detection methods, firstly, a new residual self-attention and
convolution mixed module (Ramix) is proposed to fuse local and global features
in feature maps. Secondly, a novel densely connected backbone extraction
network is designed to efficiently propagate rich edge feature information from
shallow layers into deep layers. Compared with existing methods, it is shown on
different public datasets that our method is able to generate sharper and
richer edge maps. In addition, testing on the Dunhuang mural dataset shows that
our method can achieve very competitive performance.
</p></li>
</ul>

<h3>Title: Planogram Compliance Control via Object Detection, Sequence Alignment, and Focused Iterative Search. (arXiv:2212.01004v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.01004">http://arxiv.org/abs/2212.01004</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.01004] Planogram Compliance Control via Object Detection, Sequence Alignment, and Focused Iterative Search](http://arxiv.org/abs/2212.01004) #extraction</code></li>
<li>Summary: <p>Smart retail stores are becoming the fact of our lives. Several computer
vision and sensor based systems are working together to achieve such a complex
and automated operation. Besides, the retail sector already has several open
and challenging problems which can be solved with the help of pattern
recognition and computer vision methods. One important problem to be tackled is
the planogram compliance control. In this study, we propose a novel method to
solve it. The proposed method is based on object detection, planogram
compliance control, and focused and iterative search steps. The object
detection step is formed by local feature extraction and implicit shape model
formation. The planogram compliance control step is formed by sequence
alignment via the modified Needleman-Wunsch algorithm. The focused and
iterative search step aims to improve the performance of the object detection
and planogram compliance control steps. We tested all three steps on two
different datasets. Based on these tests, we summarize the key findings as well
as strengths and weaknesses of the proposed method.
</p></li>
</ul>

<h3>Title: DWRSeg: Dilation-wise Residual Network for Real-time Semantic Segmentation. (arXiv:2212.01173v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.01173">http://arxiv.org/abs/2212.01173</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.01173] DWRSeg: Dilation-wise Residual Network for Real-time Semantic Segmentation](http://arxiv.org/abs/2212.01173) #extraction</code></li>
<li>Summary: <p>Real-time semantic segmentation has played an important role in intelligent
vehicle scenarios. Recently, numerous networks have incorporated information
from multi-size receptive fields to facilitate feature extraction in real-time
semantic segmentation tasks. However, these methods preferentially adopt
massive receptive fields to elicit more contextual information, which may
result in inefficient feature extraction. We believe that the elaborated
receptive fields are crucial, considering the demand for efficient feature
extraction in real-time tasks. Therefore, we propose an effective and efficient
architecture termed Dilation-wise Residual segmentation (DWRSeg), which
possesses different sets of receptive field sizes within different stages. The
architecture involves (i) a Dilation-wise Residual (DWR) module for extracting
features based on different scales of receptive fields in the high level of the
network; (ii) a Simple Inverted Residual (SIR) module that uses an inverted
bottleneck structure to extract features from the low stage; and (iii) a simple
fully convolutional network (FCN)-like decoder for aggregating multiscale
feature maps to generate the prediction. Extensive experiments on the
Cityscapes and CamVid datasets demonstrate the effectiveness of our method by
achieving a state-of-the-art trade-off between accuracy and inference speed, in
addition to being lighter weight. Without using pretraining or resorting to any
training trick, we achieve 72.7% mIoU on the Cityscapes test set at a speed of
319.5 FPS on one NVIDIA GeForce GTX 1080 Ti card, which is significantly faster
than existing methods. The code and trained models are publicly available.
</p></li>
</ul>

<h3>Title: BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks. (arXiv:2212.01231v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.01231">http://arxiv.org/abs/2212.01231</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.01231] BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks](http://arxiv.org/abs/2212.01231) #extraction</code></li>
<li>Summary: <p>Bird's-Eye-View (BEV) 3D Object Detection is a crucial multi-view technique
for autonomous driving systems. Recently, plenty of works are proposed,
following a similar paradigm consisting of three essential components, i.e.,
camera feature extraction, BEV feature construction, and task heads. Among the
three components, BEV feature construction is BEV-specific compared with 2D
tasks. Existing methods aggregate the multi-view camera features to the
flattened grid in order to construct the BEV feature. However, flattening the
BEV space along the height dimension fails to emphasize the informative
features of different heights. For example, the barrier is located at a low
height while the truck is located at a high height. In this paper, we propose a
novel method named BEV Slice Attention Network (BEV-SAN) for exploiting the
intrinsic characteristics of different heights. Instead of flattening the BEV
space, we first sample along the height dimension to build the global and local
BEV slices. Then, the features of BEV slices are aggregated from the camera
features and merged by the attention mechanism. Finally, we fuse the merged
local and global BEV features by a transformer to generate the final feature
map for task heads. The purpose of local BEV slices is to emphasize informative
heights. In order to find them, we further propose a LiDAR-guided sampling
strategy to leverage the statistical distribution of LiDAR to determine the
heights of local slices. Compared with uniform sampling, LiDAR-guided sampling
can determine more informative heights. We conduct detailed experiments to
demonstrate the effectiveness of BEV-SAN. Code will be released.
</p></li>
</ul>

<h3>Title: Exploring Faithful Rationale for Multi-hop Fact Verification via Salience-Aware Graph Learning. (arXiv:2212.01060v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.01060">http://arxiv.org/abs/2212.01060</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.01060] Exploring Faithful Rationale for Multi-hop Fact Verification via Salience-Aware Graph Learning](http://arxiv.org/abs/2212.01060) #extraction</code></li>
<li>Summary: <p>The opaqueness of the multi-hop fact verification model imposes imperative
requirements for explainability. One feasible way is to extract rationales, a
subset of inputs, where the performance of prediction drops dramatically when
being removed. Though being explainable, most rationale extraction methods for
multi-hop fact verification explore the semantic information within each piece
of evidence individually, while ignoring the topological information
interaction among different pieces of evidence. Intuitively, a faithful
rationale bears complementary information being able to extract other
rationales through the multi-hop reasoning process. To tackle such
disadvantages, we cast explainable multi-hop fact verification as subgraph
extraction, which can be solved based on graph convolutional network (GCN) with
salience-aware graph learning. In specific, GCN is utilized to incorporate the
topological interaction information among multiple pieces of evidence for
learning evidence representation. Meanwhile, to alleviate the influence of
noisy evidence, the salience-aware graph perturbation is induced into the
message passing of GCN. Moreover, the multi-task model with three diagnostic
properties of rationale is elaborately designed to improve the quality of an
explanation without any explicit annotations. Experimental results on the
FEVEROUS benchmark show significant gains over previous state-of-the-art
methods for both rationale extraction and fact verification.
</p></li>
</ul>

<h3>Title: Joint Open Knowledge Base Canonicalization and Linking. (arXiv:2212.01207v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.01207">http://arxiv.org/abs/2212.01207</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.01207] Joint Open Knowledge Base Canonicalization and Linking](http://arxiv.org/abs/2212.01207) #extraction</code></li>
<li>Summary: <p>Open Information Extraction (OIE) methods extract a large number of OIE
triples (noun phrase, relation phrase, noun phrase) from text, which compose
large Open Knowledge Bases (OKBs). However, noun phrases (NPs) and relation
phrases (RPs) in OKBs are not canonicalized and often appear in different
paraphrased textual variants, which leads to redundant and ambiguous facts. To
address this problem, there are two related tasks: OKB canonicalization (i.e.,
convert NPs and RPs to canonicalized form) and OKB linking (i.e., link NPs and
RPs with their corresponding entities and relations in a curated Knowledge Base
(e.g., DBPedia). These two tasks are tightly coupled, and one task can benefit
significantly from the other. However, they have been studied in isolation so
far. In this paper, we explore the task of joint OKB canonicalization and
linking for the first time, and propose a novel framework JOCL based on factor
graph model to make them reinforce each other. JOCL is flexible enough to
combine different signals from both tasks, and able to extend to fit any new
signals. A thorough experimental study over two large scale OIE triple data
sets shows that our framework outperforms all the baseline methods for the task
of OKB canonicalization (OKB linking) in terms of average F1 (accuracy).
</p></li>
</ul>

<h3>Title: Ripple: Concept-Based Interpretation for Raw Time Series Models in Education. (arXiv:2212.01133v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.01133">http://arxiv.org/abs/2212.01133</a></li>
<li>Code URL: <a href="https://github.com/epfl-ml4ed/ripple">https://github.com/epfl-ml4ed/ripple</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2212.01133] Ripple: Concept-Based Interpretation for Raw Time Series Models in Education](http://arxiv.org/abs/2212.01133) #extraction</code></li>
<li>Summary: <p>Time series is the most prevalent form of input data for educational
prediction tasks. The vast majority of research using time series data focuses
on hand-crafted features, designed by experts for predictive performance and
interpretability. However, extracting these features is labor-intensive for
humans and computers. In this paper, we propose an approach that utilizes
irregular multivariate time series modeling with graph neural networks to
achieve comparable or better accuracy with raw time series clickstreams in
comparison to hand-crafted features. Furthermore, we extend concept activation
vectors for interpretability in raw time series models. We analyze these
advances in the education domain, addressing the task of early student
performance prediction for downstream targeted interventions and instructional
support. Our experimental analysis on 23 MOOCs with millions of combined
interactions over six behavioral dimensions show that models designed with our
approach can (i) beat state-of-the-art educational time series baselines with
no feature extraction and (ii) provide interpretable insights for personalized
interventions. Source code: https://github.com/epfl-ml4ed/ripple/.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: FedCoCo: A Memory Efficient Federated Self-supervised Framework for On-Device Visual Representation Learning. (arXiv:2212.01006v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.01006">http://arxiv.org/abs/2212.01006</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.01006] FedCoCo: A Memory Efficient Federated Self-supervised Framework for On-Device Visual Representation Learning](http://arxiv.org/abs/2212.01006) #federate</code></li>
<li>Summary: <p>The ubiquity of edge devices has led to a growing amount of unlabeled data
produced at the edge. Deep learning models deployed on edge devices are
required to learn from these unlabeled data to continuously improve accuracy.
Self-supervised representation learning has achieved promising performances
using centralized unlabeled data. However, the increasing awareness of privacy
protection limits centralizing the distributed unlabeled image data on edge
devices. While federated learning has been widely adopted to enable distributed
machine learning with privacy preservation, without a data selection method to
efficiently select streaming data, the traditional federated learning framework
fails to handle these huge amounts of decentralized unlabeled data with limited
storage resources on edge. To address these challenges, we propose a Federated
on-device Contrastive learning framework with Coreset selection, which we call
FedCoCo, to automatically select a coreset that consists of the most
representative samples into the replay buffer on each device. It preserves data
privacy as each client does not share raw data while learning good visual
representations. Experiments demonstrate the effectiveness and significance of
the proposed method in visual representation learning.
</p></li>
</ul>

<h3>Title: Faster Adaptive Federated Learning. (arXiv:2212.00974v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.00974">http://arxiv.org/abs/2212.00974</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.00974] Faster Adaptive Federated Learning](http://arxiv.org/abs/2212.00974) #federate</code></li>
<li>Summary: <p>Federated learning has attracted increasing attention with the emergence of
distributed data. While extensive federated learning algorithms have been
proposed for the non-convex distributed problem, the federated learning in
practice still faces numerous challenges, such as the large training iterations
to converge since the sizes of models and datasets keep increasing, and the
lack of adaptivity by SGD-based model updates. Meanwhile, the study of adaptive
methods in federated learning is scarce and existing works either lack a
complete theoretical convergence guarantee or have slow sample complexity. In
this paper, we propose an efficient adaptive algorithm (i.e., FAFED) based on
the momentum-based variance reduced technique in cross-silo FL. We first
explore how to design the adaptive algorithm in the FL setting. By providing a
counter-example, we prove that a simple combination of FL and adaptive methods
could lead to divergence. More importantly, we provide a convergence analysis
for our method and prove that our algorithm is the first adaptive FL algorithm
to reach the best-known samples $O(\epsilon^{-3})$ and $O(\epsilon^{-2})$
communication rounds to find an $\epsilon$-stationary point without large
batches. The experimental results on the language modeling task and image
classification task with heterogeneous data demonstrate the efficiency of our
algorithms.
</p></li>
</ul>

<h3>Title: On the Energy and Communication Efficiency Tradeoffs in Federated and Multi-Task Learning. (arXiv:2212.01049v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.01049">http://arxiv.org/abs/2212.01049</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.01049] On the Energy and Communication Efficiency Tradeoffs in Federated and Multi-Task Learning](http://arxiv.org/abs/2212.01049) #federate</code></li>
<li>Summary: <p>Recent advances in Federated Learning (FL) have paved the way towards the
design of novel strategies for solving multiple learning tasks simultaneously,
by leveraging cooperation among networked devices. Multi-Task Learning (MTL)
exploits relevant commonalities across tasks to improve efficiency compared
with traditional transfer learning approaches. By learning multiple tasks
jointly, significant reduction in terms of energy footprints can be obtained.
This article provides a first look into the energy costs of MTL processes
driven by the Model-Agnostic Meta-Learning (MAML) paradigm and implemented in
distributed wireless networks. The paper targets a clustered multi-task network
setup where autonomous agents learn different but related tasks. The MTL
process is carried out in two stages: the optimization of a meta-model that can
be quickly adapted to learn new tasks, and a task-specific model adaptation
stage where the learned meta-model is transferred to agents and tailored for a
specific task. This work analyzes the main factors that influence the MTL
energy balance by considering a multi-task Reinforcement Learning (RL) setup in
a robotized environment. Results show that the MAML method can reduce the
energy bill by at least 2 times compared with traditional approaches without
inductive transfer. Moreover, it is shown that the optimal energy balance in
wireless networks depends on uplink/downlink and sidelink communication
efficiencies.
</p></li>
</ul>

<h3>Title: Generative Data Augmentation for Non-IID Problem in Decentralized Clinical Machine Learning. (arXiv:2212.01109v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.01109">http://arxiv.org/abs/2212.01109</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.01109] Generative Data Augmentation for Non-IID Problem in Decentralized Clinical Machine Learning](http://arxiv.org/abs/2212.01109) #federate</code></li>
<li>Summary: <p>Swarm learning (SL) is an emerging promising decentralized machine learning
paradigm and has achieved high performance in clinical applications. SL solves
the problem of a central structure in federated learning by combining edge
computing and blockchain-based peer-to-peer network. While there are promising
results in the assumption of the independent and identically distributed (IID)
data across participants, SL suffers from performance degradation as the degree
of the non-IID data increases. To address this problem, we propose a generative
augmentation framework in swarm learning called SL-GAN, which augments the
non-IID data by generating the synthetic data from participants. SL-GAN trains
generators and discriminators locally, and periodically aggregation via a
randomly elected coordinator in SL network. Under the standard assumptions, we
theoretically prove the convergence of SL-GAN using stochastic approximations.
Experimental results demonstrate that SL-GAN outperforms state-of-art methods
on three real world clinical datasets including Tuberculosis, Leukemia,
COVID-19.
</p></li>
</ul>

<h3>Title: FedALA: Adaptive Local Aggregation for Personalized Federated Learning. (arXiv:2212.01197v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.01197">http://arxiv.org/abs/2212.01197</a></li>
<li>Code URL: <a href="https://github.com/TsingZ0/FedALA">https://github.com/TsingZ0/FedALA</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2212.01197] FedALA: Adaptive Local Aggregation for Personalized Federated Learning](http://arxiv.org/abs/2212.01197) #federate</code></li>
<li>Summary: <p>A key challenge in federated learning (FL) is the statistical heterogeneity
that impairs the generalization of the global model on each client. To address
this, we propose a method Federated learning with Adaptive Local Aggregation
(FedALA) by capturing the desired information in the global model for client
models in personalized FL. The key component of FedALA is an Adaptive Local
Aggregation (ALA) module, which can adaptively aggregate the downloaded global
model and local model towards the local objective on each client to initialize
the local model before training in each iteration. To evaluate the
effectiveness of FedALA, we conduct extensive experiments with five benchmark
datasets in computer vision and natural language processing domains. FedALA
outperforms eleven state-of-the-art baselines by up to 3.27% in test accuracy.
Furthermore, we also apply ALA module to other federated learning methods and
achieve up to 24.19% improvement in test accuracy.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Tackling Low-Resourced Sign Language Translation: UPC at WMT-SLT 22. (arXiv:2212.01140v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.01140">http://arxiv.org/abs/2212.01140</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.01140] Tackling Low-Resourced Sign Language Translation: UPC at WMT-SLT 22](http://arxiv.org/abs/2212.01140) #fair</code></li>
<li>Summary: <p>This paper describes the system developed at the Universitat Polit`ecnica de
Catalunya for the Workshop on Machine Translation 2022 Sign Language
Translation Task, in particular, for the sign-to-text direction. We use a
Transformer model implemented with the Fairseq modeling toolkit. We have
experimented with the vocabulary size, data augmentation techniques and
pretraining the model with the PHOENIX-14T dataset. Our system obtains 0.50
BLEU score for the test set, improving the organizers' baseline by 0.38 BLEU.
We remark the poor results for both the baseline and our system, and thus, the
unreliability of our findings.
</p></li>
</ul>

<h3>Title: Legal Prompting: Teaching a Language Model to Think Like a Lawyer. (arXiv:2212.01326v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.01326">http://arxiv.org/abs/2212.01326</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.01326] Legal Prompting: Teaching a Language Model to Think Like a Lawyer](http://arxiv.org/abs/2212.01326) #fair</code></li>
<li>Summary: <p>Large language models that are capable of zero or few-shot prompting
approaches have given rise to the new research area of prompt engineering.
Recent advances showed that for example Chain-of-Thought (CoT) prompts can
improve arithmetic or common sense tasks significantly. We explore how such
approaches fair with legal reasoning tasks and take the COLIEE entailment task
based on the Japanese Bar exam for testing zero-shot/few-shot and fine-tuning
approaches. Our findings show that while CoT prompting and fine-tuning with
explanations approaches show improvements, the best results are produced by
prompts that are derived from specific legal reasoning techniques such as IRAC
(Issue, Rule, Application, Conclusion). Based on our experiments we improve the
2021 best result from 0.7037 accuracy to 0.8148 accuracy and beat the 2022 best
system of 0.6789 accuracy with an accuracy of 0.7431.
</p></li>
</ul>

<h3>Title: Fair Generative Models via Transfer Learning. (arXiv:2212.00926v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.00926">http://arxiv.org/abs/2212.00926</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.00926] Fair Generative Models via Transfer Learning](http://arxiv.org/abs/2212.00926) #fair</code></li>
<li>Summary: <p>This work addresses fair generative models. Dataset biases have been a major
cause of unfairness in deep generative models. Previous work had proposed to
augment large, biased datasets with small, unbiased reference datasets. Under
this setup, a weakly-supervised approach has been proposed, which achieves
state-of-the-art quality and fairness in generated samples. In our work, based
on this setup, we propose a simple yet effective approach. Specifically, first,
we propose fairTL, a transfer learning approach to learn fair generative
models. Under fairTL, we pre-train the generative model with the available
large, biased datasets and subsequently adapt the model using the small,
unbiased reference dataset. We find that our fairTL can learn expressive sample
generation during pre-training, thanks to the large (biased) dataset. This
knowledge is then transferred to the target model during adaptation, which also
learns to capture the underlying fair distribution of the small reference
dataset. Second, we propose fairTL++, where we introduce two additional
innovations to improve upon fairTL: (i) multiple feedback and (ii)
Linear-Probing followed by Fine-Tuning (LP-FT). Taking one step further, we
consider an alternative, challenging setup when only a pre-trained (potentially
biased) model is available but the dataset that was used to pre-train the model
is inaccessible. We demonstrate that our proposed fairTL and fairTL++ remain
very effective under this setup. We note that previous work requires access to
the large, biased datasets and is incapable of handling this more challenging
setup. Extensive experiments show that fairTL and fairTL++ achieve
state-of-the-art in both quality and fairness of generated samples. The code
and additional resources can be found at bearwithchris.github.io/fairTL/.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Semantic Role Labeling Meets Definition Modeling: Using Natural Language to Describe Predicate-Argument Structures. (arXiv:2212.01094v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.01094">http://arxiv.org/abs/2212.01094</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.01094] Semantic Role Labeling Meets Definition Modeling: Using Natural Language to Describe Predicate-Argument Structures](http://arxiv.org/abs/2212.01094) #interpretability</code></li>
<li>Summary: <p>One of the common traits of past and present approaches for Semantic Role
Labeling (SRL) is that they rely upon discrete labels drawn from a predefined
linguistic inventory to classify predicate senses and their arguments. However,
we argue this need not be the case. In this paper, we present an approach that
leverages Definition Modeling to introduce a generalized formulation of SRL as
the task of describing predicate-argument structures using natural language
definitions instead of discrete labels. Our novel formulation takes a first
step towards placing interpretability and flexibility foremost, and yet our
experiments and analyses on PropBank-style and FrameNet-style, dependency-based
and span-based SRL also demonstrate that a flexible model with an interpretable
output does not necessarily come at the expense of performance. We release our
software for research purposes at https://github.com/SapienzaNLP/dsrl.
</p></li>
</ul>

<h3>Title: Credit Assignment for Trained Neural Networks Based on Koopman Operator Theory. (arXiv:2212.00998v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.00998">http://arxiv.org/abs/2212.00998</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.00998] Credit Assignment for Trained Neural Networks Based on Koopman Operator Theory](http://arxiv.org/abs/2212.00998) #interpretability</code></li>
<li>Summary: <p>Credit assignment problem of neural networks refers to evaluating the credit
of each network component to the final outputs. For an untrained neural
network, approaches to tackling it have made great contributions to parameter
update and model revolution during the training phase. This problem on trained
neural networks receives rare attention, nevertheless, it plays an increasingly
important role in neural network patch, specification and verification. Based
on Koopman operator theory, this paper presents an alternative perspective of
linear dynamics on dealing with the credit assignment problem for trained
neural networks. Regarding a neural network as the composition of sub-dynamics
series, we utilize step-delay embedding to capture snapshots of each component,
characterizing the established mapping as exactly as possible. To circumvent
the dimension-difference problem encountered during the embedding, a
composition and decomposition of an auxiliary linear layer, termed minimal
linear dimension alignment, is carefully designed with rigorous formal
guarantee. Afterwards, each component is approximated by a Koopman operator and
we derive the Jacobian matrix and its corresponding determinant, similar to
backward propagation. Then, we can define a metric with algebraic
interpretability for the credit assignment of each network component. Moreover,
experiments conducted on typical neural networks demonstrate the effectiveness
of the proposed method.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: Role of Audio in Audio-Visual Video Summarization. (arXiv:2212.01040v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.01040">http://arxiv.org/abs/2212.01040</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.01040] Role of Audio in Audio-Visual Video Summarization](http://arxiv.org/abs/2212.01040) #explainability</code></li>
<li>Summary: <p>Video summarization attracts attention for efficient video representation,
retrieval, and browsing to ease volume and traffic surge problems. Although
video summarization mostly uses the visual channel for compaction, the benefits
of audio-visual modeling appeared in recent literature. The information coming
from the audio channel can be a result of audio-visual correlation in the video
content. In this study, we propose a new audio-visual video summarization
framework integrating four ways of audio-visual information fusion with
GRU-based and attention-based networks. Furthermore, we investigate a new
explainability methodology using audio-visual canonical correlation analysis
(CCA) to better understand and explain the role of audio in the video
summarization task. Experimental evaluations on the TVSum dataset attain F1
score and Kendall-tau score improvements for the audio-visual video
summarization. Furthermore, splitting video content on TVSum and COGNIMUSE
datasets based on audio-visual CCA as positively and negatively correlated
videos yields a strong performance improvement over the positively correlated
videos for audio-only and audio-visual video summarization.
</p></li>
</ul>

<h3>Title: SOLD: Sinhala Offensive Language Dataset. (arXiv:2212.00851v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.00851">http://arxiv.org/abs/2212.00851</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.00851] SOLD: Sinhala Offensive Language Dataset](http://arxiv.org/abs/2212.00851) #explainability</code></li>
<li>Summary: <p>The widespread of offensive content online, such as hate speech and
cyber-bullying, is a global phenomenon. This has sparked interest in the
artificial intelligence (AI) and natural language processing (NLP) communities,
motivating the development of various systems trained to detect potentially
harmful content automatically. These systems require annotated datasets to
train the machine learning (ML) models. However, with a few notable exceptions,
most datasets on this topic have dealt with English and a few other
high-resource languages. As a result, the research in offensive language
identification has been limited to these languages. This paper addresses this
gap by tackling offensive language identification in Sinhala, a low-resource
Indo-Aryan language spoken by over 17 million people in Sri Lanka. We introduce
the Sinhala Offensive Language Dataset (SOLD) and present multiple experiments
on this dataset. SOLD is a manually annotated dataset containing 10,000 posts
from Twitter annotated as offensive and not offensive at both sentence-level
and token-level, improving the explainability of the ML models. SOLD is the
first large publicly available offensive language dataset compiled for Sinhala.
We also introduce SemiSOLD, a larger dataset containing more than 145,000
Sinhala tweets, annotated following a semi-supervised approach.
</p></li>
</ul>

<h3>Title: VeriX: Towards Verified Explainability of Deep Neural Networks. (arXiv:2212.01051v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.01051">http://arxiv.org/abs/2212.01051</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.01051] VeriX: Towards Verified Explainability of Deep Neural Networks](http://arxiv.org/abs/2212.01051) #explainability</code></li>
<li>Summary: <p>We present VeriX, a first step towards verified explainability of machine
learning models in safety-critical applications. Specifically, our sound and
optimal explanations can guarantee prediction invariance against bounded
perturbations. We utilise constraint solving techniques together with feature
sensitivity ranking to efficiently compute these explanations. We evaluate our
approach on image recognition benchmarks and a real-world scenario of
autonomous aircraft taxiing.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: 3D-LDM: Neural Implicit 3D Shape Generation with Latent Diffusion Models. (arXiv:2212.00842v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.00842">http://arxiv.org/abs/2212.00842</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.00842] 3D-LDM: Neural Implicit 3D Shape Generation with Latent Diffusion Models](http://arxiv.org/abs/2212.00842) #diffusion</code></li>
<li>Summary: <p>Diffusion models have shown great promise for image generation, beating GANs
in terms of generation diversity, with comparable image quality. However, their
application to 3D shapes has been limited to point or voxel representations
that can in practice not accurately represent a 3D surface. We propose a
diffusion model for neural implicit representations of 3D shapes that operates
in the latent space of an auto-decoder. This allows us to generate diverse and
high quality 3D surfaces. We additionally show that we can condition our model
on images or text to enable image-to-3D generation and text-to-3D generation
using CLIP embeddings. Furthermore, adding noise to the latent codes of
existing shapes allows us to explore shape variations.
</p></li>
</ul>

<h3>Title: ObjectStitch: Generative Object Compositing. (arXiv:2212.00932v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.00932">http://arxiv.org/abs/2212.00932</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.00932] ObjectStitch: Generative Object Compositing](http://arxiv.org/abs/2212.00932) #diffusion</code></li>
<li>Summary: <p>Object compositing based on 2D images is a challenging problem since it
typically involves multiple processing stages such as color harmonization,
geometry correction and shadow generation to generate realistic results.
Furthermore, annotating training data pairs for compositing requires
substantial manual effort from professionals, and is hardly scalable. Thus,
with the recent advances in generative models, in this work, we propose a
self-supervised framework for object compositing by leveraging the power of
conditional diffusion models. Our framework can hollistically address the
object compositing task in a unified model, transforming the viewpoint,
geometry, color and shadow of the generated object while requiring no manual
labeling. To preserve the input object's characteristics, we introduce a
content adaptor that helps to maintain categorical semantics and object
appearance. A data augmentation method is further adopted to improve the
fidelity of the generator. Our method outperforms relevant baselines in both
realism and faithfulness of the synthesized result images in a user study on
various real-world images.
</p></li>
</ul>

<h3>Title: DiffRF: Rendering-Guided 3D Radiance Field Diffusion. (arXiv:2212.01206v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.01206">http://arxiv.org/abs/2212.01206</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.01206] DiffRF: Rendering-Guided 3D Radiance Field Diffusion](http://arxiv.org/abs/2212.01206) #diffusion</code></li>
<li>Summary: <p>We introduce DiffRF, a novel approach for 3D radiance field synthesis based
on denoising diffusion probabilistic models. While existing diffusion-based
methods operate on images, latent codes, or point cloud data, we are the first
to directly generate volumetric radiance fields. To this end, we propose a 3D
denoising model which directly operates on an explicit voxel grid
representation. However, as radiance fields generated from a set of posed
images can be ambiguous and contain artifacts, obtaining ground truth radiance
field samples is non-trivial. We address this challenge by pairing the
denoising formulation with a rendering loss, enabling our model to learn a
deviated prior that favours good image quality instead of trying to replicate
fitting errors like floating artifacts. In contrast to 2D-diffusion models, our
model learns multi-view consistent priors, enabling free-view synthesis and
accurate shape generation. Compared to 3D GANs, our diffusion-based approach
naturally enables conditional generation such as masked completion or
single-view 3D synthesis at inference time.
</p></li>
</ul>

<h3>Title: Diffusion Generative Models in Infinite Dimensions. (arXiv:2212.00886v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.00886">http://arxiv.org/abs/2212.00886</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.00886] Diffusion Generative Models in Infinite Dimensions](http://arxiv.org/abs/2212.00886) #diffusion</code></li>
<li>Summary: <p>Diffusion generative models have recently been applied to domains where the
available data can be seen as a discretization of an underlying function, such
as audio signals or time series. However, these models operate directly on the
discretized data, and there are no semantics in the modeling process that
relate the observed data to the underlying functional forms. We generalize
diffusion models to operate directly in function space by developing the
foundational theory for such models in terms of Gaussian measures on Hilbert
spaces. A significant benefit of our function space point of view is that it
allows us to explicitly specify the space of functions we are working in,
leading us to develop methods for diffusion generative modeling in Sobolev
spaces. Our approach allows us to perform both unconditional and conditional
generation of function-valued data. We demonstrate our methods on several
synthetic and real-world benchmarks.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
