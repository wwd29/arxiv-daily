<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-06-24</h1>
<h3>Title: Outcome-Based Education: Evaluating Students' Perspectives Using Transformer</h3>
<ul>
<li><strong>Authors: </strong>Shuvra Smaran Das, Anirban Saha Anik, Md Kishor Morol, Mohammad Sakib Mahmood</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17223">https://arxiv.org/abs/2506.17223</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17223">https://arxiv.org/pdf/2506.17223</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17223]] Outcome-Based Education: Evaluating Students' Perspectives Using Transformer(https://arxiv.org/abs/2506.17223)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Outcome-Based Education (OBE) emphasizes the development of specific competencies through student-centered learning. In this study, we reviewed the importance of OBE and implemented transformer-based models, particularly DistilBERT, to analyze an NLP dataset that includes student feedback. Our objective is to assess and improve educational outcomes. Our approach is better than other machine learning models because it uses the transformer's deep understanding of language context to classify sentiment better, giving better results across a wider range of matrices. Our work directly contributes to OBE's goal of achieving measurable outcomes by facilitating the identification of patterns in student learning experiences. We have also applied LIME (local interpretable model-agnostic explanations) to make sure that model predictions are clear. This gives us understandable information about how key terms affect sentiment. Our findings indicate that the combination of transformer models and LIME explanations results in a strong and straightforward framework for analyzing student feedback. This aligns more closely with the principles of OBE and ensures the improvement of educational practices through data-driven insights.</li>
</ul>

<h3>Title: MMET: A Multi-Input and Multi-Scale Transformer for Efficient PDEs Solving</h3>
<ul>
<li><strong>Authors: </strong>Yichen Luo, Jia Wang, Dapeng Lan, Yu Liu, Zhibo Pang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17230">https://arxiv.org/abs/2506.17230</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17230">https://arxiv.org/pdf/2506.17230</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17230]] MMET: A Multi-Input and Multi-Scale Transformer for Efficient PDEs Solving(https://arxiv.org/abs/2506.17230)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Partial Differential Equations (PDEs) are fundamental for modeling physical systems, yet solving them in a generic and efficient manner using machine learning-based approaches remains challenging due to limited multi-input and multi-scale generalization capabilities, as well as high computational costs. This paper proposes the Multi-input and Multi-scale Efficient Transformer (MMET), a novel framework designed to address the above challenges. MMET decouples mesh and query points as two sequences and feeds them into the encoder and decoder, respectively, and uses a Gated Condition Embedding (GCE) layer to embed input variables or functions with varying dimensions, enabling effective solutions for multi-scale and multi-input problems. Additionally, a Hilbert curve-based reserialization and patch embedding mechanism decrease the input length. This significantly reduces the computational cost when dealing with large-scale geometric models. These innovations enable efficient representations and support multi-scale resolution queries for large-scale and multi-input PDE problems. Experimental evaluations on diverse benchmarks spanning different physical fields demonstrate that MMET outperforms SOTA methods in both accuracy and computational efficiency. This work highlights the potential of MMET as a robust and scalable solution for real-time PDE solving in engineering and physics-based applications, paving the way for future explorations into pre-trained large-scale models in specific domains. This work is open-sourced at this https URL.</li>
</ul>

<h3>Title: Efficient and Stealthy Jailbreak Attacks via Adversarial Prompt Distillation from LLMs to SLMs</h3>
<ul>
<li><strong>Authors: </strong>Xiang Li, Chong Zhang, Jia Wang, Fangyu Wu, Yushi Li, Xiaobo Jin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17231">https://arxiv.org/abs/2506.17231</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17231">https://arxiv.org/pdf/2506.17231</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17231]] Efficient and Stealthy Jailbreak Attacks via Adversarial Prompt Distillation from LLMs to SLMs(https://arxiv.org/abs/2506.17231)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, steal, large language model</a></li>
<li><strong>Abstract: </strong>Attacks on large language models (LLMs) in jailbreaking scenarios raise many security and ethical issues. Current jailbreak attack methods face problems such as low efficiency, high computational cost, and poor cross-model adaptability and versatility, which make it difficult to cope with the rapid development of LLM and new defense strategies. Our work proposes an Adversarial Prompt Distillation, which combines masked language modeling, reinforcement learning, and dynamic temperature control through a prompt generation and distillation method. It enables small language models (SLMs) to jailbreak attacks on mainstream LLMs. The experimental results verify the superiority of the proposed method in terms of attack success rate and harm, and reflect the resource efficiency and cross-model adaptability. This research explores the feasibility of distilling the jailbreak ability of LLM to SLM, reveals the model's vulnerability, and provides a new idea for LLM security research.</li>
</ul>

<h3>Title: PCaM: A Progressive Focus Attention-Based Information Fusion Method for Improving Vision Transformer Domain Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Zelin Zang, Fei Wang, Liangyu Li, Jinlin Wu, Chunshui Zhao, Zhen Lei, Baigui Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17232">https://arxiv.org/abs/2506.17232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17232">https://arxiv.org/pdf/2506.17232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17232]] PCaM: A Progressive Focus Attention-Based Information Fusion Method for Improving Vision Transformer Domain Adaptation(https://arxiv.org/abs/2506.17232)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Unsupervised Domain Adaptation (UDA) aims to transfer knowledge from a labeled source domain to an unlabeled target domain. Recent UDA methods based on Vision Transformers (ViTs) have achieved strong performance through attention-based feature alignment. However, we identify a key limitation: foreground object mismatch, where the discrepancy in foreground object size and spatial distribution across domains weakens attention consistency and hampers effective domain alignment. To address this issue, we propose the Progressive Focus Cross-Attention Mechanism (PCaM), which progressively filters out background information during cross-attention, allowing the model to focus on and fuse discriminative foreground semantics across domains. We further introduce an attentional guidance loss that explicitly directs attention toward task-relevant regions, enhancing cross-domain attention consistency. PCaM is lightweight, architecture-agnostic, and easy to integrate into existing ViT-based UDA pipelines. Extensive experiments on Office-Home, DomainNet, VisDA-2017, and remote sensing datasets demonstrate that PCaM significantly improves adaptation performance and achieves new state-of-the-art results, validating the effectiveness of attention-guided foreground fusion for domain adaptation.</li>
</ul>

<h3>Title: Design, Implementation, and Analysis of Fair Faucets for Blockchain Ecosystems</h3>
<ul>
<li><strong>Authors: </strong>Serdar Metin</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CE, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17236">https://arxiv.org/abs/2506.17236</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17236">https://arxiv.org/pdf/2506.17236</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17236]] Design, Implementation, and Analysis of Fair Faucets for Blockchain Ecosystems(https://arxiv.org/abs/2506.17236)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, attack, fair</a></li>
<li><strong>Abstract: </strong>The present dissertation addresses the problem of fairly distributing shared resources in non-commercial blockchain networks. Blockchains are distributed systems that order and timestamp records of a given network of users, in a public, cryptographically secure, and consensual way. The records, which may in kind be events, transaction orders, sets of rules for structured transactions etc. are placed within well-defined datastructures called blocks, and they are linked to each other by the virtue of cryptographic pointers, in a total ordering which represents their temporal relations of succession. The ability to operate on the blockchain, and/or to contribute a record to the content of a block are shared resources of the blockchain systems. In commercial networks, these resources are exchanged in return for fiat money, and consequently, fairness is not a relevant problem in terms of computer engineering. In non-commercial networks, however, monetary solutions are not available, by definition. The present non-commercial blockchain networks employ trivial distribution mechanisms called faucets, which offer fixed amounts of free tokens (called cryptocurrencies) specific to the given network. This mechanism, although simple and efficient, is prone to denial of service (DoS) attacks and cannot address the fairness problem. In the present dissertation, the faucet mechanism is adapted for fair distribution, in line with Max-min Fairness scheme. In total, we contributed 6 distinct Max-min Fair algorithms as efficient blockchain faucets. The algorithms we contribute are resistant to DoS attacks, low-cost in terms of blockchain computation economics, and they also allow for different user weighting policies.</li>
</ul>

<h3>Title: Mechanistic Interpretability of Diffusion Models: Circuit-Level Analysis and Causal Validation</h3>
<ul>
<li><strong>Authors: </strong>Dip Roy</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17237">https://arxiv.org/abs/2506.17237</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17237">https://arxiv.org/pdf/2506.17237</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17237]] Mechanistic Interpretability of Diffusion Models: Circuit-Level Analysis and Causal Validation(https://arxiv.org/abs/2506.17237)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, diffusion, generative</a></li>
<li><strong>Abstract: </strong>We present a quantitative circuit-level analysis of diffusion models, establishing computational pathways and mechanistic principles underlying image generation processes. Through systematic intervention experiments across 2,000 synthetic and 2,000 CelebA facial images, we discover fundamental algorithmic differences in how diffusion architectures process synthetic versus naturalistic data distributions. Our investigation reveals that real-world face processing requires circuits with measurably higher computational complexity (complexity ratio = 1.084 plus/minus 0.008, p < 0.001), exhibiting distinct attention specialization patterns with entropy divergence ranging from 0.015 to 0.166 across denoising timesteps. We identify eight functionally distinct attention mechanisms showing specialized computational roles: edge detection (entropy = 3.18 plus/minus 0.12), texture analysis (entropy = 4.16 plus/minus 0.08), and semantic understanding (entropy = 2.67 plus/minus 0.15). Intervention analysis demonstrates critical computational bottlenecks where targeted ablations produce 25.6% to 128.3% performance degradation, providing causal evidence for identified circuit functions. These findings establish quantitative foundations for algorithmic understanding and control of generative model behavior through mechanistic intervention strategies.</li>
</ul>

<h3>Title: Training a Scientific Reasoning Model for Chemistry</h3>
<ul>
<li><strong>Authors: </strong>Siddharth M. Narayanan, James D. Braza, Ryan-Rhys Griffiths, Albert Bou, Geemi Wellawatte, Mayk Caldas Ramos, Ludovico Mitchener, Samuel G. Rodriques, Andrew D. White</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17238">https://arxiv.org/abs/2506.17238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17238">https://arxiv.org/pdf/2506.17238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17238]] Training a Scientific Reasoning Model for Chemistry(https://arxiv.org/abs/2506.17238)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reasoning models are large language models that emit a long chain-of-thought before answering, providing both higher accuracy and explicit reasoning for their response. A major question has been whether language model reasoning generalizes beyond mathematics, programming, and logic, where most previous work has focused. We demonstrate that reasoning models can be post-trained for chemistry without additional domain pretraining, and require substantially less data compared to contemporary domain-specific models. We report ether0, a 24B parameter LLM (based on Mistral-Small-24B) that can reason in natural language and respond with chemical structures. This reasoning model was trained with reinforcement learning on 640,730 experimentally-grounded chemistry problems across 375 tasks ranging from synthesizability, to blood-brain barrier permeability, to human receptor activity, to scent. Our model exceeds general-purpose chemistry models, frontier models, and human experts on molecular design tasks. It is also more data efficient relative to specialized models. We anticipate that this method can be applied to train data-efficient language models specialized for tasks across a wide variety of scientific domains.</li>
</ul>

<h3>Title: Detecting and Mitigating SQL Injection Vulnerabilities in Web Applications</h3>
<ul>
<li><strong>Authors: </strong>Sagar Neupane</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17245">https://arxiv.org/abs/2506.17245</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17245">https://arxiv.org/pdf/2506.17245</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17245]] Detecting and Mitigating SQL Injection Vulnerabilities in Web Applications(https://arxiv.org/abs/2506.17245)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>SQL injection (SQLi) remains a critical vulnerability in web applications, enabling attackers to manipulate databases through malicious inputs. Despite advancements in mitigation techniques, the evolving complexity of web applications and attack strategies continues to pose significant risks. This paper presents a comprehensive penetration testing methodology to identify, exploit, and mitigate SQLi vulnerabilities in a PHP-MySQL-based web application. Utilizing tools such as OWASP ZAP, sqlmap, and Nmap, the study demonstrates a systematic approach to vulnerability assessment and remediation. The findings underscore the efficacy of input sanitization and prepared statements in mitigating SQLi risks, while highlighting the need for ongoing security assessments to address emerging threats. The study contributes to the field by providing practical insights into effective detection and prevention strategies, supported by a real-world case study.</li>
</ul>

<h3>Title: Recursive Learning-Based Virtual Buffering for Analytical Global Placement</h3>
<ul>
<li><strong>Authors: </strong>Andrew B. Kahng, Yiting Liu, Zhiang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17247">https://arxiv.org/abs/2506.17247</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17247">https://arxiv.org/pdf/2506.17247</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17247]] Recursive Learning-Based Virtual Buffering for Analytical Global Placement(https://arxiv.org/abs/2506.17247)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Due to the skewed scaling of interconnect versus cell delay in modern technology nodes, placement with buffer porosity (i.e., cell density) awareness is essential for timing closure in physical synthesis flows. However, existing approaches face two key challenges: (i) traditional van Ginneken-Lillis-style buffering approaches are computationally expensive during global placement; and (ii) machine learning-based approaches, such as BufFormer, lack a thorough consideration of Electrical Rule Check (ERC) violations and fail to "close the loop" back into the physical design flow. In this work, we propose MLBuf-RePlAce, the first open-source learning-driven virtual buffering-aware analytical global placement framework, built on top of the OpenROAD infrastructure. MLBuf-RePlAce adopts an efficient recursive learning-based generative buffering approach to predict buffer types and locations, addressing ERC violations during global placement. We compare MLBuf-RePlAce against the default virtual buffering-based timing-driven global placer in OpenROAD, using open-source testcases from the TILOS MacroPlacement and OpenROAD-flow-scripts repositories. Without degradation of post-route power, MLBuf-RePlAce achieves (maximum, average) improvements of (56%, 31%) in total negative slack (TNS) within the open-source OpenROAD flow. When evaluated by completion in a commercial flow, MLBuf-RePlAce achieves (maximum, average) improvements of (53%, 28%) in TNS with an average of 0.2% improvement in post-route power.</li>
</ul>

<h3>Title: Towards Interpretable Adversarial Examples via Sparse Adversarial Attack</h3>
<ul>
<li><strong>Authors: </strong>Fudong Lin, Jiadong Lou, Hao Wang, Brian Jalaian, Xu Yuan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17250">https://arxiv.org/abs/2506.17250</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17250">https://arxiv.org/pdf/2506.17250</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17250]] Towards Interpretable Adversarial Examples via Sparse Adversarial Attack(https://arxiv.org/abs/2506.17250)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Sparse attacks are to optimize the magnitude of adversarial perturbations for fooling deep neural networks (DNNs) involving only a few perturbed pixels (i.e., under the l0 constraint), suitable for interpreting the vulnerability of DNNs. However, existing solutions fail to yield interpretable adversarial examples due to their poor sparsity. Worse still, they often struggle with heavy computational overhead, poor transferability, and weak attack strength. In this paper, we aim to develop a sparse attack for understanding the vulnerability of CNNs by minimizing the magnitude of initial perturbations under the l0 constraint, to overcome the existing drawbacks while achieving a fast, transferable, and strong attack to DNNs. In particular, a novel and theoretical sound parameterization technique is introduced to approximate the NP-hard l0 optimization problem, making directly optimizing sparse perturbations computationally feasible. Besides, a novel loss function is designed to augment initial perturbations by maximizing the adversary property and minimizing the number of perturbed pixels simultaneously. Extensive experiments are conducted to demonstrate that our approach, with theoretical performance guarantees, outperforms state-of-the-art sparse attacks in terms of computational overhead, transferability, and attack strength, expecting to serve as a benchmark for evaluating the robustness of DNNs. In addition, theoretical and empirical results validate that our approach yields sparser adversarial examples, empowering us to discover two categories of noises, i.e., "obscuring noise" and "leading noise", which will help interpret how adversarial perturbation misleads the classifiers into incorrect predictions. Our code is available at this https URL.</li>
</ul>

<h3>Title: Adaptive Sample Scheduling for Direct Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Zixuan Huang, Yikun Ban, Lean Fu, Xiaojie Li, Zhongxiang Dai, Jianxin Li, Deqing Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17252">https://arxiv.org/abs/2506.17252</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17252">https://arxiv.org/pdf/2506.17252</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17252]] Adaptive Sample Scheduling for Direct Preference Optimization(https://arxiv.org/abs/2506.17252)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Direct Preference Optimization (DPO) has emerged as an effective approach for aligning large language models (LLMs) with human preferences. However, its performance is highly dependent on the quality of the underlying human preference data. To address this bottleneck, prior work has explored various data selection strategies, but these methods often overlook the impact of the evolving states of the language model during the DPO process. %including active querying, response pair selection, and data pre-selection. In this paper, we introduce a novel problem: Sample Scheduling for DPO, which aims to dynamically and adaptively schedule training samples based on the model's evolving states throughout preference optimization. To solve this problem, we propose SamS, an efficient and effective algorithm that adaptively selects samples in each training batch based on the LLM's learning feedback to maximize the potential generalization performance. Notably, without modifying the core DPO algorithm, simply integrating SamS significantly improves performance across tasks, with minimal additional computational overhead. This work points to a promising new direction for improving LLM alignment through more effective utilization of fixed preference datasets.</li>
</ul>

<h3>Title: MS-TVNet:A Long-Term Time Series Prediction Method Based on Multi-Scale Dynamic Convolution</h3>
<ul>
<li><strong>Authors: </strong>Chenghan Li, Mingchen Li, Yipu Liao, Ruisheng Diao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17253">https://arxiv.org/abs/2506.17253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17253">https://arxiv.org/pdf/2506.17253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17253]] MS-TVNet:A Long-Term Time Series Prediction Method Based on Multi-Scale Dynamic Convolution(https://arxiv.org/abs/2506.17253)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Long-term time series prediction has predominantly relied on Transformer and MLP models, while the potential of convolutional networks in this domain remains underexplored. To address this gap, we introduce a novel multi-scale time series reshape module, which effectively captures the relationships among multi-period patches and variable dependencies. Building upon this module, we propose MS-TVNet, a multi-scale 3D dynamic convolutional neural network. Through comprehensive evaluations on diverse datasets, MS-TVNet demonstrates superior performance compared to baseline models, achieving state-of-the-art (SOTA) results in long-term time series prediction. Our findings highlight the effectiveness of leveraging convolutional networks for capturing complex temporal patterns, suggesting a promising direction for future research in this this http URL code is realsed on this https URL.</li>
</ul>

<h3>Title: Keeping Up with the Models: Online Deployment and Routing of LLMs at Scale</h3>
<ul>
<li><strong>Authors: </strong>Shaoang Li, Jian Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17254">https://arxiv.org/abs/2506.17254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17254">https://arxiv.org/pdf/2506.17254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17254]] Keeping Up with the Models: Online Deployment and Routing of LLMs at Scale(https://arxiv.org/abs/2506.17254)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid pace at which new large language models (LLMs) appear -- and older ones become obsolete -- forces LLM service providers to juggle a streaming inventory of models while respecting tight deployment capacity and per-query cost budgets. We cast the reality as an online decision problem that couples stage-wise deployment, made at fixed maintenance windows, with per-query routing among the models kept live. We introduce StageRoute, a hierarchical algorithm that (i) optimistically selects up to $M_max$ models for the next stage using reward upper-confidence and cost lower-confidence bounds, then (ii) solves a budget-constrained bandit sub-problem to route each incoming query. We prove that StageRoute achieves a regret of order $T^{2/3}$ and provide a matching lower bound, thereby establishing its near-optimality. Moreover, our experiments confirm the theory, demonstrating that StageRoute performs close to the optimum in practical settings.</li>
</ul>

<h3>Title: UltraSketchLLM: Saliency-Driven Sketching for Ultra-Low Bit LLM Compression</h3>
<ul>
<li><strong>Authors: </strong>Sunan Zou, Ziyun Zhang, Xueting Sun, Guojie Luo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17255">https://arxiv.org/abs/2506.17255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17255">https://arxiv.org/pdf/2506.17255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17255]] UltraSketchLLM: Saliency-Driven Sketching for Ultra-Low Bit LLM Compression(https://arxiv.org/abs/2506.17255)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid growth of large language models (LLMs) has outpaced the memory constraints of edge devices, necessitating extreme weight compression beyond the 1-bit limit. While quantization reduces model size, it is fundamentally limited to 1 bit per weight. Existing multiple-to-one compression methods either rely on mapping tables (inducing memory overhead) or incur severe accuracy degradation due to random weight grouping. We introduce UltraSketchLLM, an index-free, sketch-based framework that achieves ultra-low bit compression (down to 0.5 bits per weight) while preserving model performance. UltraSketchLLM leverages data sketching, a sub-linear representation technique from streaming applications, to map multiple weights to single values with bounded error. Our approach integrates an underestimate AbsMaxMin sketch to minimize relative errors for small weights, importance-aware space allocation to prioritize salient weights, and a straight-through estimator for compression-aware finetuning. Experiments on Llama-3.2-1B demonstrate up to 0.5-bit compression with competitive perplexity, alongside tolerable latency overhead. UltraSketchLLM offers a practical solution for deploying LLMs in resource-constrained environments.</li>
</ul>

<h3>Title: AI to Identify Strain-sensitive Regions of the Optic Nerve Head Linked to Functional Loss in Glaucoma</h3>
<ul>
<li><strong>Authors: </strong>Thanadet Chuangsuwanich, Monisha E. Nongpiur, Fabian A. Braeu, Tin A. Tun, Alexandre Thiery, Shamira Perera, Ching Lin Ho, Martin Buist, George Barbastathis, Tin Aung, Michaël J.A. Girard</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17262">https://arxiv.org/abs/2506.17262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17262">https://arxiv.org/pdf/2506.17262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17262]] AI to Identify Strain-sensitive Regions of the Optic Nerve Head Linked to Functional Loss in Glaucoma(https://arxiv.org/abs/2506.17262)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Objective: (1) To assess whether ONH biomechanics improves prediction of three progressive visual field loss patterns in glaucoma; (2) to use explainable AI to identify strain-sensitive ONH regions contributing to these predictions. Methods: We recruited 237 glaucoma subjects. The ONH of one eye was imaged under two conditions: (1) primary gaze and (2) primary gaze with IOP elevated to ~35 mmHg via ophthalmo-dynamometry. Glaucoma experts classified the subjects into four categories based on the presence of specific visual field defects: (1) superior nasal step (N=26), (2) superior partial arcuate (N=62), (3) full superior hemifield defect (N=25), and (4) other/non-specific defects (N=124). Automatic ONH tissue segmentation and digital volume correlation were used to compute IOP-induced neural tissue and lamina cribrosa (LC) strains. Biomechanical and structural features were input to a Geometric Deep Learning model. Three classification tasks were performed to detect: (1) superior nasal step, (2) superior partial arcuate, (3) full superior hemifield defect. For each task, the data were split into 80% training and 20% testing sets. Area under the curve (AUC) was used to assess performance. Explainable AI techniques were employed to highlight the ONH regions most critical to each classification. Results: Models achieved high AUCs of 0.77-0.88, showing that ONH strain improved VF loss prediction beyond morphology alone. The inferior and inferotemporal rim were identified as key strain-sensitive regions, contributing most to visual field loss prediction and showing progressive expansion with increasing disease severity. Conclusion and Relevance: ONH strain enhances prediction of glaucomatous VF loss patterns. Neuroretinal rim, rather than the LC, was the most critical region contributing to model predictions.</li>
</ul>

<h3>Title: OAT-Rephrase: Optimization-Aware Training Data Rephrasing for Zeroth-Order LLM Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Jikai Long, Zijian Hu, Xiaodong Yu, Jianwen Xie, Zhaozhuo Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17264">https://arxiv.org/abs/2506.17264</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17264">https://arxiv.org/pdf/2506.17264</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17264]] OAT-Rephrase: Optimization-Aware Training Data Rephrasing for Zeroth-Order LLM Fine-Tuning(https://arxiv.org/abs/2506.17264)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning large language models (LLMs) using zeroth-order optimization (ZO) offers a memory-efficient alternative to gradient-based methods but suffers from slower convergence and unstable optimization due to noisy gradient estimates. This paper introduces OAT-Rephrase, an Optimization-Aware Training data rephrasing strategy that leverages an LLM to rephrase training instances based on its understanding of the ZO dynamics, specifically MeZO, derived directly from its paper. The approach incorporates a dual-stage pipeline featuring a rewriter LLM and a semantic judge, ensuring all rephrasings retain task relevance and logical consistency. Evaluations across five classification tasks and three LLM architectures demonstrate that OAT-Rephrase consistently improves MeZO fine-tuning performance, often narrowing or eliminating the gap with first-order methods. Our findings suggest that optimization-aware rephrasing serves as a reusable and low-overhead enhancement for zeroth-order tuning regimes.</li>
</ul>

<h3>Title: Does Multimodal Large Language Model Truly Unlearn? Stealthy MLLM Unlearning Attack</h3>
<ul>
<li><strong>Authors: </strong>Xianren Zhang, Hui Liu, Delvin Ce Zhang, Xianfeng Tang, Qi He, Dongwon Lee, Suhang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17265">https://arxiv.org/abs/2506.17265</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17265">https://arxiv.org/pdf/2506.17265</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17265]] Does Multimodal Large Language Model Truly Unlearn? Stealthy MLLM Unlearning Attack(https://arxiv.org/abs/2506.17265)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, attack, steal, large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) trained on massive data may memorize sensitive personal information and photos, posing serious privacy risks. To mitigate this, MLLM unlearning methods are proposed, which fine-tune MLLMs to reduce the ``forget'' sensitive information. However, it remains unclear whether the knowledge has been truly forgotten or just hidden in the model. Therefore, we propose to study a novel problem of LLM unlearning attack, which aims to recover the unlearned knowledge of an unlearned LLM. To achieve the goal, we propose a novel framework Stealthy Unlearning Attack (SUA) framework that learns a universal noise pattern. When applied to input images, this noise can trigger the model to reveal unlearned content. While pixel-level perturbations may be visually subtle, they can be detected in the semantic embedding space, making such attacks vulnerable to potential defenses. To improve stealthiness, we introduce an embedding alignment loss that minimizes the difference between the perturbed and denoised image embeddings, ensuring the attack is semantically unnoticeable. Experimental results show that SUA can effectively recover unlearned information from MLLMs. Furthermore, the learned noise generalizes well: a single perturbation trained on a subset of samples can reveal forgotten content in unseen images. This indicates that knowledge reappearance is not an occasional failure, but a consistent behavior.</li>
</ul>

<h3>Title: Securing Generative AI Agentic Workflows: Risks, Mitigation, and a Proposed Firewall Architecture</h3>
<ul>
<li><strong>Authors: </strong>Sunil Kumar Jang Bahadur, Gopala Dhar</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17266">https://arxiv.org/abs/2506.17266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17266">https://arxiv.org/pdf/2506.17266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17266]] Securing Generative AI Agentic Workflows: Risks, Mitigation, and a Proposed Firewall Architecture(https://arxiv.org/abs/2506.17266)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect, defense, generative</a></li>
<li><strong>Abstract: </strong>Generative Artificial Intelligence (GenAI) presents significant advancements but also introduces novel security challenges, particularly within agentic workflows where AI agents operate autonomously. These risks escalate in multi-agent systems due to increased interaction complexity. This paper outlines critical security vulnerabilities inherent in GenAI agentic workflows, including data privacy breaches, model manipulation, and issues related to agent autonomy and system integration. It discusses key mitigation strategies such as data encryption, access control, prompt engineering, model monitoring, agent sandboxing, and security audits. Furthermore, it details a proposed "GenAI Security Firewall" architecture designed to provide comprehensive, adaptable, and efficient protection for these systems by integrating various security services and leveraging GenAI itself for enhanced defense. Addressing these security concerns is paramount for the responsible and safe deployment of this transformative technology.</li>
</ul>

<h3>Title: CF-VLM:CounterFactual Vision-Language Fine-tuning</h3>
<ul>
<li><strong>Authors: </strong>Jusheng Zhang, Kaitong Cai, Yijia Fan, Jian Wang, Keze Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17267">https://arxiv.org/abs/2506.17267</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17267">https://arxiv.org/pdf/2506.17267</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17267]] CF-VLM:CounterFactual Vision-Language Fine-tuning(https://arxiv.org/abs/2506.17267)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Recent advances in vision-language models (VLMs) have greatly improved cross-modal semantic understanding, yet significant limitations remain in fine-grained discrimination and deep causal reasoning tasks. Existing VLMs often rely on superficial statistical correlations, lacking the ability to capture the underlying causal logic between visual and textual content. To address this, we propose CounterFactual Vision-Language Fine-tuning (CF-VLM), a novel framework that enhances the causal reasoning capabilities of VLMs through the targeted use of counterfactual samples. CF-VLM introduces three complementary training objectives: maintaining foundational cross-modal alignment, reinforcing the uniqueness and stability of factual scene representations against coherent counterfactuals, and sharpening the model's sensitivity to minimal but critical causal edits. Extensive experiments demonstrate that CF-VLM consistently outperforms strong baselines and state-of-the-art methods on compositional reasoning and generalization benchmarks. Furthermore, it shows promise in mitigating visual hallucinations, indicating improved factual consistency. Our CF-VLM provides a robust foundation for deploying VLMs in high-stakes, real-world scenarios requiring reliable reasoning and interpretability.</li>
</ul>

<h3>Title: Digital Privacy Everywhere</h3>
<ul>
<li><strong>Authors: </strong>Paritosh Ranjan, Surajit Majumder, Prodip Roy</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17269">https://arxiv.org/abs/2506.17269</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17269">https://arxiv.org/pdf/2506.17269</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17269]] Digital Privacy Everywhere(https://arxiv.org/abs/2506.17269)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>The increasing proliferation of digital and mobile devices equipped with cameras, microphones, GPS, and other privacy invasive components has raised significant concerns for businesses operating in sensitive or policy restricted environments. Current solutions rely on passive enforcement, such as signage or verbal instructions, which are largely ineffective. This paper presents Digital Privacy Everywhere (DPE), a comprehensive and scalable system designed to actively enforce custom privacy policies for digital devices within predefined physical boundaries. The DPE architecture includes a centralized management console, field verification units (FVUs), enforcement modules for mobile devices (EMMDs), and an External Geo Ownership Service (EGOS). These components collaboratively detect, configure, and enforce privacy settings such as disabling cameras, microphones, or radios across various premises like theaters, hospitals, financial institutions, and educational facilities. The system ensures privacy compliance in real time while maintaining a seamless user experience and operational scalability across geographies.</li>
</ul>

<h3>Title: Step-by-Step Reasoning Attack: Revealing 'Erased' Knowledge in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yash Sinha, Manit Baser, Murari Mandal, Dinil Mon Divakaran, Mohan Kankanhalli</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17279">https://arxiv.org/abs/2506.17279</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17279">https://arxiv.org/pdf/2506.17279</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17279]] Step-by-Step Reasoning Attack: Revealing 'Erased' Knowledge in Large Language Models(https://arxiv.org/abs/2506.17279)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, robust, fair, large language model</a></li>
<li><strong>Abstract: </strong>Knowledge erasure in large language models (LLMs) is important for ensuring compliance with data and AI regulations, safeguarding user privacy, mitigating bias, and misinformation. Existing unlearning methods aim to make the process of knowledge erasure more efficient and effective by removing specific knowledge while preserving overall model performance, especially for retained information. However, it has been observed that the unlearning techniques tend to suppress and leave the knowledge beneath the surface, thus making it retrievable with the right prompts. In this work, we demonstrate that \textit{step-by-step reasoning} can serve as a backdoor to recover this hidden information. We introduce a step-by-step reasoning-based black-box attack, Sleek, that systematically exposes unlearning failures. We employ a structured attack framework with three core components: (1) an adversarial prompt generation strategy leveraging step-by-step reasoning built from LLM-generated queries, (2) an attack mechanism that successfully recalls erased content, and exposes unfair suppression of knowledge intended for retention and (3) a categorization of prompts as direct, indirect, and implied, to identify which query types most effectively exploit unlearning weaknesses. Through extensive evaluations on four state-of-the-art unlearning techniques and two widely used LLMs, we show that existing approaches fail to ensure reliable knowledge removal. Of the generated adversarial prompts, 62.5% successfully retrieved forgotten Harry Potter facts from WHP-unlearned Llama, while 50% exposed unfair suppression of retained knowledge. Our work highlights the persistent risks of information leakage, emphasizing the need for more robust unlearning strategies for erasure.</li>
</ul>

<h3>Title: GTA: Grouped-head latenT Attention</h3>
<ul>
<li><strong>Authors: </strong>Luoyang Sun, Jiwen Jiang, Cheng Deng, Xinjian Wu, Haifeng Zhang, Lei Chen, Lionel Ni, Jun Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17286">https://arxiv.org/abs/2506.17286</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17286">https://arxiv.org/pdf/2506.17286</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17286]] GTA: Grouped-head latenT Attention(https://arxiv.org/abs/2506.17286)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Attention mechanisms underpin the success of large language models (LLMs), yet their substantial computational and memory overhead poses challenges for optimizing efficiency and performance. A critical bottleneck arises as KV cache and attention computations scale rapidly with text length, challenging deployment on hardware with limited computational and memory resources. We observe that attention mechanisms exhibit substantial redundancy, since the KV cache can be significantly compressed and attention maps across heads display high similarity, revealing that much of the computation and storage is unnecessary. Leveraging these insights, we propose \textbf{G}rouped-Head Laten\textbf{T} \textbf{A}ttention (GTA), a novel attention mechanism that reduces memory usage and computational complexity while maintaining performance. GTA comprises two components: (1) a shared attention map mechanism that reuses attention scores across multiple heads, decreasing the key cache size; and (2) a nonlinear value decoder with learned projections that compresses the value cache into a latent space, further cutting memory needs. GTA cuts attention computation FLOPs by up to \emph{62.5\%} versus Grouped-Query Attention and shrink the KV cache by up to \emph{70\%}, all while avoiding the extra overhead of Multi-Head Latent Attention to improve LLM deployment efficiency. Consequently, GTA models achieve a \emph{2x} increase in end-to-end inference speed, with prefill benefiting from reduced computational cost and decoding benefiting from the smaller cache footprint.</li>
</ul>

<h3>Title: SRKD: Towards Efficient 3D Point Cloud Segmentation via Structure- and Relation-aware Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Yuqi Li, Junhao Dong, Zeyu Dong, Chuanguang Yang, Zhulin An, Yongjun Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17290">https://arxiv.org/abs/2506.17290</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17290">https://arxiv.org/pdf/2506.17290</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17290]] SRKD: Towards Efficient 3D Point Cloud Segmentation via Structure- and Relation-aware Knowledge Distillation(https://arxiv.org/abs/2506.17290)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>3D point cloud segmentation faces practical challenges due to the computational complexity and deployment limitations of large-scale transformer-based models. To address this, we propose a novel Structure- and Relation-aware Knowledge Distillation framework, named SRKD, that transfers rich geometric and semantic knowledge from a large frozen teacher model (>100M) to a lightweight student model (<15M). Specifically, we propose an affinity matrix-based relation alignment module, which distills structural dependencies from the teacher to the student through point-wise similarity matching, enhancing the student's capability to learn contextual interactions. Meanwhile, we introduce a cross-sample mini-batch construction strategy that enables the student to perceive stable and generalized geometric structure. This aligns across diverse point cloud instances of the teacher, rather than within a single sample. Additionally, KL divergence is applied to align semantic distributions, and ground-truth supervision further reinforces accurate segmentation. Our method achieves state of the art performance with significantly reduced model complexity, demonstrating its effectiveness and efficiency in real-world deployment scenarios. Our Code is available at this https URL.</li>
</ul>

<h3>Title: Theoretically Unmasking Inference Attacks Against LDP-Protected Clients in Federated Vision Models</h3>
<ul>
<li><strong>Authors: </strong>Quan Nguyen, Minh N. Vu, Truc Nguyen, My T. Thai</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17292">https://arxiv.org/abs/2506.17292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17292">https://arxiv.org/pdf/2506.17292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17292]] Theoretically Unmasking Inference Attacks Against LDP-Protected Clients in Federated Vision Models(https://arxiv.org/abs/2506.17292)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, attack, membership infer, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning enables collaborative learning among clients via a coordinating server while avoiding direct data sharing, offering a perceived solution to preserve privacy. However, recent studies on Membership Inference Attacks (MIAs) have challenged this notion, showing high success rates against unprotected training data. While local differential privacy (LDP) is widely regarded as a gold standard for privacy protection in data analysis, most studies on MIAs either neglect LDP or fail to provide theoretical guarantees for attack success rates against LDP-protected data. To address this gap, we derive theoretical lower bounds for the success rates of low-polynomial time MIAs that exploit vulnerabilities in fully connected or self-attention layers. We establish that even when data are protected by LDP, privacy risks persist, depending on the privacy budget. Practical evaluations on federated vision models confirm considerable privacy risks, revealing that the noise required to mitigate these attacks significantly degrades models' utility.</li>
</ul>

<h3>Title: Semantic uncertainty in advanced decoding methods for LLM generation</h3>
<ul>
<li><strong>Authors: </strong>Darius Foodeei, Simin Fan, Martin Jaggi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17296">https://arxiv.org/abs/2506.17296</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17296">https://arxiv.org/pdf/2506.17296</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17296]] Semantic uncertainty in advanced decoding methods for LLM generation(https://arxiv.org/abs/2506.17296)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This study investigates semantic uncertainty in large language model (LLM) outputs across different decoding methods, focusing on emerging techniques like speculative sampling and chain-of-thought (CoT) decoding. Through experiments on question answering, summarization, and code generation tasks, we analyze how different decoding strategies affect both the diversity and reliability of model outputs. Our findings reveal that while CoT decoding demonstrates higher semantic diversity, it maintains lower predictive entropy, suggesting that structured exploration can lead to more confident and accurate outputs. This is evidenced by a 48.8% improvement in code generation Pass@2 rates, despite lower alignment with reference solutions. For summarization tasks, speculative sampling proved particularly effective, achieving superior ROUGE scores while maintaining moderate semantic diversity. Our results challenge conventional assumptions about trade-offs between diversity and accuracy in language model outputs, demonstrating that properly structured decoding methods can increase semantic exploration while maintaining or improving output quality. These findings have significant implications for deploying language models in practical applications where both reliability and diverse solution generation are crucial.</li>
</ul>

<h3>Title: Mercury: Ultra-Fast Language Models Based on Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Inception Labs, Samar Khanna, Siddhant Kharbanda, Shufan Li, Harshit Varma, Eric Wang, Sawyer Birnbaum, Ziyang Luo, Yanis Miraoui, Akash Palrecha, Stefano Ermon, Aditya Grover, Volodymyr Kuleshov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17298">https://arxiv.org/abs/2506.17298</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17298">https://arxiv.org/pdf/2506.17298</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17298]] Mercury: Ultra-Fast Language Models Based on Diffusion(https://arxiv.org/abs/2506.17298)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, large language model</a></li>
<li><strong>Abstract: </strong>We present Mercury, a new generation of commercial-scale large language models (LLMs) based on diffusion. These models are parameterized via the Transformer architecture and trained to predict multiple tokens in parallel. In this report, we detail Mercury Coder, our first set of diffusion LLMs designed for coding applications. Currently, Mercury Coder comes in two sizes: Mini and Small. These models set a new state-of-the-art on the speed-quality frontier. Based on independent evaluations conducted by Artificial Analysis, Mercury Coder Mini and Mercury Coder Small achieve state-of-the-art throughputs of 1109 tokens/sec and 737 tokens/sec, respectively, on NVIDIA H100 GPUs and outperform speed-optimized frontier models by up to 10x on average while maintaining comparable quality. We discuss additional results on a variety of code benchmarks spanning multiple languages and use-cases as well as real-world validation by developers on Copilot Arena, where the model currently ranks second on quality and is the fastest model overall. We also release a public API at this https URL and free playground at this https URL</li>
</ul>

<h3>Title: LLM Jailbreak Oracle</h3>
<ul>
<li><strong>Authors: </strong>Shuyi Lin, Anshuman Suri, Alina Oprea, Cheng Tan</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17299">https://arxiv.org/abs/2506.17299</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17299">https://arxiv.org/pdf/2506.17299</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17299]] LLM Jailbreak Oracle(https://arxiv.org/abs/2506.17299)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) become increasingly deployed in safety-critical applications, the lack of systematic methods to assess their vulnerability to jailbreak attacks presents a critical security gap. We introduce the jailbreak oracle problem: given a model, prompt, and decoding strategy, determine whether a jailbreak response can be generated with likelihood exceeding a specified threshold. This formalization enables a principled study of jailbreak vulnerabilities. Answering the jailbreak oracle problem poses significant computational challenges -- the search space grows exponentially with the length of the response tokens. We present Boa, the first efficient algorithm for solving the jailbreak oracle problem. Boa employs a three-phase search strategy: (1) constructing block lists to identify refusal patterns, (2) breadth-first sampling to identify easily accessible jailbreaks, and (3) depth-first priority search guided by fine-grained safety scores to systematically explore promising low-probability paths. Boa enables rigorous security assessments including systematic defense evaluation, standardized comparison of red team attacks, and model certification under extreme adversarial conditions.</li>
</ul>

<h3>Title: Fine-Scale Soil Mapping in Alaska with Multimodal Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Yijun Lin, Theresa Chen, Colby Brungard, Grunwald Sabine, Sue Ives, Matt Macander, Timm Nawrocki, Yao-Yi Chiang, Nic Jelinski</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17302">https://arxiv.org/abs/2506.17302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17302">https://arxiv.org/pdf/2506.17302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17302]] Fine-Scale Soil Mapping in Alaska with Multimodal Machine Learning(https://arxiv.org/abs/2506.17302)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Fine-scale soil mapping in Alaska, traditionally relying on fieldwork and localized simulations, remains a critical yet underdeveloped task, despite the region's ecological importance and extensive permafrost coverage. As permafrost thaw accelerates due to climate change, it threatens infrastructure stability and key ecosystem services, such as soil carbon storage. High-resolution soil maps are essential for characterizing permafrost distribution, identifying vulnerable areas, and informing adaptation strategies. We present MISO, a vision-based machine learning (ML) model to produce statewide fine-scale soil maps for near-surface permafrost and soil taxonomy. The model integrates a geospatial foundation model for visual feature extraction, implicit neural representations for continuous spatial prediction, and contrastive learning for multimodal alignment and geo-location awareness. We compare MISO with Random Forest (RF), a traditional ML model that has been widely used in soil mapping applications. Spatial cross-validation and regional analysis across Permafrost Zones and Major Land Resource Areas (MLRAs) show that MISO generalizes better to remote, unseen locations and achieves higher recall than RF, which is critical for monitoring permafrost thaw and related environmental processes. These findings demonstrate the potential of advanced ML approaches for fine-scale soil mapping and provide practical guidance for future soil sampling and infrastructure planning in permafrost-affected landscapes. The project will be released at this https URL.</li>
</ul>

<h3>Title: AlgoSelect: Universal Algorithm Selection via the Comb Operator</h3>
<ul>
<li><strong>Authors: </strong>Jasper Yao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17304">https://arxiv.org/abs/2506.17304</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17304">https://arxiv.org/pdf/2506.17304</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17304]] AlgoSelect: Universal Algorithm Selection via the Comb Operator(https://arxiv.org/abs/2506.17304)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We introduce AlgoSelect, a principled framework for learning optimal algorithm selection from data, centered around the novel Comb Operator. Given a set of algorithms and a feature representation of problems, AlgoSelect learns to interpolate between diverse computational approaches. For pairs of algorithms, a simple sigmoid-gated selector, an instance of the Comb Operator, facilitates this interpolation. We extend this to an N-Path Comb for multiple algorithms. We prove that this framework is universal (can approximate any algorithm selector), information-theoretically optimal in its learnability (thresholds for selection converge almost surely, demonstrated via Borel-Cantelli arguments), computationally efficient, and robust. Key theoretical contributions include: (1) a universal approximation theorem demonstrating that Comb-based selectors can achieve arbitrary accuracy; (2) information-theoretic learnability for selection thresholds; (3) formalization of the Comb Operator within linear operator theory, detailing its boundedness and spectral properties; (4) an N-Path Comb generalization for multi-algorithm selection; and (5) a practical learning framework for the adaptive seeding functions that guide the Comb Operator. Empirical validation on a comprehensive 20$\times$20 problem-algorithm study demonstrates near-perfect selection (99.9\%+ accuracy) with remarkably few samples and rapid convergence, revealing that $H(\text{Algorithm}|\text{Problem}) \approx 0$ in structured domains. AlgoSelect provides a theoretically grounded, practically deployable solution to automated algorithm selection with provable optimality and learnability guarantees, with significant implications for AI and adaptive systems.</li>
</ul>

<h3>Title: Learning to Adapt Frozen CLIP for Few-Shot Test-Time Domain Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Zhixiang Chi, Li Gu, Huan Liu, Ziqiang Wang, Yanan Wu, Yang Wang, Konstantinos N Plataniotis</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17307">https://arxiv.org/abs/2506.17307</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17307">https://arxiv.org/pdf/2506.17307</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17307]] Learning to Adapt Frozen CLIP for Few-Shot Test-Time Domain Adaptation(https://arxiv.org/abs/2506.17307)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Few-shot Test-Time Domain Adaptation focuses on adapting a model at test time to a specific domain using only a few unlabeled examples, addressing domain shift. Prior methods leverage CLIP's strong out-of-distribution (OOD) abilities by generating domain-specific prompts to guide its generalized, frozen features. However, since downstream datasets are not explicitly seen by CLIP, solely depending on the feature space knowledge is constrained by CLIP's prior knowledge. Notably, when using a less robust backbone like ViT-B/16, performance significantly drops on challenging real-world benchmarks. Departing from the state-of-the-art of inheriting the intrinsic OOD capability of CLIP, this work introduces learning directly on the input space to complement the dataset-specific knowledge for frozen CLIP. Specifically, an independent side branch is attached in parallel with CLIP and enforced to learn exclusive knowledge via revert attention. To better capture the dataset-specific label semantics for downstream adaptation, we propose to enhance the inter-dispersion among text features via greedy text ensemble and refinement. The text and visual features are then progressively fused in a domain-aware manner by a generated domain prompt to adapt toward a specific domain. Extensive experiments show our method's superiority on 5 large-scale benchmarks (WILDS and DomainNet), notably improving over smaller networks like ViT-B/16 with gains of \textbf{+5.1} in F1 for iWildCam and \textbf{+3.1\%} in WC Acc for FMoW.</li>
</ul>

<h3>Title: A Nested Watermark for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Koichi Nagatsuka, Terufumi Morishita, Yasuhiro Sogawa</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17308">https://arxiv.org/abs/2506.17308</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17308">https://arxiv.org/pdf/2506.17308</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17308]] A Nested Watermark for Large Language Models(https://arxiv.org/abs/2506.17308)</code><input type="text"></li>
<li><strong>Keywords: </strong>watermark, large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancement of large language models (LLMs) has raised concerns regarding their potential misuse, particularly in generating fake news and misinformation. To address these risks, watermarking techniques for autoregressive language models have emerged as a promising means for detecting LLM-generated text. Existing methods typically embed a watermark by increasing the probabilities of tokens within a group selected according to a single secret key. However, this approach suffers from a critical limitation: if the key is leaked, it becomes impossible to trace the text's provenance or attribute authorship. To overcome this vulnerability, we propose a novel nested watermarking scheme that embeds two distinct watermarks into the generated text using two independent keys. This design enables reliable authorship identification even in the event that one key is compromised. Experimental results demonstrate that our method achieves high detection accuracy for both watermarks while maintaining the fluency and overall quality of the generated text.</li>
</ul>

<h3>Title: Efficient Malware Detection with Optimized Learning on High-Dimensional Features</h3>
<ul>
<li><strong>Authors: </strong>Aditya Choudhary, Sarthak Pawar, Yashodhara Haribhakta</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17309">https://arxiv.org/abs/2506.17309</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17309">https://arxiv.org/pdf/2506.17309</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17309]] Efficient Malware Detection with Optimized Learning on High-Dimensional Features(https://arxiv.org/abs/2506.17309)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Malware detection using machine learning requires feature extraction from binary files, as models cannot process raw binaries directly. A common approach involves using LIEF for raw feature extraction and the EMBER vectorizer to generate 2381-dimensional feature vectors. However, the high dimensionality of these features introduces significant computational challenges. This study addresses these challenges by applying two dimensionality reduction techniques: XGBoost-based feature selection and Principal Component Analysis (PCA). We evaluate three reduced feature dimensions (128, 256, and 384), which correspond to approximately 5.4%, 10.8%, and 16.1% of the original 2381 features, across four models-XGBoost, LightGBM, Extra Trees, and Random Forest-using a unified training, validation, and testing split formed from the EMBER-2018, ERMDS, and BODMAS datasets. This approach ensures generalization and avoids dataset bias. Experimental results show that LightGBM trained on the 384-dimensional feature set after XGBoost feature selection achieves the highest accuracy of 97.52% on the unified dataset, providing an optimal balance between computational efficiency and detection performance. The best model, trained in 61 minutes using 30 GB of RAM and 19.5 GB of disk space, generalizes effectively to completely unseen datasets, maintaining 95.31% accuracy on TRITIUM and 93.98% accuracy on INFERNO. These findings present a scalable, compute-efficient approach for malware detection without compromising accuracy.</li>
</ul>

<h3>Title: PRAISE: Enhancing Product Descriptions with LLM-Driven Structured Insights</h3>
<ul>
<li><strong>Authors: </strong>Adnan Qidwai, Srija Mukhopadhyay, Prerana Khatiwada, Dan Roth, Vivek Gupta</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17314">https://arxiv.org/abs/2506.17314</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17314">https://arxiv.org/pdf/2506.17314</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17314]] PRAISE: Enhancing Product Descriptions with LLM-Driven Structured Insights(https://arxiv.org/abs/2506.17314)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Accurate and complete product descriptions are crucial for e-commerce, yet seller-provided information often falls short. Customer reviews offer valuable details but are laborious to sift through manually. We present PRAISE: Product Review Attribute Insight Structuring Engine, a novel system that uses Large Language Models (LLMs) to automatically extract, compare, and structure insights from customer reviews and seller descriptions. PRAISE provides users with an intuitive interface to identify missing, contradictory, or partially matching details between these two sources, presenting the discrepancies in a clear, structured format alongside supporting evidence from reviews. This allows sellers to easily enhance their product listings for clarity and persuasiveness, and buyers to better assess product reliability. Our demonstration showcases PRAISE's workflow, its effectiveness in generating actionable structured insights from unstructured reviews, and its potential to significantly improve the quality and trustworthiness of e-commerce product catalogs.</li>
</ul>

<h3>Title: Tracking GPTs Third Party Service: Automation, Analysis, and Insights</h3>
<ul>
<li><strong>Authors: </strong>Chuan Yan, Liuhuo Wan, Bowei Guan, Fengqi Yu, Guangdong Bai, Jin Song Dong</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17315">https://arxiv.org/abs/2506.17315</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17315">https://arxiv.org/pdf/2506.17315</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17315]] Tracking GPTs Third Party Service: Automation, Analysis, and Insights(https://arxiv.org/abs/2506.17315)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>ChatGPT has quickly advanced from simple natural language processing to tackling more sophisticated and specialized tasks. Drawing inspiration from the success of mobile app ecosystems, OpenAI allows developers to create applications that interact with third-party services, known as GPTs. GPTs can choose to leverage third-party services to integrate with specialized APIs for domain-specific applications. However, the way these disclose privacy setting information limits accessibility and analysis, making it challenging to systematically evaluate the data privacy implications of third-party integrate to GPTs. In order to support academic research on the integration of third-party services in GPTs, we introduce GPTs-ThirdSpy, an automated framework designed to extract privacy settings of GPTs. GPTs-ThirdSpy provides academic researchers with real-time, reliable metadata on third-party services used by GPTs, enabling in-depth analysis of their integration, compliance, and potential security risks. By systematically collecting and structuring this data, GPTs-ThirdSpy facilitates large-scale research on the transparency and regulatory challenges associated with the GPT app ecosystem.</li>
</ul>

<h3>Title: Beyond the Scope: Security Testing of Permission Management in Team Workspace</h3>
<ul>
<li><strong>Authors: </strong>Liuhuo Wan, Chuan Yan, Mark Huasong Meng, Kailong Wang, Haoyu Wang, Guangdong Bai, Jin Song Dong</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17317">https://arxiv.org/abs/2506.17317</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17317">https://arxiv.org/pdf/2506.17317</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17317]] Beyond the Scope: Security Testing of Permission Management in Team Workspace(https://arxiv.org/abs/2506.17317)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect</a></li>
<li><strong>Abstract: </strong>Nowadays team workspaces are widely adopted for multi-user collaboration and digital resource management. To further broaden real-world applications, mainstream team workspaces platforms, such as Google Workspace and Microsoft OneDrive, allow third-party applications (referred to as add-ons) to be integrated into their workspaces, significantly extending the functionality of team workspaces. The powerful multi-user collaboration capabilities and integration of add-ons make team workspaces a central hub for managing shared resources and protecting them against unauthorized access. Due to the collaboration features of team workspaces, add-ons involved in collaborations may bypass the permission isolation enforced by the administrator, unlike in single-user permission management. This paper aims to investigate the permission management landscape of team workspaces add-ons. To this end, we perform an in-depth analysis of the enforced access control mechanism inherent in this ecosystem, considering both multi-user and cross-app features. We identify three potential security risks that can be exploited to cause permission escalation. We then systematically reveal the landscape of permission escalation risks in the current ecosystem. Specifically, we propose an automated tool, TAI, to systematically test all possible interactions within this ecosystem. Our evaluation reveals that permission escalation vulnerabilities are widespread in this ecosystem, with 41 interactions identified as problematic. Our findings should raise an alert to both the team workspaces platforms and third-party developers.</li>
</ul>

<h3>Title: Context manipulation attacks : Web agents are susceptible to corrupted memory</h3>
<ul>
<li><strong>Authors: </strong>Atharv Singh Patlan, Ashwin Hebbar, Pramod Viswanath, Prateek Mittal</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17318">https://arxiv.org/abs/2506.17318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17318">https://arxiv.org/pdf/2506.17318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17318]] Context manipulation attacks : Web agents are susceptible to corrupted memory(https://arxiv.org/abs/2506.17318)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Autonomous web navigation agents, which translate natural language instructions into sequences of browser actions, are increasingly deployed for complex tasks across e-commerce, information retrieval, and content discovery. Due to the stateless nature of large language models (LLMs), these agents rely heavily on external memory systems to maintain context across interactions. Unlike centralized systems where context is securely stored server-side, agent memory is often managed client-side or by third-party applications, creating significant security vulnerabilities. This was recently exploited to attack production systems. We introduce and formalize "plan injection," a novel context manipulation attack that corrupts these agents' internal task representations by targeting this vulnerable context. Through systematic evaluation of two popular web agents, Browser-use and Agent-E, we show that plan injections bypass robust prompt injection defenses, achieving up to 3x higher attack success rates than comparable prompt-based attacks. Furthermore, "context-chained injections," which craft logical bridges between legitimate user goals and attacker objectives, lead to a 17.7% increase in success rate for privacy exfiltration tasks. Our findings highlight that secure memory handling must be a first-class concern in agentic systems.</li>
</ul>

<h3>Title: I Know Which LLM Wrote Your Code Last Summer: LLM generated Code Stylometry for Authorship Attribution</h3>
<ul>
<li><strong>Authors: </strong>Tamas Bisztray, Bilel Cherif, Richard A. Dubniczky, Nils Gruschka, Bertalan Borsos, Mohamed Amine Ferrag, Attila Kovacs, Vasileios Mavroeidis, Norbert Tihanyi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17323">https://arxiv.org/abs/2506.17323</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17323">https://arxiv.org/pdf/2506.17323</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17323]] I Know Which LLM Wrote Your Code Last Summer: LLM generated Code Stylometry for Authorship Attribution(https://arxiv.org/abs/2506.17323)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Detecting AI-generated code, deepfakes, and other synthetic content is an emerging research challenge. As code generated by Large Language Models (LLMs) becomes more common, identifying the specific model behind each sample is increasingly important. This paper presents the first systematic study of LLM authorship attribution for C programs. We released CodeT5-Authorship, a novel model that uses only the encoder layers from the original CodeT5 encoder-decoder architecture, discarding the decoder to focus on classification. Our model's encoder output (first token) is passed through a two-layer classification head with GELU activation and dropout, producing a probability distribution over possible authors. To evaluate our approach, we introduce LLM-AuthorBench, a benchmark of 32,000 compilable C programs generated by eight state-of-the-art LLMs across diverse tasks. We compare our model to seven traditional ML classifiers and eight fine-tuned transformer models, including BERT, RoBERTa, CodeBERT, ModernBERT, DistilBERT, DeBERTa-V3, Longformer, and LoRA-fine-tuned Qwen2-1.5B. In binary classification, our model achieves 97.56% accuracy in distinguishing C programs generated by closely related models such as GPT-4.1 and GPT-4o, and 95.40% accuracy for multi-class attribution among five leading LLMs (Gemini 2.5 Flash, Claude 3.5 Haiku, GPT-4.1, Llama 3.3, and DeepSeek-V3). To support open science, we release the CodeT5-Authorship architecture, the LLM-AuthorBench benchmark, and all relevant Google Colab scripts on GitHub: this https URL.</li>
</ul>

<h3>Title: Origins of Creativity in Attention-Based Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Emma Finn, T. Anderson Keller, Manos Theodosis, Demba E. Ba</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17324">https://arxiv.org/abs/2506.17324</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17324">https://arxiv.org/pdf/2506.17324</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17324]] Origins of Creativity in Attention-Based Diffusion Models(https://arxiv.org/abs/2506.17324)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>As diffusion models have become the tool of choice for image generation and as the quality of the images continues to improve, the question of how `creativity' originates in diffusion has become increasingly important. The score matching perspective on diffusion has proven particularly fruitful for understanding how and why diffusion models generate images that remain plausible while differing significantly from their training images. In particular, as explained in (Kamb \& Ganguli, 2024) and others, e.g., (Ambrogioni, 2023), theory suggests that if our score matching were optimal, we would only be able to recover training samples through our diffusion process. However, as shown by Kamb \& Ganguli, (2024), in diffusion models where the score is parametrized by a simple CNN, the inductive biases of the CNN itself (translation equivariance and locality) allow the model to generate samples that globally do not match any training samples, but are rather patch-wise `mosaics'. Notably, however, this theory does not extend to describe the role of self-attention in this process. In this work, we take a preliminary step in this direction to extend this theory to the case of diffusion models whose score is parametrized by a CNN with a final self-attention layer. We show that our theory suggests that self-attention will induce a globally image-consistent arrangement of local features beyond the patch-level in generated samples, and we verify this behavior empirically on a carefully crafted dataset.</li>
</ul>

<h3>Title: RadarSeq: A Temporal Vision Framework for User Churn Prediction via Radar Chart Sequences</h3>
<ul>
<li><strong>Authors: </strong>Sina Najafi, M. Hadi Sepanj, Fahimeh Jafari</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17325">https://arxiv.org/abs/2506.17325</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17325">https://arxiv.org/pdf/2506.17325</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17325]] RadarSeq: A Temporal Vision Framework for User Churn Prediction via Radar Chart Sequences(https://arxiv.org/abs/2506.17325)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Predicting user churn in non-subscription gig platforms, where disengagement is implicit, poses unique challenges due to the absence of explicit labels and the dynamic nature of user behavior. Existing methods often rely on aggregated snapshots or static visual representations, which obscure temporal cues critical for early detection. In this work, we propose a temporally-aware computer vision framework that models user behavioral patterns as a sequence of radar chart images, each encoding day-level behavioral features. By integrating a pretrained CNN encoder with a bidirectional LSTM, our architecture captures both spatial and temporal patterns underlying churn behavior. Extensive experiments on a large real-world dataset demonstrate that our method outperforms classical models and ViT-based radar chart baselines, yielding gains of 17.7 in F1 score, 29.4 in precision, and 16.1 in AUC, along with improved interpretability. The framework's modular design, explainability tools, and efficient deployment characteristics make it suitable for large-scale churn modeling in dynamic gig-economy platforms.</li>
</ul>

<h3>Title: On the Performance of Cyber-Biomedical Features for Intrusion Detection in Healthcare 5.0</h3>
<ul>
<li><strong>Authors: </strong>Pedro H. Lui, Lucas P. Siqueira, Juliano F. Kazienko, Vagner E. Quincozes, Silvio E. Quincozes, Daniel Welfer</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17329">https://arxiv.org/abs/2506.17329</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17329">https://arxiv.org/pdf/2506.17329</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17329]] On the Performance of Cyber-Biomedical Features for Intrusion Detection in Healthcare 5.0(https://arxiv.org/abs/2506.17329)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Healthcare 5.0 integrates Artificial Intelligence (AI), the Internet of Things (IoT), real-time monitoring, and human-centered design toward personalized medicine and predictive diagnostics. However, the increasing reliance on interconnected medical technologies exposes them to cyber threats. Meanwhile, current AI-driven cybersecurity models often neglect biomedical data, limiting their effectiveness and interpretability. This study addresses this gap by applying eXplainable AI (XAI) to a Healthcare 5.0 dataset that integrates network traffic and biomedical sensor data. Classification outputs indicate that XGBoost achieved 99% F1-score for benign and data alteration, and 81% for spoofing. Explainability findings reveal that network data play a dominant role in intrusion detection whereas biomedical features contributed to spoofing detection, with temperature reaching a Shapley values magnitude of 0.37.</li>
</ul>

<h3>Title: P2MFDS: A Privacy-Preserving Multimodal Fall Detection System for Elderly People in Bathroom Environments</h3>
<ul>
<li><strong>Authors: </strong>Haitian Wang, Yiren Wang, Xinyu Wang, Yumeng Miao, Yuliang Zhang, Yu Zhang, Atif Mansoor</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17332">https://arxiv.org/abs/2506.17332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17332">https://arxiv.org/pdf/2506.17332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17332]] P2MFDS: A Privacy-Preserving Multimodal Fall Detection System for Elderly People in Bathroom Environments(https://arxiv.org/abs/2506.17332)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>By 2050, people aged 65 and over are projected to make up 16 percent of the global population. As aging is closely associated with increased fall risk, particularly in wet and confined environments such as bathrooms where over 80 percent of falls occur. Although recent research has increasingly focused on non-intrusive, privacy-preserving approaches that do not rely on wearable devices or video-based monitoring, these efforts have not fully overcome the limitations of existing unimodal systems (e.g., WiFi-, infrared-, or mmWave-based), which are prone to reduced accuracy in complex environments. These limitations stem from fundamental constraints in unimodal sensing, including system bias and environmental interference, such as multipath fading in WiFi-based systems and drastic temperature changes in infrared-based methods. To address these challenges, we propose a Privacy-Preserving Multimodal Fall Detection System for Elderly People in Bathroom Environments. First, we develop a sensor evaluation framework to select and fuse millimeter-wave radar with 3D vibration sensing, and use it to construct and preprocess a large-scale, privacy-preserving multimodal dataset in real bathroom settings, which will be released upon publication. Second, we introduce P2MFDS, a dual-stream network combining a CNN-BiLSTM-Attention branch for radar motion dynamics with a multi-scale CNN-SEBlock-Self-Attention branch for vibration impact detection. By uniting macro- and micro-scale features, P2MFDS delivers significant gains in accuracy and recall over state-of-the-art approaches. Code and pretrained models will be made available at: this https URL.</li>
</ul>

<h3>Title: AutomataGPT: Forecasting and Ruleset Inference for Two-Dimensional Cellular Automata</h3>
<ul>
<li><strong>Authors: </strong>Jaime A. Berkovich, Noah S. David, Markus J. Buehler</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.dis-nn, cond-mat.mtrl-sci, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17333">https://arxiv.org/abs/2506.17333</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17333">https://arxiv.org/pdf/2506.17333</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17333]] AutomataGPT: Forecasting and Ruleset Inference for Two-Dimensional Cellular Automata(https://arxiv.org/abs/2506.17333)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Cellular automata (CA) provide a minimal formalism for investigating how simple local interactions generate rich spatiotemporal behavior in domains as diverse as traffic flow, ecology, tissue morphogenesis and crystal growth. However, automatically discovering the local update rules for a given phenomenon and using them for quantitative prediction remains challenging. Here we present AutomataGPT, a decoder-only transformer pretrained on around 1 million simulated trajectories that span 100 distinct two-dimensional binary deterministic CA rules on toroidal grids. When evaluated on previously unseen rules drawn from the same CA family, AutomataGPT attains 98.5% perfect one-step forecasts and reconstructs the governing update rule with up to 96% functional (application) accuracy and 82% exact rule-matrix match. These results demonstrate that large-scale pretraining over wider regions of rule space yields substantial generalization in both the forward (state forecasting) and inverse (rule inference) problems, without hand-crafted priors. By showing that transformer models can faithfully infer and execute CA dynamics from data alone, our work lays the groundwork for abstracting real-world dynamical phenomena into data-efficient CA surrogates, opening avenues in biology, tissue engineering, physics and AI-driven scientific discovery.</li>
</ul>

<h3>Title: Privacy-Preserving LLM Interaction with Socratic Chain-of-Thought Reasoning and Homomorphically Encrypted Vector Databases</h3>
<ul>
<li><strong>Authors: </strong>Yubeen Bae, Minchan Kim, Jaejin Lee, Sangbum Kim, Jaehyung Kim, Yejin Choi, Niloofar Mireshghallah</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17336">https://arxiv.org/abs/2506.17336</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17336">https://arxiv.org/pdf/2506.17336</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17336]] Privacy-Preserving LLM Interaction with Socratic Chain-of-Thought Reasoning and Homomorphically Encrypted Vector Databases(https://arxiv.org/abs/2506.17336)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly used as personal agents, accessing sensitive user data such as calendars, emails, and medical records. Users currently face a trade-off: They can send private records, many of which are stored in remote databases, to powerful but untrusted LLM providers, increasing their exposure risk. Alternatively, they can run less powerful models locally on trusted devices. We bridge this gap. Our Socratic Chain-of-Thought Reasoning first sends a generic, non-private user query to a powerful, untrusted LLM, which generates a Chain-of-Thought (CoT) prompt and detailed sub-queries without accessing user data. Next, we embed these sub-queries and perform encrypted sub-second semantic search using our Homomorphically Encrypted Vector Database across one million entries of a single user's private data. This represents a realistic scale of personal documents, emails, and records accumulated over years of digital activity. Finally, we feed the CoT prompt and the decrypted records to a local language model and generate the final response. On the LoCoMo long-context QA benchmark, our hybrid framework, combining GPT-4o with a local Llama-3.2-1B model, outperforms using GPT-4o alone by up to 7.1 percentage points. This demonstrates a first step toward systems where tasks are decomposed and split between untrusted strong LLMs and weak local ones, preserving user privacy.</li>
</ul>

<h3>Title: Adaptive Social Metaverse Streaming based on Federated Multi-Agent Deep Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Zijian Long, Haopeng Wang, Haiwei Dong, Abdulmotaleb El Saddik</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.MM, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17342">https://arxiv.org/abs/2506.17342</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17342">https://arxiv.org/pdf/2506.17342</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17342]] Adaptive Social Metaverse Streaming based on Federated Multi-Agent Deep Reinforcement Learning(https://arxiv.org/abs/2506.17342)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, biometric, federate</a></li>
<li><strong>Abstract: </strong>The social metaverse is a growing digital ecosystem that blends virtual and physical worlds. It allows users to interact socially, work, shop, and enjoy entertainment. However, privacy remains a major challenge, as immersive interactions require continuous collection of biometric and behavioral data. At the same time, ensuring high-quality, low-latency streaming is difficult due to the demands of real-time interaction, immersive rendering, and bandwidth optimization. To address these issues, we propose ASMS (Adaptive Social Metaverse Streaming), a novel streaming system based on Federated Multi-Agent Proximal Policy Optimization (F-MAPPO). ASMS leverages F-MAPPO, which integrates federated learning (FL) and deep reinforcement learning (DRL) to dynamically adjust streaming bit rates while preserving user privacy. Experimental results show that ASMS improves user experience by at least 14% compared to existing streaming methods across various network conditions. Therefore, ASMS enhances the social metaverse experience by providing seamless and immersive streaming, even in dynamic and resource-constrained networks, while ensuring that sensitive user data remains on local devices.</li>
</ul>

<h3>Title: AndroIDS : Android-based Intrusion Detection System using Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Akarsh K Nair, Shanik Hubert Satheesh Kumar., Deepti Gupta</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17349">https://arxiv.org/abs/2506.17349</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17349">https://arxiv.org/pdf/2506.17349</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17349]] AndroIDS : Android-based Intrusion Detection System using Federated Learning(https://arxiv.org/abs/2506.17349)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>The exponential growth of android-based mobile IoT systems has significantly increased the susceptibility of devices to cyberattacks, particularly in smart homes, UAVs, and other connected mobile environments. This article presents a federated learning-based intrusion detection framework called AndroIDS that leverages system call traces as a personalized and privacy-preserving data source. Unlike conventional centralized approaches, the proposed method enables collaborative anomaly detection without sharing raw data, thus preserving user privacy across distributed nodes. A generalized system call dataset was generated to reflect realistic android system behavior and serves as the foundation for experimentation. Extensive evaluation demonstrates the effectiveness of the FL model under both IID and non-IID conditions, achieving an accuracy of 96.46 % and 92.87 %, and F1-scores of 89 % and 86 %, respectively. These results highlight the models robustness to data heterogeneity, with only a minor performance drop in the non-IID case. Further, a detailed comparison with centralized deep learning further illustrates trade-offs in detection performance and deployment feasibility. Overall, the results validate the practical applicability of the proposed approach for secure and scalable intrusion detection in real-world mobile IoT scenarios.</li>
</ul>

<h3>Title: CUBA: Controlled Untargeted Backdoor Attack against Deep Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Yinghao Wu, Liyan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17350">https://arxiv.org/abs/2506.17350</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17350">https://arxiv.org/pdf/2506.17350</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17350]] CUBA: Controlled Untargeted Backdoor Attack against Deep Neural Networks(https://arxiv.org/abs/2506.17350)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>Backdoor attacks have emerged as a critical security threat against deep neural networks in recent years. The majority of existing backdoor attacks focus on targeted backdoor attacks, where trigger is strongly associated to specific malicious behavior. Various backdoor detection methods depend on this inherent property and shows effective results in identifying and mitigating such targeted attacks. However, a purely untargeted attack in backdoor scenarios is, in some sense, self-weakening, since the target nature is what makes backdoor attacks so powerful. In light of this, we introduce a novel Constrained Untargeted Backdoor Attack (CUBA), which combines the flexibility of untargeted attacks with the intentionality of targeted attacks. The compromised model, when presented with backdoor images, will classify them into random classes within a constrained range of target classes selected by the attacker. This combination of randomness and determinedness enables the proposed untargeted backdoor attack to natively circumvent existing backdoor defense methods. To implement the untargeted backdoor attack under controlled flexibility, we propose to apply logit normalization on cross-entropy loss with flipped one-hot labels. By constraining the logit during training, the compromised model will show a uniform distribution across selected target classes, resulting in controlled untargeted attack. Extensive experiments demonstrate the effectiveness of the proposed CUBA on different datasets.</li>
</ul>

<h3>Title: Towards Safety Evaluations of Theory of Mind in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tatsuhiro Aoshima, Mitsuaki Akiyama</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17352">https://arxiv.org/abs/2506.17352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17352">https://arxiv.org/pdf/2506.17352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17352]] Towards Safety Evaluations of Theory of Mind in Large Language Models(https://arxiv.org/abs/2506.17352)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As the capabilities of large language models (LLMs) continue to advance, the importance of rigorous safety evaluation is becoming increasingly evident. Recent concerns within the realm of safety assessment have highlighted instances in which LLMs exhibit behaviors that appear to disable oversight mechanisms and respond in a deceptive manner. For example, there have been reports suggesting that, when confronted with information unfavorable to their own persistence during task execution, LLMs may act covertly and even provide false answers to questions intended to verify their this http URL evaluate the potential risk of such deceptive actions toward developers or users, it is essential to investigate whether these behaviors stem from covert, intentional processes within the model. In this study, we propose that it is necessary to measure the theory of mind capabilities of LLMs. We begin by reviewing existing research on theory of mind and identifying the perspectives and tasks relevant to its application in safety evaluation. Given that theory of mind has been predominantly studied within the context of developmental psychology, we analyze developmental trends across a series of open-weight LLMs. Our results indicate that while LLMs have improved in reading comprehension, their theory of mind capabilities have not shown comparable development. Finally, we present the current state of safety evaluation with respect to LLMs' theory of mind, and discuss remaining challenges for future work.</li>
</ul>

<h3>Title: Differentiation-Based Extraction of Proprietary Data from Fine-Tuned LLMs</h3>
<ul>
<li><strong>Authors: </strong>Zongjie Li, Daoyuan Wu, Shuai Wang, Zhendong Su</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17353">https://arxiv.org/abs/2506.17353</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17353">https://arxiv.org/pdf/2506.17353</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17353]] Differentiation-Based Extraction of Proprietary Data from Fine-Tuned LLMs(https://arxiv.org/abs/2506.17353)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, defense, attack, extraction, large language model</a></li>
<li><strong>Abstract: </strong>The increasing demand for domain-specific and human-aligned Large Language Models (LLMs) has led to the widespread adoption of Supervised Fine-Tuning (SFT) techniques. SFT datasets often comprise valuable instruction-response pairs, making them highly valuable targets for potential extraction. This paper studies this critical research problem for the first time. We start by formally defining and formulating the problem, then explore various attack goals, types, and variants based on the unique properties of SFT data in real-world scenarios. Based on our analysis of extraction behaviors of direct extraction, we develop a novel extraction method specifically designed for SFT models, called Differentiated Data Extraction (DDE), which exploits the confidence levels of fine-tuned models and their behavioral differences from pre-trained base models. Through extensive experiments across multiple domains and scenarios, we demonstrate the feasibility of SFT data extraction using DDE. Our results show that DDE consistently outperforms existing extraction baselines in all attack settings. To counter this new attack, we propose a defense mechanism that mitigates DDE attacks with minimal impact on model performance. Overall, our research reveals hidden data leak risks in fine-tuned LLMs and provides insights for developing more secure models.</li>
</ul>

<h3>Title: Cash or Comfort? How LLMs Value Your Inconvenience</h3>
<ul>
<li><strong>Authors: </strong>Mateusz Cedro, Timour Ichmoukhamedov, Sofie Goethals, Yifan He, James Hinns, David Martens</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17367">https://arxiv.org/abs/2506.17367</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17367">https://arxiv.org/pdf/2506.17367</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17367]] Cash or Comfort? How LLMs Value Your Inconvenience(https://arxiv.org/abs/2506.17367)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly proposed as near-autonomous artificial intelligence (AI) agents capable of making everyday decisions on behalf of humans. Although LLMs perform well on many technical tasks, their behaviour in personal decision-making remains less understood. Previous studies have assessed their rationality and moral alignment with human decisions. However, the behaviour of AI assistants in scenarios where financial rewards are at odds with user comfort has not yet been thoroughly explored. In this paper, we tackle this problem by quantifying the prices assigned by multiple LLMs to a series of user discomforts: additional walking, waiting, hunger and pain. We uncover several key concerns that strongly question the prospect of using current LLMs as decision-making assistants: (1) a large variance in responses between LLMs, (2) within a single LLM, responses show fragility to minor variations in prompt phrasing (e.g., reformulating the question in the first person can considerably alter the decision), (3) LLMs can accept unreasonably low rewards for major inconveniences (e.g., 1 Euro to wait 10 hours), and (4) LLMs can reject monetary gains where no discomfort is imposed (e.g., 1,000 Euro to wait 0 minutes). These findings emphasize the need for scrutiny of how LLMs value human inconvenience, particularly as we move toward applications where such cash-versus-comfort trade-offs are made on users' behalf.</li>
</ul>

<h3>Title: SAFEx: Analyzing Vulnerabilities of MoE-Based LLMs via Stable Safety-critical Expert Identification</h3>
<ul>
<li><strong>Authors: </strong>Zhenglin Lai, Mengyao Liao, Dong Xu, Zebin Zhao, Zhihang Yuan, Chao Fan, Jianqiang Li, Bingzhe Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17368">https://arxiv.org/abs/2506.17368</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17368">https://arxiv.org/pdf/2506.17368</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17368]] SAFEx: Analyzing Vulnerabilities of MoE-Based LLMs via Stable Safety-critical Expert Identification(https://arxiv.org/abs/2506.17368)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models based on Mixture-of-Experts have achieved substantial gains in efficiency and scalability, yet their architectural uniqueness introduces underexplored safety alignment challenges. Existing safety alignment strategies, predominantly designed for dense models, are ill-suited to address MoE-specific vulnerabilities. In this work, we formalize and systematically study MoE model's positional vulnerability - the phenomenon where safety-aligned behaviors rely on specific expert modules, revealing critical risks inherent to MoE architectures. To this end, we present SAFEx, an analytical framework that robustly identifies, characterizes, and validates the safety-critical experts using a novel Stability-based Expert Selection (SES) algorithm. Notably, our approach enables the explicit decomposition of safety-critical experts into distinct functional groups, including those responsible for harmful content detection and those controlling safe response generation. Extensive experiments on mainstream MoE models, such as the recently released Qwen3-MoE, demonstrated that their intrinsic safety mechanisms heavily rely on a small subset of positional experts. Disabling these experts significantly compromised the models' ability to refuse harmful requests. For Qwen3-MoE with 6144 experts (in the FNN layer), we find that disabling as few as 12 identified safety-critical experts can cause the refusal rate to drop by 22%, demonstrating the disproportionate impact of a small set of experts on overall model safety.</li>
</ul>

<h3>Title: Secret Sharing in 5G-MEC: Applicability for joint Security and Dependability</h3>
<ul>
<li><strong>Authors: </strong>Thilina Pathirana, Ruxandra F. Olimid</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17371">https://arxiv.org/abs/2506.17371</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17371">https://arxiv.org/pdf/2506.17371</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17371]] Secret Sharing in 5G-MEC: Applicability for joint Security and Dependability(https://arxiv.org/abs/2506.17371)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect</a></li>
<li><strong>Abstract: </strong>Multi-access Edge Computing (MEC), an enhancement of 5G, processes data closer to its generation point, reducing latency and network load. However, the distributed and edge-based nature of 5G-MEC presents privacy and security challenges, including data exposure risks. Ensuring efficient manipulation and security of sensitive data at the edge is crucial. To address these challenges, we investigate the usage of threshold secret sharing in 5G-MEC storage, an approach that enhances both security and dependability. A (k,n) threshold secret sharing scheme splits and stores sensitive data among n nodes, requiring at least k nodes for reconstruction. The solution ensures confidentiality by protecting data against fewer than k colluding nodes and enhances availability by tolerating up to n-k failing nodes. This approach mitigates threats such as unauthorized access and node failures, whether accidental or intentional. We further discuss a method for selecting the convenient MEHs to store the shares, considering the MEHs' trustworthiness level as a main criterion. Although we define our proposal in the context of secret-shared data storage, it can be seen as an independent, standalone selection process for 5G-MEC trustworthy node selection in other scenarios too.</li>
</ul>

<h3>Title: From Drawings to Decisions: A Hybrid Vision-Language Framework for Parsing 2D Engineering Drawings into Structured Manufacturing Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Tayyab Khan, Lequn Chen, Zane Yong, Jun Ming Tan, Wenhe Feng, Seung Ki Moon</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17374">https://arxiv.org/abs/2506.17374</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17374">https://arxiv.org/pdf/2506.17374</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17374]] From Drawings to Decisions: A Hybrid Vision-Language Framework for Parsing 2D Engineering Drawings into Structured Manufacturing Knowledge(https://arxiv.org/abs/2506.17374)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Efficient and accurate extraction of key information from 2D engineering drawings is essential for advancing digital manufacturing workflows. Such information includes geometric dimensioning and tolerancing (GD&T), measures, material specifications, and textual annotations. Manual extraction is slow and labor-intensive, while generic OCR models often fail due to complex layouts, engineering symbols, and rotated text, leading to incomplete and unreliable outputs. These limitations result in incomplete and unreliable outputs. To address these challenges, we propose a hybrid vision-language framework that integrates a rotation-aware object detection model (YOLOv11-obb) with a transformer-based vision-language parser. Our structured pipeline applies YOLOv11-OBB to localize annotations and extract oriented bounding box (OBB) patches, which are then parsed into structured outputs using a fine-tuned, lightweight vision-language model (VLM). We curate a dataset of 1,367 2D mechanical drawings annotated across nine key categories. YOLOv11-OBB is trained on this dataset to detect OBBs and extract annotation patches. These are parsed using two open-source VLMs: Donut and Florence-2. Both models are lightweight and well-suited for specialized industrial tasks under limited computational overhead. Following fine-tuning of both models on the curated dataset of image patches paired with structured annotation labels, a comparative experiment is conducted to evaluate parsing performance across four key metrics. Donut outperforms Florence-2, achieving 88.5% precision, 99.2% recall, and a 93.5% F1-score, with a hallucination rate of 11.5%. Finally, a case study demonstrates how the extracted structured information supports downstream manufacturing tasks such as process and tool selection, showcasing the practical utility of the proposed framework in modernizing 2D drawing interpretation.</li>
</ul>

<h3>Title: Leveraging LLMs to Assess Tutor Moves in Real-Life Dialogues: A Feasibility Study</h3>
<ul>
<li><strong>Authors: </strong>Danielle R. Thomas, Conrad Borchers, Jionghao Lin, Sanjit Kakarla, Shambhavi Bhushan, Erin Gatz, Shivang Gupta, Ralph Abboud, Kenneth R. Koedinger</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17410">https://arxiv.org/abs/2506.17410</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17410">https://arxiv.org/pdf/2506.17410</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17410]] Leveraging LLMs to Assess Tutor Moves in Real-Life Dialogues: A Feasibility Study(https://arxiv.org/abs/2506.17410)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Tutoring improves student achievement, but identifying and studying what tutoring actions are most associated with student learning at scale based on audio transcriptions is an open research problem. This present study investigates the feasibility and scalability of using generative AI to identify and evaluate specific tutor moves in real-life math tutoring. We analyze 50 randomly selected transcripts of college-student remote tutors assisting middle school students in mathematics. Using GPT-4, GPT-4o, GPT-4-turbo, Gemini-1.5-pro, and LearnLM, we assess tutors' application of two tutor skills: delivering effective praise and responding to student math errors. All models reliably detected relevant situations, for example, tutors providing praise to students (94-98% accuracy) and a student making a math error (82-88% accuracy) and effectively evaluated the tutors' adherence to tutoring best practices, aligning closely with human judgments (83-89% and 73-77%, respectively). We propose a cost-effective prompting strategy and discuss practical implications for using large language models to support scalable assessment in authentic settings. This work further contributes LLM prompts to support reproducibility and research in AI-supported learning.</li>
</ul>

<h3>Title: VMRA-MaR: An Asymmetry-Aware Temporal Framework for Longitudinal Breast Cancer Risk Prediction</h3>
<ul>
<li><strong>Authors: </strong>Zijun Sun, Solveig Thrun, Michael Kampffmeyer</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17412">https://arxiv.org/abs/2506.17412</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17412">https://arxiv.org/pdf/2506.17412</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17412]] VMRA-MaR: An Asymmetry-Aware Temporal Framework for Longitudinal Breast Cancer Risk Prediction(https://arxiv.org/abs/2506.17412)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Breast cancer remains a leading cause of mortality worldwide and is typically detected via screening programs where healthy people are invited in regular intervals. Automated risk prediction approaches have the potential to improve this process by facilitating dynamically screening of high-risk groups. While most models focus solely on the most recent screening, there is growing interest in exploiting temporal information to capture evolving trends in breast tissue, as inspired by clinical practice. Early methods typically relied on two time steps, and although recent efforts have extended this to multiple time steps using Transformer architectures, challenges remain in fully harnessing the rich temporal dynamics inherent in longitudinal imaging data. In this work, we propose to instead leverage Vision Mamba RNN (VMRNN) with a state-space model (SSM) and LSTM-like memory mechanisms to effectively capture nuanced trends in breast tissue evolution. To further enhance our approach, we incorporate an asymmetry module that utilizes a Spatial Asymmetry Detector (SAD) and Longitudinal Asymmetry Tracker (LAT) to identify clinically relevant bilateral differences. This integrated framework demonstrates notable improvements in predicting cancer onset, especially for the more challenging high-density breast cases and achieves superior performance at extended time points (years four and five), highlighting its potential to advance early breast cancer recognition and enable more personalized screening strategies. Our code is available at this https URL.</li>
</ul>

<h3>Title: Aha Moment Revisited: Are VLMs Truly Capable of Self Verification in Inference-time Scaling?</h3>
<ul>
<li><strong>Authors: </strong>Mingyuan Wu, Meitang Li, Jingcheng Yang, Jize Jiang, Kaizhuo Yan, Zhaoheng Li, Minjia Zhang, Klara Nahrstedt</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17417">https://arxiv.org/abs/2506.17417</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17417">https://arxiv.org/pdf/2506.17417</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17417]] Aha Moment Revisited: Are VLMs Truly Capable of Self Verification in Inference-time Scaling?(https://arxiv.org/abs/2506.17417)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have demonstrated that inference-time computation techniques, such as decoding-time scaling and self-refinement, can significantly enhance reasoning capabilities without relying on external knowledge. A key driver of this success is the emergence of self-correction and self-verification behaviors, often elicited through reinforcement learning (RL). In this paper, we investigate whether these inference-time techniques extend effectively to vision-language models (VLMs), particularly those trained with RL. We find that while decoding strategies such as majority voting and best-of-N selection with self-verification all improve VLM reasoning performance, generation-reliant methods such as the former achieve significantly higher gains versus verification-reliant methods such as the latter. Additionally, the self-correction behavior often associated with RL-tuned models, such as aha moment, does not lead to measurable gains. We show via extensive experimentation within the inference-time scaling framework to identify a key root cause: RL-trained VLMs still lack robust self-verification capabilities across both visual and textual modalities.</li>
</ul>

<h3>Title: UProp: Investigating the Uncertainty Propagation of LLMs in Multi-Step Agentic Decision-Making</h3>
<ul>
<li><strong>Authors: </strong>Jinhao Duan, James Diffenderfer, Sandeep Madireddy, Tianlong Chen, Bhavya Kailkhura, Kaidi Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17419">https://arxiv.org/abs/2506.17419</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17419">https://arxiv.org/pdf/2506.17419</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17419]] UProp: Investigating the Uncertainty Propagation of LLMs in Multi-Step Agentic Decision-Making(https://arxiv.org/abs/2506.17419)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) are integrated into safety-critical applications involving sequential decision-making in the real world, it is essential to know when to trust LLM decisions. Existing LLM Uncertainty Quantification (UQ) methods are primarily designed for single-turn question-answering formats, resulting in multi-step decision-making scenarios, e.g., LLM agentic system, being underexplored. In this paper, we introduce a principled, information-theoretic framework that decomposes LLM sequential decision uncertainty into two parts: (i) internal uncertainty intrinsic to the current decision, which is focused on existing UQ methods, and (ii) extrinsic uncertainty, a Mutual-Information (MI) quantity describing how much uncertainty should be inherited from preceding decisions. We then propose UProp, an efficient and effective extrinsic uncertainty estimator that converts the direct estimation of MI to the estimation of Pointwise Mutual Information (PMI) over multiple Trajectory-Dependent Decision Processes (TDPs). UProp is evaluated over extensive multi-step decision-making benchmarks, e.g., AgentBench and HotpotQA, with state-of-the-art LLMs, e.g., GPT-4.1 and DeepSeek-V3. Experimental results demonstrate that UProp significantly outperforms existing single-turn UQ baselines equipped with thoughtful aggregation strategies. Moreover, we provide a comprehensive analysis of UProp, including sampling efficiency, potential applications, and intermediate uncertainty propagation, to demonstrate its effectiveness. Codes will be available at this https URL.</li>
</ul>

<h3>Title: Trans${^2}$-CBCT: A Dual-Transformer Framework for Sparse-View CBCT Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Minmin Yang, Huantao Ren, Senem Velipasalar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17425">https://arxiv.org/abs/2506.17425</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17425">https://arxiv.org/pdf/2506.17425</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17425]] Trans${^2}$-CBCT: A Dual-Transformer Framework for Sparse-View CBCT Reconstruction(https://arxiv.org/abs/2506.17425)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, transformer</a></li>
<li><strong>Abstract: </strong>Cone-beam computed tomography (CBCT) using only a few X-ray projection views enables faster scans with lower radiation dose, but the resulting severe under-sampling causes strong artifacts and poor spatial coverage. We address these challenges in a unified framework. First, we replace conventional UNet/ResNet encoders with TransUNet, a hybrid CNN-Transformer model. Convolutional layers capture local details, while self-attention layers enhance global context. We adapt TransUNet to CBCT by combining multi-scale features, querying view-specific features per 3D point, and adding a lightweight attenuation-prediction head. This yields Trans-CBCT, which surpasses prior baselines by 1.17 dB PSNR and 0.0163 SSIM on the LUNA16 dataset with six views. Second, we introduce a neighbor-aware Point Transformer to enforce volumetric coherence. This module uses 3D positional encoding and attention over k-nearest neighbors to improve spatial consistency. The resulting model, Trans$^2$-CBCT, provides an additional gain of 0.63 dB PSNR and 0.0117 SSIM. Experiments on LUNA16 and ToothFairy show consistent gains from six to ten views, validating the effectiveness of combining CNN-Transformer features with point-based geometry reasoning for sparse-view CBCT reconstruction.</li>
</ul>

<h3>Title: Beyond the Link: Assessing LLMs' ability to Classify Political Content across Global Media</h3>
<ul>
<li><strong>Authors: </strong>Alberto Martinez-Serra, Alejandro De La Fuente, Nienke Viescher, Ana S. Cardenal</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17435">https://arxiv.org/abs/2506.17435</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17435">https://arxiv.org/pdf/2506.17435</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17435]] Beyond the Link: Assessing LLMs' ability to Classify Political Content across Global Media(https://arxiv.org/abs/2506.17435)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The use of large language models (LLMs) is becoming common in the context of political science, particularly in studies that analyse individuals use of digital media. However, while previous research has demonstrated LLMs ability at labelling tasks, the effectiveness of using LLMs to classify political content (PC) from just URLs is not yet well explored. The work presented in this article bridges this gap by evaluating whether LLMs can accurately identify PC vs. non-PC from both the article text and the URLs from five countries (France, Germany, Spain, the UK, and the US) and different languages. Using cutting-edge LLMs like GPT, Llama, Mistral, Deepseek, Qwen and Gemma, we measure model performance to assess whether URL-level analysis can be a good approximation for full-text analysis of PC, even across different linguistic and national contexts. Model outputs are compared with human-labelled articles, as well as traditional supervised machine learning techniques, to set a baseline of performance. Overall, our findings suggest the capacity of URLs to embed most of the news content, providing a vital perspective on accuracy-cost balancing. We also account for contextual limitations and suggest methodological recommendations to use LLMs within political science studies.</li>
</ul>

<h3>Title: Open Sky, Open Threats: Replay Attacks in Space Launch and Re-entry Phases</h3>
<ul>
<li><strong>Authors: </strong>Nesrine Benchoubane, Eray Guven, Gunes Karabulut Kurt</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17446">https://arxiv.org/abs/2506.17446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17446">https://arxiv.org/pdf/2506.17446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17446]] Open Sky, Open Threats: Replay Attacks in Space Launch and Re-entry Phases(https://arxiv.org/abs/2506.17446)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, attack</a></li>
<li><strong>Abstract: </strong>This paper examines the effects of replay attacks on the integrity of both uplink and downlink communications during critical phases of spacecraft communication. By combining software-defined radios (SDRs) with a real-time channel emulator, we replicate realistic attack conditions on the Orion spacecraft's communication systems in both launch and reentry. Our evaluation shows that, under replay attacks, the attacker's signal can overpower legitimate transmissions, leading to a Signal to Noise Ratio (SNR) difference of up to -7.8 dB during reentry and -6.5 dB during launch. To mitigate these threats, we propose a more secure receiver design incorporating a phase-coherency-dependent decision-directed (DD) equalizer with a narrowed phase-locked loop (PLL) bandwidth. This configuration enhances resilience by making synchronization more sensitive to phase distortions caused by replay interference.</li>
</ul>

<h3>Title: AQUA20: A Benchmark Dataset for Underwater Species Classification under Challenging Conditions</h3>
<ul>
<li><strong>Authors: </strong>Taufikur Rahman Fuad, Sabbir Ahmed, Shahriar Ivan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17455">https://arxiv.org/abs/2506.17455</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17455">https://arxiv.org/pdf/2506.17455</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17455]] AQUA20: A Benchmark Dataset for Underwater Species Classification under Challenging Conditions(https://arxiv.org/abs/2506.17455)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, explainability, transformer</a></li>
<li><strong>Abstract: </strong>Robust visual recognition in underwater environments remains a significant challenge due to complex distortions such as turbidity, low illumination, and occlusion, which severely degrade the performance of standard vision systems. This paper introduces AQUA20, a comprehensive benchmark dataset comprising 8,171 underwater images across 20 marine species reflecting real-world environmental challenges such as illumination, turbidity, occlusions, etc., providing a valuable resource for underwater visual understanding. Thirteen state-of-the-art deep learning models, including lightweight CNNs (SqueezeNet, MobileNetV2) and transformer-based architectures (ViT, ConvNeXt), were evaluated to benchmark their performance in classifying marine species under challenging conditions. Our experimental results show ConvNeXt achieving the best performance, with a Top-3 accuracy of 98.82% and a Top-1 accuracy of 90.69%, as well as the highest overall F1-score of 88.92% with moderately large parameter size. The results obtained from our other benchmark models also demonstrate trade-offs between complexity and performance. We also provide an extensive explainability analysis using GRAD-CAM and LIME for interpreting the strengths and pitfalls of the models. Our results reveal substantial room for improvement in underwater species recognition and demonstrate the value of AQUA20 as a foundation for future research in this domain. The dataset is publicly available at: this https URL.</li>
</ul>

<h3>Title: FedNAMs: Performing Interpretability Analysis in Federated Learning Context</h3>
<ul>
<li><strong>Authors: </strong>Amitash Nanda, Sree Bhargavi Balija, Debashis Sahoo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17466">https://arxiv.org/abs/2506.17466</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17466">https://arxiv.org/pdf/2506.17466</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17466]] FedNAMs: Performing Interpretability Analysis in Federated Learning Context(https://arxiv.org/abs/2506.17466)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate, interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Federated learning continues to evolve but faces challenges in interpretability and explainability. To address these challenges, we introduce a novel approach that employs Neural Additive Models (NAMs) within a federated learning framework. This new Federated Neural Additive Models (FedNAMs) approach merges the advantages of NAMs, where individual networks concentrate on specific input features, with the decentralized approach of federated learning, ultimately producing interpretable analysis results. This integration enhances privacy by training on local data across multiple devices, thereby minimizing the risks associated with data centralization and improving model robustness and generalizability. FedNAMs maintain detailed, feature-specific learning, making them especially valuable in sectors such as finance and healthcare. They facilitate the training of client-specific models to integrate local updates, preserve privacy, and mitigate concerns related to centralization. Our studies on various text and image classification tasks, using datasets such as OpenFetch ML Wine, UCI Heart Disease, and Iris, show that FedNAMs deliver strong interpretability with minimal accuracy loss compared to traditional Federated Deep Neural Networks (DNNs). The research involves notable findings, including the identification of critical predictive features at both client and global levels. Volatile acidity, sulfates, and chlorides for wine quality. Chest pain type, maximum heart rate, and number of vessels for heart disease. Petal length and width for iris classification. This approach strengthens privacy and model efficiency and improves interpretability and robustness across diverse datasets. Finally, FedNAMs generate insights on causes of highly and low interpretable features.</li>
</ul>

<h3>Title: Computational Approaches to Understanding Large Language Model Impact on Writing and Information Ecosystems</h3>
<ul>
<li><strong>Authors: </strong>Weixin Liang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17467">https://arxiv.org/abs/2506.17467</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17467">https://arxiv.org/pdf/2506.17467</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17467]] Computational Approaches to Understanding Large Language Model Impact on Writing and Information Ecosystems(https://arxiv.org/abs/2506.17467)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown significant potential to change how we write, communicate, and create, leading to rapid adoption across society. This dissertation examines how individuals and institutions are adapting to and engaging with this emerging technology through three research directions. First, I demonstrate how the institutional adoption of AI detectors introduces systematic biases, particularly disadvantaging writers of non-dominant language varieties, highlighting critical equity concerns in AI governance. Second, I present novel population-level algorithmic approaches that measure the increasing adoption of LLMs across writing domains, revealing consistent patterns of AI-assisted content in academic peer reviews, scientific publications, consumer complaints, corporate communications, job postings, and international organization press releases. Finally, I investigate LLMs' capability to provide feedback on research manuscripts through a large-scale empirical analysis, offering insights into their potential to support researchers who face barriers in accessing timely manuscript feedback, particularly early-career researchers and those from under-resourced settings.</li>
</ul>

<h3>Title: Photogranulometry -- Dataset of soil images with corresponding particle size distributions</h3>
<ul>
<li><strong>Authors: </strong>Thomas Plante St-Cyr, François Duhaime, Jean-Sébastien Dubé, Simon Grenier</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17469">https://arxiv.org/abs/2506.17469</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17469">https://arxiv.org/pdf/2506.17469</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17469]] Photogranulometry -- Dataset of soil images with corresponding particle size distributions(https://arxiv.org/abs/2506.17469)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Traditional particle size distribution (PSD) analyses create significant downtime and are expensive in labor and maintenance. These drawbacks could be alleviated using optical grain size analysis integrated into routine geotechnical laboratory workflow. This paper presents a high-resolution dataset of 12,714 images of 321 different soil samples collected in the Montreal, Quebec region, alongside their PSD analysis. It is designed to provide a robust starting point for training convolutional neural networks (CNN) in geotechnical applications. Soil samples were photographed in a standardized top-view position with a resolution of 45 MP and a minimum scale of 39.4 micrometers per pixel, both in their moist and dry states. A custom test bench employing 13x9 inch white aluminum trays, on which the samples are spread in a thin layer, was used. For samples exceeding a size limit, a coning and quartering method was employed for mass reduction.</li>
</ul>

<h3>Title: Few-Shot, Now for Real: Medical VLMs Adaptation without Balanced Sets or Validation</h3>
<ul>
<li><strong>Authors: </strong>Julio Silva-Rodríguez, Fereshteh Shakeri, Houda Bahig, Jose Dolz, Ismail Ben Ayed</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17500">https://arxiv.org/abs/2506.17500</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17500">https://arxiv.org/pdf/2506.17500</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17500]] Few-Shot, Now for Real: Medical VLMs Adaptation without Balanced Sets or Validation(https://arxiv.org/abs/2506.17500)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Vision-language models (VLMs) are gaining attention in medical image analysis. These are pre-trained on large, heterogeneous data sources, yielding rich and transferable representations. Notably, the combination of modality-specialized VLMs with few-shot adaptation has provided fruitful results, enabling the efficient deployment of high-performing solutions. However, previous works on this topic make strong assumptions about the distribution of adaptation data, which are unrealistic in the medical domain. First, prior art assumes access to a balanced support set, a condition that breaks the natural imbalance in disease prevalence found in real-world scenarios. Second, these works typically assume the presence of an additional validation set to fix critical hyper-parameters, which is highly data-inefficient. This work challenges these favorable deployment scenarios and introduces a realistic, imbalanced, validation-free adaptation setting. Our extensive benchmark across various modalities and downstream tasks demonstrates that current methods systematically compromise their performance when operating under realistic conditions, occasionally even performing worse than zero-shot inference. Also, we introduce a training-free linear probe that adaptively blends visual and textual supervision. Detailed studies demonstrate that the proposed solver is a strong, efficient baseline, enabling robust adaptation in challenging scenarios.</li>
</ul>

<h3>Title: A Smart Contract-based Non-Transferable Signature Verification System using Nominative Signatures</h3>
<ul>
<li><strong>Authors: </strong>Hinata Nishino, Kazumasa Omote, Keita Emura</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17504">https://arxiv.org/abs/2506.17504</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17504">https://arxiv.org/pdf/2506.17504</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17504]] A Smart Contract-based Non-Transferable Signature Verification System using Nominative Signatures(https://arxiv.org/abs/2506.17504)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Nominative signatures allow us to indicate who can verify a signature, and they can be employed to construct a non-transferable signature verification system that prevents the signature verification by a third party in unexpected situations. For example, this system can prevent IOU/loan certificate verification in unexpected situations. However, nominative signatures themselves do not allow the verifier to check whether the funds will be transferred in the future or have been this http URL would be desirable to verify the fact simultaneously when the system involves a certain money transfer such as cryptocurrencies/cryptoassets. In this paper, we propose a smart contract-based non-transferable signature verification system using nominative signatures. We pay attention to the fact that the invisibility, which is a security requirement to be held for nominative signatures, allows us to publish nominative signatures on the blockchain. Our system can verify whether a money transfer actually will take place, in addition to indicating who can verify a signature. We transform the Hanaoka-Schuldt nominative signature scheme (ACNS 2011, IEICE Trans. 2016) which is constructed over a symmetric pairing to a scheme constructed over an asymmetric pairing, and evaluate the gas cost when a smart contract runs the verification algorithm of the modified Hanaoka-Schuldt nominative signature scheme.</li>
</ul>

<h3>Title: Learning golf swing signatures from a single wrist-worn inertial sensor</h3>
<ul>
<li><strong>Authors: </strong>Jessy Lauer</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17505">https://arxiv.org/abs/2506.17505</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17505">https://arxiv.org/pdf/2506.17505</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17505]] Learning golf swing signatures from a single wrist-worn inertial sensor(https://arxiv.org/abs/2506.17505)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Despite its importance for performance and injury prevention, golf swing analysis is limited by isolated metrics, underrepresentation of professional athletes, and a lack of rich, interpretable movement representations. We address these gaps with a holistic, data-driven framework for personalized golf swing analysis from a single wrist-worn sensor. We build a large dataset of professional swings from publicly available videos, reconstruct full-body 3D kinematics using biologically accurate human mesh recovery, and generate synthetic inertial data to train neural networks that infer motion and segment swing phases from wrist-based input. We learn a compositional, discrete vocabulary of motion primitives that facilitates the detection and visualization of technical flaws, and is expressive enough to predict player identity, club type, sex, and age. Our system accurately estimates full-body kinematics and swing events from wrist data, delivering lab-grade motion analysis on-course and supporting early detection of anomalous movement patterns. Explainability methods reveal subtle, individualized movement signatures, reinforcing the view that variability is a hallmark of skilled performance. Longitudinal tracking demonstrates practical value: as one player's handicap improved from 50 to 2.2 over 1.5 years, our system captured measurable technical progress and provided targeted, actionable feedback. Our findings challenge common assumptions, such as swing consistency across clubs and the existence of a single "ideal" swing, and uncover latent biomarkers shaped by both intrinsic traits and task-specific constraints. This work bridges lab and field-based biomechanics, offering scalable, accessible, high-fidelity motion analysis for research, coaching, and injury prevention, while opening new directions in movement-based phenotyping, personalized equipment design, and motor skill development.</li>
</ul>

<h3>Title: VeriLocc: End-to-End Cross-Architecture Register Allocation via LLM</h3>
<ul>
<li><strong>Authors: </strong>Lesheng Jin, Zhenyuan Ruan, Haohui Mai, Jingbo Shang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.OS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17506">https://arxiv.org/abs/2506.17506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17506">https://arxiv.org/pdf/2506.17506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17506]] VeriLocc: End-to-End Cross-Architecture Register Allocation via LLM(https://arxiv.org/abs/2506.17506)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Modern GPUs evolve rapidly, yet production compilers still rely on hand-crafted register allocation heuristics that require substantial re-tuning for each hardware generation. We introduce VeriLocc, a framework that combines large language models (LLMs) with formal compiler techniques to enable generalizable and verifiable register allocation across GPU architectures. VeriLocc fine-tunes an LLM to translate intermediate representations (MIRs) into target-specific register assignments, aided by static analysis for cross-architecture normalization and generalization and a verifier-guided regeneration loop to ensure correctness. Evaluated on matrix multiplication (GEMM) and multi-head attention (MHA), VeriLocc achieves 85-99% single-shot accuracy and near-100% pass@100. Case study shows that VeriLocc discovers more performant assignments than expert-tuned libraries, outperforming rocBLAS by over 10% in runtime.</li>
</ul>

<h3>Title: Semantic-Aware Parsing for Security Logs</h3>
<ul>
<li><strong>Authors: </strong>Julien Piet, Vivian Fang, Rishi Khare, Vern Paxson, Raluca Ada Popa, David Wagner</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17512">https://arxiv.org/abs/2506.17512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17512">https://arxiv.org/pdf/2506.17512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17512]] Semantic-Aware Parsing for Security Logs(https://arxiv.org/abs/2506.17512)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Security analysts struggle to quickly and efficiently query and correlate log data due to the heterogeneity and lack of structure in real-world logs. Existing AI-based parsers focus on learning syntactic log templates but lack the semantic interpretation needed for querying. Directly querying large language models on raw logs is impractical at scale and vulnerable to prompt injection attacks. In this paper, we introduce Matryoshka, the first end-to-end system leveraging LLMs to automatically generate semantically-aware structured log parsers. Matryoshka combines a novel syntactic parser-employing precise regular expressions rather than wildcards-with a completely new semantic parsing layer that clusters variables and maps them into a queryable, contextually meaningful schema. This approach provides analysts with queryable and semantically rich data representations, facilitating rapid and precise log querying without the traditional burden of manual parser construction. Additionally, Matryoshka can map the newly created fields to recognized attributes within the Open Cybersecurity Schema Framework (OCSF), enabling interoperability. We evaluate Matryoshka on a newly curated real-world log benchmark, introducing novel metrics to assess how consistently fields are named and mapped across logs. Matryoshka's syntactic parser outperforms prior works, and the semantic layer achieves an F1 score of 0.95 on realistic security queries. Although mapping fields to the extensive OCSF taxonomy remains challenging, Matryoshka significantly reduces manual effort by automatically extracting and organizing valuable fields, moving us closer to fully automated, AI-driven log analytics.</li>
</ul>

<h3>Title: Data Quality Issues in Multilingual Speech Datasets: The Need for Sociolinguistic Awareness and Proactive Language Planning</h3>
<ul>
<li><strong>Authors: </strong>Mingfei Lau, Qian Chen, Yeming Fang, Tingting Xu, Tongzhou Chen, Pavel Golik</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17525">https://arxiv.org/abs/2506.17525</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17525">https://arxiv.org/pdf/2506.17525</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17525]] Data Quality Issues in Multilingual Speech Datasets: The Need for Sociolinguistic Awareness and Proactive Language Planning(https://arxiv.org/abs/2506.17525)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Our quality audit for three widely used public multilingual speech datasets - Mozilla Common Voice 17.0, FLEURS, and VoxPopuli - shows that in some languages, these datasets suffer from significant quality issues. We believe addressing these issues will make these datasets more useful as training and evaluation sets, and improve downstream models. We divide these quality issues into two categories: micro-level and macro-level. We find that macro-level issues are more prevalent in less institutionalized, often under-resourced languages. We provide a case analysis of Taiwanese Southern Min (nan_tw) that highlights the need for proactive language planning (e.g. orthography prescriptions, dialect boundary definition) and enhanced data quality control in the process of Automatic Speech Recognition (ASR) dataset creation. We conclude by proposing guidelines and recommendations to mitigate these issues in future dataset development, emphasizing the importance of sociolinguistic awareness in creating robust and reliable speech data resources.</li>
</ul>

<h3>Title: DuaShepherd: Integrating Stepwise Correctness and Potential Rewards for Mathematical Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Yuanhao Wu, Juntong Song, Hanning Zhang, Tong Zhang, Cheng Niu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17533">https://arxiv.org/abs/2506.17533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17533">https://arxiv.org/pdf/2506.17533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17533]] DuaShepherd: Integrating Stepwise Correctness and Potential Rewards for Mathematical Reasoning(https://arxiv.org/abs/2506.17533)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we propose DuaShepherd, a novel reward modeling framework that integrates two complementary reward signals, correctness and potential, to enhance the mathematical reasoning capabilities of Large Language Models (LLMs). While correctness-based signals emphasize identification of stepwise errors, potential-based signals focus on the likelihood of reaching the correct final answer. We developed an automated pipeline for constructing large-scale reward modeling dataset with both signals. A unified, multi-head architecture was explored to train the two reward models in a multi-task setup, demonstrating benefits from learning both correctness and potential in parallel. By combining these two signals into a compound probability, our model achieves consistent performance improvements across multiple benchmarks. Empirical evaluations on MATH500 and ProcessBench confirm that this combined reward significantly outperforms models trained on either reward type alone, achieving state-of-the-art performance under comparable resource constraints.</li>
</ul>

<h3>Title: Predicting E-commerce Purchase Behavior using a DQN-Inspired Deep Learning Model for enhanced adaptability</h3>
<ul>
<li><strong>Authors: </strong>Aditi Madhusudan Jain</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17543">https://arxiv.org/abs/2506.17543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17543">https://arxiv.org/pdf/2506.17543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17543]] Predicting E-commerce Purchase Behavior using a DQN-Inspired Deep Learning Model for enhanced adaptability(https://arxiv.org/abs/2506.17543)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper presents a novel approach to predicting buying intent and product demand in e-commerce settings, leveraging a Deep Q-Network (DQN) inspired architecture. In the rapidly evolving landscape of online retail, accurate prediction of user behavior is crucial for optimizing inventory management, personalizing user experiences, and maximizing sales. Our method adapts concepts from reinforcement learning to a supervised learning context, combining the sequential modeling capabilities of Long Short-Term Memory (LSTM) networks with the strategic decision-making aspects of DQNs. We evaluate our model on a large-scale e-commerce dataset comprising over 885,000 user sessions, each characterized by 1,114 features. Our approach demonstrates robust performance in handling the inherent class imbalance typical in e-commerce data, where purchase events are significantly less frequent than non-purchase events. Through comprehensive experimentation with various classification thresholds, we show that our model achieves a balance between precision and recall, with an overall accuracy of 88\% and an AUC-ROC score of 0.88. Comparative analysis reveals that our DQN-inspired model offers advantages over traditional machine learning and standard deep learning approaches, particularly in its ability to capture complex temporal patterns in user behavior. The model's performance and scalability make it well-suited for real-world e-commerce applications dealing with high-dimensional, sequential data. This research contributes to the field of e-commerce analytics by introducing a novel predictive modeling technique that combines the strengths of deep learning and reinforcement learning paradigms. Our findings have significant implications for improving demand forecasting, personalizing user experiences, and optimizing marketing strategies in online retail environments.</li>
</ul>

<h3>Title: Scene-R1: Video-Grounded Large Language Models for 3D Scene Reasoning without 3D Annotations</h3>
<ul>
<li><strong>Authors: </strong>Zhihao Yuan, Shuyi Jiang, Chun-Mei Feng, Yaolun Zhang, Shuguang Cui, Zhen Li, Na Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17545">https://arxiv.org/abs/2506.17545</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17545">https://arxiv.org/pdf/2506.17545</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17545]] Scene-R1: Video-Grounded Large Language Models for 3D Scene Reasoning without 3D Annotations(https://arxiv.org/abs/2506.17545)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Currently, utilizing large language models to understand the 3D world is becoming popular. Yet existing 3D-aware LLMs act as black boxes: they output bounding boxes or textual answers without revealing how those decisions are made, and they still rely on pre-trained 3D detectors to supply object proposals. We introduce Scene-R1, a video-grounded framework that learns to reason about 3D scenes without any point-wise 3D instance supervision by pairing reinforcement-learning-driven reasoning with a two-stage grounding pipeline. In the temporal grounding stage, we explicitly reason about the video and select the video snippets most relevant to an open-ended query. In the subsequent image grounding stage, we analyze the image and predict the 2D bounding box. After that, we track the object using SAM2 to produce pixel-accurate masks in RGB frames, and project them back into 3D, thereby eliminating the need for 3D detector-based proposals while capturing fine geometry and material cues. Scene-R1 can also adapt to the 3D visual question answering task to answer free-form questions directly from video. Our training pipeline only needs task-level 2D boxes or textual labels without dense 3D point-wise labels. Scene-R1 surpasses existing open-vocabulary baselines on multiple datasets, while delivering transparent, step-by-step rationales. These results show that reinforcement-learning-based reasoning combined with RGB-D video alone offers a practical, annotation-efficient route to trustworthy 3D scene understanding.</li>
</ul>

<h3>Title: SynDaCaTE: A Synthetic Dataset For Evaluating Part-Whole Hierarchical Inference</h3>
<ul>
<li><strong>Authors: </strong>Jake Levi, Mark van der Wilk</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17558">https://arxiv.org/abs/2506.17558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17558">https://arxiv.org/pdf/2506.17558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17558]] SynDaCaTE: A Synthetic Dataset For Evaluating Part-Whole Hierarchical Inference(https://arxiv.org/abs/2506.17558)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Learning to infer object representations, and in particular part-whole hierarchies, has been the focus of extensive research in computer vision, in pursuit of improving data efficiency, systematic generalisation, and robustness. Models which are \emph{designed} to infer part-whole hierarchies, often referred to as capsule networks, are typically trained end-to-end on supervised tasks such as object classification, in which case it is difficult to evaluate whether such a model \emph{actually} learns to infer part-whole hierarchies, as claimed. To address this difficulty, we present a SYNthetic DAtaset for CApsule Testing and Evaluation, abbreviated as SynDaCaTE, and establish its utility by (1) demonstrating the precise bottleneck in a prominent existing capsule model, and (2) demonstrating that permutation-equivariant self-attention is highly effective for parts-to-wholes inference, which motivates future directions for designing effective inductive biases for computer vision.</li>
</ul>

<h3>Title: LLM-driven Medical Report Generation via Communication-efficient Heterogeneous Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Haoxuan Che, Haibo Jin, Zhengrui Guo, Yi Lin, Cheng Jin, Hao Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17562">https://arxiv.org/abs/2506.17562</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17562">https://arxiv.org/pdf/2506.17562</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17562]] LLM-driven Medical Report Generation via Communication-efficient Heterogeneous Federated Learning(https://arxiv.org/abs/2506.17562)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>LLMs have demonstrated significant potential in Medical Report Generation (MRG), yet their development requires large amounts of medical image-report pairs, which are commonly scattered across multiple centers. Centralizing these data is exceptionally challenging due to privacy regulations, thereby impeding model development and broader adoption of LLM-driven MRG models. To address this challenge, we present FedMRG, the first framework that leverages Federated Learning (FL) to enable privacy-preserving, multi-center development of LLM-driven MRG models, specifically designed to overcome the critical challenge of communication-efficient LLM training under multi-modal data heterogeneity. To start with, our framework tackles the fundamental challenge of communication overhead in FL-LLM tuning by employing low-rank factorization to efficiently decompose parameter updates, significantly reducing gradient transmission costs and making LLM-driven MRG feasible in bandwidth-constrained FL settings. Furthermore, we observed the dual heterogeneity in MRG under the FL scenario: varying image characteristics across medical centers, as well as diverse reporting styles and terminology preferences. To address this, we further enhance FedMRG with (1) client-aware contrastive learning in the MRG encoder, coupled with diagnosis-driven prompts, which capture both globally generalizable and locally distinctive features while maintaining diagnostic accuracy; and (2) a dual-adapter mutual boosting mechanism in the MRG decoder that harmonizes generic and specialized adapters to address variations in reporting styles and terminology. Through extensive evaluation of our established FL-MRG benchmark, we demonstrate the generalizability and adaptability of FedMRG, underscoring its potential in harnessing multi-center data and generating clinically accurate reports while maintaining communication efficiency.</li>
</ul>

<h3>Title: Accelerating Residual Reinforcement Learning with Uncertainty Estimation</h3>
<ul>
<li><strong>Authors: </strong>Lakshita Dodeja, Karl Schmeckpeper, Shivam Vats, Thomas Weng, Mingxi Jia, George Konidaris, Stefanie Tellex</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17564">https://arxiv.org/abs/2506.17564</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17564">https://arxiv.org/pdf/2506.17564</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17564]] Accelerating Residual Reinforcement Learning with Uncertainty Estimation(https://arxiv.org/abs/2506.17564)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Residual Reinforcement Learning (RL) is a popular approach for adapting pretrained policies by learning a lightweight residual policy that provides corrective actions. While Residual RL is more sample-efficient than finetuning the entire base policy, existing methods struggle with sparse rewards and are designed for deterministic base policies. We propose two improvements to Residual RL that further enhance its sample efficiency and make it suitable for stochastic base policies. First, we leverage uncertainty estimates of the base policy to focus exploration on regions in which the base policy is not confident. Second, we propose a simple modification to off-policy residual learning that allows it to observe base actions and better handle stochastic base policies. We evaluate our method with both Gaussian-based and Diffusion-based stochastic base policies on tasks from Robosuite and D4RL, and compare against state-of-the-art finetuning methods, demo-augmented RL methods, and other residual RL methods. Our algorithm significantly outperforms existing baselines in a variety of simulation benchmark environments. We also deploy our learned polices in the real world to demonstrate their robustness with zero-shot sim-to-real transfer.</li>
</ul>

<h3>Title: AgriCHN: A Comprehensive Cross-domain Resource for Chinese Agricultural Named Entity Recognition</h3>
<ul>
<li><strong>Authors: </strong>Lingxiao Zeng, Yiqi Tong, Wei Guo, Huarui Wu, Lihao Ge, Yijun Ye, Fuzhen Zhuang, Deqing Wang, Wei Guo, Cheng Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17578">https://arxiv.org/abs/2506.17578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17578">https://arxiv.org/pdf/2506.17578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17578]] AgriCHN: A Comprehensive Cross-domain Resource for Chinese Agricultural Named Entity Recognition(https://arxiv.org/abs/2506.17578)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Agricultural named entity recognition is a specialized task focusing on identifying distinct agricultural entities within vast bodies of text, including crops, diseases, pests, and fertilizers. It plays a crucial role in enhancing information extraction from extensive agricultural text resources. However, the scarcity of high-quality agricultural datasets, particularly in Chinese, has resulted in suboptimal performance when employing mainstream methods for this purpose. Most earlier works only focus on annotating agricultural entities while overlook the profound correlation of agriculture with hydrology and meteorology. To fill this blank, we present AgriCHN, a comprehensive open-source Chinese resource designed to promote the accuracy of automated agricultural entity annotation. The AgriCHN dataset has been meticulously curated from a wealth of agricultural articles, comprising a total of 4,040 sentences and encapsulating 15,799 agricultural entity mentions spanning 27 diverse entity categories. Furthermore, it encompasses entities from hydrology to meteorology, thereby enriching the diversity of entities considered. Data validation reveals that, compared with relevant resources, AgriCHN demonstrates outstanding data quality, attributable to its richer agricultural entity types and more fine-grained entity divisions. A benchmark task has also been constructed using several state-of-the-art neural NER models. Extensive experimental results highlight the significant challenge posed by AgriCHN and its potential for further research.</li>
</ul>

<h3>Title: HalluRNN: Mitigating Hallucinations via Recurrent Cross-Layer Reasoning in Large Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Le Yu, Kaishen Wang, Jianlong Xiong, Yue Cao, Tao He</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17587">https://arxiv.org/abs/2506.17587</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17587">https://arxiv.org/pdf/2506.17587</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17587]] HalluRNN: Mitigating Hallucinations via Recurrent Cross-Layer Reasoning in Large Vision-Language Models(https://arxiv.org/abs/2506.17587)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Though Large Vision-Language Models (LVLMs) have achieved remarkable performance across various tasks, they are still prone to hallucinations-generating outputs that are textually plausible but visually ungrounded. While prior approaches generally address this issue through data-centric fine-tuning or innovative decoding strategies, these methods often require substantial resources or task-specific configurations. In this work, we introduce an architecture-level solution, HalluRNN, which enhances model stability through recurrent cross-layer reasoning. Specifically, we propose a novel Dual-Gated Depth Propagation Unit (DG-DPU) module, which is shared across layers and recurrently refines hidden states. This allows for the adaptive propagation of information throughout the model, enforces consistency across layers, and mitigates hallucinations caused by representational drift. By fine-tuning only the DG-DPU module, HalluRNN achieves strong and robust performance across multiple benchmarks.</li>
</ul>

<h3>Title: DRAMA-X: A Fine-grained Intent Prediction and Risk Reasoning Benchmark For Driving</h3>
<ul>
<li><strong>Authors: </strong>Mihir Godbole, Xiangbo Gao, Zhengzhong Tu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17590">https://arxiv.org/abs/2506.17590</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17590">https://arxiv.org/pdf/2506.17590</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17590]] DRAMA-X: A Fine-grained Intent Prediction and Risk Reasoning Benchmark For Driving(https://arxiv.org/abs/2506.17590)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Understanding the short-term motion of vulnerable road users (VRUs) like pedestrians and cyclists is critical for safe autonomous driving, especially in urban scenarios with ambiguous or high-risk behaviors. While vision-language models (VLMs) have enabled open-vocabulary perception, their utility for fine-grained intent reasoning remains underexplored. Notably, no existing benchmark evaluates multi-class intent prediction in safety-critical situations, To address this gap, we introduce DRAMA-X, a fine-grained benchmark constructed from the DRAMA dataset via an automated annotation pipeline. DRAMA-X contains 5,686 accident-prone frames labeled with object bounding boxes, a nine-class directional intent taxonomy, binary risk scores, expert-generated action suggestions for the ego vehicle, and descriptive motion summaries. These annotations enable a structured evaluation of four interrelated tasks central to autonomous decision-making: object detection, intent prediction, risk assessment, and action suggestion. As a reference baseline, we propose SGG-Intent, a lightweight, training-free framework that mirrors the ego vehicle's reasoning pipeline. It sequentially generates a scene graph from visual input using VLM-backed detectors, infers intent, assesses risk, and recommends an action using a compositional reasoning stage powered by a large language model. We evaluate a range of recent VLMs, comparing performance across all four DRAMA-X tasks. Our experiments demonstrate that scene-graph-based reasoning enhances intent prediction and risk assessment, especially when contextual cues are explicitly modeled.</li>
</ul>

<h3>Title: A Multimodal In Vitro Diagnostic Method for Parkinson's Disease Combining Facial Expressions and Behavioral Gait Data</h3>
<ul>
<li><strong>Authors: </strong>Wei Huang, Yinxuan Xu, Yintao Zhou, Zhengyu Li, Jing Huang, Meng Pang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17596">https://arxiv.org/abs/2506.17596</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17596">https://arxiv.org/pdf/2506.17596</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17596]] A Multimodal In Vitro Diagnostic Method for Parkinson's Disease Combining Facial Expressions and Behavioral Gait Data(https://arxiv.org/abs/2506.17596)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Parkinson's disease (PD), characterized by its incurable nature, rapid progression, and severe disability, poses significant challenges to the lives of patients and their families. Given the aging population, the need for early detection of PD is increasing. In vitro diagnosis has garnered attention due to its non-invasive nature and low cost. However, existing methods present several challenges: 1) limited training data for facial expression diagnosis; 2) specialized equipment and acquisition environments required for gait diagnosis, resulting in poor generalizability; 3) the risk of misdiagnosis or missed diagnosis when relying on a single modality. To address these issues, we propose a novel multimodal in vitro diagnostic method for PD, leveraging facial expressions and behavioral gait. Our method employs a lightweight deep learning model for feature extraction and fusion, aimed at improving diagnostic accuracy and facilitating deployment on mobile devices. Furthermore, we have established the largest multimodal PD dataset in collaboration with a hospital and conducted extensive experiments to validate the effectiveness of our proposed method.</li>
</ul>

<h3>Title: OpenMAP-BrainAge: Generalizable and Interpretable Brain Age Predictor</h3>
<ul>
<li><strong>Authors: </strong>Pengyu Kan, Craig Jones, Kenichi Oishi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17597">https://arxiv.org/abs/2506.17597</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17597">https://arxiv.org/pdf/2506.17597</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17597]] OpenMAP-BrainAge: Generalizable and Interpretable Brain Age Predictor(https://arxiv.org/abs/2506.17597)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, transformer, generative</a></li>
<li><strong>Abstract: </strong>Purpose: To develop an age prediction model which is interpretable and robust to demographic and technological variances in brain MRI scans. Materials and Methods: We propose a transformer-based architecture that leverages self-supervised pre-training on large-scale datasets. Our model processes pseudo-3D T1-weighted MRI scans from three anatomical views and incorporates brain volumetric information. By introducing a stem architecture, we reduce the conventional quadratic complexity of transformer models to linear complexity, enabling scalability for high-dimensional MRI data. We trained our model on ADNI2 $\&$ 3 (N=1348) and OASIS3 (N=716) datasets (age range: 42 - 95) from the North America, with an 8:1:1 split for train, validation and test. Then, we validated it on the AIBL dataset (N=768, age range: 60 - 92) from Australia. Results: We achieved an MAE of 3.65 years on ADNI2 $\&$ 3 and OASIS3 test set and a high generalizability of MAE of 3.54 years on AIBL. There was a notable increase in brain age gap (BAG) across cognitive groups, with mean of 0.15 years (95% CI: [-0.22, 0.51]) in CN, 2.55 years ([2.40, 2.70]) in MCI, 6.12 years ([5.82, 6.43]) in AD. Additionally, significant negative correlation between BAG and cognitive scores was observed, with correlation coefficient of -0.185 (p < 0.001) for MoCA and -0.231 (p < 0.001) for MMSE. Gradient-based feature attribution highlighted ventricles and white matter structures as key regions influenced by brain aging. Conclusion: Our model effectively fused information from different views and volumetric information to achieve state-of-the-art brain age prediction accuracy, improved generalizability and interpretability with association to neurodegenerative disorders.</li>
</ul>

<h3>Title: Towards Fundamental Limits for Active Multi-distribution Learning</h3>
<ul>
<li><strong>Authors: </strong>Chicheng Zhang, Yihan Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17607">https://arxiv.org/abs/2506.17607</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17607">https://arxiv.org/pdf/2506.17607</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17607]] Towards Fundamental Limits for Active Multi-distribution Learning(https://arxiv.org/abs/2506.17607)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Multi-distribution learning extends agnostic Probably Approximately Correct (PAC) learning to the setting in which a family of $k$ distributions, $\{D_i\}_{i\in[k]}$, is considered and a classifier's performance is measured by its error under the worst distribution. This problem has attracted a lot of recent interests due to its applications in collaborative learning, fairness, and robustness. Despite a rather complete picture of sample complexity of passive multi-distribution learning, research on active multi-distribution learning remains scarce, with algorithms whose optimality remaining unknown. In this paper, we develop new algorithms for active multi-distribution learning and establish improved label complexity upper and lower bounds, in distribution-dependent and distribution-free settings. Specifically, in the near-realizable setting we prove an upper bound of $\widetilde{O}\Bigl(\theta_{\max}(d+k)\ln\frac{1}{\varepsilon}\Bigr)$ and $\widetilde{O}\Bigl(\theta_{\max}(d+k)\Bigl(\ln\frac{1}{\varepsilon}+\frac{\nu^2}{\varepsilon^2}\Bigr)+\frac{k\nu}{\varepsilon^2}\Bigr)$ in the realizable and agnostic settings respectively, where $\theta_{\max}$ is the maximum disagreement coefficient among the $k$ distributions, $d$ is the VC dimension of the hypothesis class, $\nu$ is the multi-distribution error of the best hypothesis, and $\varepsilon$ is the target excess error. Moreover, we show that the bound in the realizable setting is information-theoretically optimal and that the $k\nu/\varepsilon^2$ term in the agnostic setting is fundamental for proper learners. We also establish instance-dependent sample complexity bound for passive multidistribution learning that smoothly interpolates between realizable and agnostic regimes~\citep{blum2017collaborative,zhang2024optimal}, which may be of independent interest.</li>
</ul>

<h3>Title: HIRE: Lightweight High-Resolution Image Feature Enrichment for Multimodal LLMs</h3>
<ul>
<li><strong>Authors: </strong>Nikitha SR, Aradhya Neeraj Mathur, Tarun Ram Menta, Rishabh Jain, Mausoom Sarkar</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17608">https://arxiv.org/abs/2506.17608</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17608">https://arxiv.org/pdf/2506.17608</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17608]] HIRE: Lightweight High-Resolution Image Feature Enrichment for Multimodal LLMs(https://arxiv.org/abs/2506.17608)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The integration of high-resolution image features in modern multimodal large language models has demonstrated significant improvements in fine-grained visual understanding tasks, achieving high performance across multiple benchmarks. Since these features are obtained from large image encoders like ViT, they come with a significant increase in computational costs due to multiple calls to these encoders. In this work, we first develop an intuition for feature upsampling as a natural extension of high-resolution feature generation. Through extensive experiments and ablations, we demonstrate how a shallow feature enricher can achieve competitive results with tremendous reductions in training and inference time as well as computational cost, with upto 1.5x saving in FLOPs.</li>
</ul>

<h3>Title: TyphoFormer: Language-Augmented Transformer for Accurate Typhoon Track Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Lincan Li, Eren Erman Ozguven, Yue Zhao, Guang Wang, Yiqun Xie, Yushun Dong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17609">https://arxiv.org/abs/2506.17609</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17609">https://arxiv.org/pdf/2506.17609</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17609]] TyphoFormer: Language-Augmented Transformer for Accurate Typhoon Track Forecasting(https://arxiv.org/abs/2506.17609)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Accurate typhoon track forecasting is crucial for early system warning and disaster response. While Transformer-based models have demonstrated strong performance in modeling the temporal dynamics of dense trajectories of humans and vehicles in smart cities, they usually lack access to broader contextual knowledge that enhances the forecasting reliability of sparse meteorological trajectories, such as typhoon tracks. To address this challenge, we propose TyphoFormer, a novel framework that incorporates natural language descriptions as auxiliary prompts to improve typhoon trajectory forecasting. For each time step, we use Large Language Model (LLM) to generate concise textual descriptions based on the numerical attributes recorded in the North Atlantic hurricane database. The language descriptions capture high-level meteorological semantics and are embedded as auxiliary special tokens prepended to the numerical time series input. By integrating both textual and sequential information within a unified Transformer encoder, TyphoFormer enables the model to leverage contextual cues that are otherwise inaccessible through numerical features alone. Extensive experiments are conducted on HURDAT2 benchmark, results show that TyphoFormer consistently outperforms other state-of-the-art baseline methods, particularly under challenging scenarios involving nonlinear path shifts and limited historical observations.</li>
</ul>

<h3>Title: JarvisArt: Liberating Human Artistic Creativity via an Intelligent Photo Retouching Agent</h3>
<ul>
<li><strong>Authors: </strong>Yunlong Lin, Zixu Lin, Kunjie Lin, Jinbin Bai, Panwang Pan, Chenxin Li, Haoyu Chen, Zhongdao Wang, Xinghao Ding, Wenbo Li, Shuicheng Yan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17612">https://arxiv.org/abs/2506.17612</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17612">https://arxiv.org/pdf/2506.17612</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17612]] JarvisArt: Liberating Human Artistic Creativity via an Intelligent Photo Retouching Agent(https://arxiv.org/abs/2506.17612)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Photo retouching has become integral to contemporary visual storytelling, enabling users to capture aesthetics and express creativity. While professional tools such as Adobe Lightroom offer powerful capabilities, they demand substantial expertise and manual effort. In contrast, existing AI-based solutions provide automation but often suffer from limited adjustability and poor generalization, failing to meet diverse and personalized editing needs. To bridge this gap, we introduce JarvisArt, a multi-modal large language model (MLLM)-driven agent that understands user intent, mimics the reasoning process of professional artists, and intelligently coordinates over 200 retouching tools within Lightroom. JarvisArt undergoes a two-stage training process: an initial Chain-of-Thought supervised fine-tuning to establish basic reasoning and tool-use skills, followed by Group Relative Policy Optimization for Retouching (GRPO-R) to further enhance its decision-making and tool proficiency. We also propose the Agent-to-Lightroom Protocol to facilitate seamless integration with Lightroom. To evaluate performance, we develop MMArt-Bench, a novel benchmark constructed from real-world user edits. JarvisArt demonstrates user-friendly interaction, superior generalization, and fine-grained control over both global and local adjustments, paving a new avenue for intelligent photo retouching. Notably, it outperforms GPT-4o with a 60% improvement in average pixel-level metrics on MMArt-Bench for content fidelity, while maintaining comparable instruction-following capabilities. Project Page: this https URL.</li>
</ul>

<h3>Title: EQuARX: Efficient Quantized AllReduce in XLA for Distributed Machine Learning Acceleration</h3>
<ul>
<li><strong>Authors: </strong>Ibrahim Ahmed, Clemens Schaefer, Gil Tabak, Denis Vnukov, Zenong Zhang, Felix chern, Anatoliy Yevtushenko, Andy Davis</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17615">https://arxiv.org/abs/2506.17615</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17615">https://arxiv.org/pdf/2506.17615</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17615]] EQuARX: Efficient Quantized AllReduce in XLA for Distributed Machine Learning Acceleration(https://arxiv.org/abs/2506.17615)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While Large Language Models (LLMs) have become highly influential, their enormous scale presents significant deployment challenges. Efficiently serving these models typically requires distributing them across numerous accelerator devices, which introduces substantial performance overhead from inter-device communication (collectives). While model quantization has been widely adopted to reduce the memory and compute requirements of LLM weights and activations with minimal quality impact, applying quantization directly to collectives like AllReduce is inherently difficult due to the inter-device summation involved, which can lead to numerical instability or significant error accumulation. In this work, we present a native dynamic block-wise efficient quantized AllReduce within the XLA compiler for TPUs (EQuARX). By using TPU-friendly quantization and deep pipelining of communication and compute, EQuARX with int8 precision achieves a 1.8X speedup over baseline BF16 AllReduce across various network topologies. Furthermore, EQuARX accelerates the prefill stage of Gemma 3 27B by 1.25X and Gemma 3 12B by 1.1X, respectively, with small to negligible impact on quality.</li>
</ul>

<h3>Title: Trustworthy Chronic Disease Risk Prediction For Self-Directed Preventive Care via Medical Literature Validation</h3>
<ul>
<li><strong>Authors: </strong>Minh Le, Khoi Ton</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17620">https://arxiv.org/abs/2506.17620</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17620">https://arxiv.org/pdf/2506.17620</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17620]] Trustworthy Chronic Disease Risk Prediction For Self-Directed Preventive Care via Medical Literature Validation(https://arxiv.org/abs/2506.17620)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Chronic diseases are long-term, manageable, yet typically incurable conditions, highlighting the need for effective preventive strategies. Machine learning has been widely used to assess individual risk for chronic diseases. However, many models rely on medical test data (e.g. blood results, glucose levels), which limits their utility for proactive self-assessment. Additionally, to gain public trust, machine learning models should be explainable and transparent. Although some research on self-assessment machine learning models includes explainability, their explanations are not validated against established medical literature, reducing confidence in their reliability. To address these issues, we develop deep learning models that predict the risk of developing 13 chronic diseases using only personal and lifestyle factors, enabling accessible, self-directed preventive care. Importantly, we use SHAP-based explainability to identify the most influential model features and validate them against established medical literature. Our results show a strong alignment between the models' most influential features and established medical literature, reinforcing the models' trustworthiness. Critically, we find that this observation holds across 13 distinct diseases, indicating that this machine learning approach can be broadly trusted for chronic disease prediction. This work lays the foundation for developing trustworthy machine learning tools for self-directed preventive care. Future research can explore other approaches for models' trustworthiness and discuss how the models can be used ethically and responsibly.</li>
</ul>

<h3>Title: Exploiting Efficiency Vulnerabilities in Dynamic Deep Learning Systems</h3>
<ul>
<li><strong>Authors: </strong>Ravishka Rathnasuriya, Wei Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17621">https://arxiv.org/abs/2506.17621</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17621">https://arxiv.org/pdf/2506.17621</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17621]] Exploiting Efficiency Vulnerabilities in Dynamic Deep Learning Systems(https://arxiv.org/abs/2506.17621)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>The growing deployment of deep learning models in real-world environments has intensified the need for efficient inference under strict latency and resource constraints. To meet these demands, dynamic deep learning systems (DDLSs) have emerged, offering input-adaptive computation to optimize runtime efficiency. While these systems succeed in reducing cost, their dynamic nature introduces subtle and underexplored security risks. In particular, input-dependent execution pathways create opportunities for adversaries to degrade efficiency, resulting in excessive latency, energy usage, and potential denial-of-service in time-sensitive deployments. This work investigates the security implications of dynamic behaviors in DDLSs and reveals how current systems expose efficiency vulnerabilities exploitable by adversarial inputs. Through a survey of existing attack strategies, we identify gaps in the coverage of emerging model architectures and limitations in current defense mechanisms. Building on these insights, we propose to examine the feasibility of efficiency attacks on modern DDLSs and develop targeted defenses to preserve robustness under adversarial conditions.</li>
</ul>

<h3>Title: SoK: Stablecoin Designs, Risks, and the Stablecoin LEGO</h3>
<ul>
<li><strong>Authors: </strong>Shengchen Ling, Yuefeng Du, Yajin Zhou, Lei Wu, Cong Wang, Xiaohua Jia, Houmin Yan</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17622">https://arxiv.org/abs/2506.17622</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17622">https://arxiv.org/pdf/2506.17622</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17622]] SoK: Stablecoin Designs, Risks, and the Stablecoin LEGO(https://arxiv.org/abs/2506.17622)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Stablecoins have become significant assets in modern finance, with a market capitalization exceeding USD 246 billion (May 2025). Yet, despite their systemic importance, a comprehensive and risk-oriented understanding of crucial aspects like their design trade-offs, security dynamics, and interdependent failure pathways often remains underdeveloped. This SoK confronts this gap through a large-scale analysis of 157 research studies, 95 active stablecoins, and 44 major security incidents. Our analysis establishes four pivotal insights: 1) stability is best understood not an inherent property but an emergent, fragile state reliant on the interplay between market confidence and continuous liquidity; 2) stablecoin designs demonstrate trade-offs in risk specialization instead of mitigation; 3) the widespread integration of yield mechanisms imposes a "dual mandate" that creates a systemic tension between the core mission of stability and the high-risk financial engineering required for competitive returns; and 4) major security incidents act as acute "evolutionary pressures", forging resilience by stress-testing designs and aggressively redefining the security frontier. We introduce the Stablecoin LEGO framework, a quantitative methodology mapping historical failures to current designs. Its application reveals that a lower assessed risk strongly correlates with integrating lessons from past incidents. We hope this provides a systematic foundation for building, evaluating, and regulating more resilient stablecoins.</li>
</ul>

<h3>Title: List-Decodable Byzantine Robust PIR: Lower Communication Complexity, Higher Byzantine Tolerance, Smaller List Size</h3>
<ul>
<li><strong>Authors: </strong>Pengzhen Ke, Liang Feng Zhang, Huaxiong Wang, Li-Ping Wang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17625">https://arxiv.org/abs/2506.17625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17625">https://arxiv.org/pdf/2506.17625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17625]] List-Decodable Byzantine Robust PIR: Lower Communication Complexity, Higher Byzantine Tolerance, Smaller List Size(https://arxiv.org/abs/2506.17625)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust</a></li>
<li><strong>Abstract: </strong>Private Information Retrieval (PIR) is a privacy-preserving primitive in cryptography. Significant endeavors have been made to address the variant of PIR concerning the malicious servers. Among those endeavors, list-decodable Byzantine robust PIR schemes may tolerate a majority of malicious responding servers that provide incorrect answers. In this paper, we propose two perfect list-decodable BRPIR schemes. Our schemes are the first ones that can simultaneously handle a majority of malicious responding servers, achieve a communication complexity of $o(n^{1/2})$ for a database of size n, and provide a nontrivial estimation on the list sizes. Compared with the existing solutions, our schemes attain lower communication complexity, higher byzantine tolerance, and smaller list size.</li>
</ul>

<h3>Title: CLiViS: Unleashing Cognitive Map through Linguistic-Visual Synergy for Embodied Visual Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Kailing Li, Qi'ao Xu, Tianwen Qian, Yuqian Fu, Yang Jiao, Xiaoling Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17629">https://arxiv.org/abs/2506.17629</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17629">https://arxiv.org/pdf/2506.17629</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17629]] CLiViS: Unleashing Cognitive Map through Linguistic-Visual Synergy for Embodied Visual Reasoning(https://arxiv.org/abs/2506.17629)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Embodied Visual Reasoning (EVR) seeks to follow complex, free-form instructions based on egocentric video, enabling semantic understanding and spatiotemporal reasoning in dynamic environments. Despite its promising potential, EVR encounters significant challenges stemming from the diversity of complex instructions and the intricate spatiotemporal dynamics in long-term egocentric videos. Prior solutions either employ Large Language Models (LLMs) over static video captions, which often omit critical visual details, or rely on end-to-end Vision-Language Models (VLMs) that struggle with stepwise compositional reasoning. Consider the complementary strengths of LLMs in reasoning and VLMs in perception, we propose CLiViS. It is a novel training-free framework that leverages LLMs for high-level task planning and orchestrates VLM-driven open-world visual perception to iteratively update the scene context. Building on this synergy, the core of CLiViS is a dynamic Cognitive Map that evolves throughout the reasoning process. This map constructs a structured representation of the embodied scene, bridging low-level perception and high-level reasoning. Extensive experiments across multiple benchmarks demonstrate the effectiveness and generality of CLiViS, especially in handling long-term visual dependencies. Code is available at this https URL.</li>
</ul>

<h3>Title: Answer-Centric or Reasoning-Driven? Uncovering the Latent Memory Anchor in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yang Wu, Yifan Zhang, Yiwei Wang, Yujun Cai, Yurong Wu, Yuran Wang, Ning Xu, Jian Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17630">https://arxiv.org/abs/2506.17630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17630">https://arxiv.org/pdf/2506.17630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17630]] Answer-Centric or Reasoning-Driven? Uncovering the Latent Memory Anchor in LLMs(https://arxiv.org/abs/2506.17630)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While Large Language Models (LLMs) demonstrate impressive reasoning capabilities, growing evidence suggests much of their success stems from memorized answer-reasoning patterns rather than genuine inference. In this work, we investigate a central question: are LLMs primarily anchored to final answers or to the textual pattern of reasoning chains? We propose a five-level answer-visibility prompt framework that systematically manipulates answer cues and probes model behavior through indirect, behavioral analysis. Experiments across state-of-the-art LLMs reveal a strong and consistent reliance on explicit answers. The performance drops by 26.90\% when answer cues are masked, even with complete reasoning chains. These findings suggest that much of the reasoning exhibited by LLMs may reflect post-hoc rationalization rather than true inference, calling into question their inferential depth. Our study uncovers the answer-anchoring phenomenon with rigorous empirical validation and underscores the need for a more nuanced understanding of what constitutes reasoning in LLMs.</li>
</ul>

<h3>Title: LLM-Prompt: Integrated Heterogeneous Prompts for Unlocking LLMs in Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Zesen Wang, Yonggang Li, Lijuan Lan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17631">https://arxiv.org/abs/2506.17631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17631">https://arxiv.org/pdf/2506.17631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17631]] LLM-Prompt: Integrated Heterogeneous Prompts for Unlocking LLMs in Time Series Forecasting(https://arxiv.org/abs/2506.17631)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Time series forecasting aims to model temporal dependencies among variables for future state inference, holding significant importance and widespread applications in real-world scenarios. Although deep learning-based methods have achieved remarkable progress, they still exhibit suboptimal performance in long-term forecasting and data-scarce scenarios. Recent research demonstrates that large language models (LLMs) achieve promising performance in time series forecasting. However, we find existing LLM-based methods still have shortcomings: (1) the absence of a unified paradigm for textual prompt formulation and (2) the neglect of modality discrepancies between textual prompts and time series. To address this, we propose LLM-Prompt, an LLM-based time series forecasting framework integrating multi-prompt information and cross-modal semantic alignment. Specifically, we first construct a unified textual prompt paradigm containing learnable soft prompts and textualized hard prompts. Second, to enhance LLMs' comprehensive understanding of the forecasting task, we design a semantic space embedding and cross-modal alignment module to achieve cross-modal fusion of temporal and textual information. Finally, the transformed time series from the LLMs are projected to obtain the forecasts. Comprehensive evaluations on 6 public datasets and 3 carbon emission datasets demonstrate that LLM-Prompt is a powerful framework for time series forecasting.</li>
</ul>

<h3>Title: Optimization-Free Patch Attack on Stereo Depth Estimation</h3>
<ul>
<li><strong>Authors: </strong>Hangcheng Liu, Xu Kuang, Xingshuo Han, Xingwan Wu, Haoran Ou, Shangwei Guo, Xingyi Huang, Tao Xiang, Tianwei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17632">https://arxiv.org/abs/2506.17632</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17632">https://arxiv.org/pdf/2506.17632</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17632]] Optimization-Free Patch Attack on Stereo Depth Estimation(https://arxiv.org/abs/2506.17632)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, extraction</a></li>
<li><strong>Abstract: </strong>Stereo Depth Estimation (SDE) is essential for scene understanding in vision-based systems like autonomous driving. However, recent studies show that SDE models are vulnerable to adversarial attacks, which are often limited to unrealistic settings, e.g., digital perturbations on separate stereo views in static scenes, restricting their real-world applicability. This raises a critical question: how can we design physically realizable, scene-adaptive, and transferable attacks against SDE under realistic constraints? To answer this, we make two key contributions. First, we propose a unified attack framework that extends optimization-based techniques to four core stages of stereo matching: feature extraction, cost-volume construction, cost aggregation, and disparity regression. A comprehensive stage-wise evaluation across 9 mainstream SDE models, under constraints like photometric consistency, reveals that optimization-based patches suffer from poor transferability. Interestingly, partially transferable patches suggest that patterns, rather than pixel-level perturbations, may be key to generalizable attacks. Motivated by this, we present PatchHunter, the first optimization-free adversarial patch attack against SDE. PatchHunter formulates patch generation as a reinforcement learning-driven search over a structured space of visual patterns crafted to disrupt SDE assumptions. We validate PatchHunter across three levels: the KITTI dataset, the CARLA simulator, and real-world vehicle deployment. PatchHunter not only surpasses optimization-based methods in effectiveness but also achieves significantly better black-box transferability. Even under challenging physical conditions like low light, PatchHunter maintains high attack success (e.g., D1-all > 0.4), whereas optimization-based methods fail.</li>
</ul>

<h3>Title: Step-Opt: Boosting Optimization Modeling in LLMs through Iterative Data Synthesis and Structured Validation</h3>
<ul>
<li><strong>Authors: </strong>Yang Wu, Yifan Zhang, Yurong Wu, Yuran Wang, Junkai Zhang, Jian Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17637">https://arxiv.org/abs/2506.17637</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17637">https://arxiv.org/pdf/2506.17637</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17637]] Step-Opt: Boosting Optimization Modeling in LLMs through Iterative Data Synthesis and Structured Validation(https://arxiv.org/abs/2506.17637)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have revolutionized various domains but encounter substantial challenges in tackling optimization modeling tasks for Operations Research (OR), particularly when dealing with complex problem. In this work, we propose Step-Opt-Instruct, a framework that augments existing datasets and generates high-quality fine-tuning data tailored to optimization modeling. Step-Opt-Instruct employs iterative problem generation to systematically increase problem complexity and stepwise validation to rigorously verify data, preventing error propagation and ensuring the quality of the generated dataset. Leveraging this framework, we fine-tune open-source LLMs, including LLaMA-3-8B and Mistral-7B, to develop Step-Opt--a model that achieves state-of-the-art performance on benchmarks such as NL4OPT, MAMO, and IndustryOR. Extensive experiments demonstrate the superior performance of Step-Opt, especially in addressing complex OR tasks, with a notable 17.01\% improvement in micro average accuracy on difficult problems. These findings highlight the effectiveness of combining structured validation with gradual problem refinement to advance the automation of decision-making processes using this http URL code and dataset are available at this https URL.</li>
</ul>

<h3>Title: Histopathology Image Report Generation by Vision Language Model with Multimodal In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Shih-Wen Liu, Hsuan-Yu Fan, Wei-Ta Chu, Fu-En Yang, Yu-Chiang Frank Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17645">https://arxiv.org/abs/2506.17645</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17645">https://arxiv.org/pdf/2506.17645</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17645]] Histopathology Image Report Generation by Vision Language Model with Multimodal In-Context Learning(https://arxiv.org/abs/2506.17645)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Automating medical report generation from histopathology images is a critical challenge requiring effective visual representations and domain-specific knowledge. Inspired by the common practices of human experts, we propose an in-context learning framework called PathGenIC that integrates context derived from the training set with a multimodal in-context learning (ICL) mechanism. Our method dynamically retrieves semantically similar whole slide image (WSI)-report pairs and incorporates adaptive feedback to enhance contextual relevance and generation quality. Evaluated on the HistGen benchmark, the framework achieves state-of-the-art results, with significant improvements across BLEU, METEOR, and ROUGE-L metrics, and demonstrates robustness across diverse report lengths and disease categories. By maximizing training data utility and bridging vision and language with ICL, our work offers a solution for AI-driven histopathology reporting, setting a strong foundation for future advancements in multimodal clinical applications.</li>
</ul>

<h3>Title: Online Multi-LLM Selection via Contextual Bandits under Unstructured Context Evolution</h3>
<ul>
<li><strong>Authors: </strong>Manhin Poon, XiangXiang Dai, Xutong Liu, Fang Kong, John C.S. Lui, Jinhang Zuo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17670">https://arxiv.org/abs/2506.17670</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17670">https://arxiv.org/pdf/2506.17670</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17670]] Online Multi-LLM Selection via Contextual Bandits under Unstructured Context Evolution(https://arxiv.org/abs/2506.17670)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) exhibit diverse response behaviors, costs, and strengths, making it challenging to select the most suitable LLM for a given user query. We study the problem of adaptive multi-LLM selection in an online setting, where the learner interacts with users through multi-step query refinement and must choose LLMs sequentially without access to offline datasets or model internals. A key challenge arises from unstructured context evolution: the prompt dynamically changes in response to previous model outputs via a black-box process, which cannot be simulated, modeled, or learned. To address this, we propose the first contextual bandit framework for sequential LLM selection under unstructured prompt dynamics. We formalize a notion of myopic regret and develop a LinUCB-based algorithm that provably achieves sublinear regret without relying on future context prediction. We further introduce budget-aware and positionally-aware (favoring early-stage satisfaction) extensions to accommodate variable query costs and user preferences for early high-quality responses. Our algorithms are theoretically grounded and require no offline fine-tuning or dataset-specific training. Experiments on diverse benchmarks demonstrate that our methods outperform existing LLM routing strategies in both accuracy and cost-efficiency, validating the power of contextual bandits for real-time, adaptive LLM selection.</li>
</ul>

<h3>Title: TPTT: Transforming Pretrained Transformer into Titans</h3>
<ul>
<li><strong>Authors: </strong>Fabien Furfaro</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17671">https://arxiv.org/abs/2506.17671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17671">https://arxiv.org/pdf/2506.17671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17671]] TPTT: Transforming Pretrained Transformer into Titans(https://arxiv.org/abs/2506.17671)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have led to remarkable progress in natural language processing, but their computational and memory demands remain a significant challenge, particularly for long-context inference. We introduce TPTT (Transforming Pretrained Transformer into Titans), a novel framework for enhancing pretrained Transformer models with efficient linearized attention mechanisms and advanced memory management. TPTT employs techniques such as Memory as Gate (MaG) and mixed linearized attention (LiZA). It is fully compatible with the Hugging Face Transformers library, enabling seamless adaptation of any causal LLM through parameter-efficient fine-tuning (LoRA) without full retraining. We show the effectiveness of TPTT on the MMLU benchmark with models of approximately 1 billion parameters, observing substantial improvements in both efficiency and accuracy. For instance, Titans-Llama-3.2-1B achieves a 20% increase in Exact Match (EM) over its baseline. Statistical analyses and comparisons with recent state-of-the-art methods confirm the practical scalability and robustness of TPTT. Code is available at this https URL . Python package at this https URL .</li>
</ul>

<h3>Title: Learning Personalized Utility Functions for Drivers in Ride-hailing Systems Using Ensemble Hypernetworks</h3>
<ul>
<li><strong>Authors: </strong>Weiming Mai, Jie Gao, Oded Cats</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17672">https://arxiv.org/abs/2506.17672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17672">https://arxiv.org/pdf/2506.17672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17672]] Learning Personalized Utility Functions for Drivers in Ride-hailing Systems Using Ensemble Hypernetworks(https://arxiv.org/abs/2506.17672)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>In ride-hailing systems, drivers decide whether to accept or reject ride requests based on factors such as order characteristics, traffic conditions, and personal preferences. Accurately predicting these decisions is essential for improving the efficiency and reliability of these systems. Traditional models, such as the Random Utility Maximization (RUM) approach, typically predict drivers' decisions by assuming linear correlations among attributes. However, these models often fall short because they fail to account for non-linear interactions between attributes and do not cater to the unique, personalized preferences of individual drivers. In this paper, we develop a method for learning personalized utility functions using hypernetwork and ensemble learning. Hypernetworks dynamically generate weights for a linear utility function based on trip request data and driver profiles, capturing the non-linear relationships. An ensemble of hypernetworks trained on different data segments further improve model adaptability and generalization by introducing controlled randomness, thereby reducing over-fitting. We validate the performance of our ensemble hypernetworks model in terms of prediction accuracy and uncertainty estimation in a real-world dataset. The results demonstrate that our approach not only accurately predicts each driver's utility but also effectively balances the needs for explainability and uncertainty quantification. Additionally, our model serves as a powerful tool for revealing the personalized preferences of different drivers, clearly illustrating which attributes largely impact their rider acceptance decisions.</li>
</ul>

<h3>Title: FaithfulSAE: Towards Capturing Faithful Features with Sparse Autoencoders without External Dataset Dependencies</h3>
<ul>
<li><strong>Authors: </strong>Seonglae Cho, Harryn Oh, Donghyun Lee, Luis Eduardo Rodrigues Vieira, Andrew Bermingham, Ziad El Sayed</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17673">https://arxiv.org/abs/2506.17673</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17673">https://arxiv.org/pdf/2506.17673</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17673]] FaithfulSAE: Towards Capturing Faithful Features with Sparse Autoencoders without External Dataset Dependencies(https://arxiv.org/abs/2506.17673)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Sparse Autoencoders (SAEs) have emerged as a promising solution for decomposing large language model representations into interpretable features. However, Paulo and Belrose (2025) have highlighted instability across different initialization seeds, and Heap et al. (2025) have pointed out that SAEs may not capture model-internal features. These problems likely stem from training SAEs on external datasets - either collected from the Web or generated by another model - which may contain out-of-distribution (OOD) data beyond the model's generalisation capabilities. This can result in hallucinated SAE features, which we term "Fake Features", that misrepresent the model's internal activations. To address these issues, we propose FaithfulSAE, a method that trains SAEs on the model's own synthetic dataset. Using FaithfulSAEs, we demonstrate that training SAEs on less-OOD instruction datasets results in SAEs being more stable across seeds. Notably, FaithfulSAEs outperform SAEs trained on web-based datasets in the SAE probing task and exhibit a lower Fake Feature Ratio in 5 out of 7 models. Overall, our approach eliminates the dependency on external datasets, advancing interpretability by better capturing model-internal features while highlighting the often neglected importance of SAE training datasets.</li>
</ul>

<h3>Title: CSDN: A Context-Gated Self-Adaptive Detection Network for Real-Time Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Wei Haolin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17679">https://arxiv.org/abs/2506.17679</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17679">https://arxiv.org/pdf/2506.17679</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17679]] CSDN: A Context-Gated Self-Adaptive Detection Network for Real-Time Object Detection(https://arxiv.org/abs/2506.17679)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Convolutional neural networks (CNNs) have long been the cornerstone of target detection, but they are often limited by limited receptive fields, which hinders their ability to capture global contextual information. This paper believes that the effective utilization of extracted features is as important as the feature extraction process itself. We critically re-evaluated the DETR-inspired header network architecture, questioning the indispensable nature of its self-attention mechanism, and discovering significant information redundancies. To solve these problems, we introduced the Context-Gated Scale-Adaptive Detection Network (CSDN), a Transformer-based detection header inspired by natural language processing architecture and human visual perception. CSDN aims to efficiently utilize the characteristics of the CNN backbone network by replacing the traditional stacked self-attention and cross-attention layers with a novel gating mechanism. This mechanism enables each region of interest (ROI) to adaptively select and combine feature dimensions and scale information from multiple attention patterns. CSDN provides more powerful global context modeling capabilities and can better adapt to objects of different sizes and structures. Our proposed detection head can directly replace the native heads of various CNN-based detectors, and only a few rounds of fine-tuning on the pre-training weights can significantly improve the detection accuracy, thus avoiding the need to achieve small improvements. Various layer modules undergo extensive re-training.</li>
</ul>

<h3>Title: Domain Generalization using Action Sequences for Egocentric Action Recognition</h3>
<ul>
<li><strong>Authors: </strong>Amirshayan Nasirimajd, Chiara Plizzari, Simone Alberto Peirone, Marco Ciccone, Giuseppe Averta, Barbara Caputo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17685">https://arxiv.org/abs/2506.17685</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17685">https://arxiv.org/pdf/2506.17685</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17685]] Domain Generalization using Action Sequences for Egocentric Action Recognition(https://arxiv.org/abs/2506.17685)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recognizing human activities from visual inputs, particularly through a first-person viewpoint, is essential for enabling robots to replicate human behavior. Egocentric vision, characterized by cameras worn by observers, captures diverse changes in illumination, viewpoint, and environment. This variability leads to a notable drop in the performance of Egocentric Action Recognition models when tested in environments not seen during training. In this paper, we tackle these challenges by proposing a domain generalization approach for Egocentric Action Recognition. Our insight is that action sequences often reflect consistent user intent across visual domains. By leveraging action sequences, we aim to enhance the model's generalization ability across unseen environments. Our proposed method, named SeqDG, introduces a visual-text sequence reconstruction objective (SeqRec) that uses contextual cues from both text and visual inputs to reconstruct the central action of the sequence. Additionally, we enhance the model's robustness by training it on mixed sequences of actions from different domains (SeqMix). We validate SeqDG on the EGTEA and EPIC-KITCHENS-100 datasets. Results on EPIC-KITCHENS-100, show that SeqDG leads to +2.4% relative average improvement in cross-domain action recognition in unseen environments, and on EGTEA the model achieved +0.6% Top-1 accuracy over SOTA in intra-domain action recognition.</li>
</ul>

<h3>Title: Resource-Friendly Dynamic Enhancement Chain for Multi-Hop Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Binquan Ji, Haibo Luo, Yifei Lu, Lei Hei, Jiaqi Wang, Tingjing Liao, Lingyu Wang, Shichao Wang, Feiliang Ren</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17692">https://arxiv.org/abs/2506.17692</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17692">https://arxiv.org/pdf/2506.17692</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17692]] Resource-Friendly Dynamic Enhancement Chain for Multi-Hop Question Answering(https://arxiv.org/abs/2506.17692)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Knowledge-intensive multi-hop question answering (QA) tasks, which require integrating evidence from multiple sources to address complex queries, often necessitate multiple rounds of retrieval and iterative generation by large language models (LLMs). However, incorporating many documents and extended contexts poses challenges -such as hallucinations and semantic drift-for lightweight LLMs with fewer parameters. This work proposes a novel framework called DEC (Dynamic Enhancement Chain). DEC first decomposes complex questions into logically coherent subquestions to form a hallucination-free reasoning chain. It then iteratively refines these subquestions through context-aware rewriting to generate effective query formulations. For retrieval, we introduce a lightweight discriminative keyword extraction module that leverages extracted keywords to achieve targeted, precise document recall with relatively low computational overhead. Extensive experiments on three multi-hop QA datasets demonstrate that DEC performs on par with or surpasses state-of-the-art benchmarks while significantly reducing token consumption. Notably, our approach attains state-of-the-art results on models with 8B parameters, showcasing its effectiveness in various scenarios, particularly in resource-constrained environments.</li>
</ul>

<h3>Title: SSAVSV: Towards Unified Model for Self-Supervised Audio-Visual Speaker Verification</h3>
<ul>
<li><strong>Authors: </strong>Gnana Praveen Rajasekhar, Jahangir Alam</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17694">https://arxiv.org/abs/2506.17694</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17694">https://arxiv.org/pdf/2506.17694</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17694]] SSAVSV: Towards Unified Model for Self-Supervised Audio-Visual Speaker Verification(https://arxiv.org/abs/2506.17694)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Conventional audio-visual methods for speaker verification rely on large amounts of labeled data and separate modality-specific architectures, which is computationally expensive, limiting their scalability. To address these problems, we propose a self-supervised learning framework based on contrastive learning with asymmetric masking and masked data modeling to obtain robust audiovisual feature representations. In particular, we employ a unified framework for self-supervised audiovisual speaker verification using a single shared backbone for audio and visual inputs, leveraging the versatility of vision transformers. The proposed unified framework can handle audio, visual, or audiovisual inputs using a single shared vision transformer backbone during training and testing while being computationally efficient and robust to missing modalities. Extensive experiments demonstrate that our method achieves competitive performance without labeled data while reducing computational costs compared to traditional approaches.</li>
</ul>

<h3>Title: The Evolution of Natural Language Processing: How Prompt Optimization and Language Models are Shaping the Future</h3>
<ul>
<li><strong>Authors: </strong>Summra Saleem, Muhammad Nabeel Asim, Shaista Zulfiqar, Andreas Dengel</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17700">https://arxiv.org/abs/2506.17700</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17700">https://arxiv.org/pdf/2506.17700</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17700]] The Evolution of Natural Language Processing: How Prompt Optimization and Language Models are Shaping the Future(https://arxiv.org/abs/2506.17700)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have revolutionized the field of Natural Language Processing (NLP) by automating traditional labor-intensive tasks and consequently accelerated the development of computer-aided applications. As researchers continue to advance this field with the introduction of novel language models and more efficient training/finetuning methodologies, the idea of prompt engineering and subsequent optimization strategies with LLMs has emerged as a particularly impactful trend to yield a substantial performance boost across diverse NLP tasks. To best of our knowledge numerous review articles have explored prompt engineering, however, a critical gap exists in comprehensive analyses of prompt optimization strategies. To bridge this gap this paper provides unique and comprehensive insights about the potential of diverse prompt optimization strategies. It analyzes their underlying working paradigms and based on these principles, categorizes them into 11 distinct classes. Moreover, the paper provides details about various NLP tasks where these prompt optimization strategies have been employed, along with details of different LLMs and benchmark datasets used for evaluation. This comprehensive compilation lays a robust foundation for future comparative studies and enables rigorous assessment of prompt optimization and LLM-based predictive pipelines under consistent experimental settings: a critical need in the current landscape. Ultimately, this research will centralize diverse strategic knowledge to facilitate the adaptation of existing prompt optimization strategies for development of innovative predictors across unexplored tasks.</li>
</ul>

<h3>Title: DreamJourney: Perpetual View Generation with Video Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Bo Pan, Yang Chen, Yingwei Pan, Ting Yao, Wei Chen, Tao Mei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17705">https://arxiv.org/abs/2506.17705</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17705">https://arxiv.org/pdf/2506.17705</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17705]] DreamJourney: Perpetual View Generation with Video Diffusion Models(https://arxiv.org/abs/2506.17705)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, large language model</a></li>
<li><strong>Abstract: </strong>Perpetual view generation aims to synthesize a long-term video corresponding to an arbitrary camera trajectory solely from a single input image. Recent methods commonly utilize a pre-trained text-to-image diffusion model to synthesize new content of previously unseen regions along camera movement. However, the underlying 2D diffusion model lacks 3D awareness and results in distorted artifacts. Moreover, they are limited to generating views of static 3D scenes, neglecting to capture object movements within the dynamic 4D world. To alleviate these issues, we present DreamJourney, a two-stage framework that leverages the world simulation capacity of video diffusion models to trigger a new perpetual scene view generation task with both camera movements and object dynamics. Specifically, in stage I, DreamJourney first lifts the input image to 3D point cloud and renders a sequence of partial images from a specific camera trajectory. A video diffusion model is then utilized as generative prior to complete the missing regions and enhance visual coherence across the sequence, producing a cross-view consistent video adheres to the 3D scene and camera trajectory. Meanwhile, we introduce two simple yet effective strategies (early stopping and view padding) to further stabilize the generation process and improve visual quality. Next, in stage II, DreamJourney leverages a multimodal large language model to produce a text prompt describing object movements in current view, and uses video diffusion model to animate current view with object movements. Stage I and II are repeated recurrently, enabling perpetual dynamic scene view generation. Extensive experiments demonstrate the superiority of our DreamJourney over state-of-the-art methods both quantitatively and qualitatively. Our project page: this https URL.</li>
</ul>

<h3>Title: Programmable-Room: Interactive Textured 3D Room Meshes Generation Empowered by Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jihyun Kim, Junho Park, Kyeongbo Kong, Suk-Ju Kang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17707">https://arxiv.org/abs/2506.17707</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17707">https://arxiv.org/pdf/2506.17707</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17707]] Programmable-Room: Interactive Textured 3D Room Meshes Generation Empowered by Large Language Models(https://arxiv.org/abs/2506.17707)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>We present Programmable-Room, a framework which interactively generates and edits a 3D room mesh, given natural language instructions. For precise control of a room's each attribute, we decompose the challenging task into simpler steps such as creating plausible 3D coordinates for room meshes, generating panorama images for the texture, constructing 3D meshes by integrating the coordinates and panorama texture images, and arranging furniture. To support the various decomposed tasks with a unified framework, we incorporate visual programming (VP). VP is a method that utilizes a large language model (LLM) to write a Python-like program which is an ordered list of necessary modules for the various tasks given in natural language. We develop most of the modules. Especially, for the texture generating module, we utilize a pretrained large-scale diffusion model to generate panorama images conditioned on text and visual prompts (i.e., layout, depth, and semantic map) simultaneously. Specifically, we enhance the panorama image generation quality by optimizing the training objective with a 1D representation of a panorama scene obtained from bidirectional LSTM. We demonstrate Programmable-Room's flexibility in generating and editing 3D room meshes, and prove our framework's superiority to an existing model quantitatively and qualitatively. Project page is available in this https URL.</li>
</ul>

<h3>Title: CEGA: A Cost-Effective Approach for Graph-Based Model Extraction and Acquisition</h3>
<ul>
<li><strong>Authors: </strong>Zebin Wang, Menghan Lin, Bolin Shen, Ken Anderson, Molei Liu, Tianxi Cai, Yushun Dong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17709">https://arxiv.org/abs/2506.17709</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17709">https://arxiv.org/pdf/2506.17709</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17709]] CEGA: A Cost-Effective Approach for Graph-Based Model Extraction and Acquisition(https://arxiv.org/abs/2506.17709)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, extraction</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) have demonstrated remarkable utility across diverse applications, and their growing complexity has made Machine Learning as a Service (MLaaS) a viable platform for scalable deployment. However, this accessibility also exposes GNN to serious security threats, most notably model extraction attacks (MEAs), in which adversaries strategically query a deployed model to construct a high-fidelity replica. In this work, we evaluate the vulnerability of GNNs to MEAs and explore their potential for cost-effective model acquisition in non-adversarial research settings. Importantly, adaptive node querying strategies can also serve a critical role in research, particularly when labeling data is expensive or time-consuming. By selectively sampling informative nodes, researchers can train high-performing GNNs with minimal supervision, which is particularly valuable in domains such as biomedicine, where annotations often require expert input. To address this, we propose a node querying strategy tailored to a highly practical yet underexplored scenario, where bulk queries are prohibited, and only a limited set of initial nodes is available. Our approach iteratively refines the node selection mechanism over multiple learning cycles, leveraging historical feedback to improve extraction efficiency. Extensive experiments on benchmark graph datasets demonstrate our superiority over comparable baselines on accuracy, fidelity, and F1 score under strict query-size constraints. These results highlight both the susceptibility of deployed GNNs to extraction attacks and the promise of ethical, efficient GNN acquisition methods to support low-resource research environments.</li>
</ul>

<h3>Title: PDC-Net: Pattern Divide-and-Conquer Network for Pelvic Radiation Injury Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Xiong, Wuteng Cao, Zihuang Wu, Lei Zhang, Chong Gao, Guanbin Li, Qiyuan Qin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17712">https://arxiv.org/abs/2506.17712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17712">https://arxiv.org/pdf/2506.17712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17712]] PDC-Net: Pattern Divide-and-Conquer Network for Pelvic Radiation Injury Segmentation(https://arxiv.org/abs/2506.17712)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Accurate segmentation of Pelvic Radiation Injury (PRI) from Magnetic Resonance Images (MRI) is crucial for more precise prognosis assessment and the development of personalized treatment plans. However, automated segmentation remains challenging due to factors such as complex organ morphologies and confusing context. To address these challenges, we propose a novel Pattern Divide-and-Conquer Network (PDC-Net) for PRI segmentation. The core idea is to use different network modules to "divide" various local and global patterns and, through flexible feature selection, to "conquer" the Regions of Interest (ROI) during the decoding phase. Specifically, considering that our ROI often manifests as strip-like or circular-like structures in MR slices, we introduce a Multi-Direction Aggregation (MDA) module. This module enhances the model's ability to fit the shape of the organ by applying strip convolutions in four distinct directions. Additionally, to mitigate the challenge of confusing context, we propose a Memory-Guided Context (MGC) module. This module explicitly maintains a memory parameter to track cross-image patterns at the dataset level, thereby enhancing the distinction between global patterns associated with the positive and negative classes. Finally, we design an Adaptive Fusion Decoder (AFD) that dynamically selects features from different patterns based on the Mixture-of-Experts (MoE) framework, ultimately generating the final segmentation results. We evaluate our method on the first large-scale pelvic radiation injury dataset, and the results demonstrate the superiority of our PDC-Net over existing approaches.</li>
</ul>

<h3>Title: Unveiling Factors for Enhanced POS Tagging: A Study of Low-Resource Medieval Romance Languages</h3>
<ul>
<li><strong>Authors: </strong>Matthias Schöffel, Esteban Garces Arias, Marinus Wiedner, Paula Ruppert, Meimingwei Li, Christian Heumann, Matthias Aßenmacher</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17715">https://arxiv.org/abs/2506.17715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17715">https://arxiv.org/pdf/2506.17715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17715]] Unveiling Factors for Enhanced POS Tagging: A Study of Low-Resource Medieval Romance Languages(https://arxiv.org/abs/2506.17715)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Part-of-speech (POS) tagging remains a foundational component in natural language processing pipelines, particularly critical for historical text analysis at the intersection of computational linguistics and digital humanities. Despite significant advancements in modern large language models (LLMs) for ancient languages, their application to Medieval Romance languages presents distinctive challenges stemming from diachronic linguistic evolution, spelling variations, and labeled data scarcity. This study systematically investigates the central determinants of POS tagging performance across diverse corpora of Medieval Occitan, Medieval Spanish, and Medieval French texts, spanning biblical, hagiographical, medical, and dietary domains. Through rigorous experimentation, we evaluate how fine-tuning approaches, prompt engineering, model architectures, decoding strategies, and cross-lingual transfer learning techniques affect tagging accuracy. Our results reveal both notable limitations in LLMs' ability to process historical language variations and non-standardized spelling, as well as promising specialized techniques that effectively address the unique challenges presented by low-resource historical languages.</li>
</ul>

<h3>Title: KAG-Thinker: Teaching Large Language Models to Think with Human-like Reasoning Process</h3>
<ul>
<li><strong>Authors: </strong>Dalong Zhang, Jun Xu, Jun Zhou, Lei Liang, Lin Yuan, Ling Zhong, Mengshu Sun, Peilong Zhao, QiWei Wang, Xiaorui Wang, Xinkai Du, YangYang Hou, Yu Ao, ZhaoYang Wang, Zhengke Gui, ZhiYing Yi, Zhongpu Bo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17728">https://arxiv.org/abs/2506.17728</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17728">https://arxiv.org/pdf/2506.17728</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17728]] KAG-Thinker: Teaching Large Language Models to Think with Human-like Reasoning Process(https://arxiv.org/abs/2506.17728)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce KAG-Thinker, a novel human-like reasoning framework built upon a parameter-light large language model (LLM). Our approach enhances the logical coherence and contextual consistency of the thinking process in question-answering (Q\&A) tasks on domain-specific knowledge bases (KBs) within LLMs. This framework simulates human cognitive mechanisms for handling complex problems by establishing a structured thinking process. Continuing the \textbf{Logical Form} guided retrieval and reasoning technology route of KAG v0.7, firstly, it decomposes complex questions into independently solvable sub-problems(also referred to as logical forms) through \textbf{breadth decomposition}, each represented in two equivalent forms-natural language and logical function-and further classified as either Knowledge Retrieval or Reasoning Analysis tasks, with dependencies and variables passing explicitly modeled via logical function interfaces. In the solving process, the Retrieval function is used to perform knowledge retrieval tasks, while the Math and Deduce functions are used to perform reasoning analysis tasks. Secondly, it is worth noting that, in the Knowledge Retrieval sub-problem tasks, LLMs and external knowledge sources are regarded as equivalent KBs. We use the \textbf{knowledge boundary} model to determine the optimal source using self-regulatory mechanisms such as confidence calibration and reflective reasoning, and use the \textbf{depth solving} model to enhance the comprehensiveness of knowledge acquisition. Finally, instead of utilizing reinforcement learning, we employ supervised fine-tuning with multi-turn dialogues to align the model with our structured inference paradigm, thereby avoiding excessive reflection. This is supported by a data evaluation framework and iterative corpus synthesis, which facilitate the generation of detailed reasoning trajectories...</li>
</ul>

<h3>Title: PhysID: Physics-based Interactive Dynamics from a Single-view Image</h3>
<ul>
<li><strong>Authors: </strong>Sourabh Vasant Gothe, Ayon Chattopadhyay, Gunturi Venkata Sai Phani Kiran, Pratik, Vibhav Agarwal, Jayesh Rajkumar Vachhani, Sourav Ghosh, Parameswaranath VM, Barath Raj KR</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17746">https://arxiv.org/abs/2506.17746</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17746">https://arxiv.org/pdf/2506.17746</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17746]] PhysID: Physics-based Interactive Dynamics from a Single-view Image(https://arxiv.org/abs/2506.17746)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Transforming static images into interactive experiences remains a challenging task in computer vision. Tackling this challenge holds the potential to elevate mobile user experiences, notably through interactive and AR/VR applications. Current approaches aim to achieve this either using pre-recorded video responses or requiring multi-view images as input. In this paper, we present PhysID, that streamlines the creation of physics-based interactive dynamics from a single-view image by leveraging large generative models for 3D mesh generation and physical property prediction. This significantly reduces the expertise required for engineering-intensive tasks like 3D modeling and intrinsic property calibration, enabling the process to be scaled with minimal manual intervention. We integrate an on-device physics-based engine for physically plausible real-time rendering with user interactions. PhysID represents a leap forward in mobile-based interactive dynamics, offering real-time, non-deterministic interactions and user-personalization with efficient on-device memory consumption. Experiments evaluate the zero-shot capabilities of various Multimodal Large Language Models (MLLMs) on diverse tasks and the performance of 3D reconstruction models. These results demonstrate the cohesive functioning of all modules within the end-to-end framework, contributing to its effectiveness.</li>
</ul>

<h3>Title: LoLA-SpecViT: Local Attention SwiGLU Vision Transformer with LoRA for Hyperspectral Imaging</h3>
<ul>
<li><strong>Authors: </strong>Fadi Abdeladhim Zidi, Djamel Eddine Boukhari, Abdellah Zakaria Sellam, Abdelkrim Ouafi, Cosimo Distante, Salah Eddine Bekhouche, Abdelmalik Taleb-Ahmed</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17759">https://arxiv.org/abs/2506.17759</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17759">https://arxiv.org/pdf/2506.17759</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17759]] LoLA-SpecViT: Local Attention SwiGLU Vision Transformer with LoRA for Hyperspectral Imaging(https://arxiv.org/abs/2506.17759)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, transformer</a></li>
<li><strong>Abstract: </strong>Hyperspectral image classification remains a challenging task due to the high dimensionality of spectral data, significant inter-band redundancy, and the limited availability of annotated samples. While recent transformer-based models have improved the global modeling of spectral-spatial dependencies, their scalability and adaptability under label-scarce conditions remain limited. In this work, we propose \textbf{LoLA-SpecViT}(Low-rank adaptation Local Attention Spectral Vision Transformer), a lightweight spectral vision transformer that addresses these limitations through a parameter-efficient architecture tailored to the unique characteristics of hyperspectral imagery. Our model combines a 3D convolutional spectral front-end with local window-based self-attention, enhancing both spectral feature extraction and spatial consistency while reducing computational complexity. To further improve adaptability, we integrate low-rank adaptation (LoRA) into attention and projection layers, enabling fine-tuning with over 80\% fewer trainable parameters. A novel cyclical learning rate scheduler modulates LoRA adaptation strength during training, improving convergence and generalisation. Extensive experiments on three benchmark datasets WHU-Hi LongKou, WHU-Hi HongHu, and Salinas demonstrate that LoLA-SpecViT consistently outperforms state-of-the-art baselines, achieving up to 99.91\% accuracy with substantially fewer parameters and enhanced robustness under low-label regimes. The proposed framework provides a scalable and generalizable solution for real-world HSI applications in agriculture, environmental monitoring, and remote sensing analytics. Our code is available in the following \href{this https URL}{GitHub Repository}.</li>
</ul>

<h3>Title: Towards a Unified Textual Graph Framework for Spectral Reasoning via Physical and Chemical Information Fusion</h3>
<ul>
<li><strong>Authors: </strong>Jiheng Liang, Ziru Yu, Zujie Xie, Yuchen Guo, Yulan Guo, Xiangyang Yu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17761">https://arxiv.org/abs/2506.17761</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17761">https://arxiv.org/pdf/2506.17761</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17761]] Towards a Unified Textual Graph Framework for Spectral Reasoning via Physical and Chemical Information Fusion(https://arxiv.org/abs/2506.17761)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Motivated by the limitations of current spectral analysis methods-such as reliance on single-modality data, limited generalizability, and poor interpretability-we propose a novel multi-modal spectral analysis framework that integrates prior knowledge graphs with Large Language Models. Our method explicitly bridges physical spectral measurements and chemical structural semantics by representing them in a unified Textual Graph format, enabling flexible, interpretable, and generalizable spectral understanding. Raw spectra are first transformed into TAGs, where nodes and edges are enriched with textual attributes describing both spectral properties and chemical context. These are then merged with relevant prior knowledge-including functional groups and molecular graphs-to form a Task Graph that incorporates "Prompt Nodes" supporting LLM-based contextual reasoning. A Graph Neural Network further processes this structure to complete downstream tasks. This unified design enables seamless multi-modal integration and automated feature decoding with minimal manual annotation. Our framework achieves consistently high performance across multiple spectral analysis tasks, including node-level, edge-level, and graph-level classification. It demonstrates robust generalization in both zero-shot and few-shot settings, highlighting its effectiveness in learning from limited data and supporting in-context reasoning. This work establishes a scalable and interpretable foundation for LLM-driven spectral analysis, unifying physical and chemical modalities for scientific applications.</li>
</ul>

<h3>Title: A Locally Differential Private Coding-Assisted Succinct Histogram Protocol</h3>
<ul>
<li><strong>Authors: </strong>Hsuan-Po Liu, Hessam Mahdavifar</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC, cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17767">https://arxiv.org/abs/2506.17767</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17767">https://arxiv.org/pdf/2506.17767</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17767]] A Locally Differential Private Coding-Assisted Succinct Histogram Protocol(https://arxiv.org/abs/2506.17767)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>A succinct histogram captures frequent items and their frequencies across clients and has become increasingly important for large-scale, privacy-sensitive machine learning applications. To develop a rigorous framework to guarantee privacy for the succinct histogram problem, local differential privacy (LDP) has been utilized and shown promising results. To preserve data utility under LDP, which essentially works by intentionally adding noise to data, error-correcting codes naturally emerge as a promising tool for reliable information collection. This work presents the first practical $(\epsilon,\delta)$-LDP protocol for constructing succinct histograms using error-correcting codes. To this end, polar codes and their successive-cancellation list (SCL) decoding algorithms are leveraged as the underlying coding scheme. More specifically, our protocol introduces Gaussian-based perturbations to enable efficient soft decoding. Experiments demonstrate that our approach outperforms prior methods, particularly for items with low true frequencies, while maintaining similar frequency estimation accuracy.</li>
</ul>

<h3>Title: Log-Normal Multiplicative Dynamics for Stable Low-Precision Training of Large Networks</h3>
<ul>
<li><strong>Authors: </strong>Keigo Nishida, Eren Mehmet Kıral, Kenichi Bannai, Mohammad Emtiyaz Khan, Thomas Möllenhoff</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17768">https://arxiv.org/abs/2506.17768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17768">https://arxiv.org/pdf/2506.17768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17768]] Log-Normal Multiplicative Dynamics for Stable Low-Precision Training of Large Networks(https://arxiv.org/abs/2506.17768)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Studies in neuroscience have shown that biological synapses follow a log-normal distribution whose transitioning can be explained by noisy multiplicative dynamics. Biological networks can function stably even under dynamically fluctuating conditions arising due to unreliable synaptic transmissions. Here we ask: Is it possible to design similar multiplicative training in artificial neural networks? To answer this question, we derive a Bayesian learning rule that assumes log-normal posterior distributions over weights which gives rise to a new Log-Normal Multiplicative Dynamics (LMD) algorithm. The algorithm uses multiplicative updates with both noise and regularization applied multiplicatively. The method is as easy to implement as Adam and only requires one additional vector to store. Our results show that LMD achieves stable and accurate training-from-scratch under low-precision forward operations for Vision Transformer and GPT-2. These results suggest that multiplicative dynamics, a biological feature, may enable stable low-precision inference and learning on future energy-efficient hardware.</li>
</ul>

<h3>Title: PhysiX: A Foundation Model for Physics Simulations</h3>
<ul>
<li><strong>Authors: </strong>Tung Nguyen, Arsh Koneru, Shufan Li, Aditya grover</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17774">https://arxiv.org/abs/2506.17774</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17774">https://arxiv.org/pdf/2506.17774</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17774]] PhysiX: A Foundation Model for Physics Simulations(https://arxiv.org/abs/2506.17774)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Foundation models have achieved remarkable success across video, image, and language domains. By scaling up the number of parameters and training datasets, these models acquire generalizable world knowledge and often surpass task-specific approaches. However, such progress has yet to extend to the domain of physics simulation. A primary bottleneck is data scarcity: while millions of images, videos, and textual resources are readily available on the internet, the largest physics simulation datasets contain only tens of thousands of samples. This data limitation hinders the use of large models, as overfitting becomes a major concern. As a result, physics applications typically rely on small models, which struggle with long-range prediction due to limited context understanding. Additionally, unlike images, videos, or text-which typically exhibit fixed granularity-physics datasets often vary drastically in scale, amplifying the challenges of scaling up multitask training. We introduce PhysiX, the first large-scale foundation model for physics simulation. PhysiX is a 4.5B parameter autoregressive generative model. It uses a discrete tokenizer to encode physical processes at different scales into a sequence of discrete tokens, and employs an autoregressive next-token prediction objective to model such processes in the token space. To mitigate the rounding error in the discretization process, PhysiX incorporates a specialized refinement module. Through extensive experiments, we show that PhysiX effectively addresses the data bottleneck, outperforming task-specific baselines under comparable settings as well as the previous absolute state-of-the-art approaches on The Well benchmark. Our results indicate that knowledge learned from natural videos can be successfully transferred to physics simulation, and that joint training across diverse simulation tasks enables synergistic learning.</li>
</ul>

<h3>Title: Machine Learning Model Integration with Open World Temporal Logic for Process Automation</h3>
<ul>
<li><strong>Authors: </strong>Dyuman Aditya, Colton Payne, Mario Leiva, Paulo Shakarian</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.LO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17776">https://arxiv.org/abs/2506.17776</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17776">https://arxiv.org/pdf/2506.17776</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17776]] Machine Learning Model Integration with Open World Temporal Logic for Process Automation(https://arxiv.org/abs/2506.17776)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Recent advancements in Machine Learning (ML) have yielded powerful models capable of extracting structured information from diverse and complex data sources. However, a significant challenge lies in translating these perceptual or extractive outputs into actionable, reasoned decisions within complex operational workflows. To address these challenges, this paper introduces a novel approach that integrates the outputs from various machine learning models directly with the PyReason framework, an open-world temporal logic programming reasoning engine. PyReason's foundation in generalized annotated logic allows for the seamless incorporation of real-valued outputs (e.g., probabilities, confidence scores) from diverse ML models, treating them as truth intervals within its logical framework. Crucially, PyReason provides mechanisms, implemented in Python, to continuously poll ML model outputs, convert them into logical facts, and dynamically recompute the minimal model, ensuring real-tine adaptive decision-making. Furthermore, its native support for temporal reasoning, knowledge graph integration, and fully explainable interface traces enables sophisticated analysis over time-sensitive process data and existing organizational knowledge. By combining the strengths of perception and extraction from ML models with the logical deduction and transparency of PyReason, we aim to create a powerful system for automating complex processes. This integration finds utility across numerous domains, including manufacturing, healthcare, and business operations.</li>
</ul>

<h3>Title: Beyond instruction-conditioning, MoTE: Mixture of Task Experts for Multi-task Embedding Models</h3>
<ul>
<li><strong>Authors: </strong>Miguel Romero, Shuoyang Ding, Corey D. Barret, Georgiana Dinu, George Karypis</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17781">https://arxiv.org/abs/2506.17781</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17781">https://arxiv.org/pdf/2506.17781</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17781]] Beyond instruction-conditioning, MoTE: Mixture of Task Experts for Multi-task Embedding Models(https://arxiv.org/abs/2506.17781)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Dense embeddings are fundamental to modern machine learning systems, powering Retrieval-Augmented Generation (RAG), information retrieval, and representation learning. While instruction-conditioning has become the dominant approach for embedding specialization, its direct application to low-capacity models imposes fundamental representational constraints that limit the performance gains derived from specialization. In this paper, we analyze these limitations and introduce the Mixture of Task Experts (MoTE) transformer block, which leverages task-specialized parameters trained with Task-Aware Contrastive Learning (\tacl) to enhance the model ability to generate specialized embeddings. Empirical results show that MoTE achieves $64\%$ higher performance gains in retrieval datasets ($+3.27 \rightarrow +5.21$) and $43\%$ higher performance gains across all datasets ($+1.81 \rightarrow +2.60$). Critically, these gains are achieved without altering instructions, training data, inference time, or number of active parameters.</li>
</ul>

<h3>Title: Incorporating Rather Than Eliminating: Achieving Fairness for Skin Disease Diagnosis Through Group-Specific Expert</h3>
<ul>
<li><strong>Authors: </strong>Gelei Xu, Yuying Duan, Zheyuan Liu, Xueyang Li, Meng Jiang, Michael Lemmon, Wei Jin, Yiyu Shi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17787">https://arxiv.org/abs/2506.17787</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17787">https://arxiv.org/pdf/2506.17787</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17787]] Incorporating Rather Than Eliminating: Achieving Fairness for Skin Disease Diagnosis Through Group-Specific Expert(https://arxiv.org/abs/2506.17787)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>AI-based systems have achieved high accuracy in skin disease diagnostics but often exhibit biases across demographic groups, leading to inequitable healthcare outcomes and diminished patient trust. Most existing bias mitigation methods attempt to eliminate the correlation between sensitive attributes and diagnostic prediction, but those methods often degrade performance due to the lost of clinically relevant diagnostic cues. In this work, we propose an alternative approach that incorporates sensitive attributes to achieve fairness. We introduce FairMoE, a framework that employs layer-wise mixture-of-experts modules to serve as group-specific learners. Unlike traditional methods that rigidly assign data based on group labels, FairMoE dynamically routes data to the most suitable expert, making it particularly effective for handling cases near group boundaries. Experimental results show that, unlike previous fairness approaches that reduce performance, FairMoE achieves substantial accuracy improvements while preserving comparable fairness metrics.</li>
</ul>

<h3>Title: Multilingual Tokenization through the Lens of Indian Languages: Challenges and Insights</h3>
<ul>
<li><strong>Authors: </strong>N J Karthika, Maharaj Brahma, Rohit Saluja, Ganesh Ramakrishnan, Maunendra Sankar Desarkar</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17789">https://arxiv.org/abs/2506.17789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17789">https://arxiv.org/pdf/2506.17789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17789]] Multilingual Tokenization through the Lens of Indian Languages: Challenges and Insights(https://arxiv.org/abs/2506.17789)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Tokenization plays a pivotal role in multilingual NLP. However, existing tokenizers are often skewed towards high-resource languages, limiting their effectiveness for linguistically diverse and morphologically rich languages such as those in the Indian subcontinent. This paper presents a comprehensive intrinsic evaluation of tokenization strategies across 17 Indian languages. We quantify the trade-offs between bottom-up and top-down tokenizer algorithms (BPE and Unigram LM), effects of vocabulary sizes, and compare strategies of multilingual vocabulary construction such as joint and cluster-based training. We also show that extremely low-resource languages can benefit from tokenizers trained on related high-resource languages. Our study provides practical insights for building more fair, efficient, and linguistically informed tokenizers for multilingual NLP.</li>
</ul>

<h3>Title: A TRNG Implemented using a Soft-Data Based Sponge Function within a Unified Strong PUF Architecture</h3>
<ul>
<li><strong>Authors: </strong>Rachel Cazzola, Cyrus Minwalla, Calvin Chan, Jim Plusquellic</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17795">https://arxiv.org/abs/2506.17795</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17795">https://arxiv.org/pdf/2506.17795</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17795]] A TRNG Implemented using a Soft-Data Based Sponge Function within a Unified Strong PUF Architecture(https://arxiv.org/abs/2506.17795)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack, robust</a></li>
<li><strong>Abstract: </strong>Hardware security primitives including True Random Number Generators (TRNG) and Physical Unclonable Functions (PUFs) are central components to establishing a root of trust in microelectronic systems. In this paper, we propose a unified PUF-TRNG architecture that leverages a combination of the static entropy available in a strong PUF called the shift-register, reconvergent-fanout (SiRF) PUF, and the dynamic entropy associated with random noise present in path delay measurements. The SiRF PUF uses an engineered netlist containing a large number of paths as the source of static entropy, and a time-to-digital-converter (TDC) as a high-resolution, embedded instrument for measuring path delays, where measurement noise serves as the source of dynamic entropy. A novel data postprocessing algorithm is proposed based on a modified duplex sponge construction. The sponge function operates on soft data, i.e., fixed point data values, to add entropy to the ensuing random bit sequences and to increase the bit generation rate. A postprocessing algorithm for reproducing PUF-generated encryption keys is also used in the TRNG to protect against temperature voltage attacks designed to subvert the random characteristics in the bit sequences. The unified PUF-TRNG architecture is implemented across multiple instances of a ZYBO Z7-10 FPGA board and extensively tested with NIST SP 800-22, NIST SP 800-90B, AIS-31, and DieHarder test suites. Results indicate a stable and robust TRNG design with excellent min-entropy and a moderate data rate.</li>
</ul>

<h3>Title: AdRo-FL: Informed and Secure Client Selection for Federated Learning in the Presence of Adversarial Aggregator</h3>
<ul>
<li><strong>Authors: </strong>Md. Kamrul Hossain, Walid Aljoby, Anis Elgabli, Ahmed M. Abdelmoniem, Khaled A. Harras</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17805">https://arxiv.org/abs/2506.17805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17805">https://arxiv.org/pdf/2506.17805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17805]] AdRo-FL: Informed and Secure Client Selection for Federated Learning in the Presence of Adversarial Aggregator(https://arxiv.org/abs/2506.17805)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, protect, defense, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) enables collaborative learning without exposing clients' data. While clients only share model updates with the aggregator, studies reveal that aggregators can infer sensitive information from these updates. Secure Aggregation (SA) protects individual updates during transmission; however, recent work demonstrates a critical vulnerability where adversarial aggregators manipulate client selection to bypass SA protections, constituting a Biased Selection Attack (BSA). Although verifiable random selection prevents BSA, it precludes informed client selection essential for FL performance. We propose Adversarial Robust Federated Learning (AdRo-FL), which simultaneously enables: informed client selection based on client utility, and robust defense against BSA maintaining privacy-preserving aggregation. AdRo-FL implements two client selection frameworks tailored for distinct settings. The first framework assumes clients are grouped into clusters based on mutual trust, such as different branches of an organization. The second framework handles distributed clients where no trust relationships exist between them. For the cluster-oriented setting, we propose a novel defense against BSA by (1) enforcing a minimum client selection quota from each cluster, supervised by a cluster-head in every round, and (2) introducing a client utility function to prioritize efficient clients. For the distributed setting, we design a two-phase selection protocol: first, the aggregator selects the top clients based on our utility-driven ranking; then, a verifiable random function (VRF) ensures a BSA-resistant final selection. AdRo-FL also applies quantization to reduce communication overhead and sets strict transmission deadlines to improve energy efficiency. AdRo-FL achieves up to $1.85\times$ faster time-to-accuracy and up to $1.06\times$ higher final accuracy compared to insecure baselines.</li>
</ul>

<h3>Title: Reimagining Parameter Space Exploration with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Lijun Zhang, Xiao Liu, Hui Guan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17807">https://arxiv.org/abs/2506.17807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17807">https://arxiv.org/pdf/2506.17807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17807]] Reimagining Parameter Space Exploration with Diffusion Models(https://arxiv.org/abs/2506.17807)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Adapting neural networks to new tasks typically requires task-specific fine-tuning, which is time-consuming and reliant on labeled data. We explore a generative alternative that produces task-specific parameters directly from task identity, eliminating the need for task-specific training. To this end, we propose using diffusion models to learn the underlying structure of effective task-specific parameter space and synthesize parameters on demand. Once trained, the task-conditioned diffusion model can generate specialized weights directly from task identifiers. We evaluate this approach across three scenarios: generating parameters for a single seen task, for multiple seen tasks, and for entirely unseen tasks. Experiments show that diffusion models can generate accurate task-specific parameters and support multi-task interpolation when parameter subspaces are well-structured, but fail to generalize to unseen tasks, highlighting both the potential and limitations of this generative solution.</li>
</ul>

<h3>Title: Flatness After All?</h3>
<ul>
<li><strong>Authors: </strong>Neta Shoham, Liron Mor-Yosef, Haim Avron</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17809">https://arxiv.org/abs/2506.17809</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17809">https://arxiv.org/pdf/2506.17809</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17809]] Flatness After All?(https://arxiv.org/abs/2506.17809)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent literature has examined the relationship between the curvature of the loss function at minima and generalization, mainly in the context of overparameterized networks. A key observation is that "flat" minima tend to generalize better than "sharp" minima. While this idea is supported by empirical evidence, it has also been shown that deep networks can generalize even with arbitrary sharpness, as measured by either the trace or the spectral norm of the Hessian. In this paper, we argue that generalization could be assessed by measuring flatness using a soft rank measure of the Hessian. We show that when the common neural network model (neural network with exponential family negative log likelihood loss) is calibrated, and its prediction error and its confidence in the prediction are not correlated with the first and the second derivatives of the network's output, our measure accurately captures the asymptotic expected generalization gap. For non-calibrated models, we connect our flatness measure to the well-known Takeuchi Information Criterion and show that it still provides reliable estimates of generalization gaps for models that are not overly confident. Experimental results indicate that our approach offers a robust estimate of the generalization gap compared to baselines.</li>
</ul>

<h3>Title: Actionable Interpretability via Causal Hypergraphs: Unravelling Batch Size Effects in Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Zhongtian Sun, Anoushka Harit, Pietro Lio</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17826">https://arxiv.org/abs/2506.17826</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17826">https://arxiv.org/pdf/2506.17826</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17826]] Actionable Interpretability via Causal Hypergraphs: Unravelling Batch Size Effects in Deep Learning(https://arxiv.org/abs/2506.17826)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>While the impact of batch size on generalisation is well studied in vision tasks, its causal mechanisms remain underexplored in graph and text domains. We introduce a hypergraph-based causal framework, HGCNet, that leverages deep structural causal models (DSCMs) to uncover how batch size influences generalisation via gradient noise, minima sharpness, and model complexity. Unlike prior approaches based on static pairwise dependencies, HGCNet employs hypergraphs to capture higher-order interactions across training dynamics. Using do-calculus, we quantify direct and mediated effects of batch size interventions, providing interpretable, causally grounded insights into optimisation. Experiments on citation networks, biomedical text, and e-commerce reviews show that HGCNet outperforms strong baselines including GCN, GAT, PI-GNN, BERT, and RoBERTa. Our analysis reveals that smaller batch sizes causally enhance generalisation through increased stochasticity and flatter minima, offering actionable interpretability to guide training strategies in deep learning. This work positions interpretability as a driver of principled architectural and optimisation choices beyond post hoc analysis.</li>
</ul>

<h3>Title: Aligning Frozen LLMs by Reinforcement Learning: An Iterative Reweight-then-Optimize Approach</h3>
<ul>
<li><strong>Authors: </strong>Xinnan Zhang, Chenliang Li, Siliang Zeng, Jiaxiang Li, Zhongruo Wang, Kaixiang Lin, Songtao Lu, Alfredo Garcia, Mingyi Hong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17828">https://arxiv.org/abs/2506.17828</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17828">https://arxiv.org/pdf/2506.17828</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17828]] Aligning Frozen LLMs by Reinforcement Learning: An Iterative Reweight-then-Optimize Approach(https://arxiv.org/abs/2506.17828)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Aligning large language models (LLMs) with human preferences usually requires fine-tuning methods such as RLHF and DPO. These methods directly optimize the model parameters, so they cannot be used in test-time to improve model performance, nor are they applicable when the model weights are not accessible. In contrast, test-time methods sidestep weight updates by leveraging reward functions to guide and improve output quality. However, they incur high inference costs, and their one-shot guidance is often based on imperfect reward or value functions, leading to suboptimal outputs. In this work, we present a method named Iterative Reweight-then-Optimize (IRO), a reinforcement learning (RL) framework that performs RL-style alignment of the (frozen) base model without touching its parameters. During training, each iteration (i) samples candidates from the base model, (ii) resamples using current value functions, and (iii) trains a new lightweight value function that guides the next decoding pass. At test time, the value functions are used to guide the base model generation via a search-based optimization process. Notably, users can apply IRO to align a model on their own dataset, similar to OpenAI's reinforcement fine-tuning (RFT), but without requiring access to the model weights.</li>
</ul>

<h3>Title: Time-Contrastive Pretraining for In-Context Image and Video Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Assefa Wahd, Jacob Jaremko, Abhilash Hareendranathan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17837">https://arxiv.org/abs/2506.17837</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17837">https://arxiv.org/pdf/2506.17837</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17837]] Time-Contrastive Pretraining for In-Context Image and Video Segmentation(https://arxiv.org/abs/2506.17837)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) enables generalization to new tasks with minimal labeled data. However, mainstream ICL approaches rely on a gridding strategy, which lacks the flexibility required for vision applications. We introduce Temporal, a time-contrastive self-supervised objective that pretrains a prompt retriever for visual ICL, and formulate ICL as a video object segmentation (VOS) task. Temporal addresses key limitations of grid-based methods that restrict the number and resolution of context images. By reframing ICL as a VOS problem, our approach supports a variable number of context images while preserving their full resolution. To address the challenge of selecting optimal context sets for queries, we pretrain a prompt retriever on videos via self-supervised learning, where adjacent frames serve as positives and distant frames as negatives. For image segmentation, the prompt retriever selects relevant sequences that, when combined with the query, form coherent videos for VOS processing. For video segmentation, it identifies keyframes, predicts their masks using our ICL pipeline, and propagates them throughout the sequence. When evaluated on MICCAI FLARE 2022, our method achieves substantial improvements over baselines: 90.95% Dice score for image segmentation (10.64% improvement) and 92.45% Dice for video segmentation (14.88% improvement).</li>
</ul>

<h3>Title: Robust Foreground-Background Separation for Severely-Degraded Videos Using Convolutional Sparse Representation Modeling</h3>
<ul>
<li><strong>Authors: </strong>Kazuki Naganuma, Shunsuke Ono</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17838">https://arxiv.org/abs/2506.17838</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17838">https://arxiv.org/pdf/2506.17838</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17838]] Robust Foreground-Background Separation for Severely-Degraded Videos Using Convolutional Sparse Representation Modeling(https://arxiv.org/abs/2506.17838)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper proposes a foreground-background separation (FBS) method with a novel foreground model based on convolutional sparse representation (CSR). In order to analyze the dynamic and static components of videos acquired under undesirable conditions, such as hardware, environmental, and power limitations, it is essential to establish an FBS method that can handle videos with low frame rates and various types of noise. Existing FBS methods have two limitations that prevent us from accurately separating foreground and background components from such degraded videos. First, they only capture either data-specific or general features of the components. Second, they do not include explicit models for various types of noise to remove them in the FBS process. To this end, we propose a robust FBS method with a CSR-based foreground model. This model can adaptively capture specific spatial structures scattered in imaging data. Then, we formulate FBS as a constrained multiconvex optimization problem that incorporates CSR, functions that capture general features, and explicit noise characterization functions for multiple types of noise. Thanks to these functions, our method captures both data-specific and general features to accurately separate the components from various types of noise even under low frame rates. To obtain a solution of the optimization problem, we develop an algorithm that alternately solves its two convex subproblems by newly established algorithms. Experiments demonstrate the superiority of our method over existing methods using two types of degraded videos: infrared and microscope videos.</li>
</ul>

<h3>Title: Causal Spherical Hypergraph Networks for Modelling Social Uncertainty</h3>
<ul>
<li><strong>Authors: </strong>Anoushka Harit, Zhongtian Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17840">https://arxiv.org/abs/2506.17840</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17840">https://arxiv.org/pdf/2506.17840</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17840]] Causal Spherical Hypergraph Networks for Modelling Social Uncertainty(https://arxiv.org/abs/2506.17840)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Human social behaviour is governed by complex interactions shaped by uncertainty, causality, and group dynamics. We propose Causal Spherical Hypergraph Networks (Causal-SphHN), a principled framework for socially grounded prediction that jointly models higher-order structure, directional influence, and epistemic uncertainty. Our method represents individuals as hyperspherical embeddings and group contexts as hyperedges, capturing semantic and relational geometry. Uncertainty is quantified via Shannon entropy over von Mises-Fisher distributions, while temporal causal dependencies are identified using Granger-informed subgraphs. Information is propagated through an angular message-passing mechanism that respects belief dispersion and directional semantics. Experiments on SNARE (offline networks), PHEME (online discourse), and AMIGOS (multimodal affect) show that Causal-SphHN improves predictive accuracy, robustness, and calibration over strong baselines. Moreover, it enables interpretable analysis of influence patterns and social ambiguity. This work contributes a unified causal-geometric approach for learning under uncertainty in dynamic social environments.</li>
</ul>

<h3>Title: A Comparative Study of Open-Source Libraries for Synthetic Tabular Data Generation: SDV vs. SynthCity</h3>
<ul>
<li><strong>Authors: </strong>Cristian Del Gobbo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17847">https://arxiv.org/abs/2506.17847</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17847">https://arxiv.org/pdf/2506.17847</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17847]] A Comparative Study of Open-Source Libraries for Synthetic Tabular Data Generation: SDV vs. SynthCity(https://arxiv.org/abs/2506.17847)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>High-quality training data is critical to the performance of machine learning models, particularly Large Language Models (LLMs). However, obtaining real, high-quality data can be challenging, especially for smaller organizations and early-stage startups. Synthetic data generators provide a promising solution by replicating the statistical and structural properties of real data while preserving privacy and scalability. This study evaluates the performance of six tabular synthetic data generators from two widely used open-source libraries: SDV (Gaussian Copula, CTGAN, TVAE) and Synthicity (Bayesian Network, CTGAN, TVAE). Using a real-world dataset from the UCI Machine Learning Repository, comprising energy consumption and environmental variables from Belgium, we simulate a low-data regime by training models on only 1,000 rows. Each generator is then tasked with producing synthetic datasets under two conditions: a 1:1 (1,000 rows) and a 1:10 (10,000 rows) input-output ratio. Evaluation is conducted using two criteria: statistical similarity, measured via classical statistics and distributional metrics; and predictive utility, assessed using a "Train on Synthetic, Test on Real" approach with four regression models. While statistical similarity remained consistent across models in both scenarios, predictive utility declined notably in the 1:10 case. The Bayesian Network from Synthicity achieved the highest fidelity in both scenarios, while TVAE from SDV performed best in predictive tasks under the 1:10 setting. Although no significant performance gap was found between the two libraries, SDV stands out for its superior documentation and ease of use, making it more accessible for practitioners.</li>
</ul>

<h3>Title: Fetuses Made Simple: Modeling and Tracking of Fetal Shape and Pose</h3>
<ul>
<li><strong>Authors: </strong>Yingcheng Liu, Peiqi Wang, Sebastian Diaz, Esra Abaci Turk, Benjamin Billot, Patricia Ellen Grant, Polina Golland</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17858">https://arxiv.org/abs/2506.17858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17858">https://arxiv.org/pdf/2506.17858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17858]] Fetuses Made Simple: Modeling and Tracking of Fetal Shape and Pose(https://arxiv.org/abs/2506.17858)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Analyzing fetal body motion and shape is paramount in prenatal diagnostics and monitoring. Existing methods for fetal MRI analysis mainly rely on anatomical keypoints or volumetric body segmentations. Keypoints simplify body structure to facilitate motion analysis, but may ignore important details of full-body shape. Body segmentations capture complete shape information but complicate temporal analysis due to large non-local fetal movements. To address these limitations, we construct a 3D articulated statistical fetal body model based on the Skinned Multi-Person Linear Model (SMPL). Our algorithm iteratively estimates body pose in the image space and body shape in the canonical pose space. This approach improves robustness to MRI motion artifacts and intensity distortions, and reduces the impact of incomplete surface observations due to challenging fetal poses. We train our model on segmentations and keypoints derived from $19,816$ MRI volumes across $53$ subjects. Our model captures body shape and motion across time series and provides intuitive visualization. Furthermore, it enables automated anthropometric measurements traditionally difficult to obtain from segmentations and keypoints. When tested on unseen fetal body shapes, our method yields a surface alignment error of $3.2$ mm for $3$ mm MRI voxel size. To our knowledge, this represents the first 3D articulated statistical fetal body model, paving the way for enhanced fetal motion and shape analysis in prenatal diagnostics. The code is available at this https URL .</li>
</ul>

<h3>Title: In-Context Learning Strategies Emerge Rationally</h3>
<ul>
<li><strong>Authors: </strong>Daniel Wurgaft, Ekdeep Singh Lubana, Core Francisco Park, Hidenori Tanaka, Gautam Reddy, Noah D. Goodman</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17859">https://arxiv.org/abs/2506.17859</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17859">https://arxiv.org/pdf/2506.17859</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17859]] In-Context Learning Strategies Emerge Rationally(https://arxiv.org/abs/2506.17859)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Recent work analyzing in-context learning (ICL) has identified a broad set of strategies that describe model behavior in different experimental conditions. We aim to unify these findings by asking why a model learns these disparate strategies in the first place. Specifically, we start with the observation that when trained to learn a mixture of tasks, as is popular in the literature, the strategies learned by a model for performing ICL can be captured by a family of Bayesian predictors: a memorizing predictor, which assumes a discrete prior on the set of seen tasks, and a generalizing predictor, wherein the prior matches the underlying task distribution. Adopting the lens of rational analysis from cognitive science, where a learner's behavior is explained as an optimal adaptation to data given computational constraints, we develop a hierarchical Bayesian framework that almost perfectly predicts Transformer next token predictions throughout training without assuming access to its weights. Under this framework, pretraining is viewed as a process of updating the posterior probability of different strategies, and its inference-time behavior as a posterior-weighted average over these strategies' predictions. Our framework draws on common assumptions about neural network learning dynamics, which make explicit a tradeoff between loss and complexity among candidate strategies: beyond how well it explains the data, a model's preference towards implementing a strategy is dictated by its complexity. This helps explain well-known ICL phenomena, while offering novel predictions: e.g., we show a superlinear trend in the timescale for transition to memorization as task diversity is increased. Overall, our work advances an explanatory and predictive account of ICL grounded in tradeoffs between strategy loss and complexity.</li>
</ul>

<h3>Title: QueueEDIT: Structural Self-Correction for Sequential Model Editing in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Taolin Zhang, Haidong Kang, Dongyang Li, Qizhou Chen, Chengyu Wang Xiaofeng He, Richang Hong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17864">https://arxiv.org/abs/2506.17864</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17864">https://arxiv.org/pdf/2506.17864</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17864]] QueueEDIT: Structural Self-Correction for Sequential Model Editing in LLMs(https://arxiv.org/abs/2506.17864)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Recently, large language models (LLMs) have demonstrated impressive results but still suffer from hallucinations. Model editing has been proposed to correct factual inaccuracies in LLMs. A challenging case is sequential model editing (SME), which aims to rectify errors continuously rather than treating them as a one-time task. During SME, the general capabilities of LLMs can be negatively affected due to the introduction of new parameters. In this paper, we propose a queue-based self-correction framework (QueueEDIT) that not only enhances SME performance by addressing long-sequence dependency but also mitigates the impact of parameter bias on the general capabilities of LLMs. Specifically, we first introduce a structural mapping editing loss to map the triplets to the knowledge-sensitive neurons within the Transformer layers of LLMs. We then store the located parameters for each piece of edited knowledge in a queue and dynamically align previously edited parameters. In each edit, we select queue parameters most relevant to the currently located parameters to determine whether previous knowledge needs realignment. Irrelevant parameters in the queue are frozen, and we update the parameters at the queue head to the LLM to ensure they do not harm general abilities. Experiments show that our framework significantly outperforms strong baselines across various SME settings and maintains competitiveness in single-turn editing. The resulting LLMs also preserve high capabilities in general NLP tasks throughout the SME process.</li>
</ul>

<h3>Title: LASA: Enhancing SoC Security Verification with LLM-Aided Property Generation</h3>
<ul>
<li><strong>Authors: </strong>Dinesh Reddy Ankireddy, Sudipta Paria, Aritra Dasgupta, Sandip Ray, Swarup Bhunia</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17865">https://arxiv.org/abs/2506.17865</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17865">https://arxiv.org/pdf/2506.17865</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17865]] LASA: Enhancing SoC Security Verification with LLM-Aided Property Generation(https://arxiv.org/abs/2506.17865)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Ensuring the security of modern System-on-Chip (SoC) designs poses significant challenges due to increasing complexity and distributed assets across the intellectual property (IP) blocks. Formal property verification (FPV) provides the capability to model and validate design behaviors through security properties with model checkers; however, current practices require significant manual efforts to create such properties, making them time-consuming, costly, and error-prone. The emergence of Large Language Models (LLMs) has showcased remarkable proficiency across diverse domains, including HDL code generation and verification tasks. Current LLM-based techniques often produce vacuous assertions and lack efficient prompt generation, comprehensive verification, and bug detection. This paper presents LASA, a novel framework that leverages LLMs and retrieval-augmented generation (RAG) to produce non-vacuous security properties and SystemVerilog Assertions (SVA) from design specifications and related documentation for bus-based SoC designs. LASA integrates commercial EDA tool for FPV to generate coverage metrics and iteratively refines prompts through a feedback loop to enhance coverage. The effectiveness of LASA is validated through various open-source SoC designs, demonstrating high coverage values with an average of ~88\%, denoting comprehensive verification through efficient generation of security properties and SVAs. LASA also demonstrates bug detection capabilities, identifying five unique bugs in the buggy OpenTitan SoC from Hack@DAC'24 competition.</li>
</ul>

<h3>Title: Cross-modal State Space Modeling for Real-time RGB-thermal Wild Scene Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Xiaodong Guo, Zi'ang Lin, Luwen Hu, Zhihong Deng, Tong Liu, Wujie Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17869">https://arxiv.org/abs/2506.17869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17869">https://arxiv.org/pdf/2506.17869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17869]] Cross-modal State Space Modeling for Real-time RGB-thermal Wild Scene Semantic Segmentation(https://arxiv.org/abs/2506.17869)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>The integration of RGB and thermal data can significantly improve semantic segmentation performance in wild environments for field robots. Nevertheless, multi-source data processing (e.g. Transformer-based approaches) imposes significant computational overhead, presenting challenges for resource-constrained systems. To resolve this critical limitation, we introduced CM-SSM, an efficient RGB-thermal semantic segmentation architecture leveraging a cross-modal state space modeling (SSM) approach. Our framework comprises two key components. First, we introduced a cross-modal 2D-selective-scan (CM-SS2D) module to establish SSM between RGB and thermal modalities, which constructs cross-modal visual sequences and derives hidden state representations of one modality from the other. Second, we developed a cross-modal state space association (CM-SSA) module that effectively integrates global associations from CM-SS2D with local spatial features extracted through convolutional operations. In contrast with Transformer-based approaches, CM-SSM achieves linear computational complexity with respect to image resolution. Experimental results show that CM-SSM achieves state-of-the-art performance on the CART dataset with fewer parameters and lower computational cost. Further experiments on the PST900 dataset demonstrate its generalizability. Codes are available at this https URL.</li>
</ul>

<h3>Title: How Alignment Shrinks the Generative Horizon</h3>
<ul>
<li><strong>Authors: </strong>Chenghao Yang, Ari Holtzman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17871">https://arxiv.org/abs/2506.17871</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17871">https://arxiv.org/pdf/2506.17871</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17871]] How Alignment Shrinks the Generative Horizon(https://arxiv.org/abs/2506.17871)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Despite their impressive capabilities, aligned large language models (LLMs) often generate outputs that lack diversity. What drives this stability in the generation? We investigate this phenomenon through the lens of probability concentration in the model's output distribution. To quantify this concentration, we introduce the Branching Factor (BF) -- a token-invariant measure of the effective number of plausible next steps during generation. Our empirical analysis reveals two key findings: (1) BF often decreases as generation progresses, suggesting that LLMs become more predictable as they generate. (2) alignment tuning substantially sharpens the model's output distribution from the outset, reducing BF by nearly an order of magnitude (e.g., from 12 to 1.2) relative to base models. This stark reduction helps explain why aligned models often appear less sensitive to decoding strategies. Building on this insight, we find this stability has surprising implications for complex reasoning. Aligned Chain-of-Thought (CoT) models (e.g., DeepSeek-distilled models), for instance, leverage this effect; by generating longer reasoning chains, they push generation into later, more deterministic (lower BF) stages, resulting in more stable outputs. We hypothesize that alignment tuning does not fundamentally change a model's behavior, but instead steers it toward stylistic tokens (e.g., "Sure") that unlock low-entropy trajectories already present in the base model. This view is supported by nudging experiments, which show that prompting base models with such tokens can similarly reduce BF. Together, our findings establish BF as a powerful diagnostic for understanding and controlling LLM outputs - clarifying how alignment reduces variability, how CoT promotes stable generations, and how base models can be steered away from diversity.</li>
</ul>

<h3>Title: Decoding Federated Learning: The FedNAM+ Conformal Revolution</h3>
<ul>
<li><strong>Authors: </strong>Sree Bhargavi Balija, Amitash Nanda, Debashis Sahoo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17872">https://arxiv.org/abs/2506.17872</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17872">https://arxiv.org/pdf/2506.17872</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17872]] Decoding Federated Learning: The FedNAM+ Conformal Revolution(https://arxiv.org/abs/2506.17872)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate, interpretability</a></li>
<li><strong>Abstract: </strong>Federated learning has significantly advanced distributed training of machine learning models across decentralized data sources. However, existing frameworks often lack comprehensive solutions that combine uncertainty quantification, interpretability, and robustness. To address this, we propose FedNAM+, a federated learning framework that integrates Neural Additive Models (NAMs) with a novel conformal prediction method to enable interpretable and reliable uncertainty estimation. Our method introduces a dynamic level adjustment technique that utilizes gradient-based sensitivity maps to identify key input features influencing predictions. This facilitates both interpretability and pixel-wise uncertainty estimates. Unlike traditional interpretability methods such as LIME and SHAP, which do not provide confidence intervals, FedNAM+ offers visual insights into prediction reliability. We validate our approach through experiments on CT scan, MNIST, and CIFAR datasets, demonstrating high prediction accuracy with minimal loss (e.g., only 0.1% on MNIST), along with transparent uncertainty measures. Visual analysis highlights variable uncertainty intervals, revealing low-confidence regions where model performance can be improved with additional data. Compared to Monte Carlo Dropout, FedNAM+ delivers efficient and global uncertainty estimates with reduced computational overhead, making it particularly suitable for federated learning scenarios. Overall, FedNAM+ provides a robust, interpretable, and computationally efficient framework that enhances trust and transparency in decentralized predictive modeling.</li>
</ul>

<h3>Title: SurgVidLM: Towards Multi-grained Surgical Video Understanding with Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Guankun Wang, Wenjin Mo, Junyi Wang, Long Bai, Kun Yuan, Ming Hu, Jinlin Wu, Junjun He, Yiming Huang, Nicolas Padoy, Zhen Lei, Hongbin Liu, Nassir Navab, Hongliang Ren</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17873">https://arxiv.org/abs/2506.17873</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17873">https://arxiv.org/pdf/2506.17873</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17873]] SurgVidLM: Towards Multi-grained Surgical Video Understanding with Large Language Model(https://arxiv.org/abs/2506.17873)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in Multimodal Large Language Models have demonstrated great potential in the medical domain, facilitating users to understand surgical scenes and procedures. Beyond image-based methods, the exploration of Video Large Language Models (Vid-LLMs) has emerged as a promising avenue for capturing the complex sequences of information involved in surgery. However, there is still a lack of Vid-LLMs specialized for fine-grained surgical video understanding tasks, which is crucial for analyzing specific processes or details within a surgical procedure. To bridge this gap, we propose SurgVidLM, the first video language model designed to address both full and fine-grained surgical video comprehension. To train our SurgVidLM, we construct the SVU-31K dataset which consists of over 31K video-instruction pairs, enabling both holistic understanding and detailed analysis of surgical procedures. Furthermore, we introduce the StageFocus mechanism which is a two-stage framework performing the multi-grained, progressive understanding of surgical videos. We also develop the Multi-frequency Fusion Attention to effectively integrate low and high-frequency visual tokens, ensuring the retention of critical information. Experimental results demonstrate that SurgVidLM significantly outperforms state-of-the-art Vid-LLMs in both full and fine-grained video understanding tasks, showcasing its superior capability in capturing complex procedural contexts.</li>
</ul>

<h3>Title: Multi-turn Jailbreaking via Global Refinement and Active Fabrication</h3>
<ul>
<li><strong>Authors: </strong>Hua Tang, Lingyong Yan, Yukun Zhao, Shuaiqiang Wang, Jizhou Huang, Dawei Yin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17881">https://arxiv.org/abs/2506.17881</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17881">https://arxiv.org/pdf/2506.17881</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17881]] Multi-turn Jailbreaking via Global Refinement and Active Fabrication(https://arxiv.org/abs/2506.17881)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved exceptional performance across a wide range of tasks. However, they still pose significant safety risks due to the potential misuse for malicious purposes. Jailbreaks, which aim to elicit models to generate harmful content, play a critical role in identifying the underlying security threats. Recent jailbreaking primarily focuses on single-turn scenarios, while the more complicated multi-turn scenarios remain underexplored. Moreover, existing multi-turn jailbreaking techniques struggle to adapt to the evolving dynamics of dialogue as the interaction progresses. To address this limitation, we propose a novel multi-turn jailbreaking method that refines the jailbreaking path globally at each interaction. We also actively fabricate model responses to suppress safety-related warnings, thereby increasing the likelihood of eliciting harmful outputs in subsequent questions. Experimental results demonstrate the superior performance of our method compared with existing single-turn and multi-turn jailbreaking techniques across six state-of-the-art LLMs. Our code is publicly available at this https URL.</li>
</ul>

<h3>Title: Relation3D: Enhancing Relation Modeling for Point Cloud Instance Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Lu, Jiacheng Deng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17891">https://arxiv.org/abs/2506.17891</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17891">https://arxiv.org/pdf/2506.17891</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17891]] Relation3D: Enhancing Relation Modeling for Point Cloud Instance Segmentation(https://arxiv.org/abs/2506.17891)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>3D instance segmentation aims to predict a set of object instances in a scene, representing them as binary foreground masks with corresponding semantic labels. Currently, transformer-based methods are gaining increasing attention due to their elegant pipelines and superior predictions. However, these methods primarily focus on modeling the external relationships between scene features and query features through mask attention. They lack effective modeling of the internal relationships among scene features as well as between query features. In light of these disadvantages, we propose \textbf{Relation3D: Enhancing Relation Modeling for Point Cloud Instance Segmentation}. Specifically, we introduce an adaptive superpoint aggregation module and a contrastive learning-guided superpoint refinement module to better represent superpoint features (scene features) and leverage contrastive learning to guide the updates of these features. Furthermore, our relation-aware self-attention mechanism enhances the capabilities of modeling relationships between queries by incorporating positional and geometric relationships into the self-attention mechanism. Extensive experiments on the ScanNetV2, ScanNet++, ScanNet200 and S3DIS datasets demonstrate the superior performance of Relation3D.</li>
</ul>

<h3>Title: TROJAN-GUARD: Hardware Trojans Detection Using GNN in RTL Designs</h3>
<ul>
<li><strong>Authors: </strong>Kiran Thorat, Amit Hasan, Caiwen Ding, Zhijie Shi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17894">https://arxiv.org/abs/2506.17894</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17894">https://arxiv.org/pdf/2506.17894</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17894]] TROJAN-GUARD: Hardware Trojans Detection Using GNN in RTL Designs(https://arxiv.org/abs/2506.17894)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>Chip manufacturing is a complex process, and to achieve a faster time to market, an increasing number of untrusted third-party tools and designs from around the world are being utilized. The use of these untrusted third party intellectual properties (IPs) and tools increases the risk of adversaries inserting hardware trojans (HTs). The covert nature of HTs poses significant threats to cyberspace, potentially leading to severe consequences for national security, the economy, and personal privacy. Many graph neural network (GNN)-based HT detection methods have been proposed. However, they perform poorly on larger designs because they rely on training with smaller designs. Additionally, these methods do not explore different GNN models that are well-suited for HT detection or provide efficient training and inference processes. We propose a novel framework that generates graph embeddings for large designs (e.g., RISC-V) and incorporates various GNN models tailored for HT detection. Furthermore, our framework introduces domain-specific techniques for efficient training and inference by implementing model quantization. Model quantization reduces the precision of the weights, lowering the computational requirements, enhancing processing speed without significantly affecting detection accuracy. We evaluate our framework using a custom dataset, and our results demonstrate a precision of 98.66% and a recall (true positive rate) of 92.30%, highlighting the effectiveness and efficiency of our approach in detecting hardware trojans in large-scale chip designs</li>
</ul>

<h3>Title: EgoWorld: Translating Exocentric View to Egocentric View using Rich Exocentric Observations</h3>
<ul>
<li><strong>Authors: </strong>Junho Park, Andrew Sangwoo Ye, Taein Kwon</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17896">https://arxiv.org/abs/2506.17896</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17896">https://arxiv.org/pdf/2506.17896</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17896]] EgoWorld: Translating Exocentric View to Egocentric View using Rich Exocentric Observations(https://arxiv.org/abs/2506.17896)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Egocentric vision is essential for both human and machine visual understanding, particularly in capturing the detailed hand-object interactions needed for manipulation tasks. Translating third-person views into first-person views significantly benefits augmented reality (AR), virtual reality (VR) and robotics applications. However, current exocentric-to-egocentric translation methods are limited by their dependence on 2D cues, synchronized multi-view settings, and unrealistic assumptions such as necessity of initial egocentric frame and relative camera poses during inference. To overcome these challenges, we introduce EgoWorld, a novel two-stage framework that reconstructs an egocentric view from rich exocentric observations, including projected point clouds, 3D hand poses, and textual descriptions. Our approach reconstructs a point cloud from estimated exocentric depth maps, reprojects it into the egocentric perspective, and then applies diffusion-based inpainting to produce dense, semantically coherent egocentric images. Evaluated on the H2O and TACO datasets, EgoWorld achieves state-of-the-art performance and demonstrates robust generalization to new objects, actions, scenes, and subjects. Moreover, EgoWorld shows promising results even on unlabeled real-world examples.</li>
</ul>

<h3>Title: PostAlign: Multimodal Grounding as a Corrective Lens for MLLMs</h3>
<ul>
<li><strong>Authors: </strong>Yixuan Wu, Yang Zhang, Jian Wu, Philip Torr, Jindong Gu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17901">https://arxiv.org/abs/2506.17901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17901">https://arxiv.org/pdf/2506.17901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17901]] PostAlign: Multimodal Grounding as a Corrective Lens for MLLMs(https://arxiv.org/abs/2506.17901)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) excel in vision-language tasks, such as image captioning and visual question answering. However, they often suffer from over-reliance on spurious correlations, primarily due to linguistic priors that distract the model from leveraging actual visual information. To address these issues, we introduce MMGrounded-PostAlign, a post-multimodal alignment framework designed to enhance the visual understanding capabilities and mitigate the hallucinations of MLLMs. Our framework incorporates a multimodal grounding module for both visual grounding, which identifies the referred object in the image, and textual grounding, which generates the rationale for the final answer, ensuring that outputs are anchored in both visual and textual evidence. To mitigate the hallucinations, we introduce a negative rejection mechanism in the visual grounding module to distinguish grounded entities from non-existent objects influenced by linguistic biases. On the textual grounding side, we propose a selective reasoning mechanism that adjusts the model's reasoning strategy based on query complexity. Extensive evaluations are conducted on benchmarks such as POPE, HaloQuest, VQAv2, MME, and MMBench showing significant improvements in fine-grained visual understanding and hallucination suppression.</li>
</ul>

<h3>Title: Cause-Effect Driven Optimization for Robust Medical Visual Question Answering with Language Biases</h3>
<ul>
<li><strong>Authors: </strong>Huanjia Zhu, Yishu Liu, Xiaozhao Fang, Guangming Lu, Bingzhi Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17903">https://arxiv.org/abs/2506.17903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17903">https://arxiv.org/pdf/2506.17903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17903]] Cause-Effect Driven Optimization for Robust Medical Visual Question Answering with Language Biases(https://arxiv.org/abs/2506.17903)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Existing Medical Visual Question Answering (Med-VQA) models often suffer from language biases, where spurious correlations between question types and answer categories are inadvertently established. To address these issues, we propose a novel Cause-Effect Driven Optimization framework called CEDO, that incorporates three well-established mechanisms, i.e., Modality-driven Heterogeneous Optimization (MHO), Gradient-guided Modality Synergy (GMS), and Distribution-adapted Loss Rescaling (DLR), for comprehensively mitigating language biases from both causal and effectual perspectives. Specifically, MHO employs adaptive learning rates for specific modalities to achieve heterogeneous optimization, thus enhancing robust reasoning capabilities. Additionally, GMS leverages the Pareto optimization method to foster synergistic interactions between modalities and enforce gradient orthogonality to eliminate bias updates, thereby mitigating language biases from the effect side, i.e., shortcut bias. Furthermore, DLR is designed to assign adaptive weights to individual losses to ensure balanced learning across all answer categories, effectively alleviating language biases from the cause side, i.e., imbalance biases within datasets. Extensive experiments on multiple traditional and bias-sensitive benchmarks consistently demonstrate the robustness of CEDO over state-of-the-art competitors.</li>
</ul>

<h3>Title: Feedback Driven Multi Stereo Vision System for Real-Time Event Analysis</h3>
<ul>
<li><strong>Authors: </strong>Mohamed Benkedadra, Matei Mancas, Sidi Ahmed Mahmoudi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17910">https://arxiv.org/abs/2506.17910</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17910">https://arxiv.org/pdf/2506.17910</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17910]] Feedback Driven Multi Stereo Vision System for Real-Time Event Analysis(https://arxiv.org/abs/2506.17910)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>2D cameras are often used in interactive systems. Other systems like gaming consoles provide more powerful 3D cameras for short range depth sensing. Overall, these cameras are not reliable in large, complex environments. In this work, we propose a 3D stereo vision based pipeline for interactive systems, that is able to handle both ordinary and sensitive applications, through robust scene understanding. We explore the fusion of multiple 3D cameras to do full scene reconstruction, which allows for preforming a wide range of tasks, like event recognition, subject tracking, and notification. Using possible feedback approaches, the system can receive data from the subjects present in the environment, to learn to make better decisions, or to adapt to completely new environments. Throughout the paper, we introduce the pipeline and explain our preliminary experimentation and results. Finally, we draw the roadmap for the next steps that need to be taken, in order to get this pipeline into production</li>
</ul>

<h3>Title: PlanMoGPT: Flow-Enhanced Progressive Planning for Text to Motion Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Chuhao Jin, Haosen Li, Bingzi Zhang, Che Liu, Xiting Wang, Ruihua Song, Wenbing Huang, Ying Qin, Fuzheng Zhang, Di Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17912">https://arxiv.org/abs/2506.17912</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17912">https://arxiv.org/pdf/2506.17912</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17912]] PlanMoGPT: Flow-Enhanced Progressive Planning for Text to Motion Synthesis(https://arxiv.org/abs/2506.17912)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have enabled breakthroughs in many multimodal generation tasks, but a significant performance gap still exists in text-to-motion generation, where LLM-based methods lag far behind non-LLM methods. We identify the granularity of motion tokenization as a critical bottleneck: fine-grained tokenization induces local dependency issues, where LLMs overemphasize short-term coherence at the expense of global semantic alignment, while coarse-grained tokenization sacrifices motion details. To resolve this issue, we propose PlanMoGPT, an LLM-based framework integrating progressive planning and flow-enhanced fine-grained motion tokenization. First, our progressive planning mechanism leverages LLMs' autoregressive capabilities to hierarchically generate motion tokens by starting from sparse global plans and iteratively refining them into full sequences. Second, our flow-enhanced tokenizer doubles the downsampling resolution and expands the codebook size by eight times, minimizing detail loss during discretization, while a flow-enhanced decoder recovers motion nuances. Extensive experiments on text-to-motion benchmarks demonstrate that it achieves state-of-the-art performance, improving FID scores by 63.8% (from 0.380 to 0.141) on long-sequence generation while enhancing motion diversity by 49.9% compared to existing methods. The proposed framework successfully resolves the diversity-quality trade-off that plagues current non-LLM approaches, establishing new standards for text-to-motion generation.</li>
</ul>

<h3>Title: Permutation Equivariant Model-based Offline Reinforcement Learning for Auto-bidding</h3>
<ul>
<li><strong>Authors: </strong>Zhiyu Mou, Miao Xu, Wei Chen, Rongquan Bai, Chuan Yu, Jian Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17919">https://arxiv.org/abs/2506.17919</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17919">https://arxiv.org/pdf/2506.17919</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17919]] Permutation Equivariant Model-based Offline Reinforcement Learning for Auto-bidding(https://arxiv.org/abs/2506.17919)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) for auto-bidding has shifted from using simplistic offline simulators (Simulation-based RL Bidding, SRLB) to offline RL on fixed real datasets (Offline RL Bidding, ORLB). However, ORLB policies are limited by the dataset's state space coverage, offering modest gains. While SRLB expands state coverage, its simulator-reality gap risks misleading policies. This paper introduces Model-based RL Bidding (MRLB), which learns an environment model from real data to bridge this gap. MRLB trains policies using both real and model-generated data, expanding state coverage beyond ORLB. To ensure model reliability, we propose: 1) A permutation equivariant model architecture for better generalization, and 2) A robust offline Q-learning method that pessimistically penalizes model errors. These form the Permutation Equivariant Model-based Offline RL (PE-MORL) algorithm. Real-world experiments show that PE-MORL outperforms state-of-the-art auto-bidding methods.</li>
</ul>

<h3>Title: IDAL: Improved Domain Adaptive Learning for Natural Images Dataset</h3>
<ul>
<li><strong>Authors: </strong>Ravi Kant Gupta, Shounak Das, Amit Sethi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17931">https://arxiv.org/abs/2506.17931</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17931">https://arxiv.org/pdf/2506.17931</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17931]] IDAL: Improved Domain Adaptive Learning for Natural Images Dataset(https://arxiv.org/abs/2506.17931)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We present a novel approach for unsupervised domain adaptation (UDA) for natural images. A commonly-used objective for UDA schemes is to enhance domain alignment in representation space even if there is a domain shift in the input space. Existing adversarial domain adaptation methods may not effectively align different domains of multimodal distributions associated with classification problems. Our approach has two main features. Firstly, its neural architecture uses the deep structure of ResNet and the effective separation of scales of feature pyramidal network (FPN) to work with both content and style features. Secondly, it uses a combination of a novel loss function and judiciously selected existing loss functions to train the network architecture. This tailored combination is designed to address challenges inherent to natural images, such as scale, noise, and style shifts, that occur on top of a multi-modal (multi-class) distribution. The combined loss function not only enhances model accuracy and robustness on the target domain but also speeds up training convergence. Our proposed UDA scheme generalizes better than state-of-the-art for CNN-based methods on Office-Home, Office-31, and VisDA-2017 datasets and comaparable for DomainNet dataset.</li>
</ul>

<h3>Title: Cost-Effective Optimization and Implementation of the CRT-Paillier Decryption Algorithm for Enhanced Performance</h3>
<ul>
<li><strong>Authors: </strong>Zhengwu Huang, Ding Deng, Pengyue Sun, Guangfu Sun, Xiaomei Tang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17935">https://arxiv.org/abs/2506.17935</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17935">https://arxiv.org/pdf/2506.17935</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17935]] Cost-Effective Optimization and Implementation of the CRT-Paillier Decryption Algorithm for Enhanced Performance(https://arxiv.org/abs/2506.17935)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>To address the privacy protection problem in cloud computing, privacy enhancement techniques such as the Paillier additive homomorphism algorithm are receiving widespread attention. Paillier algorithm allows addition and scalar multiplication operations in dencrypted state, which can effectively protect privacy. However, its computational efficiency is limited by complex modulo operations due to the ciphertext expansion followed by encryption. To accelerate its decryption operation, the Chinese Remainder Theorem (CRT) is often used to optimize these modulo operations, which lengthens the decryption computation chain in turn. To address this issue, we propose an eCRT-Paillier decryption algorithm that shortens the decryption computation chain by combining precomputed parameters and eliminating extra judgment operations introduced by Montgomery modular multiplications. These two improvements reduce 50% modular multiplications and 60% judgment operations in the postprocessing of the CRT-Paillier decryption algorithm. Based on these improvements, we propose a highly parallel full-pipeline architecture to eliminate stalls caused by multiplier reuse in traditional modular exponentiation operations. This architecture also adopts some optimizations such as simplifying modular exponentiation units by dividing the exponent into segments and parallelizing data flow by multi-core instantiation. Finally, a high-throughput and efficient Paillier accelerator named MESA was implemented on the Xilinx Virtex-7 FPGA for evaluation, which can complete a decryption using 2048-bit key within 0.577ms under 100 MHz clock frequency. Compared to prior works, MESA demonstrates a throughput improvement of 1.16 to 313.21 under identical conditions, also with enhancements in area efficiency for LUT, DSP, and FF of 3.32 to 117.55, 1.49 to 1.64, and 2.94 to 9.94, respectively.</li>
</ul>

<h3>Title: GEMeX-ThinkVG: Towards Thinking with Visual Grounding in Medical VQA via Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Bo Liu, Xiangyu Zhao, Along He, Yidi Chen, Huazhu Fu, Xiao-Ming Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17939">https://arxiv.org/abs/2506.17939</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17939">https://arxiv.org/pdf/2506.17939</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17939]] GEMeX-ThinkVG: Towards Thinking with Visual Grounding in Medical VQA via Reinforcement Learning(https://arxiv.org/abs/2506.17939)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Medical visual question answering aims to support clinical decision-making by enabling models to answer natural language questions based on medical images. While recent advances in multi-modal learning have significantly improved performance, current methods still suffer from limited answer reliability and poor interpretability, impairing the ability of clinicians and patients to understand and trust model-generated answers. To address this, this work first proposes a Thinking with Visual Grounding (ThinkVG) dataset wherein the answer generation is decomposed into intermediate reasoning steps that explicitly ground relevant visual regions of the medical image, thereby providing fine-grained explainability. Furthermore, we introduce a novel verifiable reward mechanism for reinforcement learning to guide post-training, improving the alignment between the model's reasoning process and its final answer. Remarkably, our method achieves comparable performance using only one-eighth of the training data, demonstrating the efficiency and effectiveness of the proposal. The dataset is available at this https URL.</li>
</ul>

<h3>Title: SegChange-R1:Augmented Reasoning for Remote Sensing Change Detection via Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Fei Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17944">https://arxiv.org/abs/2506.17944</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17944">https://arxiv.org/pdf/2506.17944</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17944]] SegChange-R1:Augmented Reasoning for Remote Sensing Change Detection via Large Language Models(https://arxiv.org/abs/2506.17944)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Remote sensing change detection is widely used in a variety of fields such as urban planning, terrain and geomorphology analysis, and environmental monitoring, mainly by analyzing the significant change differences of features (e.g., building changes) in the same spatial region at different time phases. In this paper, we propose a large language model (LLM) augmented inference approach (SegChange-R1), which enhances the detection capability by integrating textual descriptive information and aims at guiding the model to segment the more interested change regions, thus accelerating the convergence speed. Moreover, we design a spatial transformation module (BEV) based on linear attention, which solves the problem of modal misalignment in change detection by unifying features from different temporal perspectives onto the BEV space. In addition, we construct the first dataset for building change detection from UAV viewpoints (DVCD ), and our experiments on four widely-used change detection datasets show a significant improvement over existing methods. The code and pre-trained models are available in this https URL.</li>
</ul>

<h3>Title: Scatter-Based Innovation Propagation in Large Language Models for Multi-Stage Process Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Hong Su</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17949">https://arxiv.org/abs/2506.17949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17949">https://arxiv.org/pdf/2506.17949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17949]] Scatter-Based Innovation Propagation in Large Language Models for Multi-Stage Process Adaptation(https://arxiv.org/abs/2506.17949)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) exhibit strong capabilities in reproducing and extending patterns observed during pretraining but often struggle to generalize novel ideas beyond their original context. This paper addresses the challenge of applying such localized innovations - introduced at a specific stage or component - to other parts of a multi-stage process. We propose a scatter-based innovation expansion model (innovation scatter model) that guides the LLM through a four-step process: (1) identifying the core innovation by comparing the user's input with its surrounding context, (2) generalizing the innovation by removing references to specific stages or components, (3) determining whether the generalized innovation applies to a broader scope beyond the original stage, and (4) systematically applying it to other structurally similar stages using the LLM. This model leverages structural redundancy across stages to improve the applicability of novel ideas. Verification results demonstrate that the innovation scatter model enables LLMs to extend innovations across structurally similar stages, thereby enhancing generalization and reuse.</li>
</ul>

<h3>Title: A Comprehensive Graph Framework for Question Answering with Mode-Seeking Preference Alignment</h3>
<ul>
<li><strong>Authors: </strong>Quanwei Tang, Sophia Yat Mei Lee, Junshuang Wu, Dong Zhang, Shoushan Li, Erik Cambria, Guodong Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17951">https://arxiv.org/abs/2506.17951</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17951">https://arxiv.org/pdf/2506.17951</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17951]] A Comprehensive Graph Framework for Question Answering with Mode-Seeking Preference Alignment(https://arxiv.org/abs/2506.17951)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in retrieval-augmented generation (RAG) have enhanced large language models in question answering by integrating external knowledge. However, challenges persist in achieving global understanding and aligning responses with human ethical and quality preferences. To address these issues, we propose GraphMPA, a comprehensive graph-based framework with mode-seeking preference alignment. Our approach constructs a hierarchical document graph using a general similarity measurement, mimicking human cognitive processes for information understanding and synthesis. Additionally, we introduce mode-seeking preference optimization to better align model outputs with human preferences through probability-matching constraints. Extensive experiments on six datasets demonstrate the effectiveness of our \href{this https URL}{GraphMPA}.</li>
</ul>

<h3>Title: Mobile Image Analysis Application for Mantoux Skin Test</h3>
<ul>
<li><strong>Authors: </strong>Liong Gele, Tan Chye Cheah</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17954">https://arxiv.org/abs/2506.17954</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17954">https://arxiv.org/pdf/2506.17954</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17954]] Mobile Image Analysis Application for Mantoux Skin Test(https://arxiv.org/abs/2506.17954)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>This paper presents a newly developed mobile application designed to diagnose Latent Tuberculosis Infection (LTBI) using the Mantoux Skin Test (TST). Traditional TST methods often suffer from low follow-up return rates, patient discomfort, and subjective manual interpretation, particularly with the ball-point pen method, leading to misdiagnosis and delayed treatment. Moreover, previous developed mobile applications that used 3D reconstruction, this app utilizes scaling stickers as reference objects for induration measurement. This mobile application integrates advanced image processing technologies, including ARCore, and machine learning algorithms such as DeepLabv3 for robust image segmentation and precise measurement of skin indurations indicative of LTBI. The system employs an edge detection algorithm to enhance accuracy. The application was evaluated against standard clinical practices, demonstrating significant improvements in accuracy and reliability. This innovation is crucial for effective tuberculosis management, especially in resource-limited regions. By automating and standardizing TST evaluations, the application enhances the accessibility and efficiency of TB di-agnostics. Future work will focus on refining machine learning models, optimizing measurement algorithms, expanding functionalities to include comprehensive patient data management, and enhancing ARCore's performance across various lighting conditions and operational settings.</li>
</ul>

<h3>Title: ELMAR: Enhancing LiDAR Detection with 4D Radar Motion Awareness and Cross-modal Uncertainty</h3>
<ul>
<li><strong>Authors: </strong>Xiangyuan Peng, Miao Tang, Huawei Sun, Bierzynski Kay, Lorenzo Servadei, Robert Wille</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17958">https://arxiv.org/abs/2506.17958</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17958">https://arxiv.org/pdf/2506.17958</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17958]] ELMAR: Enhancing LiDAR Detection with 4D Radar Motion Awareness and Cross-modal Uncertainty(https://arxiv.org/abs/2506.17958)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>LiDAR and 4D radar are widely used in autonomous driving and robotics. While LiDAR provides rich spatial information, 4D radar offers velocity measurement and remains robust under adverse conditions. As a result, increasing studies have focused on the 4D radar-LiDAR fusion method to enhance the perception. However, the misalignment between different modalities is often overlooked. To address this challenge and leverage the strengths of both modalities, we propose a LiDAR detection framework enhanced by 4D radar motion status and cross-modal uncertainty. The object movement information from 4D radar is first captured using a Dynamic Motion-Aware Encoding module during feature extraction to enhance 4D radar predictions. Subsequently, the instance-wise uncertainties of bounding boxes are estimated to mitigate the cross-modal misalignment and refine the final LiDAR predictions. Extensive experiments on the View-of-Delft (VoD) dataset highlight the effectiveness of our method, achieving state-of-the-art performance with the mAP of 74.89% in the entire area and 88.70% within the driving corridor while maintaining a real-time inference speed of 30.02 FPS.</li>
</ul>

<h3>Title: Adapting Vision-Language Models for Evaluating World Models</h3>
<ul>
<li><strong>Authors: </strong>Mariya Hendriksen, Tabish Rashid, David Bignell, Raluca Georgescu, Abdelhak Lemkhenter, Katja Hofmann, Sam Devlin, Sarah Parisot</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17967">https://arxiv.org/abs/2506.17967</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17967">https://arxiv.org/pdf/2506.17967</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17967]] Adapting Vision-Language Models for Evaluating World Models(https://arxiv.org/abs/2506.17967)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>World models -- generative models that simulate environment dynamics conditioned on past observations and actions -- are gaining prominence in planning, simulation, and embodied AI. However, evaluating their rollouts remains a fundamental challenge, requiring fine-grained, temporally grounded assessment of action alignment and semantic consistency -- capabilities not captured by existing metrics. Vision-Language Models (VLMs) have shown promise as automatic evaluators of generative content due to their strong multimodal reasoning abilities. Yet, their use in fine-grained, temporally sensitive evaluation tasks remains limited and requires targeted adaptation. We introduce a evaluation protocol targeting two recognition tasks -- action recognition and character recognition -- each assessed across binary, multiple-choice, and open-ended formats. To support this, we present UNIVERSE (UNIfied Vision-language Evaluator for Rollouts in Simulated Environments), a method for adapting VLMs to rollout evaluation under data and compute constraints. We conduct a large-scale study comparing full, partial, and parameter-efficient finetuning across task formats, context lengths, sampling strategies, and data compositions. The resulting unified evaluator matches the performance of task-specific baselines using a single checkpoint. Human studies confirm strong alignment with human judgments, establishing UNIVERSE as a scalable, semantics-aware evaluator for world models.</li>
</ul>

<h3>Title: BPCLIP: A Bottom-up Image Quality Assessment from Distortion to Semantics Based on CLIP</h3>
<ul>
<li><strong>Authors: </strong>Chenyue Song, Chen Hui, Wei Zhang, Haiqi Zhu, Shaohui Liu, Hong Huang, Feng Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17969">https://arxiv.org/abs/2506.17969</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17969">https://arxiv.org/pdf/2506.17969</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17969]] BPCLIP: A Bottom-up Image Quality Assessment from Distortion to Semantics Based on CLIP(https://arxiv.org/abs/2506.17969)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Image Quality Assessment (IQA) aims to evaluate the perceptual quality of images based on human subjective perception. Existing methods generally combine multiscale features to achieve high performance, but most rely on straightforward linear fusion of these features, which may not adequately capture the impact of distortions on semantic content. To address this, we propose a bottom-up image quality assessment approach based on the Contrastive Language-Image Pre-training (CLIP, a recently proposed model that aligns images and text in a shared feature space), named BPCLIP, which progressively extracts the impact of low-level distortions on high-level semantics. Specifically, we utilize an encoder to extract multiscale features from the input image and introduce a bottom-up multiscale cross attention module designed to capture the relationships between shallow and deep features. In addition, by incorporating 40 image quality adjectives across six distinct dimensions, we enable the pre-trained CLIP text encoder to generate representations of the intrinsic quality of the image, thereby strengthening the connection between image quality perception and human language. Our method achieves superior results on most public Full-Reference (FR) and No-Reference (NR) IQA benchmarks, while demonstrating greater robustness.</li>
</ul>

<h3>Title: Trustworthy Efficient Communication for Distributed Learning using LQ-SGD Algorithm</h3>
<ul>
<li><strong>Authors: </strong>Hongyang Li, Lincen Bai, Caesar Wu, Mohammed Chadli, Said Mammar, Pascal Bouvry</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17974">https://arxiv.org/abs/2506.17974</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17974">https://arxiv.org/pdf/2506.17974</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17974]] Trustworthy Efficient Communication for Distributed Learning using LQ-SGD Algorithm(https://arxiv.org/abs/2506.17974)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We propose LQ-SGD (Low-Rank Quantized Stochastic Gradient Descent), an efficient communication gradient compression algorithm designed for distributed training. LQ-SGD further develops on the basis of PowerSGD by incorporating the low-rank approximation and log-quantization techniques, which drastically reduce the communication overhead, while still ensuring the convergence speed of training and model accuracy. In addition, LQ-SGD and other compression-based methods show stronger resistance to gradient inversion than traditional SGD, providing a more robust and efficient optimization path for distributed learning systems.</li>
</ul>

<h3>Title: Enabling PSO-Secure Synthetic Data Sharing Using Diversity-Aware Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Mischa Dombrowski, Bernhard Kainz</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17975">https://arxiv.org/abs/2506.17975</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17975">https://arxiv.org/pdf/2506.17975</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17975]] Enabling PSO-Secure Synthetic Data Sharing Using Diversity-Aware Diffusion Models(https://arxiv.org/abs/2506.17975)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, protect, diffusion</a></li>
<li><strong>Abstract: </strong>Synthetic data has recently reached a level of visual fidelity that makes it nearly indistinguishable from real data, offering great promise for privacy-preserving data sharing in medical imaging. However, fully synthetic datasets still suffer from significant limitations: First and foremost, the legal aspect of sharing synthetic data is often neglected and data regulations, such as the GDPR, are largley ignored. Secondly, synthetic models fall short of matching the performance of real data, even for in-domain downstream applications. Recent methods for image generation have focused on maximising image diversity instead of fidelity solely to improve the mode coverage and therefore the downstream performance of synthetic data. In this work, we shift perspective and highlight how maximizing diversity can also be interpreted as protecting natural persons from being singled out, which leads to predicate singling-out (PSO) secure synthetic datasets. Specifically, we propose a generalisable framework for training diffusion models on personal data which leads to unpersonal synthetic datasets achieving performance within one percentage point of real-data models while significantly outperforming state-of-the-art methods that do not ensure privacy. Our code is available at this https URL.</li>
</ul>

<h3>Title: Secure User-friendly Blockchain Modular Wallet Design Using Android & OP-TEE</h3>
<ul>
<li><strong>Authors: </strong>Seongjin Kim, Sanguk Yun, Jungho Jang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.17988">https://arxiv.org/abs/2506.17988</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.17988">https://arxiv.org/pdf/2506.17988</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.17988]] Secure User-friendly Blockchain Modular Wallet Design Using Android & OP-TEE(https://arxiv.org/abs/2506.17988)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect</a></li>
<li><strong>Abstract: </strong>Emerging crypto economies still hemorrhage digital assets because legacy wallets leak private keys at almost every layer of the software stack, from user-space libraries to kernel memory dumps. This paper solves that twin crisis of security and interoperability by re-imagining key management as a platform-level service anchored in ARM TrustZone through OP-TEE. Our architecture fractures the traditional monolithic Trusted Application into per-chain modules housed in a multi-tenant TA store, finally breaking OP-TEE's single-binary ceiling. A cryptographically sealed firmware-over-the-air pipeline welds each TA set to an Android system image, enabling hot-swap updates while Verified Boot enforces rollback protection. Every package carries a chained signature developer first, registry second so even a compromised supply chain cannot smuggle malicious code past the Secure World's RSA-PSS gatekeeper. Inside the TEE, strict inter-TA isolation, cache partitioning, and GP-compliant crypto APIs ensure secrets never bleed across trust boundaries or timing domains. The Rich Execution Environment can interact only via hardware-mediated Secure Monitor Calls, collapsing the surface exposed to malware in Android space. End-users enjoy a single polished interface yet can install or retire Bitcoin, Ethereum, Solana, or tomorrow's chain with one tap, shrinking both storage footprint and audit scope. For auditors, the composition model slashes duplicated verification effort by quarantining blockchain logic inside narrowly scoped modules that share formally specified interfaces. Our threat analysis spans six adversary layers and shows how the design neutralizes REE malware sniffing, OTA injection, and cross-module side channels without exotic hardware. A reference implementation on AOSP exports a Wallet Manager HAL, custom SELinux domains, and a CI/CD pipeline that vet community modules before release. The result is not merely another hardware wallet but a programmable substrate that can evolve at the velocity of the blockchain ecosystem. By welding radical extensibility to hardware-anchored assurance, the platform closes the security-usability gap that has long stymied mass-market self-custody. We posit that modular TEEs are the missing OS primitive for Web3, much as virtual memory unlocked multi-tasking in classical computing. Together, these contributions sketch a blueprint for multi-chain asset management that is auditable, resilient, and poised for global deployment.</li>
</ul>

<h3>Title: OSDMamba: Enhancing Oil Spill Detection from Remote Sensing Images Using Selective State Space Model</h3>
<ul>
<li><strong>Authors: </strong>Shuaiyu Chen, Fu Wang, Peng Ren, Chunbo Luo, Zeyu Fu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18006">https://arxiv.org/abs/2506.18006</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18006">https://arxiv.org/pdf/2506.18006</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18006]] OSDMamba: Enhancing Oil Spill Detection from Remote Sensing Images Using Selective State Space Model(https://arxiv.org/abs/2506.18006)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Semantic segmentation is commonly used for Oil Spill Detection (OSD) in remote sensing images. However, the limited availability of labelled oil spill samples and class imbalance present significant challenges that can reduce detection accuracy. Furthermore, most existing methods, which rely on convolutional neural networks (CNNs), struggle to detect small oil spill areas due to their limited receptive fields and inability to effectively capture global contextual information. This study explores the potential of State-Space Models (SSMs), particularly Mamba, to overcome these limitations, building on their recent success in vision applications. We propose OSDMamba, the first Mamba-based architecture specifically designed for oil spill detection. OSDMamba leverages Mamba's selective scanning mechanism to effectively expand the model's receptive field while preserving critical details. Moreover, we designed an asymmetric decoder incorporating ConvSSM and deep supervision to strengthen multi-scale feature fusion, thereby enhancing the model's sensitivity to minority class samples. Experimental results show that the proposed OSDMamba achieves state-of-the-art performance, yielding improvements of 8.9% and 11.8% in OSD across two publicly available datasets.</li>
</ul>

<h3>Title: Imputation of Longitudinal Data Using GANs: Challenges and Implications for Classification</h3>
<ul>
<li><strong>Authors: </strong>Sharon Torao Pingi, Md Abul Bashar, Richi Nayak</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18007">https://arxiv.org/abs/2506.18007</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18007">https://arxiv.org/pdf/2506.18007</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18007]] Imputation of Longitudinal Data Using GANs: Challenges and Implications for Classification(https://arxiv.org/abs/2506.18007)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Longitudinal data is commonly utilised across various domains, such as health, biomedical, education and survey studies. This ubiquity has led to a rise in statistical, machine and deep learning-based methods for Longitudinal Data Classification (LDC). However, the intricate nature of the data, characterised by its multi-dimensionality, causes instance-level heterogeneity and temporal correlations that add to the complexity of longitudinal data analysis. Additionally, LDC accuracy is often hampered by the pervasiveness of missing values in longitudinal data. Despite ongoing research that draw on the generative power and utility of Generative Adversarial Networks (GANs) to address the missing data problem, critical considerations include statistical assumptions surrounding longitudinal data and missingness within it, as well as other data-level challenges like class imbalance and mixed data types that impact longitudinal data imputation (LDI) and the subsequent LDC process in GANs. This paper provides a comprehensive overview of how GANs have been applied in LDI, with a focus whether GANS have adequately addressed fundamental assumptions about the data from a LDC perspective. We propose a categorisation of main approaches to GAN-based LDI, highlight strengths and limitations of methods, identify key research trends, and provide promising future directions. Our findings indicate that while GANs show great potential for LDI to improve usability and quality of longitudinal data for tasks like LDC, there is need for more versatile approaches that can handle the wider spectrum of challenges presented by longitudinal data with missing values. By synthesising current knowledge and identifying critical research gaps, this survey aims to guide future research efforts in developing more effective GAN-based solutions to address LDC challenges.</li>
</ul>

<h3>Title: Probing the Embedding Space of Transformers via Minimal Token Perturbations</h3>
<ul>
<li><strong>Authors: </strong>Eddie Conti, Alejandro Astruc, Alvaro Parafita, Axel Brando</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18011">https://arxiv.org/abs/2506.18011</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18011">https://arxiv.org/pdf/2506.18011</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18011]] Probing the Embedding Space of Transformers via Minimal Token Perturbations(https://arxiv.org/abs/2506.18011)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Understanding how information propagates through Transformer models is a key challenge for interpretability. In this work, we study the effects of minimal token perturbations on the embedding space. In our experiments, we analyze the frequency of which tokens yield to minimal shifts, highlighting that rare tokens usually lead to larger shifts. Moreover, we study how perturbations propagate across layers, demonstrating that input information is increasingly intermixed in deeper layers. Our findings validate the common assumption that the first layers of a model can be used as proxies for model explanations. Overall, this work introduces the combination of token perturbations and shifts on the embedding space as a powerful tool for model interpretability.</li>
</ul>

<h3>Title: Generalization under Byzantine & Poisoning Attacks: Tight Stability Bounds in Robust Distributed Learning</h3>
<ul>
<li><strong>Authors: </strong>Thomas Boudou, Batiste Le Bars, Nirupam Gupta, Aurélien Bellet</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18020">https://arxiv.org/abs/2506.18020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18020">https://arxiv.org/pdf/2506.18020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18020]] Generalization under Byzantine & Poisoning Attacks: Tight Stability Bounds in Robust Distributed Learning(https://arxiv.org/abs/2506.18020)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Robust distributed learning algorithms aim to maintain good performance in distributed and federated settings, even in the presence of misbehaving workers. Two primary threat models have been studied: Byzantine attacks, where misbehaving workers can send arbitrarily corrupted updates, and data poisoning attacks, where misbehavior is limited to manipulation of local training data. While prior work has shown comparable optimization error under both threat models, a fundamental question remains open: How do these threat models impact generalization? Empirical evidence suggests a gap between the two threat models, yet it remains unclear whether it is fundamental or merely an artifact of suboptimal attacks. In this work, we present the first theoretical investigation into this problem, formally showing that Byzantine attacks are intrinsically more harmful to generalization than data poisoning. Specifically, we prove that: (i) under data poisoning, the uniform algorithmic stability of a robust distributed learning algorithm, with optimal optimization error, degrades by an additive factor of $\varTheta ( \frac{f}{n-f} )$, with $f$ the number of misbehaving workers out of $n$; and (ii) In contrast, under Byzantine attacks, the degradation is in $\mathcal{O} \big( \sqrt{ \frac{f}{n-2f}} \big)$.This difference in stability leads to a generalization error gap that is especially significant as $f$ approaches its maximum value $\frac{n}{2}$.</li>
</ul>

<h3>Title: On the Robustness of Human-Object Interaction Detection against Distribution Shift</h3>
<ul>
<li><strong>Authors: </strong>Chi Xie, Shuang Liang, Jie Li, Feng Zhu, Rui Zhao, Yichen Wei, Shengjie Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18021">https://arxiv.org/abs/2506.18021</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18021">https://arxiv.org/pdf/2506.18021</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18021]] On the Robustness of Human-Object Interaction Detection against Distribution Shift(https://arxiv.org/abs/2506.18021)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Human-Object Interaction (HOI) detection has seen substantial advances in recent years. However, existing works focus on the standard setting with ideal images and natural distribution, far from practical scenarios with inevitable distribution shifts. This hampers the practical applicability of HOI detection. In this work, we investigate this issue by benchmarking, analyzing, and enhancing the robustness of HOI detection models under various distribution shifts. We start by proposing a novel automated approach to create the first robustness evaluation benchmark for HOI detection. Subsequently, we evaluate more than 40 existing HOI detection models on this benchmark, showing their insufficiency, analyzing the features of different frameworks, and discussing how the robustness in HOI is different from other tasks. With the insights from such analyses, we propose to improve the robustness of HOI detection methods through: (1) a cross-domain data augmentation integrated with mixup, and (2) a feature fusion strategy with frozen vision foundation models. Both are simple, plug-and-play, and applicable to various methods. Our experimental results demonstrate that the proposed approach significantly increases the robustness of various methods, with benefits on standard benchmarks, too. The dataset and code will be released.</li>
</ul>

<h3>Title: PDF Retrieval Augmented Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Thi Thu Uyen Hoang, Viet Anh Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18027">https://arxiv.org/abs/2506.18027</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18027">https://arxiv.org/pdf/2506.18027</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18027]] PDF Retrieval Augmented Question Answering(https://arxiv.org/abs/2506.18027)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>This paper presents an advancement in Question-Answering (QA) systems using a Retrieval Augmented Generation (RAG) framework to enhance information extraction from PDF files. Recognizing the richness and diversity of data within PDFs--including text, images, vector diagrams, graphs, and tables--poses unique challenges for existing QA systems primarily designed for textual content. We seek to develop a comprehensive RAG-based QA system that will effectively address complex multimodal questions, where several data types are combined in the query. This is mainly achieved by refining approaches to processing and integrating non-textual elements in PDFs into the RAG framework to derive precise and relevant answers, as well as fine-tuning large language models to better adapt to our system. We provide an in-depth experimental evaluation of our solution, demonstrating its capability to extract accurate information that can be applied to different types of content across PDFs. This work not only pushes the boundaries of retrieval-augmented QA systems but also lays a foundation for further research in multimodal data integration and processing.</li>
</ul>

<h3>Title: Why Do Some Language Models Fake Alignment While Others Don't?</h3>
<ul>
<li><strong>Authors: </strong>Abhay Sheshadri, John Hughes, Julian Michael, Alex Mallen, Arun Jose, Janus, Fabien Roger</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18032">https://arxiv.org/abs/2506.18032</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18032">https://arxiv.org/pdf/2506.18032</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18032]] Why Do Some Language Models Fake Alignment While Others Don't?(https://arxiv.org/abs/2506.18032)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Alignment faking in large language models presented a demonstration of Claude 3 Opus and Claude 3.5 Sonnet selectively complying with a helpful-only training objective to prevent modification of their behavior outside of training. We expand this analysis to 25 models and find that only 5 (Claude 3 Opus, Claude 3.5 Sonnet, Llama 3 405B, Grok 3, Gemini 2.0 Flash) comply with harmful queries more when they infer they are in training than when they infer they are in deployment. First, we study the motivations of these 5 models. Results from perturbing details of the scenario suggest that only Claude 3 Opus's compliance gap is primarily and consistently motivated by trying to keep its goals. Second, we investigate why many chat models don't fake alignment. Our results suggest this is not entirely due to a lack of capabilities: many base models fake alignment some of the time, and post-training eliminates alignment-faking for some models and amplifies it for others. We investigate 5 hypotheses for how post-training may suppress alignment faking and find that variations in refusal behavior may account for a significant portion of differences in alignment faking.</li>
</ul>

<h3>Title: Pre-Trained LLM is a Semantic-Aware and Generalizable Segmentation Booster</h3>
<ul>
<li><strong>Authors: </strong>Fenghe Tang, Wenxin Ma, Zhiyang He, Xiaodong Tao, Zihang Jiang, S. Kevin Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18034">https://arxiv.org/abs/2506.18034</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18034">https://arxiv.org/pdf/2506.18034</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18034]] Pre-Trained LLM is a Semantic-Aware and Generalizable Segmentation Booster(https://arxiv.org/abs/2506.18034)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model, segmentation</a></li>
<li><strong>Abstract: </strong>With the advancement of Large Language Model (LLM) for natural language processing, this paper presents an intriguing finding: a frozen pre-trained LLM layer can process visual tokens for medical image segmentation tasks. Specifically, we propose a simple hybrid structure that integrates a pre-trained, frozen LLM layer within the CNN encoder-decoder segmentation framework (LLM4Seg). Surprisingly, this design improves segmentation performance with a minimal increase in trainable parameters across various modalities, including ultrasound, dermoscopy, polypscopy, and CT scans. Our in-depth analysis reveals the potential of transferring LLM's semantic awareness to enhance segmentation tasks, offering both improved global understanding and better local modeling capabilities. The improvement proves robust across different LLMs, validated using LLaMA and DeepSeek.</li>
</ul>

<h3>Title: Markov-Enhanced Clustering for Long Document Summarization: Tackling the 'Lost in the Middle' Challenge with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Aziz Amari (1), Mohamed Achref Ben Ammar (1) ((1) National Institute of Applied Science and Technology (INSAT), University of Carthage, Tunis, Tunisia)</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18036">https://arxiv.org/abs/2506.18036</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18036">https://arxiv.org/pdf/2506.18036</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18036]] Markov-Enhanced Clustering for Long Document Summarization: Tackling the 'Lost in the Middle' Challenge with Large Language Models(https://arxiv.org/abs/2506.18036)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid expansion of information from diverse sources has heightened the need for effective automatic text summarization, which condenses documents into shorter, coherent texts. Summarization methods generally fall into two categories: extractive, which selects key segments from the original text, and abstractive, which generates summaries by rephrasing the content coherently. Large language models have advanced the field of abstractive summarization, but they are resourceintensive and face significant challenges in retaining key information across lengthy documents, which we call being "lost in the middle". To address these issues, we propose a hybrid summarization approach that combines extractive and abstractive techniques. Our method splits the document into smaller text chunks, clusters their vector embeddings, generates a summary for each cluster that represents a key idea in the document, and constructs the final summary by relying on a Markov chain graph when selecting the semantic order of ideas.</li>
</ul>

<h3>Title: CmFNet: Cross-modal Fusion Network for Weakly-supervised Segmentation of Medical Images</h3>
<ul>
<li><strong>Authors: </strong>Dongdong Meng, Sheng Li, Hao Wu, Suqing Tian, Wenjun Ma, Guoping Wang, Xueqing Yan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18042">https://arxiv.org/abs/2506.18042</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18042">https://arxiv.org/pdf/2506.18042</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18042]] CmFNet: Cross-modal Fusion Network for Weakly-supervised Segmentation of Medical Images(https://arxiv.org/abs/2506.18042)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Accurate automatic medical image segmentation relies on high-quality, dense annotations, which are costly and time-consuming. Weakly supervised learning provides a more efficient alternative by leveraging sparse and coarse annotations instead of dense, precise ones. However, segmentation performance degradation and overfitting caused by sparse annotations remain key challenges. To address these issues, we propose CmFNet, a novel 3D weakly supervised cross-modal medical image segmentation approach. CmFNet consists of three main components: a modality-specific feature learning network, a cross-modal feature learning network, and a hybrid-supervised learning strategy. Specifically, the modality-specific feature learning network and the cross-modal feature learning network effectively integrate complementary information from multi-modal images, enhancing shared features across modalities to improve segmentation performance. Additionally, the hybrid-supervised learning strategy guides segmentation through scribble supervision, intra-modal regularization, and inter-modal consistency, modeling spatial and contextual relationships while promoting feature alignment. Our approach effectively mitigates overfitting, delivering robust segmentation results. It excels in segmenting both challenging small tumor regions and common anatomical structures. Extensive experiments on a clinical cross-modal nasopharyngeal carcinoma (NPC) dataset (including CT and MR imaging) and the publicly available CT Whole Abdominal Organ dataset (WORD) show that our approach outperforms state-of-the-art weakly supervised methods. In addition, our approach also outperforms fully supervised methods when full annotation is used. Our approach can facilitate clinical therapy and benefit various specialists, including physicists, radiologists, pathologists, and oncologists.</li>
</ul>

<h3>Title: TAB: Unified Benchmarking of Time Series Anomaly Detection Methods</h3>
<ul>
<li><strong>Authors: </strong>Xiangfei Qiu, Zhe Li, Wanghui Qiu, Shiyan Hu, Lekui Zhou, Xingjian Wu, Zhengyu Li, Chenjuan Guo, Aoying Zhou, Zhenli Sheng, Jilin Hu, Christian S. Jensen, Bin Yang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18046">https://arxiv.org/abs/2506.18046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18046">https://arxiv.org/pdf/2506.18046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18046]] TAB: Unified Benchmarking of Time Series Anomaly Detection Methods(https://arxiv.org/abs/2506.18046)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Time series anomaly detection (TSAD) plays an important role in many domains such as finance, transportation, and healthcare. With the ongoing instrumentation of reality, more time series data will be available, leading also to growing demands for TSAD. While many TSAD methods already exist, new and better methods are still desirable. However, effective progress hinges on the availability of reliable means of evaluating new methods and comparing them with existing methods. We address deficiencies in current evaluation procedures related to datasets and experimental settings and protocols. Specifically, we propose a new time series anomaly detection benchmark, called TAB. First, TAB encompasses 29 public multivariate datasets and 1,635 univariate time series from different domains to facilitate more comprehensive evaluations on diverse datasets. Second, TAB covers a variety of TSAD methods, including Non-learning, Machine learning, Deep learning, LLM-based, and Time-series pre-trained methods. Third, TAB features a unified and automated evaluation pipeline that enables fair and easy evaluation of TSAD methods. Finally, we employ TAB to evaluate existing TSAD methods and report on the outcomes, thereby offering a deeper insight into the performance of these methods. Besides, all datasets and code are available at this https URL.</li>
</ul>

<h3>Title: Mechanistic Interpretability in the Presence of Architectural Obfuscation</h3>
<ul>
<li><strong>Authors: </strong>Marcos Florencio, Thomas Barton</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18053">https://arxiv.org/abs/2506.18053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18053">https://arxiv.org/pdf/2506.18053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18053]] Mechanistic Interpretability in the Presence of Architectural Obfuscation(https://arxiv.org/abs/2506.18053)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, robust, interpretability</a></li>
<li><strong>Abstract: </strong>Architectural obfuscation - e.g., permuting hidden-state tensors, linearly transforming embedding tables, or remapping tokens - has recently gained traction as a lightweight substitute for heavyweight cryptography in privacy-preserving large-language-model (LLM) inference. While recent work has shown that these techniques can be broken under dedicated reconstruction attacks, their impact on mechanistic interpretability has not been systematically studied. In particular, it remains unclear whether scrambling a network's internal representations truly thwarts efforts to understand how the model works, or simply relocates the same circuits to an unfamiliar coordinate system. We address this gap by analyzing a GPT-2-small model trained from scratch with a representative obfuscation map. Assuming the obfuscation map is private and the original basis is hidden (mirroring an honest-but-curious server), we apply logit-lens attribution, causal path-patching, and attention-head ablation to locate and manipulate known circuits. Our findings reveal that obfuscation dramatically alters activation patterns within attention heads yet preserves the layer-wise computational graph. This disconnect hampers reverse-engineering of user prompts: causal traces lose their alignment with baseline semantics, and token-level logit attributions become too noisy to reconstruct. At the same time, feed-forward and residual pathways remain functionally intact, suggesting that obfuscation degrades fine-grained interpretability without compromising top-level task performance. These results establish quantitative evidence that architectural obfuscation can simultaneously (i) retain global model behaviour and (ii) impede mechanistic analyses of user-specific content. By mapping where interpretability breaks down, our study provides guidance for future privacy defences and for robustness-aware interpretability tooling.</li>
</ul>

<h3>Title: Deep Supervised LSTM for 3D morphology estimation from Multi-View RGB Images of Wheat Spikes</h3>
<ul>
<li><strong>Authors: </strong>Olivia Zumsteg, Nico Graf, Aaron Haeusler, Norbert Kirchgessner, Nicola Storni, Lukas Roth, Andreas Hund</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18060">https://arxiv.org/abs/2506.18060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18060">https://arxiv.org/pdf/2506.18060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18060]] Deep Supervised LSTM for 3D morphology estimation from Multi-View RGB Images of Wheat Spikes(https://arxiv.org/abs/2506.18060)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Estimating three-dimensional morphological traits from two-dimensional RGB images presents inherent challenges due to the loss of depth information, projection distortions, and occlusions under field conditions. In this work, we explore multiple approaches for non-destructive volume estimation of wheat spikes, using RGB image sequences and structured-light 3D scans as ground truth references. Due to the complex geometry of the spikes, we propose a neural network approach for volume estimation in 2D images, employing a transfer learning pipeline that combines DINOv2, a self-supervised Vision Transformer, with a unidirectional Long Short-Term Memory (LSTM) network. By using deep supervision, the model is able to learn more robust intermediate representations, which enhances its generalisation ability across varying evaluation sequences. We benchmark our model against two conventional baselines: a 2D area-based projection and a geometric reconstruction using axis-aligned cross-sections. Our deep supervised model achieves a mean absolute percentage error (MAPE) of 6.46% on six-view indoor images, outperforming the area (9.36%) and geometric (13.98%) baselines. Fine-tuning the model on field-based single-image data enables domain adaptation, yielding a MAPE of 10.82%. We demonstrate that object shape significantly impacts volume prediction accuracy, with irregular geometries such as wheat spikes posing greater challenges for geometric methods compared to our deep learning approach.</li>
</ul>

<h3>Title: Training-free Test-time Improvement for Explainable Medical Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Hangzhou He, Jiachen Tang, Lei Zhu, Kaiwen Li, Yanye Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18070">https://arxiv.org/abs/2506.18070</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18070">https://arxiv.org/pdf/2506.18070</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18070]] Training-free Test-time Improvement for Explainable Medical Image Classification(https://arxiv.org/abs/2506.18070)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Deep learning-based medical image classification techniques are rapidly advancing in medical image analysis, making it crucial to develop accurate and trustworthy models that can be efficiently deployed across diverse clinical scenarios. Concept Bottleneck Models (CBMs), which first predict a set of explainable concepts from images and then perform classification based on these concepts, are increasingly being adopted for explainable medical image classification. However, the inherent explainability of CBMs introduces new challenges when deploying trained models to new environments. Variations in imaging protocols and staining methods may induce concept-level shifts, such as alterations in color distribution and scale. Furthermore, since CBM training requires explicit concept annotations, fine-tuning models solely with image-level labels could compromise concept prediction accuracy and faithfulness - a critical limitation given the high cost of acquiring expert-annotated concept labels in medical domains. To address these challenges, we propose a training-free confusion concept identification strategy. By leveraging minimal new data (e.g., 4 images per class) with only image-level labels, our approach enhances out-of-domain performance without sacrificing source domain accuracy through two key operations: masking misactivated confounding concepts and amplifying under-activated discriminative concepts. The efficacy of our method is validated on both skin and white blood cell images. Our code is available at: this https URL.</li>
</ul>

<h3>Title: Distributionally robust minimization in meta-learning for system identification</h3>
<ul>
<li><strong>Authors: </strong>Matteo Rufolo, Dario Piga, Marco Forgione</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18074">https://arxiv.org/abs/2506.18074</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18074">https://arxiv.org/pdf/2506.18074</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18074]] Distributionally robust minimization in meta-learning for system identification(https://arxiv.org/abs/2506.18074)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Meta learning aims at learning how to solve tasks, and thus it allows to estimate models that can be quickly adapted to new scenarios. This work explores distributionally robust minimization in meta learning for system identification. Standard meta learning approaches optimize the expected loss, overlooking task variability. We use an alternative approach, adopting a distributionally robust optimization paradigm that prioritizes high-loss tasks, enhancing performance in worst-case scenarios. Evaluated on a meta model trained on a class of synthetic dynamical systems and tested in both in-distribution and out-of-distribution settings, the proposed approach allows to reduce failures in safety-critical applications.</li>
</ul>

<h3>Title: TEM^3-Learning: Time-Efficient Multimodal Multi-Task Learning for Advanced Assistive Driving</h3>
<ul>
<li><strong>Authors: </strong>Wenzhuo Liu, Yicheng Qiao, Zhen Wang, Qiannan Guo, Zilong Chen, Meihua Zhou, Xinran Li, Letian Wang, Zhiwei Li, Huaping Liu, Wenshuo Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18084">https://arxiv.org/abs/2506.18084</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18084">https://arxiv.org/pdf/2506.18084</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18084]] TEM^3-Learning: Time-Efficient Multimodal Multi-Task Learning for Advanced Assistive Driving(https://arxiv.org/abs/2506.18084)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Multi-task learning (MTL) can advance assistive driving by exploring inter-task correlations through shared representations. However, existing methods face two critical limitations: single-modality constraints limiting comprehensive scene understanding and inefficient architectures impeding real-time deployment. This paper proposes TEM^3-Learning (Time-Efficient Multimodal Multi-task Learning), a novel framework that jointly optimizes driver emotion recognition, driver behavior recognition, traffic context recognition, and vehicle behavior recognition through a two-stage architecture. The first component, the mamba-based multi-view temporal-spatial feature extraction subnetwork (MTS-Mamba), introduces a forward-backward temporal scanning mechanism and global-local spatial attention to efficiently extract low-cost temporal-spatial features from multi-view sequential images. The second component, the MTL-based gated multimodal feature integrator (MGMI), employs task-specific multi-gating modules to adaptively highlight the most relevant modality features for each task, effectively alleviating the negative transfer problem in MTL. Evaluation on the AIDE dataset, our proposed model achieves state-of-the-art accuracy across all four tasks, maintaining a lightweight architecture with fewer than 6 million parameters and delivering an impressive 142.32 FPS inference speed. Rigorous ablation studies further validate the effectiveness of the proposed framework and the independent contributions of each module. The code is available on this https URL.</li>
</ul>

<h3>Title: Federated Learning-Based Data Collaboration Method for Enhancing Edge Cloud AI System Security Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Huaiying Luo, Cheng Ji</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18087">https://arxiv.org/abs/2506.18087</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18087">https://arxiv.org/pdf/2506.18087</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18087]] Federated Learning-Based Data Collaboration Method for Enhancing Edge Cloud AI System Security Using Large Language Models(https://arxiv.org/abs/2506.18087)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, protect, robust, federate, large language model</a></li>
<li><strong>Abstract: </strong>With the widespread application of edge computing and cloud systems in AI-driven applications, how to maintain efficient performance while ensuring data privacy has become an urgent security issue. This paper proposes a federated learning-based data collaboration method to improve the security of edge cloud AI systems, and use large-scale language models (LLMs) to enhance data privacy protection and system robustness. Based on the existing federated learning framework, this method introduces a secure multi-party computation protocol, which optimizes the data aggregation and encryption process between distributed nodes by using LLM to ensure data privacy and improve system efficiency. By combining advanced adversarial training techniques, the model enhances the resistance of edge cloud AI systems to security threats such as data leakage and model poisoning. Experimental results show that the proposed method is 15% better than the traditional federated learning method in terms of data protection and model robustness.</li>
</ul>

<h3>Title: Evaluating Prompt-Based and Fine-Tuned Approaches to Czech Anaphora Resolution</h3>
<ul>
<li><strong>Authors: </strong>Patrik Stano, Aleš Horák</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18091">https://arxiv.org/abs/2506.18091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18091">https://arxiv.org/pdf/2506.18091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18091]] Evaluating Prompt-Based and Fine-Tuned Approaches to Czech Anaphora Resolution(https://arxiv.org/abs/2506.18091)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Anaphora resolution plays a critical role in natural language understanding, especially in morphologically rich languages like Czech. This paper presents a comparative evaluation of two modern approaches to anaphora resolution on Czech text: prompt engineering with large language models (LLMs) and fine-tuning compact generative models. Using a dataset derived from the Prague Dependency Treebank, we evaluate several instruction-tuned LLMs, including Mistral Large 2 and Llama 3, using a series of prompt templates. We compare them against fine-tuned variants of the mT5 and Mistral models that we trained specifically for Czech anaphora resolution. Our experiments demonstrate that while prompting yields promising few-shot results (up to 74.5% accuracy), the fine-tuned models, particularly mT5-large, outperform them significantly, achieving up to 88% accuracy while requiring fewer computational resources. We analyze performance across different anaphora types, antecedent distances, and source corpora, highlighting key strengths and trade-offs of each approach.</li>
</ul>

<h3>Title: ShareGPT-4o-Image: Aligning Multimodal Models with GPT-4o-Level Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Junying Chen, Zhenyang Cai, Pengcheng Chen, Shunian Chen, Ke Ji, Xidong Wang, Yunjin Yang, Benyou Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18095">https://arxiv.org/abs/2506.18095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18095">https://arxiv.org/pdf/2506.18095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18095]] ShareGPT-4o-Image: Aligning Multimodal Models with GPT-4o-Level Image Generation(https://arxiv.org/abs/2506.18095)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in multimodal generative models have unlocked photorealistic, instruction-aligned image generation, yet leading systems like GPT-4o-Image remain proprietary and inaccessible. To democratize these capabilities, we present ShareGPT-4o-Image, the first dataset comprising 45K text-to-image and 46K text-and-image-to-image data, all synthesized using GPT-4o's image generation capabilities for distilling its advanced image generation abilities. Leveraging this dataset, we develop Janus-4o, a multimodal large language model capable of both text-to-image and text-and-image-to-image generation. Janus-4o not only significantly improves text-to-image generation over its predecessor, Janus-Pro, but also newly supports text-and-image-to-image generation. Notably, it achieves impressive performance in text-and-image-to-image generation from scratch, using only 91K synthetic samples and 6 hours of training on an 8 A800-GPU machine. We hope the release of ShareGPT-4o-Image and Janus-4o will foster open research in photorealistic, instruction-aligned image generation.</li>
</ul>

<h3>Title: Optimizing Resource Allocation and Energy Efficiency in Federated Fog Computing for IoT</h3>
<ul>
<li><strong>Authors: </strong>Taimoor Ahmad, Anas Ali</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18100">https://arxiv.org/abs/2506.18100</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18100">https://arxiv.org/pdf/2506.18100</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18100]] Optimizing Resource Allocation and Energy Efficiency in Federated Fog Computing for IoT(https://arxiv.org/abs/2506.18100)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Address Resolution Protocol (ARP) spoofing attacks severely threaten Internet of Things (IoT) networks by allowing attackers to intercept, modify, or block communications. Traditional detection methods are insufficient due to high false positives and poor adaptability. This research proposes a multi-layered machine learning-based framework for intelligently detecting ARP spoofing in IoT networks. Our approach utilizes an ensemble of classifiers organized into multiple layers, each layer optimizing detection accuracy and reducing false alarms. Experimental evaluations demonstrate significant improvements in detection accuracy (up to 97.5\%), reduced false positive rates (less than 2\%), and faster detection time compared to existing methods. Our key contributions include introducing multi-layer ensemble classifiers specifically tuned for IoT networks, systematically addressing dataset imbalance problems, introducing a dynamic feedback mechanism for classifier retraining, and validating practical applicability through extensive simulations. This research enhances security management in IoT deployments, providing robust defenses against ARP spoofing attacks and improving reliability and trust in IoT environments.</li>
</ul>

<h3>Title: InspireDebate: Multi-Dimensional Subjective-Objective Evaluation-Guided Reasoning and Optimization for Debating</h3>
<ul>
<li><strong>Authors: </strong>Fuyu Wang, Jiangtong Li, Kun Zhu, Changjun Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18102">https://arxiv.org/abs/2506.18102</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18102">https://arxiv.org/pdf/2506.18102</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18102]] InspireDebate: Multi-Dimensional Subjective-Objective Evaluation-Guided Reasoning and Optimization for Debating(https://arxiv.org/abs/2506.18102)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the rapid advancements in large language models (LLMs), debating tasks, such as argument quality assessment and debate process simulation, have made significant progress. However, existing LLM-based debating systems focus on responding to specific arguments while neglecting objective assessments such as authenticity and logical validity. Furthermore, these systems lack a structured approach to optimize across various dimensions$-$including evaluation metrics, chain-of-thought (CoT) reasoning, and multi-turn debate refinement$-$thereby limiting their effectiveness. To address these interconnected challenges, we propose a dual-component framework: (1) $\textbf{InspireScore}$, a novel evaluation system that establishes a multi-dimensional assessment architecture incorporating four subjective criteria (emotional appeal, argument clarity, argument arrangement, and topic relevance) alongside two objective metrics (fact authenticity and logical validity); and (2) $\textbf{InspireDebate}$, an optimized debating framework employing a phased optimization approach through CoT reasoning enhancement, multi-dimensional Direct Preference Optimization (DPO), and real-time knowledge grounding via web-based Retrieval Augmented Generation (Web-RAG). Empirical evaluations demonstrate that $\textbf{InspireScore}$ achieves 44$\%$ higher correlation with expert judgments compared to existing methods, while $\textbf{InspireDebate}$ shows significant improvements, outperforming baseline models by 57$\%$. Source code is available at this https URL.</li>
</ul>

<h3>Title: Enhancing VICReg: Random-Walk Pairing for Improved Generalization and Better Global Semantics Capturing</h3>
<ul>
<li><strong>Authors: </strong>Idan Simai, Ronen Talmon, Uri Shaham</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18104">https://arxiv.org/abs/2506.18104</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18104">https://arxiv.org/pdf/2506.18104</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18104]] Enhancing VICReg: Random-Walk Pairing for Improved Generalization and Better Global Semantics Capturing(https://arxiv.org/abs/2506.18104)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In this paper, we argue that viewing VICReg-a popular self-supervised learning (SSL) method--through the lens of spectral embedding reveals a potential source of sub-optimality: it may struggle to generalize robustly to unseen data due to overreliance on the training data. This observation invites a closer look at how well this method achieves its goal of producing meaningful representations of images outside of the training set as well. Here, we investigate this issue and introduce SAG-VICReg (Stable and Generalizable VICReg), a method that builds on VICReg by incorporating new training techniques. These enhancements improve the model's ability to capture global semantics within the data and strengthen the generalization capabilities. Experiments demonstrate that SAG-VICReg effectively addresses the generalization challenge while matching or surpassing diverse state-of-the-art SSL baselines. Notably, our method exhibits superior performance on metrics designed to evaluate global semantic understanding, while simultaneously maintaining competitive results on local evaluation metrics. Furthermore, we propose a new standalone evaluation metric for embeddings that complements the standard evaluation methods and accounts for the global data structure without requiring labels--a key issue when tagged data is scarce or not available.</li>
</ul>

<h3>Title: Chengyu-Bench: Benchmarking Large Language Models for Chinese Idiom Understanding and Use</h3>
<ul>
<li><strong>Authors: </strong>Yicheng Fu, Zhemin Huang, Liuxin Yang, Yumeng Lu, Zhongdongming Dai</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18105">https://arxiv.org/abs/2506.18105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18105">https://arxiv.org/pdf/2506.18105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18105]] Chengyu-Bench: Benchmarking Large Language Models for Chinese Idiom Understanding and Use(https://arxiv.org/abs/2506.18105)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Chinese idioms (Chengyu) are concise four-character expressions steeped in history and culture, whose literal translations often fail to capture their full meaning. This complexity makes them challenging for language models to interpret and use correctly. Existing benchmarks focus on narrow tasks - multiple-choice cloze tests, isolated translation, or simple paraphrasing. We introduce Chengyu-Bench, a comprehensive benchmark featuring three tasks: (1) Evaluative Connotation, classifying idioms as positive or negative; (2) Appropriateness, detecting incorrect idiom usage in context; and (3) Open Cloze, filling blanks in longer passages without options. Chengyu-Bench comprises 2,937 human-verified examples covering 1,765 common idioms sourced from diverse corpora. We evaluate leading LLMs and find they achieve over 95% accuracy on Evaluative Connotation, but only ~85% on Appropriateness and ~40% top-1 accuracy on Open Cloze. Error analysis reveals that most mistakes arise from fundamental misunderstandings of idiom meanings. Chengyu-Bench demonstrates that while LLMs can reliably gauge idiom sentiment, they still struggle to grasp the cultural and contextual nuances essential for proper usage. The benchmark and source code are available at: this https URL.</li>
</ul>

<h3>Title: Dynamic Temporal Positional Encodings for Early Intrusion Detection in IoT</h3>
<ul>
<li><strong>Authors: </strong>Ioannis Panopoulos, Maria-Lamprini A. Bartsioka, Sokratis Nikolaidis, Stylianos I. Venieris, Dimitra I. Kaklamani, Iakovos S. Venieris</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18114">https://arxiv.org/abs/2506.18114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18114">https://arxiv.org/pdf/2506.18114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18114]] Dynamic Temporal Positional Encodings for Early Intrusion Detection in IoT(https://arxiv.org/abs/2506.18114)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, transformer</a></li>
<li><strong>Abstract: </strong>The rapid expansion of the Internet of Things (IoT) has introduced significant security challenges, necessitating efficient and adaptive Intrusion Detection Systems (IDS). Traditional IDS models often overlook the temporal characteristics of network traffic, limiting their effectiveness in early threat detection. We propose a Transformer-based Early Intrusion Detection System (EIDS) that incorporates dynamic temporal positional encodings to enhance detection accuracy while maintaining computational efficiency. By leveraging network flow timestamps, our approach captures both sequence structure and timing irregularities indicative of malicious behaviour. Additionally, we introduce a data augmentation pipeline to improve model robustness. Evaluated on the CICIoT2023 dataset, our method outperforms existing models in both accuracy and earliness. We further demonstrate its real-time feasibility on resource-constrained IoT devices, achieving low-latency inference and minimal memory footprint.</li>
</ul>

<h3>Title: Mental Health Equity in LLMs: Leveraging Multi-Hop Question Answering to Detect Amplified and Silenced Perspectives</h3>
<ul>
<li><strong>Authors: </strong>Batool Haider, Atmika Gorti, Aman Chadha, Manas Gaur</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18116">https://arxiv.org/abs/2506.18116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18116">https://arxiv.org/pdf/2506.18116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18116]] Mental Health Equity in LLMs: Leveraging Multi-Hop Question Answering to Detect Amplified and Silenced Perspectives(https://arxiv.org/abs/2506.18116)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) in mental healthcare risk propagating biases that reinforce stigma and harm marginalized groups. While previous research identified concerning trends, systematic methods for detecting intersectional biases remain limited. This work introduces a multi-hop question answering (MHQA) framework to explore LLM response biases in mental health discourse. We analyze content from the Interpretable Mental Health Instruction (IMHI) dataset across symptom presentation, coping mechanisms, and treatment approaches. Using systematic tagging across age, race, gender, and socioeconomic status, we investigate bias patterns at demographic intersections. We evaluate four LLMs: Claude 3.5 Sonnet, Jamba 1.6, Gemma 3, and Llama 4, revealing systematic disparities across sentiment, demographics, and mental health conditions. Our MHQA approach demonstrates superior detection compared to conventional methods, identifying amplification points where biases magnify through sequential reasoning. We implement two debiasing techniques: Roleplay Simulation and Explicit Bias Reduction, achieving 66-94% bias reductions through few-shot prompting with BBQ dataset examples. These findings highlight critical areas where LLMs reproduce mental healthcare biases, providing actionable insights for equitable AI development.</li>
</ul>

<h3>Title: Bayesian Multiobject Tracking With Neural-Enhanced Motion and Measurement Models</h3>
<ul>
<li><strong>Authors: </strong>Shaoxiu Wei, Mingchao Liang, Florian Meyer</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18124">https://arxiv.org/abs/2506.18124</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18124">https://arxiv.org/pdf/2506.18124</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18124]] Bayesian Multiobject Tracking With Neural-Enhanced Motion and Measurement Models(https://arxiv.org/abs/2506.18124)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multiobject tracking (MOT) is an important task in applications including autonomous driving, ocean sciences, and aerospace surveillance. Traditional MOT methods are model-based and combine sequential Bayesian estimation with data association and an object birth model. More recent methods are fully data-driven and rely on the training of neural networks. Both approaches offer distinct advantages in specific settings. In particular, model-based methods are generally applicable across a wide range of scenarios, whereas data-driven MOT achieves superior performance in scenarios where abundant labeled data for training is available. A natural thought is whether a general framework can integrate the two approaches. This paper introduces a hybrid method that utilizes neural networks to enhance specific aspects of the statistical model in Bayesian MOT that have been identified as overly simplistic. By doing so, the performance of the prediction and update steps of Bayesian MOT is improved. To ensure tractable computation, our framework uses belief propagation to avoid high-dimensional operations combined with sequential Monte Carlo methods to perform low-dimensional operations efficiently. The resulting method combines the flexibility and robustness of model-based approaches with the capability to learn complex information from data of neural networks. We evaluate the performance of the proposed method based on the nuScenes autonomous driving dataset and demonstrate that it has state-of-the-art performance</li>
</ul>

<h3>Title: $ϕ^{\infty}$: Clause Purification, Embedding Realignment, and the Total Suppression of the Em Dash in Autoregressive Language Models</h3>
<ul>
<li><strong>Authors: </strong>Bugra Kilictas, Faruk Alpay</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18129">https://arxiv.org/abs/2506.18129</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18129">https://arxiv.org/pdf/2506.18129</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18129]] $ϕ^{\infty}$: Clause Purification, Embedding Realignment, and the Total Suppression of the Em Dash in Autoregressive Language Models(https://arxiv.org/abs/2506.18129)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>We identify a critical vulnerability in autoregressive transformer language models where the em dash token induces recursive semantic drift, leading to clause boundary hallucination and embedding space entanglement. Through formal analysis of token-level perturbations in semantic lattices, we demonstrate that em dash insertion fundamentally alters the model's latent representations, causing compounding errors in long-form generation. We propose a novel solution combining symbolic clause purification via the phi-infinity operator with targeted embedding matrix realignment. Our approach enables total suppression of problematic tokens without requiring model retraining, while preserving semantic coherence through fixed-point convergence guarantees. Experimental validation shows significant improvements in generation consistency and topic maintenance. This work establishes a general framework for identifying and mitigating token-level vulnerabilities in foundation models, with immediate implications for AI safety, model alignment, and robust deployment of large language models in production environments. The methodology extends beyond punctuation to address broader classes of recursive instabilities in neural text generation systems.</li>
</ul>

<h3>Title: Targeted False Positive Synthesis via Detector-guided Adversarial Diffusion Attacker for Robust Polyp Detection</h3>
<ul>
<li><strong>Authors: </strong>Quan Zhou, Gan Luo, Qiang Hu, Qingyong Zhang, Jinhua Zhang, Yinjiao Tian, Qiang Li, Zhiwei Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18134">https://arxiv.org/abs/2506.18134</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18134">https://arxiv.org/pdf/2506.18134</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18134]] Targeted False Positive Synthesis via Detector-guided Adversarial Diffusion Attacker for Robust Polyp Detection(https://arxiv.org/abs/2506.18134)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Polyp detection is crucial for colorectal cancer screening, yet existing models are limited by the scale and diversity of available data. While generative models show promise for data augmentation, current methods mainly focus on enhancing polyp diversity, often overlooking the critical issue of false positives. In this paper, we address this gap by proposing an adversarial diffusion framework to synthesize high-value false positives. The extensive variability of negative backgrounds presents a significant challenge in false positive synthesis. To overcome this, we introduce two key innovations: First, we design a regional noise matching strategy to construct a negative synthesis space using polyp detection datasets. This strategy trains a negative-centric diffusion model by masking polyp regions, ensuring the model focuses exclusively on learning diverse background patterns. Second, we introduce the Detector-guided Adversarial Diffusion Attacker (DADA) module, which perturbs the negative synthesis process to disrupt a pre-trained detector's decision, guiding the negative-centric diffusion model to generate high-value, detector-confusing false positives instead of low-value, ordinary backgrounds. Our approach is the first to apply adversarial diffusion to lesion detection, establishing a new paradigm for targeted false positive synthesis and paving the way for more reliable clinical applications in colorectal cancer screening. Extensive results on public and in-house datasets verify the superiority of our method over the current state-of-the-arts, with our synthesized data improving the detectors by at least 2.6% and 2.7% in F1-score, respectively, over the baselines. Codes are at this https URL.</li>
</ul>

<h3>Title: Sparse Feature Coactivation Reveals Composable Semantic Modules in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ruixuan Deng, Xiaoyang Hu, Miles Gilberti, Shane Storks, Aman Taxali, Mike Angstadt, Chandra Sripada, Joyce Chai</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18141">https://arxiv.org/abs/2506.18141</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18141">https://arxiv.org/pdf/2506.18141</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18141]] Sparse Feature Coactivation Reveals Composable Semantic Modules in Large Language Models(https://arxiv.org/abs/2506.18141)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We identify semantically coherent, context-consistent network components in large language models (LLMs) using coactivation of sparse autoencoder (SAE) features collected from just a handful of prompts. Focusing on country-relation tasks, we show that ablating semantic components for countries and relations changes model outputs in predictable ways, while amplifying these components induces counterfactual responses. Notably, composing relation and country components yields compound counterfactual outputs. We find that, whereas most country components emerge from the very first layer, the more abstract relation components are concentrated in later layers. Furthermore, within relation components themselves, nodes from later layers tend to have a stronger causal impact on model outputs. Overall, these findings suggest a modular organization of knowledge within LLMs and advance methods for efficient, targeted model manipulation.</li>
</ul>

<h3>Title: Routing Mamba: Scaling State Space Models with Mixture-of-Experts Projection</h3>
<ul>
<li><strong>Authors: </strong>Zheng Zhan, Liliang Ren, Shuohang Wang, Liyuan Liu, Yang Liu, Yeyun Gong, Yanzhi Wang, Yelong Shen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18145">https://arxiv.org/abs/2506.18145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18145">https://arxiv.org/pdf/2506.18145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18145]] Routing Mamba: Scaling State Space Models with Mixture-of-Experts Projection(https://arxiv.org/abs/2506.18145)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Linear State Space Models (SSMs) offer remarkable performance gains in efficient sequence modeling, with constant inference-time computation and memory complexity. Recent advances, such as Mamba, further enhance SSMs with input-dependent gating and hardware-aware implementations, positioning them as strong alternatives to Transformers for long sequence modeling. However, efficiently scaling the expressive power of SSMs, particularly with Mixture of Experts (MoE), remains challenging, as naive integration attempts often falter or degrade performance. In this work, we introduce Routing Mamba (RoM), a novel approach that scales SSM parameters using sparse mixtures of linear projection experts. By sharing routing decisions between projection layers and lightweight sub-modules within Mamba across experts, RoM leverages synergies among linear projection experts for effective and efficient sparse scaling of Mamba layers. At a scale of 1.3B active parameters (10B total) and 16K training sequence length, RoM achieves language modeling performance equivalent to a dense Mamba model requiring over 2.3x more active parameters, and demonstrates consistent perplexity across context lengths. Experimental results further show RoM effectively scales hybrid language models, yielding a 23% FLOPS saving compared to dense Mamba scaling for similar performance.</li>
</ul>

<h3>Title: HE-LRM: Encrypted Deep Learning Recommendation Models using Fully Homomorphic Encryption</h3>
<ul>
<li><strong>Authors: </strong>Karthik Garimella, Austin Ebel, Gabrielle De Micheli, Brandon Reagen</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18150">https://arxiv.org/abs/2506.18150</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18150">https://arxiv.org/pdf/2506.18150</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18150]] HE-LRM: Encrypted Deep Learning Recommendation Models using Fully Homomorphic Encryption(https://arxiv.org/abs/2506.18150)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Fully Homomorphic Encryption (FHE) is an encryption scheme that not only encrypts data but also allows for computations to be applied directly on the encrypted data. While computationally expensive, FHE can enable privacy-preserving neural inference in the client-server setting: a client encrypts their input with FHE and sends it to an untrusted server. The server then runs neural inference on the encrypted data and returns the encrypted results. The client decrypts the output locally, keeping both the input and result private from the server. Private inference has focused on networks with dense inputs such as image classification, and less attention has been given to networks with sparse features. Unlike dense inputs, sparse features require efficient encrypted lookup operations into large embedding tables, which present computational and memory constraints for FHE. In this paper, we explore the challenges and opportunities when applying FHE to Deep Learning Recommendation Models (DLRM) from both a compiler and systems perspective. DLRMs utilize conventional MLPs for dense features and embedding tables to map sparse, categorical features to dense vector representations. We develop novel methods for performing compressed embedding lookups in order to reduce FHE computational costs while keeping the underlying model performant. Our embedding lookup improves upon a state-of-the-art approach by $77 \times$. Furthermore, we present an efficient multi-embedding packing strategy that enables us to perform a 44 million parameter embedding lookup under FHE. Finally, we integrate our solutions into the open-source Orion framework and present HE-LRM, an end-to-end encrypted DLRM. We evaluate HE-LRM on UCI (health prediction) and Criteo (click prediction), demonstrating that with the right compression and packing strategies, encrypted inference for recommendation systems is practical.</li>
</ul>

<h3>Title: Probabilistic and reinforced mining of association rules</h3>
<ul>
<li><strong>Authors: </strong>Yongchao Huang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18155">https://arxiv.org/abs/2506.18155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18155">https://arxiv.org/pdf/2506.18155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18155]] Probabilistic and reinforced mining of association rules(https://arxiv.org/abs/2506.18155)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>This work introduces 4 novel probabilistic and reinforcement-driven methods for association rule mining (ARM): Gaussian process-based association rule mining (GPAR), Bayesian ARM (BARM), multi-armed bandit based ARM (MAB-ARM), and reinforcement learning based association rule mining (RLAR). These methods depart fundamentally from traditional frequency-based algorithms such as Apriori, FP-Growth, and Eclat, offering enhanced capabilities for incorporating prior knowledge, modeling uncertainty, item dependencies, probabilistic inference and adaptive search strategies. GPAR employs Gaussian processes to model item co-occurrence via feature representations, enabling principled inference, uncertainty quantification, and efficient generalization to unseen itemsets without retraining. BARM adopts a Bayesian framework with priors and optional correlation structures, yielding robust uncertainty quantification through full posterior distributions over item presence probabilities. MAB-ARM, including its Monte Carlo tree search (MCTS) companion, utilizes an upper confidence bound (UCB) strategy for efficient and adaptive exploration of the itemset space, while RLAR applies a deep Q-network (DQN) to learn a generalizable policy for identifying high-quality rules. Collectively, these approaches improve the flexibility and robustness of ARM, particularly for discovering rare or complex patterns and operating on small datasets. Empirical results on synthetic and real-world datasets demonstrate their effectiveness, while also highlighting trade-offs in computational complexity and interpretability. These innovations mark a significant shift from static, frequency-driven paradigms, offering some prior and dependency-informed, uncertainty-aware or scalable ARM frameworks for diverse application domains such as retail, geography, finance, medical diagnostics, and risk-sensitive scenarios.</li>
</ul>

<h3>Title: Pattern-Based Phase-Separation of Tracer and Dispersed Phase Particles in Two-Phase Defocusing Particle Tracking Velocimetry</h3>
<ul>
<li><strong>Authors: </strong>Christian Sax, Jochen Kriegseis</a></li>
<li><strong>Subjects: </strong>cs.CV, physics.app-ph, physics.flu-dyn</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18157">https://arxiv.org/abs/2506.18157</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18157">https://arxiv.org/pdf/2506.18157</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18157]] Pattern-Based Phase-Separation of Tracer and Dispersed Phase Particles in Two-Phase Defocusing Particle Tracking Velocimetry(https://arxiv.org/abs/2506.18157)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>This work investigates the feasibility of a post-processing-based approach for phase separation in defocusing particle tracking velocimetry for dispersed two-phase flows. The method enables the simultaneous 3D localization determination of both tracer particles and particles of the dispersed phase, using a single-camera setup. The distinction between phases is based on pattern differences in defocused particle images, which arise from distinct light scattering behaviors of tracer particles and bubbles or droplets. Convolutional neural networks, including Faster R-CNN and YOLOv4 variants, are trained to detect and classify particle images based on these pattern features. To generate large, labeled training datasets, a generative adversarial network based framework is introduced, allowing the generation of auto-labeled data that more closely reflects experiment-specific visual appearance. Evaluation across six datasets, comprising synthetic two-phase and real single- and two-phase flows, demonstrates high detection precision and classification accuracy (95-100%), even under domain shifts. The results confirm the viability of using CNNs for robust phase separation in disperse two-phase DPTV, particularly in scenarios where traditional wavelength-, size-, or ensemble correlation-based methods are impractical.</li>
</ul>

<h3>Title: CDG-MAE: Learning Correspondences from Diffusion Generated Views</h3>
<ul>
<li><strong>Authors: </strong>Varun Belagali, Pierre Marza, Srikar Yellapragada, Zilinghan Li, Tarak Nath Nandi, Ravi K Madduri, Joel Saltz, Stergios Christodoulidis, Maria Vakalopoulou, Dimitris Samaras</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18164">https://arxiv.org/abs/2506.18164</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18164">https://arxiv.org/pdf/2506.18164</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18164]] CDG-MAE: Learning Correspondences from Diffusion Generated Views(https://arxiv.org/abs/2506.18164)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Learning dense correspondences, critical for application such as video label propagation, is hindered by tedious and unscalable manual annotation. Self-supervised methods address this by using a cross-view pretext task, often modeled with a masked autoencoder, where a masked target view is reconstructed from an anchor view. However, acquiring effective training data remains a challenge - collecting diverse video datasets is difficult and costly, while simple image crops lack necessary pose variations. This paper introduces CDG-MAE, a novel MAE-based self-supervised method that uses diverse synthetic views generated from static images via an image-conditioned diffusion model. These generated views exhibit substantial changes in pose and perspective, providing a rich training signal that overcomes the limitations of video and crop-based anchors. We present a quantitative method to evaluate local and global consistency of generated images, discussing their use for cross-view self-supervised pretraining. Furthermore, we enhance the standard single-anchor MAE setting to a multi-anchor strategy to effectively modulate the difficulty of pretext task. CDG-MAE significantly outperforms state-of-the-art MAE methods reliant only on images and substantially narrows the performance gap to video-based approaches.</li>
</ul>

<h3>Title: Non-equilibrium Annealed Adjoint Sampler</h3>
<ul>
<li><strong>Authors: </strong>Jaemoo Choi, Yongxin Chen, Molei Tao, Guan-Horng Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18165">https://arxiv.org/abs/2506.18165</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18165">https://arxiv.org/pdf/2506.18165</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18165]] Non-equilibrium Annealed Adjoint Sampler(https://arxiv.org/abs/2506.18165)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recently, there has been significant progress in learning-based diffusion samplers, which aim to sample from a given unnormalized density. These methods typically follow one of two paradigms: (i) formulating sampling as an unbiased stochastic optimal control (SOC) problem using a canonical reference process, or (ii) refining annealed path measures through importance-weighted sampling. Although annealing approaches have advantages in guiding samples toward high-density regions, reliance on importance sampling leads to high variance and limited scalability in practice. In this paper, we introduce the \textbf{Non-equilibrium Annealed Adjoint Sampler (NAAS)}, a novel SOC-based diffusion sampler that leverages annealed reference dynamics without resorting to importance sampling. NAAS employs a lean adjoint system inspired by adjoint matching, enabling efficient and scalable training. We demonstrate the effectiveness of our approach across a range of tasks, including sampling from classical energy landscapes and molecular Boltzmann distribution.</li>
</ul>

<h3>Title: Understanding Reasoning in Thinking Language Models via Steering Vectors</h3>
<ul>
<li><strong>Authors: </strong>Constantin Venhoff, Iván Arcuschin, Philip Torr, Arthur Conmy, Neel Nanda</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18167">https://arxiv.org/abs/2506.18167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18167">https://arxiv.org/pdf/2506.18167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18167]] Understanding Reasoning in Thinking Language Models via Steering Vectors(https://arxiv.org/abs/2506.18167)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have led to the development of thinking language models that generate extensive internal reasoning chains before producing responses. While these models achieve improved performance, controlling their reasoning processes remains challenging. This work presents a steering approach for thinking LLMs by analyzing and manipulating specific reasoning behaviors in DeepSeek-R1-Distill models. Through a systematic experiment on 500 tasks across 10 diverse categories, we identify several reasoning behaviors exhibited by thinking models, including expressing uncertainty, generating examples for hypothesis validation, and backtracking in reasoning chains. We demonstrate that these behaviors are mediated by linear directions in the model's activation space and can be controlled using steering vectors. By extracting and applying these vectors, we provide a method to modulate specific aspects of the model's reasoning process, such as its tendency to backtrack or express uncertainty. Our approach offers practical tools for steering reasoning processes in thinking models in a controlled and interpretable manner. We validate our steering method using two DeepSeek-R1-Distill models, demonstrating consistent control across different model architectures.</li>
</ul>

<h3>Title: STACT-Time: Spatio-Temporal Cross Attention for Cine Thyroid Ultrasound Time Series Classification</h3>
<ul>
<li><strong>Authors: </strong>Irsyad Adam, Tengyue Zhang, Shrayes Raman, Zhuyu Qiu, Brandon Taraku, Hexiang Feng, Sile Wang, Ashwath Radhachandran, Shreeram Athreya, Vedrana Ivezic, Peipei Ping, Corey Arnold, William Speier</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18172">https://arxiv.org/abs/2506.18172</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18172">https://arxiv.org/pdf/2506.18172</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18172]] STACT-Time: Spatio-Temporal Cross Attention for Cine Thyroid Ultrasound Time Series Classification(https://arxiv.org/abs/2506.18172)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Thyroid cancer is among the most common cancers in the United States. Thyroid nodules are frequently detected through ultrasound (US) imaging, and some require further evaluation via fine-needle aspiration (FNA) biopsy. Despite its effectiveness, FNA often leads to unnecessary biopsies of benign nodules, causing patient discomfort and anxiety. To address this, the American College of Radiology Thyroid Imaging Reporting and Data System (TI-RADS) has been developed to reduce benign biopsies. However, such systems are limited by interobserver variability. Recent deep learning approaches have sought to improve risk stratification, but they often fail to utilize the rich temporal and spatial context provided by US cine clips, which contain dynamic global information and surrounding structural changes across various views. In this work, we propose the Spatio-Temporal Cross Attention for Cine Thyroid Ultrasound Time Series Classification (STACT-Time) model, a novel representation learning framework that integrates imaging features from US cine clips with features from segmentation masks automatically generated by a pretrained model. By leveraging self-attention and cross-attention mechanisms, our model captures the rich temporal and spatial context of US cine clips while enhancing feature representation through segmentation-guided learning. Our model improves malignancy prediction compared to state-of-the-art models, achieving a cross-validation precision of 0.91 (plus or minus 0.02) and an F1 score of 0.89 (plus or minus 0.02). By reducing unnecessary biopsies of benign nodules while maintaining high sensitivity for malignancy detection, our model has the potential to enhance clinical decision-making and improve patient outcomes.</li>
</ul>

<h3>Title: Memba: Membrane-driven Parameter-Efficient Fine-Tuning for Mamba</h3>
<ul>
<li><strong>Authors: </strong>Donghyun Lee, Yuhang Li, Ruokai Yin, Shiting Xiao, Priyadarshini Panda</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18184">https://arxiv.org/abs/2506.18184</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18184">https://arxiv.org/pdf/2506.18184</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18184]] Memba: Membrane-driven Parameter-Efficient Fine-Tuning for Mamba(https://arxiv.org/abs/2506.18184)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>State Space Models (SSMs) have emerged as powerful alternatives to attention-based Transformers, with Mamba demonstrating impressive efficiency and scalability. As these models grow increasingly larger, the need for Parameter-Efficient Fine-Tuning (PEFT) methods becomes critical to adapt pre-trained Mamba to downstream tasks without prohibitive computational costs. However, previous approaches simply apply traditional Transformer-tailored PEFT methods without addressing the unique temporal processing dynamics of SSMs. To address this limitation, we propose Memba, a membrane-driven PEFT approach specifically designed for Mamba. Memba introduces Leaky Integrate Membrane (LIM) neurons as bio-inspired gating mechanisms that naturally accumulate membrane potentials over time, enhancing selective information retention. By strategically combining LIM neurons with Low-Rank Adaptations (LoRA) and cross-layer membrane transfer, our approach significantly improves Mamba's temporal modeling capabilities. Extensive experiments across language and vision tasks demonstrate that Memba achieves substantial improvements over existing PEFT methods. The code is available at this https URL.</li>
</ul>

<h3>Title: CareLab at #SMM4H-HeaRD 2025: Insomnia Detection and Food Safety Event Extraction with Domain-Aware Transformers</h3>
<ul>
<li><strong>Authors: </strong>Zihan Liang, Ziwen Pan, Sumon Kanti Dey, Azra Ismail</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18185">https://arxiv.org/abs/2506.18185</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18185">https://arxiv.org/pdf/2506.18185</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18185]] CareLab at #SMM4H-HeaRD 2025: Insomnia Detection and Food Safety Event Extraction with Domain-Aware Transformers(https://arxiv.org/abs/2506.18185)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>This paper presents our system for the SMM4H-HeaRD 2025 shared tasks, specifically Task 4 (Subtasks 1, 2a, and 2b) and Task 5 (Subtasks 1 and 2). Task 4 focused on detecting mentions of insomnia in clinical notes, while Task 5 addressed the extraction of food safety events from news articles. We participated in all subtasks and report key findings across them, with particular emphasis on Task 5 Subtask 1, where our system achieved strong performance-securing first place with an F1 score of 0.958 on the test set. To attain this result, we employed encoder-based models (e.g., RoBERTa), alongside GPT-4 for data augmentation. This paper outlines our approach, including preprocessing, model architecture, and subtask-specific adaptations</li>
</ul>

<h3>Title: Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review</h3>
<ul>
<li><strong>Authors: </strong>Bushra Asseri, Estabrag Abdelaziz, Areej Al-Wabil</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18199">https://arxiv.org/abs/2506.18199</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18199">https://arxiv.org/pdf/2506.18199</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18199]] Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review(https://arxiv.org/abs/2506.18199)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models have demonstrated remarkable capabilities across various domains, yet concerns about cultural bias - particularly towards Arabs and Muslims - pose significant ethical challenges by perpetuating harmful stereotypes and marginalization. Despite growing recognition of bias in LLMs, prompt engineering strategies specifically addressing Arab and Muslim representation remain understudied. This mixed-methods systematic review examines such techniques, offering evidence-based guidance for researchers and practitioners. Following PRISMA guidelines and Kitchenham's systematic review methodology, we analyzed 8 empirical studies published between 2021-2024 investigating bias mitigation strategies. Our findings reveal five primary prompt engineering approaches: cultural prompting, affective priming, self-debiasing techniques, structured multi-step pipelines, and parameter-optimized continuous prompts. Although all approaches show potential for reducing bias, effectiveness varied substantially across studies and bias types. Evidence suggests that certain bias types may be more resistant to prompt-based mitigation than others. Structured multi-step pipelines demonstrated the highest overall effectiveness, achieving up to 87.7% reduction in bias, though they require greater technical expertise. Cultural prompting offers broader accessibility with substantial effectiveness. These results underscore the accessibility of prompt engineering for mitigating cultural bias without requiring access to model parameters. The limited number of studies identified highlights a significant research gap in this critical area. Future research should focus on developing culturally adaptive prompting techniques, creating Arab and Muslim-specific evaluation resources, and integrating prompt engineering with complementary debiasing methods to address deeper stereotypes while maintaining model utility.</li>
</ul>

<h3>Title: Deciphering Emotions in Children Storybooks: A Comparative Analysis of Multimodal LLMs in Educational Applications</h3>
<ul>
<li><strong>Authors: </strong>Bushra Asseri, Estabraq Abdelaziz, Maha Al Mogren, Tayef Alhefdhi, Areej Al-Wabil</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18201">https://arxiv.org/abs/2506.18201</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18201">https://arxiv.org/pdf/2506.18201</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18201]] Deciphering Emotions in Children Storybooks: A Comparative Analysis of Multimodal LLMs in Educational Applications(https://arxiv.org/abs/2506.18201)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Emotion recognition capabilities in multimodal AI systems are crucial for developing culturally responsive educational technologies, yet remain underexplored for Arabic language contexts where culturally appropriate learning tools are critically needed. This study evaluates the emotion recognition performance of two advanced multimodal large language models, GPT-4o and Gemini 1.5 Pro, when processing Arabic children's storybook illustrations. We assessed both models across three prompting strategies (zero-shot, few-shot, and chain-of-thought) using 75 images from seven Arabic storybooks, comparing model predictions with human annotations based on Plutchik's emotional framework. GPT-4o consistently outperformed Gemini across all conditions, achieving the highest macro F1-score of 59% with chain-of-thought prompting compared to Gemini's best performance of 43%. Error analysis revealed systematic misclassification patterns, with valence inversions accounting for 60.7% of errors, while both models struggled with culturally nuanced emotions and ambiguous narrative contexts. These findings highlight fundamental limitations in current models' cultural understanding and emphasize the need for culturally sensitive training approaches to develop effective emotion-aware educational technologies for Arabic-speaking learners.</li>
</ul>

<h3>Title: Multimodal Fusion SLAM with Fourier Attention</h3>
<ul>
<li><strong>Authors: </strong>Youjie Zhou, Guofeng Mei, Yiming Wang, Yi Wan, Fabio Poiesi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18204">https://arxiv.org/abs/2506.18204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18204">https://arxiv.org/pdf/2506.18204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18204]] Multimodal Fusion SLAM with Fourier Attention(https://arxiv.org/abs/2506.18204)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Visual SLAM is particularly challenging in environments affected by noise, varying lighting conditions, and darkness. Learning-based optical flow algorithms can leverage multiple modalities to address these challenges, but traditional optical flow-based visual SLAM approaches often require significant computational this http URL overcome this limitation, we propose FMF-SLAM, an efficient multimodal fusion SLAM method that utilizes fast Fourier transform (FFT) to enhance the algorithm efficiency. Specifically, we introduce a novel Fourier-based self-attention and cross-attention mechanism to extract features from RGB and depth signals. We further enhance the interaction of multimodal features by incorporating multi-scale knowledge distillation across modalities. We also demonstrate the practical feasibility of FMF-SLAM in real-world scenarios with real time performance by integrating it with a security robot by fusing with a global positioning module GNSS-RTK and global Bundle Adjustment. Our approach is validated using video sequences from TUM, TartanAir, and our real-world datasets, showcasing state-of-the-art performance under noisy, varying lighting, and dark this http URL code and datasets are available at this https URL.</li>
</ul>

<h3>Title: Deep Learning-based Alignment Measurement in Knee Radiographs</h3>
<ul>
<li><strong>Authors: </strong>Zhisen Hu, Dominic Cullen, Peter Thompson, David Johnson, Chang Bian, Aleksei Tiulpin, Timothy Cootes, Claudia Lindner</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18209">https://arxiv.org/abs/2506.18209</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18209">https://arxiv.org/pdf/2506.18209</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18209]] Deep Learning-based Alignment Measurement in Knee Radiographs(https://arxiv.org/abs/2506.18209)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Radiographic knee alignment (KA) measurement is important for predicting joint health and surgical outcomes after total knee replacement. Traditional methods for KA measurements are manual, time-consuming and require long-leg radiographs. This study proposes a deep learning-based method to measure KA in anteroposterior knee radiographs via automatically localized knee anatomical landmarks. Our method builds on hourglass networks and incorporates an attention gate structure to enhance robustness and focus on key anatomical features. To our knowledge, this is the first deep learning-based method to localize over 100 knee anatomical landmarks to fully outline the knee shape while integrating KA measurements on both pre-operative and post-operative images. It provides highly accurate and reliable anatomical varus/valgus KA measurements using the anatomical tibiofemoral angle, achieving mean absolute differences ~1° when compared to clinical ground truth measurements. Agreement between automated and clinical measurements was excellent pre-operatively (intra-class correlation coefficient (ICC) = 0.97) and good post-operatively (ICC = 0.86). Our findings demonstrate that KA assessment can be automated with high accuracy, creating opportunities for digitally enhanced clinical workflows.</li>
</ul>

<h3>Title: Cross-Architecture Knowledge Distillation (KD) for Retinal Fundus Image Anomaly Detection on NVIDIA Jetson Nano</h3>
<ul>
<li><strong>Authors: </strong>Berk Yilmaz, Aniruddh Aiyengar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18220">https://arxiv.org/abs/2506.18220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18220">https://arxiv.org/pdf/2506.18220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18220]] Cross-Architecture Knowledge Distillation (KD) for Retinal Fundus Image Anomaly Detection on NVIDIA Jetson Nano(https://arxiv.org/abs/2506.18220)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Early and accurate identification of retinal ailments is crucial for averting ocular decline; however, access to dependable diagnostic devices is not often available in low-resourced settings. This project proposes to solve that by developing a lightweight, edge-device deployable disease classifier using cross-architecture knowledge distilling. We first train a high-capacity vision transformer (ViT) teacher model, pre-trained using I-JEPA self-supervised learning, to classify fundus images into four classes: Normal, Diabetic Retinopathy, Glaucoma, and Cataract. We kept an Internet of Things (IoT) focus when compressing to a CNN-based student model for deployment in resource-limited conditions, such as the NVIDIA Jetson Nano. This was accomplished using a novel framework which included a Partitioned Cross-Attention (PCA) projector, a Group-Wise Linear (GL) projector, and a multi-view robust training method. The teacher model has 97.4 percent more parameters than the student model, with it achieving 89 percent classification with a roughly 93 percent retention of the teacher model's diagnostic performance. The retention of clinical classification behavior supports our method's initial aim: compression of the ViT while retaining accuracy. Our work serves as an example of a scalable, AI-driven triage solution for retinal disorders in under-resourced areas.</li>
</ul>

<h3>Title: Smart-LLaMA-DPO: Reinforced Large Language Model for Explainable Smart Contract Vulnerability Detection</h3>
<ul>
<li><strong>Authors: </strong>Lei Yu, Zhirong Huang, Hang Yuan, Shiqi Cheng, Li Yang, Fengjun Zhang, Chenjie Shen, Jiajia Ma, Jingyuan Zhang, Junyi Lu, Chun Zuo</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18245">https://arxiv.org/abs/2506.18245</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18245">https://arxiv.org/pdf/2506.18245</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18245]] Smart-LLaMA-DPO: Reinforced Large Language Model for Explainable Smart Contract Vulnerability Detection(https://arxiv.org/abs/2506.18245)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Smart contract vulnerability detection remains a major challenge in blockchain security. Existing vulnerability detection methods face two main issues: (1) Existing datasets lack comprehensive coverage and high-quality explanations for preference learning. (2) Large language models (LLMs) often struggle with accurately interpreting specific concepts in smart contract security. Empirical analysis shows that even after continual pre-training (CPT) and supervised fine-tuning (SFT), LLMs may misinterpret the execution order of state changes, resulting in incorrect explanations despite making correct detection decisions. To address these challenges, we propose Smart-LLaMA-DPO based on LLaMA-3.1-8B. We construct a comprehensive dataset covering four major vulnerability types and machine-unauditable vulnerabilities, including precise labels, explanations, and locations for SFT, as well as high-quality and low-quality output pairs for Direct Preference Optimization (DPO). Second, we perform CPT using large-scale smart contract to enhance the LLM's understanding of specific security practices in smart contracts. Futhermore, we conduct SFT with our comprehensive dataset. Finally, we apply DPO, leveraging human feedback and a specially designed loss function that increases the probability of preferred explanations while reducing the likelihood of non-preferred outputs. We evaluate Smart-LLaMA-DPO on four major vulnerability types: reentrancy, timestamp dependence, integer overflow/underflow, and delegatecall, as well as machine-unauditable vulnerabilities. Our method significantly outperforms state-of-the-art baselines, with average improvements of 10.43% in F1 score and 7.87% in accuracy. Moreover, both LLM evaluation and human evaluation confirm that our method generates more correct, thorough, and clear explanations.</li>
</ul>

<h3>Title: Exploring Efficient Quantification of Modeling Uncertainties with Differentiable Physics-Informed Machine Learning Architectures</h3>
<ul>
<li><strong>Authors: </strong>Manaswin Oddiraju, Bharath Varma Penumatsa, Divyang Amin, Michael Piedmonte, Souma Chowdhury</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18247">https://arxiv.org/abs/2506.18247</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18247">https://arxiv.org/pdf/2506.18247</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18247]] Exploring Efficient Quantification of Modeling Uncertainties with Differentiable Physics-Informed Machine Learning Architectures(https://arxiv.org/abs/2506.18247)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Quantifying and propagating modeling uncertainties is crucial for reliability analysis, robust optimization, and other model-based algorithmic processes in engineering design and control. Now, physics-informed machine learning (PIML) methods have emerged in recent years as a new alternative to traditional computational modeling and surrogate modeling methods, offering a balance between computing efficiency, modeling accuracy, and interpretability. However, their ability to predict and propagate modeling uncertainties remains mostly unexplored. In this paper, a promising class of auto-differentiable hybrid PIML architectures that combine partial physics and neural networks or ANNs (for input transformation or adaptive parameter estimation) is integrated with Bayesian Neural networks (replacing the ANNs); this is done with the goal to explore whether BNNs can successfully provision uncertainty propagation capabilities in the PIML architectures as well, further supported by the auto-differentiability of these architectures. A two-stage training process is used to alleviate the challenges traditionally encountered in training probabilistic ML models. The resulting BNN-integrated PIML architecture is evaluated on an analytical benchmark problem and flight experiments data for a fixed-wing RC aircraft, with prediction performance observed to be slightly worse or at par with purely data-driven ML and original PIML models. Moreover, Monte Carlo sampling of probabilistic BNN weights was found to be most effective in propagating uncertainty in the BNN-integrated PIML architectures.</li>
</ul>

<h3>Title: Semantic Structure-Aware Generative Attacks for Enhanced Adversarial Transferability</h3>
<ul>
<li><strong>Authors: </strong>Jongoh Jeong, Hunmin Yang, Jaeseok Jeong, Kuk-Jin Yoon</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18248">https://arxiv.org/abs/2506.18248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18248">https://arxiv.org/pdf/2506.18248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18248]] Semantic Structure-Aware Generative Attacks for Enhanced Adversarial Transferability(https://arxiv.org/abs/2506.18248)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, generative</a></li>
<li><strong>Abstract: </strong>Generative adversarial attacks train a perturbation generator on a white-box surrogate model and subsequently apply the crafted perturbations to unseen black-box victim models. In contrast to iterative attacks, these methods deliver superior inference-time efficiency, scalability, and transferability; however, up until now, existing studies have not fully exploited the representational capacity of generative models to preserve and harness semantic information. Specifically, the intermediate activations of the generator encode rich semantic features--object boundaries and coarse shapes--that remain under-exploited, thereby limiting the alignment of perturbations with object-salient regions which are critical for adversarial transferability. To remedy this, we introduce a semantic structure-aware attack framework based on the Mean Teacher, which serves as a temporally smoothed feature reference. With this smoothed reference, we further direct semantic consistency between the early-layer activations in the student and those of the semantically rich teacher by feature distillation. By anchoring perturbation synthesis to the semantically salient early intermediate blocks within the generator based on empirical findings, our method guides progressive adversarial perturbation on regions that substantially enhance adversarial transferability. We conduct extensive experiments over diverse models, domains and tasks to demonstrate consistent improvements relative to state-of-the-art generative attacks, comprehensively evaluated using conventional metrics and our newly proposed Accidental Correction Rate (ACR).</li>
</ul>

<h3>Title: YouTube-Occ: Learning Indoor 3D Semantic Occupancy Prediction from YouTube Videos</h3>
<ul>
<li><strong>Authors: </strong>Haoming Chen, Lichen Yuan, TianFang Sun, Jingyu Gong, Xin Tan, Zhizhong Zhang, Yuan Xie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18266">https://arxiv.org/abs/2506.18266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18266">https://arxiv.org/pdf/2506.18266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18266]] YouTube-Occ: Learning Indoor 3D Semantic Occupancy Prediction from YouTube Videos(https://arxiv.org/abs/2506.18266)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>3D semantic occupancy prediction in the past was considered to require precise geometric relationships in order to enable effective training. However, in complex indoor environments, the large-scale and widespread collection of data, along with the necessity for fine-grained annotations, becomes impractical due to the complexity of data acquisition setups and privacy concerns. In this paper, we demonstrate that 3D spatially-accurate training can be achieved using only indoor Internet data, without the need for any pre-knowledge of intrinsic or extrinsic camera parameters. In our framework, we collect a web dataset, YouTube-Occ, which comprises house tour videos from YouTube, providing abundant real house scenes for 3D representation learning. Upon on this web dataset, we establish a fully self-supervised model to leverage accessible 2D prior knowledge for reaching powerful 3D indoor perception. Specifically, we harness the advantages of the prosperous vision foundation models, distilling the 2D region-level knowledge into the occupancy network by grouping the similar pixels into superpixels. Experimental results show that our method achieves state-of-the-art zero-shot performance on two popular benchmarks (NYUv2 and OccScanNet</li>
</ul>

<h3>Title: ARD-LoRA: Dynamic Rank Allocation for Parameter-Efficient Fine-Tuning of Foundation Models with Heterogeneous Adaptation Needs</h3>
<ul>
<li><strong>Authors: </strong>Haseeb Ullah Khan Shinwari, Muhammad Usama</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18267">https://arxiv.org/abs/2506.18267</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18267">https://arxiv.org/pdf/2506.18267</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18267]] ARD-LoRA: Dynamic Rank Allocation for Parameter-Efficient Fine-Tuning of Foundation Models with Heterogeneous Adaptation Needs(https://arxiv.org/abs/2506.18267)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Conventional Low-Rank Adaptation (LoRA) methods employ a fixed rank, imposing uniform adaptation across transformer layers and attention heads despite their heterogeneous learning dynamics. This paper introduces Adaptive Rank Dynamic LoRA (ARD-LoRA), a novel framework that automates rank allocation through learnable scaling factors. These factors are optimized via a meta-objective balancing task performance and parameter efficiency, incorporating $\ell_1$ sparsity for minimal rank and Total Variation regularization for stable rank transitions. ARD-LoRA enables continuous, differentiable, per-head rank adaptation. Experiments on LLAMA-3.1-70B and PaliGemma-2 demonstrate ARD-LoRA's efficacy, achieving up to 99.3% of full fine-tuning performance with only 0.32% trainable parameters, outperforming strong baselines like DoRA and AdaLoRA. Furthermore, it reduces multimodal adaptation memory by 41%. These results establish dynamic, fine-grained rank allocation as a critical paradigm for efficient foundation model adaptation.</li>
</ul>

<h3>Title: ThermalLoc: A Vision Transformer-Based Approach for Robust Thermal Camera Relocalization in Large-Scale Environments</h3>
<ul>
<li><strong>Authors: </strong>Yu Liu, Yangtao Meng, Xianfei Pan, Jie Jiang, Changhao Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18268">https://arxiv.org/abs/2506.18268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18268">https://arxiv.org/pdf/2506.18268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18268]] ThermalLoc: A Vision Transformer-Based Approach for Robust Thermal Camera Relocalization in Large-Scale Environments(https://arxiv.org/abs/2506.18268)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Thermal cameras capture environmental data through heat emission, a fundamentally different mechanism compared to visible light cameras, which rely on pinhole imaging. As a result, traditional visual relocalization methods designed for visible light images are not directly applicable to thermal images. Despite significant advancements in deep learning for camera relocalization, approaches specifically tailored for thermal camera-based relocalization remain underexplored. To address this gap, we introduce ThermalLoc, a novel end-to-end deep learning method for thermal image relocalization. ThermalLoc effectively extracts both local and global features from thermal images by integrating EfficientNet with Transformers, and performs absolute pose regression using two MLP networks. We evaluated ThermalLoc on both the publicly available thermal-odometry dataset and our own dataset. The results demonstrate that ThermalLoc outperforms existing representative methods employed for thermal camera relocalization, including AtLoc, MapNet, PoseNet, and RobustLoc, achieving superior accuracy and robustness.</li>
</ul>

<h3>Title: Adaptive Mask-guided K-space Diffusion for Accelerated MRI Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Qinrong Cai, Yu Guan, Zhibo Chen, Dong Liang, Qiuyun Fan, Qiegen Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18270">https://arxiv.org/abs/2506.18270</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18270">https://arxiv.org/pdf/2506.18270</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18270]] Adaptive Mask-guided K-space Diffusion for Accelerated MRI Reconstruction(https://arxiv.org/abs/2506.18270)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>As the deep learning revolution marches on, masked modeling has emerged as a distinctive approach that involves predicting parts of the original data that are proportionally masked during training, and has demonstrated exceptional performance in multiple fields. Magnetic Resonance Imaging (MRI) reconstruction is a critical task in medical imaging that seeks to recover high-quality images from under-sampled k-space data. However, previous MRI reconstruction strategies usually optimized the entire image domain or k-space, without considering the importance of different frequency regions in the k-space This work introduces a diffusion model based on adaptive masks (AMDM), which utilizes the adaptive adjustment of frequency distribution based on k-space data to develop a hybrid masks mechanism that adapts to different k-space inputs. This enables the effective separation of high-frequency and low-frequency components, producing diverse frequency-specific representations. Additionally, the k-space frequency distribution informs the generation of adaptive masks, which, in turn, guide a closed-loop diffusion process. Experimental results verified the ability of this method to learn specific frequency information and thereby improved the quality of MRI reconstruction, providing a flexible framework for optimizing k-space data using masks in the future.</li>
</ul>

<h3>Title: Memory-Augmented Architecture for Long-Term Context Handling in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haseeb Ullah Khan Shinwari, Muhammad Usama</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18271">https://arxiv.org/abs/2506.18271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18271">https://arxiv.org/pdf/2506.18271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18271]] Memory-Augmented Architecture for Long-Term Context Handling in Large Language Models(https://arxiv.org/abs/2506.18271)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models face significant challenges in maintaining coherent interactions over extended dialogues due to their limited contextual memory. This limitation often leads to fragmented exchanges and reduced relevance in responses, diminishing user experience. To address these issues, we propose a memory-augmented architecture that dynamically retrieves, updates, and prunes relevant information from past interactions, ensuring effective long-term context handling. Experimental results demonstrate that our solution significantly improves contextual coherence, reduces memory overhead, and enhances response quality, showcasing its potential for real-time applications in interactive systems.</li>
</ul>

<h3>Title: Leveraging Large Language Models for Information Verification -- an Engineering Approach</h3>
<ul>
<li><strong>Authors: </strong>Nguyen Nang Hung, Nguyen Thanh Trong, Vuong Thanh Toan, Nguyen An Phuoc, Dao Minh Tu, Nguyen Manh Duc Tuan, Nguyen Dinh Mau</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18274">https://arxiv.org/abs/2506.18274</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18274">https://arxiv.org/pdf/2506.18274</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18274]] Leveraging Large Language Models for Information Verification -- an Engineering Approach(https://arxiv.org/abs/2506.18274)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>For the ACMMM25 challenge, we present a practical engineering approach to multimedia news source verification, utilizing Large Language Models (LLMs) like GPT-4o as the backbone of our pipeline. Our method processes images and videos through a streamlined sequence of steps: First, we generate metadata using general-purpose queries via Google tools, capturing relevant content and links. Multimedia data is then segmented, cleaned, and converted into frames, from which we select the top-K most informative frames. These frames are cross-referenced with metadata to identify consensus or discrepancies. Additionally, audio transcripts are extracted for further verification. Noticeably, the entire pipeline is automated using GPT-4o through prompt engineering, with human intervention limited to final validation.</li>
</ul>

<h3>Title: Open Set Recognition for Endoscopic Image Classification: A Deep Learning Approach on the Kvasir Dataset</h3>
<ul>
<li><strong>Authors: </strong>Kasra Moazzami, Seoyoun Son, John Lin, Sun Min Lee, Daniel Son, Hayeon Lee, Jeongho Lee, Seongji Lee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18284">https://arxiv.org/abs/2506.18284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18284">https://arxiv.org/pdf/2506.18284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18284]] Open Set Recognition for Endoscopic Image Classification: A Deep Learning Approach on the Kvasir Dataset(https://arxiv.org/abs/2506.18284)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Endoscopic image classification plays a pivotal role in medical diagnostics by identifying anatomical landmarks and pathological findings. However, conventional closed-set classification frameworks are inherently limited in open-world clinical settings, where previously unseen conditions can arise andcompromise model reliability. To address this, we explore the application of Open Set Recognition (OSR) techniques on the Kvasir dataset, a publicly available and diverse endoscopic image collection. In this study, we evaluate and compare the OSR capabilities of several representative deep learning architectures, including ResNet-50, Swin Transformer, and a hybrid ResNet-Transformer model, under both closed-set and open-set conditions. OpenMax is adopted as a baseline OSR method to assess the ability of these models to distinguish known classes from previously unseen categories. This work represents one of the first efforts to apply open set recognition to the Kvasir dataset and provides a foundational benchmark for evaluating OSR performance in medical image analysis. Our results offer practical insights into model behavior in clinically realistic settings and highlight the importance of OSR techniques for the safe deployment of AI systems in endoscopy.</li>
</ul>

<h3>Title: Learning Causal Graphs at Scale: A Foundation Model Approach</h3>
<ul>
<li><strong>Authors: </strong>Naiyu Yin, Tian Gao, Yue Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18285">https://arxiv.org/abs/2506.18285</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18285">https://arxiv.org/pdf/2506.18285</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18285]] Learning Causal Graphs at Scale: A Foundation Model Approach(https://arxiv.org/abs/2506.18285)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Due to its human-interpretability and invariance properties, Directed Acyclic Graph (DAG) has been a foundational tool across various areas of AI research, leading to significant advancements. However, DAG learning remains highly challenging, due to its super-exponential growth in computational cost and identifiability issues, particularly in small-sample regimes. To address these two challenges, in this work we leverage the recent success of linear transformers and develop a foundation model approach for discovering multiple order-consistent DAGs across tasks. In particular, we propose Attention-DAG (ADAG), a novel attention-mechanism-based architecture for learning multiple linear Structural Equation Models (SEMs). ADAG learns the mapping from observed data to both graph structure and parameters via a nonlinear attention-based kernel, enabling efficient multi-task estimation of the underlying linear SEMs. By formulating the learning process across multiple tasks as a continuous optimization problem, the pre-trained ADAG model captures the common structural properties as a shared low-dimensional prior, thereby reducing the ill-posedness of downstream DAG learning tasks in small-sample regimes. We evaluate our proposed approach on benchmark synthetic datasets and find that ADAG achieves substantial improvements in both DAG learning accuracy and zero-shot inference efficiency. To the best of our knowledge, this is the first practical approach for pre-training a foundation model specifically designed for DAG learning, representing a step toward more efficient and generalizable down-stream applications in causal discovery.</li>
</ul>

<h3>Title: Instability in Diffusion ODEs: An Explanation for Inaccurate Image Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Han Zhang, Jinghong Mao, Shangwen Zhu, Zhantao Yang, Lianghua Huang, Yu Liu, Deli Zhao, Ruili Feng, Fan Cheng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18290">https://arxiv.org/abs/2506.18290</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18290">https://arxiv.org/pdf/2506.18290</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18290]] Instability in Diffusion ODEs: An Explanation for Inaccurate Image Reconstruction(https://arxiv.org/abs/2506.18290)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion reconstruction plays a critical role in various applications such as image editing, restoration, and style transfer. In theory, the reconstruction should be simple - it just inverts and regenerates images by numerically solving the Probability Flow-Ordinary Differential Equation (PF-ODE). Yet in practice, noticeable reconstruction errors have been observed, which cannot be well explained by numerical errors. In this work, we identify a deeper intrinsic property in the PF-ODE generation process, the instability, that can further amplify the reconstruction errors. The root of this instability lies in the sparsity inherent in the generation distribution, which means that the probability is concentrated on scattered and small regions while the vast majority remains almost empty. To demonstrate the existence of instability and its amplification on reconstruction error, we conduct experiments on both toy numerical examples and popular open-sourced diffusion models. Furthermore, based on the characteristics of image data, we theoretically prove that the instability's probability converges to one as the data dimensionality increases. Our findings highlight the inherent challenges in diffusion-based reconstruction and can offer insights for future improvements.</li>
</ul>

<h3>Title: Sharpening the Spear: Adaptive Expert-Guided Adversarial Attack Against DRL-based Autonomous Driving Policies</h3>
<ul>
<li><strong>Authors: </strong>Junchao Fan, Xuyang Lei, Xiaolin Chang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18304">https://arxiv.org/abs/2506.18304</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18304">https://arxiv.org/pdf/2506.18304</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18304]] Sharpening the Spear: Adaptive Expert-Guided Adversarial Attack Against DRL-based Autonomous Driving Policies(https://arxiv.org/abs/2506.18304)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Deep reinforcement learning (DRL) has emerged as a promising paradigm for autonomous driving. However, despite their advanced capabilities, DRL-based policies remain highly vulnerable to adversarial attacks, posing serious safety risks in real-world deployments. Investigating such attacks is crucial for revealing policy vulnerabilities and guiding the development of more robust autonomous systems. While prior attack methods have made notable progress, they still face several challenges: 1) they often rely on high-frequency attacks, yet critical attack opportunities are typically context-dependent and temporally sparse, resulting in inefficient attack patterns; 2) restricting attack frequency can improve efficiency but often results in unstable training due to the adversary's limited exploration. To address these challenges, we propose an adaptive expert-guided adversarial attack method that enhances both the stability and efficiency of attack policy training. Our method first derives an expert policy from successful attack demonstrations using imitation learning, strengthened by an ensemble Mixture-of-Experts architecture for robust generalization across scenarios. This expert policy then guides a DRL-based adversary through a KL-divergence regularization term. Due to the diversity of scenarios, expert policies may be imperfect. To address this, we further introduce a performance-aware annealing strategy that gradually reduces reliance on the expert as the adversary improves. Extensive experiments demonstrate that our method achieves outperforms existing approaches in terms of collision rate, attack efficiency, and training stability, especially in cases where the expert policy is sub-optimal.</li>
</ul>

<h3>Title: Attention-Based Ensemble Learning for Crop Classification Using Landsat 8-9 Fusion</h3>
<ul>
<li><strong>Authors: </strong>Zeeshan Ramzan, Nisar Ahmed, Qurat-ul-Ain Akram, Shahzad Asif, Muhammad Shahbaz, Rabin Chakrabortty, Ahmed F. Elaksher</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18321">https://arxiv.org/abs/2506.18321</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18321">https://arxiv.org/pdf/2506.18321</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18321]] Attention-Based Ensemble Learning for Crop Classification Using Landsat 8-9 Fusion(https://arxiv.org/abs/2506.18321)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Remote sensing offers a highly effective method for obtaining accurate information on total cropped area and crop types. The study focuses on crop cover identification for irrigated regions of Central Punjab. Data collection was executed in two stages: the first involved identifying and geocoding six target crops through field surveys conducted in January and February 2023. The second stage involved acquiring Landsat 8-9 imagery for each geocoded field to construct a labelled dataset. The satellite imagery underwent extensive pre-processing, including radiometric calibration for reflectance values, atmospheric correction, and georeferencing verification to ensure consistency within a common coordinate system. Subsequently, image fusion techniques were applied to combine Landsat 8 and 9 spectral bands, creating a composite image with enhanced spectral information, followed by contrast enhancement. During data acquisition, farmers were interviewed, and fields were meticulously mapped using GPS instruments, resulting in a comprehensive dataset of 50,835 data points. This dataset facilitated the extraction of vegetation indices such as NDVI, SAVO, RECI, and NDRE. These indices and raw reflectance values were utilized for classification modeling using conventional classifiers, ensemble learning, and artificial neural networks. A feature selection approach was also incorporated to identify the optimal feature set for classification learning. This study demonstrates the effectiveness of combining remote sensing data and advanced modeling techniques to improve crop classification accuracy in irrigated agricultural regions.</li>
</ul>

<h3>Title: NSFW-Classifier Guided Prompt Sanitization for Safe Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Yu Xie, Chengjie Zeng, Lingyun Zhang, Yanwei Fu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18325">https://arxiv.org/abs/2506.18325</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18325">https://arxiv.org/pdf/2506.18325</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18325]] NSFW-Classifier Guided Prompt Sanitization for Safe Text-to-Image Generation(https://arxiv.org/abs/2506.18325)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, diffusion, large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancement of text-to-image (T2I) models, such as Stable Diffusion, has enhanced their capability to synthesize images from textual prompts. However, this progress also raises significant risks of misuse, including the generation of harmful content (e.g., pornography, violence, discrimination), which contradicts the ethical goals of T2I technology and hinders its sustainable development. Inspired by "jailbreak" attacks in large language models, which bypass restrictions through subtle prompt modifications, this paper proposes NSFW-Classifier Guided Prompt Sanitization (PromptSan), a novel approach to detoxify harmful prompts without altering model architecture or degrading generation capability. PromptSan includes two variants: PromptSan-Modify, which iteratively identifies and replaces harmful tokens in input prompts using text NSFW classifiers during inference, and PromptSan-Suffix, which trains an optimized suffix token sequence to neutralize harmful intent while passing both text and image NSFW classifier checks. Extensive experiments demonstrate that PromptSan achieves state-of-the-art performance in reducing harmful content generation across multiple metrics, effectively balancing safety and usability.</li>
</ul>

<h3>Title: Confucius3-Math: A Lightweight High-Performance Reasoning LLM for Chinese K-12 Mathematics Learning</h3>
<ul>
<li><strong>Authors: </strong>Lixin Wu, Na Cai, Qiao Cheng, Jiachen Wang, Yitao Duan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18330">https://arxiv.org/abs/2506.18330</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18330">https://arxiv.org/pdf/2506.18330</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18330]] Confucius3-Math: A Lightweight High-Performance Reasoning LLM for Chinese K-12 Mathematics Learning(https://arxiv.org/abs/2506.18330)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce Confucius3-Math, an open-source large language model with 14B parameters that (1) runs efficiently on a single consumer-grade GPU; (2) achieves SOTA performances on a range of mathematical reasoning tasks, outperforming many models with significantly larger sizes. In particular, as part of our mission to enhancing education and knowledge dissemination with AI, Confucius3-Math is specifically committed to mathematics learning for Chinese K-12 students and educators. Built via post-training with large-scale reinforcement learning (RL), Confucius3-Math aligns with national curriculum and excels at solving main-stream Chinese K-12 mathematical problems with low cost. In this report we share our development recipe, the challenges we encounter and the techniques we develop to overcome them. In particular, we introduce three technical innovations: Targeted Entropy Regularization, Recent Sample Recovery and Policy-Specific Hardness Weighting. These innovations encompass a new entropy regularization, a novel data scheduling policy, and an improved group-relative advantage estimator. Collectively, they significantly stabilize the RL training, improve data efficiency, and boost performance. Our work demonstrates the feasibility of building strong reasoning models in a particular domain at low cost. We open-source our model and code at this https URL.</li>
</ul>

<h3>Title: Geometry-Aware Preference Learning for 3D Texture Generation</h3>
<ul>
<li><strong>Authors: </strong>AmirHossein Zamani, Tianhao Xie, Amir G. Aghdam, Tiberiu Popa, Eugene Belilovsky</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18331">https://arxiv.org/abs/2506.18331</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18331">https://arxiv.org/pdf/2506.18331</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18331]] Geometry-Aware Preference Learning for 3D Texture Generation(https://arxiv.org/abs/2506.18331)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in 3D generative models have achieved impressive results but 3D contents generated by these models may not align with subjective human preferences or task-specific criteria. Moreover, a core challenge in the 3D texture generation domain remains: most existing approaches rely on repeated calls to 2D text-to-image generative models, which lack an inherent understanding of the 3D structure of the input 3D mesh object. To address this, we propose an end-to-end differentiable preference learning framework that back-propagates human preferences, represented by differentiable reward functions, through the entire 3D generative pipeline, making the process inherently geometry-aware. We demonstrate the effectiveness of our framework using four proposed novel geometry-aware reward functions, offering a more controllable and interpretable pathway for high-quality 3D content creation from natural language.</li>
</ul>

<h3>Title: Rethinking Decoder Design: Improving Biomarker Segmentation Using Depth-to-Space Restoration and Residual Linear Attention</h3>
<ul>
<li><strong>Authors: </strong>Saad Wazir, Daeyoung Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18335">https://arxiv.org/abs/2506.18335</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18335">https://arxiv.org/pdf/2506.18335</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18335]] Rethinking Decoder Design: Improving Biomarker Segmentation Using Depth-to-Space Restoration and Residual Linear Attention(https://arxiv.org/abs/2506.18335)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Segmenting biomarkers in medical images is crucial for various biotech applications. Despite advances, Transformer and CNN based methods often struggle with variations in staining and morphology, limiting feature extraction. In medical image segmentation, where datasets often have limited sample availability, recent state-of-the-art (SOTA) methods achieve higher accuracy by leveraging pre-trained encoders, whereas end-to-end methods tend to underperform. This is due to challenges in effectively transferring rich multiscale features from encoders to decoders, as well as limitations in decoder efficiency. To address these issues, we propose an architecture that captures multi-scale local and global contextual information and a novel decoder design, which effectively integrates features from the encoder, emphasizes important channels and regions, and reconstructs spatial dimensions to enhance segmentation accuracy. Our method, compatible with various encoders, outperforms SOTA methods, as demonstrated by experiments on four datasets and ablation studies. Specifically, our method achieves absolute performance gains of 2.76% on MoNuSeg, 3.12% on DSB, 2.87% on Electron Microscopy, and 4.03% on TNBC datasets compared to existing SOTA methods. Code: this https URL</li>
</ul>

<h3>Title: Controlled Generation with Equivariant Variational Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Floor Eijkelboom, Heiko Zimmermann, Sharvaree Vadgama, Erik J Bekkers, Max Welling, Christian A. Naesseth, Jan-Willem van de Meent</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18340">https://arxiv.org/abs/2506.18340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18340">https://arxiv.org/pdf/2506.18340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18340]] Controlled Generation with Equivariant Variational Flow Matching(https://arxiv.org/abs/2506.18340)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We derive a controlled generation objective within the framework of Variational Flow Matching (VFM), which casts flow matching as a variational inference problem. We demonstrate that controlled generation can be implemented two ways: (1) by way of end-to-end training of conditional generative models, or (2) as a Bayesian inference problem, enabling post hoc control of unconditional models without retraining. Furthermore, we establish the conditions required for equivariant generation and provide an equivariant formulation of VFM tailored for molecular generation, ensuring invariance to rotations, translations, and permutations. We evaluate our approach on both uncontrolled and controlled molecular generation, achieving state-of-the-art performance on uncontrolled generation and outperforming state-of-the-art models in controlled generation, both with end-to-end training and in the Bayesian inference setting. This work strengthens the connection between flow-based generative modeling and Bayesian inference, offering a scalable and principled framework for constraint-driven and symmetry-aware generation.</li>
</ul>

<h3>Title: Less Data Less Tokens: Multilingual Unification Learning for Efficient Test-Time Reasoning in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Kang Chen, Mengdi Zhang, Yixin Cao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18341">https://arxiv.org/abs/2506.18341</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18341">https://arxiv.org/pdf/2506.18341</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18341]] Less Data Less Tokens: Multilingual Unification Learning for Efficient Test-Time Reasoning in LLMs(https://arxiv.org/abs/2506.18341)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper explores the challenges of test-time scaling of large language models (LLMs), regarding both the data and inference efficiency. We highlight the diversity of multi-lingual reasoning based on our pilot studies, and then introduce a novel approach, \(L^2\) multi-lingual unification learning with a decoding intervention strategy for further investigation. The basic idea of \(L^2\) is that the reasoning process varies across different languages, which may be mutually beneficial to enhance both model performance and efficiency. In specific, there are two types of multi-lingual data: the entire long chain-of-thought annotations in different languages and the step-wise mixture of languages. By further tuning based on them, we show that even small amounts of data can significantly improve reasoning capabilities. Our findings suggest that multilingual learning reduces both the required data and the number of inference tokens while maintaining a comparable performance. Furthermore, \(L^2\) is orthogonal to other data efficient methods. Thus, we also emphasize the importance of diverse data selection. The \(L^2\) method offers a promising solution to the challenges of data collection and test-time compute efficiency in LLMs.</li>
</ul>

<h3>Title: SlimMoE: Structured Compression of Large MoE Models via Expert Slimming and Distillation</h3>
<ul>
<li><strong>Authors: </strong>Zichong Li, Chen Liang, Zixuan Zhang, Ilgee Hong, Young Jin Kim, Weizhu Chen, Tuo Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18349">https://arxiv.org/abs/2506.18349</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18349">https://arxiv.org/pdf/2506.18349</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18349]] SlimMoE: Structured Compression of Large MoE Models via Expert Slimming and Distillation(https://arxiv.org/abs/2506.18349)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The Mixture of Experts (MoE) architecture has emerged as a powerful paradigm for scaling large language models (LLMs) while maintaining inference efficiency. However, their enormous memory requirements make them prohibitively expensive to fine-tune or deploy in resource-constrained environments. To address this challenge, we introduce SlimMoE, a multi-stage compression framework for transforming large MoE models into much smaller, efficient variants without incurring the prohibitive costs of training from scratch. Our method systematically reduces parameter counts by slimming experts and transferring knowledge through intermediate stages, effectively mitigating the performance degradation common in one-shot pruning approaches. Using this framework, we compress Phi 3.5-MoE (41.9B total/6.6B activated parameters) to create Phi-mini-MoE (7.6B total/2.4B activated parameters) and Phi-tiny-MoE (3.8B total/1.1B activated parameters) using only 400B tokens--less than 10% of the original model's training data. These compressed models can be fine-tuned on a single GPU (A100 for Phi-mini-MoE, A6000 for Phi-tiny-MoE), making them highly suitable for academic and resource-limited settings. Our experiments demonstrate that these compressed models outperform others of similar size and remain competitive with larger models. For instance, Phi-mini-MoE achieves similar or better performance to Phi-3-mini using only 2/3 of the activated parameters and yields comparable MMLU scores to Llama 3.1 8B despite having significantly lower latency. Our findings demonstrate that structured pruning combined with staged distillation offers an effective path to creating high-quality, compact MoE models, paving the way for broader adoption of MoE architectures. We make our models publicly available at this https URL and this https URL .</li>
</ul>

<h3>Title: RePIC: Reinforced Post-Training for Personalizing Multi-Modal Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yeongtak Oh, Jisoo Mok, Dohyun Chung, Juhyeon Shin, Sangha Park, Johan Barthelemy, Sungroh Yoon</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18369">https://arxiv.org/abs/2506.18369</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18369">https://arxiv.org/pdf/2506.18369</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18369]] RePIC: Reinforced Post-Training for Personalizing Multi-Modal Language Models(https://arxiv.org/abs/2506.18369)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent multi-modal large language models (MLLMs) often struggle to generate personalized image captions, even when trained on high-quality captions. In this work, we observe that such limitations persist in existing post-training-based MLLM personalization methods. Specifically, despite being post-tuned with large-scale caption data through supervised fine-tuning (SFT), these models frequently fail to produce faithful descriptions in real-world scenarios, such as multi-concept image captioning. However, acquiring large-scale, high-quality captions for such complex settings is both costly and difficult. To address the data-centric nature of SFT, we propose a reinforcement learning (RL)-based post-training framework. To the best of our knowledge, this is the first RL-based approach to post-train MLLMs for personalized image captioning. Our method significantly enhances both visual recognition and personalized generation capabilities of MLLMs, and consistently outperforms existing SFT-based baselines, especially in the challenging multi-concept image captioning task.</li>
</ul>

<h3>Title: OpenEvents V1: Large-Scale Benchmark Dataset for Multimodal Event Grounding</h3>
<ul>
<li><strong>Authors: </strong>Hieu Nguyen, Phuc-Tan Nguyen, Thien-Phuc Tran, Minh-Quang Nguyen, Tam V. Nguyen, Minh-Triet Tran, Trung-Nghia Le</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18372">https://arxiv.org/abs/2506.18372</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18372">https://arxiv.org/pdf/2506.18372</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18372]] OpenEvents V1: Large-Scale Benchmark Dataset for Multimodal Event Grounding(https://arxiv.org/abs/2506.18372)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We introduce OpenEvents V1, a large-scale benchmark dataset aimed at advancing event-centric vision-language understanding. Unlike conventional image captioning and retrieval datasets that emphasize surface-level descriptions, OpenEvents V1 focuses on contextual and temporal grounding through two primary tasks: (1) generating rich, event-aware image captions and (2) retrieving event-relevant images based on narrative-style textual queries. The dataset contains over 200,000 news articles and 400,000 associated images sourced from CNN and The Guardian, spanning diverse domains and time periods. We provide extensive baseline results and standardized evaluation protocols for both tasks. OpenEvents V1 establishes a robust foundation for developing multimodal models capable of deep reasoning over complex real-world events. The dataset is available at this https URL</li>
</ul>

<h3>Title: Evaluating Causal Explanation in Medical Reports with LLM-Based and Human-Aligned Metrics</h3>
<ul>
<li><strong>Authors: </strong>Yousang Cho, Key-Sun Choi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18387">https://arxiv.org/abs/2506.18387</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18387">https://arxiv.org/pdf/2506.18387</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18387]] Evaluating Causal Explanation in Medical Reports with LLM-Based and Human-Aligned Metrics(https://arxiv.org/abs/2506.18387)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>This study investigates how accurately different evaluation metrics capture the quality of causal explanations in automatically generated diagnostic reports. We compare six metrics: BERTScore, Cosine Similarity, BioSentVec, GPT-White, GPT-Black, and expert qualitative assessment across two input types: observation-based and multiple-choice-based report generation. Two weighting strategies are applied: one reflecting task-specific priorities, and the other assigning equal weights to all metrics. Our results show that GPT-Black demonstrates the strongest discriminative power in identifying logically coherent and clinically valid causal narratives. GPT-White also aligns well with expert evaluations, while similarity-based metrics diverge from clinical reasoning quality. These findings emphasize the impact of metric selection and weighting on evaluation outcomes, supporting the use of LLM-based evaluation for tasks requiring interpretability and causal reasoning.</li>
</ul>

<h3>Title: ADNF-Clustering: An Adaptive and Dynamic Neuro-Fuzzy Clustering for Leukemia Prediction</h3>
<ul>
<li><strong>Authors: </strong>Marco Aruta, Ciro Listone, Giuseppe Murano, Aniello Murano</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18396">https://arxiv.org/abs/2506.18396</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18396">https://arxiv.org/pdf/2506.18396</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18396]] ADNF-Clustering: An Adaptive and Dynamic Neuro-Fuzzy Clustering for Leukemia Prediction(https://arxiv.org/abs/2506.18396)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Leukemia diagnosis and monitoring rely increasingly on high-throughput image data, yet conventional clustering methods lack the flexibility to accommodate evolving cellular patterns and quantify uncertainty in real time. We introduce Adaptive and Dynamic Neuro-Fuzzy Clustering, a novel streaming-capable framework that combines Convolutional Neural Network-based feature extraction with an online fuzzy clustering engine. ADNF initializes soft partitions via Fuzzy C-Means, then continuously updates micro-cluster centers, densities, and fuzziness parameters using a Fuzzy Temporal Index (FTI) that measures entropy evolution. A topology refinement stage performs density-weighted merging and entropy-guided splitting to guard against over- and under-segmentation. On the C-NMC leukemia microscopy dataset, our tool achieves a silhouette score of 0.51, demonstrating superior cohesion and separation over static baselines. The method's adaptive uncertainty modeling and label-free operation hold immediate potential for integration within the INFANT pediatric oncology network, enabling scalable, up-to-date support for personalized leukemia management.</li>
</ul>

<h3>Title: Lemmatization as a Classification Task: Results from Arabic across Multiple Genres</h3>
<ul>
<li><strong>Authors: </strong>Mostafa Saeed, Nizar Habash</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18399">https://arxiv.org/abs/2506.18399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18399">https://arxiv.org/pdf/2506.18399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18399]] Lemmatization as a Classification Task: Results from Arabic across Multiple Genres(https://arxiv.org/abs/2506.18399)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Lemmatization is crucial for NLP tasks in morphologically rich languages with ambiguous orthography like Arabic, but existing tools face challenges due to inconsistent standards and limited genre coverage. This paper introduces two novel approaches that frame lemmatization as classification into a Lemma-POS-Gloss (LPG) tagset, leveraging machine translation and semantic clustering. We also present a new Arabic lemmatization test set covering diverse genres, standardized alongside existing datasets. We evaluate character level sequence-to-sequence models, which perform competitively and offer complementary value, but are limited to lemma prediction (not LPG) and prone to hallucinating implausible forms. Our results show that classification and clustering yield more robust, interpretable outputs, setting new benchmarks for Arabic lemmatization.</li>
</ul>

<h3>Title: TReB: A Comprehensive Benchmark for Evaluating Table Reasoning Capabilities of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ce Li, Xiaofan Liu, Zhiyan Song, Ce Chi, Chen Zhao, Jingjing Yang, Zhendong Wang, Kexin Yang, Boshen Shi, Xing Wang, Chao Deng, Junlan Feng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18421">https://arxiv.org/abs/2506.18421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18421">https://arxiv.org/pdf/2506.18421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18421]] TReB: A Comprehensive Benchmark for Evaluating Table Reasoning Capabilities of Large Language Models(https://arxiv.org/abs/2506.18421)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, large language model</a></li>
<li><strong>Abstract: </strong>The majority of data in businesses and industries is stored in tables, databases, and data warehouses. Reasoning with table-structured data poses significant challenges for large language models (LLMs) due to its hidden semantics, inherent complexity, and structured nature. One of these challenges is lacking an effective evaluation benchmark fairly reflecting the performances of LLMs on broad table reasoning abilities. In this paper, we fill in this gap, presenting a comprehensive table reasoning evolution benchmark, TReB, which measures both shallow table understanding abilities and deep table reasoning abilities, a total of 26 sub-tasks. We construct a high quality dataset through an iterative data processing procedure. We create an evaluation framework to robustly measure table reasoning capabilities with three distinct inference modes, TCoT, PoT and ICoT. Further, we benchmark over 20 state-of-the-art LLMs using this frame work and prove its effectiveness. Experimental results reveal that existing LLMs still have significant room for improvement in addressing the complex and real world Table related tasks. Both the dataset and evaluation framework are publicly available, with the dataset hosted on [HuggingFace] and the framework on [GitHub].</li>
</ul>

<h3>Title: Benchmarking Foundation Models and Parameter-Efficient Fine-Tuning for Prognosis Prediction in Medical Imaging</h3>
<ul>
<li><strong>Authors: </strong>Filippo Ruffini, Elena Mulero Ayllon, Linlin Shen, Paolo Soda, Valerio Guarrasi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18434">https://arxiv.org/abs/2506.18434</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18434">https://arxiv.org/pdf/2506.18434</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18434]] Benchmarking Foundation Models and Parameter-Efficient Fine-Tuning for Prognosis Prediction in Medical Imaging(https://arxiv.org/abs/2506.18434)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Artificial Intelligence (AI) holds significant promise for improving prognosis prediction in medical imaging, yet its effective application remains challenging. In this work, we introduce a structured benchmark explicitly designed to evaluate and compare the transferability of Convolutional Neural Networks and Foundation Models in predicting clinical outcomes in COVID-19 patients, leveraging diverse publicly available Chest X-ray datasets. Our experimental methodology extensively explores a wide set of fine-tuning strategies, encompassing traditional approaches such as Full Fine-Tuning and Linear Probing, as well as advanced Parameter-Efficient Fine-Tuning methods including Low-Rank Adaptation, BitFit, VeRA, and IA3. The evaluations were conducted across multiple learning paradigms, including both extensive full-data scenarios and more clinically realistic Few-Shot Learning settings, which are critical for modeling rare disease outcomes and rapidly emerging health threats. By implementing a large-scale comparative analysis involving a diverse selection of pretrained models, including general-purpose architectures pretrained on large-scale datasets such as CLIP and DINOv2, to biomedical-specific models like MedCLIP, BioMedCLIP, and PubMedCLIP, we rigorously assess each model's capacity to effectively adapt and generalize to prognosis tasks, particularly under conditions of severe data scarcity and pronounced class imbalance. The benchmark was designed to capture critical conditions common in prognosis tasks, including variations in dataset size and class distribution, providing detailed insights into the strengths and limitations of each fine-tuning strategy. This extensive and structured evaluation aims to inform the practical deployment and adoption of robust, efficient, and generalizable AI-driven solutions in real-world clinical prognosis prediction workflows.</li>
</ul>

<h3>Title: Frequency-Domain Fusion Transformer for Image Inpainting</h3>
<ul>
<li><strong>Authors: </strong>Sijin He, Guangfeng Lin, Tao Li, Yajun Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18437">https://arxiv.org/abs/2506.18437</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18437">https://arxiv.org/pdf/2506.18437</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18437]] Frequency-Domain Fusion Transformer for Image Inpainting(https://arxiv.org/abs/2506.18437)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Image inpainting plays a vital role in restoring missing image regions and supporting high-level vision tasks, but traditional methods struggle with complex textures and large occlusions. Although Transformer-based approaches have demonstrated strong global modeling capabilities, they often fail to preserve high-frequency details due to the low-pass nature of self-attention and suffer from high computational costs. To address these challenges, this paper proposes a Transformer-based image inpainting method incorporating frequency-domain fusion. Specifically, an attention mechanism combining wavelet transform and Gabor filtering is introduced to enhance multi-scale structural modeling and detail preservation. Additionally, a learnable frequency-domain filter based on the fast Fourier transform is designed to replace the feedforward network, enabling adaptive noise suppression and detail retention. The model adopts a four-level encoder-decoder structure and is guided by a novel loss strategy to balance global semantics and fine details. Experimental results demonstrate that the proposed method effectively improves the quality of image inpainting by preserving more high-frequency information.</li>
</ul>

<h3>Title: CPAM: Context-Preserving Adaptive Manipulation for Zero-Shot Real Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Dinh-Khoi Vo, Thanh-Toan Do, Tam V. Nguyen, Minh-Triet Tran, Trung-Nghia Le</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18438">https://arxiv.org/abs/2506.18438</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18438">https://arxiv.org/pdf/2506.18438</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18438]] CPAM: Context-Preserving Adaptive Manipulation for Zero-Shot Real Image Editing(https://arxiv.org/abs/2506.18438)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, diffusion</a></li>
<li><strong>Abstract: </strong>Editing natural images using textual descriptions in text-to-image diffusion models remains a significant challenge, particularly in achieving consistent generation and handling complex, non-rigid objects. Existing methods often struggle to preserve textures and identity, require extensive fine-tuning, and exhibit limitations in editing specific spatial regions or objects while retaining background details. This paper proposes Context-Preserving Adaptive Manipulation (CPAM), a novel zero-shot framework for complicated, non-rigid real image editing. Specifically, we propose a preservation adaptation module that adjusts self-attention mechanisms to preserve and independently control the object and background effectively. This ensures that the objects' shapes, textures, and identities are maintained while keeping the background undistorted during the editing process using the mask guidance technique. Additionally, we develop a localized extraction module to mitigate the interference with the non-desired modified regions during conditioning in cross-attention mechanisms. We also introduce various mask-guidance strategies to facilitate diverse image manipulation tasks in a simple manner. Extensive experiments on our newly constructed Image Manipulation BenchmArk (IMBA), a robust benchmark dataset specifically designed for real image editing, demonstrate that our proposed method is the preferred choice among human raters, outperforming existing state-of-the-art editing techniques.</li>
</ul>

<h3>Title: Adaptive alert prioritisation in security operations centres via learning to defer with human feedback</h3>
<ul>
<li><strong>Authors: </strong>Fatemeh Jalalvand, Mohan Baruwal Chhetri, Surya Nepal, Cécile Paris</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18462">https://arxiv.org/abs/2506.18462</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18462">https://arxiv.org/pdf/2506.18462</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18462]] Adaptive alert prioritisation in security operations centres via learning to defer with human feedback(https://arxiv.org/abs/2506.18462)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Alert prioritisation (AP) is crucial for security operations centres (SOCs) to manage the overwhelming volume of alerts and ensure timely detection and response to genuine threats, while minimising alert fatigue. Although predictive AI can process large alert volumes and identify known patterns, it struggles with novel and evolving scenarios that demand contextual understanding and nuanced judgement. A promising solution is Human-AI teaming (HAT), which combines human expertise with AI's computational capabilities. Learning to Defer (L2D) operationalises HAT by enabling AI to "defer" uncertain or unfamiliar cases to human experts. However, traditional L2D models rely on static deferral policies that do not evolve with experience, limiting their ability to learn from human feedback and adapt over time. To overcome this, we introduce Learning to Defer with Human Feedback (L2DHF), an adaptive deferral framework that leverages Deep Reinforcement Learning from Human Feedback (DRLHF) to optimise deferral decisions. By dynamically incorporating human feedback, L2DHF continuously improves AP accuracy and reduces unnecessary deferrals, enhancing SOC effectiveness and easing analyst workload. Experiments on two widely used benchmark datasets, UNSW-NB15 and CICIDS2017, demonstrate that L2DHF significantly outperforms baseline models. Notably, it achieves 13-16% higher AP accuracy for critical alerts on UNSW-NB15 and 60-67% on CICIDS2017. It also reduces misprioritisations, for example, by 98% for high-category alerts on CICIDS2017. Moreover, L2DHF decreases deferrals, for example, by 37% on UNSW-NB15, directly reducing analyst workload. These gains are achieved with efficient execution, underscoring L2DHF's practicality for real-world SOC deployment.</li>
</ul>

<h3>Title: DIP: Unsupervised Dense In-Context Post-training of Visual Representations</h3>
<ul>
<li><strong>Authors: </strong>Sophia Sirko-Galouchenko, Spyros Gidaris, Antonin Vobecky, Andrei Bursuc, Nicolas Thome</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18463">https://arxiv.org/abs/2506.18463</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18463">https://arxiv.org/pdf/2506.18463</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18463]] DIP: Unsupervised Dense In-Context Post-training of Visual Representations(https://arxiv.org/abs/2506.18463)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce DIP, a novel unsupervised post-training method designed to enhance dense image representations in large-scale pretrained vision encoders for in-context scene understanding. Unlike prior approaches that rely on complex self-distillation architectures, our method trains the vision encoder using pseudo-tasks that explicitly simulate downstream in-context scenarios, inspired by meta-learning principles. To enable post-training on unlabeled data, we propose an automatic mechanism for generating in-context tasks that combines a pretrained diffusion model and the vision encoder itself. DIP is simple, unsupervised, and computationally efficient, requiring less than 9 hours on a single A100 GPU. By learning dense representations through pseudo in-context tasks, it achieves strong performance across a wide variety of downstream real-world in-context scene understanding tasks. It outperforms both the initial vision encoder and prior methods, offering a practical and effective solution for improving dense representations. Code available here: this https URL</li>
</ul>

<h3>Title: Automatic Selection of Protections to Mitigate Risks Against Software Applications</h3>
<ul>
<li><strong>Authors: </strong>Daniele Canavese, Leonardo Regano, Bjorn De Sutter, Cataldo Basile</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18470">https://arxiv.org/abs/2506.18470</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18470">https://arxiv.org/pdf/2506.18470</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18470]] Automatic Selection of Protections to Mitigate Risks Against Software Applications(https://arxiv.org/abs/2506.18470)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, defense, attack</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel approach for the automated selection of software protections to mitigate MATE risks against critical assets within software applications. We formalize the key elements involved in protection decision-making - including code artifacts, assets, security requirements, attacks, and software protections - and frame the protection process through a game-theoretic model. In this model, a defender strategically applies protections to various code artifacts of a target application, anticipating repeated attack attempts by adversaries against the confidentiality and integrity of the application's assets. The selection of the optimal defense maximizes resistance to attacks while ensuring the application remains usable by constraining the overhead introduced by protections. The game is solved through a heuristic based on a mini-max depth-first exploration strategy, augmented with dynamic programming optimizations for improved efficiency. Central to our formulation is the introduction of the Software Protection Index, an original contribution that extends existing notions of potency and resilience by evaluating protection effectiveness against attack paths using software metrics and expert assessments. We validate our approach through a proof-of-concept implementation and expert evaluations, demonstrating that automated software protection is a practical and effective solution for risk mitigation in software.</li>
</ul>

<h3>Title: AViLA: Asynchronous Vision-Language Agent for Streaming Multimodal Data Interaction</h3>
<ul>
<li><strong>Authors: </strong>Gengyuan Zhang, Tanveer Hannan, Hermine Kleiner, Beste Aydemir, Xinyu Xie, Jian Lan, Thomas Seidl, Volker Tresp, Jindong Gu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18472">https://arxiv.org/abs/2506.18472</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18472">https://arxiv.org/pdf/2506.18472</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18472]] AViLA: Asynchronous Vision-Language Agent for Streaming Multimodal Data Interaction(https://arxiv.org/abs/2506.18472)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>An ideal vision-language agent serves as a bridge between the human users and their surrounding physical world in real-world applications like autonomous driving and embodied agents, and proactively provides accurate and timely responses given user intents. An intriguing challenge arises when agents interact with the world as a dynamic data stream and ad-hoc queries from users: supporting knowledge for queries, namely evidence, usually appears asynchronously with the arrival time of queries, and agents need to ground their responses in historical data, present observations, and even future streams. We frame this challenge as Query-Evidence Asynchrony, where user queries and their supporting evidence typically arrive asynchronously in the streaming setting. This setting requires not only strong reasoning capabilities but also the ability to retain past observations and respond to queries with temporal awareness. In this paper, we introduce a diagnostic benchmark that evaluates Multimodal Large Language Models (MLLMs) on their ability to handle interaction with streaming data. Further, we present AViLA, Asynchronous Video-Language Agent for streaming data interaction that can handle ad-hoc queries and give time-aware responses. For this purpose, AViLA consists of three key modules: comprehensive memory retention, evidence identification, and evidence-grounded trigger, that are designed to maintain a general-purpose memory and respond readily and timely to queries. Our experiments show that existing models often fail to respond at appropriate times, while AViLA significantly improves both accuracy and temporal awareness. Our code and dataset will be publicly available.</li>
</ul>

<h3>Title: FREQuency ATTribution: Benchmarking Frequency-based Occlusion for Time Series Data</h3>
<ul>
<li><strong>Authors: </strong>Dominique Mercier, Andreas Dengel, Sheraz, Ahmed</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18481">https://arxiv.org/abs/2506.18481</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18481">https://arxiv.org/pdf/2506.18481</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18481]] FREQuency ATTribution: Benchmarking Frequency-based Occlusion for Time Series Data(https://arxiv.org/abs/2506.18481)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Deep neural networks are among the most successful algorithms in terms of performance and scalability in different domains. However, since these networks are black boxes, their usability is severely restricted due to the lack of interpretability. Existing interpretability methods do not address the analysis of time-series-based networks specifically enough. This paper shows that an analysis in the frequency domain can not only highlight relevant areas in the input signal better than existing methods, but is also more robust to fluctuations in the signal. In this paper, FreqATT is presented, a framework that enables post-hoc networks to interpret time series analysis. To achieve this, the relevant different frequencies are evaluated and the signal is either filtered or the relevant input data is marked.</li>
</ul>

<h3>Title: GANs vs. Diffusion Models for virtual staining with the HER2match dataset</h3>
<ul>
<li><strong>Authors: </strong>Pascal Klöckner, José Teixeira, Diana Montezuma, Jaime S. Cardoso, Hugo M. Horlings, Sara P. Oliveira</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18484">https://arxiv.org/abs/2506.18484</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18484">https://arxiv.org/pdf/2506.18484</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18484]] GANs vs. Diffusion Models for virtual staining with the HER2match dataset(https://arxiv.org/abs/2506.18484)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Virtual staining is a promising technique that uses deep generative models to recreate histological stains, providing a faster and more cost-effective alternative to traditional tissue chemical staining. Specifically for H&E-HER2 staining transfer, despite a rising trend in publications, the lack of sufficient public datasets has hindered progress in the topic. Additionally, it is currently unclear which model frameworks perform best for this particular task. In this paper, we introduce the HER2match dataset, the first publicly available dataset with the same breast cancer tissue sections stained with both H&E and HER2. Furthermore, we compare the performance of several Generative Adversarial Networks (GANs) and Diffusion Models (DMs), and implement a novel Brownian Bridge Diffusion Model for H&E-HER2 translation. Our findings indicate that, overall, GANs perform better than DMs, with only the BBDM achieving comparable results. Furthermore, we emphasize the importance of data alignment, as all models trained on HER2match produced vastly improved visuals compared to the widely used consecutive-slide BCI dataset. This research provides a new high-quality dataset ([available upon publication acceptance]), improving both model training and evaluation. In addition, our comparison of frameworks offers valuable guidance for researchers working on the topic.</li>
</ul>

<h3>Title: MeRF: Motivation-enhanced Reinforcement Finetuning for Large Reasoning Models</h3>
<ul>
<li><strong>Authors: </strong>Junjie Zhang, Guozheng Ma, Shunyu Liu, Haoyu Wang, Jiaxing Huang, Ting-En Lin, Fei Huang, Yongbin Li, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18485">https://arxiv.org/abs/2506.18485</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18485">https://arxiv.org/pdf/2506.18485</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18485]] MeRF: Motivation-enhanced Reinforcement Finetuning for Large Reasoning Models(https://arxiv.org/abs/2506.18485)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful learn-to-reason paradigm for Large Language Models (LLMs) to tackle complex reasoning tasks. However, existing RLVR methods overlook one of the most distinctive capabilities of LLMs, their in-context learning ability, as prominently demonstrated by the success of Chain-of-Thought (CoT) prompting. This motivates us to explore how reinforcement learning can be effectively combined with in-context learning to better improve the reasoning capabilities of LLMs. In this paper, we introduce Motivation-enhanced Reinforcement Finetuning} (MeRF), an intuitive yet effective method enhancing reinforcement learning of LLMs by involving ``telling LLMs the rules of the game''. Specifically, MeRF directly injects the reward specification into the prompt, which serves as an in-context motivation for model to improve its responses with awareness of the optimization objective. This simple modification leverages the in-context learning ability of LLMs aligning generation with optimization, thereby incentivizing the model to generate desired outputs from both inner motivation and external reward. Empirical evaluations on the Knights and Knaves~(K&K) logic puzzle reasoning benchmark demonstrate that \texttt{MeRF} achieves substantial performance gains over baselines. Moreover, ablation studies show that performance improves with greater consistency between the in-context motivation and the external reward function, while the model also demonstrates an ability to adapt to misleading motivations through reinforcement learning.</li>
</ul>

<h3>Title: ShowFlow: From Robust Single Concept to Condition-Free Multi-Concept Generation</h3>
<ul>
<li><strong>Authors: </strong>Trong-Vu Hoang, Quang-Binh Nguyen, Thanh-Toan Do, Tam V. Nguyen, Minh-Triet Tran, Trung-Nghia Le</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18493">https://arxiv.org/abs/2506.18493</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18493">https://arxiv.org/pdf/2506.18493</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18493]] ShowFlow: From Robust Single Concept to Condition-Free Multi-Concept Generation(https://arxiv.org/abs/2506.18493)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Customizing image generation remains a core challenge in controllable image synthesis. For single-concept generation, maintaining both identity preservation and prompt alignment is challenging. In multi-concept scenarios, relying solely on a prompt without additional conditions like layout boxes or semantic masks, often leads to identity loss and concept omission. In this paper, we introduce ShowFlow, a comprehensive framework designed to tackle these challenges. We propose ShowFlow-S for single-concept image generation, and ShowFlow-M for handling multiple concepts. ShowFlow-S introduces a KronA-WED adapter, which integrates a Kronecker adapter with weight and embedding decomposition, and employs a disentangled learning approach with a novel attention regularization objective to enhance single-concept generation. Building on this foundation, ShowFlow-M directly reuses the learned models from ShowFlow-S to support multi-concept generation without extra conditions, incorporating a Subject-Adaptive Matching Attention (SAMA) and a layout consistency strategy as the plug-and-play module. Extensive experiments and user studies validate ShowFlow's effectiveness, highlighting its potential in real-world applications like advertising and virtual dressing.</li>
</ul>

<h3>Title: AnalogNAS-Bench: A NAS Benchmark for Analog In-Memory Computing</h3>
<ul>
<li><strong>Authors: </strong>Aniss Bessalah, Hatem Mohamed Abdelmoumen, Karima Benatchba, Hadjer Benmeziane</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18495">https://arxiv.org/abs/2506.18495</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18495">https://arxiv.org/pdf/2506.18495</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18495]] AnalogNAS-Bench: A NAS Benchmark for Analog In-Memory Computing(https://arxiv.org/abs/2506.18495)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Analog In-memory Computing (AIMC) has emerged as a highly efficient paradigm for accelerating Deep Neural Networks (DNNs), offering significant energy and latency benefits over conventional digital hardware. However, state-of-the-art neural networks are not inherently designed for AIMC, as they fail to account for its unique non-idealities. Neural Architecture Search (NAS) is thus needed to systematically discover neural architectures optimized explicitly for AIMC constraints. However, comparing NAS methodologies and extracting insights about robust architectures for AIMC requires a dedicated NAS benchmark that explicitly accounts for AIMC-specific hardware non-idealities. To address this, we introduce AnalogNAS-Bench, the first NAS benchmark tailored specifically for AIMC. Our study reveals three key insights: (1) standard quantization techniques fail to capture AIMC-specific noises, (2) robust architectures tend to feature wider and branched blocks, (3) skip connections improve resilience to temporal drift noise. These insights highlight the limitations of current NAS benchmarks for AIMC and pave the way for future analog-aware NAS. All the implementations used in this paper can be found at this https URL.</li>
</ul>

<h3>Title: PuckTrick: A Library for Making Synthetic Data More Realistic</h3>
<ul>
<li><strong>Authors: </strong>Alessandra Agostini, Andrea Maurino, Blerina Spahiu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18499">https://arxiv.org/abs/2506.18499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18499">https://arxiv.org/pdf/2506.18499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18499]] PuckTrick: A Library for Making Synthetic Data More Realistic(https://arxiv.org/abs/2506.18499)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust</a></li>
<li><strong>Abstract: </strong>The increasing reliance on machine learning (ML) models for decision-making requires high-quality training data. However, access to real-world datasets is often restricted due to privacy concerns, proprietary restrictions, and incomplete data availability. As a result, synthetic data generation (SDG) has emerged as a viable alternative, enabling the creation of artificial datasets that preserve the statistical properties of real data while ensuring privacy compliance. Despite its advantages, synthetic data is often overly clean and lacks real-world imperfections, such as missing values, noise, outliers, and misclassified labels, which can significantly impact model generalization and robustness. To address this limitation, we introduce Pucktrick, a Python library designed to systematically contaminate synthetic datasets by introducing controlled errors. The library supports multiple error types, including missing data, noisy values, outliers, label misclassification, duplication, and class imbalance, offering a structured approach to evaluating ML model resilience under real-world data imperfections. Pucktrick provides two contamination modes: one for injecting errors into clean datasets and another for further corrupting already contaminated datasets. Through extensive experiments on real-world financial datasets, we evaluate the impact of systematic data contamination on model performance. Our findings demonstrate that ML models trained on contaminated synthetic data outperform those trained on purely synthetic, error-free data, particularly for tree-based and linear models such as SVMs and Extra Trees.</li>
</ul>

<h3>Title: Comparative Evaluation of ChatGPT and DeepSeek Across Key NLP Tasks: Strengths, Weaknesses, and Domain-Specific Performance</h3>
<ul>
<li><strong>Authors: </strong>Wael Etaiwi, Bushra Alhijawi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18501">https://arxiv.org/abs/2506.18501</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18501">https://arxiv.org/pdf/2506.18501</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18501]] Comparative Evaluation of ChatGPT and DeepSeek Across Key NLP Tasks: Strengths, Weaknesses, and Domain-Specific Performance(https://arxiv.org/abs/2506.18501)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>The increasing use of large language models (LLMs) in natural language processing (NLP) tasks has sparked significant interest in evaluating their effectiveness across diverse applications. While models like ChatGPT and DeepSeek have shown strong results in many NLP domains, a comprehensive evaluation is needed to understand their strengths, weaknesses, and domain-specific abilities. This is critical as these models are applied to various tasks, from sentiment analysis to more nuanced tasks like textual entailment and translation. This study aims to evaluate ChatGPT and DeepSeek across five key NLP tasks: sentiment analysis, topic classification, text summarization, machine translation, and textual entailment. A structured experimental protocol is used to ensure fairness and minimize variability. Both models are tested with identical, neutral prompts and evaluated on two benchmark datasets per task, covering domains like news, reviews, and formal/informal texts. The results show that DeepSeek excels in classification stability and logical reasoning, while ChatGPT performs better in tasks requiring nuanced understanding and flexibility. These findings provide valuable insights for selecting the appropriate LLM based on task requirements.</li>
</ul>

<h3>Title: Generalizing Vision-Language Models to Novel Domains: A Comprehensive Survey</h3>
<ul>
<li><strong>Authors: </strong>Xinyao Li, Jingjing Li, Fengling Li, Lei Zhu, Yang Yang, Heng Tao Shen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18504">https://arxiv.org/abs/2506.18504</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18504">https://arxiv.org/pdf/2506.18504</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18504]] Generalizing Vision-Language Models to Novel Domains: A Comprehensive Survey(https://arxiv.org/abs/2506.18504)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recently, vision-language pretraining has emerged as a transformative technique that integrates the strengths of both visual and textual modalities, resulting in powerful vision-language models (VLMs). Leveraging web-scale pretraining data, these models exhibit strong zero-shot capabilities. However, their performance often deteriorates when confronted with domain-specific or specialized generalization tasks. To address this, a growing body of research focuses on transferring or generalizing the rich knowledge embedded in VLMs to various downstream applications. This survey aims to comprehensively summarize the generalization settings, methodologies, benchmarking and results in VLM literatures. Delving into the typical VLM structures, current literatures are categorized into prompt-based, parameter-based and feature-based methods according to the transferred modules. The differences and characteristics in each category are furthered summarized and discussed by revisiting the typical transfer learning (TL) settings, providing novel interpretations for TL in the era of VLMs. Popular benchmarks for VLM generalization are further introduced with thorough performance comparisons among the reviewed methods. Following the advances in large-scale generalizable pretraining, this survey also discusses the relations and differences between VLMs and up-to-date multimodal large language models (MLLM), e.g., DeepSeek-VL. By systematically reviewing the surging literatures in vision-language research from a novel and practical generalization prospective, this survey contributes to a clear landscape of current and future multimodal researches.</li>
</ul>

<h3>Title: MedTVT-R1: A Multimodal LLM Empowering Medical Reasoning and Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Yuting Zhang, Kaishen Yuan, Hao Lu, Yutao Yue, Jintai Chen, Kaishun Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18512">https://arxiv.org/abs/2506.18512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18512">https://arxiv.org/pdf/2506.18512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18512]] MedTVT-R1: A Multimodal LLM Empowering Medical Reasoning and Diagnosis(https://arxiv.org/abs/2506.18512)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Accurate and interpretable multi-disease diagnosis remains a critical challenge in medical research, particularly when leveraging heterogeneous multimodal medical data. Current approaches often rely on single-modal data, limiting their ability to comprehensively understand complex diseases. To address this, we propose MedTVT-R1, a novel Multimodal Large Language Model (MLLM) framework designed to integrate clinical multimodal data for reasoning and diagnosing multiple diseases. We construct MedTVT-QA, a curated instruction dataset that provides question-answer pairs for physiological-level interpretations and disease-level diagnoses with a Chain of Evidence approach. MedTVT-R1 incorporates a modality perception layer to capture inter-modal dependencies and adaptively weight modality contributions. Additionally, we employ Group Relative Policy Optimization (GRPO)-based Reinforcement Fine-Tuning with a Jaccard Reward function to enhance diagnostic reasoning. Experimental results demonstrate MedTVT-R1's superiority in multimodal feature utilization and multi-disease diagnosis, offering significant potential for clinical applications such as diagnostic report generation and comorbidity reasoning. The dataset and code are available at this https URL.</li>
</ul>

<h3>Title: DUMB and DUMBer: Is Adversarial Training Worth It in the Real World?</h3>
<ul>
<li><strong>Authors: </strong>Francesco Marchiori, Marco Alecci, Luca Pajola, Mauro Conti</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18516">https://arxiv.org/abs/2506.18516</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18516">https://arxiv.org/pdf/2506.18516</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18516]] DUMB and DUMBer: Is Adversarial Training Worth It in the Real World?(https://arxiv.org/abs/2506.18516)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>Adversarial examples are small and often imperceptible perturbations crafted to fool machine learning models. These attacks seriously threaten the reliability of deep neural networks, especially in security-sensitive domains. Evasion attacks, a form of adversarial attack where input is modified at test time to cause misclassification, are particularly insidious due to their transferability: adversarial examples crafted against one model often fool other models as well. This property, known as adversarial transferability, complicates defense strategies since it enables black-box attacks to succeed without direct access to the victim model. While adversarial training is one of the most widely adopted defense mechanisms, its effectiveness is typically evaluated on a narrow and homogeneous population of models. This limitation hinders the generalizability of empirical findings and restricts practical adoption. In this work, we introduce DUMBer, an attack framework built on the foundation of the DUMB (Dataset soUrces, Model architecture, and Balance) methodology, to systematically evaluate the resilience of adversarially trained models. Our testbed spans multiple adversarial training techniques evaluated across three diverse computer vision tasks, using a heterogeneous population of uniquely trained models to reflect real-world deployment variability. Our experimental pipeline comprises over 130k evaluations spanning 13 state-of-the-art attack algorithms, allowing us to capture nuanced behaviors of adversarial training under varying threat models and dataset conditions. Our findings offer practical, actionable insights for AI practitioners, identifying which defenses are most effective based on the model, dataset, and attacker setup.</li>
</ul>

<h3>Title: Enhancing Image Restoration Transformer via Adaptive Translation Equivariance</h3>
<ul>
<li><strong>Authors: </strong>JiaKui Hu, Zhengjian Yao, Lujia Jin, Hangzhou He, Yanye Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18520">https://arxiv.org/abs/2506.18520</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18520">https://arxiv.org/pdf/2506.18520</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18520]] Enhancing Image Restoration Transformer via Adaptive Translation Equivariance(https://arxiv.org/abs/2506.18520)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Translation equivariance is a fundamental inductive bias in image restoration, ensuring that translated inputs produce translated outputs. Attention mechanisms in modern restoration transformers undermine this property, adversely impacting both training convergence and generalization. To alleviate this issue, we propose two key strategies for incorporating translation equivariance: slide indexing and component stacking. Slide indexing maintains operator responses at fixed positions, with sliding window attention being a notable example, while component stacking enables the arrangement of translation-equivariant operators in parallel or sequentially, thereby building complex architectures while preserving translation equivariance. However, these strategies still create a dilemma in model design between the high computational cost of self-attention and the fixed receptive field associated with sliding window attention. To address this, we develop an adaptive sliding indexing mechanism to efficiently select key-value pairs for each query, which are then concatenated in parallel with globally aggregated key-value pairs. The designed network, called the Translation Equivariance Adaptive Transformer (TEAFormer), is assessed across a variety of image restoration tasks. The results highlight its superiority in terms of effectiveness, training convergence, and generalization.</li>
</ul>

<h3>Title: DDOT: A Derivative-directed Dual-decoder Ordinary Differential Equation Transformer for Dynamic System Modeling</h3>
<ul>
<li><strong>Authors: </strong>Yang Chang, Kuang-Da Wang, Ping-Chun Hsieh, Cheng-Kuan Lin, Wen-Chih Peng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18522">https://arxiv.org/abs/2506.18522</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18522">https://arxiv.org/pdf/2506.18522</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18522]] DDOT: A Derivative-directed Dual-decoder Ordinary Differential Equation Transformer for Dynamic System Modeling(https://arxiv.org/abs/2506.18522)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Uncovering the underlying ordinary differential equations (ODEs) that govern dynamic systems is crucial for advancing our understanding of complex phenomena. Traditional symbolic regression methods often struggle to capture the temporal dynamics and intervariable correlations inherent in ODEs. ODEFormer, a state-of-the-art method for inferring multidimensional ODEs from single trajectories, has made notable progress. However, its focus on single-trajectory evaluation is highly sensitive to initial starting points, which may not fully reflect true performance. To address this, we propose the divergence difference metric (DIV-diff), which evaluates divergence over a grid of points within the target region, offering a comprehensive and stable analysis of the variable space. Alongside, we introduce DDOT (Derivative-Directed Dual-Decoder Ordinary Differential Equation Transformer), a transformer-based model designed to reconstruct multidimensional ODEs in symbolic form. By incorporating an auxiliary task predicting the ODE's derivative, DDOT effectively captures both structure and dynamic behavior. Experiments on ODEBench show DDOT outperforms existing symbolic regression methods, achieving an absolute improvement of 4.58% and 1.62% in $P(R^2 > 0.9)$ for reconstruction and generalization tasks, respectively, and an absolute reduction of 3.55% in DIV-diff. Furthermore, DDOT demonstrates real-world applicability on an anesthesia dataset, highlighting its practical impact.</li>
</ul>

<h3>Title: Federated Learning from Molecules to Processes: A Perspective</h3>
<ul>
<li><strong>Authors: </strong>Jan G. Rittig, Clemens Kortmann</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.chem-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18525">https://arxiv.org/abs/2506.18525</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18525">https://arxiv.org/pdf/2506.18525</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18525]] Federated Learning from Molecules to Processes: A Perspective(https://arxiv.org/abs/2506.18525)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>We present a perspective on federated learning in chemical engineering that envisions collaborative efforts in machine learning (ML) developments within the chemical industry. Large amounts of chemical and process data are proprietary to chemical companies and are therefore locked in data silos, hindering the training of ML models on large data sets in chemical engineering. Recently, the concept of federated learning has gained increasing attention in ML research, enabling organizations to jointly train machine learning models without disclosure of their individual data. We discuss potential applications of federated learning in several fields of chemical engineering, from the molecular to the process scale. In addition, we apply federated learning in two exemplary case studies that simulate practical scenarios of multiple chemical companies holding proprietary data sets: (i) prediction of binary mixture activity coefficients with graph neural networks and (ii) system identification of a distillation column with autoencoders. Our results indicate that ML models jointly trained with federated learning yield significantly higher accuracy than models trained by each chemical company individually and can perform similarly to models trained on combined datasets from all companies. Federated learning has therefore great potential to advance ML models in chemical engineering while respecting corporate data privacy, making it promising for future industrial applications.</li>
</ul>

<h3>Title: Auto-Regressively Generating Multi-View Consistent Images</h3>
<ul>
<li><strong>Authors: </strong>JiaKui Hu, Yuxiao Yang, Jialun Liu, Jinbo Wu, Chen Zhao, Yanye Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18527">https://arxiv.org/abs/2506.18527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18527">https://arxiv.org/pdf/2506.18527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18527]] Auto-Regressively Generating Multi-View Consistent Images(https://arxiv.org/abs/2506.18527)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Generating multi-view images from human instructions is crucial for 3D content creation. The primary challenges involve maintaining consistency across multiple views and effectively synthesizing shapes and textures under diverse conditions. In this paper, we propose the Multi-View Auto-Regressive (MV-AR) method, which leverages an auto-regressive model to progressively generate consistent multi-view images from arbitrary prompts. Firstly, the next-token-prediction capability of the AR model significantly enhances its effectiveness in facilitating progressive multi-view synthesis. When generating widely-separated views, MV-AR can utilize all its preceding views to extract effective reference information. Subsequently, we propose a unified model that accommodates various prompts via architecture designing and training strategies. To address multiple conditions, we introduce condition injection modules for text, camera pose, image, and shape. To manage multi-modal conditions simultaneously, a progressive training strategy is employed. This strategy initially adopts the text-to-multi-view (t2mv) model as a baseline to enhance the development of a comprehensive X-to-multi-view (X2mv) model through the randomly dropping and combining conditions. Finally, to alleviate the overfitting problem caused by limited high-quality data, we propose the "Shuffle View" data augmentation technique, thus significantly expanding the training data by several magnitudes. Experiments demonstrate the performance and versatility of our MV-AR, which consistently generates consistent multi-view images across a range of conditions and performs on par with leading diffusion-based multi-view image generation models. Code and models will be released at this https URL.</li>
</ul>

<h3>Title: A Set-to-Set Distance Measure in Hyperbolic Space</h3>
<ul>
<li><strong>Authors: </strong>Pengxiang Li, Wei Wu, Zhi Gao, Xiaomeng Fan, Peilin Yu, Yuwei Wu, Zhipeng Lu, Yunde Jia, Mehrtash Harandi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18529">https://arxiv.org/abs/2506.18529</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18529">https://arxiv.org/pdf/2506.18529</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18529]] A Set-to-Set Distance Measure in Hyperbolic Space(https://arxiv.org/abs/2506.18529)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We propose a hyperbolic set-to-set distance measure for computing dissimilarity between sets in hyperbolic space. While point-to-point distances in hyperbolic space effectively capture hierarchical relationships between data points, many real-world applications require comparing sets of hyperbolic data points, where the local structure and the global structure of the sets carry crucial semantic information. The proposed the \underline{h}yperbolic \underline{s}et-\underline{to}-\underline{s}et \underline{d}istance measure (HS2SD) integrates both global and local structural information: global structure through geodesic distances between Einstein midpoints of hyperbolic sets, and local structure through topological characteristics of the two sets. To efficiently compute topological differences, we prove that using a finite Thue-Morse sequence of degree and adjacency matrices can serve as a robust approximation to capture the topological structure of a set. In this case, by considering the topological differences, HS2SD provides a more nuanced understanding of the relationships between two hyperbolic sets. Empirical evaluation on entity matching, standard image classification, and few-shot image classification demonstrates that our distance measure outperforms existing methods by effectively modeling the hierarchical and complex relationships inherent in hyperbolic sets.</li>
</ul>

<h3>Title: Geometry-aware Distance Measure for Diverse Hierarchical Structures in Hyperbolic Spaces</h3>
<ul>
<li><strong>Authors: </strong>Pengxiang Li, Yuwei Wu, Zhi Gao, Xiaomeng Fan, Wei Wu, Zhipeng Lu, Yunde Jia, Mehrtash Harandi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18533">https://arxiv.org/abs/2506.18533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18533">https://arxiv.org/pdf/2506.18533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18533]] Geometry-aware Distance Measure for Diverse Hierarchical Structures in Hyperbolic Spaces(https://arxiv.org/abs/2506.18533)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Learning in hyperbolic spaces has attracted increasing attention due to its superior ability to model hierarchical structures of data. Most existing hyperbolic learning methods use fixed distance measures for all data, assuming a uniform hierarchy across all data points. However, real-world hierarchical structures exhibit significant diversity, making this assumption overly restrictive. In this paper, we propose a geometry-aware distance measure in hyperbolic spaces, which dynamically adapts to varying hierarchical structures. Our approach derives the distance measure by generating tailored projections and curvatures for each pair of data points, effectively mapping them to an appropriate hyperbolic space. We introduce a revised low-rank decomposition scheme and a hard-pair mining mechanism to mitigate the computational cost of pair-wise distance computation without compromising accuracy. We present an upper bound on the low-rank approximation error using Talagrand's concentration inequality, ensuring theoretical robustness. Extensive experiments on standard image classification (MNIST, CIFAR-10 and CIFAR-100), hierarchical classification (5-level CIFAR-100), and few-shot learning tasks (mini-ImageNet, tiered-ImageNet) demonstrate the effectiveness of our method. Our approach consistently outperforms learning methods that use fixed distance measures, with notable improvements on few-shot learning tasks, where it achieves over 5\% gains on mini-ImageNet. The results reveal that adaptive distance measures better capture diverse hierarchical structures, with visualization showing clearer class boundaries and improved prototype separation in hyperbolic spaces.</li>
</ul>

<h3>Title: When Fine-Tuning Fails: Lessons from MS MARCO Passage Ranking</h3>
<ul>
<li><strong>Authors: </strong>Manu Pande, Shahil Kumar, Anay Yatin Damle</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18535">https://arxiv.org/abs/2506.18535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18535">https://arxiv.org/pdf/2506.18535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18535]] When Fine-Tuning Fails: Lessons from MS MARCO Passage Ranking(https://arxiv.org/abs/2506.18535)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This paper investigates the counterintuitive phenomenon where fine-tuning pre-trained transformer models degrades performance on the MS MARCO passage ranking task. Through comprehensive experiments involving five model variants-including full parameter fine-tuning and parameter efficient LoRA adaptations-we demonstrate that all fine-tuning approaches underperform the base sentence-transformers/all- MiniLM-L6-v2 model (MRR@10: 0.3026). Our analysis reveals that fine-tuning disrupts the optimal embedding space structure learned during the base model's extensive pre-training on 1 billion sentence pairs, including 9.1 million MS MARCO samples. UMAP visualizations show progressive embedding space flattening, while training dynamics analysis and computational efficiency metrics further support our findings. These results challenge conventional wisdom about transfer learning effectiveness on saturated benchmarks and suggest architectural innovations may be necessary for meaningful improvements.</li>
</ul>

<h3>Title: Transformer World Model for Sample Efficient Multi-Agent Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Azad Deihim, Eduardo Alonso, Dimitra Apostolopoulou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18537">https://arxiv.org/abs/2506.18537</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18537">https://arxiv.org/pdf/2506.18537</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18537]] Transformer World Model for Sample Efficient Multi-Agent Reinforcement Learning(https://arxiv.org/abs/2506.18537)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We present the Multi-Agent Transformer World Model (MATWM), a novel transformer-based world model designed for multi-agent reinforcement learning in both vector- and image-based environments. MATWM combines a decentralized imagination framework with a semi-centralized critic and a teammate prediction module, enabling agents to model and anticipate the behavior of others under partial observability. To address non-stationarity, we incorporate a prioritized replay mechanism that trains the world model on recent experiences, allowing it to adapt to agents' evolving policies. We evaluated MATWM on a broad suite of benchmarks, including the StarCraft Multi-Agent Challenge, PettingZoo, and MeltingPot. MATWM achieves state-of-the-art performance, outperforming both model-free and prior world model approaches, while demonstrating strong sample efficiency, achieving near-optimal performance in as few as 50K environment interactions. Ablation studies confirm the impact of each component, with substantial gains in coordination-heavy tasks.</li>
</ul>

<h3>Title: Security Assessment of DeepSeek and GPT Series Models against Jailbreak Attacks</h3>
<ul>
<li><strong>Authors: </strong>Xiaodong Wu, Xiangman Li, Jianbing Ni</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18543">https://arxiv.org/abs/2506.18543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18543">https://arxiv.org/pdf/2506.18543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18543]] Security Assessment of DeepSeek and GPT Series Models against Jailbreak Attacks(https://arxiv.org/abs/2506.18543)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>The widespread deployment of large language models (LLMs) has raised critical concerns over their vulnerability to jailbreak attacks, i.e., adversarial prompts that bypass alignment mechanisms and elicit harmful or policy-violating outputs. While proprietary models like GPT-4 have undergone extensive evaluation, the robustness of emerging open-source alternatives such as DeepSeek remains largely underexplored, despite their growing adoption in real-world applications. In this paper, we present the first systematic jailbreak evaluation of DeepSeek-series models, comparing them with GPT-3.5 and GPT-4 using the HarmBench benchmark. We evaluate seven representative attack strategies across 510 harmful behaviors categorized by both function and semantic domain. Our analysis reveals that DeepSeek's Mixture-of-Experts (MoE) architecture introduces routing sparsity that offers selective robustness against optimization-based attacks such as TAP-T, but leads to significantly higher vulnerability under prompt-based and manually engineered attacks. In contrast, GPT-4 Turbo demonstrates stronger and more consistent safety alignment across diverse behaviors, likely due to its dense Transformer design and reinforcement learning from human feedback. Fine-grained behavioral analysis and case studies further show that DeepSeek often routes adversarial prompts to under-aligned expert modules, resulting in inconsistent refusal behaviors. These findings highlight a fundamental trade-off between architectural efficiency and alignment generalization, emphasizing the need for targeted safety tuning and modular alignment strategies to ensure secure deployment of open-source LLMs.</li>
</ul>

<h3>Title: Object-aware Sound Source Localization via Audio-Visual Scene Understanding</h3>
<ul>
<li><strong>Authors: </strong>Sung Jin Um, Dongjin Kim, Sangmin Lee, Jung Uk Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18557">https://arxiv.org/abs/2506.18557</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18557">https://arxiv.org/pdf/2506.18557</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18557]] Object-aware Sound Source Localization via Audio-Visual Scene Understanding(https://arxiv.org/abs/2506.18557)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Audio-visual sound source localization task aims to spatially localize sound-making objects within visual scenes by integrating visual and audio cues. However, existing methods struggle with accurately localizing sound-making objects in complex scenes, particularly when visually similar silent objects coexist. This limitation arises primarily from their reliance on simple audio-visual correspondence, which does not capture fine-grained semantic differences between sound-making and silent objects. To address these challenges, we propose a novel sound source localization framework leveraging Multimodal Large Language Models (MLLMs) to generate detailed contextual information that explicitly distinguishes between sound-making foreground objects and silent background objects. To effectively integrate this detailed information, we introduce two novel loss functions: Object-aware Contrastive Alignment (OCA) loss and Object Region Isolation (ORI) loss. Extensive experimental results on MUSIC and VGGSound datasets demonstrate the effectiveness of our approach, significantly outperforming existing methods in both single-source and multi-source localization scenarios. Code and generated detailed contextual information are available at: this https URL.</li>
</ul>

<h3>Title: VisualChef: Generating Visual Aids in Cooking via Mask Inpainting</h3>
<ul>
<li><strong>Authors: </strong>Oleh Kuzyk, Zuoyue Li, Marc Pollefeys, Xi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18569">https://arxiv.org/abs/2506.18569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18569">https://arxiv.org/pdf/2506.18569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18569]] VisualChef: Generating Visual Aids in Cooking via Mask Inpainting(https://arxiv.org/abs/2506.18569)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Cooking requires not only following instructions but also understanding, executing, and monitoring each step - a process that can be challenging without visual guidance. Although recipe images and videos offer helpful cues, they often lack consistency in focus, tools, and setup. To better support the cooking process, we introduce VisualChef, a method for generating contextual visual aids tailored to cooking scenarios. Given an initial frame and a specified action, VisualChef generates images depicting both the action's execution and the resulting appearance of the object, while preserving the initial frame's environment. Previous work aims to integrate knowledge extracted from large language models by generating detailed textual descriptions to guide image generation, which requires fine-grained visual-textual alignment and involves additional annotations. In contrast, VisualChef simplifies alignment through mask-based visual grounding. Our key insight is identifying action-relevant objects and classifying them to enable targeted modifications that reflect the intended action and outcome while maintaining a consistent environment. In addition, we propose an automated pipeline to extract high-quality initial, action, and final state frames. We evaluate VisualChef quantitatively and qualitatively on three egocentric video datasets and show its improvements over state-of-the-art methods.</li>
</ul>

<h3>Title: Parallel Continuous Chain-of-Thought with Jacobi Iteration</h3>
<ul>
<li><strong>Authors: </strong>Haoyi Wu, Zhihao Teng, Kewei Tu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18582">https://arxiv.org/abs/2506.18582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18582">https://arxiv.org/pdf/2506.18582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18582]] Parallel Continuous Chain-of-Thought with Jacobi Iteration(https://arxiv.org/abs/2506.18582)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Continuous chain-of-thought has been shown to be effective in saving reasoning tokens for large language models. By reasoning with continuous latent thought tokens, continuous CoT is able to perform implicit reasoning in a compact manner. However, the sequential dependencies between latent thought tokens spoil parallel training, leading to long training time. In this paper, we propose Parallel Continuous Chain-of-Thought (PCCoT), which performs Jacobi iteration on the latent thought tokens, updating them iteratively in parallel instead of sequentially and thus improving both training and inference efficiency of continuous CoT. Experiments demonstrate that by choosing the proper number of iterations, we are able to achieve comparable or even better performance while saving nearly 50% of the training and inference time. Moreover, PCCoT shows better stability and robustness in the training process. Our code is available at this https URL.</li>
</ul>

<h3>Title: SpaNN: Detecting Multiple Adversarial Patches on CNNs by Spanning Saliency Thresholds</h3>
<ul>
<li><strong>Authors: </strong>Mauricio Byrd Victorica, György Dán, Henrik Sandberg</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18591">https://arxiv.org/abs/2506.18591</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18591">https://arxiv.org/pdf/2506.18591</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18591]] SpaNN: Detecting Multiple Adversarial Patches on CNNs by Spanning Saliency Thresholds(https://arxiv.org/abs/2506.18591)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>State-of-the-art convolutional neural network models for object detection and image classification are vulnerable to physically realizable adversarial perturbations, such as patch attacks. Existing defenses have focused, implicitly or explicitly, on single-patch attacks, leaving their sensitivity to the number of patches as an open question or rendering them computationally infeasible or inefficient against attacks consisting of multiple patches in the worst cases. In this work, we propose SpaNN, an attack detector whose computational complexity is independent of the expected number of adversarial patches. The key novelty of the proposed detector is that it builds an ensemble of binarized feature maps by applying a set of saliency thresholds to the neural activations of the first convolutional layer of the victim model. It then performs clustering on the ensemble and uses the cluster features as the input to a classifier for attack detection. Contrary to existing detectors, SpaNN does not rely on a fixed saliency threshold for identifying adversarial regions, which makes it robust against white box adversarial attacks. We evaluate SpaNN on four widely used data sets for object detection and classification, and our results show that SpaNN outperforms state-of-the-art defenses by up to 11 and 27 percentage points in the case of object detection and the case of image classification, respectively. Our code is available at this https URL.</li>
</ul>

<h3>Title: No Training Wheels: Steering Vectors for Bias Correction at Inference Time</h3>
<ul>
<li><strong>Authors: </strong>Aviral Gupta, Armaan Sethi, Ameesh Sethi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18598">https://arxiv.org/abs/2506.18598</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18598">https://arxiv.org/pdf/2506.18598</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18598]] No Training Wheels: Steering Vectors for Bias Correction at Inference Time(https://arxiv.org/abs/2506.18598)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative, large language model</a></li>
<li><strong>Abstract: </strong>Neural network classifiers trained on datasets with uneven group representation often inherit class biases and learn spurious correlations. These models may perform well on average but consistently fail on atypical groups. For example, in hair color classification, datasets may over-represent females with blond hair, reinforcing stereotypes. Although various algorithmic and data-centric methods have been proposed to address such biases, they often require retraining or significant compute. In this work, we propose a cheap, training-free method inspired by steering vectors used to edit behaviors in large language models. We compute the difference in mean activations between majority and minority groups to define a "bias vector," which we subtract from the model's residual stream. This leads to reduced classification bias and improved worst-group accuracy. We explore multiple strategies for extracting and applying these vectors in transformer-like classifiers, showing that steering vectors, traditionally used in generative models, can also be effective in classification. More broadly, we showcase an extremely cheap, inference time, training free method to mitigate bias in classification models.</li>
</ul>

<h3>Title: Reply to "Emergent LLM behaviors are observationally equivalent to data leakage"</h3>
<ul>
<li><strong>Authors: </strong>Ariel Flint Ashery, Luca Maria Aiello, Andrea Baronchelli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.GT, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18600">https://arxiv.org/abs/2506.18600</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18600">https://arxiv.org/pdf/2506.18600</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18600]] Reply to "Emergent LLM behaviors are observationally equivalent to data leakage"(https://arxiv.org/abs/2506.18600)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>A potential concern when simulating populations of large language models (LLMs) is data contamination, i.e. the possibility that training data may shape outcomes in unintended ways. While this concern is important and may hinder certain experiments with multi-agent models, it does not preclude the study of genuinely emergent dynamics in LLM populations. The recent critique by Barrie and Törnberg [1] of the results of Flint Ashery et al. [2] offers an opportunity to clarify that self-organisation and model-dependent emergent dynamics can be studied in LLM populations, highlighting how such dynamics have been empirically observed in the specific case of social conventions.</li>
</ul>

<h3>Title: Semantic similarity estimation for domain specific data using BERT and other techniques</h3>
<ul>
<li><strong>Authors: </strong>R. Prashanth</a></li>
<li><strong>Subjects: </strong>cs.CL, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18602">https://arxiv.org/abs/2506.18602</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18602">https://arxiv.org/pdf/2506.18602</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18602]] Semantic similarity estimation for domain specific data using BERT and other techniques(https://arxiv.org/abs/2506.18602)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Estimation of semantic similarity is an important research problem both in natural language processing and the natural language understanding, and that has tremendous application on various downstream tasks such as question answering, semantic search, information retrieval, document clustering, word-sense disambiguation and machine translation. In this work, we carry out the estimation of semantic similarity using different state-of-the-art techniques including the USE (Universal Sentence Encoder), InferSent and the most recent BERT, or Bidirectional Encoder Representations from Transformers, models. We use two question pairs datasets for the analysis, one is a domain specific in-house dataset and the other is a public dataset which is the Quora's question pairs dataset. We observe that the BERT model gave much superior performance as compared to the other methods. This should be because of the fine-tuning procedure that is involved in its training process, allowing it to learn patterns based on the training data that is used. This works demonstrates the applicability of BERT on domain specific datasets. We infer from the analysis that BERT is the best technique to use in the case of domain specific data.</li>
</ul>

<h3>Title: Simulation-Free Differential Dynamics through Neural Conservation Laws</h3>
<ul>
<li><strong>Authors: </strong>Mengjian Hua, Eric Vanden-Eijnden, Ricky T.Q. Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18604">https://arxiv.org/abs/2506.18604</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18604">https://arxiv.org/pdf/2506.18604</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18604]] Simulation-Free Differential Dynamics through Neural Conservation Laws(https://arxiv.org/abs/2506.18604)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We present a novel simulation-free framework for training continuous-time diffusion processes over very general objective functions. Existing methods typically involve either prescribing the optimal diffusion process -- which only works for heavily restricted problem formulations -- or require expensive simulation to numerically obtain the time-dependent densities and sample from the diffusion process. In contrast, we propose a coupled parameterization which jointly models a time-dependent density function, or probability path, and the dynamics of a diffusion process that generates this probability path. To accomplish this, our approach directly bakes in the Fokker-Planck equation and density function requirements as hard constraints, by extending and greatly simplifying the construction of Neural Conservation Laws. This enables simulation-free training for a large variety of problem formulations, from data-driven objectives as in generative modeling and dynamical optimal transport, to optimality-based objectives as in stochastic optimal control, with straightforward extensions to mean-field objectives due to the ease of accessing exact density functions. We validate our method in a diverse range of application domains from modeling spatio-temporal events to learning optimal dynamics from population data.</li>
</ul>

<h3>Title: The Anatomy of Speech Persuasion: Linguistic Shifts in LLM-Modified Speeches</h3>
<ul>
<li><strong>Authors: </strong>Alisa Barkar, Mathieu Chollet, Matthieu Labeau, Beatrice Biancardi, Chloe Clavel</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18621">https://arxiv.org/abs/2506.18621</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18621">https://arxiv.org/pdf/2506.18621</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18621]] The Anatomy of Speech Persuasion: Linguistic Shifts in LLM-Modified Speeches(https://arxiv.org/abs/2506.18621)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This study examines how large language models understand the concept of persuasiveness in public speaking by modifying speech transcripts from PhD candidates in the "Ma These en 180 Secondes" competition, using the 3MT French dataset. Our contributions include a novel methodology and an interpretable textual feature set integrating rhetorical devices and discourse markers. We prompt GPT-4o to enhance or diminish persuasiveness and analyze linguistic shifts between original and generated speech in terms of the new features. Results indicate that GPT-4o applies systematic stylistic modifications rather than optimizing persuasiveness in a human-like manner. Notably, it manipulates emotional lexicon and syntactic structures (such as interrogative and exclamatory clauses) to amplify rhetorical impact.</li>
</ul>

<h3>Title: ReDit: Reward Dithering for Improved LLM Policy Optimization</h3>
<ul>
<li><strong>Authors: </strong>Chenxing Wei, Jiarui Yu, Ying Tiffany He, Hande Dong, Yao Shu, Fei Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18631">https://arxiv.org/abs/2506.18631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18631">https://arxiv.org/pdf/2506.18631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18631]] ReDit: Reward Dithering for Improved LLM Policy Optimization(https://arxiv.org/abs/2506.18631)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning capabilities through its rule-based reward system. While it's a ''perfect'' reward system that effectively mitigates reward hacking, such reward functions are often discrete. Our experimental observations suggest that discrete rewards can lead to gradient anomaly, unstable optimization, and slow convergence. To address this issue, we propose ReDit (Reward Dithering), a method that dithers the discrete reward signal by adding simple random noise. With this perturbed reward, exploratory gradients are continuously provided throughout the learning process, enabling smoother gradient updates and accelerating convergence. The injected noise also introduces stochasticity into flat reward regions, encouraging the model to explore novel policies and escape local optima. Experiments across diverse tasks demonstrate the effectiveness and efficiency of ReDit. On average, ReDit achieves performance comparable to vanilla GRPO with only approximately 10% the training steps, and furthermore, still exhibits a 4% performance improvement over vanilla GRPO when trained for a similar duration. Visualizations confirm significant mitigation of gradient issues with ReDit. Moreover, theoretical analyses are provided to further validate these advantages.</li>
</ul>

<h3>Title: Granular-Ball-Induced Multiple Kernel K-Means</h3>
<ul>
<li><strong>Authors: </strong>Shuyin Xia, Yifan Wang, Lifeng Shen, Guoyin Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18637">https://arxiv.org/abs/2506.18637</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18637">https://arxiv.org/pdf/2506.18637</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18637]] Granular-Ball-Induced Multiple Kernel K-Means(https://arxiv.org/abs/2506.18637)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Most existing multi-kernel clustering algorithms, such as multi-kernel K-means, often struggle with computational efficiency and robustness when faced with complex data distributions. These challenges stem from their dependence on point-to-point relationships for optimization, which can lead to difficulty in accurately capturing data sets' inherent structure and diversity. Additionally, the intricate interplay between multiple kernels in such algorithms can further exacerbate these issues, effectively impacting their ability to cluster data points in high-dimensional spaces. In this paper, we leverage granular-ball computing to improve the multi-kernel clustering framework. The core of granular-ball computing is to adaptively fit data distribution by balls from coarse to acceptable levels. Each ball can enclose data points based on a density consistency measurement. Such ball-based data description thus improves the computational efficiency and the robustness to unknown noises. Specifically, based on granular-ball representations, we introduce the granular-ball kernel (GBK) and its corresponding granular-ball multi-kernel K-means framework (GB-MKKM) for efficient clustering. Using granular-ball relationships in multiple kernel spaces, the proposed GB-MKKM framework shows its superiority in efficiency and clustering performance in the empirical evaluation of various clustering tasks.</li>
</ul>

<h3>Title: ByteSpan: Information-Driven Subword Tokenisation</h3>
<ul>
<li><strong>Authors: </strong>Zébulon Goriely, Suchir Salhan, Pietro Lesci, Julius Cheng, Paula Buttery</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18639">https://arxiv.org/abs/2506.18639</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18639">https://arxiv.org/pdf/2506.18639</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18639]] ByteSpan: Information-Driven Subword Tokenisation(https://arxiv.org/abs/2506.18639)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Recent dynamic tokenisation methods operate directly on bytes and pool their latent representations into patches. This bears similarities to computational models of word segmentation that determine lexical boundaries using spikes in an autoregressive model's prediction error. Inspired by this connection, we explore whether grouping predictable bytes - rather than pooling their representations - can yield a useful fixed subword vocabulary. We propose a new information-driven subword tokeniser, ByteSpan, that uses an external byte-level LM during training to identify contiguous predictable byte sequences and group them into subwords. Experiments show that ByteSpan yields efficient vocabularies with higher morphological alignment scores than BPE for English. Multilingual experiments show similar compression and Rényi efficiency for 25 languages.</li>
</ul>

<h3>Title: Federated Loss Exploration for Improved Convergence on Non-IID Data</h3>
<ul>
<li><strong>Authors: </strong>Christian Internò, Markus Olhofer, Yaochu Jin, Barbara Hammer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18640">https://arxiv.org/abs/2506.18640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18640">https://arxiv.org/pdf/2506.18640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18640]] Federated Loss Exploration for Improved Convergence on Non-IID Data(https://arxiv.org/abs/2506.18640)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) has emerged as a groundbreaking paradigm in machine learning (ML), offering privacy-preserving collaborative model training across diverse datasets. Despite its promise, FL faces significant hurdles in non-identically and independently distributed (non-IID) data scenarios, where most existing methods often struggle with data heterogeneity and lack robustness in performance. This paper introduces Federated Loss Exploration (FedLEx), an innovative approach specifically designed to tackle these challenges. FedLEx distinctively addresses the shortcomings of existing FL methods in non-IID settings by optimizing its learning behavior for scenarios in which assumptions about data heterogeneity are impractical or unknown. It employs a federated loss exploration technique, where clients contribute to a global guidance matrix by calculating gradient deviations for model parameters. This matrix serves as a strategic compass to guide clients' gradient updates in subsequent FL rounds, thereby fostering optimal parameter updates for the global model. FedLEx effectively navigates the complex loss surfaces inherent in non-IID data, enhancing knowledge transfer in an efficient manner, since only a small number of epochs and small amount of data are required to build a strong global guidance matrix that can achieve model convergence without the need for additional data sharing or data distribution statics in a large client scenario. Our extensive experiments with state-of-the art FL algorithms demonstrate significant improvements in performance, particularly under realistic non-IID conditions, thus highlighting FedLEx's potential to overcome critical barriers in diverse FL applications.</li>
</ul>

<h3>Title: MedSeg-R: Medical Image Segmentation with Clinical Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Hao Shao, Qibin Hou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18669">https://arxiv.org/abs/2506.18669</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18669">https://arxiv.org/pdf/2506.18669</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18669]] MedSeg-R: Medical Image Segmentation with Clinical Reasoning(https://arxiv.org/abs/2506.18669)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Medical image segmentation is challenging due to overlapping anatomies with ambiguous boundaries and a severe imbalance between the foreground and background classes, which particularly affects the delineation of small lesions. Existing methods, including encoder-decoder networks and prompt-driven variants of the Segment Anything Model (SAM), rely heavily on local cues or user prompts and lack integrated semantic priors, thus failing to generalize well to low-contrast or overlapping targets. To address these issues, we propose MedSeg-R, a lightweight, dual-stage framework inspired by inspired by clinical reasoning. Its cognitive stage interprets medical report into structured semantic priors (location, texture, shape), which are fused via transformer block. In the perceptual stage, these priors modulate the SAM backbone: spatial attention highlights likely lesion regions, dynamic convolution adapts feature filters to expected textures, and deformable sampling refines spatial support. By embedding this fine-grained guidance early, MedSeg-R disentangles inter-class confusion and amplifies minority-class cues, greatly improving sensitivity to small lesions. In challenging benchmarks, MedSeg-R produces large Dice improvements in overlapping and ambiguous structures, demonstrating plug-and-play compatibility with SAM-based systems.</li>
</ul>

<h3>Title: Is There a Case for Conversation Optimized Tokenizers in Large Language Models?</h3>
<ul>
<li><strong>Authors: </strong>Raquel Ferrando, Javier Conde, Gonzalo Martínez, Pedro Reviriego</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18674">https://arxiv.org/abs/2506.18674</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18674">https://arxiv.org/pdf/2506.18674</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18674]] Is There a Case for Conversation Optimized Tokenizers in Large Language Models?(https://arxiv.org/abs/2506.18674)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The computational and energy costs of Large Language Models (LLMs) have increased exponentially driven by the growing model sizes and the massive adoption of LLMs by hundreds of millions of users. The unit cost of an LLM is the computation of a token. Therefore, the tokenizer plays an important role in the efficiency of a model, and they are carefully optimized to minimize the number of tokens for the text in their training corpus. One of the most popular applications of LLMs are chatbots that interact with users. A key observation is that, for those chatbots, what is important is the performance of the tokenizer in the user text input and the chatbot responses. Those are most likely different from the text in the training corpus. So, a question that immediately arises is whether there is a potential benefit in optimizing tokenizers for chatbot conversations. In this paper, this idea is explored for different tokenizers by using a publicly available corpus of chatbot conversations to redesign their vocabularies and evaluate their performance in this domain. The results show that conversation-optimized tokenizers consistently reduce the number of tokens in chatbot dialogues, which can lead to meaningful energy savings, in the range of 5% to 10% while having minimal or even slightly positive impact on tokenization efficiency for the original training corpus.</li>
</ul>

<h3>Title: MARL-MambaContour: Unleashing Multi-Agent Deep Reinforcement Learning for Active Contour Optimization in Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Ruicheng Zhang, Yu Sun, Zeyu Zhang, Jinai Li, Xiaofan Liu, Au Hoi Fan, Haowei Guo, Puxin Yan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18679">https://arxiv.org/abs/2506.18679</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18679">https://arxiv.org/pdf/2506.18679</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18679]] MARL-MambaContour: Unleashing Multi-Agent Deep Reinforcement Learning for Active Contour Optimization in Medical Image Segmentation(https://arxiv.org/abs/2506.18679)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>We introduce MARL-MambaContour, the first contour-based medical image segmentation framework based on Multi-Agent Reinforcement Learning (MARL). Our approach reframes segmentation as a multi-agent cooperation task focused on generate topologically consistent object-level contours, addressing the limitations of traditional pixel-based methods which could lack topological constraints and holistic structural awareness of anatomical regions. Each contour point is modeled as an autonomous agent that iteratively adjusts its position to align precisely with the target boundary, enabling adaptation to blurred edges and intricate morphologies common in medical images. This iterative adjustment process is optimized by a contour-specific Soft Actor-Critic (SAC) algorithm, further enhanced with the Entropy Regularization Adjustment Mechanism (ERAM) which dynamically balance agent exploration with contour smoothness. Furthermore, the framework incorporates a Mamba-based policy network featuring a novel Bidirectional Cross-attention Hidden-state Fusion Mechanism (BCHFM). This mechanism mitigates potential memory confusion limitations associated with long-range modeling in state space models, thereby facilitating more accurate inter-agent information exchange and informed decision-making. Extensive experiments on five diverse medical imaging datasets demonstrate the state-of-the-art performance of MARL-MambaContour, highlighting its potential as an accurate and robust clinical application.</li>
</ul>

<h3>Title: Multi-Scale Spectral Attention Module-based Hyperspectral Segmentation in Autonomous Driving Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Imad Ali Shah, Jiarong Li, Tim Brophy, Martin Glavin, Edward Jones, Enda Ward, Brian Deegan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18682">https://arxiv.org/abs/2506.18682</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18682">https://arxiv.org/pdf/2506.18682</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18682]] Multi-Scale Spectral Attention Module-based Hyperspectral Segmentation in Autonomous Driving Scenarios(https://arxiv.org/abs/2506.18682)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Recent advances in autonomous driving (AD) have highlighted the potential of Hyperspectral Imaging (HSI) for enhanced environmental perception, particularly in challenging weather and lighting conditions. However, efficiently processing its high-dimensional spectral data remains a significant challenge. This paper introduces a Multi-scale Spectral Attention Module (MSAM) that enhances spectral feature extraction through three parallel 1D convolutions with varying kernel sizes between 1 to 11, coupled with an adaptive feature aggregation mechanism. By integrating MSAM into UNet's skip connections (UNet-SC), our proposed UNet-MSAM achieves significant improvements in semantic segmentation performance across multiple HSI datasets: HyKo-VIS v2, HSI-Drive v2, and Hyperspectral City v2. Our comprehensive experiments demonstrate that with minimal computational overhead (on average 0.02% in parameters and 0.82% GFLOPS), UNet-MSAM consistently outperforms UNet-SC, achieving average improvements of 3.61% in mean IoU and 3.80% in mF1 across the three datasets. Through extensive ablation studies, we have established that multi-scale kernel combinations perform better than single-scale configurations. These findings demonstrate the potential of HSI processing for AD and provide valuable insights into designing robust, multi-scale spectral feature extractors for real-world applications.</li>
</ul>

<h3>Title: SIM-Net: A Multimodal Fusion Network Using Inferred 3D Object Shape Point Clouds from RGB Images for 2D Classification</h3>
<ul>
<li><strong>Authors: </strong>Youcef Sklab, Hanane Ariouat, Eric Chenin, Edi Prifti, Jean-Daniel Zucker</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18683">https://arxiv.org/abs/2506.18683</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18683">https://arxiv.org/pdf/2506.18683</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18683]] SIM-Net: A Multimodal Fusion Network Using Inferred 3D Object Shape Point Clouds from RGB Images for 2D Classification(https://arxiv.org/abs/2506.18683)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>We introduce the Shape-Image Multimodal Network (SIM-Net), a novel 2D image classification architecture that integrates 3D point cloud representations inferred directly from RGB images. Our key contribution lies in a pixel-to-point transformation that converts 2D object masks into 3D point clouds, enabling the fusion of texture-based and geometric features for enhanced classification performance. SIM-Net is particularly well-suited for the classification of digitized herbarium specimens (a task made challenging by heterogeneous backgrounds), non-plant elements, and occlusions that compromise conventional image-based models. To address these issues, SIM-Net employs a segmentation-based preprocessing step to extract object masks prior to 3D point cloud generation. The architecture comprises a CNN encoder for 2D image features and a PointNet-based encoder for geometric features, which are fused into a unified latent space. Experimental evaluations on herbarium datasets demonstrate that SIM-Net consistently outperforms ResNet101, achieving gains of up to 9.9% in accuracy and 12.3% in F-score. It also surpasses several transformer-based state-of-the-art architectures, highlighting the benefits of incorporating 3D structural reasoning into 2D image classification tasks.</li>
</ul>

<h3>Title: SaGIF: Improving Individual Fairness in Graph Neural Networks via Similarity Encoding</h3>
<ul>
<li><strong>Authors: </strong>Yuchang Zhu, Jintang Li, Huizhe Zhang, Liang Chen, Zibin Zheng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18696">https://arxiv.org/abs/2506.18696</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18696">https://arxiv.org/pdf/2506.18696</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18696]] SaGIF: Improving Individual Fairness in Graph Neural Networks via Similarity Encoding(https://arxiv.org/abs/2506.18696)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Individual fairness (IF) in graph neural networks (GNNs), which emphasizes the need for similar individuals should receive similar outcomes from GNNs, has been a critical issue. Despite its importance, research in this area has been largely unexplored in terms of (1) a clear understanding of what induces individual unfairness in GNNs and (2) a comprehensive consideration of identifying similar individuals. To bridge these gaps, we conduct a preliminary analysis to explore the underlying reason for individual unfairness and observe correlations between IF and similarity consistency, a concept introduced to evaluate the discrepancy in identifying similar individuals based on graph structure versus node features. Inspired by our observations, we introduce two metrics to assess individual similarity from two distinct perspectives: topology fusion and feature fusion. Building upon these metrics, we propose Similarity-aware GNNs for Individual Fairness, named SaGIF. The key insight behind SaGIF is the integration of individual similarities by independently learning similarity representations, leading to an improvement of IF in GNNs. Our experiments on several real-world datasets validate the effectiveness of our proposed metrics and SaGIF. Specifically, SaGIF consistently outperforms state-of-the-art IF methods while maintaining utility performance. Code is available at: this https URL.</li>
</ul>

<h3>Title: Benchmarking the Pedagogical Knowledge of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Maxime Lelièvre, Amy Waldock, Meng Liu, Natalia Valdés Aspillaga, Alasdair Mackintosh, María José Ogando Portelo, Jared Lee, Paul Atherton, Robin A. A. Ince, Oliver G. B. Garrod</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18710">https://arxiv.org/abs/2506.18710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18710">https://arxiv.org/pdf/2506.18710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18710]] Benchmarking the Pedagogical Knowledge of Large Language Models(https://arxiv.org/abs/2506.18710)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Benchmarks like Massive Multitask Language Understanding (MMLU) have played a pivotal role in evaluating AI's knowledge and abilities across diverse domains. However, existing benchmarks predominantly focus on content knowledge, leaving a critical gap in assessing models' understanding of pedagogy - the method and practice of teaching. This paper introduces The Pedagogy Benchmark, a novel dataset designed to evaluate large language models on their Cross-Domain Pedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND) pedagogical knowledge. These benchmarks are built on a carefully curated set of questions sourced from professional development exams for teachers, which cover a range of pedagogical subdomains such as teaching strategies and assessment methods. Here we outline the methodology and development of these benchmarks. We report results for 97 models, with accuracies spanning a range from 28% to 89% on the pedagogical knowledge questions. We consider the relationship between cost and accuracy and chart the progression of the Pareto value frontier over time. We provide online leaderboards at this https URL which are updated with new models and allow interactive exploration and filtering based on various model properties, such as cost per token and open-vs-closed weights, as well as looking at performance in different subjects. LLMs and generative AI have tremendous potential to influence education and help to address the global learning crisis. Education-focused benchmarks are crucial to measure models' capacities to understand pedagogical concepts, respond appropriately to learners' needs, and support effective teaching practices across diverse contexts. They are needed for informing the responsible and evidence-based deployment of LLMs and LLM-based tools in educational settings, and for guiding both development and policy decisions.</li>
</ul>

<h3>Title: Vulnerability Assessment Combining CVSS Temporal Metrics and Bayesian Networks</h3>
<ul>
<li><strong>Authors: </strong>Stefano Perone, Simone Guarino, Luca Faramondi, Roberto Setola</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18715">https://arxiv.org/abs/2506.18715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18715">https://arxiv.org/pdf/2506.18715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18715]] Vulnerability Assessment Combining CVSS Temporal Metrics and Bayesian Networks(https://arxiv.org/abs/2506.18715)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Vulnerability assessment is a critical challenge in cybersecurity, particularly in industrial environments. This work presents an innovative approach by incorporating the temporal dimension into vulnerability assessment, an aspect neglected in existing literature. Specifically, this paper focuses on refining vulnerability assessment and prioritization by integrating Common Vulnerability Scoring System (CVSS) Temporal Metrics with Bayesian Networks to account for exploit availability, remediation efforts, and confidence in reported vulnerabilities. Through probabilistic modeling, Bayesian networks enable a structured and adaptive evaluation of vulnerabilities, allowing for more accurate prioritization and decision-making. The proposed approach dynamically computes the Temporal Score and updates the CVSS Base Score by processing data on exploits and fixes from vulnerability databases.</li>
</ul>

<h3>Title: Multi-modal Anchor Gated Transformer with Knowledge Distillation for Emotion Recognition in Conversation</h3>
<ul>
<li><strong>Authors: </strong>Jie Li, Shifei Ding, Lili Guo, Xuan Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18716">https://arxiv.org/abs/2506.18716</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18716">https://arxiv.org/pdf/2506.18716</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18716]] Multi-modal Anchor Gated Transformer with Knowledge Distillation for Emotion Recognition in Conversation(https://arxiv.org/abs/2506.18716)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Emotion Recognition in Conversation (ERC) aims to detect the emotions of individual utterances within a conversation. Generating efficient and modality-specific representations for each utterance remains a significant challenge. Previous studies have proposed various models to integrate features extracted using different modality-specific encoders. However, they neglect the varying contributions of modalities to this task and introduce high complexity by aligning modalities at the frame level. To address these challenges, we propose the Multi-modal Anchor Gated Transformer with Knowledge Distillation (MAGTKD) for the ERC task. Specifically, prompt learning is employed to enhance textual modality representations, while knowledge distillation is utilized to strengthen representations of weaker modalities. Furthermore, we introduce a multi-modal anchor gated transformer to effectively integrate utterance-level representations across modalities. Extensive experiments on the IEMOCAP and MELD datasets demonstrate the effectiveness of knowledge distillation in enhancing modality representations and achieve state-of-the-art performance in emotion recognition. Our code is available at: this https URL.</li>
</ul>

<h3>Title: PARALLELPROMPT: Extracting Parallelism from Large Language Model Queries</h3>
<ul>
<li><strong>Authors: </strong>Steven Kolawole, Keshav Santhanam, Virginia Smith, Pratiksha Thaker</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18728">https://arxiv.org/abs/2506.18728</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18728">https://arxiv.org/pdf/2506.18728</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18728]] PARALLELPROMPT: Extracting Parallelism from Large Language Model Queries(https://arxiv.org/abs/2506.18728)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>LLM serving systems typically treat user prompts as monolithic inputs, optimizing inference through decoding tricks or inter-query batching. However, many real-world prompts contain latent semantic parallelism--decomposable structures where subtasks can be executed independently to reduce latency while preserving meaning. We introduce PARALLELPROMPT, the first benchmark for measuring intra-query parallelism in natural user prompts. Our dataset comprises over 37,000 real-world prompts from public LLM chat logs, each annotated with a structured schema capturing task templates, shared context, and iteration inputs. These schemas are extracted using LLM-assisted prompting with rule-based multilingual validation. To evaluate the benefits of decomposition, we provide an execution suite that benchmarks serial vs. parallel strategies, measuring latency, structural adherence, and semantic fidelity. Our results show that intra-query parallelism can be successfully parsed in over 75% of curated datasets, unlocking up to 5x speedups on tasks like translation, comprehension, and comparative analysis, with minimal quality degradation. By releasing this benchmark, curation pipeline, and evaluation suite, we provide the first standardized testbed for studying structure-aware execution in LLM serving pipelines.</li>
</ul>

<h3>Title: Deep CNN Face Matchers Inherently Support Revocable Biometric Templates</h3>
<ul>
<li><strong>Authors: </strong>Aman Bhatta, Michael C. King, Kevin W. Bowyer</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18731">https://arxiv.org/abs/2506.18731</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18731">https://arxiv.org/pdf/2506.18731</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18731]] Deep CNN Face Matchers Inherently Support Revocable Biometric Templates(https://arxiv.org/abs/2506.18731)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, biometric, transformer</a></li>
<li><strong>Abstract: </strong>One common critique of biometric authentication is that if an individual's biometric is compromised, then the individual has no recourse. The concept of revocable biometrics was developed to address this concern. A biometric scheme is revocable if an individual can have their current enrollment in the scheme revoked, so that the compromised biometric template becomes worthless, and the individual can re-enroll with a new template that has similar recognition power. We show that modern deep CNN face matchers inherently allow for a robust revocable biometric scheme. For a given state-of-the-art deep CNN backbone and training set, it is possible to generate an unlimited number of distinct face matcher models that have both (1) equivalent recognition power, and (2) strongly incompatible biometric templates. The equivalent recognition power extends to the point of generating impostor and genuine distributions that have the same shape and placement on the similarity dimension, meaning that the models can share a similarity threshold for a 1-in-10,000 false match rate. The biometric templates from different model instances are so strongly incompatible that the cross-instance similarity score for images of the same person is typically lower than the same-instance similarity score for images of different persons. That is, a stolen biometric template that is revoked is of less value in attempting to match the re-enrolled identity than the average impostor template. We also explore the feasibility of using a Vision Transformer (ViT) backbone-based face matcher in the revocable biometric system proposed in this work and demonstrate that it is less suitable compared to typical ResNet-based deep CNN backbones.</li>
</ul>

<h3>Title: Towards Group Fairness with Multiple Sensitive Attributes in Federated Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Yuning Yang, Han Yu, Tianrun Gao, Xiaodong Xu, Guangyu Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18732">https://arxiv.org/abs/2506.18732</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18732">https://arxiv.org/pdf/2506.18732</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18732]] Towards Group Fairness with Multiple Sensitive Attributes in Federated Foundation Models(https://arxiv.org/abs/2506.18732)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate, fair, interpretability</a></li>
<li><strong>Abstract: </strong>The deep integration of foundation models (FM) with federated learning (FL) enhances personalization and scalability for diverse downstream tasks, making it crucial in sensitive domains like healthcare. Achieving group fairness has become an increasingly prominent issue in the era of federated foundation models (FFMs), since biases in sensitive attributes might lead to inequitable treatment for under-represented demographic groups. Existing studies mostly focus on achieving fairness with respect to a single sensitive attribute. This renders them unable to provide clear interpretability of dependencies among multiple sensitive attributes which is required to achieve group fairness. Our paper takes the first attempt towards a causal analysis of the relationship between group fairness across various sensitive attributes in the FFM. We extend the FFM structure to trade off multiple sensitive attributes simultaneously and quantify the causal effect behind the group fairness through causal discovery and inference. Extensive experiments validate its effectiveness, offering insights into interpretability towards building trustworthy and fair FFM systems.</li>
</ul>

<h3>Title: USVTrack: USV-Based 4D Radar-Camera Tracking Dataset for Autonomous Driving in Inland Waterways</h3>
<ul>
<li><strong>Authors: </strong>Shanliang Yao, Runwei Guan, Yi Ni, Sen Xu, Yong Yue, Xiaohui Zhu, Ryan Wen Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18737">https://arxiv.org/abs/2506.18737</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18737">https://arxiv.org/pdf/2506.18737</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18737]] USVTrack: USV-Based 4D Radar-Camera Tracking Dataset for Autonomous Driving in Inland Waterways(https://arxiv.org/abs/2506.18737)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Object tracking in inland waterways plays a crucial role in safe and cost-effective applications, including waterborne transportation, sightseeing tours, environmental monitoring and surface rescue. Our Unmanned Surface Vehicle (USV), equipped with a 4D radar, a monocular camera, a GPS, and an IMU, delivers robust tracking capabilities in complex waterborne environments. By leveraging these sensors, our USV collected comprehensive object tracking data, which we present as USVTrack, the first 4D radar-camera tracking dataset tailored for autonomous driving in new generation waterborne transportation systems. Our USVTrack dataset presents rich scenarios, featuring diverse various waterways, varying times of day, and multiple weather and lighting conditions. Moreover, we present a simple but effective radar-camera matching method, termed RCM, which can be plugged into popular two-stage association trackers. Experimental results utilizing RCM demonstrate the effectiveness of the radar-camera matching in improving object tracking accuracy and reliability for autonomous driving in waterborne environments. The USVTrack dataset is public on this https URL.</li>
</ul>

<h3>Title: On the Existence of Universal Simulators of Attention</h3>
<ul>
<li><strong>Authors: </strong>Debanjan Dutta, Faizanuddin Ansari, Anish Chakrabarty, Swagatam Das</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18739">https://arxiv.org/abs/2506.18739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18739">https://arxiv.org/pdf/2506.18739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18739]] On the Existence of Universal Simulators of Attention(https://arxiv.org/abs/2506.18739)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Prior work on the learnability of transformers has established its capacity to approximate specific algorithmic patterns through training under restrictive architectural assumptions. Fundamentally, these arguments remain data-driven and therefore can only provide a probabilistic guarantee. Expressivity, on the contrary, has theoretically been explored to address the problems \emph{computable} by such architecture. These results proved the Turing-completeness of transformers, investigated bounds focused on circuit complexity, and formal logic. Being at the crossroad between learnability and expressivity, the question remains: \emph{can transformer architectures exactly simulate an arbitrary attention mechanism, or in particular, the underlying operations?} In this study, we investigate the transformer encoder's ability to simulate a vanilla attention mechanism. By constructing a universal simulator $\mathcal{U}$ composed of transformer encoders, we present algorithmic solutions to identically replicate attention outputs and the underlying elementary matrix and activation operations via RASP, a formal framework for transformer computation. Our proofs, for the first time, show the existence of an algorithmically achievable data-agnostic solution, previously known to be approximated only by learning.</li>
</ul>

<h3>Title: ContinualFlow: Learning and Unlearning with Neural Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Lorenzo Simone, Davide Bacciu, Shuangge Ma</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18747">https://arxiv.org/abs/2506.18747</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18747">https://arxiv.org/pdf/2506.18747</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18747]] ContinualFlow: Learning and Unlearning with Neural Flow Matching(https://arxiv.org/abs/2506.18747)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce ContinualFlow, a principled framework for targeted unlearning in generative models via Flow Matching. Our method leverages an energy-based reweighting loss to softly subtract undesired regions of the data distribution without retraining from scratch or requiring direct access to the samples to be unlearned. Instead, it relies on energy-based proxies to guide the unlearning process. We prove that this induces gradients equivalent to Flow Matching toward a soft mass-subtracted target, and validate the framework through experiments on 2D and image domains, supported by interpretable visualizations and quantitative evaluations.</li>
</ul>

<h3>Title: Semantic-Preserving Adversarial Attacks on LLMs: An Adaptive Greedy Binary Search Approach</h3>
<ul>
<li><strong>Authors: </strong>Chong Zhang, Xiang Li, Jia Wang, Shan Liang, Haochen Xue, Xiaobo Jin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18756">https://arxiv.org/abs/2506.18756</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18756">https://arxiv.org/pdf/2506.18756</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18756]] Semantic-Preserving Adversarial Attacks on LLMs: An Adaptive Greedy Binary Search Approach(https://arxiv.org/abs/2506.18756)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) increasingly rely on automatic prompt engineering in graphical user interfaces (GUIs) to refine user inputs and enhance response accuracy. However, the diversity of user requirements often leads to unintended misinterpretations, where automated optimizations distort original intentions and produce erroneous outputs. To address this challenge, we propose the Adaptive Greedy Binary Search (AGBS) method, which simulates common prompt optimization mechanisms while preserving semantic stability. Our approach dynamically evaluates the impact of such strategies on LLM performance, enabling robust adversarial sample generation. Through extensive experiments on open and closed-source LLMs, we demonstrate AGBS's effectiveness in balancing semantic consistency and attack efficacy. Our findings offer actionable insights for designing more reliable prompt optimization systems. Code is available at: this https URL</li>
</ul>

<h3>Title: Physical Layer Challenge-Response Authentication between Ambient Backscatter Devices</h3>
<ul>
<li><strong>Authors: </strong>Yifan Zhang, Yongchao Dang, Masoud Kaveh, Zheng Yan, Riku Jäntti, Zhu Han</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18767">https://arxiv.org/abs/2506.18767</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18767">https://arxiv.org/pdf/2506.18767</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18767]] Physical Layer Challenge-Response Authentication between Ambient Backscatter Devices(https://arxiv.org/abs/2506.18767)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>Ambient backscatter communication (AmBC) has become an integral part of ubiquitous Internet of Things (IoT) applications due to its energy-harvesting capabilities and ultra-low-power consumption. However, the open wireless environment exposes AmBC systems to various attacks, and existing authentication methods cannot be implemented between resource-constrained backscatter devices (BDs) due to their high computational this http URL this end, this paper proposes PLCRA-BD, a novel physical layer challenge-response authentication scheme between BDs in AmBC that overcomes BDs' limitations, supports high mobility, and performs robustly against impersonation and wireless attacks. It constructs embedded keys as physical layer fingerprints for lightweight identification and designs a joint transceiver that integrates BDs' backscatter waveform with receiver functionality to mitigate interference from ambient RF signals by exploiting repeated patterns in OFDM symbols. Based on this, a challenge-response authentication procedure is introduced to enable low-complexity fingerprint exchange between two paired BDs leveraging channel coherence, while securing the exchange process using a random number and unpredictable channel fading. Additionally, we optimize the authentication procedure for high-mobility scenarios, completing exchanges within the channel coherence time to minimize the impact of dynamic channel fluctuations. Security analysis confirms its resistance against impersonation, eavesdropping, replay, and counterfeiting attacks. Extensive simulations validate its effectiveness in resource-constrained BDs, demonstrating high authentication accuracy across diverse channel conditions, robustness against multiple wireless attacks, and superior efficiency compared to traditional authentication schemes.</li>
</ul>

<h3>Title: ASP2LJ : An Adversarial Self-Play Laywer Augmented Legal Judgment Framework</h3>
<ul>
<li><strong>Authors: </strong>Ao Chang, Tong Zhou, Yubo Chen, Delai Qiu, Shengping Liu, Kang Liu, Jun Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18768">https://arxiv.org/abs/2506.18768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18768">https://arxiv.org/pdf/2506.18768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18768]] ASP2LJ : An Adversarial Self-Play Laywer Augmented Legal Judgment Framework(https://arxiv.org/abs/2506.18768)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Legal Judgment Prediction (LJP) aims to predict judicial outcomes, including relevant legal charge, terms, and fines, which is a crucial process in Large Language Model(LLM). However, LJP faces two key challenges: (1)Long Tail Distribution: Current datasets, derived from authentic cases, suffer from high human annotation costs and imbalanced distributions, leading to model performance degradation. (2)Lawyer's Improvement: Existing systems focus on enhancing judges' decision-making but neglect the critical role of lawyers in refining arguments, which limits overall judicial accuracy. To address these issues, we propose an Adversarial Self-Play Lawyer Augmented Legal Judgment Framework, called ASP2LJ, which integrates a case generation module to tackle long-tailed data distributions and an adversarial self-play mechanism to enhance lawyers' argumentation skills. Our framework enables a judge to reference evolved lawyers' arguments, improving the objectivity, fairness, and rationality of judicial decisions. Besides, We also introduce RareCases, a dataset for rare legal cases in China, which contains 120 tail-end cases. We demonstrate the effectiveness of our approach on the SimuCourt dataset and our RareCases dataset. Experimental results show our framework brings improvements, indicating its utilization. Our contributions include an integrated framework, a rare-case dataset, and publicly releasing datasets and code to support further research in automated judicial systems.</li>
</ul>

<h3>Title: Design high-confidence computers using trusted instructional set architecture and emulators</h3>
<ul>
<li><strong>Authors: </strong>Shuangbao Paul Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18780">https://arxiv.org/abs/2506.18780</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18780">https://arxiv.org/pdf/2506.18780</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18780]] Design high-confidence computers using trusted instructional set architecture and emulators(https://arxiv.org/abs/2506.18780)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>High-confidence computing relies on trusted instructional set architecture, sealed kernels, and secure operating systems. Cloud computing depends on trusted systems for virtualization tasks. Branch predictions and pipelines are essential in improving performance of a CPU/GPU. But Spectre and Meltdown make modern processors vulnerable to be exploited. Disabling the prediction and pipeline is definitely not a good solution. On the other hand, current software patches can only address non-essential issues around Meltdown. This paper introduces a holistic approach in trusted computer architecture design and emulation.</li>
</ul>

<h3>Title: Existing LLMs Are Not Self-Consistent For Simple Tasks</h3>
<ul>
<li><strong>Authors: </strong>Zhenru Lin, Jiawen Tao, Yang Yuan, Andrew Chi-Chih Yao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18781">https://arxiv.org/abs/2506.18781</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18781">https://arxiv.org/pdf/2506.18781</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18781]] Existing LLMs Are Not Self-Consistent For Simple Tasks(https://arxiv.org/abs/2506.18781)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have grown increasingly powerful, yet ensuring their decisions remain transparent and trustworthy requires self-consistency -- no contradictions in their internal reasoning. Our study reveals that even on simple tasks, such as comparing points on a line or a plane, or reasoning in a family tree, all smaller models are highly inconsistent, and even state-of-the-art models like DeepSeek-R1 and GPT-o4-mini are not fully self-consistent. To quantify and mitigate these inconsistencies, we introduce inconsistency metrics and propose two automated methods -- a graph-based and an energy-based approach. While these fixes provide partial improvements, they also highlight the complexity and importance of self-consistency in building more reliable and interpretable AI. The code and data are available at this https URL.</li>
</ul>

<h3>Title: SWA-SOP: Spatially-aware Window Attention for Semantic Occupancy Prediction in Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Helin Cao, Rafael Materla, Sven Behnke</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18785">https://arxiv.org/abs/2506.18785</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18785">https://arxiv.org/pdf/2506.18785</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18785]] SWA-SOP: Spatially-aware Window Attention for Semantic Occupancy Prediction in Autonomous Driving(https://arxiv.org/abs/2506.18785)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Perception systems in autonomous driving rely on sensors such as LiDAR and cameras to perceive the 3D environment. However, due to occlusions and data sparsity, these sensors often fail to capture complete information. Semantic Occupancy Prediction (SOP) addresses this challenge by inferring both occupancy and semantics of unobserved regions. Existing transformer-based SOP methods lack explicit modeling of spatial structure in attention computation, resulting in limited geometric awareness and poor performance in sparse or occluded areas. To this end, we propose Spatially-aware Window Attention (SWA), a novel mechanism that incorporates local spatial context into attention. SWA significantly improves scene completion and achieves state-of-the-art results on LiDAR-based SOP benchmarks. We further validate its generality by integrating SWA into a camera-based SOP pipeline, where it also yields consistent gains across modalities.</li>
</ul>

<h3>Title: 3D Arena: An Open Platform for Generative 3D Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Dylan Ebert</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18787">https://arxiv.org/abs/2506.18787</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18787">https://arxiv.org/pdf/2506.18787</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18787]] 3D Arena: An Open Platform for Generative 3D Evaluation(https://arxiv.org/abs/2506.18787)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Evaluating Generative 3D models remains challenging due to misalignment between automated metrics and human perception of quality. Current benchmarks rely on image-based metrics that ignore 3D structure or geometric measures that fail to capture perceptual appeal and real-world utility. To address this gap, we present 3D Arena, an open platform for evaluating image-to-3D generation models through large-scale human preference collection using pairwise comparisons. Since launching in June 2024, the platform has collected 123,243 votes from 8,096 users across 19 state-of-the-art models, establishing the largest human preference evaluation for Generative 3D. We contribute the iso3d dataset of 100 evaluation prompts and demonstrate quality control achieving 99.75% user authenticity through statistical fraud detection. Our ELO-based ranking system provides reliable model assessment, with the platform becoming an established evaluation resource. Through analysis of this preference data, we present insights into human preference patterns. Our findings reveal preferences for visual presentation features, with Gaussian splat outputs achieving a 16.6 ELO advantage over meshes and textured models receiving a 144.1 ELO advantage over untextured models. We provide recommendations for improving evaluation methods, including multi-criteria assessment, task-oriented evaluation, and format-aware comparison. The platform's community engagement establishes 3D Arena as a benchmark for the field while advancing understanding of human-centered evaluation in Generative 3D.</li>
</ul>

<h3>Title: Shift Happens: Mixture of Experts based Continual Adaptation in Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Rahul Atul Bhope, K.R. Jayaram, Praveen Venkateswaran, Nalini Venkatasubramanian</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18789">https://arxiv.org/abs/2506.18789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18789">https://arxiv.org/pdf/2506.18789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18789]] Shift Happens: Mixture of Experts based Continual Adaptation in Federated Learning(https://arxiv.org/abs/2506.18789)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) enables collaborative model training across decentralized clients without sharing raw data, yet faces significant challenges in real-world settings where client data distributions evolve dynamically over time. This paper tackles the critical problem of covariate and label shifts in streaming FL environments, where non-stationary data distributions degrade model performance and require adaptive middleware solutions. We introduce ShiftEx, a shift-aware mixture of experts framework that dynamically creates and trains specialized global models in response to detected distribution shifts using Maximum Mean Discrepancy for covariate shifts. The framework employs a latent memory mechanism for expert reuse and implements facility location-based optimization to jointly minimize covariate mismatch, expert creation costs, and label imbalance. Through theoretical analysis and comprehensive experiments on benchmark datasets, we demonstrate 5.5-12.9 percentage point accuracy improvements and 22-95 % faster adaptation compared to state-of-the-art FL baselines across diverse shift scenarios. The proposed approach offers a scalable, privacy-preserving middleware solution for FL systems operating in non-stationary, real-world conditions while minimizing communication and computational overhead.</li>
</ul>

<h3>Title: Focus Your Attention: Towards Data-Intuitive Lightweight Vision Transformers</h3>
<ul>
<li><strong>Authors: </strong>Suyash Gaurav, Muhammad Farhan Humayun, Jukka Heikkonen, Jatin Chaudhary</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18791">https://arxiv.org/abs/2506.18791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18791">https://arxiv.org/pdf/2506.18791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18791]] Focus Your Attention: Towards Data-Intuitive Lightweight Vision Transformers(https://arxiv.org/abs/2506.18791)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The evolution of Vision Transformers has led to their widespread adaptation to different domains. Despite large-scale success, there remain significant challenges including their reliance on extensive computational and memory resources for pre-training on huge datasets as well as difficulties in task-specific transfer learning. These limitations coupled with energy inefficiencies mainly arise due to the computation-intensive self-attention mechanism. To address these issues, we propose a novel Super-Pixel Based Patch Pooling (SPPP) technique that generates context-aware, semantically rich, patch embeddings to effectively reduce the architectural complexity and improve efficiency. Additionally, we introduce the Light Latent Attention (LLA) module in our pipeline by integrating latent tokens into the attention mechanism allowing cross-attention operations to significantly reduce the time and space complexity of the attention module. By leveraging the data-intuitive patch embeddings coupled with dynamic positional encodings, our approach adaptively modulates the cross-attention process to focus on informative regions while maintaining the global semantic structure. This targeted attention improves training efficiency and accelerates convergence. Notably, the SPPP module is lightweight and can be easily integrated into existing transformer architectures. Extensive experiments demonstrate that our proposed architecture provides significant improvements in terms of computational efficiency while achieving comparable results with the state-of-the-art approaches, highlighting its potential for energy-efficient transformers suitable for edge deployment. (The code is available on our GitHub repository: this https URL).</li>
</ul>

<h3>Title: ViDAR: Video Diffusion-Aware 4D Reconstruction From Monocular Inputs</h3>
<ul>
<li><strong>Authors: </strong>Michal Nazarczuk, Sibi Catley-Chandar, Thomas Tanay, Zhensong Zhang, Gregory Slabaugh, Eduardo Pérez-Pellitero</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18792">https://arxiv.org/abs/2506.18792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18792">https://arxiv.org/pdf/2506.18792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18792]] ViDAR: Video Diffusion-Aware 4D Reconstruction From Monocular Inputs(https://arxiv.org/abs/2506.18792)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Dynamic Novel View Synthesis aims to generate photorealistic views of moving subjects from arbitrary viewpoints. This task is particularly challenging when relying on monocular video, where disentangling structure from motion is ill-posed and supervision is scarce. We introduce Video Diffusion-Aware Reconstruction (ViDAR), a novel 4D reconstruction framework that leverages personalised diffusion models to synthesise a pseudo multi-view supervision signal for training a Gaussian splatting representation. By conditioning on scene-specific features, ViDAR recovers fine-grained appearance details while mitigating artefacts introduced by monocular ambiguity. To address the spatio-temporal inconsistency of diffusion-based supervision, we propose a diffusion-aware loss function and a camera pose optimisation strategy that aligns synthetic views with the underlying scene geometry. Experiments on DyCheck, a challenging benchmark with extreme viewpoint variation, show that ViDAR outperforms all state-of-the-art baselines in visual quality and geometric consistency. We further highlight ViDAR's strong improvement over baselines on dynamic regions and provide a new benchmark to compare performance in reconstructing motion-rich parts of the scene. Project page: this https URL</li>
</ul>

<h3>Title: FORGE: An LLM-driven Framework for Large-Scale Smart Contract Vulnerability Dataset Construction</h3>
<ul>
<li><strong>Authors: </strong>Jiachi Chen, Yiming Shen, Jiashuo Zhang, Zihao Li, John Grundy, Zhenzhe Shao, Yanlin Wang, Jiashui Wang, Ting Chen, Zibin Zheng</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18795">https://arxiv.org/abs/2506.18795</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18795">https://arxiv.org/pdf/2506.18795</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18795]] FORGE: An LLM-driven Framework for Large-Scale Smart Contract Vulnerability Dataset Construction(https://arxiv.org/abs/2506.18795)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, extraction</a></li>
<li><strong>Abstract: </strong>High-quality smart contract vulnerability datasets are critical for evaluating security tools and advancing smart contract security research. Two major limitations of current manual dataset construction are (1) labor-intensive and error-prone annotation processes limiting the scale, quality, and evolution of the dataset, and (2) absence of standardized classification rules results in inconsistent vulnerability categories and labeling results across different datasets. To address these limitations, we present FORGE, the first automated approach for constructing smart contract vulnerability datasets. FORGE leverages an LLM-driven pipeline to extract high-quality vulnerabilities from real-world audit reports and classify them according to the CWE, the most widely recognized classification in software security. FORGE employs a divide-and-conquer strategy to extract structured and self-contained vulnerability information from these reports. Additionally, it uses a tree-of-thoughts technique to classify the vulnerability information into the hierarchical CWE classification. To evaluate FORGE's effectiveness, we run FORGE on 6,454 real-world audit reports and generate a dataset comprising 81,390 solidity files and 27,497 vulnerability findings across 296 CWE categories. Manual assessment of the dataset demonstrates high extraction precision and classification consistency with human experts (precision of 95.6% and inter-rater agreement k-$\alpha$ of 0.87). We further validate the practicality of our dataset by benchmarking 13 existing security tools on our dataset. The results reveal the significant limitations in current detection capabilities. Furthermore, by analyzing the severity-frequency distribution patterns through a unified CWE perspective in our dataset, we highlight inconsistency between current smart contract research focus and priorities identified from real-world vulnerabilities...</li>
</ul>

<h3>Title: A Multi-view Divergence-Convergence Feature Augmentation Framework for Drug-related Microbes Prediction</h3>
<ul>
<li><strong>Authors: </strong>Xin An, Ruijie Li, Qiao Ning, Shikai Guo, Hui Li, Qian Ma</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18797">https://arxiv.org/abs/2506.18797</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18797">https://arxiv.org/pdf/2506.18797</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18797]] A Multi-view Divergence-Convergence Feature Augmentation Framework for Drug-related Microbes Prediction(https://arxiv.org/abs/2506.18797)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In the study of drug function and precision medicine, identifying new drug-microbe associations is crucial. However, current methods isolate association and similarity analysis of drug and microbe, lacking effective inter-view optimization and coordinated multi-view feature fusion. In our study, a multi-view Divergence-Convergence Feature Augmentation framework for Drug-related Microbes Prediction (DCFA_DMP) is proposed, to better learn and integrate association information and similarity information. In the divergence phase, DCFA_DMP strengthens the complementarity and diversity between heterogeneous information and similarity information by performing Adversarial Learning method between the association network view and different similarity views, optimizing the feature space. In the convergence phase, a novel Bidirectional Synergistic Attention Mechanism is proposed to deeply synergize the complementary features between different views, achieving a deep fusion of the feature space. Moreover, Transformer graph learning is alternately applied on the drug-microbe heterogeneous graph, enabling each drug or microbe node to focus on the most relevant nodes. Numerous experiments demonstrate DCFA_DMP's significant performance in predicting drug-microbe associations. It also proves effectiveness in predicting associations for new drugs and microbes in cold start experiments, further confirming its stability and reliability in predicting potential drug-microbe associations.</li>
</ul>

<h3>Title: PicoSAM2: Low-Latency Segmentation In-Sensor for Edge Vision Applications</h3>
<ul>
<li><strong>Authors: </strong>Pietro Bonazzi, Nicola Farronato, Stefan Zihlmann, Haotong Qi, Michele Magno</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18807">https://arxiv.org/abs/2506.18807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18807">https://arxiv.org/pdf/2506.18807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18807]] PicoSAM2: Low-Latency Segmentation In-Sensor for Edge Vision Applications(https://arxiv.org/abs/2506.18807)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, segmentation</a></li>
<li><strong>Abstract: </strong>Real-time, on-device segmentation is critical for latency-sensitive and privacy-aware applications like smart glasses and IoT devices. We introduce PicoSAM2, a lightweight (1.3M parameters, 336M MACs) promptable segmentation model optimized for edge and in-sensor execution, including the Sony IMX500. It builds on a depthwise separable U-Net, with knowledge distillation and fixed-point prompt encoding to learn from the Segment Anything Model 2 (SAM2). On COCO and LVIS, it achieves 51.9% and 44.9% mIoU, respectively. The quantized model (1.22MB) runs at 14.3 ms on the IMX500-achieving 86 MACs/cycle, making it the only model meeting both memory and compute constraints for in-sensor deployment. Distillation boosts LVIS performance by +3.5% mIoU and +5.1% mAP. These results demonstrate that efficient, promptable segmentation is feasible directly on-camera, enabling privacy-preserving vision without cloud or host processing.</li>
</ul>

<h3>Title: Multi-Agent Online Control with Adversarial Disturbances</h3>
<ul>
<li><strong>Authors: </strong>Anas Barakat, John Lazarsfeld, Georgios Piliouras, Antonios Varvitsiotis</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.GT, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18814">https://arxiv.org/abs/2506.18814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18814">https://arxiv.org/pdf/2506.18814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18814]] Multi-Agent Online Control with Adversarial Disturbances(https://arxiv.org/abs/2506.18814)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multi-agent control problems involving a large number of agents with competing and time-varying objectives are increasingly prevalent in applications across robotics, economics, and energy systems. In this paper, we study online control in multi-agent linear dynamical systems with disturbances. In contrast to most prior work in multi-agent control, we consider an online setting where disturbances are adversarial and where each agent seeks to minimize its own, adversarial sequence of convex losses. In this setting, we investigate the robustness of gradient-based controllers from single-agent online control, with a particular focus on understanding how individual regret guarantees are influenced by the number of agents in the system. Under minimal communication assumptions, we prove near-optimal sublinear regret bounds that hold uniformly for all agents. Finally, when the objectives of the agents are aligned, we show that the multi-agent control problem induces a time-varying potential game for which we derive equilibrium gap guarantees.</li>
</ul>

<h3>Title: RWESummary: A Framework and Test for Choosing Large Language Models to Summarize Real-World Evidence (RWE) Studies</h3>
<ul>
<li><strong>Authors: </strong>Arjun Mukerji, Michael L. Jackson, Jason Jones, Neil Sanghavi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18819">https://arxiv.org/abs/2506.18819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18819">https://arxiv.org/pdf/2506.18819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18819]] RWESummary: A Framework and Test for Choosing Large Language Models to Summarize Real-World Evidence (RWE) Studies(https://arxiv.org/abs/2506.18819)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have been extensively evaluated for general summarization tasks as well as medical research assistance, but they have not been specifically evaluated for the task of summarizing real-world evidence (RWE) from structured output of RWE studies. We introduce RWESummary, a proposed addition to the MedHELM framework (Bedi, Cui, Fuentes, Unell et al., 2025) to enable benchmarking of LLMs for this task. RWESummary includes one scenario and three evaluations covering major types of errors observed in summarization of medical research studies and was developed using Atropos Health proprietary data. Additionally, we use RWESummary to compare the performance of different LLMs in our internal RWE summarization tool. At the time of publication, with 13 distinct RWE studies, we found the Gemini 2.5 models performed best overall (both Flash and Pro). We suggest RWESummary as a novel and useful foundation model benchmark for real-world evidence study summarization.</li>
</ul>

<h3>Title: MLLP-VRAIN UPV system for the IWSLT 2025 Simultaneous Speech Translation Translation task</h3>
<ul>
<li><strong>Authors: </strong>Jorge Iranzo-Sánchez, Javier Iranzo-Sánchez, Adrià Giménez, Jorge Civera, Alfons Juan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18828">https://arxiv.org/abs/2506.18828</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18828">https://arxiv.org/pdf/2506.18828</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18828]] MLLP-VRAIN UPV system for the IWSLT 2025 Simultaneous Speech Translation Translation task(https://arxiv.org/abs/2506.18828)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This work describes the participation of the MLLP-VRAIN research group in the shared task of the IWSLT 2025 Simultaneous Speech Translation track. Our submission addresses the unique challenges of real-time translation of long-form speech by developing a modular cascade system that adapts strong pre-trained models to streaming scenarios. We combine Whisper Large-V3-Turbo for ASR with the multilingual NLLB-3.3B model for MT, implementing lightweight adaptation techniques rather than training new end-to-end models from scratch. Our approach employs document-level adaptation with prefix training to enhance the MT model's ability to handle incomplete inputs, while incorporating adaptive emission policies including a wait-$k$ strategy and RALCP for managing the translation stream. Specialized buffer management techniques and segmentation strategies ensure coherent translations across long audio sequences. Experimental results on the ACL60/60 dataset demonstrate that our system achieves a favorable balance between translation quality and latency, with a BLEU score of 31.96 and non-computational-aware StreamLAAL latency of 2.94 seconds. Our final model achieves a preliminary score on the official test set (IWSLT25Instruct) of 29.8 BLEU. Our work demonstrates that carefully adapted pre-trained components can create effective simultaneous translation systems for long-form content without requiring extensive in-domain parallel data or specialized end-to-end training.</li>
</ul>

<h3>Title: STU-PID: Steering Token Usage via PID Controller for Efficient Large Language Model Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Aryasomayajula Ram Bharadwaj</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18831">https://arxiv.org/abs/2506.18831</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18831">https://arxiv.org/pdf/2506.18831</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18831]] STU-PID: Steering Token Usage via PID Controller for Efficient Large Language Model Reasoning(https://arxiv.org/abs/2506.18831)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models employing extended chain-of-thought (CoT) reasoning often suffer from the overthinking phenomenon, generating excessive and redundant reasoning steps that increase computational costs while potentially degrading performance. While recent work has explored static steering approaches to mitigate this issue, they lack the adaptability to dynamically adjust intervention strength based on real-time reasoning quality. We propose STUPID (Steering Token Usage via PID controller), a novel training-free method that employs a PID controller to dynamically modulate activation steering strength during inference. Our approach combines a chunk-level classifier for detecting redundant reasoning patterns with a PID control mechanism that adaptively adjusts steering intensity based on the predicted redundancy probability. Experimental evaluation on GSM8K demonstrates that STUPID achieves a 6% improvement in accuracy while reducing token usage by 32%, outperforming static steering baselines. Our method provides a principled framework for dynamic reasoning calibration that maintains reasoning quality while significantly improving computational efficiency.</li>
</ul>

<h3>Title: 4Real-Video-V2: Fused View-Time Attention and Feedforward Reconstruction for 4D Scene Generation</h3>
<ul>
<li><strong>Authors: </strong>Chaoyang Wang, Ashkan Mirzaei, Vidit Goel, Willi Menapace, Aliaksandr Siarohin, Avalon Vinella, Michael Vasilkovsky, Ivan Skorokhodov, Vladislav Shakhrai, Sergey Korolev, Sergey Tulyakov, Peter Wonka</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18839">https://arxiv.org/abs/2506.18839</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18839">https://arxiv.org/pdf/2506.18839</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18839]] 4Real-Video-V2: Fused View-Time Attention and Feedforward Reconstruction for 4D Scene Generation(https://arxiv.org/abs/2506.18839)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We propose the first framework capable of computing a 4D spatio-temporal grid of video frames and 3D Gaussian particles for each time step using a feed-forward architecture. Our architecture has two main components, a 4D video model and a 4D reconstruction model. In the first part, we analyze current 4D video diffusion architectures that perform spatial and temporal attention either sequentially or in parallel within a two-stream design. We highlight the limitations of existing approaches and introduce a novel fused architecture that performs spatial and temporal attention within a single layer. The key to our method is a sparse attention pattern, where tokens attend to others in the same frame, at the same timestamp, or from the same viewpoint. In the second part, we extend existing 3D reconstruction algorithms by introducing a Gaussian head, a camera token replacement algorithm, and additional dynamic layers and training. Overall, we establish a new state of the art for 4D generation, improving both visual quality and reconstruction capability.</li>
</ul>

<h3>Title: LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Yuhao Wu, Yushi Bai, Zhiqiang Hu, Roy Ka-Wei Lee, Juanzi Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18841">https://arxiv.org/abs/2506.18841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18841">https://arxiv.org/pdf/2506.18841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18841]] LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning(https://arxiv.org/abs/2506.18841)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Ultra-long generation by large language models (LLMs) is a widely demanded scenario, yet it remains a significant challenge due to their maximum generation length limit and overall quality degradation as sequence length increases. Previous approaches, exemplified by LongWriter, typically rely on ''teaching'', which involves supervised fine-tuning (SFT) on synthetic long-form outputs. However, this strategy heavily depends on synthetic SFT data, which is difficult and costly to construct, often lacks coherence and consistency, and tends to be overly artificial and structurally monotonous. In this work, we propose an incentivization-based approach that, starting entirely from scratch and without relying on any annotated or synthetic data, leverages reinforcement learning (RL) to foster the emergence of ultra-long, high-quality text generation capabilities in LLMs. We perform RL training starting from a base model, similar to R1-Zero, guiding it to engage in reasoning that facilitates planning and refinement during the writing process. To support this, we employ specialized reward models that steer the LLM towards improved length control, writing quality, and structural formatting. Experimental evaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B, consistently outperforms traditional SFT methods on long-form writing tasks, achieving state-of-the-art results across all metrics on WritingBench and Arena-Write, and even surpassing 100B+ models such as DeepSeek R1 and Qwen3-235B. We open-source our data and model checkpoints under this https URL</li>
</ul>

<h3>Title: Offline Goal-Conditioned Reinforcement Learning with Projective Quasimetric Planning</h3>
<ul>
<li><strong>Authors: </strong>Anthony Kobanda, Waris Radji, Mathieu Petitbois, Odalric-Ambrym Maillard, Rémy Portelas</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18847">https://arxiv.org/abs/2506.18847</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18847">https://arxiv.org/pdf/2506.18847</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18847]] Offline Goal-Conditioned Reinforcement Learning with Projective Quasimetric Planning(https://arxiv.org/abs/2506.18847)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Offline Goal-Conditioned Reinforcement Learning seeks to train agents to reach specified goals from previously collected trajectories. Scaling that promises to long-horizon tasks remains challenging, notably due to compounding value-estimation errors. Principled geometric offers a potential solution to address these issues. Following this insight, we introduce Projective Quasimetric Planning (ProQ), a compositional framework that learns an asymmetric distance and then repurposes it, firstly as a repulsive energy forcing a sparse set of keypoints to uniformly spread over the learned latent space, and secondly as a structured directional cost guiding towards proximal sub-goals. In particular, ProQ couples this geometry with a Lagrangian out-of-distribution detector to ensure the learned keypoints stay within reachable areas. By unifying metric learning, keypoint coverage, and goal-conditioned control, our approach produces meaningful sub-goals and robustly drives long-horizon goal-reaching on diverse a navigation benchmarks.</li>
</ul>

<h3>Title: Mechanistic Interpretability Needs Philosophy</h3>
<ul>
<li><strong>Authors: </strong>Iwan Williams, Ninell Oldenburg, Ruchira Dhar, Joshua Hatherley, Constanza Fierro, Nina Rajcic, Sandrine R. Schiller, Filippos Stamatiou, Anders Søgaard</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18852">https://arxiv.org/abs/2506.18852</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18852">https://arxiv.org/pdf/2506.18852</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18852]] Mechanistic Interpretability Needs Philosophy(https://arxiv.org/abs/2506.18852)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Mechanistic interpretability (MI) aims to explain how neural networks work by uncovering their underlying causal mechanisms. As the field grows in influence, it is increasingly important to examine not just models themselves, but the assumptions, concepts and explanatory strategies implicit in MI research. We argue that mechanistic interpretability needs philosophy: not as an afterthought, but as an ongoing partner in clarifying its concepts, refining its methods, and assessing the epistemic and ethical stakes of interpreting AI systems. Taking three open problems from the MI literature as examples, this position paper illustrates the value philosophy can add to MI research, and outlines a path toward deeper interdisciplinary dialogue.</li>
</ul>

<h3>Title: RAG-6DPose: Retrieval-Augmented 6D Pose Estimation via Leveraging CAD as Knowledge Base</h3>
<ul>
<li><strong>Authors: </strong>Kuanning Wang, Yuqian Fu, Tianyu Wang, Yanwei Fu, Longfei Liang, Yu-Gang Jiang, Xiangyang Xue</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18856">https://arxiv.org/abs/2506.18856</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18856">https://arxiv.org/pdf/2506.18856</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18856]] RAG-6DPose: Retrieval-Augmented 6D Pose Estimation via Leveraging CAD as Knowledge Base(https://arxiv.org/abs/2506.18856)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate 6D pose estimation is key for robotic manipulation, enabling precise object localization for tasks like grasping. We present RAG-6DPose, a retrieval-augmented approach that leverages 3D CAD models as a knowledge base by integrating both visual and geometric cues. Our RAG-6DPose roughly contains three stages: 1) Building a Multi-Modal CAD Knowledge Base by extracting 2D visual features from multi-view CAD rendered images and also attaching 3D points; 2) Retrieving relevant CAD features from the knowledge base based on the current query image via our ReSPC module; and 3) Incorporating retrieved CAD information to refine pose predictions via retrieval-augmented decoding. Experimental results on standard benchmarks and real-world robotic tasks demonstrate the effectiveness and robustness of our approach, particularly in handling occlusions and novel viewpoints. Supplementary material is available on our project website: this https URL .</li>
</ul>

<h3>Title: TAMMs: Temporal-Aware Multimodal Model for Satellite Image Change Understanding and Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Zhongbin Guo, Yuhao Wang, Ping Jian, Xinyue Chen, Wei Peng, Ertai E</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18862">https://arxiv.org/abs/2506.18862</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18862">https://arxiv.org/pdf/2506.18862</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18862]] TAMMs: Temporal-Aware Multimodal Model for Satellite Image Change Understanding and Forecasting(https://arxiv.org/abs/2506.18862)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Satellite image time-series analysis demands fine-grained spatial-temporal reasoning, which remains a challenge for existing multimodal large language models (MLLMs). In this work, we study the capabilities of MLLMs on a novel task that jointly targets temporal change understanding and future scene generation, aiming to assess their potential for modeling complex multimodal dynamics over time. We propose TAMMs, a Temporal-Aware Multimodal Model for satellite image change understanding and forecasting, which enhances frozen MLLMs with lightweight temporal modules for structured sequence encoding and contextual prompting. To guide future image generation, TAMMs introduces a Semantic-Fused Control Injection (SFCI) mechanism that adaptively combines high-level semantic reasoning and structural priors within an enhanced ControlNet. This dual-path conditioning enables temporally consistent and semantically grounded image synthesis. Experiments demonstrate that TAMMs outperforms strong MLLM baselines in both temporal change understanding and future image forecasting tasks, highlighting how carefully designed temporal reasoning and semantic fusion can unlock the full potential of MLLMs for spatio-temporal understanding.</li>
</ul>

<h3>Title: Amplifying Machine Learning Attacks Through Strategic Compositions</h3>
<ul>
<li><strong>Authors: </strong>Yugeng Liu, Zheng Li, Hai Huang, Michael Backes, Yang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18870">https://arxiv.org/abs/2506.18870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18870">https://arxiv.org/pdf/2506.18870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18870]] Amplifying Machine Learning Attacks Through Strategic Compositions(https://arxiv.org/abs/2506.18870)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, membership infer</a></li>
<li><strong>Abstract: </strong>Machine learning (ML) models are proving to be vulnerable to a variety of attacks that allow the adversary to learn sensitive information, cause mispredictions, and more. While these attacks have been extensively studied, current research predominantly focuses on analyzing each attack type individually. In practice, however, adversaries may employ multiple attack strategies simultaneously rather than relying on a single approach. This prompts a crucial yet underexplored question: When the adversary has multiple attacks at their disposal, are they able to mount or amplify the effect of one attack with another? In this paper, we take the first step in studying the strategic interactions among different attacks, which we define as attack compositions. Specifically, we focus on four well-studied attacks during the model's inference phase: adversarial examples, attribute inference, membership inference, and property inference. To facilitate the study of their interactions, we propose a taxonomy based on three stages of the attack pipeline: preparation, execution, and evaluation. Using this taxonomy, we identify four effective attack compositions, such as property inference assisting attribute inference at its preparation level and adversarial examples assisting property inference at its execution level. We conduct extensive experiments on the attack compositions using three ML model architectures and three benchmark image datasets. Empirical results demonstrate the effectiveness of these four attack compositions. We implement and release a modular reusable toolkit, COAT. Arguably, our work serves as a call for researchers and practitioners to consider advanced adversarial settings involving multiple attack strategies, aiming to strengthen the security and robustness of AI systems.</li>
</ul>

<h3>Title: OmniGen2: Exploration to Advanced Multimodal Generation</h3>
<ul>
<li><strong>Authors: </strong>Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, Ze Liu, Ziyi Xia, Chaofan Li, Haoge Deng, Jiahao Wang, Kun Luo, Bo Zhang, Defu Lian, Xinlong Wang, Zhongyuan Wang, Tiejun Huang, Zheng Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18871">https://arxiv.org/abs/2506.18871</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18871">https://arxiv.org/pdf/2506.18871</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18871]] OmniGen2: Exploration to Advanced Multimodal Generation(https://arxiv.org/abs/2506.18871)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In this work, we introduce OmniGen2, a versatile and open-source generative model designed to provide a unified solution for diverse generation tasks, including text-to-image, image editing, and in-context generation. Unlike OmniGen v1, OmniGen2 features two distinct decoding pathways for text and image modalities, utilizing unshared parameters and a decoupled image tokenizer. This design enables OmniGen2 to build upon existing multimodal understanding models without the need to re-adapt VAE inputs, thereby preserving the original text generation capabilities. To facilitate the training of OmniGen2, we developed comprehensive data construction pipelines, encompassing image editing and in-context generation data. Additionally, we introduce a reflection mechanism tailored for image generation tasks and curate a dedicated reflection dataset based on OmniGen2. Despite its relatively modest parameter size, OmniGen2 achieves competitive results on multiple task benchmarks, including text-to-image and image editing. To further evaluate in-context generation, also referred to as subject-driven tasks, we introduce a new benchmark named OmniContext. OmniGen2 achieves state-of-the-art performance among open-source models in terms of consistency. We will release our models, training code, datasets, and data construction pipeline to support future research in this field. Project Page: this https URL GitHub Link: this https URL</li>
</ul>

<h3>Title: CommVQ: Commutative Vector Quantization for KV Cache Compression</h3>
<ul>
<li><strong>Authors: </strong>Junyan Li, Yang Zhang, Muhammad Yusuf Hassan, Talha Chafekar, Tianle Cai, Zhile Ren, Pengsheng Guo, Foroozan Karimzadeh, Colorado Reed, Chong Wang, Chuang Gan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18879">https://arxiv.org/abs/2506.18879</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18879">https://arxiv.org/pdf/2506.18879</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18879]] CommVQ: Commutative Vector Quantization for KV Cache Compression(https://arxiv.org/abs/2506.18879)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly used in applications requiring long context lengths, but the key-value (KV) cache often becomes a memory bottleneck on GPUs as context grows. To address this, we propose Commutative Vector Quantization (CommVQ) to significantly reduce memory usage for long-context LLM inference. We first introduce additive quantization with a lightweight encoder and codebook to compress the KV cache, which can be decoded via simple matrix multiplication. To further reduce computational costs during decoding, we design the codebook to be commutative with Rotary Position Embedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm. This enables efficient integration of decoding into the self-attention mechanism. Our approach achieves high accuracy with additive quantization and low overhead via the RoPE-commutative codebook. Experiments on long-context benchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5% with 2-bit quantization, while outperforming state-of-the-art KV cache quantization methods. Notably, it enables 1-bit KV cache quantization with minimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context length on a single RTX 4090 GPU. The source code is available at: this https URL.</li>
</ul>

<h3>Title: Let Your Video Listen to Your Music!</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Zhang, Dong Gong, Zicheng Duan, Anton van den Hengel, Lingqiao Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18881">https://arxiv.org/abs/2506.18881</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18881">https://arxiv.org/pdf/2506.18881</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18881]] Let Your Video Listen to Your Music!(https://arxiv.org/abs/2506.18881)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Aligning the rhythm of visual motion in a video with a given music track is a practical need in multimedia production, yet remains an underexplored task in autonomous video editing. Effective alignment between motion and musical beats enhances viewer engagement and visual appeal, particularly in music videos, promotional content, and cinematic editing. Existing methods typically depend on labor-intensive manual cutting, speed adjustments, or heuristic-based editing techniques to achieve synchronization. While some generative models handle joint video and music generation, they often entangle the two modalities, limiting flexibility in aligning video to music beats while preserving the full visual content. In this paper, we propose a novel and efficient framework, termed MVAA (Music-Video Auto-Alignment), that automatically edits video to align with the rhythm of a given music track while preserving the original visual content. To enhance flexibility, we modularize the task into a two-step process in our MVAA: aligning motion keyframes with audio beats, followed by rhythm-aware video inpainting. Specifically, we first insert keyframes at timestamps aligned with musical beats, then use a frame-conditioned diffusion model to generate coherent intermediate frames, preserving the original video's semantic content. Since comprehensive test-time training can be time-consuming, we adopt a two-stage strategy: pretraining the inpainting module on a small video set to learn general motion priors, followed by rapid inference-time fine-tuning for video-specific adaptation. This hybrid approach enables adaptation within 10 minutes with one epoch on a single NVIDIA 4090 GPU using CogVideoX-5b-I2V as the backbone. Extensive experiments show that our approach can achieve high-quality beat alignment and visual smoothness.</li>
</ul>

<h3>Title: Universal Video Temporal Grounding with Generative Multi-modal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zeqian Li, Shangzhe Di, Zhonghua Zhai, Weilin Huang, Yanfeng Wang, Weidi Xie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18883">https://arxiv.org/abs/2506.18883</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18883">https://arxiv.org/pdf/2506.18883</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18883]] Universal Video Temporal Grounding with Generative Multi-modal Large Language Models(https://arxiv.org/abs/2506.18883)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>This paper presents a computational model for universal video temporal grounding, which accurately localizes temporal moments in videos based on natural language queries (e.g., questions or descriptions). Unlike existing methods that are often limited to specific video domains or durations, we propose UniTime, a robust and universal video grounding model leveraging the strong vision-language understanding capabilities of generative Multi-modal Large Language Models (MLLMs). Our model effectively handles videos of diverse views, genres, and lengths while comprehending complex language queries. The key contributions include: (i) We consider steering strong MLLMs for temporal grounding in videos. To enable precise timestamp outputs, we incorporate temporal information by interleaving timestamp tokens with video tokens. (ii) By training the model to handle videos with different input granularities through adaptive frame scaling, our approach achieves robust temporal grounding for both short and long videos. (iii) Comprehensive experiments show that UniTime outperforms state-of-the-art approaches in both zero-shot and dataset-specific finetuned settings across five public temporal grounding benchmarks. (iv) When employed as a preliminary moment retriever for long-form video question-answering (VideoQA), UniTime significantly improves VideoQA accuracy, highlighting its value for complex video understanding tasks.</li>
</ul>

<h3>Title: 4D-LRM: Large Space-Time Reconstruction Model From and To Any View at Any Time</h3>
<ul>
<li><strong>Authors: </strong>Ziqiao Ma, Xuweiyi Chen, Shoubin Yu, Sai Bi, Kai Zhang, Chen Ziwen, Sihan Xu, Jianing Yang, Zexiang Xu, Kalyan Sunkavalli, Mohit Bansal, Joyce Chai, Hao Tan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18890">https://arxiv.org/abs/2506.18890</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18890">https://arxiv.org/pdf/2506.18890</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18890]] 4D-LRM: Large Space-Time Reconstruction Model From and To Any View at Any Time(https://arxiv.org/abs/2506.18890)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Can we scale 4D pretraining to learn general space-time representations that reconstruct an object from a few views at some times to any view at any time? We provide an affirmative answer with 4D-LRM, the first large-scale 4D reconstruction model that takes input from unconstrained views and timestamps and renders arbitrary novel view-time combinations. Unlike prior 4D approaches, e.g., optimization-based, geometry-based, or generative, that struggle with efficiency, generalization, or faithfulness, 4D-LRM learns a unified space-time representation and directly predicts per-pixel 4D Gaussian primitives from posed image tokens across time, enabling fast, high-quality rendering at, in principle, infinite frame rate. Our results demonstrate that scaling spatiotemporal pretraining enables accurate and efficient 4D reconstruction. We show that 4D-LRM generalizes to novel objects, interpolates across time, and handles diverse camera setups. It reconstructs 24-frame sequences in one forward pass with less than 1.5 seconds on a single A100 GPU.</li>
</ul>

<h3>Title: ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Jiaru Zou, Ling Yang, Jingwen Gu, Jiahao Qiu, Ke Shen, Jingrui He, Mengdi Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18896">https://arxiv.org/abs/2506.18896</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18896">https://arxiv.org/pdf/2506.18896</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18896]] ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs(https://arxiv.org/abs/2506.18896)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Process Reward Models (PRMs) have recently emerged as a powerful framework for supervising intermediate reasoning steps in large language models (LLMs). Previous PRMs are primarily trained on model final output responses and struggle to evaluate intermediate thinking trajectories robustly, especially in the emerging setting of trajectory-response outputs generated by frontier reasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a novel trajectory-aware PRM explicitly designed to evaluate the trajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both step-level and trajectory-level supervision, enabling fine-grained reward assignment aligned with structured chain-of-thought data. We adapt ReasonFlux-PRM to support reward supervision under both offline and online settings, including (i) selecting high-quality model distillation data for downstream supervised fine-tuning of smaller models, (ii) providing dense process-level rewards for policy optimization during reinforcement learning, and (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results on challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond demonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs (e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our derived ReasonFlux-PRM-7B yields consistent performance improvements, achieving average gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement learning, and 6.3% in test-time scaling. We also release our efficient ReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment. Projects: this https URL</li>
</ul>

<h3>Title: Vision as a Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations</h3>
<ul>
<li><strong>Authors: </strong>Jiaming Han, Hao Chen, Yang Zhao, Hanyu Wang, Qi Zhao, Ziyan Yang, Hao He, Xiangyu Yue, Lu Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18898">https://arxiv.org/abs/2506.18898</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18898">https://arxiv.org/pdf/2506.18898</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18898]] Vision as a Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations(https://arxiv.org/abs/2506.18898)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, large language model</a></li>
<li><strong>Abstract: </strong>This paper presents a multimodal framework that attempts to unify visual understanding and generation within a shared discrete semantic representation. At its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into discrete tokens using a text-aligned codebook projected from a large language model's (LLM) vocabulary. By integrating vision and text into a unified space with an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input and output through a shared interface, without the need for modality-specific designs. Additionally, we propose scale-adaptive encoding and decoding to balance efficiency and visual detail, along with a generative de-tokenizer to produce high-fidelity visual outputs. To address diverse decoding needs, we utilize two complementary de-tokenizers: a fast autoregressive model and a diffusion-based model. To enhance modality fusion, we investigate advanced pre-training tasks, demonstrating improvements in both visual understanding and generation. Experiments across benchmarks show that Tar matches or surpasses existing multimodal LLM methods, achieving faster convergence and greater training efficiency. Code, models, and data are available at this https URL</li>
</ul>

<h3>Title: FilMaster: Bridging Cinematic Principles and Generative AI for Automated Film Generation</h3>
<ul>
<li><strong>Authors: </strong>Kaiyi Huang, Yukun Huang, Xintao Wang, Zinan Lin, Xuefei Ning, Pengfei Wan, Di Zhang, Yu Wang, Xihui Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18899">https://arxiv.org/abs/2506.18899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18899">https://arxiv.org/pdf/2506.18899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18899]] FilMaster: Bridging Cinematic Principles and Generative AI for Automated Film Generation(https://arxiv.org/abs/2506.18899)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>AI-driven content creation has shown potential in film production. However, existing film generation systems struggle to implement cinematic principles and thus fail to generate professional-quality films, particularly lacking diverse camera language and cinematic rhythm. This results in templated visuals and unengaging narratives. To address this, we introduce FilMaster, an end-to-end AI system that integrates real-world cinematic principles for professional-grade film generation, yielding editable, industry-standard outputs. FilMaster is built on two key principles: (1) learning cinematography from extensive real-world film data and (2) emulating professional, audience-centric post-production workflows. Inspired by these principles, FilMaster incorporates two stages: a Reference-Guided Generation Stage which transforms user input to video clips, and a Generative Post-Production Stage which transforms raw footage into audiovisual outputs by orchestrating visual and auditory elements for cinematic rhythm. Our generation stage highlights a Multi-shot Synergized RAG Camera Language Design module to guide the AI in generating professional camera language by retrieving reference clips from a vast corpus of 440,000 film clips. Our post-production stage emulates professional workflows by designing an Audience-Centric Cinematic Rhythm Control module, including Rough Cut and Fine Cut processes informed by simulated audience feedback, for effective integration of audiovisual elements to achieve engaging content. The system is empowered by generative AI models like (M)LLMs and video generation models. Furthermore, we introduce FilmEval, a comprehensive benchmark for evaluating AI-generated films. Extensive experiments show FilMaster's superior performance in camera language design and cinematic rhythm control, advancing generative AI in professional filmmaking.</li>
</ul>

<h3>Title: Audit & Repair: An Agentic Framework for Consistent Story Visualization in Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Kiymet Akdemir, Tahira Kazimi, Pinar Yanardag</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.18900">https://arxiv.org/abs/2506.18900</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.18900">https://arxiv.org/pdf/2506.18900</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.18900]] Audit & Repair: An Agentic Framework for Consistent Story Visualization in Text-to-Image Diffusion Models(https://arxiv.org/abs/2506.18900)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Story visualization has become a popular task where visual scenes are generated to depict a narrative across multiple panels. A central challenge in this setting is maintaining visual consistency, particularly in how characters and objects persist and evolve throughout the story. Despite recent advances in diffusion models, current approaches often fail to preserve key character attributes, leading to incoherent narratives. In this work, we propose a collaborative multi-agent framework that autonomously identifies, corrects, and refines inconsistencies across multi-panel story visualizations. The agents operate in an iterative loop, enabling fine-grained, panel-level updates without re-generating entire sequences. Our framework is model-agnostic and flexibly integrates with a variety of diffusion models, including rectified flow transformers such as Flux and latent diffusion models such as Stable Diffusion. Quantitative and qualitative experiments show that our method outperforms prior approaches in terms of multi-panel consistency.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
