<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: FedSDG-FS: Efficient and Secure Feature Selection for Vertical Federated Learning. (arXiv:2302.10417v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10417">http://arxiv.org/abs/2302.10417</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10417] FedSDG-FS: Efficient and Secure Feature Selection for Vertical Federated Learning](http://arxiv.org/abs/2302.10417) #secure</code></li>
<li>Summary: <p>Vertical Federated Learning (VFL) enables multiple data owners, each holding
a different subset of features about largely overlapping sets of data
sample(s), to jointly train a useful global model. Feature selection (FS) is
important to VFL. It is still an open research problem as existing FS works
designed for VFL either assumes prior knowledge on the number of noisy features
or prior knowledge on the post-training threshold of useful features to be
selected, making them unsuitable for practical applications. To bridge this
gap, we propose the Federated Stochastic Dual-Gate based Feature Selection
(FedSDG-FS) approach. It consists of a Gaussian stochastic dual-gate to
efficiently approximate the probability of a feature being selected, with
privacy protection through Partially Homomorphic Encryption without a trusted
third-party. To reduce overhead, we propose a feature importance initialization
method based on Gini impurity, which can accomplish its goals with only two
parameter transmissions between the server and the clients. Extensive
experiments on both synthetic and real-world datasets show that FedSDG-FS
significantly outperforms existing approaches in terms of achieving accurate
selection of high-quality features as well as building global models with
improved performance.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Crop mapping in the small sample/no sample case: an approach using a two-level cascade classifier and integrating domain knowledge. (arXiv:2302.10270v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10270">http://arxiv.org/abs/2302.10270</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10270] Crop mapping in the small sample/no sample case: an approach using a two-level cascade classifier and integrating domain knowledge](http://arxiv.org/abs/2302.10270) #security</code></li>
<li>Summary: <p>Mapping crops using remote sensing technology is important for food security
and land management. Machine learning-based methods has become a popular
approach for crop mapping in recent years. However, the key to machine
learning, acquiring ample and accurate samples, is usually time-consuming and
laborious. To solve this problem, a crop mapping method in the small sample/no
sample case that integrating domain knowledge and using a cascaded
classification framework that combine a weak classifier learned from samples
with strong features and a strong classifier trained by samples with weak
feature was proposed. First, based on the domain knowledge of various crops, a
low-capacity classifier such as decision tree was applied to acquire those
pixels with distinctive features and complete observation sequences as "strong
feature" samples. Then, to improve the representativeness of these samples,
sample augmentation strategy that artificially remove the observations of
"strong feature" samples according to the average valid observation proportion
in target area was applied. Finally, based on the original samples and
augmented samples, a large-capacity classifier such as random forest was
trained for crop mapping. The method achieved an overall accuracy of 82% in the
MAP crop recognition competition held by Syngenta Group, China in 2021 (third
prize, ranked fourth). This method integrates domain knowledge to overcome the
difficulties of sample acquisition, providing a convenient, fast and accurate
solution for crop mapping.
</p></li>
</ul>

<h3>Title: Exploring the Limits of Transfer Learning with Unified Model in the Cybersecurity Domain. (arXiv:2302.10346v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10346">http://arxiv.org/abs/2302.10346</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10346] Exploring the Limits of Transfer Learning with Unified Model in the Cybersecurity Domain](http://arxiv.org/abs/2302.10346) #security</code></li>
<li>Summary: <p>With the increase in cybersecurity vulnerabilities of software systems, the
ways to exploit them are also increasing. Besides these, malware threats,
irregular network interactions, and discussions about exploits in public forums
are also on the rise. To identify these threats faster, to detect potentially
relevant entities from any texts, and to be aware of software vulnerabilities,
automated approaches are necessary. Application of natural language processing
(NLP) techniques in the Cybersecurity domain can help in achieving this.
However, there are challenges such as the diverse nature of texts involved in
the cybersecurity domain, the unavailability of large-scale publicly available
datasets, and the significant cost of hiring subject matter experts for
annotations. One of the solutions is building multi-task models that can be
trained jointly with limited data. In this work, we introduce a generative
multi-task model, Unified Text-to-Text Cybersecurity (UTS), trained on malware
reports, phishing site URLs, programming code constructs, social media data,
blogs, news articles, and public forum posts. We show UTS improves the
performance of some cybersecurity datasets. We also show that with a few
examples, UTS can be adapted to novel unseen tasks and the nature of data
</p></li>
</ul>

<h3>Title: Few-shot Detection of Anomalies in Industrial Cyber-Physical System via Prototypical Network and Contrastive Learning. (arXiv:2302.10601v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10601">http://arxiv.org/abs/2302.10601</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10601] Few-shot Detection of Anomalies in Industrial Cyber-Physical System via Prototypical Network and Contrastive Learning](http://arxiv.org/abs/2302.10601) #security</code></li>
<li>Summary: <p>The rapid development of Industry 4.0 has amplified the scope and
destructiveness of industrial Cyber-Physical System (CPS) by network attacks.
Anomaly detection techniques are employed to identify these attacks and
guarantee the normal operation of industrial CPS. However, it is still a
challenging problem to cope with scenarios with few labeled samples. In this
paper, we propose a few-shot anomaly detection model (FSL-PN) based on
prototypical network and contrastive learning for identifying anomalies with
limited labeled data from industrial CPS. Specifically, we design a contrastive
loss to assist the training process of the feature extractor and learn more
fine-grained features to improve the discriminative performance. Subsequently,
to tackle the overfitting issue during classifying, we construct a robust cost
function with a specific regularizer to enhance the generalization capability.
Experimental results based on two public imbalanced datasets with few-shot
settings show that the FSL-PN model can significantly improve F1 score and
reduce false alarm rate (FAR) for identifying anomalous signals to guarantee
the security of industrial CPS.
</p></li>
</ul>

<h3>Title: A Survey of Trustworthy Federated Learning with Perspectives on Security, Robustness, and Privacy. (arXiv:2302.10637v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10637">http://arxiv.org/abs/2302.10637</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10637] A Survey of Trustworthy Federated Learning with Perspectives on Security, Robustness, and Privacy](http://arxiv.org/abs/2302.10637) #security</code></li>
<li>Summary: <p>Trustworthy artificial intelligence (AI) technology has revolutionized daily
life and greatly benefited human society. Among various AI technologies,
Federated Learning (FL) stands out as a promising solution for diverse
real-world scenarios, ranging from risk evaluation systems in finance to
cutting-edge technologies like drug discovery in life sciences. However,
challenges around data isolation and privacy threaten the trustworthiness of FL
systems. Adversarial attacks against data privacy, learning algorithm
stability, and system confidentiality are particularly concerning in the
context of distributed training in federated learning. Therefore, it is crucial
to develop FL in a trustworthy manner, with a focus on security, robustness,
and privacy. In this survey, we propose a comprehensive roadmap for developing
trustworthy FL systems and summarize existing efforts from three key aspects:
security, robustness, and privacy. We outline the threats that pose
vulnerabilities to trustworthy federated learning across different stages of
development, including data processing, model training, and deployment. To
guide the selection of the most appropriate defense methods, we discuss
specific technical solutions for realizing each aspect of Trustworthy FL (TFL).
Our approach differs from previous work that primarily discusses TFL from a
legal perspective or presents FL from a high-level, non-technical viewpoint.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Speech Privacy Leakage from Shared Gradients in Distributed Learning. (arXiv:2302.10441v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10441">http://arxiv.org/abs/2302.10441</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10441] Speech Privacy Leakage from Shared Gradients in Distributed Learning](http://arxiv.org/abs/2302.10441) #privacy</code></li>
<li>Summary: <p>Distributed machine learning paradigms, such as federated learning, have been
recently adopted in many privacy-critical applications for speech analysis.
However, such frameworks are vulnerable to privacy leakage attacks from shared
gradients. Despite extensive efforts in the image domain, the exploration of
speech privacy leakage from gradients is quite limited. In this paper, we
explore methods for recovering private speech/speaker information from the
shared gradients in distributed learning settings. We conduct experiments on a
keyword spotting model with two different types of speech features to quantify
the amount of leaked information by measuring the similarity between the
original and recovered speech signals. We further demonstrate the feasibility
of inferring various levels of side-channel information, including speech
content and speaker identity, under the distributed learning framework without
accessing the user's data.
</p></li>
</ul>

<h3>Title: FedSpeed: Larger Local Interval, Less Communication Round, and Higher Generalization Accuracy. (arXiv:2302.10429v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10429">http://arxiv.org/abs/2302.10429</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10429] FedSpeed: Larger Local Interval, Less Communication Round, and Higher Generalization Accuracy](http://arxiv.org/abs/2302.10429) #privacy</code></li>
<li>Summary: <p>Federated learning is an emerging distributed machine learning framework
which jointly trains a global model via a large number of local devices with
data privacy protections. Its performance suffers from the non-vanishing biases
introduced by the local inconsistent optimal and the rugged client-drifts by
the local over-fitting. In this paper, we propose a novel and practical method,
FedSpeed, to alleviate the negative impacts posed by these problems.
Concretely, FedSpeed applies the prox-correction term on the current local
updates to efficiently reduce the biases introduced by the prox-term, a
necessary regularizer to maintain the strong local consistency. Furthermore,
FedSpeed merges the vanilla stochastic gradient with a perturbation computed
from an extra gradient ascent step in the neighborhood, thereby alleviating the
issue of local over-fitting. Our theoretical analysis indicates that the
convergence rate is related to both the communication rounds $T$ and local
intervals $K$ with a upper bound $\small \mathcal{O}(1/T)$ if setting a proper
local interval. Moreover, we conduct extensive experiments on the real-world
dataset to demonstrate the efficiency of our proposed FedSpeed, which performs
significantly faster and achieves the state-of-the-art (SOTA) performance on
the general FL experimental settings than several baselines including FedAvg,
FedProx, FedCM, FedAdam, SCAFFOLD, FedDyn, FedADMM, etc.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Reliability Analysis of Vision Transformers. (arXiv:2302.10468v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10468">http://arxiv.org/abs/2302.10468</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10468] Reliability Analysis of Vision Transformers](http://arxiv.org/abs/2302.10468) #protect</code></li>
<li>Summary: <p>Vision Transformers (ViTs) that leverage self-attention mechanism have shown
superior performance on many classical vision tasks compared to convolutional
neural networks (CNNs) and gain increasing popularity recently. Existing ViTs
works mainly optimize performance and accuracy, but ViTs reliability issues
induced by hardware faults in large-scale VLSI designs have generally been
overlooked. In this work, we mainly study the reliability of ViTs and
investigate the vulnerability from different architecture granularities ranging
from models, layers, modules, and patches for the first time. The investigation
reveals that ViTs with the self-attention mechanism are generally more
resilient on linear computing including general matrix-matrix multiplication
(GEMM) and full connection (FC), and show a relatively even vulnerability
distribution across the patches. However, ViTs involve more fragile non-linear
computing such as softmax and GELU compared to typical CNNs. With the above
observations, we propose an adaptive algorithm-based fault tolerance algorithm
(ABFT) to protect the linear computing implemented with distinct sizes of GEMM
and apply a range-based protection scheme to mitigate soft errors in non-linear
computing. According to our experiments, the proposed fault-tolerant approaches
enhance ViT accuracy significantly with minor computing overhead in presence of
various soft errors.
</p></li>
</ul>

<h3>Title: ApproxABFT: Approximate Algorithm-Based Fault Tolerance for Vision Transformers. (arXiv:2302.10469v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10469">http://arxiv.org/abs/2302.10469</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10469] ApproxABFT: Approximate Algorithm-Based Fault Tolerance for Vision Transformers](http://arxiv.org/abs/2302.10469) #protect</code></li>
<li>Summary: <p>Vision Transformers (ViTs) with outstanding performance becomes a popular
backbone of deep learning models for the main-stream vision tasks including
classification, object detection, and segmentation. Other than the performance,
reliability is also a critical metric for the adoption of ViTs in
safety-critical applications such as autonomous driving and robotics. With the
observation that the major computing blocks in ViTs such as multi-head
attention and feed forward are usually performed with general matrix
multiplication (GEMM), we propose to adopt a classical algorithm-based fault
tolerance (ABFT) strategy originally developed for GEMM to protect ViTs against
soft errors in the underlying computing engines. Unlike classical ABFT that
will invoke the expensive error recovery procedure whenever computing errors
are detected, we leverage the inherent fault-tolerance of ViTs and propose an
approximate ABFT, namely ApproxABFT, to invoke the error recovery procedure
only when the computing errors are significant enough, which skips many useless
error recovery procedures and simplifies the overall GEMM error recovery.
According to our experiments, ApproxABFT reduces the computing overhead by
25.92% to 81.62% and improves the model accuracy by 2.63% to 72.56% compared to
the baseline ABFT.
</p></li>
</ul>

<h3>Title: MalProtect: Stateful Defense Against Adversarial Query Attacks in ML-based Malware Detection. (arXiv:2302.10739v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10739">http://arxiv.org/abs/2302.10739</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10739] MalProtect: Stateful Defense Against Adversarial Query Attacks in ML-based Malware Detection](http://arxiv.org/abs/2302.10739) #protect</code></li>
<li>Summary: <p>ML models are known to be vulnerable to adversarial query attacks. In these
attacks, queries are iteratively perturbed towards a particular class without
any knowledge of the target model besides its output. The prevalence of
remotely-hosted ML classification models and Machine-Learning-as-a-Service
platforms means that query attacks pose a real threat to the security of these
systems. To deal with this, stateful defenses have been proposed to detect
query attacks and prevent the generation of adversarial examples by monitoring
and analyzing the sequence of queries received by the system. Several stateful
defenses have been proposed in recent years. However, these defenses rely
solely on similarity or out-of-distribution detection methods that may be
effective in other domains. In the malware detection domain, the methods to
generate adversarial examples are inherently different, and therefore we find
that such detection mechanisms are significantly less effective. Hence, in this
paper, we present MalProtect, which is a stateful defense against query attacks
in the malware detection domain. MalProtect uses several threat indicators to
detect attacks. Our results show that it reduces the evasion rate of
adversarial query attacks by 80+\% in Android and Windows malware, across a
range of attacker scenarios. In the first evaluation of its kind, we show that
MalProtect outperforms prior stateful defenses, especially under the peak
adversarial threat.
</p></li>
</ul>

<h3>Title: Scalable Infomin Learning. (arXiv:2302.10701v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10701">http://arxiv.org/abs/2302.10701</a></li>
<li>Code URL: <a href="https://github.com/cyz-ai/infomin">https://github.com/cyz-ai/infomin</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10701] Scalable Infomin Learning](http://arxiv.org/abs/2302.10701) #protect</code></li>
<li>Summary: <p>The task of infomin learning aims to learn a representation with high utility
while being uninformative about a specified target, with the latter achieved by
minimising the mutual information between the representation and the target. It
has broad applications, ranging from training fair prediction models against
protected attributes, to unsupervised learning with disentangled
representations. Recent works on infomin learning mainly use adversarial
training, which involves training a neural network to estimate mutual
information or its proxy and thus is slow and difficult to optimise. Drawing on
recent advances in slicing techniques, we propose a new infomin learning
approach, which uses a novel proxy metric to mutual information. We further
derive an accurate and analytically computable approximation to this proxy
metric, thereby removing the need of constructing neural network-based mutual
information estimators. Experiments on algorithmic fairness, disentangled
representation learning and domain adaptation verify that our method can
effectively remove unwanted information with limited time budget.
</p></li>
</ul>

<h3>Title: Provable Copyright Protection for Generative Models. (arXiv:2302.10870v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10870">http://arxiv.org/abs/2302.10870</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10870] Provable Copyright Protection for Generative Models](http://arxiv.org/abs/2302.10870) #protect</code></li>
<li>Summary: <p>There is a growing concern that learned conditional generative models may
output samples that are substantially similar to some copyrighted data $C$ that
was in their training set. We give a formal definition of $\textit{near
access-freeness (NAF)}$ and prove bounds on the probability that a model
satisfying this definition outputs a sample similar to $C$, even if $C$ is
included in its training set. Roughly speaking, a generative model $p$ is
$\textit{$k$-NAF}$ if for every potentially copyrighted data $C$, the output of
$p$ diverges by at most $k$-bits from the output of a model $q$ that
$\textit{did not access $C$ at all}$. We also give generative model learning
algorithms, which efficiently modify the original generative model learning
algorithm in a black box manner, that output generative models with strong
bounds on the probability of sampling protected content. Furthermore, we
provide promising experiments for both language (transformers) and image
(diffusion) generative models, showing minimal degradation in output quality
while ensuring strong protections against sampling protected content.
</p></li>
</ul>

<h2>defense</h2>
<h3>Title: Potential Penetrative Pass (P3). (arXiv:2302.10760v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10760">http://arxiv.org/abs/2302.10760</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10760] Potential Penetrative Pass (P3)](http://arxiv.org/abs/2302.10760) #defense</code></li>
<li>Summary: <p>To score goals in football, a team needs to move forward on the pitch and
there are various ways to do so. Depending on the game plan &amp; philosophy; some
teams prefer to play long balls from either wings or defense. Others, prefer to
penetrate in depth with passes and outplay the opponent players. To objectively
&amp; in an automated way evaluate how teams play penetrative passes compared to
the number of times they had the potential to do so, the "Potential Penetrative
Pass (P3)" concept is presented here.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Take Me Home: Reversing Distribution Shifts using Reinforcement Learning. (arXiv:2302.10341v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10341">http://arxiv.org/abs/2302.10341</a></li>
<li>Code URL: <a href="https://github.com/vwlin/superstar">https://github.com/vwlin/superstar</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10341] Take Me Home: Reversing Distribution Shifts using Reinforcement Learning](http://arxiv.org/abs/2302.10341) #attack</code></li>
<li>Summary: <p>Deep neural networks have repeatedly been shown to be non-robust to the
uncertainties of the real world. Even subtle adversarial attacks and naturally
occurring distribution shifts wreak havoc on systems relying on deep neural
networks. In response to this, current state-of-the-art techniques use
data-augmentation to enrich the training distribution of the model and
consequently improve robustness to natural distribution shifts. We propose an
alternative approach that allows the system to recover from distribution shifts
online. Specifically, our method applies a sequence of semantic-preserving
transformations to bring the shifted data closer in distribution to the
training set, as measured by the Wasserstein distance. We formulate the problem
of sequence selection as an MDP, which we solve using reinforcement learning.
To aid in our estimates of Wasserstein distance, we employ dimensionality
reduction through orthonormal projection. We provide both theoretical and
empirical evidence that orthonormal projection preserves characteristics of the
data at the distributional level. Finally, we apply our distribution shift
recovery approach to the ImageNet-C benchmark for distribution shifts,
targeting shifts due to additive noise and image histogram modifications. We
demonstrate an improvement in average accuracy up to 14.21% across a variety of
state-of-the-art ImageNet classifiers.
</p></li>
</ul>

<h3>Title: Hello Me, Meet the Real Me: Audio Deepfake Attacks on Voice Assistants. (arXiv:2302.10328v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10328">http://arxiv.org/abs/2302.10328</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10328] Hello Me, Meet the Real Me: Audio Deepfake Attacks on Voice Assistants](http://arxiv.org/abs/2302.10328) #attack</code></li>
<li>Summary: <p>The radical advances in telecommunications and computer science have enabled
a myriad of applications and novel seamless interaction with computing
interfaces. Voice Assistants (VAs) have become a norm for smartphones, and
millions of VAs incorporated in smart devices are used to control these devices
in the smart home context. Previous research has shown that they are prone to
attacks, leading vendors to countermeasures. One of these measures is to allow
only a specific individual, the device's owner, to perform possibly dangerous
tasks, that is, tasks that may disclose personal information, involve monetary
transactions etc. To understand the extent to which VAs provide the necessary
protection to their users, we experimented with two of the most widely used
VAs, which the participants trained. We then utilised voice synthesis using
samples provided by participants to synthesise commands that were used to
trigger the corresponding VA and perform a dangerous task. Our extensive
results showed that more than 30\% of our deepfake attacks were successful and
that there was at least one successful attack for more than half of the
participants. Moreover, they illustrate statistically significant variation
among vendors and, in one case, even gender bias. The outcomes are rather
alarming and require the deployment of further countermeasures to prevent
exploitation, as the number of VAs in use is currently comparable to the world
population.
</p></li>
</ul>

<h3>Title: Characterizing the Optimal 0-1 Loss for Multi-class Classification with a Test-time Attacker. (arXiv:2302.10722v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10722">http://arxiv.org/abs/2302.10722</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10722] Characterizing the Optimal 0-1 Loss for Multi-class Classification with a Test-time Attacker](http://arxiv.org/abs/2302.10722) #attack</code></li>
<li>Summary: <p>Finding classifiers robust to adversarial examples is critical for their safe
deployment. Determining the robustness of the best possible classifier under a
given threat model for a given data distribution and comparing it to that
achieved by state-of-the-art training methods is thus an important diagnostic
tool. In this paper, we find achievable information-theoretic lower bounds on
loss in the presence of a test-time attacker for multi-class classifiers on any
discrete dataset. We provide a general framework for finding the optimal 0-1
loss that revolves around the construction of a conflict hypergraph from the
data and adversarial constraints. We further define other variants of the
attacker-classifier game that determine the range of the optimal loss more
efficiently than the full-fledged hypergraph construction. Our evaluation
shows, for the first time, an analysis of the gap to optimal robustness for
classifiers in the multi-class setting on benchmark datasets.
</p></li>
</ul>

<h3>Title: Generalization Bounds for Adversarial Contrastive Learning. (arXiv:2302.10633v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10633">http://arxiv.org/abs/2302.10633</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10633] Generalization Bounds for Adversarial Contrastive Learning](http://arxiv.org/abs/2302.10633) #attack</code></li>
<li>Summary: <p>Deep networks are well-known to be fragile to adversarial attacks, and
adversarial training is one of the most popular methods used to train a robust
model. To take advantage of unlabeled data, recent works have applied
adversarial training to contrastive learning (Adversarial Contrastive Learning;
ACL for short) and obtain promising robust performance. However, the theory of
ACL is not well understood. To fill this gap, we leverage the Rademacher
complexity to analyze the generalization performance of ACL, with a particular
focus on linear models and multi-layer neural networks under $\ell_p$ attack
($p \ge 1$). Our theory shows that the average adversarial risk of the
downstream tasks can be upper bounded by the adversarial unsupervised risk of
the upstream task. The experimental results validate our theory.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Fast and Painless Image Reconstruction in Deep Image Prior Subspaces. (arXiv:2302.10279v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10279">http://arxiv.org/abs/2302.10279</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10279] Fast and Painless Image Reconstruction in Deep Image Prior Subspaces](http://arxiv.org/abs/2302.10279) #robust</code></li>
<li>Summary: <p>The deep image prior (DIP) is a state-of-the-art unsupervised approach for
solving linear inverse problems in imaging. We address two key issues that have
held back practical deployment of the DIP: the long computing time needed to
train a separate deep network per reconstruction, and the susceptibility to
overfitting due to a lack of robust early stopping strategies in the
unsupervised setting. To this end, we restrict DIP optimisation to a sparse
linear subspace of the full parameter space. We construct the subspace from the
principal eigenspace of a set of parameter vectors sampled at equally spaced
intervals during DIP pre-training on synthetic task-agnostic data. The
low-dimensionality of the resulting subspace reduces DIP's capacity to fit
noise and allows the use of fast second order optimisation methods, e.g.,
natural gradient descent or L-BFGS. Experiments across tomographic tasks of
different geometry, ill-posedness and stopping criteria consistently show that
second order optimisation in a subspace is Pareto-optimal in terms of
optimisation time to reconstruction fidelity trade-off.
</p></li>
</ul>

<h3>Title: LiT Tuned Models for Efficient Species Detection. (arXiv:2302.10281v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10281">http://arxiv.org/abs/2302.10281</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10281] LiT Tuned Models for Efficient Species Detection](http://arxiv.org/abs/2302.10281) #robust</code></li>
<li>Summary: <p>Recent advances in training vision-language models have demonstrated
unprecedented robustness and transfer learning effectiveness; however, standard
computer vision datasets are image-only, and therefore not well adapted to such
training methods. Our paper introduces a simple methodology for adapting any
fine-grained image classification dataset for distributed vision-language
pretraining. We implement this methodology on the challenging iNaturalist-2021
dataset, comprised of approximately 2.7 million images of macro-organisms
across 10,000 classes, and achieve a new state-of-the art model in terms of
zero-shot classification accuracy. Somewhat surprisingly, our model (trained
using a new method called locked-image text tuning) uses a pre-trained, frozen
vision representation, proving that language alignment alone can attain strong
transfer learning performance, even on fractious, long-tailed datasets. Our
approach opens the door for utilizing high quality vision-language pretrained
models in agriculturally relevant applications involving species detection.
</p></li>
</ul>

<h3>Title: OppLoD: the Opponency based Looming Detector, Model Extension of Looming Sensitivity from LGMD to LPLC2. (arXiv:2302.10284v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10284">http://arxiv.org/abs/2302.10284</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10284] OppLoD: the Opponency based Looming Detector, Model Extension of Looming Sensitivity from LGMD to LPLC2](http://arxiv.org/abs/2302.10284) #robust</code></li>
<li>Summary: <p>Looming detection plays an important role in insect collision prevention
systems. As a vital capability evolutionary survival, it has been extensively
studied in neuroscience and is attracting increasing research interest in
robotics due to its close relationship with collision detection and navigation.
Visual cues such as angular size, angular velocity, and expansion have been
widely studied for looming detection by means of optic flow or elementary
neural computing research. However, a critical visual motion cue has been long
neglected because it is so easy to be confused with expansion, that is
radial-opponent-motion (ROM). Recent research on the discovery of LPLC2, a
ROM-sensitive neuron in Drosophila, has revealed its ultra-selectivity because
it only responds to stimuli with focal, outward movement. This characteristic
of ROM-sensitivity is consistent with the demand for collision detection
because it is strongly associated with danger looming that is moving towards
the center of the observer. Thus, we hope to extend the well-studied neural
model of the lobula giant movement detector (LGMD) with ROM-sensibility in
order to enhance robustness and accuracy at the same time. In this paper, we
investigate the potential to extend an image velocity-based looming detector,
the lobula giant movement detector (LGMD), with ROM-sensibility. To achieve
this, we propose the mathematical definition of ROM and its main property, the
radial motion opponency (RMO). Then, a synaptic neuropile that analogizes the
synaptic processing of LPLC2 is proposed in the form of lateral inhibition and
attention. Thus, our proposed model is the first to perform both image velocity
selectivity and ROM sensitivity. Systematic experiments are conducted to
exhibit the huge potential of the proposed bio-inspired looming detector.
</p></li>
</ul>

<h3>Title: CertViT: Certified Robustness of Pre-Trained Vision Transformers. (arXiv:2302.10287v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10287">http://arxiv.org/abs/2302.10287</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10287] CertViT: Certified Robustness of Pre-Trained Vision Transformers](http://arxiv.org/abs/2302.10287) #robust</code></li>
<li>Summary: <p>Lipschitz bounded neural networks are certifiably robust and have a good
trade-off between clean and certified accuracy. Existing Lipschitz bounding
methods train from scratch and are limited to moderately sized networks (< 6M
parameters). They require a fair amount of hyper-parameter tuning and are
computationally prohibitive for large networks like Vision Transformers (5M to
660M parameters). Obtaining certified robustness of transformers is not
feasible due to the non-scalability and inflexibility of the current methods.
This work presents CertViT, a two-step proximal-projection method to achieve
certified robustness from pre-trained weights. The proximal step tries to lower
the Lipschitz bound and the projection step tries to maintain the clean
accuracy of pre-trained weights. We show that CertViT networks have better
certified accuracy than state-of-the-art Lipschitz trained networks. We apply
CertViT on several variants of pre-trained vision transformers and show
adversarial robustness using standard attacks. Code :
https://github.com/sagarverma/transformer-lipschitz
</p></li>
</ul>

<h3>Title: Interpretable Out-Of-Distribution Detection Using Pattern Identification. (arXiv:2302.10303v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10303">http://arxiv.org/abs/2302.10303</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10303] Interpretable Out-Of-Distribution Detection Using Pattern Identification](http://arxiv.org/abs/2302.10303) #robust</code></li>
<li>Summary: <p>Out-of-distribution (OoD) detection for data-based programs is a goal of
paramount importance. Common approaches in the literature tend to train
detectors requiring inside-of-distribution (in-distribution, or IoD) and OoD
validation samples, and/or implement confidence metrics that are often abstract
and therefore difficult to interpret. In this work, we propose to use existing
work from the field of explainable AI, namely the PARTICUL pattern
identification algorithm, in order to build more interpretable and robust OoD
detectors for visual classifiers. Crucially, this approach does not require to
retrain the classifier and is tuned directly to the IoD dataset, making it
applicable to domains where OoD does not have a clear definition. Moreover,
pattern identification allows us to provide images from the IoD dataset as
reference points to better explain the confidence scores. We demonstrates that
the detection capabilities of this approach are on par with existing methods
through an extensive benchmark across four datasets and two definitions of OoD.
In particular, we introduce a new benchmark based on perturbations of the IoD
dataset which provides a known and quantifiable evaluation of the discrepancy
between the IoD and OoD datasets that serves as a reference value for the
comparison between various OoD detection methods. Our experiments show that the
robustness of all metrics under test does not solely depend on the nature of
the IoD dataset or the OoD definition, but also on the architecture of the
classifier, which stresses the need for thorough experimentations for future
work on OoD detection.
</p></li>
</ul>

<h3>Title: Automotive RADAR sub-sampling via object detection networks: Leveraging prior signal information. (arXiv:2302.10450v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10450">http://arxiv.org/abs/2302.10450</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10450] Automotive RADAR sub-sampling via object detection networks: Leveraging prior signal information](http://arxiv.org/abs/2302.10450) #robust</code></li>
<li>Summary: <p>Automotive radar has increasingly attracted attention due to growing interest
in autonomous driving technologies. Acquiring situational awareness using
multimodal data collected at high sampling rates by various sensing devices
including cameras, LiDAR, and radar requires considerable power, memory and
compute resources which are often limited at an edge device. In this paper, we
present a novel adaptive radar sub-sampling algorithm designed to identify
regions that require more detailed/accurate reconstruction based on prior
environmental conditions' knowledge, enabling near-optimal performance at
considerably lower effective sampling rates. Designed to robustly perform under
variable weather conditions, the algorithm was shown on the Oxford raw radar
and RADIATE dataset to achieve accurate reconstruction utilizing only 10% of
the original samples in good weather and 20% in extreme (snow, fog) weather
conditions. A further modification of the algorithm incorporates object motion
to enable reliable identification of important regions. This includes
monitoring possible future occlusions caused by objects detected in the present
frame. Finally, we train a YOLO network on the RADIATE dataset to perform
object detection directly on RADAR data and obtain a 6.6% AP50 improvement over
the baseline Faster R-CNN network.
</p></li>
</ul>

<h3>Title: Learning Gradually Non-convex Image Priors Using Score Matching. (arXiv:2302.10502v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10502">http://arxiv.org/abs/2302.10502</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10502] Learning Gradually Non-convex Image Priors Using Score Matching](http://arxiv.org/abs/2302.10502) #robust</code></li>
<li>Summary: <p>In this paper, we propose a unified framework of denoising score-based models
in the context of graduated non-convex energy minimization. We show that for
sufficiently large noise variance, the associated negative log density -- the
energy -- becomes convex. Consequently, denoising score-based models
essentially follow a graduated non-convexity heuristic. We apply this framework
to learning generalized Fields of Experts image priors that approximate the
joint density of noisy images and their associated variances. These priors can
be easily incorporated into existing optimization algorithms for solving
inverse problems and naturally implement a fast and robust graduated
non-convexity mechanism.
</p></li>
</ul>

<h3>Title: MulGT: Multi-task Graph-Transformer with Task-aware Knowledge Injection and Domain Knowledge-driven Pooling for Whole Slide Image Analysis. (arXiv:2302.10574v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10574">http://arxiv.org/abs/2302.10574</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10574] MulGT: Multi-task Graph-Transformer with Task-aware Knowledge Injection and Domain Knowledge-driven Pooling for Whole Slide Image Analysis](http://arxiv.org/abs/2302.10574) #robust</code></li>
<li>Summary: <p>Whole slide image (WSI) has been widely used to assist automated diagnosis
under the deep learning fields. However, most previous works only discuss the
SINGLE task setting which is not aligned with real clinical setting, where
pathologists often conduct multiple diagnosis tasks simultaneously. Also, it is
commonly recognized that the multi-task learning paradigm can improve learning
efficiency by exploiting commonalities and differences across multiple tasks.
To this end, we present a novel multi-task framework (i.e., MulGT) for WSI
analysis by the specially designed Graph-Transformer equipped with Task-aware
Knowledge Injection and Domain Knowledge-driven Graph Pooling modules.
Basically, with the Graph Neural Network and Transformer as the building
commons, our framework is able to learn task-agnostic low-level local
information as well as task-specific high-level global representation.
Considering that different tasks in WSI analysis depend on different features
and properties, we also design a novel Task-aware Knowledge Injection module to
transfer the task-shared graph embedding into task-specific feature spaces to
learn more accurate representation for different tasks. Further, we elaborately
design a novel Domain Knowledge-driven Graph Pooling module for each task to
improve both the accuracy and robustness of different tasks by leveraging
different diagnosis patterns of multiple tasks. We evaluated our method on two
public WSI datasets from TCGA projects, i.e., esophageal carcinoma and kidney
carcinoma. Experimental results show that our method outperforms single-task
counterparts and the state-of-theart methods on both tumor typing and staging
tasks.
</p></li>
</ul>

<h3>Title: SU-Net: Pose estimation network for non-cooperative spacecraft on-orbit. (arXiv:2302.10602v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10602">http://arxiv.org/abs/2302.10602</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10602] SU-Net: Pose estimation network for non-cooperative spacecraft on-orbit](http://arxiv.org/abs/2302.10602) #robust</code></li>
<li>Summary: <p>Spacecraft pose estimation plays a vital role in many on-orbit space
missions, such as rendezvous and docking, debris removal, and on-orbit
maintenance. At present, space images contain widely varying lighting
conditions, high contrast and low resolution, pose estimation of space objects
is more challenging than that of objects on earth. In this paper, we analyzing
the radar image characteristics of spacecraft on-orbit, then propose a new deep
learning neural Network structure named Dense Residual U-shaped Network
(DR-U-Net) to extract image features. We further introduce a novel neural
network based on DR-U-Net, namely Spacecraft U-shaped Network (SU-Net) to
achieve end-to-end pose estimation for non-cooperative spacecraft.
Specifically, the SU-Net first preprocess the image of non-cooperative
spacecraft, then transfer learning was used for pre-training. Subsequently, in
order to solve the problem of radar image blur and low ability of spacecraft
contour recognition, we add residual connection and dense connection to the
backbone network U-Net, and we named it DR-U-Net. In this way, the feature loss
and the complexity of the model is reduced, and the degradation of deep neural
network during training is avoided. Finally, a layer of feedforward neural
network is used for pose estimation of non-cooperative spacecraft on-orbit.
Experiments prove that the proposed method does not rely on the hand-made
object specific features, and the model has robust robustness, and the
calculation accuracy outperforms the state-of-the-art pose estimation methods.
The absolute error is 0.1557 to 0.4491 , the mean error is about 0.302 , and
the standard deviation is about 0.065 .
</p></li>
</ul>

<h3>Title: Effects of Architectures on Continual Semantic Segmentation. (arXiv:2302.10718v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10718">http://arxiv.org/abs/2302.10718</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10718] Effects of Architectures on Continual Semantic Segmentation](http://arxiv.org/abs/2302.10718) #robust</code></li>
<li>Summary: <p>Research in the field of Continual Semantic Segmentation is mainly
investigating novel learning algorithms to overcome catastrophic forgetting of
neural networks. Most recent publications have focused on improving learning
algorithms without distinguishing effects caused by the choice of neural
architecture.Therefore, we study how the choice of neural network architecture
affects catastrophic forgetting in class- and domain-incremental semantic
segmentation. Specifically, we compare the well-researched CNNs to recently
proposed Transformers and Hybrid architectures, as well as the impact of the
choice of novel normalization layers and different decoder heads. We find that
traditional CNNs like ResNet have high plasticity but low stability, while
transformer architectures are much more stable. When the inductive biases of
CNN architectures are combined with transformers in hybrid architectures, it
leads to higher plasticity and stability. The stability of these models can be
explained by their ability to learn general features that are robust against
distribution shifts. Experiments with different normalization layers show that
Continual Normalization achieves the best trade-off in terms of adaptability
and stability of the model. In the class-incremental setting, the choice of the
normalization layer has much less impact. Our experiments suggest that the
right choice of architecture can significantly reduce forgetting even with
naive fine-tuning and confirm that for real-world applications, the
architecture is an important factor in designing a continual learning model.
</p></li>
</ul>

<h3>Title: Evaluating the Effectiveness of Pre-trained Language Models in Predicting the Helpfulness of Online Product Reviews. (arXiv:2302.10199v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10199">http://arxiv.org/abs/2302.10199</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10199] Evaluating the Effectiveness of Pre-trained Language Models in Predicting the Helpfulness of Online Product Reviews](http://arxiv.org/abs/2302.10199) #robust</code></li>
<li>Summary: <p>Businesses and customers can gain valuable information from product reviews.
The sheer number of reviews often necessitates ranking them based on their
potential helpfulness. However, only a few reviews ever receive any helpfulness
votes on online marketplaces. Sorting all reviews based on the few existing
votes can cause helpful reviews to go unnoticed because of the limited
attention span of readers. The problem of review helpfulness prediction is even
more important for higher review volumes, and newly written reviews or launched
products. In this work we compare the use of RoBERTa and XLM-R language models
to predict the helpfulness of online product reviews. The contributions of our
work in relation to literature include extensively investigating the efficacy
of state-of-the-art language models -- both monolingual and multilingual --
against a robust baseline, taking ranking metrics into account when assessing
these approaches, and assessing multilingual models for the first time. We
employ the Amazon review dataset for our experiments. According to our study on
several product categories, multilingual and monolingual pre-trained language
models outperform the baseline that utilizes random forest with handcrafted
features as much as 23% in RMSE. Pre-trained language models reduce the need
for complex text feature engineering. However, our results suggest that
pre-trained multilingual models may not be used for fine-tuning only one
language. We assess the performance of language models with and without
additional features. Our results show that including additional features like
product rating by the reviewer can further help the predictive methods.
</p></li>
</ul>

<h3>Title: Mask-guided BERT for Few Shot Text Classification. (arXiv:2302.10447v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10447">http://arxiv.org/abs/2302.10447</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10447] Mask-guided BERT for Few Shot Text Classification](http://arxiv.org/abs/2302.10447) #robust</code></li>
<li>Summary: <p>Transformer-based language models have achieved significant success in
various domains. However, the data-intensive nature of the transformer
architecture requires much labeled data, which is challenging in low-resource
scenarios (i.e., few-shot learning (FSL)). The main challenge of FSL is the
difficulty of training robust models on small amounts of samples, which
frequently leads to overfitting. Here we present Mask-BERT, a simple and
modular framework to help BERT-based architectures tackle FSL. The proposed
approach fundamentally differs from existing FSL strategies such as prompt
tuning and meta-learning. The core idea is to selectively apply masks on text
inputs and filter out irrelevant information, which guides the model to focus
on discriminative tokens that influence prediction results. In addition, to
make the text representations from different categories more separable and the
text representations from the same category more compact, we introduce a
contrastive learning loss function. Experimental results on public-domain
benchmark datasets demonstrate the effectiveness of Mask-BERT.
</p></li>
</ul>

<h3>Title: Connecting Humanities and Social Sciences: Applying Language and Speech Technology to Online Panel Surveys. (arXiv:2302.10593v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10593">http://arxiv.org/abs/2302.10593</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10593] Connecting Humanities and Social Sciences: Applying Language and Speech Technology to Online Panel Surveys](http://arxiv.org/abs/2302.10593) #robust</code></li>
<li>Summary: <p>In this paper, we explore the application of language and speech technology
to open-ended questions in a Dutch panel survey. In an experimental wave
respondents could choose to answer open questions via speech or keyboard.
Automatic speech recognition (ASR) was used to process spoken responses. We
evaluated answers from these input modalities to investigate differences
between spoken and typed answers.We report the errors the ASR system produces
and investigate the impact of these errors on downstream analyses. Open-ended
questions give more freedom to answer for respondents, but entail a non-trivial
amount of work to analyse. We evaluated the feasibility of using
transformer-based models (e.g. BERT) to apply sentiment analysis and topic
modelling on the answers of open questions. A big advantage of
transformer-based models is that they are trained on a large amount of language
materials and do not necessarily need training on the target materials. This is
especially advantageous for survey data, which does not contain a lot of text
materials. We tested the quality of automatic sentiment analysis by comparing
automatic labeling with three human raters and tested the robustness of topic
modelling by comparing the generated models based on automatic and manually
transcribed spoken answers.
</p></li>
</ul>

<h3>Title: On Robust Numerical Solver for ODE via Self-Attention Mechanism. (arXiv:2302.10184v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10184">http://arxiv.org/abs/2302.10184</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10184] On Robust Numerical Solver for ODE via Self-Attention Mechanism](http://arxiv.org/abs/2302.10184) #robust</code></li>
<li>Summary: <p>With the development of deep learning techniques, AI-enhanced numerical
solvers are expected to become a new paradigm for solving differential
equations due to their versatility and effectiveness in alleviating the
accuracy-speed trade-off in traditional numerical solvers. However, this
paradigm still inevitably requires a large amount of high-quality data, whose
acquisition is often very expensive in natural science and engineering
problems. Therefore, in this paper, we explore training efficient and robust
AI-enhanced numerical solvers with a small data size by mitigating intrinsic
noise disturbances. We first analyze the ability of the self-attention
mechanism to regulate noise in supervised learning and then propose a
simple-yet-effective numerical solver, AttSolver, which introduces an additive
self-attention mechanism to the numerical solution of differential equations
based on the dynamical system perspective of the residual neural network. Our
results on benchmarks, ranging from high-dimensional problems to chaotic
systems, demonstrate the effectiveness of AttSolver in generally improving the
performance of existing traditional numerical solvers without any elaborated
model crafting. Finally, we analyze the convergence, generalization, and
robustness of the proposed method experimentally and theoretically.
</p></li>
</ul>

<h3>Title: Active Learning with Positive and Negative Pairwise Feedback. (arXiv:2302.10295v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10295">http://arxiv.org/abs/2302.10295</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10295] Active Learning with Positive and Negative Pairwise Feedback](http://arxiv.org/abs/2302.10295) #robust</code></li>
<li>Summary: <p>In this paper, we propose a generic framework for active clustering with
queries for pairwise similarities between objects. First, the pairwise
similarities can be any positive or negative number, yielding full flexibility
in the type of feedback that a user/annotator can provide. Second, the process
of querying pairwise similarities is separated from the clustering algorithm,
leading to more flexibility in how the query strategies can be constructed.
Third, the queries are robust to noise by allowing multiple queries for the
same pairwise similarity (i.e., a non-persistent noise model is assumed).
Finally, the number of clusters is automatically identified based on the
currently known pairwise similarities. In addition, we propose and analyze a
number of novel query strategies suited to this active clustering framework. We
demonstrate the effectiveness of our framework and the proposed query
strategies via several experimental studies.
</p></li>
</ul>

<h3>Title: Understanding the effect of varying amounts of replay per step. (arXiv:2302.10311v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10311">http://arxiv.org/abs/2302.10311</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10311] Understanding the effect of varying amounts of replay per step](http://arxiv.org/abs/2302.10311) #robust</code></li>
<li>Summary: <p>Model-based reinforcement learning uses models to plan, where the predictions
and policies of an agent can be improved by using more computation without
additional data from the environment, thereby improving sample efficiency.
However, learning accurate estimates of the model is hard. Subsequently, the
natural question is whether we can get similar benefits as planning with
model-free methods. Experience replay is an essential component of many
model-free algorithms enabling sample-efficient learning and stability by
providing a mechanism to store past experiences for further reuse in the
gradient computational process. Prior works have established connections
between models and experience replay by planning with the latter. This involves
increasing the number of times a mini-batch is sampled and used for updates at
each step (amount of replay per step). We attempt to exploit this connection by
doing a systematic study on the effect of varying amounts of replay per step in
a well-known model-free algorithm: Deep Q-Network (DQN) in the Mountain Car
environment. We empirically show that increasing replay improves DQN's sample
efficiency, reduces the variation in its performance, and makes it more robust
to change in hyperparameters. Altogether, this takes a step toward a better
algorithm for deployment.
</p></li>
</ul>

<h3>Title: Weather2K: A Multivariate Spatio-Temporal Benchmark Dataset for Meteorological Forecasting Based on Real-Time Observation Data from Ground Weather Stations. (arXiv:2302.10493v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10493">http://arxiv.org/abs/2302.10493</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10493] Weather2K: A Multivariate Spatio-Temporal Benchmark Dataset for Meteorological Forecasting Based on Real-Time Observation Data from Ground Weather Stations](http://arxiv.org/abs/2302.10493) #robust</code></li>
<li>Summary: <p>Weather forecasting is one of the cornerstones of meteorological work. In
this paper, we present a new benchmark dataset named Weather2K, which aims to
make up for the deficiencies of existing weather forecasting datasets in terms
of real-time, reliability, and diversity, as well as the key bottleneck of data
quality. To be specific, our Weather2K is featured from the following aspects:
1) Reliable and real-time data. The data is hourly collected from 2,130 ground
weather stations covering an area of 6 million square kilometers. 2)
Multivariate meteorological variables. 20 meteorological factors and 3
constants for position information are provided with a length of 40,896 time
steps. 3) Applicable to diverse tasks. We conduct a set of baseline tests on
time series forecasting and spatio-temporal forecasting. To the best of our
knowledge, our Weather2K is the first attempt to tackle weather forecasting
task by taking full advantage of the strengths of observation data from ground
weather stations. Based on Weather2K, we further propose Meteorological Factors
based Multi-Graph Convolution Network (MFMGCN), which can effectively construct
the intrinsic correlation among geographic locations based on meteorological
factors. Sufficient experiments show that MFMGCN improves both the forecasting
performance and temporal robustness. We hope our Weather2K can significantly
motivate researchers to develop efficient and accurate algorithms to advance
the task of weather forecasting. The dataset can be available at
https://github.com/bycnfz/weather2k/.
</p></li>
</ul>

<h3>Title: UAV Path Planning Employing MPC- Reinforcement Learning Method for search and rescue mission. (arXiv:2302.10669v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10669">http://arxiv.org/abs/2302.10669</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10669] UAV Path Planning Employing MPC- Reinforcement Learning Method for search and rescue mission](http://arxiv.org/abs/2302.10669) #robust</code></li>
<li>Summary: <p>In this paper, we tackle the problem of Unmanned Aerial (UA V) path planning
in complex and uncertain environments by designing a Model Predictive Control
(MPC), based on a Long-Short-Term Memory (LSTM) network integrated into the
Deep Deterministic Policy Gradient algorithm. In the proposed solution,
LSTM-MPC operates as a deterministic policy within the DDPG network, and it
leverages a predicting pool to store predicted future states and actions for
improved robustness and efficiency. The use of the predicting pool also enables
the initialization of the critic network, leading to improved convergence speed
and reduced failure rate compared to traditional reinforcement learning and
deep reinforcement learning methods. The effectiveness of the proposed solution
is evaluated by numerical simulations.
</p></li>
</ul>

<h3>Title: Hybridization of K-means with improved firefly algorithm for automatic clustering in high dimension. (arXiv:2302.10765v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10765">http://arxiv.org/abs/2302.10765</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10765] Hybridization of K-means with improved firefly algorithm for automatic clustering in high dimension](http://arxiv.org/abs/2302.10765) #robust</code></li>
<li>Summary: <p>K-means Clustering is the most well-known partitioning algorithm among all
clustering, by which we can partition the data objects very easily in to more
than one clusters. However, for K-means to choose an appropriate number of
clusters without any prior domain knowledge about the dataset is challenging,
especially in high-dimensional data objects. Hence, we have implemented the
Silhouette and Elbow methods with PCA to find an optimal number of clusters.
Also, previously, so many meta-heuristic swarm intelligence algorithms inspired
by nature have been employed to handle the automatic data clustering problem.
Firefly is efficient and robust for automatic clustering. However, in the
Firefly algorithm, the entire population is automatically subdivided into
sub-populations that decrease the convergence rate speed and trapping to local
minima in high-dimensional optimization problems. Thus, our study proposed an
enhanced firefly, i.e., a hybridized K-means with an ODFA model for automatic
clustering. The experimental part shows output and graphs of the Silhouette and
Elbow methods as well as the Firefly algorithm
</p></li>
</ul>

<h3>Title: Utilizing Domain Knowledge: Robust Machine Learning for Building Energy Prediction with Small, Inconsistent Datasets. (arXiv:2302.10784v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10784">http://arxiv.org/abs/2302.10784</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10784] Utilizing Domain Knowledge: Robust Machine Learning for Building Energy Prediction with Small, Inconsistent Datasets](http://arxiv.org/abs/2302.10784) #robust</code></li>
<li>Summary: <p>The demand for a huge amount of data for machine learning (ML) applications
is currently a bottleneck in an empirically dominated field. We propose a
method to combine prior knowledge with data-driven methods to significantly
reduce their data dependency. In this study, component-based machine learning
(CBML) as the knowledge-encoded data-driven method is examined in the context
of energy-efficient building engineering. It encodes the abstraction of
building structural knowledge as semantic information in the model
organization. We design a case experiment to understand the efficacy of
knowledge-encoded ML in sparse data input (1% - 0.0125% sampling rate). The
result reveals its three advanced features compared with pure ML methods: 1.
Significant improvement in the robustness of ML to extremely small-size and
inconsistent datasets; 2. Efficient data utilization from different entities'
record collections; 3. Characteristics of accepting incomplete data with high
interpretability and reduced training time. All these features provide a
promising path to alleviating the deployment bottleneck of data-intensive
methods and contribute to efficient real-world data usage. Moreover, four
necessary prerequisites are summarized in this study that ensures the target
scenario benefits by combining prior knowledge and ML generalization.
</p></li>
</ul>

<h3>Title: Benchmarking sparse system identification with low-dimensional chaos. (arXiv:2302.10787v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10787">http://arxiv.org/abs/2302.10787</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10787] Benchmarking sparse system identification with low-dimensional chaos](http://arxiv.org/abs/2302.10787) #robust</code></li>
<li>Summary: <p>Sparse system identification is the data-driven process of obtaining
parsimonious differential equations that describe the evolution of a dynamical
system, balancing model complexity and accuracy. There has been rapid
innovation in system identification across scientific domains, but there
remains a gap in the literature for large-scale methodological comparisons that
are evaluated on a variety of dynamical systems. In this work, we
systematically benchmark sparse regression variants by utilizing the dysts
standardized database of chaotic systems. In particular, we demonstrate how
this open-source tool can be used to quantitatively compare different methods
of system identification. To illustrate how this benchmark can be utilized, we
perform a large comparison of four algorithms for solving the sparse
identification of nonlinear dynamics (SINDy) optimization problem, finding
strong performance of the original algorithm and a recent mixed-integer
discrete algorithm. In all cases, we used ensembling to improve the noise
robustness of SINDy and provide statistical comparisons. In addition, we show
very compelling evidence that the weak SINDy formulation provides significant
improvements over the traditional method, even on clean data. Lastly, we
investigate how Pareto-optimal models generated from SINDy algorithms depend on
the properties of the equations, finding that the performance shows no
significant dependence on a set of dynamical properties that quantify the
amount of chaos, scale separation, degree of nonlinearity, and the syntactic
complexity.
</p></li>
</ul>

<h3>Title: A Novel Noise Injection-based Training Scheme for Better Model Robustness. (arXiv:2302.10802v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10802">http://arxiv.org/abs/2302.10802</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10802] A Novel Noise Injection-based Training Scheme for Better Model Robustness](http://arxiv.org/abs/2302.10802) #robust</code></li>
<li>Summary: <p>Noise injection-based method has been shown to be able to improve the
robustness of artificial neural networks in previous work. In this work, we
propose a novel noise injection-based training scheme for better model
robustness. Specifically, we first develop a likelihood ratio method to
estimate the gradient with respect to both synaptic weights and noise levels
for stochastic gradient descent training. Then, we design an approximation for
the vanilla noise injection-based training method to reduce memory and improve
computational efficiency. Next, we apply our proposed scheme to spiking neural
networks and evaluate the performance of classification accuracy and robustness
on MNIST and Fashion-MNIST datasets. Experiment results show that our proposed
method achieves a much better performance on adversarial robustness and
slightly better performance on original accuracy, compared with the
conventional gradient-based training method.
</p></li>
</ul>

<h3>Title: Minimax-Bayes Reinforcement Learning. (arXiv:2302.10831v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10831">http://arxiv.org/abs/2302.10831</a></li>
<li>Code URL: <a href="https://github.com/minimaxbrl/minimax-bayes-rl">https://github.com/minimaxbrl/minimax-bayes-rl</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10831] Minimax-Bayes Reinforcement Learning](http://arxiv.org/abs/2302.10831) #robust</code></li>
<li>Summary: <p>While the Bayesian decision-theoretic framework offers an elegant solution to
the problem of decision making under uncertainty, one question is how to
appropriately select the prior distribution. One idea is to employ a worst-case
prior. However, this is not as easy to specify in sequential decision making as
in simple statistical estimation problems. This paper studies (sometimes
approximate) minimax-Bayes solutions for various reinforcement learning
problems to gain insights into the properties of the corresponding priors and
policies. We find that while the worst-case prior depends on the setting, the
corresponding minimax policies are more robust than those that assume a
standard (i.e. uniform) prior.
</p></li>
</ul>

<h3>Title: Some Fundamental Aspects about Lipschitz Continuity of Neural Network Functions. (arXiv:2302.10886v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10886">http://arxiv.org/abs/2302.10886</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10886] Some Fundamental Aspects about Lipschitz Continuity of Neural Network Functions](http://arxiv.org/abs/2302.10886) #robust</code></li>
<li>Summary: <p>Lipschitz continuity is a simple yet pivotal functional property of any
predictive model that lies at the core of its robustness, generalisation, and
adversarial vulnerability. Our aim is to thoroughly investigate and
characterise the Lipschitz behaviour of the functions learned via neural
networks. Despite the significant tightening of the bounds in the recent years,
precisely estimating the Lipschitz constant continues to be a practical
challenge and tight theoretical analyses, similarly, remain intractable.
Therefore, we shift our perspective and instead attempt to uncover insights
about the nature of Lipschitz constant of neural networks functions -- by
relying on the simplest and most general upper and lower bounds. We carry out
an empirical investigation in a range of different settings (architectures,
losses, optimisers, label noise, etc.), which reveals several fundamental and
intriguing traits of the Lipschitz continuity of neural networks functions, In
particular, we identify a remarkable double descent trend in both upper and
lower bounds to the Lipschitz constant which tightly aligns with the typical
double descent trend in the test loss.
</p></li>
</ul>

<h2>biometric</h2>
<h3>Title: Combining Blockchain and Biometrics: A Survey on Technical Aspects and a First Legal Analysis. (arXiv:2302.10883v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10883">http://arxiv.org/abs/2302.10883</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10883] Combining Blockchain and Biometrics: A Survey on Technical Aspects and a First Legal Analysis](http://arxiv.org/abs/2302.10883) #biometric</code></li>
<li>Summary: <p>Biometric recognition as a unique, hard-to-forge, and efficient way of
identification and verification has become an indispensable part of the current
digital world. The fast evolution of this technology has been a strong
incentive for integrating it into many applications. Meanwhile, blockchain, the
very attractive decentralized ledger technology, has been widely received both
by the research and industry in the past years and it is being increasingly
deployed nowadays in many different applications, such as money transfer, IoT,
healthcare, or logistics. Recently, researchers have started to speculate what
would be the pros and cons and what would be the best applications when these
two technologies cross paths. This paper provides a survey of technical
literature research on the combination of blockchain and biometrics and
includes a first legal analysis of this integration to shed light on challenges
and potentials. While this combination is still in its infancy and a growing
body of literature discusses specific blockchain applications and solutions in
an advanced technological set-up, this paper presents a holistic understanding
of blockchains applicability in the biometric sector. This study demonstrates
that combining blockchain and biometrics would be beneficial for novel
applications in biometrics such as the PKI mechanism, distributed trusted
service, and identity management. However, blockchain networks at their current
stage are not efficient and economical for real-time applications. From a legal
point of view, the allocation of accountability remains a main issue, while
other difficulties remain, such as conducting a proper Data Protection Impact
Assessment. Finally, it supplies technical and legal recommendations to reap
the benefits and mitigate the risks of the combination.
</p></li>
</ul>

<h3>Title: Criminal Investigation Tracker with Suspect Prediction using Machine Learning. (arXiv:2302.10423v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10423">http://arxiv.org/abs/2302.10423</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10423] Criminal Investigation Tracker with Suspect Prediction using Machine Learning](http://arxiv.org/abs/2302.10423) #biometric</code></li>
<li>Summary: <p>An automated approach to identifying offenders in Sri Lanka would be better
than the current system. Obtaining information from eyewitnesses is one of the
less reliable approaches and procedures still in use today. Automated criminal
identification has the ability to save lives, notwithstanding Sri Lankan
culture's lack of awareness of the issue. Using cutting-edge technology like
biometrics to finish this task would be the most accurate strategy. The most
notable outcomes will be obtained by applying fingerprint and face recognition
as biometric techniques. The main responsibilities will be image optimization
and criminality. CCTV footage may be used to identify a person's fingerprint,
identify a person's face, and identify crimes involving weapons. Additionally,
we unveil a notification system and condense the police report to Additionally,
to make it simpler for police officers to understand the essential points of
the crime, we develop a notification system and condense the police report.
Additionally, if an incident involving a weapon is detected, an automated
notice of the crime with all the relevant facts is sent to the closest police
station. The summarization of the police report is what makes this the most
original. In order to improve the efficacy of the overall image, the system
will quickly and precisely identify the full crime scene, identify, and
recognize the suspects using their faces and fingerprints, and detect firearms.
This study provides a novel approach for crime prediction based on real-world
data, and criminality incorporation. A crime or occurrence should be reported
to the appropriate agencies, and the suggested web application should be
improved further to offer a workable channel of communication.
</p></li>
</ul>

<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: DrasCLR: A Self-supervised Framework of Learning Disease-related and Anatomy-specific Representation for 3D Medical Images. (arXiv:2302.10390v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10390">http://arxiv.org/abs/2302.10390</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10390] DrasCLR: A Self-supervised Framework of Learning Disease-related and Anatomy-specific Representation for 3D Medical Images](http://arxiv.org/abs/2302.10390) #extraction</code></li>
<li>Summary: <p>Large-scale volumetric medical images with annotation are rare, costly, and
time prohibitive to acquire. Self-supervised learning (SSL) offers a promising
pre-training and feature extraction solution for many downstream tasks, as it
only uses unlabeled data. Recently, SSL methods based on instance
discrimination have gained popularity in the medical imaging domain. However,
SSL pre-trained encoders may use many clues in the image to discriminate an
instance that are not necessarily disease-related. Moreover, pathological
patterns are often subtle and heterogeneous, requiring the ability of the
desired method to represent anatomy-specific features that are sensitive to
abnormal changes in different body parts. In this work, we present a novel SSL
framework, named DrasCLR, for 3D medical imaging to overcome these challenges.
We propose two domain-specific contrastive learning strategies: one aims to
capture subtle disease patterns inside a local anatomical region, and the other
aims to represent severe disease patterns that span larger regions. We
formulate the encoder using conditional hyper-parameterized network, in which
the parameters are dependant on the anatomical location, to extract
anatomically sensitive features. Extensive experiments on large-scale computer
tomography (CT) datasets of lung images show that our method improves the
performance of many downstream prediction and segmentation tasks. The
patient-level representation improves the performance of the patient survival
prediction task. We show how our method can detect emphysema subtypes via dense
prediction. We demonstrate that fine-tuning the pre-trained model can
significantly reduce annotation efforts without sacrificing emphysema detection
accuracy. Our ablation study highlights the importance of incorporating
anatomical context into the SSL framework.
</p></li>
</ul>

<h3>Title: Few-Shot Point Cloud Semantic Segmentation via Contrastive Self-Supervision and Multi-Resolution Attention. (arXiv:2302.10501v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10501">http://arxiv.org/abs/2302.10501</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10501] Few-Shot Point Cloud Semantic Segmentation via Contrastive Self-Supervision and Multi-Resolution Attention](http://arxiv.org/abs/2302.10501) #extraction</code></li>
<li>Summary: <p>This paper presents an effective few-shot point cloud semantic segmentation
approach for real-world applications. Existing few-shot segmentation methods on
point cloud heavily rely on the fully-supervised pretrain with large annotated
datasets, which causes the learned feature extraction bias to those pretrained
classes. However, as the purpose of few-shot learning is to handle
unknown/unseen classes, such class-specific feature extraction in pretrain is
not ideal to generalize into new classes for few-shot learning. Moreover, point
cloud datasets hardly have a large number of classes due to the annotation
difficulty. To address these issues, we propose a contrastive self-supervision
framework for few-shot learning pretrain, which aims to eliminate the feature
extraction bias through class-agnostic contrastive supervision. Specifically,
we implement a novel contrastive learning approach with a learnable augmentor
for a 3D point cloud to achieve point-wise differentiation, so that to enhance
the pretrain with managed overfitting through the self-supervision.
Furthermore, we develop a multi-resolution attention module using both the
nearest and farthest points to extract the local and global point information
more effectively, and a center-concentrated multi-prototype is adopted to
mitigate the intra-class sparsity. Comprehensive experiments are conducted to
evaluate the proposed approach, which shows our approach achieves
state-of-the-art performance. Moreover, a case study on practical CAM/CAD
segmentation is presented to demonstrate the effectiveness of our approach for
real-world applications.
</p></li>
</ul>

<h3>Title: Zero-Shot Information Extraction via Chatting with ChatGPT. (arXiv:2302.10205v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10205">http://arxiv.org/abs/2302.10205</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10205] Zero-Shot Information Extraction via Chatting with ChatGPT](http://arxiv.org/abs/2302.10205) #extraction</code></li>
<li>Summary: <p>Zero-shot information extraction (IE) aims to build IE systems from the
unannotated text. It is challenging due to involving little human intervention.
Challenging but worthwhile, zero-shot IE reduces the time and effort that data
labeling takes. Recent efforts on large language models (LLMs, e.g., GPT-3,
ChatGPT) show promising performance on zero-shot settings, thus inspiring us to
explore prompt-based methods. In this work, we ask whether strong IE models can
be constructed by directly prompting LLMs. Specifically, we transform the
zero-shot IE task into a multi-turn question-answering problem with a two-stage
framework (ChatIE). With the power of ChatGPT, we extensively evaluate our
framework on three IE tasks: entity-relation triple extract, named entity
recognition, and event extraction. Empirical results on six datasets across two
languages show that ChatIE achieves impressive performance and even surpasses
some full-shot models on several datasets (e.g., NYT11-HRL). We believe that
our work could shed light on building IE models with limited resources.
</p></li>
</ul>

<h3>Title: SparCA: Sparse Compressed Agglomeration for Feature Extraction and Dimensionality Reduction. (arXiv:2302.10776v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10776">http://arxiv.org/abs/2302.10776</a></li>
<li>Code URL: <a href="https://github.com/Neurology-AI-Program/sparca">https://github.com/Neurology-AI-Program/sparca</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10776] SparCA: Sparse Compressed Agglomeration for Feature Extraction and Dimensionality Reduction](http://arxiv.org/abs/2302.10776) #extraction</code></li>
<li>Summary: <p>The most effective dimensionality reduction procedures produce interpretable
features from the raw input space while also providing good performance for
downstream supervised learning tasks. For many methods, this requires
optimizing one or more hyperparameters for a specific task, which can limit
generalizability. In this study we propose sparse compressed agglomeration
(SparCA), a novel dimensionality reduction procedure that involves a multistep
hierarchical feature grouping, compression, and feature selection process. We
demonstrate the characteristics and performance of the SparCA method across
heterogenous synthetic and real-world datasets, including images, natural
language, and single cell gene expression data. Our results show that SparCA is
applicable to a wide range of data types, produces highly interpretable
features, and shows compelling performance on downstream supervised learning
tasks without the need for hyperparameter tuning.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: CADIS: Handling Cluster-skewed Non-IID Data in Federated Learning with Clustered Aggregation and Knowledge DIStilled Regularization. (arXiv:2302.10413v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10413">http://arxiv.org/abs/2302.10413</a></li>
<li>Code URL: <a href="https://github.com/aiot-lab-bkai/oro-ccgrid2023-cadis">https://github.com/aiot-lab-bkai/oro-ccgrid2023-cadis</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10413] CADIS: Handling Cluster-skewed Non-IID Data in Federated Learning with Clustered Aggregation and Knowledge DIStilled Regularization](http://arxiv.org/abs/2302.10413) #federate</code></li>
<li>Summary: <p>Federated learning enables edge devices to train a global model
collaboratively without exposing their data. Despite achieving outstanding
advantages in computing efficiency and privacy protection, federated learning
faces a significant challenge when dealing with non-IID data, i.e., data
generated by clients that are typically not independent and identically
distributed. In this paper, we tackle a new type of Non-IID data, called
cluster-skewed non-IID, discovered in actual data sets. The cluster-skewed
non-IID is a phenomenon in which clients can be grouped into clusters with
similar data distributions. By performing an in-depth analysis of the behavior
of a classification model's penultimate layer, we introduce a metric that
quantifies the similarity between two clients' data distributions without
violating their privacy. We then propose an aggregation scheme that guarantees
equality between clusters. In addition, we offer a novel local training
regularization based on the knowledge-distillation technique that reduces the
overfitting problem at clients and dramatically boosts the training scheme's
performance. We theoretically prove the superiority of the proposed aggregation
over the benchmark FedAvg. Extensive experimental results on both standard
public datasets and our in-house real-world dataset demonstrate that the
proposed approach improves accuracy by up to 16% compared to the FedAvg
algorithm.
</p></li>
</ul>

<h3>Title: FedST: Federated Shapelet Transformation for Interpretable Time Series Classification. (arXiv:2302.10631v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10631">http://arxiv.org/abs/2302.10631</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10631] FedST: Federated Shapelet Transformation for Interpretable Time Series Classification](http://arxiv.org/abs/2302.10631) #federate</code></li>
<li>Summary: <p>This paper studies how to develop accurate and interpretable time series
classification (TSC) models with the help of external data in a
privacy-preserving federated learning (FL) scenario. To the best of our
knowledge, we are the first to study on this essential topic. Achieving this
goal requires us to seamlessly integrate the techniques from multiple fields
including Data Mining, Machine Learning, and Security. In this paper, we
formulate the problem and identify the interpretability constraints under the
FL setting. We systematically investigate existing TSC solutions for the
centralized scenario and propose FedST, a novel FL-enabled TSC framework based
on a shapelet transformation method. We recognize the federated shapelet search
step as the kernel of FedST. Thus, we design FedSS-B, a basic protocol for the
FedST kernel that we prove to be secure and accurate. Further, we identify the
efficiency bottlenecks of the basic protocol and propose optimizations tailored
for the FL setting for acceleration. Our theoretical analysis shows that the
proposed optimizations are secure and more efficient. We conduct extensive
experiments using both synthetic and real-world datasets. Empirical results
show that our FedST solution is effective in terms of TSC accuracy, and the
proposed optimizations can achieve three orders of magnitude of speedup.
</p></li>
</ul>

<h3>Title: Clustered Data Sharing for Non-IID Federated Learning over Wireless Networks. (arXiv:2302.10747v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10747">http://arxiv.org/abs/2302.10747</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10747] Clustered Data Sharing for Non-IID Federated Learning over Wireless Networks](http://arxiv.org/abs/2302.10747) #federate</code></li>
<li>Summary: <p>Federated Learning (FL) is a novel distributed machine learning approach to
leverage data from Internet of Things (IoT) devices while maintaining data
privacy. However, the current FL algorithms face the challenges of
non-independent and identically distributed (non-IID) data, which causes high
communication costs and model accuracy declines. To address the statistical
imbalances in FL, we propose a clustered data sharing framework which spares
the partial data from cluster heads to credible associates through
device-to-device (D2D) communication. Moreover, aiming at diluting the data
skew on nodes, we formulate the joint clustering and data sharing problem based
on the privacy-preserving constrained graph. To tackle the serious coupling of
decisions on the graph, we devise a distribution-based adaptive clustering
algorithm (DACA) basing on three deductive cluster-forming conditions, which
ensures the maximum yield of data sharing. The experiments show that the
proposed framework facilitates FL on non-IID datasets with better convergence
and model accuracy under a limited communication environment.
</p></li>
</ul>

<h3>Title: Federated Gradient Matching Pursuit. (arXiv:2302.10755v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10755">http://arxiv.org/abs/2302.10755</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10755] Federated Gradient Matching Pursuit](http://arxiv.org/abs/2302.10755) #federate</code></li>
<li>Summary: <p>Traditional machine learning techniques require centralizing all training
data on one server or data hub. Due to the development of communication
technologies and a huge amount of decentralized data on many clients,
collaborative machine learning has become the main interest while providing
privacy-preserving frameworks. In particular, federated learning (FL) provides
such a solution to learn a shared model while keeping training data at local
clients. On the other hand, in a wide range of machine learning and signal
processing applications, the desired solution naturally has a certain structure
that can be framed as sparsity with respect to a certain dictionary. This
problem can be formulated as an optimization problem with sparsity constraints
and solving it efficiently has been one of the primary research topics in the
traditional centralized setting. In this paper, we propose a novel algorithmic
framework, federated gradient matching pursuit (FedGradMP), to solve the
sparsity constrained minimization problem in the FL setting. We also generalize
our algorithms to accommodate various practical FL scenarios when only a subset
of clients participate per round, when the local model estimation at clients
could be inexact, or when the model parameters are sparse with respect to
general dictionaries. Our theoretical analysis shows the linear convergence of
the proposed algorithms. A variety of numerical experiments are conducted to
demonstrate the great potential of the proposed framework -- fast convergence
both in communication rounds and computation time for many important scenarios
without sophisticated parameter tuning.
</p></li>
</ul>

<h3>Title: Distributed Learning in Heterogeneous Environment: federated learning with adaptive aggregation and computation reduction. (arXiv:2302.10757v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10757">http://arxiv.org/abs/2302.10757</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10757] Distributed Learning in Heterogeneous Environment: federated learning with adaptive aggregation and computation reduction](http://arxiv.org/abs/2302.10757) #federate</code></li>
<li>Summary: <p>Although federated learning has achieved many breakthroughs recently, the
heterogeneous nature of the learning environment greatly limits its performance
and hinders its real-world applications. The heterogeneous data, time-varying
wireless conditions and computing-limited devices are three main challenges,
which often result in an unstable training process and degraded accuracy.
Herein, we propose strategies to address these challenges. Targeting the
heterogeneous data distribution, we propose a novel adaptive mixing aggregation
(AMA) scheme that mixes the model updates from previous rounds with current
rounds to avoid large model shifts and thus, maintain training stability. We
further propose a novel staleness-based weighting scheme for the asynchronous
model updates caused by the dynamic wireless environment. Lastly, we propose a
novel CPU-friendly computation-reduction scheme based on transfer learning by
sharing the feature extractor (FES) and letting the computing-limited devices
update only the classifier. The simulation results show that the proposed
framework outperforms existing state-of-the-art solutions and increases the
test accuracy, and training stability by up to 2.38%, 93.10% respectively.
Additionally, the proposed framework can tolerate communication delay of up to
15 rounds under a moderate delay environment without significant accuracy
degradation.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Multivariate Systemic Risk Measures and Deep Learning Algorithms. (arXiv:2302.10183v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10183">http://arxiv.org/abs/2302.10183</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10183] Multivariate Systemic Risk Measures and Deep Learning Algorithms](http://arxiv.org/abs/2302.10183) #fair</code></li>
<li>Summary: <p>In this work we propose deep learning-based algorithms for the computation of
systemic shortfall risk measures defined via multivariate utility functions. We
discuss the key related theoretical aspects, with a particular focus on the
fairness properties of primal optima and associated risk allocations. The
algorithms we provide allow for learning primal optimizers, optima for the dual
representation and corresponding fair risk allocations. We test our algorithms
by comparison to a benchmark model, based on a paired exponential utility
function, for which we can provide explicit formulas. We also show evidence of
convergence in a case for which explicit formulas are not available.
</p></li>
</ul>

<h3>Title: A Unifying Perspective on Multi-Calibration: Unleashing Game Dynamics for Multi-Objective Learning. (arXiv:2302.10863v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10863">http://arxiv.org/abs/2302.10863</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10863] A Unifying Perspective on Multi-Calibration: Unleashing Game Dynamics for Multi-Objective Learning](http://arxiv.org/abs/2302.10863) #fair</code></li>
<li>Summary: <p>We provide a unifying framework for the design and analysis of
multi-calibrated and moment-multi-calibrated predictors. Placing the
multi-calibration problem in the general setting of \emph{multi-objective
learning} -- where learning guarantees must hold simultaneously over a set of
distributions and loss functions -- we exploit connections to game dynamics to
obtain state-of-the-art guarantees for a diverse set of multi-calibration
learning problems. In addition to shedding light on existing multi-calibration
guarantees, and greatly simplifying their analysis, our approach yields a
$1/\epsilon^2$ improvement in the number of oracle calls compared to the
state-of-the-art algorithm of Jung et al. 2021 for learning deterministic
moment-calibrated predictors and an exponential improvement in $k$ compared to
the state-of-the-art algorithm of Gopalan et al. 2022 for learning a $k$-class
multi-calibrated predictor. Beyond multi-calibration, we use these game
dynamics to address existing and emerging considerations in the study of group
fairness and multi-distribution learning.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Can Large Language Models Change User Preference Adversarially?. (arXiv:2302.10291v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10291">http://arxiv.org/abs/2302.10291</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10291] Can Large Language Models Change User Preference Adversarially?](http://arxiv.org/abs/2302.10291) #interpretability</code></li>
<li>Summary: <p>Pretrained large language models (LLMs) are becoming increasingly powerful
and ubiquitous in mainstream applications such as being a personal assistant, a
dialogue model, etc. As these models become proficient in deducing user
preferences and offering tailored assistance, there is an increasing concern
about the ability of these models to influence, modify and in the extreme case
manipulate user preference adversarially. The issue of lack of interpretability
in these models in adversarial settings remains largely unsolved. This work
tries to study adversarial behavior in user preferences from the lens of
attention probing, red teaming and white-box analysis. Specifically, it
provides a bird's eye view of existing literature, offers red teaming samples
for dialogue models like ChatGPT and GODEL and probes the attention mechanism
in the latter for non-adversarial and adversarial settings.
</p></li>
</ul>

<h3>Title: Tell Model Where to Attend: Improving Interpretability of Aspect-Based Sentiment Classification via Small Explanation Annotations. (arXiv:2302.10479v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10479">http://arxiv.org/abs/2302.10479</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10479] Tell Model Where to Attend: Improving Interpretability of Aspect-Based Sentiment Classification via Small Explanation Annotations](http://arxiv.org/abs/2302.10479) #interpretability</code></li>
<li>Summary: <p>Gradient-based explanation methods play an important role in the field of
interpreting complex deep neural networks for NLP models. However, the existing
work has shown that the gradients of a model are unstable and easily
manipulable, which impacts the model's reliability largely. According to our
preliminary analyses, we also find the interpretability of gradient-based
methods is limited for complex tasks, such as aspect-based sentiment
classification (ABSC). In this paper, we propose an
\textbf{I}nterpretation-\textbf{E}nhanced \textbf{G}radient-based framework for
\textbf{A}BSC via a small number of explanation annotations, namely
\texttt{{IEGA}}. Particularly, we first calculate the word-level saliency map
based on gradients to measure the importance of the words in the sentence
towards the given aspect. Then, we design a gradient correction module to
enhance the model's attention on the correct parts (e.g., opinion words). Our
model is model agnostic and task agnostic so that it can be integrated into the
existing ABSC methods or other tasks. Comprehensive experimental results on
four benchmark datasets show that our \texttt{IEGA} can improve not only the
interpretability of the model but also the performance and robustness.
</p></li>
</ul>

<h3>Title: Parallel Sentence-Level Explanation Generation for Real-World Low-Resource Scenarios. (arXiv:2302.10707v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10707">http://arxiv.org/abs/2302.10707</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10707] Parallel Sentence-Level Explanation Generation for Real-World Low-Resource Scenarios](http://arxiv.org/abs/2302.10707) #interpretability</code></li>
<li>Summary: <p>In order to reveal the rationale behind model predictions, many works have
exploited providing explanations in various forms. Recently, to further
guarantee readability, more and more works turn to generate sentence-level
human language explanations. However, current works pursuing sentence-level
explanations rely heavily on annotated training data, which limits the
development of interpretability to only a few tasks. As far as we know, this
paper is the first to explore this problem smoothly from weak-supervised
learning to unsupervised learning. Besides, we also notice the high latency of
autoregressive sentence-level explanation generation, which leads to
asynchronous interpretability after prediction. Therefore, we propose a
non-autoregressive interpretable model to facilitate parallel explanation
generation and simultaneous prediction. Through extensive experiments on
Natural Language Inference task and Spouse Prediction task, we find that users
are able to train classifiers with comparable performance $10-15\times$ faster
with parallel explanation generation using only a few or no annotated training
data.
</p></li>
</ul>

<h3>Title: On Inductive Biases for Machine Learning in Data Constrained Settings. (arXiv:2302.10692v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10692">http://arxiv.org/abs/2302.10692</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10692] On Inductive Biases for Machine Learning in Data Constrained Settings](http://arxiv.org/abs/2302.10692) #interpretability</code></li>
<li>Summary: <p>Learning with limited data is one of the biggest problems of machine
learning. Current approaches to this issue consist in learning general
representations from huge amounts of data before fine-tuning the model on a
small dataset of interest. While such technique, coined transfer learning, is
very effective in domains such as computer vision or natural langage
processing, it does not yet solve common problems of deep learning such as
model interpretability or the overall need for data. This thesis explores a
different answer to the problem of learning expressive models in data
constrained settings: instead of relying on big datasets to learn neural
networks, we will replace some modules by known functions reflecting the
structure of the data. Very often, these functions will be drawn from the rich
literature of kernel methods. Indeed, many kernels can reflect the underlying
structure of the data, thus sparing learning parameters to some extent. Our
approach falls under the hood of "inductive biases", which can be defined as
hypothesis on the data at hand restricting the space of models to explore
during learning. We demonstrate the effectiveness of this approach in the
context of sequences, such as sentences in natural language or protein
sequences, and graphs, such as molecules. We also highlight the relationship
between our work and recent advances in deep learning. Additionally, we study
convex machine learning models. Here, rather than proposing new models, we
wonder which proportion of the samples in a dataset is really needed to learn a
"good" model. More precisely, we study the problem of safe sample screening,
i.e, executing simple tests to discard uninformative samples from a dataset
even before fitting a machine learning model, without affecting the optimal
model. Such techniques can be used to prune datasets or mine for rare samples.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: Route, Interpret, Repeat: Blurring the Line Between Post hoc Explainability and Interpretable Models. (arXiv:2302.10289v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10289">http://arxiv.org/abs/2302.10289</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10289] Route, Interpret, Repeat: Blurring the Line Between Post hoc Explainability and Interpretable Models](http://arxiv.org/abs/2302.10289) #explainability</code></li>
<li>Summary: <p>The current approach to ML model design is either to choose a flexible
Blackbox model and explain it post hoc or to start with an interpretable model.
Blackbox models are flexible but difficult to explain, whereas interpretable
models are designed to be explainable. However, developing interpretable models
necessitates extensive ML knowledge, and the resulting models tend to be less
flexible, offering potentially subpar performance compared to their Blackbox
equivalents. This paper aims to blur the distinction between a post hoc
explanation of a BlackBox and constructing interpretable models. We propose
beginning with a flexible BlackBox model and gradually \emph{carving out} a
mixture of interpretable models and a \emph{residual network}. Our design
identifies a subset of samples and \emph{routes} them through the interpretable
models. The remaining samples are routed through a flexible residual network.
We adopt First Order Logic (FOL) as the interpretable model's backbone, which
provides basic reasoning on concepts retrieved from the BlackBox model. On the
residual network, we repeat the method until the proportion of data explained
by the residual network falls below a desired threshold. Our approach offers
several advantages. First, the mixture of interpretable and flexible residual
networks results in almost no compromise in performance. Second, the route,
interpret, and repeat approach yields a highly flexible interpretable model.
Our extensive experiment demonstrates the performance of the model on various
datasets. We show that by editing the FOL model, we can fix the shortcut
learned by the original BlackBox model. Finally, our method provides a
framework for a hybrid symbolic-connectionist network that is simple to train
and adaptable to many applications.
</p></li>
</ul>

<h2>watermark</h2>
<h3>Title: On Function-Coupled Watermarks for Deep Neural Networks. (arXiv:2302.10296v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10296">http://arxiv.org/abs/2302.10296</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10296] On Function-Coupled Watermarks for Deep Neural Networks](http://arxiv.org/abs/2302.10296) #watermark</code></li>
<li>Summary: <p>Well-performed deep neural networks (DNNs) generally require massive labelled
data and computational resources for training. Various watermarking techniques
are proposed to protect such intellectual properties (IPs), wherein the DNN
providers implant secret information into the model so that they can later
claim IP ownership by retrieving their embedded watermarks with some dedicated
trigger inputs. While promising results are reported in the literature,
existing solutions suffer from watermark removal attacks, such as model
fine-tuning and model pruning.
</p></li>
</ul>

<p>In this paper, we propose a novel DNN watermarking solution that can
effectively defend against the above attacks. Our key insight is to enhance the
coupling of the watermark and model functionalities such that removing the
watermark would inevitably degrade the model's performance on normal inputs. To
this end, unlike previous methods relying on secret features learnt from
out-of-distribution data, our method only uses features learnt from
in-distribution data. Specifically, on the one hand, we propose to sample
inputs from the original training dataset and fuse them as watermark triggers.
On the other hand, we randomly mask model weights during training so that the
information of our embedded watermarks spreads in the network. By doing so,
model fine-tuning/pruning would not forget our function-coupled watermarks.
Evaluation results on various image classification tasks show a 100\% watermark
authentication success rate under aggressive watermark removal attacks,
significantly outperforming existing solutions. Code is available:
https://github.com/cure-lab/Function-Coupled-Watermark.
</p>

<h2>diffusion</h2>
<h3>Title: Analyzing Multimodal Objectives Through the Lens of Generative Diffusion Guidance. (arXiv:2302.10305v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10305">http://arxiv.org/abs/2302.10305</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10305] Analyzing Multimodal Objectives Through the Lens of Generative Diffusion Guidance](http://arxiv.org/abs/2302.10305) #diffusion</code></li>
<li>Summary: <p>Recent years have witnessed astonishing advances in the field of multimodal
representation learning, with contrastive learning being the cornerstone for
major breakthroughs. Latest works delivered further improvements by
incorporating different objectives such as masked modeling and captioning into
the frameworks, but our understanding on how these objectives facilitate
learning remains vastly incomplete. In this paper, we leverage the fact that
classifier-guided diffusion models generate images that reflect the semantic
signals provided by the classifier to study the characteristics of multimodal
learning objectives. Specifically, we compare contrastive, matching and
captioning loss in terms of their semantic signals, and introduce a simple
baseline that not only supports our analyses but also improves the quality of
generative guidance in a straightforward manner.
</p></li>
</ul>

<h3>Title: Unsupervised Out-of-Distribution Detection with Diffusion Inpainting. (arXiv:2302.10326v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10326">http://arxiv.org/abs/2302.10326</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10326] Unsupervised Out-of-Distribution Detection with Diffusion Inpainting](http://arxiv.org/abs/2302.10326) #diffusion</code></li>
<li>Summary: <p>Unsupervised out-of-distribution detection (OOD) seeks to identify
out-of-domain data by learning only from unlabeled in-domain data. We present a
novel approach for this task - Lift, Map, Detect (LMD) - that leverages recent
advancement in diffusion models. Diffusion models are one type of generative
models. At their core, they learn an iterative denoising process that gradually
maps a noisy image closer to their training manifolds. LMD leverages this
intuition for OOD detection. Specifically, LMD lifts an image off its original
manifold by corrupting it, and maps it towards the in-domain manifold with a
diffusion model. For an out-of-domain image, the mapped image would have a
large distance away from its original manifold, and LMD would identify it as
OOD accordingly. We show through extensive experiments that LMD achieves
competitive performance across a broad variety of datasets.
</p></li>
</ul>

<h3>Title: Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels. (arXiv:2302.10586v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10586">http://arxiv.org/abs/2302.10586</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10586] Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels](http://arxiv.org/abs/2302.10586) #diffusion</code></li>
<li>Summary: <p>We propose a three-stage training strategy called dual pseudo training (DPT)
for conditional image generation and classification in semi-supervised
learning. First, a classifier is trained on partially labeled data and predicts
pseudo labels for all data. Second, a conditional generative model is trained
on all data with pseudo labels and generates pseudo images given labels.
Finally, the classifier is trained on real data augmented by pseudo images with
labels. We demonstrate large-scale diffusion models and semi-supervised
learners benefit mutually with a few labels via DPT. In particular, on the
ImageNet 256x256 generation benchmark, DPT can generate realistic, diverse, and
semantically correct images with very few labels. With two (i.e., < 0.2%) and
five (i.e., < 0.4%) labels per class, DPT achieves an FID of 3.44 and 3.37
respectively, outperforming strong diffusion models with full labels, such as
IDDPM, CDM, ADM, and LDM. Besides, DPT outperforms competitive semi-supervised
baselines substantially on ImageNet classification benchmarks with one, two,
and five labels per class, achieving state-of-the-art top-1 accuracies of 59.0
(+2.8), 69.5 (+3.0), and 73.6 (+1.2) respectively.
</p></li>
</ul>

<h3>Title: RealFusion: 360{\deg} Reconstruction of Any Object from a Single Image. (arXiv:2302.10663v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10663">http://arxiv.org/abs/2302.10663</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10663] RealFusion: 360{\deg} Reconstruction of Any Object from a Single Image](http://arxiv.org/abs/2302.10663) #diffusion</code></li>
<li>Summary: <p>We consider the problem of reconstructing a full 360{\deg} photographic model
of an object from a single image of it. We do so by fitting a neural radiance
field to the image, but find this problem to be severely ill-posed. We thus
take an off-the-self conditional image generator based on diffusion and
engineer a prompt that encourages it to ``dream up'' novel views of the object.
Using an approach inspired by DreamFields and DreamFusion, we fuse the given
input view, the conditional prior, and other regularizers in a final,
consistent reconstruction. We demonstrate state-of-the-art reconstruction
results on benchmark images when compared to prior methods for monocular 3D
reconstruction of objects. Qualitatively, our reconstructions provide a
faithful match of the input view and a plausible extrapolation of its
appearance and 3D shape, including to the side of the object not visible in the
image.
</p></li>
</ul>

<h3>Title: $PC^2$: Projection-Conditioned Point Cloud Diffusion for Single-Image 3D Reconstruction. (arXiv:2302.10668v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10668">http://arxiv.org/abs/2302.10668</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10668] $PC^2$: Projection-Conditioned Point Cloud Diffusion for Single-Image 3D Reconstruction](http://arxiv.org/abs/2302.10668) #diffusion</code></li>
<li>Summary: <p>Reconstructing the 3D shape of an object from a single RGB image is a
long-standing and highly challenging problem in computer vision. In this paper,
we propose a novel method for single-image 3D reconstruction which generates a
sparse point cloud via a conditional denoising diffusion process. Our method
takes as input a single RGB image along with its camera pose and gradually
denoises a set of 3D points, whose positions are initially sampled randomly
from a three-dimensional Gaussian distribution, into the shape of an object.
The key to our method is a geometrically-consistent conditioning process which
we call projection conditioning: at each step in the diffusion process, we
project local image features onto the partially-denoised point cloud from the
given camera pose. This projection conditioning process enables us to generate
high-resolution sparse geometries that are well-aligned with the input image,
and can additionally be used to predict point colors after shape
reconstruction. Moreover, due to the probabilistic nature of the diffusion
process, our method is naturally capable of generating multiple different
shapes consistent with a single input image. In contrast to prior work, our
approach not only performs well on synthetic benchmarks, but also gives large
qualitative improvements on complex real-world data.
</p></li>
</ul>

<h3>Title: On Calibrating Diffusion Probabilistic Models. (arXiv:2302.10688v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10688">http://arxiv.org/abs/2302.10688</a></li>
<li>Code URL: <a href="https://github.com/thudzj/calibrated-dpms">https://github.com/thudzj/calibrated-dpms</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10688] On Calibrating Diffusion Probabilistic Models](http://arxiv.org/abs/2302.10688) #diffusion</code></li>
<li>Summary: <p>Recently, diffusion probabilistic models (DPMs) have achieved promising
results in diverse generative tasks. A typical DPM framework includes a forward
process that gradually diffuses the data distribution and a reverse process
that recovers the data distribution from time-dependent data scores. In this
work, we observe that the stochastic reverse process of data scores is a
martingale, from which concentration bounds and the optional stopping theorem
for data scores can be derived. Then, we discover a simple way for calibrating
an arbitrary pretrained DPM, with which the score matching loss can be reduced
and the lower bounds of model likelihood can consequently be increased. We
provide general calibration guidelines under various model parametrizations.
Our calibration method is performed only once and the resulting models can be
used repeatedly for sampling. We conduct experiments on multiple datasets to
empirically validate our proposal. Our code is at
https://github.com/thudzj/Calibrated-DPMs.
</p></li>
</ul>

<h3>Title: Learning 3D Photography Videos via Self-supervised Diffusion on Single Images. (arXiv:2302.10781v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10781">http://arxiv.org/abs/2302.10781</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10781] Learning 3D Photography Videos via Self-supervised Diffusion on Single Images](http://arxiv.org/abs/2302.10781) #diffusion</code></li>
<li>Summary: <p>3D photography renders a static image into a video with appealing 3D visual
effects. Existing approaches typically first conduct monocular depth
estimation, then render the input frame to subsequent frames with various
viewpoints, and finally use an inpainting model to fill those missing/occluded
regions. The inpainting model plays a crucial role in rendering quality, but it
is normally trained on out-of-domain data. To reduce the training and inference
gap, we propose a novel self-supervised diffusion model as the inpainting
module. Given a single input image, we automatically construct a training pair
of the masked occluded image and the ground-truth image with random
cycle-rendering. The constructed training samples are closely aligned to the
testing instances, without the need of data annotation. To make full use of the
masked images, we design a Masked Enhanced Block (MEB), which can be easily
plugged into the UNet and enhance the semantic conditions. Towards real-world
animation, we present a novel task: out-animation, which extends the space and
time of input objects. Extensive experiments on real datasets show that our
method achieves competitive results with existing SOTA methods.
</p></li>
</ul>

<h3>Title: Diffusion Probabilistic Models for Graph-Structured Prediction. (arXiv:2302.10506v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2302.10506">http://arxiv.org/abs/2302.10506</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2302.10506] Diffusion Probabilistic Models for Graph-Structured Prediction](http://arxiv.org/abs/2302.10506) #diffusion</code></li>
<li>Summary: <p>This paper studies graph-structured prediction for supervised learning on
graphs with node-wise or edge-wise target dependencies. To solve this problem,
recent works investigated combining graph neural networks (GNNs) with
conventional structured prediction algorithms like conditional random fields.
However, in this work, we pursue an alternative direction building on the
recent successes of diffusion probabilistic models (DPMs). That is, we propose
a new framework using DPMs to make graph-structured predictions. In the fully
supervised setting, our DPM captures the target dependencies by iteratively
updating each target estimate based on the estimates of nearby targets. We also
propose a variational expectation maximization algorithm to train our DPM in
the semi-supervised setting. Extensive experiments verify that our framework
consistently outperforms existing neural structured prediction models on
inductive and transductive node classification. We also demonstrate the
competitive performance of our framework for algorithmic reasoning tasks.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
