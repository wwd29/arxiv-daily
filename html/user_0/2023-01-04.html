<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Surveillance Face Anti-spoofing. (arXiv:2301.00975v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.00975">http://arxiv.org/abs/2301.00975</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.00975] Surveillance Face Anti-spoofing](http://arxiv.org/abs/2301.00975) #secure</code></li>
<li>Summary: <p>Face Anti-spoofing (FAS) is essential to secure face recognition systems from
various physical attacks. However, recent research generally focuses on
short-distance applications (i.e., phone unlocking) while lacking consideration
of long-distance scenes (i.e., surveillance security checks). In order to
promote relevant research and fill this gap in the community, we collect a
large-scale Surveillance High-Fidelity Mask (SuHiFiMask) dataset captured under
40 surveillance scenes, which has 101 subjects from different age groups with
232 3D attacks (high-fidelity masks), 200 2D attacks (posters, portraits, and
screens), and 2 adversarial attacks. In this scene, low image resolution and
noise interference are new challenges faced in surveillance FAS. Together with
the SuHiFiMask dataset, we propose a Contrastive Quality-Invariance Learning
(CQIL) network to alleviate the performance degradation caused by image quality
from three aspects: (1) An Image Quality Variable module (IQV) is introduced to
recover image information associated with discrimination by combining the
super-resolution network. (2) Using generated sample pairs to simulate quality
variance distributions to help contrastive learning strategies obtain robust
feature representation under quality variation. (3) A Separate Quality Network
(SQN) is designed to learn discriminative features independent of image
quality. Finally, a large number of experiments verify the quality of the
SuHiFiMask dataset and the superiority of the proposed CQIL.
</p></li>
</ul>

<h3>Title: Bringing data minimization to digital wallets at scale with general-purpose zero-knowledge proofs. (arXiv:2301.00823v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.00823">http://arxiv.org/abs/2301.00823</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.00823] Bringing data minimization to digital wallets at scale with general-purpose zero-knowledge proofs](http://arxiv.org/abs/2301.00823) #secure</code></li>
<li>Summary: <p>Today, digital identity management for individuals is either inconvenient and
error-prone or creates undesirable lock-in effects and violates privacy and
security expectations. These shortcomings inhibit the digital transformation in
general and seem particularly concerning in the context of novel applications
such as access control for decentralized autonomous organizations and
identification in the Metaverse. Decentralized or self-sovereign identity (SSI)
aims to offer a solution to this dilemma by empowering individuals to manage
their digital identity through machine-verifiable attestations stored in a
"digital wallet" application on their edge devices. However, when presented to
a relying party, these attestations typically reveal more attributes than
required and allow tracking end users' activities. Several academic works and
practical solutions exist to reduce or avoid such excessive information
disclosure, from simple selective disclosure to data-minimizing anonymous
credentials based on zero-knowledge proofs (ZKPs). We first demonstrate that
the SSI solutions that are currently built with anonymous credentials still
lack essential features such as scalable revocation, certificate chaining, and
integration with secure elements. We then argue that general-purpose ZKPs in
the form of zk-SNARKs can appropriately address these pressing challenges. We
describe our implementation and conduct performance tests on different edge
devices to illustrate that the performance of zk-SNARK-based anonymous
credentials is already practical. We also discuss further advantages that
general-purpose ZKPs can easily provide for digital wallets, for instance, to
create "designated verifier presentations" that facilitate new design options
for digital identity infrastructures that previously were not accessible
because of the threat of man-in-the-middle attacks.
</p></li>
</ul>

<h3>Title: RSA+: An algorithm at least as secure as RSA. (arXiv:2301.01282v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.01282">http://arxiv.org/abs/2301.01282</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.01282] RSA+: An algorithm at least as secure as RSA](http://arxiv.org/abs/2301.01282) #secure</code></li>
<li>Summary: <p>The RSA algorithm has been around for nearly five decades and remains one of
the most studied public key cryptosystems. Many attempts have been made to
break it or improve it and questions remain about the equivalence of the
strength of its security to well known hard problems in computational number
theory. In this note we propose a modified version which we call RSA+ which is
at least as secure as RSA and show that breaking RSA+ is probably
computationally equivalent to factoring $n$, the public modulus. The motivation
came from wanting to obscure the encryption exponent in RSA.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: SAFEMYRIDES: Application of Decentralized Control Edge-Computing to Ridesharing Monitoring Services. (arXiv:2301.00888v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.00888">http://arxiv.org/abs/2301.00888</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.00888] SAFEMYRIDES: Application of Decentralized Control Edge-Computing to Ridesharing Monitoring Services](http://arxiv.org/abs/2301.00888) #security</code></li>
<li>Summary: <p>Edge computing is changing the face of many industries and services. Common
edge computing models offload computing which is prone to security risks and
privacy violation. However, advances in deep learning enabled Internet of
Things (IoTs) to take decisions and run cognitive tasks locally. This research
introduces a decentralized-control edge model where most computation and
decisions are moved to the IoT level. The model aims at decreasing
communication to the edge which in return enhances efficiency and decreases
latency. The model also avoids data transfer which raises security and privacy
risks. To examine the model, we developed SAFEMYRIDES, a scene-aware
ridesharing monitoring system where smart phones are detecting violations at
the runtime. Current real-time monitoring systems are costly and require
continuous network connectivity. The system uses optimized deep learning that
run locally on IoTs to detect violations in ridesharing and record violation
incidences. The system would enhance safety and security in ridesharing without
violating privacy.
</p></li>
</ul>

<h3>Title: Boosting Neural Networks to Decompile Optimized Binaries. (arXiv:2301.00969v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.00969">http://arxiv.org/abs/2301.00969</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.00969] Boosting Neural Networks to Decompile Optimized Binaries](http://arxiv.org/abs/2301.00969) #security</code></li>
<li>Summary: <p>Decompilation aims to transform a low-level program language (LPL) (eg.,
binary file) into its functionally-equivalent high-level program language (HPL)
(e.g., C/C++). It is a core technology in software security, especially in
vulnerability discovery and malware analysis. In recent years, with the
successful application of neural machine translation (NMT) models in natural
language processing (NLP), researchers have tried to build neural decompilers
by borrowing the idea of NMT. They formulate the decompilation process as a
translation problem between LPL and HPL, aiming to reduce the human cost
required to develop decompilation tools and improve their generalizability.
However, state-of-the-art learning-based decompilers do not cope well with
compiler-optimized binaries. Since real-world binaries are mostly
compiler-optimized, decompilers that do not consider optimized binaries have
limited practical significance. In this paper, we propose a novel
learning-based approach named NeurDP, that targets compiler-optimized binaries.
NeurDP uses a graph neural network (GNN) model to convert LPL to an
intermediate representation (IR), which bridges the gap between source code and
optimized binary. We also design an Optimized Translation Unit (OTU) to split
functions into smaller code fragments for better translation performance.
Evaluation results on datasets containing various types of statements show that
NeurDP can decompile optimized binaries with 45.21% higher accuracy than
state-of-the-art neural decompilation frameworks.
</p></li>
</ul>

<h3>Title: Joint Space-Time Sparsity Based Jamming Detection for Mission-Critical mMTC Networks. (arXiv:2301.01058v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.01058">http://arxiv.org/abs/2301.01058</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.01058] Joint Space-Time Sparsity Based Jamming Detection for Mission-Critical mMTC Networks](http://arxiv.org/abs/2301.01058) #security</code></li>
<li>Summary: <p>For mission-critical massive machine-type communications (mMTC) applications,
the messages are required to be delivered in real-time. However, due to the
weak security protection capabilities of the low-cost and low-complexity
machine-type devices, active jamming attack in the uplink access is a serious
threat. Uplink access jamming (UAJ) can increase the number of
dropped/retransmitted packets and restrict or prevent the normal device access.
To tackle this vital and challenging problem, we propose a novel UAJ detection
method based on the joint space-time sparsity (JSTS). Our key insight is that
the JSTS-based feature will be significantly impacted if UAJ happens, since
only a small fraction of the devices are active and the traffic pattern for
each device is sporadic in the normal state. Unlike the existing detection
methods under batch mode (i.e., all sample observations are collected before
making a decision), the JSTS-based detection is performed in a sequential
manner by processing the received signals one by one, which can detect UAJ as
quickly as possible. Moreover, the proposed JSTS-based method does not rely on
the prior knowledge of the attackers, since it only cares the abrupt change in
the JSTS-based feature on each frame. Numerical results evaluate and confirm
the effectiveness of our method.
</p></li>
</ul>

<h3>Title: Unlocking Metaverse-as-a-Service The three pillars to watch: Privacy and Security, Edge Computing, and Blockchain. (arXiv:2301.01221v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.01221">http://arxiv.org/abs/2301.01221</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.01221] Unlocking Metaverse-as-a-Service The three pillars to watch: Privacy and Security, Edge Computing, and Blockchain](http://arxiv.org/abs/2301.01221) #security</code></li>
<li>Summary: <p>In this article, the authors provide a comprehensive overview on three core
pillars of metaverse-as-a-service (MaaS) platforms; privacy and security, edge
computing, and blockchain technology. The article starts by investigating
security aspects for the wireless access to the metaverse. Then it goes through
the privacy and security issues inside the metaverse from data-centric,
learning-centric, and human-centric points-of-view. The authors address private
and secure mechanisms for privatizing sensitive data attributes and securing
machine learning algorithms running in a distributed manner within the
metaverse platforms. Novel visions and less-investigated methods are reviewed
to help mobile network operators and metaverse service providers facilitate the
realization of secure and private MaaS through different layers of the
metaverse, ranging from the access layer to the social interactions among
clients. Later in the article, it has been explained how the paradigm of edge
computing can strengthen different aspects of the metaverse. Along with that,
the challenges of using edge computing in the metaverse have been
comprehensively investigated. Additionally, the paper has comprehensively
investigated and analyzed 10 main challenges of MaaS platforms and thoroughly
discussed how blockchain technology provides solutions for these constraints.
At the final, future vision and directions, such as content-centric security
and zero-trust metaverse, some blockchain's unsolved challenges are also
discussed to bring further insights for the network designers in the metaverse
era.
</p></li>
</ul>

<h3>Title: MERLIN: Multi-agent offline and transfer learning for occupant-centric energy flexible operation of grid-interactive communities using smart meter data and CityLearn. (arXiv:2301.01148v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.01148">http://arxiv.org/abs/2301.01148</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.01148] MERLIN: Multi-agent offline and transfer learning for occupant-centric energy flexible operation of grid-interactive communities using smart meter data and CityLearn](http://arxiv.org/abs/2301.01148) #security</code></li>
<li>Summary: <p>The decarbonization of buildings presents new challenges for the reliability
of the electrical grid as a result of the intermittency of renewable energy
sources and increase in grid load brought about by end-use electrification. To
restore reliability, grid-interactive efficient buildings can provide
flexibility services to the grid through demand response. Residential demand
response programs are hindered by the need for manual intervention by
customers. To maximize the energy flexibility potential of residential
buildings, an advanced control architecture is needed. Reinforcement learning
is well-suited for the control of flexible resources as it is able to adapt to
unique building characteristics compared to expert systems. Yet, factors
hindering the adoption of RL in real-world applications include its large data
requirements for training, control security and generalizability. Here we
address these challenges by proposing the MERLIN framework and using a digital
twin of a real-world 17-building grid-interactive residential community in
CityLearn. We show that 1) independent RL-controllers for batteries improve
building and district level KPIs compared to a reference RBC by tailoring their
policies to individual buildings, 2) despite unique occupant behaviours,
transferring the RL policy of any one of the buildings to other buildings
provides comparable performance while reducing the cost of training, 3)
training RL-controllers on limited temporal data that does not capture full
seasonality in occupant behaviour has little effect on performance. Although,
the zero-net-energy (ZNE) condition of the buildings could be maintained or
worsened as a result of controlled batteries, KPIs that are typically improved
by ZNE condition (electricity price and carbon emissions) are further improved
when the batteries are managed by an advanced controller.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Procedural Humans for Computer Vision. (arXiv:2301.01161v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.01161">http://arxiv.org/abs/2301.01161</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.01161] Procedural Humans for Computer Vision](http://arxiv.org/abs/2301.01161) #privacy</code></li>
<li>Summary: <p>Recent work has shown the benefits of synthetic data for use in computer
vision, with applications ranging from autonomous driving to face landmark
detection and reconstruction. There are a number of benefits of using synthetic
data from privacy preservation and bias elimination to quality and feasibility
of annotation. Generating human-centered synthetic data is a particular
challenge in terms of realism and domain-gap, though recent work has shown that
effective machine learning models can be trained using synthetic face data
alone. We show that this can be extended to include the full body by building
on the pipeline of Wood et al. to generate synthetic images of humans in their
entirety, with ground-truth annotations for computer vision applications.
</p></li>
</ul>

<p>In this report we describe how we construct a parametric model of the face
and body, including articulated hands; our rendering pipeline to generate
realistic images of humans based on this body model; an approach for training
DNNs to regress a dense set of landmarks covering the entire body; and a method
for fitting our body model to dense landmarks predicted from multiple views.
</p>

<h3>Title: Generative appearance replay for continual unsupervised domain adaptation. (arXiv:2301.01211v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.01211">http://arxiv.org/abs/2301.01211</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.01211] Generative appearance replay for continual unsupervised domain adaptation](http://arxiv.org/abs/2301.01211) #privacy</code></li>
<li>Summary: <p>Deep learning models can achieve high accuracy when trained on large amounts
of labeled data. However, real-world scenarios often involve several
challenges: Training data may become available in installments, may originate
from multiple different domains, and may not contain labels for training.
Certain settings, for instance medical applications, often involve further
restrictions that prohibit retention of previously seen data due to privacy
regulations. In this work, to address such challenges, we study unsupervised
segmentation in continual learning scenarios that involve domain shift. To that
end, we introduce GarDA (Generative Appearance Replay for continual Domain
Adaptation), a generative-replay based approach that can adapt a segmentation
model sequentially to new domains with unlabeled data. In contrast to
single-step unsupervised domain adaptation (UDA), continual adaptation to a
sequence of domains enables leveraging and consolidation of information from
multiple domains. Unlike previous approaches in incremental UDA, our method
does not require access to previously seen data, making it applicable in many
practical scenarios. We evaluate GarDA on two datasets with different organs
and modalities, where it substantially outperforms existing techniques.
</p></li>
</ul>

<h3>Title: Five Common Misconceptions About Privacy-Preserving Internet of Things. (arXiv:2301.00920v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.00920">http://arxiv.org/abs/2301.00920</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.00920] Five Common Misconceptions About Privacy-Preserving Internet of Things](http://arxiv.org/abs/2301.00920) #privacy</code></li>
<li>Summary: <p>Billions of devices in the Internet of Things (IoT) collect sensitive data
about people, creating data privacy risks and breach vulnerabilities.
Accordingly, data privacy preservation is vital for sustaining the
proliferation of IoT services. In particular, privacy-preserving IoT connects
devices embedded with sensors and maintains the data privacy of people.
However, common misconceptions exist among IoT researchers, service providers,
and users about privacy-preserving IoT.
</p></li>
</ul>

<p>This article refutes five common misconceptions about privacy-preserving IoT
concerning data sensing and innovation, regulations, and privacy safeguards.
For example, IoT users have a common misconception that no data collection is
permitted in data privacy regulations. On the other hand, IoT service providers
often think data privacy impedes IoT sensing and innovation. Addressing these
misconceptions is essential for making progress in privacy-preserving IoT. This
article refutes such common misconceptions using real-world experiments and
online survey research. First, the experiments indicate that data privacy
should not be perceived as an impediment in IoT but as an opportunity to
increase customer retention and trust. Second, privacy-preserving IoT is not
exclusively a regulatory problem but also a functional necessity that must be
incorporated in the early stages of any IoT design. Third, people do not trust
services that lack sufficient privacy measures. Fourth, conventional data
security principles do not guarantee data privacy protection, and data privacy
can be exposed even if data is securely stored. Fifth, IoT decentralization
does not attain absolute privacy preservation.
</p>

<h3>Title: Recent Trends on Privacy-Preserving Technologies under Standardization at the IETF. (arXiv:2301.01124v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.01124">http://arxiv.org/abs/2301.01124</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.01124] Recent Trends on Privacy-Preserving Technologies under Standardization at the IETF](http://arxiv.org/abs/2301.01124) #privacy</code></li>
<li>Summary: <p>End-users are concerned about protecting the privacy of their personal data
generated while working on information systems. This extends to both the data
they actively provide including personal identification in exchange for
products and services as well as its related metadata such as unnecessary
access to location. This is where Internet Engineering Task Force (IETF) plays
a major role by incorporating privacy on the evolving new technologies at the
fundamental level. Thus, this paper offers an overview of the
privacy-preserving mechanisms under standardization at the IETF including
DNS-over-TLS (DoT), DNS-over-HTTP (DoH) and DNS-over-QUIC (DoQ) classified as
DNS encryption. The paper also discusses Privacy Pass Protocol and its
application in generating Private Access Tokens and Passkeys to replace
passwords for authentication at the end-user's devices. To further protect the
user privacy at the IP level, Private Relays and MASQUE are discussed. This
aims to make designers, implementers and users of the Internet aware about the
privacy-related design choices.
</p></li>
</ul>

<h3>Title: Unlearnable Clusters: Towards Label-agnostic Unlearnable Examples. (arXiv:2301.01217v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.01217">http://arxiv.org/abs/2301.01217</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.01217] Unlearnable Clusters: Towards Label-agnostic Unlearnable Examples](http://arxiv.org/abs/2301.01217) #privacy</code></li>
<li>Summary: <p>There is a growing interest in developing unlearnable examples (UEs) against
visual privacy leaks on the Internet. UEs are training samples added with
invisible but unlearnable noise, which have been found can prevent unauthorized
training of machine learning models. UEs typically are generated via a bilevel
optimization framework with a surrogate model to remove (minimize) errors from
the original samples, and then applied to protect the data against unknown
target models. However, existing UE generation methods all rely on an ideal
assumption called label-consistency, where the hackers and protectors are
assumed to hold the same label for a given sample. In this work, we propose and
promote a more practical label-agnostic setting, where the hackers may exploit
the protected data quite differently from the protectors. E.g., a m-class
unlearnable dataset held by the protector may be exploited by the hacker as a
n-class dataset. Existing UE generation methods are rendered ineffective in
this challenging setting. To tackle this challenge, we present a novel
technique called Unlearnable Clusters (UCs) to generate label-agnostic
unlearnable examples with cluster-wise perturbations. Furthermore, we propose
to leverage VisionandLanguage Pre-trained Models (VLPMs) like CLIP as the
surrogate model to improve the transferability of the crafted UCs to diverse
domains. We empirically verify the effectiveness of our proposed approach under
a variety of settings with different datasets, target models, and even
commercial platforms Microsoft Azure and Baidu PaddlePaddle.
</p></li>
</ul>

<h3>Title: On the causality-preservation capabilities of generative modelling. (arXiv:2301.01109v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.01109">http://arxiv.org/abs/2301.01109</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.01109] On the causality-preservation capabilities of generative modelling](http://arxiv.org/abs/2301.01109) #privacy</code></li>
<li>Summary: <p>Modeling lies at the core of both the financial and the insurance industry
for a wide variety of tasks. The rise and development of machine learning and
deep learning models have created many opportunities to improve our modeling
toolbox. Breakthroughs in these fields often come with the requirement of large
amounts of data. Such large datasets are often not publicly available in
finance and insurance, mainly due to privacy and ethics concerns. This lack of
data is currently one of the main hurdles in developing better models. One
possible option to alleviating this issue is generative modeling. Generative
models are capable of simulating fake but realistic-looking data, also referred
to as synthetic data, that can be shared more freely. Generative Adversarial
Networks (GANs) is such a model that increases our capacity to fit very
high-dimensional distributions of data. While research on GANs is an active
topic in fields like computer vision, they have found limited adoption within
the human sciences, like economics and insurance. Reason for this is that in
these fields, most questions are inherently about identification of causal
effects, while to this day neural networks, which are at the center of the GAN
framework, focus mostly on high-dimensional correlations. In this paper we
study the causal preservation capabilities of GANs and whether the produced
synthetic data can reliably be used to answer causal questions. This is done by
performing causal analyses on the synthetic data, produced by a GAN, with
increasingly more lenient assumptions. We consider the cross-sectional case,
the time series case and the case with a complete structural model. It is shown
that in the simple cross-sectional scenario where correlation equals causation
the GAN preserves causality, but that challenges arise for more advanced
analyses.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: DGNet: Distribution Guided Efficient Learning for Oil Spill Image Segmentation. (arXiv:2301.01202v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.01202">http://arxiv.org/abs/2301.01202</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.01202] DGNet: Distribution Guided Efficient Learning for Oil Spill Image Segmentation](http://arxiv.org/abs/2301.01202) #protect</code></li>
<li>Summary: <p>Successful implementation of oil spill segmentation in Synthetic Aperture
Radar (SAR) images is vital for marine environmental protection. In this paper,
we develop an effective segmentation framework named DGNet, which performs oil
spill segmentation by incorporating the intrinsic distribution of backscatter
values in SAR images. Specifically, our proposed segmentation network is
constructed with two deep neural modules running in an interactive manner,
where one is the inference module to achieve latent feature variable inference
from SAR images, and the other is the generative module to produce oil spill
segmentation maps by drawing the latent feature variables as inputs. Thus, to
yield accurate segmentation, we take into account the intrinsic distribution of
backscatter values in SAR images and embed it in our segmentation model. The
intrinsic distribution originates from SAR imagery, describing the physical
characteristics of oil spills. In the training process, the formulated
intrinsic distribution guides efficient learning of optimal latent feature
variable inference for oil spill segmentation. The efficient learning enables
the training of our proposed DGNet with a small amount of image data. This is
economically beneficial to oil spill segmentation where the availability of oil
spill SAR image data is limited in practice. Additionally, benefiting from
optimal latent feature variable inference, our proposed DGNet performs accurate
oil spill segmentation. We evaluate the segmentation performance of our
proposed DGNet with different metrics, and experimental evaluations demonstrate
its effective segmentations.
</p></li>
</ul>

<h2>defense</h2>
<h2>attack</h2>
<h3>Title: Look, Listen, and Attack: Backdoor Attacks Against Video Action Recognition. (arXiv:2301.00986v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.00986">http://arxiv.org/abs/2301.00986</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.00986] Look, Listen, and Attack: Backdoor Attacks Against Video Action Recognition](http://arxiv.org/abs/2301.00986) #attack</code></li>
<li>Summary: <p>Deep neural networks (DNNs) are vulnerable to a class of attacks called
"backdoor attacks", which create an association between a backdoor trigger and
a target label the attacker is interested in exploiting. A backdoored DNN
performs well on clean test images, yet persistently predicts an
attacker-defined label for any sample in the presence of the backdoor trigger.
Although backdoor attacks have been extensively studied in the image domain,
there are very few works that explore such attacks in the video domain, and
they tend to conclude that image backdoor attacks are less effective in the
video domain. In this work, we revisit the traditional backdoor threat model
and incorporate additional video-related aspects to that model. We show that
poisoned-label image backdoor attacks could be extended temporally in two ways,
statically and dynamically, leading to highly effective attacks in the video
domain. In addition, we explore natural video backdoors to highlight the
seriousness of this vulnerability in the video domain. And, for the first time,
we study multi-modal (audiovisual) backdoor attacks against video action
recognition models, where we show that attacking a single modality is enough
for achieving a high attack success rate.
</p></li>
</ul>

<h3>Title: Tracing the Origin of Adversarial Attack for Forensic Investigation and Deterrence. (arXiv:2301.01218v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.01218">http://arxiv.org/abs/2301.01218</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.01218] Tracing the Origin of Adversarial Attack for Forensic Investigation and Deterrence](http://arxiv.org/abs/2301.01218) #attack</code></li>
<li>Summary: <p>Deep neural networks are vulnerable to adversarial attacks. In this paper, we
take the role of investigators who want to trace the attack and identify the
source, that is, the particular model which the adversarial examples are
generated from. Techniques derived would aid forensic investigation of attack
incidents and serve as deterrence to potential attacks. We consider the
buyers-seller setting where a machine learning model is to be distributed to
various buyers and each buyer receives a slightly different copy with same
functionality. A malicious buyer generates adversarial examples from a
particular copy $\mathcal{M}_i$ and uses them to attack other copies. From
these adversarial examples, the investigator wants to identify the source
$\mathcal{M}_i$. To address this problem, we propose a two-stage
separate-and-trace framework. The model separation stage generates multiple
copies of a model for a same classification task. This process injects unique
characteristics into each copy so that adversarial examples generated have
distinct and traceable features. We give a parallel structure which embeds a
``tracer'' in each copy, and a noise-sensitive training loss to achieve this
goal. The tracing stage takes in adversarial examples and a few candidate
models, and identifies the likely source. Based on the unique features induced
by the noise-sensitive loss function, we could effectively trace the potential
adversarial copy by considering the output logits from each tracer. Empirical
results show that it is possible to trace the origin of the adversarial example
and the mechanism can be applied to a wide range of architectures and datasets.
</p></li>
</ul>

<h3>Title: Analysis of Label-Flip Poisoning Attack on Machine Learning Based Malware Detector. (arXiv:2301.01044v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.01044">http://arxiv.org/abs/2301.01044</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.01044] Analysis of Label-Flip Poisoning Attack on Machine Learning Based Malware Detector](http://arxiv.org/abs/2301.01044) #attack</code></li>
<li>Summary: <p>With the increase in machine learning (ML) applications in different domains,
incentives for deceiving these models have reached more than ever. As data is
the core backbone of ML algorithms, attackers shifted their interest toward
polluting the training data. Data credibility is at even higher risk with the
rise of state-of-art research topics like open design principles, federated
learning, and crowd-sourcing. Since the machine learning model depends on
different stakeholders for obtaining data, there are no reliable automated
mechanisms to verify the veracity of data from each source.
</p></li>
</ul>

<p>Malware detection is arduous due to its malicious nature with the addition of
metamorphic and polymorphic ability in the evolving samples. ML has proven to
solve the zero-day malware detection problem, which is unresolved by
traditional signature-based approaches. The poisoning of malware training data
can allow the malware files to go undetected by the ML-based malware detectors,
helping the attackers to fulfill their malicious goals. A feasibility analysis
of the data poisoning threat in the malware detection domain is still lacking.
Our work will focus on two major sections: training ML-based malware detectors
and poisoning the training data using the label-poisoning approach. We will
analyze the robustness of different machine learning models against data
poisoning with varying volumes of poisoning data.
</p>

<h3>Title: Backdoor Attacks Against Dataset Distillation. (arXiv:2301.01197v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.01197">http://arxiv.org/abs/2301.01197</a></li>
<li>Code URL: <a href="https://github.com/liuyugeng/baadd">https://github.com/liuyugeng/baadd</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2301.01197] Backdoor Attacks Against Dataset Distillation](http://arxiv.org/abs/2301.01197) #attack</code></li>
<li>Summary: <p>Dataset distillation has emerged as a prominent technique to improve data
efficiency when training machine learning models. It encapsulates the knowledge
from a large dataset into a smaller synthetic dataset. A model trained on this
smaller distilled dataset can attain comparable performance to a model trained
on the original training dataset. However, the existing dataset distillation
techniques mainly aim at achieving the best trade-off between resource usage
efficiency and model utility. The security risks stemming from them have not
been explored. This study performs the first backdoor attack against the models
trained on the data distilled by dataset distillation models in the image
domain. Concretely, we inject triggers into the synthetic data during the
distillation procedure rather than during the model training stage, where all
previous attacks are performed. We propose two types of backdoor attacks,
namely NAIVEATTACK and DOORPING. NAIVEATTACK simply adds triggers to the raw
data at the initial distillation phase, while DOORPING iteratively updates the
triggers during the entire distillation procedure. We conduct extensive
evaluations on multiple datasets, architectures, and dataset distillation
techniques. Empirical evaluation shows that NAIVEATTACK achieves decent attack
success rate (ASR) scores in some cases, while DOORPING reaches higher ASR
scores (close to 1.0) in all cases. Furthermore, we conduct a comprehensive
ablation study to analyze the factors that may affect the attack performance.
Finally, we evaluate multiple defense mechanisms against our backdoor attacks
and show that our attacks can practically circumvent these defense mechanisms.
</p></li>
</ul>

<h3>Title: ExploreADV: Towards exploratory attack for Neural Networks. (arXiv:2301.01223v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.01223">http://arxiv.org/abs/2301.01223</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.01223] ExploreADV: Towards exploratory attack for Neural Networks](http://arxiv.org/abs/2301.01223) #attack</code></li>
<li>Summary: <p>Although deep learning has made remarkable progress in processing various
types of data such as images, text and speech, they are known to be susceptible
to adversarial perturbations: perturbations specifically designed and added to
the input to make the target model produce erroneous output. Most of the
existing studies on generating adversarial perturbations attempt to perturb the
entire input indiscriminately. In this paper, we propose ExploreADV, a general
and flexible adversarial attack system that is capable of modeling regional and
imperceptible attacks, allowing users to explore various kinds of adversarial
examples as needed. We adapt and combine two existing boundary attack methods,
DeepFool and Brendel\&amp;Bethge Attack, and propose a mask-constrained adversarial
attack system, which generates minimal adversarial perturbations under the
pixel-level constraints, namely ``mask-constraints''. We study different ways
of generating such mask-constraints considering the variance and importance of
the input features, and show that our adversarial attack system offers users
good flexibility to focus on sub-regions of inputs, explore imperceptible
perturbations and understand the vulnerability of pixels/regions to adversarial
attacks. We demonstrate our system to be effective based on extensive
experiments and user study.
</p></li>
</ul>

<h3>Title: Automated Black-box Testing of Mass Assignment Vulnerabilities in RESTful APIs. (arXiv:2301.01261v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.01261">http://arxiv.org/abs/2301.01261</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.01261] Automated Black-box Testing of Mass Assignment Vulnerabilities in RESTful APIs](http://arxiv.org/abs/2301.01261) #attack</code></li>
<li>Summary: <p>Mass assignment is one of the most prominent vulnerabilities in RESTful APIs.
This vulnerability originates from a misconfiguration in common web frameworks,
such that naming convention and automatic binding can be exploited by an
attacker to craft malicious requests writing confidential resources and
(massively) overriding data, that should be read-only and/or confidential. In
this paper, we adopt a black-box testing perspective to automatically detect
mass assignment vulnerabilities in RESTful APIs. Execution scenarios are
generated purely based on the OpenAPI specification, that lists the available
operations and their message format. Clustering is used to group similar
operations and reveal read-only fields, the latter are candidate for mass
assignment. Then, interaction sequences are automatically generated by
instantiating abstract testing templates, trying to exploit the potential
vulnerabilities. Finally, test cases are run, and their execution is assessed
by a specific oracle, in order to reveal whether the vulnerability could be
successfully exploited. The proposed novel approach has been implemented and
evaluated on a set of case studies written in different programming languages.
The evaluation highlights that the approach is quite effective in detecting
seeded vulnerabilities, with a remarkably high accuracy.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Efficient Robustness Assessment via Adversarial Spatial-Temporal Focus on Videos. (arXiv:2301.00896v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.00896">http://arxiv.org/abs/2301.00896</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.00896] Efficient Robustness Assessment via Adversarial Spatial-Temporal Focus on Videos](http://arxiv.org/abs/2301.00896) #robust</code></li>
<li>Summary: <p>Adversarial robustness assessment for video recognition models has raised
concerns owing to their wide applications on safety-critical tasks. Compared
with images, videos have much high dimension, which brings huge computational
costs when generating adversarial videos. This is especially serious for the
query-based black-box attacks where gradient estimation for the threat models
is usually utilized, and high dimensions will lead to a large number of
queries. To mitigate this issue, we propose to simultaneously eliminate the
temporal and spatial redundancy within the video to achieve an effective and
efficient gradient estimation on the reduced searching space, and thus query
number could decrease. To implement this idea, we design the novel Adversarial
spatial-temporal Focus (AstFocus) attack on videos, which performs attacks on
the simultaneously focused key frames and key regions from the inter-frames and
intra-frames in the video. AstFocus attack is based on the cooperative
Multi-Agent Reinforcement Learning (MARL) framework. One agent is responsible
for selecting key frames, and another agent is responsible for selecting key
regions. These two agents are jointly trained by the common rewards received
from the black-box threat models to perform a cooperative prediction. By
continuously querying, the reduced searching space composed of key frames and
key regions is becoming precise, and the whole query number becomes less than
that on the original video. Extensive experiments on four mainstream video
recognition models and three widely used action recognition datasets
demonstrate that the proposed AstFocus attack outperforms the SOTA methods,
which is prevenient in fooling rate, query number, time, and perturbation
magnitude at the same.
</p></li>
</ul>

<h3>Title: Benchmarking the Robustness of LiDAR Semantic Segmentation Models. (arXiv:2301.00970v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.00970">http://arxiv.org/abs/2301.00970</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.00970] Benchmarking the Robustness of LiDAR Semantic Segmentation Models](http://arxiv.org/abs/2301.00970) #robust</code></li>
<li>Summary: <p>When using LiDAR semantic segmentation models for safety-critical
applications such as autonomous driving, it is essential to understand and
improve their robustness with respect to a large range of LiDAR corruptions. In
this paper, we aim to comprehensively analyze the robustness of LiDAR semantic
segmentation models under various corruptions. To rigorously evaluate the
robustness and generalizability of current approaches, we propose a new
benchmark called SemanticKITTI-C, which features 16 out-of-domain LiDAR
corruptions in three groups, namely adverse weather, measurement noise and
cross-device discrepancy. Then, we systematically investigate 11 LiDAR semantic
segmentation models, especially spanning different input representations (e.g.,
point clouds, voxels, projected images, and etc.), network architectures and
training schemes. Through this study, we obtain two insights: 1) We find out
that the input representation plays a crucial role in robustness. Specifically,
under specific corruptions, different representations perform variously. 2)
Although state-of-the-art methods on LiDAR semantic segmentation achieve
promising results on clean data, they are less robust when dealing with noisy
data. Finally, based on the above observations, we design a robust LiDAR
segmentation model (RLSeg) which greatly boosts the robustness with simple but
effective modifications. It is promising that our benchmark, comprehensive
analysis, and observations can boost future research in robust LiDAR semantic
segmentation for safety-critical applications.
</p></li>
</ul>

<h3>Title: Asymmetric Co-teaching with Multi-view Consensus for Noisy Label Learning. (arXiv:2301.01143v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.01143">http://arxiv.org/abs/2301.01143</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.01143] Asymmetric Co-teaching with Multi-view Consensus for Noisy Label Learning](http://arxiv.org/abs/2301.01143) #robust</code></li>
<li>Summary: <p>Learning with noisy-labels has become an important research topic in computer
vision where state-of-the-art (SOTA) methods explore: 1) prediction
disagreement with co-teaching strategy that updates two models when they
disagree on the prediction of training samples; and 2) sample selection to
divide the training set into clean and noisy sets based on small training loss.
However, the quick convergence of co-teaching models to select the same clean
subsets combined with relatively fast overfitting of noisy labels may induce
the wrong selection of noisy label samples as clean, leading to an inevitable
confirmation bias that damages accuracy. In this paper, we introduce our
noisy-label learning approach, called Asymmetric Co-teaching (AsyCo), which
introduces novel prediction disagreement that produces more consistent
divergent results of the co-teaching models, and a new sample selection
approach that does not require small-loss assumption to enable a better
robustness to confirmation bias than previous methods. More specifically, the
new prediction disagreement is achieved with the use of different training
strategies, where one model is trained with multi-class learning and the other
with multi-label learning. Also, the new sample selection is based on
multi-view consensus, which uses the label views from training labels and model
predictions to divide the training set into clean and noisy for training the
multi-class model and to re-label the training samples with multiple top-ranked
labels for training the multi-label model. Extensive experiments on synthetic
and real-world noisy-label datasets show that AsyCo improves over current SOTA
methods.
</p></li>
</ul>

<h3>Title: Cross Modal Transformer via Coordinates Encoding for 3D Object Dectection. (arXiv:2301.01283v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.01283">http://arxiv.org/abs/2301.01283</a></li>
<li>Code URL: <a href="https://github.com/junjie18/cmt">https://github.com/junjie18/cmt</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2301.01283] Cross Modal Transformer via Coordinates Encoding for 3D Object Dectection](http://arxiv.org/abs/2301.01283) #robust</code></li>
<li>Summary: <p>In this paper, we propose a robust 3D detector, named Cross Modal Transformer
(CMT), for end-to-end 3D multi-modal detection. Without explicit view
transformation, CMT takes the image and point clouds tokens as inputs and
directly outputs accurate 3D bounding boxes. The spatial alignment of
multi-modal tokens is performed implicitly, by encoding the 3D points into
multi-modal features. The core design of CMT is quite simple while its
performance is impressive. CMT obtains 73.0% NDS on nuScenes benchmark.
Moreover, CMT has a strong robustness even if the LiDAR is missing. Code will
be released at https://github.com/junjie18/CMT.
</p></li>
</ul>

<h3>Title: Robust Average-Reward Markov Decision Processes. (arXiv:2301.00858v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.00858">http://arxiv.org/abs/2301.00858</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.00858] Robust Average-Reward Markov Decision Processes](http://arxiv.org/abs/2301.00858) #robust</code></li>
<li>Summary: <p>In robust Markov decision processes (MDPs), the uncertainty in the transition
kernel is addressed by finding a policy that optimizes the worst-case
performance over an uncertainty set of MDPs. While much of the literature has
focused on discounted MDPs, robust average-reward MDPs remain largely
unexplored. In this paper, we focus on robust average-reward MDPs, where the
goal is to find a policy that optimizes the worst-case average reward over an
uncertainty set. We first take an approach that approximates average-reward
MDPs using discounted MDPs. We prove that the robust discounted value function
converges to the robust average-reward as the discount factor $\gamma$ goes to
$1$, and moreover, when $\gamma$ is large, any optimal policy of the robust
discounted MDP is also an optimal policy of the robust average-reward. We
further design a robust dynamic programming approach, and theoretically
characterize its convergence to the optimum. Then, we investigate robust
average-reward MDPs directly without using discounted MDPs as an intermediate
step. We derive the robust Bellman equation for robust average-reward MDPs,
prove that the optimal policy can be derived from its solution, and further
design a robust relative value iteration algorithm that provably finds its
solution, or equivalently, the optimal robust policy.
</p></li>
</ul>

<h3>Title: Temporal Difference Learning with Compressed Updates: Error-Feedback meets Reinforcement Learning. (arXiv:2301.00944v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.00944">http://arxiv.org/abs/2301.00944</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.00944] Temporal Difference Learning with Compressed Updates: Error-Feedback meets Reinforcement Learning](http://arxiv.org/abs/2301.00944) #robust</code></li>
<li>Summary: <p>In large-scale machine learning, recent works have studied the effects of
compressing gradients in stochastic optimization in order to alleviate the
communication bottleneck. These works have collectively revealed that
stochastic gradient descent (SGD) is robust to structured perturbations such as
quantization, sparsification, and delays. Perhaps surprisingly, despite the
surge of interest in large-scale, multi-agent reinforcement learning, almost
nothing is known about the analogous question: Are common reinforcement
learning (RL) algorithms also robust to similar perturbations? In this paper,
we investigate this question by studying a variant of the classical temporal
difference (TD) learning algorithm with a perturbed update direction, where a
general compression operator is used to model the perturbation. Our main
technical contribution is to show that compressed TD algorithms, coupled with
an error-feedback mechanism used widely in optimization, exhibit the same
non-asymptotic theoretical guarantees as their SGD counterparts. We then extend
our results significantly to nonlinear stochastic approximation algorithms and
multi-agent settings. In particular, we prove that for multi-agent TD learning,
one can achieve linear convergence speedups in the number of agents while
communicating just $\tilde{O}(1)$ bits per agent at each time step. Our work is
the first to provide finite-time results in RL that account for general
compression operators and error-feedback in tandem with linear function
approximation and Markovian sampling. Our analysis hinges on studying the drift
of a novel Lyapunov function that captures the dynamics of a memory variable
introduced by error feedback.
</p></li>
</ul>

<h3>Title: Through-life Monitoring of Resource-constrained Systems and Fleets. (arXiv:2301.01017v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.01017">http://arxiv.org/abs/2301.01017</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.01017] Through-life Monitoring of Resource-constrained Systems and Fleets](http://arxiv.org/abs/2301.01017) #robust</code></li>
<li>Summary: <p>A Digital Twin (DT) is a simulation of a physical system that provides
information to make decisions that add economic, social or commercial value.
The behaviour of a physical system changes over time, a DT must therefore be
continually updated with data from the physical systems to reflect its changing
behaviour. For resource-constrained systems, updating a DT is non-trivial
because of challenges such as on-board learning and the off-board data
transfer. This paper presents a framework for updating data-driven DTs of
resource-constrained systems geared towards system health monitoring. The
proposed solution consists of: (1) an on-board system running a light-weight DT
allowing the prioritisation and parsimonious transfer of data generated by the
physical system; and (2) off-board robust updating of the DT and detection of
anomalous behaviours. Two case studies are considered using a production gas
turbine engine system to demonstrate the digital representation accuracy for
real-world, time-varying physical systems.
</p></li>
</ul>

<h3>Title: Risk-Averse MDPs under Reward Ambiguity. (arXiv:2301.01045v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.01045">http://arxiv.org/abs/2301.01045</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.01045] Risk-Averse MDPs under Reward Ambiguity](http://arxiv.org/abs/2301.01045) #robust</code></li>
<li>Summary: <p>We propose a distributionally robust return-risk model for Markov decision
processes (MDPs) under risk and reward ambiguity. The proposed model optimizes
the weighted average of mean and percentile performances, and it covers the
distributionally robust MDPs and the distributionally robust chance-constrained
MDPs (both under reward ambiguity) as special cases. By considering that the
unknown reward distribution lies in a Wasserstein ambiguity set, we derive the
tractable reformulation for our model. In particular, we show that that the
return-risk model can also account for risk from uncertain transition kernel
when one only seeks deterministic policies, and that a distributionally robust
MDP under the percentile criterion can be reformulated as its nominal
counterpart at an adjusted risk level. A scalable first-order algorithm is
designed to solve large-scale problems, and we demonstrate the advantages of
our proposed model and algorithm through numerical experiments.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Uncertainty in Real-Time Semantic Segmentation on Embedded Systems. (arXiv:2301.01201v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.01201">http://arxiv.org/abs/2301.01201</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.01201] Uncertainty in Real-Time Semantic Segmentation on Embedded Systems](http://arxiv.org/abs/2301.01201) #extraction</code></li>
<li>Summary: <p>Application for semantic segmentation models in areas such as autonomous
vehicles and human computer interaction require real-time predictive
capabilities. The challenges of addressing real-time application is amplified
by the need to operate on resource constrained hardware. Whilst development of
real-time methods for these platforms has increased, these models are unable to
sufficiently reason about uncertainty present. This paper addresses this by
combining deep feature extraction from pre-trained models with Bayesian
regression and moment propagation for uncertainty aware predictions. We
demonstrate how the proposed method can yield meaningful uncertainty on
embedded hardware in real-time whilst maintaining predictive performance.
</p></li>
</ul>

<h3>Title: ClusTop: An unsupervised and integrated text clustering and topic extraction framework. (arXiv:2301.00818v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.00818">http://arxiv.org/abs/2301.00818</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.00818] ClusTop: An unsupervised and integrated text clustering and topic extraction framework](http://arxiv.org/abs/2301.00818) #extraction</code></li>
<li>Summary: <p>Text clustering and topic extraction are two important tasks in text mining.
Usually, these two tasks are performed separately. For topic extraction to
facilitate clustering, we can first project texts into a topic space and then
perform a clustering algorithm to obtain clusters. To promote topic extraction
by clustering, we can first obtain clusters with a clustering algorithm and
then extract cluster-specific topics. However, this naive strategy ignores the
fact that text clustering and topic extraction are strongly correlated and
follow a chicken-and-egg relationship. Performing them separately fails to make
them mutually benefit each other to achieve the best overall performance. In
this paper, we propose an unsupervised text clustering and topic extraction
framework (ClusTop) which integrates text clustering and topic extraction into
a unified framework and can achieve high-quality clustering result and extract
topics from each cluster simultaneously. Our framework includes four
components: enhanced language model training, dimensionality reduction,
clustering and topic extraction, where the enhanced language model can be
viewed as a bridge between clustering and topic extraction. On one hand, it
provides text embeddings with a strong cluster structure which facilitates
effective text clustering; on the other hand, it pays high attention on the
topic related words for topic extraction because of its self-attention
architecture. Moreover, the training of enhanced language model is
unsupervised. Experiments on two datasets demonstrate the effectiveness of our
framework and provide benchmarks for different model combinations in this
framework.
</p></li>
</ul>

<h3>Title: PIE-QG: Paraphrased Information Extraction for Unsupervised Question Generation from Small Corpora. (arXiv:2301.01064v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.01064">http://arxiv.org/abs/2301.01064</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.01064] PIE-QG: Paraphrased Information Extraction for Unsupervised Question Generation from Small Corpora](http://arxiv.org/abs/2301.01064) #extraction</code></li>
<li>Summary: <p>Supervised Question Answering systems (QA systems) rely on domain-specific
human-labeled data for training. Unsupervised QA systems generate their own
question-answer training pairs, typically using secondary knowledge sources to
achieve this outcome. Our approach (called PIE-QG) uses Open Information
Extraction (OpenIE) to generate synthetic training questions from paraphrased
passages and uses the question-answer pairs as training data for a language
model for a state-of-the-art QA system based on BERT. Triples in the form of
<subject, predicate, object> are extracted from each passage, and questions are
formed with subjects (or objects) and predicates while objects (or subjects)
are considered as answers. Experimenting on five extractive QA datasets
demonstrates that our technique achieves on-par performance with existing
state-of-the-art QA systems with the benefit of being trained on an order of
magnitude fewer documents and without any recourse to external reference data
sources.
</p></li>
</ul>

<h3>Title: NeuroExplainer: Fine-Grained Attention Decoding to Uncover Cortical Development Patterns of Preterm Infants. (arXiv:2301.00815v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.00815">http://arxiv.org/abs/2301.00815</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.00815] NeuroExplainer: Fine-Grained Attention Decoding to Uncover Cortical Development Patterns of Preterm Infants](http://arxiv.org/abs/2301.00815) #extraction</code></li>
<li>Summary: <p>Deploying reliable deep learning techniques in interdisciplinary applications
needs learned models to output accurate and ({even more importantly})
explainable predictions. Existing approaches typically explicate network
outputs in a post-hoc fashion, under an implicit assumption that faithful
explanations come from accurate predictions/classifications. We have an
opposite claim that explanations boost (or even determine) classification. That
is, end-to-end learning of explanation factors to augment discriminative
representation extraction could be a more intuitive strategy to inversely
assure fine-grained explainability, e.g., in those neuroimaging and
neuroscience studies with high-dimensional data containing noisy, redundant,
and task-irrelevant information. In this paper, we propose such an explainable
geometric deep network dubbed as NeuroExplainer, with applications to uncover
altered infant cortical development patterns associated with preterm birth.
Given fundamental cortical attributes as network input, our NeuroExplainer
adopts a hierarchical attention-decoding framework to learn fine-grained
attentions and respective discriminative representations to accurately
recognize preterm infants from term-born infants at term-equivalent age.
NeuroExplainer learns the hierarchical attention-decoding modules under
subject-level weak supervision coupled with targeted regularizers deduced from
domain knowledge regarding brain development. These prior-guided constraints
implicitly maximizes the explainability metrics (i.e., fidelity, sparsity, and
stability) in network training, driving the learned network to output detailed
explanations and accurate classifications. Experimental results on the public
dHCP benchmark suggest that NeuroExplainer led to quantitatively reliable
explanation results that are qualitatively consistent with representative
neuroimaging studies.
</p></li>
</ul>

<h3>Title: A Concurrent CNN-RNN Approach for Multi-Step Wind Power Forecasting. (arXiv:2301.00819v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.00819">http://arxiv.org/abs/2301.00819</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.00819] A Concurrent CNN-RNN Approach for Multi-Step Wind Power Forecasting](http://arxiv.org/abs/2301.00819) #extraction</code></li>
<li>Summary: <p>Wind power forecasting helps with the planning for the power systems by
contributing to having a higher level of certainty in decision-making. Due to
the randomness inherent to meteorological events (e.g., wind speeds), making
highly accurate long-term predictions for wind power can be extremely
difficult. One approach to remedy this challenge is to utilize weather
information from multiple points across a geographical grid to obtain a
holistic view of the wind patterns, along with temporal information from the
previous power outputs of the wind farms. Our proposed CNN-RNN architecture
combines convolutional neural networks (CNNs) and recurrent neural networks
(RNNs) to extract spatial and temporal information from multi-dimensional input
data to make day-ahead predictions. In this regard, our method incorporates an
ultra-wide learning view, combining data from multiple numerical weather
prediction models, wind farms, and geographical locations. Additionally, we
experiment with global forecasting approaches to understand the impact of
training the same model over the datasets obtained from multiple different wind
farms, and we employ a method where spatial information extracted from
convolutional layers is passed to a tree ensemble (e.g., Light Gradient
Boosting Machine (LGBM)) instead of fully connected layers. The results show
that our proposed CNN-RNN architecture outperforms other models such as LGBM,
Extra Tree regressor and linear regression when trained globally, but fails to
replicate such performance when trained individually on each farm. We also
observe that passing the spatial information from CNN to LGBM improves its
performance, providing further evidence of CNN's spatial feature extraction
capabilities.
</p></li>
</ul>

<h3>Title: Tweet's popularity dynamics. (arXiv:2301.00853v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.00853">http://arxiv.org/abs/2301.00853</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.00853] Tweet's popularity dynamics](http://arxiv.org/abs/2301.00853) #extraction</code></li>
<li>Summary: <p>This article charts the work of a 4 month project aimed at automatically
identifying patterns of tweets popularity evolution using Machine Learning and
Deep Learning techniques. To apprehend both the data and the extent of the
problem, a straightforward clustering algorithm based on a point to point
distance is used. Then, in an attempt to refine the algorithm, various analyses
especially using feature extraction techniques are conducted. Although the
algorithm eventually fails to automate such a task, this exercise raises a
complex but necessary issue touching on the impact of virality on social
networks.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Distributed Machine Learning for UAV Swarms: Computing, Sensing, and Semantics. (arXiv:2301.00912v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.00912">http://arxiv.org/abs/2301.00912</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.00912] Distributed Machine Learning for UAV Swarms: Computing, Sensing, and Semantics](http://arxiv.org/abs/2301.00912) #federate</code></li>
<li>Summary: <p>Unmanned aerial vehicle (UAV) swarms are considered as a promising technique
for next-generation communication networks due to their flexibility, mobility,
low cost, and the ability to collaboratively and autonomously provide services.
Distributed learning (DL) enables UAV swarms to intelligently provide
communication services, multi-directional remote surveillance, and target
tracking. In this survey, we first introduce several popular DL algorithms such
as federated learning (FL), multi-agent Reinforcement Learning (MARL),
distributed inference, and split learning, and present a comprehensive overview
of their applications for UAV swarms, such as trajectory design, power control,
wireless resource allocation, user assignment, perception, and satellite
communications. Then, we present several state-of-the-art applications of UAV
swarms in wireless communication systems, such us reconfigurable intelligent
surface (RIS), virtual reality (VR), semantic communications, and discuss the
problems and challenges that DL-enabled UAV swarms can solve in these
applications. Finally, we describe open problems of using DL in UAV swarms and
future research directions of DL enabled UAV swarms. In summary, this survey
provides a comprehensive survey of various DL applications for UAV swarms in
extensive scenarios.
</p></li>
</ul>

<h3>Title: Mutual Information Regularization for Vertical Federated Learning. (arXiv:2301.01142v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.01142">http://arxiv.org/abs/2301.01142</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.01142] Mutual Information Regularization for Vertical Federated Learning](http://arxiv.org/abs/2301.01142) #federate</code></li>
<li>Summary: <p>Vertical Federated Learning (VFL) is widely utilized in real-world
applications to enable collaborative learning while protecting data privacy and
safety. However, previous works show that parties without labels (passive
parties) in VFL can infer the sensitive label information owned by the party
with labels (active party) or execute backdoor attacks to VFL. Meanwhile,
active party can also infer sensitive feature information from passive party.
All these pose new privacy and security challenges to VFL systems. We propose a
new general defense method which limits the mutual information between private
raw data, including both features and labels, and intermediate outputs to
achieve a better trade-off between model utility and privacy. We term this
defense Mutual Information Regularization Defense (MID). We theoretically and
experimentally testify the effectiveness of our MID method in defending
existing attacks in VFL, including label inference attacks, backdoor attacks
and feature reconstruction attacks.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: RELIANT: Fair Knowledge Distillation for Graph Neural Networks. (arXiv:2301.01150v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.01150">http://arxiv.org/abs/2301.01150</a></li>
<li>Code URL: <a href="https://github.com/yushundong/reliant">https://github.com/yushundong/reliant</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2301.01150] RELIANT: Fair Knowledge Distillation for Graph Neural Networks](http://arxiv.org/abs/2301.01150) #fair</code></li>
<li>Summary: <p>Graph Neural Networks (GNNs) have shown satisfying performance on various
graph learning tasks. To achieve better fitting capability, most GNNs are with
a large number of parameters, which makes these GNNs computationally expensive.
Therefore, it is difficult to deploy them onto edge devices with scarce
computational resources, e.g., mobile phones and wearable smart devices.
Knowledge Distillation (KD) is a common solution to compress GNNs, where a
light-weighted model (i.e., the student model) is encouraged to mimic the
behavior of a computationally expensive GNN (i.e., the teacher GNN model).
Nevertheless, most existing GNN-based KD methods lack fairness consideration.
As a consequence, the student model usually inherits and even exaggerates the
bias from the teacher GNN. To handle such a problem, we take initial steps
towards fair knowledge distillation for GNNs. Specifically, we first formulate
a novel problem of fair knowledge distillation for GNN-based teacher-student
frameworks. Then we propose a principled framework named RELIANT to mitigate
the bias exhibited by the student model. Notably, the design of RELIANT is
decoupled from any specific teacher and student model structures, and thus can
be easily adapted to various GNN-based KD frameworks. We perform extensive
experiments on multiple real-world datasets, which corroborates that RELIANT
achieves less biased GNN knowledge distillation while maintaining high
prediction utility.
</p></li>
</ul>

<h2>interpretability</h2>
<h2>explainability</h2>
<h3>Title: Explaining Imitation Learning through Frames. (arXiv:2301.01088v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.01088">http://arxiv.org/abs/2301.01088</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.01088] Explaining Imitation Learning through Frames](http://arxiv.org/abs/2301.01088) #explainability</code></li>
<li>Summary: <p>As one of the prevalent methods to achieve automation systems, Imitation
Learning (IL) presents a promising performance in a wide range of domains.
However, despite the considerable improvement in policy performance, the
corresponding research on the explainability of IL models is still limited.
Inspired by the recent approaches in explainable artificial intelligence
methods, we proposed a model-agnostic explaining framework for IL models called
R2RISE. R2RISE aims to explain the overall policy performance with respect to
the frames in demonstrations. It iteratively retrains the black-box IL model
from the randomized masked demonstrations and uses the conventional evaluation
outcome environment returns as the coefficient to build an importance map. We
also conducted experiments to investigate three major questions concerning
frames' importance equality, the effectiveness of the importance map, and
connections between importance maps from different IL models. The result shows
that R2RISE successfully distinguishes important frames from the
demonstrations.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Speed up the inference of diffusion models via shortcut MCMC sampling. (arXiv:2301.01206v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.01206">http://arxiv.org/abs/2301.01206</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.01206] Speed up the inference of diffusion models via shortcut MCMC sampling](http://arxiv.org/abs/2301.01206) #diffusion</code></li>
<li>Summary: <p>Diffusion probabilistic models have generated high quality image synthesis
recently. However, one pain point is the notorious inference to gradually
obtain clear images with thousands of steps, which is time consuming compared
to other generative models. In this paper, we present a shortcut MCMC sampling
algorithm, which balances training and inference, while keeping the generated
data's quality. In particular, we add the global fidelity constraint with
shortcut MCMC sampling to combat the local fitting from diffusion models. We do
some initial experiments and show very promising results. Our implementation is
available at https://github.com//vividitytech/diffusion-mcmc.git.
</p></li>
</ul>

<h3>Title: Exploring Complex Dynamical Systems via Nonconvex Optimization. (arXiv:2301.00923v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.00923">http://arxiv.org/abs/2301.00923</a></li>
<li>Code URL: <a href="https://github.com/hunterelliott/dense-rdn">https://github.com/hunterelliott/dense-rdn</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2301.00923] Exploring Complex Dynamical Systems via Nonconvex Optimization](http://arxiv.org/abs/2301.00923) #diffusion</code></li>
<li>Summary: <p>Cataloging the complex behaviors of dynamical systems can be challenging,
even when they are well-described by a simple mechanistic model. If such a
system is of limited analytical tractability, brute force simulation is often
the only resort. We present an alternative, optimization-driven approach using
tools from machine learning. We apply this approach to a novel,
fully-optimizable, reaction-diffusion model which incorporates complex chemical
reaction networks (termed "Dense Reaction-Diffusion Network" or "Dense RDN").
This allows us to systematically identify new states and behaviors, including
pattern formation, dissipation-maximizing nonequilibrium states, and
replication-like dynamical structures.
</p></li>
</ul>

<h3>Title: Meta-learning generalizable dynamics from trajectories. (arXiv:2301.00957v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2301.00957">http://arxiv.org/abs/2301.00957</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2301.00957] Meta-learning generalizable dynamics from trajectories](http://arxiv.org/abs/2301.00957) #diffusion</code></li>
<li>Summary: <p>We present the interpretable meta neural ordinary differential equation
(iMODE) method to rapidly learn generalizable (i.e., not parameter-specific)
dynamics from trajectories of multiple dynamical systems that vary in their
physical parameters. The iMODE method learns meta-knowledge, the functional
variations of the force field of dynamical system instances without knowing the
physical parameters, by adopting a bi-level optimization framework: an outer
level capturing the common force field form among studied dynamical system
instances and an inner level adapting to individual system instances. A priori
physical knowledge can be conveniently embedded in the neural network
architecture as inductive bias, such as conservative force field and Euclidean
symmetry. With the learned meta-knowledge, iMODE can model an unseen system
within seconds, and inversely reveal knowledge on the physical parameters of a
system, or as a Neural Gauge to "measure" the physical parameters of an unseen
system with observed trajectories. We test the validity of the iMODE method on
bistable, double pendulum, Van der Pol, Slinky, and reaction-diffusion systems.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
