<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: PA-Boot: A Formally Verified Authentication Protocol for Multiprocessor Secure Boot. (arXiv:2209.07936v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.07936">http://arxiv.org/abs/2209.07936</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.07936] PA-Boot: A Formally Verified Authentication Protocol for Multiprocessor Secure Boot](http://arxiv.org/abs/2209.07936)</code></li>
<li>Summary: <p>Hardware supply-chain attacks are raising significant security threats to the
boot process of multiprocessor systems. This paper identifies a new, prevalent
hardware supply-chain attack surface that can bypass multiprocessor secure boot
due to the absence of processor-authentication mechanisms. To defend against
such attacks, we present PA-Boot, the first formally verified
processor-authentication protocol for secure boot in multiprocessor systems.
PA-Boot is proved functionally correct and is guaranteed to detect multiple
adversarial behaviors, e.g., processor replacements, man-in-the-middle attacks,
and tampering with certificates. The fine-grained formalization of PA-Boot and
its fully mechanized security proofs are carried out in the Isabelle/HOL
theorem prover with 306 lemmas/theorems and ~7,100 LoC. Experiments on a
proof-of-concept implementation indicate that PA-Boot can effectively identify
boot-process attacks with a considerably minor overhead and thereby improve the
security of multiprocessor systems.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Fuzzy-based forest fire prevention and detection by wireless sensor networks. (arXiv:2209.07620v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.07620">http://arxiv.org/abs/2209.07620</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.07620] Fuzzy-based forest fire prevention and detection by wireless sensor networks](http://arxiv.org/abs/2209.07620)</code></li>
<li>Summary: <p>Forest fires may cause considerable damages both in ecosystems and lives.
This proposal describes the application of Internet of Things and wireless
sensor networks jointly with multi-hop routing through a real time and dynamic
monitoring system for forest fire prevention. It is based on gathering and
analyzing information related to meteorological conditions, concentrations of
polluting gases and oxygen level around particular interesting forest areas.
Unusual measurements of these environmental variables may help to prevent
wildfire incidents and make their detection more efficient. A forest fire risk
controller based on fuzzy logic has been implemented in order to activate
environmental risk alerts through a Web service and a mobile application. For
this purpose, security mechanisms have been proposed for ensuring integrity and
confidentiality in the transmission of measured environmental information.
Lamport's signature and a block cipher algorithm are used to achieve this
objective.
</p></li>
</ul>

<h3>Title: An Overview of Cyber Security and Privacy on the Electric Vehicle Charging Infrastructure. (arXiv:2209.07842v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.07842">http://arxiv.org/abs/2209.07842</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.07842] An Overview of Cyber Security and Privacy on the Electric Vehicle Charging Infrastructure](http://arxiv.org/abs/2209.07842)</code></li>
<li>Summary: <p>Electric vehicles (EVs) are key to alleviate our dependency on fossil fuels.
The future smart grid is expected to be populated by millions of EVs equipped
with high-demand batteries. To avoid an overload of the (current) electricity
grid, expensive upgrades are required. Some of the upgrades can be averted if
users of EVs participate to energy balancing mechanisms, for example through
bidirectional EV charging. As the proliferation of consumer Internet-connected
devices increases, including EV smart charging stations, their security against
cyber-attacks and the protection of private data become a growing concern. We
need to properly adapt and develop our current technology that must tackle the
security challenges in the EV charging infrastructure, which go beyond the
traditional technical applications in the domain of energy and transport
networks. Security must balance with other desirable qualities such as
interoperability, crypto-agility and energy efficiency. Evidence suggests a gap
in the current awareness of cyber security in EV charging infrastructures. This
paper fills this gap by providing the most comprehensive to date overview of
privacy and security challenges To do so, we review communication protocols
used in its ecosystem and provide a suggestion of security tools that might be
used for future research.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Memory Consistent Unsupervised Off-the-Shelf Model Adaptation for Source-Relaxed Medical Image Segmentation. (arXiv:2209.07910v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.07910">http://arxiv.org/abs/2209.07910</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.07910] Memory Consistent Unsupervised Off-the-Shelf Model Adaptation for Source-Relaxed Medical Image Segmentation](http://arxiv.org/abs/2209.07910)</code></li>
<li>Summary: <p>Unsupervised domain adaptation (UDA) has been a vital protocol for migrating
information learned from a labeled source domain to facilitate the
implementation in an unlabeled heterogeneous target domain. Although UDA is
typically jointly trained on data from both domains, accessing the labeled
source domain data is often restricted, due to concerns over patient data
privacy or intellectual property. To sidestep this, we propose "off-the-shelf
(OS)" UDA (OSUDA), aimed at image segmentation, by adapting an OS segmentor
trained in a source domain to a target domain, in the absence of source domain
data in adaptation. Toward this goal, we aim to develop a novel batch-wise
normalization (BN) statistics adaptation framework. In particular, we gradually
adapt the domain-specific low-order BN statistics, e.g., mean and variance,
through an exponential momentum decay strategy, while explicitly enforcing the
consistency of the domain shareable high-order BN statistics, e.g., scaling and
shifting factors, via our optimization objective. We also adaptively quantify
the channel-wise transferability to gauge the importance of each channel, via
both low-order statistics divergence and a scaling factor.~Furthermore, we
incorporate unsupervised self-entropy minimization into our framework to boost
performance alongside a novel queued, memory-consistent self-training strategy
to utilize the reliable pseudo label for stable and efficient unsupervised
adaptation. We evaluated our OSUDA-based framework on both cross-modality and
cross-subtype brain tumor segmentation and cardiac MR to CT segmentation tasks.
Our experimental results showed that our memory consistent OSUDA performs
better than existing source-relaxed UDA methods and yields similar performance
to UDA methods with source data.
</p></li>
</ul>

<h3>Title: Renyi Differential Privacy of Propose-Test-Release and Applications to Private and Robust Machine Learning. (arXiv:2209.07716v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.07716">http://arxiv.org/abs/2209.07716</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.07716] Renyi Differential Privacy of Propose-Test-Release and Applications to Private and Robust Machine Learning](http://arxiv.org/abs/2209.07716)</code></li>
<li>Summary: <p>Propose-Test-Release (PTR) is a differential privacy framework that works
with local sensitivity of functions, instead of their global sensitivity. This
framework is typically used for releasing robust statistics such as median or
trimmed mean in a differentially private manner. While PTR is a common
framework introduced over a decade ago, using it in applications such as robust
SGD where we need many adaptive robust queries is challenging. This is mainly
due to the lack of Renyi Differential Privacy (RDP) analysis, an essential
ingredient underlying the moments accountant approach for differentially
private deep learning. In this work, we generalize the standard PTR and derive
the first RDP bound for it when the target function has bounded global
sensitivity. We show that our RDP bound for PTR yields tighter DP guarantees
than the directly analyzed $(\eps, \delta)$-DP. We also derive the
algorithm-specific privacy amplification bound of PTR under subsampling. We
show that our bound is much tighter than the general upper bound and close to
the lower bound. Our RDP bounds enable tighter privacy loss calculation for the
composition of many adaptive runs of PTR. As an application of our analysis, we
show that PTR and our theoretical results can be used to design differentially
private variants for byzantine robust training algorithms that use robust
statistics for gradients aggregation. We conduct experiments on the settings of
label, feature, and gradient corruption across different datasets and
architectures. We show that PTR-based private and robust training algorithm
significantly improves the utility compared with the baseline.
</p></li>
</ul>

<h3>Title: Jaco: An Offline Running Privacy-aware Voice Assistant. (arXiv:2209.07775v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.07775">http://arxiv.org/abs/2209.07775</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.07775] Jaco: An Offline Running Privacy-aware Voice Assistant](http://arxiv.org/abs/2209.07775)</code></li>
<li>Summary: <p>With the recent advance in speech technology, smart voice assistants have
been improved and are now used by many people. But often these assistants are
running online as a cloud service and are not always known for a good
protection of users' privacy. This paper presents the architecture of a novel
voice assistant, called Jaco, with the following features: (a) It can run
completely offline, even on low resource devices like a RaspberryPi. (b)
Through a skill concept it can be easily extended. (c) The architectural focus
is on protecting users' privacy, but without restricting capabilities for
developers. (d) It supports multiple languages. (e) It is competitive with
other voice assistant solutions. In this respect the assistant combines and
extends the advantages of other approaches.
</p></li>
</ul>

<h3>Title: Privacy-Preserving Distributed Expectation Maximization for Gaussian Mixture Model using Subspace Perturbation. (arXiv:2209.07833v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.07833">http://arxiv.org/abs/2209.07833</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.07833] Privacy-Preserving Distributed Expectation Maximization for Gaussian Mixture Model using Subspace Perturbation](http://arxiv.org/abs/2209.07833)</code></li>
<li>Summary: <p>Privacy has become a major concern in machine learning. In fact, the
federated learning is motivated by the privacy concern as it does not allow to
transmit the private data but only intermediate updates. However, federated
learning does not always guarantee privacy-preservation as the intermediate
updates may also reveal sensitive information. In this paper, we give an
explicit information-theoretical analysis of a federated expectation
maximization algorithm for Gaussian mixture model and prove that the
intermediate updates can cause severe privacy leakage. To address the privacy
issue, we propose a fully decentralized privacy-preserving solution, which is
able to securely compute the updates in each maximization step. Additionally,
we consider two different types of security attacks: the honest-but-curious and
eavesdropping adversary models. Numerical validation shows that the proposed
approach has superior performance compared to the existing approach in terms of
both the accuracy and privacy level.
</p></li>
</ul>

<h3>Title: 'Surprised, Shocked, Worried': User Reactions to Facebook Data Collection from Third Parties. (arXiv:2209.08048v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08048">http://arxiv.org/abs/2209.08048</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08048] 'Surprised, Shocked, Worried': User Reactions to Facebook Data Collection from Third Parties](http://arxiv.org/abs/2209.08048)</code></li>
<li>Summary: <p>Data collection and aggregation by online services happens to an extent that
is often beyond awareness and comprehension of its users. Transparency tools
become crucial to inform people, though it is unclear how well they work. To
investigate this matter, we conducted a user study focusing on Facebook, which
has recently released the "Off-Facebook Activity" transparency dashboard that
informs about personal data collection from third parties. We exposed a group
of n = 100 participants to the dashboard and surveyed their level of awareness
and reactions to understand how transparency impacts users' privacy attitudes
and intended behavior. Our participants were surprised about the massive amount
of collected data, became significantly less comfortable with data collection,
and more likely to take protective measures. Collaterally, we observed that
current consent schemes are inadequate. Based on the survey findings, we make
recommendations for more usable transparency and highlight the need to raise
awareness about transparency tools and to provide easily actionable privacy
controls.
</p></li>
</ul>

<h3>Title: Federated Coordinate Descent for Privacy-Preserving Multiparty Linear Regression. (arXiv:2209.07702v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.07702">http://arxiv.org/abs/2209.07702</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.07702] Federated Coordinate Descent for Privacy-Preserving Multiparty Linear Regression](http://arxiv.org/abs/2209.07702)</code></li>
<li>Summary: <p>Distributed privacy-preserving regression schemes have been developed and
extended in various fields, where multiparty collaboratively and privately run
optimization algorithms, e.g., Gradient Descent, to learn a set of optimal
parameters. However, traditional Gradient-Descent based methods fail to solve
problems which contains objective functions with L1 regularization, such as
Lasso regression. In this paper, we present Federated Coordinate Descent, a new
distributed scheme called FCD, to address this issue securely under multiparty
scenarios. Specifically, through secure aggregation and added perturbations,
our scheme guarantees that: (1) no local information is leaked to other
parties, and (2) global model parameters are not exposed to cloud servers. The
added perturbations can eventually be eliminated by each party to derive a
global model with high performance. We show that the FCD scheme fills the gap
of multiparty secure Coordinate Descent methods and is applicable for general
linear regressions, including linear, ridge and lasso regressions. Theoretical
security analysis and experimental results demonstrate that FCD can be
performed effectively and efficiently, and provide as low MAE measure as
centralized methods under tasks of three types of linear regressions on
real-world UCI datasets.
</p></li>
</ul>

<h3>Title: Truthful Generalized Linear Models. (arXiv:2209.07815v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.07815">http://arxiv.org/abs/2209.07815</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.07815] Truthful Generalized Linear Models](http://arxiv.org/abs/2209.07815)</code></li>
<li>Summary: <p>In this paper we study estimating Generalized Linear Models (GLMs) in the
case where the agents (individuals) are strategic or self-interested and they
concern about their privacy when reporting data. Compared with the classical
setting, here we aim to design mechanisms that can both incentivize most agents
to truthfully report their data and preserve the privacy of individuals'
reports, while their outputs should also close to the underlying parameter. In
the first part of the paper, we consider the case where the covariates are
sub-Gaussian and the responses are heavy-tailed where they only have the finite
fourth moments. First, motivated by the stationary condition of the maximizer
of the likelihood function, we derive a novel private and closed form
estimator. Based on the estimator, we propose a mechanism which has the
following properties via some appropriate design of the computation and payment
scheme for several canonical models such as linear regression, logistic
regression and Poisson regression: (1) the mechanism is $o(1)$-jointly
differentially private (with probability at least $1-o(1)$); (2) it is an
$o(\frac{1}{n})$-approximate Bayes Nash equilibrium for a $(1-o(1))$-fraction
of agents to truthfully report their data, where $n$ is the number of agents;
(3) the output could achieve an error of $o(1)$ to the underlying parameter;
(4) it is individually rational for a $(1-o(1))$ fraction of agents in the
mechanism ; (5) the payment budget required from the analyst to run the
mechanism is $o(1)$. In the second part, we consider the linear regression
model under more general setting where both covariates and responses are
heavy-tailed and only have finite fourth moments. By using an $\ell_4$-norm
shrinkage operator, we propose a private estimator and payment scheme which
have similar properties as in the sub-Gaussian case.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h2>attack</h2>
<h3>Title: A Large-scale Multiple-objective Method for Black-box Attack against Object Detection. (arXiv:2209.07790v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.07790">http://arxiv.org/abs/2209.07790</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.07790] A Large-scale Multiple-objective Method for Black-box Attack against Object Detection](http://arxiv.org/abs/2209.07790)</code></li>
<li>Summary: <p>Recent studies have shown that detectors based on deep models are vulnerable
to adversarial examples, even in the black-box scenario where the attacker
cannot access the model information. Most existing attack methods aim to
minimize the true positive rate, which often shows poor attack performance, as
another sub-optimal bounding box may be detected around the attacked bounding
box to be the new true positive one. To settle this challenge, we propose to
minimize the true positive rate and maximize the false positive rate, which can
encourage more false positive objects to block the generation of new true
positive bounding boxes. It is modeled as a multi-objective optimization (MOP)
problem, of which the generic algorithm can search the Pareto-optimal. However,
our task has more than two million decision variables, leading to low searching
efficiency. Thus, we extend the standard Genetic Algorithm with Random Subset
selection and Divide-and-Conquer, called GARSDC, which significantly improves
the efficiency. Moreover, to alleviate the sensitivity to population quality in
generic algorithms, we generate a gradient-prior initial population, utilizing
the transferability between different detectors with similar backbones.
Compared with the state-of-art attack methods, GARSDC decreases by an average
12.0 in the mAP and queries by about 1000 times in extensive experiments. Our
codes can be found at https://github.com/LiangSiyuan21/ GARSDC.
</p></li>
</ul>

<h3>Title: StyleGAN Encoder-Based Attack for Block Scrambled Face Images. (arXiv:2209.07953v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.07953">http://arxiv.org/abs/2209.07953</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.07953] StyleGAN Encoder-Based Attack for Block Scrambled Face Images](http://arxiv.org/abs/2209.07953)</code></li>
<li>Summary: <p>In this paper, we propose an attack method to block scrambled face images,
particularly Encryption-then-Compression (EtC) applied images by utilizing the
existing powerful StyleGAN encoder and decoder for the first time. Instead of
reconstructing identical images as plain ones from encrypted images, we focus
on recovering styles that can reveal identifiable information from the
encrypted images. The proposed method trains an encoder by using plain and
encrypted image pairs with a particular training strategy. While
state-of-the-art attack methods cannot recover any perceptual information from
EtC images, the proposed method discloses personally identifiable information
such as hair color, skin color, eyeglasses, gender, etc. Experiments were
carried out on the CelebA dataset, and results show that reconstructed images
have some perceptual similarities compared to plain images.
</p></li>
</ul>

<h3>Title: Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned. (arXiv:2209.07858v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.07858">http://arxiv.org/abs/2209.07858</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.07858] Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned](http://arxiv.org/abs/2209.07858)</code></li>
<li>Summary: <p>We describe our early efforts to red team language models in order to
simultaneously discover, measure, and attempt to reduce their potentially
harmful outputs. We make three main contributions. First, we investigate
scaling behaviors for red teaming across 3 model sizes (2.7B, 13B, and 52B
parameters) and 4 model types: a plain language model (LM); an LM prompted to
be helpful, honest, and harmless; an LM with rejection sampling; and a model
trained to be helpful and harmless using reinforcement learning from human
feedback (RLHF). We find that the RLHF models are increasingly difficult to red
team as they scale, and we find a flat trend with scale for the other model
types. Second, we release our dataset of 38,961 red team attacks for others to
analyze and learn from. We provide our own analysis of the data and find a
variety of harmful outputs, which range from offensive language to more subtly
harmful non-violent unethical outputs. Third, we exhaustively describe our
instructions, processes, statistical methodologies, and uncertainty about red
teaming. We hope that this transparency accelerates our ability to work
together as a community in order to develop shared norms, practices, and
technical standards for how to red team language models.
</p></li>
</ul>

<h3>Title: Model Inversion Attacks against Graph Neural Networks. (arXiv:2209.07807v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.07807">http://arxiv.org/abs/2209.07807</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.07807] Model Inversion Attacks against Graph Neural Networks](http://arxiv.org/abs/2209.07807)</code></li>
<li>Summary: <p>Many data mining tasks rely on graphs to model relational structures among
individuals (nodes). Since relational data are often sensitive, there is an
urgent need to evaluate the privacy risks in graph data. One famous privacy
attack against data analysis models is the model inversion attack, which aims
to infer sensitive data in the training dataset and leads to great privacy
concerns. Despite its success in grid-like domains, directly applying model
inversion attacks on non-grid domains such as graph leads to poor attack
performance. This is mainly due to the failure to consider the unique
properties of graphs. To bridge this gap, we conduct a systematic study on
model inversion attacks against Graph Neural Networks (GNNs), one of the
state-of-the-art graph analysis tools in this paper. Firstly, in the white-box
setting where the attacker has full access to the target GNN model, we present
GraphMI to infer the private training graph data. Specifically, in GraphMI, a
projected gradient module is proposed to tackle the discreteness of graph edges
and preserve the sparsity and smoothness of graph features; a graph
auto-encoder module is used to efficiently exploit graph topology, node
attributes, and target model parameters for edge inference; a random sampling
module can finally sample discrete edges. Furthermore, in the hard-label
black-box setting where the attacker can only query the GNN API and receive the
classification results, we propose two methods based on gradient estimation and
reinforcement learning (RL-GraphMI). Our experimental results show that such
defenses are not sufficiently effective and call for more advanced defenses
against privacy attacks.
</p></li>
</ul>

<h3>Title: Malicious Source Code Detection Using Transformer. (arXiv:2209.07957v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.07957">http://arxiv.org/abs/2209.07957</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.07957] Malicious Source Code Detection Using Transformer](http://arxiv.org/abs/2209.07957)</code></li>
<li>Summary: <p>Open source code is considered a common practice in modern software
development. However, reusing other code allows bad actors to access a wide
developers' community, hence the products that rely on it. Those attacks are
categorized as supply chain attacks. Recent years saw a growing number of
supply chain attacks that leverage open source during software development,
relaying the download and installation procedures, whether automatic or manual.
Over the years, many approaches have been invented for detecting vulnerable
packages. However, it is uncommon to detect malicious code within packages.
Those detection approaches can be broadly categorized as analyzes that use
(dynamic) and do not use (static) code execution. Here, we introduce Malicious
Source code Detection using Transformers (MSDT) algorithm. MSDT is a novel
static analysis based on a deep learning method that detects real-world code
injection cases to source code packages. In this study, we used MSDT and a
dataset with over 600,000 different functions to embed various functions and
applied a clustering algorithm to the resulting vectors, detecting the
malicious functions by detecting the outliers. We evaluated MSDT's performance
by conducting extensive experiments and demonstrated that our algorithm is
capable of detecting functions that were injected with malicious code with
precision@k values of up to 0.909.
</p></li>
</ul>

<h3>Title: Web Application Weakness Ontology Based on Vulnerability Data. (arXiv:2209.08067v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08067">http://arxiv.org/abs/2209.08067</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08067] Web Application Weakness Ontology Based on Vulnerability Data](http://arxiv.org/abs/2209.08067)</code></li>
<li>Summary: <p>Web applications are becoming more ubiquitous. All manner of physical devices
are now connected and often have a variety of web applications and
web-interfaces. This proliferation of web applications has been accompanied by
an increase in reported software vulnerabilities. The objective of this
analysis of vulnerability data is to understand the current landscape of
reported web application flaws. Along those lines, this work reviews ten years
(2011 - 2020) of vulnerability data in the National Vulnerability Database.
Based on this data, most common web application weaknesses are identified and
their profiles presented. A weakness ontology is developed to capture the
attributes of these weaknesses. These include their attack method and attack
vectors. Also described is the impact of the weaknesses to software quality
attributes. Additionally, the technologies that are susceptible to each
weakness are presented, they include programming languages, frameworks,
communication protocols, and data formats.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Explicit Tradeoffs between Adversarial and Natural Distributional Robustness. (arXiv:2209.07592v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.07592">http://arxiv.org/abs/2209.07592</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.07592] Explicit Tradeoffs between Adversarial and Natural Distributional Robustness](http://arxiv.org/abs/2209.07592)</code></li>
<li>Summary: <p>Several existing works study either adversarial or natural distributional
robustness of deep neural networks separately. In practice, however, models
need to enjoy both types of robustness to ensure reliability. In this work, we
bridge this gap and show that in fact, explicit tradeoffs exist between
adversarial and natural distributional robustness. We first consider a simple
linear regression setting on Gaussian data with disjoint sets of core and
spurious features. In this setting, through theoretical and empirical analysis,
we show that (i) adversarial training with $\ell_1$ and $\ell_2$ norms
increases the model reliance on spurious features; (ii) For $\ell_\infty$
adversarial training, spurious reliance only occurs when the scale of the
spurious features is larger than that of the core features; (iii) adversarial
training can have an unintended consequence in reducing distributional
robustness, specifically when spurious correlations are changed in the new test
domain. Next, we present extensive empirical evidence, using a test suite of
twenty adversarially trained models evaluated on five benchmark datasets
(ObjectNet, RIVAL10, Salient ImageNet-1M, ImageNet-9, Waterbirds), that
adversarially trained classifiers rely on backgrounds more than their
standardly trained counterparts, validating our theoretical results. We also
show that spurious correlations in training data (when preserved in the test
domain) can improve adversarial robustness, revealing that previous claims that
adversarial vulnerability is rooted in spurious correlations are incomplete.
</p></li>
</ul>

<h3>Title: CenterLineDet: Road Lane CenterLine Graph Detection With Vehicle-Mounted Sensors by Transformer for High-definition Map Creation. (arXiv:2209.07734v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.07734">http://arxiv.org/abs/2209.07734</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.07734] CenterLineDet: Road Lane CenterLine Graph Detection With Vehicle-Mounted Sensors by Transformer for High-definition Map Creation](http://arxiv.org/abs/2209.07734)</code></li>
<li>Summary: <p>With the rapid development of autonomous vehicles, there witnesses a booming
demand for high-definition maps (HD maps) that provide reliable and robust
prior information of static surroundings in autonomous driving scenarios. As
one of the main high-level elements in the HD map, the road lane centerline is
critical for downstream tasks, such as prediction and planning. Manually
annotating lane centerline HD maps by human annotators is labor-intensive,
expensive and inefficient, severely restricting the wide application and fast
deployment of autonomous driving systems. Previous works seldom explore the
centerline HD map mapping problem due to the complicated topology and severe
overlapping issues of road centerlines. In this paper, we propose a novel
method named CenterLineDet to create the lane centerline HD map automatically.
CenterLineDet is trained by imitation learning and can effectively detect the
graph of lane centerlines by iterations with vehicle-mounted sensors. Due to
the application of the DETR-like transformer network, CenterLineDet can handle
complicated graph topology, such as lane intersections. The proposed approach
is evaluated on a large publicly available dataset Nuscenes, and the
superiority of CenterLineDet is well demonstrated by the comparison results.
This paper is accompanied by a demo video and a supplementary document that are
available at \url{https://tonyxuqaq.github.io/projects/CenterLineDet/}.
</p></li>
</ul>

<h3>Title: Enhance the Visual Representation via Discrete Adversarial Training. (arXiv:2209.07735v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.07735">http://arxiv.org/abs/2209.07735</a></li>
<li>Code URL: <a href="https://github.com/alibaba/easyrobust">https://github.com/alibaba/easyrobust</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2209.07735] Enhance the Visual Representation via Discrete Adversarial Training](http://arxiv.org/abs/2209.07735)</code></li>
<li>Summary: <p>Adversarial Training (AT), which is commonly accepted as one of the most
effective approaches defending against adversarial examples, can largely harm
the standard performance, thus has limited usefulness on industrial-scale
production and applications. Surprisingly, this phenomenon is totally opposite
in Natural Language Processing (NLP) task, where AT can even benefit for
generalization. We notice the merit of AT in NLP tasks could derive from the
discrete and symbolic input space. For borrowing the advantage from NLP-style
AT, we propose Discrete Adversarial Training (DAT). DAT leverages VQGAN to
reform the image data to discrete text-like inputs, i.e. visual words. Then it
minimizes the maximal risk on such discrete images with symbolic adversarial
perturbations. We further give an explanation from the perspective of
distribution to demonstrate the effectiveness of DAT. As a plug-and-play
technique for enhancing the visual representation, DAT achieves significant
improvement on multiple tasks including image classification, object detection
and self-supervised learning. Especially, the model pre-trained with Masked
Auto-Encoding (MAE) and fine-tuned by our DAT without extra data can get 31.40
mCE on ImageNet-C and 32.77% top-1 accuracy on Stylized-ImageNet, building the
new state-of-the-art. The code will be available at
https://github.com/alibaba/easyrobust.
</p></li>
</ul>

<h3>Title: PointCAT: Contrastive Adversarial Training for Robust Point Cloud Recognition. (arXiv:2209.07788v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.07788">http://arxiv.org/abs/2209.07788</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.07788] PointCAT: Contrastive Adversarial Training for Robust Point Cloud Recognition](http://arxiv.org/abs/2209.07788)</code></li>
<li>Summary: <p>Notwithstanding the prominent performance achieved in various applications,
point cloud recognition models have often suffered from natural corruptions and
adversarial perturbations. In this paper, we delve into boosting the general
robustness of point cloud recognition models and propose Point-Cloud
Contrastive Adversarial Training (PointCAT). The main intuition of PointCAT is
encouraging the target recognition model to narrow the decision gap between
clean point clouds and corrupted point clouds. Specifically, we leverage a
supervised contrastive loss to facilitate the alignment and uniformity of the
hypersphere features extracted by the recognition model, and design a pair of
centralizing losses with the dynamic prototype guidance to avoid these features
deviating from their belonging category clusters. To provide the more
challenging corrupted point clouds, we adversarially train a noise generator
along with the recognition model from the scratch, instead of using
gradient-based attack as the inner loop like previous adversarial training
methods. Comprehensive experiments show that the proposed PointCAT outperforms
the baseline methods and dramatically boosts the robustness of different point
cloud recognition models, under a variety of corruptions including isotropic
point noises, the LiDAR simulated noises, random point dropping and adversarial
perturbations.
</p></li>
</ul>

<h3>Title: KaliCalib: A Framework for Basketball Court Registration. (arXiv:2209.07795v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.07795">http://arxiv.org/abs/2209.07795</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.07795] KaliCalib: A Framework for Basketball Court Registration](http://arxiv.org/abs/2209.07795)</code></li>
<li>Summary: <p>Tracking the players and the ball in team sports is key to analyse the
performance or to enhance the game watching experience with augmented reality.
When the only sources for this data are broadcast videos, sports-field
registration systems are required to estimate the homography and re-project the
ball or the players from the image space to the field space. This paper
describes a new basketball court registration framework in the context of the
MMSports 2022 camera calibration challenge. The method is based on the
estimation by an encoder-decoder network of the positions of keypoints sampled
with perspective-aware constraints. The regression of the basket positions and
heavy data augmentation techniques make the model robust to different arenas.
Ablation studies show the positive effects of our contributions on the
challenge test set. Our method divides the mean squared error by 4.7 compared
to the challenge baseline.
</p></li>
</ul>

<h3>Title: SRFeat: Learning Locally Accurate and Globally Consistent Non-Rigid Shape Correspondence. (arXiv:2209.07806v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.07806">http://arxiv.org/abs/2209.07806</a></li>
<li>Code URL: <a href="https://github.com/craigleili/srfeat">https://github.com/craigleili/srfeat</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2209.07806] SRFeat: Learning Locally Accurate and Globally Consistent Non-Rigid Shape Correspondence](http://arxiv.org/abs/2209.07806)</code></li>
<li>Summary: <p>In this work, we present a novel learning-based framework that combines the
local accuracy of contrastive learning with the global consistency of geometric
approaches, for robust non-rigid matching. We first observe that while
contrastive learning can lead to powerful point-wise features, the learned
correspondences commonly lack smoothness and consistency, owing to the purely
combinatorial nature of the standard contrastive losses. To overcome this
limitation we propose to boost contrastive feature learning with two types of
smoothness regularization that inject geometric information into correspondence
learning. With this novel combination in hand, the resulting features are both
highly discriminative across individual points, and, at the same time, lead to
robust and consistent correspondences, through simple proximity queries. Our
framework is general and is applicable to local feature learning in both the 3D
and 2D domains. We demonstrate the superiority of our approach through
extensive experiments on a wide range of challenging matching benchmarks,
including 3D non-rigid shape correspondence and 2D image keypoint matching.
</p></li>
</ul>

<h3>Title: Topological Structure Learning for Weakly-Supervised Out-of-Distribution Detection. (arXiv:2209.07837v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.07837">http://arxiv.org/abs/2209.07837</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.07837] Topological Structure Learning for Weakly-Supervised Out-of-Distribution Detection](http://arxiv.org/abs/2209.07837)</code></li>
<li>Summary: <p>Out-of-distribution (OOD) detection is the key to deploying models safely in
the open world. For OOD detection, collecting sufficient in-distribution (ID)
labeled data is usually more time-consuming and costly than unlabeled data.
When ID labeled data is limited, the previous OOD detection methods are no
longer superior due to their high dependence on the amount of ID labeled data.
Based on limited ID labeled data and sufficient unlabeled data, we define a new
setting called Weakly-Supervised Out-of-Distribution Detection (WSOOD). To
solve the new problem, we propose an effective method called Topological
Structure Learning (TSL). Firstly, TSL uses a contrastive learning method to
build the initial topological structure space for ID and OOD data. Secondly,
TSL mines effective topological connections in the initial topological space.
Finally, based on limited ID labeled data and mined topological connections,
TSL reconstructs the topological structure in a new topological space to
increase the separability of ID and OOD instances. Extensive studies on several
representative datasets show that TSL remarkably outperforms the
state-of-the-art, verifying the validity and robustness of our method in the
new setting of WSOOD.
</p></li>
</ul>

<h3>Title: A Deep Moving-camera Background Model. (arXiv:2209.07923v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.07923">http://arxiv.org/abs/2209.07923</a></li>
<li>Code URL: <a href="https://github.com/bgu-cs-vil/deepmcbm">https://github.com/bgu-cs-vil/deepmcbm</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2209.07923] A Deep Moving-camera Background Model](http://arxiv.org/abs/2209.07923)</code></li>
<li>Summary: <p>In video analysis, background models have many applications such as
background/foreground separation, change detection, anomaly detection,
tracking, and more. However, while learning such a model in a video captured by
a static camera is a fairly-solved task, in the case of a Moving-camera
Background Model (MCBM), the success has been far more modest due to
algorithmic and scalability challenges that arise due to the camera motion.
Thus, existing MCBMs are limited in their scope and their supported
camera-motion types. These hurdles also impeded the employment, in this
unsupervised task, of end-to-end solutions based on deep learning (DL).
Moreover, existing MCBMs usually model the background either on the domain of a
typically-large panoramic image or in an online fashion. Unfortunately, the
former creates several problems, including poor scalability, while the latter
prevents the recognition and leveraging of cases where the camera revisits
previously-seen parts of the scene. This paper proposes a new method, called
DeepMCBM, that eliminates all the aforementioned issues and achieves
state-of-the-art results. Concretely, first we identify the difficulties
associated with joint alignment of video frames in general and in a DL setting
in particular. Next, we propose a new strategy for joint alignment that lets us
use a spatial transformer net with neither a regularization nor any form of
specialized (and non-differentiable) initialization. Coupled with an
autoencoder conditioned on unwarped robust central moments (obtained from the
joint alignment), this yields an end-to-end regularization-free MCBM that
supports a broad range of camera motions and scales gracefully. We demonstrate
DeepMCBM's utility on a variety of videos, including ones beyond the scope of
other methods. Our code is available at https://github.com/BGU-CS-VIL/DeepMCBM .
</p></li>
</ul>

<h3>Title: Towards Bridging the Performance Gaps of Joint Energy-based Models. (arXiv:2209.07959v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.07959">http://arxiv.org/abs/2209.07959</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.07959] Towards Bridging the Performance Gaps of Joint Energy-based Models](http://arxiv.org/abs/2209.07959)</code></li>
<li>Summary: <p>Can we train a hybrid discriminative-generative model within a single
network? This question has recently been answered in the affirmative,
introducing the field of Joint Energy-based Model (JEM), which achieves high
classification accuracy and image generation quality simultaneously. Despite
recent advances, there remain two performance gaps: the accuracy gap to the
standard softmax classifier, and the generation quality gap to state-of-the-art
generative models. In this paper, we introduce a variety of training techniques
to bridge the accuracy gap and the generation quality gap of JEM. 1) We
incorporate a recently proposed sharpness-aware minimization (SAM) framework to
train JEM, which promotes the energy landscape smoothness and the
generalizability of JEM. 2) We exclude data augmentation from the maximum
likelihood estimate pipeline of JEM, and mitigate the negative impact of data
augmentation to image generation quality. Extensive experiments on multiple
datasets demonstrate that our SADA-JEM achieves state-of-the-art performances
and outperforms JEM in image classification, image generation, calibration,
out-of-distribution detection and adversarial robustness by a notable margin.
</p></li>
</ul>

<h3>Title: Less is Better: Recovering Intended-Feature Subspace to Robustify NLU Models. (arXiv:2209.07879v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.07879">http://arxiv.org/abs/2209.07879</a></li>
<li>Code URL: <a href="https://github.com/cuteythyme/risk">https://github.com/cuteythyme/risk</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2209.07879] Less is Better: Recovering Intended-Feature Subspace to Robustify NLU Models](http://arxiv.org/abs/2209.07879)</code></li>
<li>Summary: <p>Datasets with significant proportions of bias present threats for training a
trustworthy model on NLU tasks. Despite yielding great progress, current
debiasing methods impose excessive reliance on the knowledge of bias
attributes. Definition of the attributes, however, is elusive and varies across
different datasets. Furthermore, leveraging these attributes at input level to
bias mitigation may leave a gap between intrinsic properties and the underlying
decision rule. To narrow down this gap and liberate the supervision on bias, we
suggest extending bias mitigation into feature space. Therefore, a novel model,
Recovering Intended-Feature Subspace with Knowledge-Free (RISK) is developed.
Assuming that shortcut features caused by various biases are unintended for
prediction, RISK views them as redundant features. When delving into a lower
manifold to remove redundancies, RISK reveals that an extremely low-dimensional
subspace with intended features can robustly represent the highly biased
dataset. Empirical results demonstrate our model can consistently improve model
generalization to out-of-distribution set, and achieves a new state-of-the-art
performance.
</p></li>
</ul>

<h3>Title: Improving Robust Fairness via Balance Adversarial Training. (arXiv:2209.07534v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.07534">http://arxiv.org/abs/2209.07534</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.07534] Improving Robust Fairness via Balance Adversarial Training](http://arxiv.org/abs/2209.07534)</code></li>
<li>Summary: <p>Adversarial training (AT) methods are effective against adversarial attacks,
yet they introduce severe disparity of accuracy and robustness between
different classes, known as the robust fairness problem. Previously proposed
Fair Robust Learning (FRL) adaptively reweights different classes to improve
fairness. However, the performance of the better-performed classes decreases,
leading to a strong performance drop. In this paper, we observed two unfair
phenomena during adversarial training: different difficulties in generating
adversarial examples from each class (source-class fairness) and disparate
target class tendencies when generating adversarial examples (target-class
fairness). From the observations, we propose Balance Adversarial Training (BAT)
to address the robust fairness problem. Regarding source-class fairness, we
adjust the attack strength and difficulties of each class to generate samples
near the decision boundary for easier and fairer model learning; considering
target-class fairness, by introducing a uniform distribution constraint, we
encourage the adversarial example generation process for each class with a fair
tendency. Extensive experiments conducted on multiple datasets (CIFAR-10,
CIFAR-100, and ImageNette) demonstrate that our method can significantly
outperform other baselines in mitigating the robust fairness problem (+5-10\%
on the worst class accuracy)
</p></li>
</ul>

<h3>Title: M$^2$DQN: A Robust Method for Accelerating Deep Q-learning Network. (arXiv:2209.07809v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.07809">http://arxiv.org/abs/2209.07809</a></li>
<li>Code URL: <a href="https://github.com/myyura/minimax-dqn">https://github.com/myyura/minimax-dqn</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2209.07809] M$^2$DQN: A Robust Method for Accelerating Deep Q-learning Network](http://arxiv.org/abs/2209.07809)</code></li>
<li>Summary: <p>Deep Q-learning Network (DQN) is a successful way which combines
reinforcement learning with deep neural networks and leads to a widespread
application of reinforcement learning. One challenging problem when applying
DQN or other reinforcement learning algorithms to real world problem is data
collection. Therefore, how to improve data efficiency is one of the most
important problems in the research of reinforcement learning. In this paper, we
propose a framework which uses the Max-Mean loss in Deep Q-Network (M$^2$DQN).
Instead of sampling one batch of experiences in the training step, we sample
several batches from the experience replay and update the parameters such that
the maximum TD-error of these batches is minimized. The proposed method can be
combined with most of existing techniques of DQN algorithm by replacing the
loss function. We verify the effectiveness of this framework with one of the
most widely used techniques, Double DQN (DDQN), in several gym games. The
results show that our method leads to a substantial improvement in both the
learning speed and performance.
</p></li>
</ul>

<h3>Title: Human-level Atari 200x faster. (arXiv:2209.07550v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.07550">http://arxiv.org/abs/2209.07550</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.07550] Human-level Atari 200x faster](http://arxiv.org/abs/2209.07550)</code></li>
<li>Summary: <p>The task of building general agents that perform well over a wide range of
tasks has been an important goal in reinforcement learning since its inception.
The problem has been subject of research of a large body of work, with
performance frequently measured by observing scores over the wide range of
environments contained in the Atari 57 benchmark. Agent57 was the first agent
to surpass the human benchmark on all 57 games, but this came at the cost of
poor data-efficiency, requiring nearly 80 billion frames of experience to
achieve. Taking Agent57 as a starting point, we employ a diverse set of
strategies to achieve a 200-fold reduction of experience needed to out perform
the human baseline. We investigate a range of instabilities and bottlenecks we
encountered while reducing the data regime, and propose effective solutions to
build a more robust and efficient agent. We also demonstrate competitive
performance with high-performing methods such as Muesli and MuZero. The four
key components to our approach are (1) an approximate trust region method which
enables stable bootstrapping from the online network, (2) a normalisation
scheme for the loss and priorities which improves robustness when learning a
set of value functions with a wide range of scales, (3) an improved
architecture employing techniques from NFNets in order to leverage deeper
networks without the need for normalization layers, and (4) a policy
distillation method which serves to smooth out the instantaneous greedy policy
overtime.
</p></li>
</ul>

<h3>Title: Adversarial Cross-View Disentangled Graph Contrastive Learning. (arXiv:2209.07699v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.07699">http://arxiv.org/abs/2209.07699</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.07699] Adversarial Cross-View Disentangled Graph Contrastive Learning](http://arxiv.org/abs/2209.07699)</code></li>
<li>Summary: <p>Graph contrastive learning (GCL) is prevalent to tackle the supervision
shortage issue in graph learning tasks. Many recent GCL methods have been
proposed with various manually designed augmentation techniques, aiming to
implement challenging augmentations on the original graph to yield robust
representation. Although many of them achieve remarkable performances, existing
GCL methods still struggle to improve model robustness without risking losing
task-relevant information because they ignore the fact the augmentation-induced
latent factors could be highly entangled with the original graph, thus it is
more difficult to discriminate the task-relevant information from irrelevant
information. Consequently, the learned representation is either brittle or
unilluminating. In light of this, we introduce the Adversarial Cross-View
Disentangled Graph Contrastive Learning (ACDGCL), which follows the information
bottleneck principle to learn minimal yet sufficient representations from graph
data. To be specific, our proposed model elicits the augmentation-invariant and
augmentation-dependent factors separately. Except for the conventional
contrastive loss which guarantees the consistency and sufficiency of the
representations across different contrastive views, we introduce a cross-view
reconstruction mechanism to pursue the representation disentanglement. Besides,
an adversarial view is added as the third view of contrastive loss to enhance
model robustness. We empirically demonstrate that our proposed model
outperforms the state-of-the-arts on graph classification task over multiple
benchmark datasets.
</p></li>
</ul>

<h3>Title: Sales Channel Optimization via Simulations Based on Observational Data with Delayed Rewards: A Case Study at LinkedIn. (arXiv:2209.07749v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.07749">http://arxiv.org/abs/2209.07749</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.07749] Sales Channel Optimization via Simulations Based on Observational Data with Delayed Rewards: A Case Study at LinkedIn](http://arxiv.org/abs/2209.07749)</code></li>
<li>Summary: <p>Training models on data obtained from randomized experiments is ideal for
making good decisions. However, randomized experiments are often
time-consuming, expensive, risky, infeasible or unethical to perform, leaving
decision makers little choice but to rely on observational data collected under
historical policies when training models. This opens questions regarding not
only which decision-making policies would perform best in practice, but also
regarding the impact of different data collection protocols on the performance
of various policies trained on the data, or the robustness of policy
performance with respect to changes in problem characteristics such as action-
or reward- specific delays in observing outcomes. We aim to answer such
questions for the problem of optimizing sales channel allocations at LinkedIn,
where sales accounts (leads) need to be allocated to one of three channels,
with the goal of maximizing the number of successful conversions over a period
of time. A key problem feature constitutes the presence of stochastic delays in
observing allocation outcomes, whose distribution is both channel- and outcome-
dependent. We built a discrete-time simulation that can handle our problem
features and used it to evaluate: a) a historical rule-based policy; b) a
supervised machine learning policy (XGBoost); and c) multi-armed bandit (MAB)
policies, under different scenarios involving: i) data collection used for
training (observational vs randomized); ii) lead conversion scenarios; iii)
delay distributions. Our simulation results indicate that LinUCB, a simple MAB
policy, consistently outperforms the other policies, achieving a 18-47% lift
relative to a rule-based policy
</p></li>
</ul>

<h3>Title: On the Robustness of Graph Neural Diffusion to Topology Perturbations. (arXiv:2209.07754v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.07754">http://arxiv.org/abs/2209.07754</a></li>
<li>Code URL: <a href="https://github.com/zknus/robustness-of-graph-neural-diffusion">https://github.com/zknus/robustness-of-graph-neural-diffusion</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2209.07754] On the Robustness of Graph Neural Diffusion to Topology Perturbations](http://arxiv.org/abs/2209.07754)</code></li>
<li>Summary: <p>Neural diffusion on graphs is a novel class of graph neural networks that has
attracted increasing attention recently. The capability of graph neural partial
differential equations (PDEs) in addressing common hurdles of graph neural
networks (GNNs), such as the problems of over-smoothing and bottlenecks, has
been investigated but not their robustness to adversarial attacks. In this
work, we explore the robustness properties of graph neural PDEs. We empirically
demonstrate that graph neural PDEs are intrinsically more robust against
topology perturbation as compared to other GNNs. We provide insights into this
phenomenon by exploiting the stability of the heat semigroup under graph
topology perturbations. We discuss various graph diffusion operators and relate
them to existing graph neural PDEs. Furthermore, we propose a general graph
neural PDE framework based on which a new class of robust GNNs can be defined.
We verify that the new model achieves comparable state-of-the-art performance
on several benchmark datasets.
</p></li>
</ul>

<h3>Title: Trustworthy Reinforcement Learning Against Intrinsic Vulnerabilities: Robustness, Safety, and Generalizability. (arXiv:2209.08025v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08025">http://arxiv.org/abs/2209.08025</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08025] Trustworthy Reinforcement Learning Against Intrinsic Vulnerabilities: Robustness, Safety, and Generalizability](http://arxiv.org/abs/2209.08025)</code></li>
<li>Summary: <p>A trustworthy reinforcement learning algorithm should be competent in solving
challenging real-world problems, including {robustly} handling uncertainties,
satisfying {safety} constraints to avoid catastrophic failures, and
{generalizing} to unseen scenarios during deployments. This study aims to
overview these main perspectives of trustworthy reinforcement learning
considering its intrinsic vulnerabilities on robustness, safety, and
generalizability. In particular, we give rigorous formulations, categorize
corresponding methodologies, and discuss benchmarks for each perspective.
Moreover, we provide an outlook section to spur promising future directions
with a brief discussion on extrinsic vulnerabilities considering human
feedback. We hope this survey could bring together separate threads of studies
together in a unified framework and promote the trustworthiness of
reinforcement learning.
</p></li>
</ul>

<h3>Title: A Systematic Evaluation of Node Embedding Robustness. (arXiv:2209.08064v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08064">http://arxiv.org/abs/2209.08064</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08064] A Systematic Evaluation of Node Embedding Robustness](http://arxiv.org/abs/2209.08064)</code></li>
<li>Summary: <p>Node embedding methods map network nodes to low dimensional vectors that can
be subsequently used in a variety of downstream prediction tasks. The
popularity of these methods has significantly increased in recent years, yet,
their robustness to perturbations of the input data is still poorly understood.
In this paper, we assess the empirical robustness of node embedding models to
random and adversarial poisoning attacks. Our systematic evaluation covers
representative embedding methods based on Skip-Gram, matrix factorization, and
deep neural networks. We compare edge addition, deletion and rewiring
strategies computed using network properties as well as node labels. We also
investigate the effect of label homophily and heterophily on robustness. We
report qualitative results via embedding visualization and quantitative results
in terms of downstream node classification and network reconstruction
performances. We found that node classification suffers from higher performance
degradation as opposed to network reconstruction, and that degree-based and
label-based attacks are on average the most damaging.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Weakly Supervised Semantic Segmentation via Progressive Patch Learning. (arXiv:2209.07828v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.07828">http://arxiv.org/abs/2209.07828</a></li>
<li>Code URL: <a href="https://github.com/tyroneli/ppl_wsss">https://github.com/tyroneli/ppl_wsss</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2209.07828] Weakly Supervised Semantic Segmentation via Progressive Patch Learning](http://arxiv.org/abs/2209.07828)</code></li>
<li>Summary: <p>Most of the existing semantic segmentation approaches with image-level class
labels as supervision, highly rely on the initial class activation map (CAM)
generated from the standard classification network. In this paper, a novel
"Progressive Patch Learning" approach is proposed to improve the local details
extraction of the classification, producing the CAM better covering the whole
object rather than only the most discriminative regions as in CAMs obtained in
conventional classification models. "Patch Learning" destructs the feature maps
into patches and independently processes each local patch in parallel before
the final aggregation. Such a mechanism enforces the network to find weak
information from the scattered discriminative local parts, achieving enhanced
local details sensitivity. "Progressive Patch Learning" further extends the
feature destruction and patch learning to multi-level granularities in a
progressive manner. Cooperating with a multi-stage optimization strategy, such
a "Progressive Patch Learning" mechanism implicitly provides the model with the
feature extraction ability across different locality-granularities. As an
alternative to the implicit multi-granularity progressive fusion approach, we
additionally propose an explicit method to simultaneously fuse features from
different granularities in a single model, further enhancing the CAM quality on
the full object coverage. Our proposed method achieves outstanding performance
on the PASCAL VOC 2012 dataset e.g., with 69.6$% mIoU on the test set), which
surpasses most existing weakly supervised semantic segmentation methods. Code
will be made publicly available here https://github.com/TyroneLi/PPL_WSSS.
</p></li>
</ul>

<h3>Title: Improving Language Model Prompting in Support of Semi-autonomous Task Learning. (arXiv:2209.07636v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.07636">http://arxiv.org/abs/2209.07636</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.07636] Improving Language Model Prompting in Support of Semi-autonomous Task Learning](http://arxiv.org/abs/2209.07636)</code></li>
<li>Summary: <p>Language models (LLMs) offer potential as a source of knowledge for agents
that need to acquire new task competencies within a performance environment. We
describe efforts toward a novel agent capability that can construct cues (or
"prompts") that result in useful LLM responses for an agent learning a new
task. Importantly, responses must not only be "reasonable" (a measure used
commonly in research on knowledge extraction from LLMs) but also specific to
the agent's task context and in a form that the agent can interpret given its
native language capacities. We summarize a series of empirical investigations
of prompting strategies and evaluate responses against the goals of targeted
and actionable responses for task learning. Our results demonstrate that
actionable task knowledge can be obtained from LLMs in support of online agent
task learning.
</p></li>
</ul>

<h3>Title: ConFiguRe: Exploring Discourse-level Chinese Figures of Speech. (arXiv:2209.07678v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.07678">http://arxiv.org/abs/2209.07678</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.07678] ConFiguRe: Exploring Discourse-level Chinese Figures of Speech](http://arxiv.org/abs/2209.07678)</code></li>
<li>Summary: <p>Figures of speech, such as metaphor and irony, are ubiquitous in literature
works and colloquial conversations. This poses great challenge for natural
language understanding since figures of speech usually deviate from their
ostensible meanings to express deeper semantic implications. Previous research
lays emphasis on the literary aspect of figures and seldom provide a
comprehensive exploration from a view of computational linguistics. In this
paper, we first propose the concept of figurative unit, which is the carrier of
a figure. Then we select 12 types of figures commonly used in Chinese, and
build a Chinese corpus for Contextualized Figure Recognition (ConFiguRe).
Different from previous token-level or sentence-level counterparts, ConFiguRe
aims at extracting a figurative unit from discourse-level context, and
classifying the figurative unit into the right figure type. On ConFiguRe, three
tasks, i.e., figure extraction, figure type classification and figure
recognition, are designed and the state-of-the-art techniques are utilized to
implement the benchmarks. We conduct thorough experiments and show that all
three tasks are challenging for existing models, thus requiring further
research. Our dataset and code are publicly available at
https://github.com/pku-tangent/ConFiguRe.
</p></li>
</ul>

<h3>Title: A Multi-turn Machine Reading Comprehension Framework with Rethink Mechanism for Emotion-Cause Pair Extraction. (arXiv:2209.07972v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.07972">http://arxiv.org/abs/2209.07972</a></li>
<li>Code URL: <a href="https://github.com/zhoucz97/ecpe-mm-r">https://github.com/zhoucz97/ecpe-mm-r</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2209.07972] A Multi-turn Machine Reading Comprehension Framework with Rethink Mechanism for Emotion-Cause Pair Extraction](http://arxiv.org/abs/2209.07972)</code></li>
<li>Summary: <p>Emotion-cause pair extraction (ECPE) is an emerging task in emotion cause
analysis, which extracts potential emotion-cause pairs from an emotional
document. Most recent studies use end-to-end methods to tackle the ECPE task.
However, these methods either suffer from a label sparsity problem or fail to
model complicated relations between emotions and causes. Furthermore, they all
do not consider explicit semantic information of clauses. To this end, we
transform the ECPE task into a document-level machine reading comprehension
(MRC) task and propose a Multi-turn MRC framework with Rethink mechanism
(MM-R). Our framework can model complicated relations between emotions and
causes while avoiding generating the pairing matrix (the leading cause of the
label sparsity problem). Besides, the multi-turn structure can fuse explicit
semantic information flow between emotions and causes. Extensive experiments on
the benchmark emotion cause corpus demonstrate the effectiveness of our
proposed framework, which outperforms existing state-of-the-art methods.
</p></li>
</ul>

<h3>Title: Transformer-based Detection of Multiword Expressions in Flower and Plant Names. (arXiv:2209.08016v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08016">http://arxiv.org/abs/2209.08016</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08016] Transformer-based Detection of Multiword Expressions in Flower and Plant Names](http://arxiv.org/abs/2209.08016)</code></li>
<li>Summary: <p>Multiword expression (MWE) is a sequence of words which collectively present
a meaning which is not derived from its individual words. The task of
processing MWEs is crucial in many natural language processing (NLP)
applications, including machine translation and terminology extraction.
Therefore, detecting MWEs in different domains is an important research topic.
In this paper, we explore state-of-the-art neural transformers in the task of
detecting MWEs in flower and plant names. We evaluate different transformer
models on a dataset created from Encyclopedia of Plants and Flower. We
empirically show that transformer models outperform the previous neural models
based on long short-term memory (LSTM).
</p></li>
</ul>

<h3>Title: Skill Extraction from Job Postings using Weak Supervision. (arXiv:2209.08071v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08071">http://arxiv.org/abs/2209.08071</a></li>
<li>Code URL: <a href="https://github.com/jjzha/skill-extraction-weak-supervision">https://github.com/jjzha/skill-extraction-weak-supervision</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08071] Skill Extraction from Job Postings using Weak Supervision](http://arxiv.org/abs/2209.08071)</code></li>
<li>Summary: <p>Aggregated data obtained from job postings provide powerful insights into
labor market demands, and emerging skills, and aid job matching. However, most
extraction approaches are supervised and thus need costly and time-consuming
annotation. To overcome this, we propose Skill Extraction with Weak
Supervision. We leverage the European Skills, Competences, Qualifications and
Occupations taxonomy to find similar skills in job ads via latent
representations. The method shows a strong positive signal, outperforming
baselines based on token-level and syntactic patterns.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h2>fair</h2>
<h3>Title: FairGBM: Gradient Boosting with Fairness Constraints. (arXiv:2209.07850v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.07850">http://arxiv.org/abs/2209.07850</a></li>
<li>Code URL: <a href="https://github.com/feedzai/fairgbm">https://github.com/feedzai/fairgbm</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2209.07850] FairGBM: Gradient Boosting with Fairness Constraints](http://arxiv.org/abs/2209.07850)</code></li>
<li>Summary: <p>Machine Learning (ML) algorithms based on gradient boosted decision trees
(GBDT) are still favored on many tabular data tasks across various mission
critical applications, from healthcare to finance. However, GBDT algorithms are
not free of the risk of bias and discriminatory decision-making. Despite GBDT's
popularity and the rapid pace of research in fair ML, existing in-processing
fair ML methods are either inapplicable to GBDT, incur in significant train
time overhead, or are inadequate for problems with high class imbalance. We
present FairGBM, a learning framework for training GBDT under fairness
constraints with little to no impact on predictive performance when compared to
unconstrained LightGBM. Since common fairness metrics are non-differentiable,
we employ a ``proxy-Lagrangian'' formulation using smooth convex error rate
proxies to enable gradient-based optimization. Additionally, our open-source
implementation shows an order of magnitude speedup in training time when
compared with related work, a pivotal aspect to foster the widespread adoption
of FairGBM by real-world practitioners.
</p></li>
</ul>

<h3>Title: A benchmark study on methods to ensure fair algorithmic decisions for credit scoring. (arXiv:2209.07912v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.07912">http://arxiv.org/abs/2209.07912</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.07912] A benchmark study on methods to ensure fair algorithmic decisions for credit scoring](http://arxiv.org/abs/2209.07912)</code></li>
<li>Summary: <p>The utility of machine learning in evaluating the creditworthiness of loan
applicants has been proofed since decades ago. However, automatic decisions may
lead to different treatments over groups or individuals, potentially causing
discrimination. This paper benchmarks 12 top bias mitigation methods discussing
their performance based on 5 different fairness metrics, accuracy achieved and
potential profits for the financial institutions. Our findings show the
difficulties in achieving fairness while preserving accuracy and profits.
Additionally, it highlights some of the best and worst performers and helps
bridging the gap between experimental machine learning and its industrial
application.
</p></li>
</ul>

<h3>Title: ImDrug: A Benchmark for Deep Imbalanced Learning in AI-aided Drug Discovery. (arXiv:2209.07921v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.07921">http://arxiv.org/abs/2209.07921</a></li>
<li>Code URL: <a href="https://github.com/DrugLT/ImDrug">https://github.com/DrugLT/ImDrug</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2209.07921] ImDrug: A Benchmark for Deep Imbalanced Learning in AI-aided Drug Discovery](http://arxiv.org/abs/2209.07921)</code></li>
<li>Summary: <p>The last decade has witnessed a prosperous development of computational
methods and dataset curation for AI-aided drug discovery (AIDD). However,
real-world pharmaceutical datasets often exhibit highly imbalanced
distribution, which is largely overlooked by the current literature but may
severely compromise the fairness and generalization of machine learning
applications. Motivated by this observation, we introduce ImDrug, a
comprehensive benchmark with an open-source Python library which consists of 4
imbalance settings, 11 AI-ready datasets, 54 learning tasks and 16 baseline
algorithms tailored for imbalanced learning. It provides an accessible and
customizable testbed for problems and solutions spanning a broad spectrum of
the drug discovery pipeline such as molecular modeling, drug-target interaction
and retrosynthesis. We conduct extensive empirical studies with novel
evaluation metrics, to demonstrate that the existing algorithms fall short of
solving medicinal and pharmaceutical challenges in the data imbalance scenario.
We believe that ImDrug opens up avenues for future research and development, on
real-world challenges at the intersection of AIDD and deep imbalanced learning.
</p></li>
</ul>

<h3>Title: A Comprehensive Benchmark for COVID-19 Predictive Modeling Using Electronic Health Records in Intensive Care: Choosing the Best Model for COVID-19 Prognosis. (arXiv:2209.07805v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.07805">http://arxiv.org/abs/2209.07805</a></li>
<li>Code URL: <a href="https://github.com/yhzhu99/covid-ehr-benchmarks">https://github.com/yhzhu99/covid-ehr-benchmarks</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2209.07805] A Comprehensive Benchmark for COVID-19 Predictive Modeling Using Electronic Health Records in Intensive Care: Choosing the Best Model for COVID-19 Prognosis](http://arxiv.org/abs/2209.07805)</code></li>
<li>Summary: <p>The COVID-19 pandemic has posed a heavy burden to the healthcare system
worldwide and caused huge social disruption and economic loss. Many deep
learning models have been proposed to conduct clinical predictive tasks such as
mortality prediction for COVID-19 patients in intensive care units using
Electronic Health Record (EHR) data. Despite their initial success in certain
clinical applications, there is currently a lack of benchmarking results to
achieve a fair comparison so that we can select the optimal model for clinical
use. Furthermore, there is a discrepancy between the formulation of traditional
prediction tasks and real-world clinical practice in intensive care. To fill
these gaps, we propose two clinical prediction tasks, Outcome-specific
length-of-stay prediction and Early mortality prediction for COVID-19 patients
in intensive care units. The two tasks are adapted from the naive
length-of-stay and mortality prediction tasks to accommodate the clinical
practice for COVID-19 patients. We propose fair, detailed, open-source
data-preprocessing pipelines and evaluate 17 state-of-the-art predictive models
on two tasks, including 5 machine learning models, 6 basic deep learning models
and 6 deep learning predictive models specifically designed for EHR data. We
provide benchmarking results using data from two real-world COVID-19 EHR
datasets. Both datasets are publicly available without needing any inquiry and
one dataset can be accessed on request. We provide fair, reproducible
benchmarking results for two tasks. We deploy all experiment results and models
on an online platform. We also allow clinicians and researchers to upload their
data to the platform and get quick prediction results using our trained models.
We hope our efforts can further facilitate deep learning and machine learning
research for COVID-19 predictive modeling.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: DBT-DMAE: An Effective Multivariate Time Series Pre-Train Model under Missing Data. (arXiv:2209.07798v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.07798">http://arxiv.org/abs/2209.07798</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.07798] DBT-DMAE: An Effective Multivariate Time Series Pre-Train Model under Missing Data](http://arxiv.org/abs/2209.07798)</code></li>
<li>Summary: <p>Multivariate time series(MTS) is a universal data type related to many
practical applications. However, MTS suffers from missing data problems, which
leads to degradation or even collapse of the downstream tasks, such as
prediction and classification. The concurrent missing data handling procedures
could inevitably arouse the biased estimation and redundancy-training problem
when encountering multiple downstream tasks. This paper presents a universally
applicable MTS pre-train model, DBT-DMAE, to conquer the abovementioned
obstacle. First, a missing representation module is designed by introducing
dynamic positional embedding and random masking processing to characterize the
missing symptom. Second, we proposed an auto-encoder structure to obtain the
generalized MTS encoded representation utilizing an ameliorated TCN structure
called dynamic-bidirectional-TCN as the basic unit, which integrates the
dynamic kernel and time-fliping trick to draw temporal features effectively.
Finally, the overall feed-in and loss strategy is established to ensure the
adequate training of the whole model. Comparative experiment results manifest
that the DBT-DMAE outperforms the other state-of-the-art methods in six
real-world datasets and two different downstream tasks. Moreover, ablation and
interpretability experiments are delivered to verify the validity of DBT-DMAE's
substructures.
</p></li>
</ul>

<h3>Title: PTab: Using the Pre-trained Language Model for Modeling Tabular Data. (arXiv:2209.08060v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2209.08060">http://arxiv.org/abs/2209.08060</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2209.08060] PTab: Using the Pre-trained Language Model for Modeling Tabular Data](http://arxiv.org/abs/2209.08060)</code></li>
<li>Summary: <p>Tabular data is the foundation of the information age and has been
extensively studied. Recent studies show that neural-based models are effective
in learning contextual representation for tabular data. The learning of an
effective contextual representation requires meaningful features and a large
amount of data. However, current methods often fail to properly learn a
contextual representation from the features without semantic information. In
addition, it's intractable to enlarge the training set through mixed tabular
datasets due to the difference between datasets. To address these problems, we
propose a novel framework PTab, using the Pre-trained language model to model
Tabular data. PTab learns a contextual representation of tabular data through a
three-stage processing: Modality Transformation(MT), Masked-Language
Fine-tuning(MF), and Classification Fine-tuning(CF). We initialize our model
with a pre-trained Model (PTM) which contains semantic information learned from
the large-scale language data. Consequently, contextual representation can be
learned effectively during the fine-tuning stages. In addition, we can
naturally mix the textualized tabular data to enlarge the training set to
further improve representation learning. We evaluate PTab on eight popular
tabular classification datasets. Experimental results show that our method has
achieved a better average AUC score in supervised settings compared to the
state-of-the-art baselines(e.g. XGBoost), and outperforms counterpart methods
under semi-supervised settings. We present visualization results that show PTab
has well instance-based interpretability.
</p></li>
</ul>

<h2>exlainability</h2>
<h2>watermark</h2>
<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
