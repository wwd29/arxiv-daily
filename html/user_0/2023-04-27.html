<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Blockchain-based Access Control for Secure Smart Industry Management Systems. (arXiv:2304.13379v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13379">http://arxiv.org/abs/2304.13379</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13379] Blockchain-based Access Control for Secure Smart Industry Management Systems](http://arxiv.org/abs/2304.13379) #secure</code></li>
<li>Summary: <p>Smart manufacturing systems involve a large number of interconnected devices
resulting in massive data generation. Cloud computing technology has recently
gained increasing attention in smart manufacturing systems for facilitating
cost-effective service provisioning and massive data management. In a
cloud-based manufacturing system, ensuring authorized access to the data is
crucial. A cloud platform is operated under a single authority. Hence, a cloud
platform is prone to a single point of failure and vulnerable to adversaries.
An internal or external adversary can easily modify users' access to allow
unauthorized users to access the data. This paper proposes a role-based access
control to prevent modification attacks by leveraging blockchain and smart
contracts in a cloud-based smart manufacturing system. The role-based access
control is developed to determine users' roles and rights in smart contracts.
The smart contracts are then deployed to the private blockchain network. We
evaluate our solution by utilizing Ethereum private blockchain network to
deploy the smart contract. The experimental results demonstrate the feasibility
and evaluation of the proposed framework's performance.
</p></li>
</ul>

<h3>Title: Secure Communication Model For Quantum Federated Learning: A Post Quantum Cryptography (PQC) Framework. (arXiv:2304.13413v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13413">http://arxiv.org/abs/2304.13413</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13413] Secure Communication Model For Quantum Federated Learning: A Post Quantum Cryptography (PQC) Framework](http://arxiv.org/abs/2304.13413) #secure</code></li>
<li>Summary: <p>We design a model of Post Quantum Cryptography (PQC) Quantum Federated
Learning (QFL). We develop a framework with a dynamic server selection and
study convergence and security conditions. The implementation and results are
publicly available1.
</p></li>
</ul>

<h3>Title: A Secure Medical Record Sharing Scheme Based on Blockchain and Two-fold Encryption. (arXiv:2304.13511v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13511">http://arxiv.org/abs/2304.13511</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13511] A Secure Medical Record Sharing Scheme Based on Blockchain and Two-fold Encryption](http://arxiv.org/abs/2304.13511) #secure</code></li>
<li>Summary: <p>Usually, a medical record (MR) contains the patients disease-oriented
sensitive information. In addition, the MR needs to be shared among different
bodies, e.g., diagnostic centres, hospitals, physicians, etc. Hence, retaining
the privacy and integrity of MR is crucial. A blockchain based secure MR
sharing system can manage these aspects properly. This paper proposes a
blockchain based electronic (e-) MR sharing scheme that (i) considers the
medical image and the text as the input, (ii) enriches the data privacy through
a two-fold encryption mechanism consisting of an asymmetric cryptosystem and
the dynamic DNA encoding, (iii) assures data integrity by storing the encrypted
e-MR in the distinct block designated for each user in the blockchain, and (iv)
eventually, enables authorized entities to regain the e-MR through decryption.
Preliminary evaluations, analyses, comparisons with state-of-the-art works,
etc., imply the efficacy of the proposed scheme.
</p></li>
</ul>

<h3>Title: A Secure Land Record Management System using Blockchain Technology. (arXiv:2304.13512v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13512">http://arxiv.org/abs/2304.13512</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13512] A Secure Land Record Management System using Blockchain Technology](http://arxiv.org/abs/2304.13512) #secure</code></li>
<li>Summary: <p>A land record (LR) contains very sensitive information related to land e.g.
owner, buyer, etc. Currently, almost all over the world, the LR is maintained
by different governmental offices and most of them maintain the LR with
paper-based approach. Some of the works focus to digitalize the existing land
record management system (LRMS) but with some security concerns. A
blockchain-based LRMS can be effective enough to solve the existing issues.
This paper proposes a blockchain-based LRMS that (i) digitalizes the existing
paper-based system, (ii) ensures LR privacy using an asymmetric cryptosystem,
(iii) preserves LR integrity, (iv) facilitates a platform for trading land
through an advertising agency, and (v) accelerates the process of changing
ownership that saves time significantly. Besides, this paper also proposes a
new way of character to integer mapping named C2I table that reduces around 33%
overhead of text to integer conversion compared to ASCII table. The
experimental results, analyses, and comparisons indicate the effectiveness of
the proposed LRMS over the state-of-the-art systems.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Efficient Explainable Face Verification based on Similarity Score Argument Backpropagation. (arXiv:2304.13409v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13409">http://arxiv.org/abs/2304.13409</a></li>
<li>Code URL: <a href="https://github.com/marcohuber/xssab">https://github.com/marcohuber/xssab</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13409] Efficient Explainable Face Verification based on Similarity Score Argument Backpropagation](http://arxiv.org/abs/2304.13409) #security</code></li>
<li>Summary: <p>Explainable Face Recognition is gaining growing attention as the use of the
technology is gaining ground in security-critical applications. Understanding
why two faces images are matched or not matched by a given face recognition
system is important to operators, users, anddevelopers to increase trust,
accountability, develop better systems, and highlight unfair behavior. In this
work, we propose xSSAB, an approach to back-propagate similarity score-based
arguments that support or oppose the face matching decision to visualize
spatial maps that indicate similar and dissimilar areas as interpreted by the
underlying FR model. Furthermore, we present Patch-LFW, a new explainable face
verification benchmark that enables along with a novel evaluation protocol, the
first quantitative evaluation of the validity of similarity and dissimilarity
maps in explainable face recognition approaches. We compare our efficient
approach to state-of-the-art approaches demonstrating a superior trade-off
between efficiency and performance. The code as well as the proposed Patch-LFW
is publicly available at: https://github.com/marcohuber/xSSAB.
</p></li>
</ul>

<h3>Title: HyMo: Vulnerability Detection in Smart Contracts using a Novel Multi-Modal Hybrid Model. (arXiv:2304.13103v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13103">http://arxiv.org/abs/2304.13103</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13103] HyMo: Vulnerability Detection in Smart Contracts using a Novel Multi-Modal Hybrid Model](http://arxiv.org/abs/2304.13103) #security</code></li>
<li>Summary: <p>With blockchain technology rapidly progress, the smart contracts have become
a common tool in a number of industries including finance, healthcare,
insurance and gaming. The number of smart contracts has multiplied, and at the
same time, the security of smart contracts has drawn considerable attention due
to the monetary losses brought on by smart contract vulnerabilities. Existing
analysis techniques are capable of identifying a large number of smart contract
security flaws, but they rely too much on rigid criteria established by
specialists, where the detection process takes much longer as the complexity of
the smart contract rises. In this paper, we propose HyMo as a multi-modal
hybrid deep learning model, which intelligently considers various input
representations to consider multimodality and FastText word embedding
technique, which represents each word as an n-gram of characters with BiGRU
deep learning technique, as a sequence processing model that consists of two
GRUs to achieve higher accuracy in smart contract vulnerability detection. The
model gathers features using various deep learning models to identify the smart
contract vulnerabilities. Through a series of studies on the currently publicly
accessible dataset such as ScrawlD, we show that our hybrid HyMo model has
excellent smart contract vulnerability detection performance. Therefore, HyMo
performs better detection of smart contract vulnerabilities against other
approaches.
</p></li>
</ul>

<h3>Title: A Security Verification Framework of Cryptographic Protocols Using Machine Learning. (arXiv:2304.13249v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13249">http://arxiv.org/abs/2304.13249</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13249] A Security Verification Framework of Cryptographic Protocols Using Machine Learning](http://arxiv.org/abs/2304.13249) #security</code></li>
<li>Summary: <p>We propose a security verification framework for cryptographic protocols
using machine learning. In recent years, as cryptographic protocols have become
more complex, research on automatic verification techniques has been focused
on. The main technique is formal verification. However, the formal verification
has two problems: it requires a large amount of computational time and does not
guarantee decidability. We propose a method that allows security verification
with computational time on the order of linear with respect to the size of the
protocol using machine learning. In training machine learning models for
security verification of cryptographic protocols, a sufficient amount of data,
i.e., a set of protocol data with security labels, is difficult to collect from
academic papers and other sources. To overcome this issue, we propose a way to
create arbitrarily large datasets by automatically generating random protocols
and assigning security labels to them using formal verification tools.
Furthermore, to exploit structural features of protocols, we construct a neural
network that processes a protocol along its series and tree structures. We
evaluate the proposed method by applying it to verification of practical
cryptographic protocols.
</p></li>
</ul>

<h3>Title: Understanding the Security and Performance of the Web Presence of Hospitals: A Measurement Study. (arXiv:2304.13278v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13278">http://arxiv.org/abs/2304.13278</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13278] Understanding the Security and Performance of the Web Presence of Hospitals: A Measurement Study](http://arxiv.org/abs/2304.13278) #security</code></li>
<li>Summary: <p>Using a total of 4,774 hospitals categorized as government, non-profit, and
proprietary hospitals, this study provides the first measurement-based analysis
of hospitals' websites and connects the findings with data breaches through a
correlation analysis. We study the security attributes of three categories,
collectively and in contrast, against domain name, content, and SSL
certificate-level features. We find that each type of hospital has a
distinctive characteristic of its utilization of domain name registrars,
top-level domain distribution, and domain creation distribution, as well as
content type and HTTP request features. Security-wise, and consistent with the
general population of websites, only 1\% of government hospitals utilized
DNSSEC, in contrast to 6\% of the proprietary hospitals. Alarmingly, we found
that 25\% of the hospitals used plain HTTP, in contrast to 20\% in the general
web population. Alarmingly too, we found that 8\%-84\% of the hospitals,
depending on their type, had some malicious contents, which are mostly
attributed to the lack of maintenance.
</p></li>
</ul>

<p>We conclude with a correlation analysis against 414 confirmed and manually
vetted hospitals' data breaches. Among other interesting findings, our study
highlights that the security attributes highlighted in our analysis of hospital
websites are forming a very strong indicator of their likelihood of being
breached. Our analyses are the first step towards understanding patient online
privacy, highlighting the lack of basic security in many hospitals' websites
and opening various potential research directions.
</p>

<h3>Title: Oracle R12 EBusiness Suite Role Based Access Control and Roles Lifecycle Management. (arXiv:2304.13514v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13514">http://arxiv.org/abs/2304.13514</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13514] Oracle R12 EBusiness Suite Role Based Access Control and Roles Lifecycle Management](http://arxiv.org/abs/2304.13514) #security</code></li>
<li>Summary: <p>Oracle E-Business Suite R12 is a widely used ERP solution that provides
integrated view of information across multiple functions and sources. It allows
for simplified business process tools for Shared service model e.g. Centralized
Operation where multiple operating units can be supported. Security
considerations are vital for such operations in large enterprises. R12
introduced Role Based Access Control security based on ANSI RBAC standard. R12
RBAC implementation is challenged with lack of Roles Lifecycle Management (RLM)
process which also contributes to challenges such as Segregation of duty (SOD),
and controlling access to PII for multi-country operation for common functional
areas. The paper will propose a possible Roles Lifecycle Management process.
</p></li>
</ul>

<h3>Title: Attention-Enhanced Deep Learning for Device-Free Through-the-Wall Presence Detection Using Indoor WiFi System. (arXiv:2304.13105v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13105">http://arxiv.org/abs/2304.13105</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13105] Attention-Enhanced Deep Learning for Device-Free Through-the-Wall Presence Detection Using Indoor WiFi System](http://arxiv.org/abs/2304.13105) #security</code></li>
<li>Summary: <p>Accurate detection of human presence in indoor environments is important for
various applications, such as energy management and security. In this paper, we
propose a novel system for human presence detection using the channel state
information (CSI) of WiFi signals. Our system named attention-enhanced deep
learning for presence detection (ALPD) employs an attention mechanism to
automatically select informative subcarriers from the CSI data and a
bidirectional long short-term memory (LSTM) network to capture temporal
dependencies in CSI. Additionally, we utilize a static feature to improve the
accuracy of human presence detection in static states. We evaluate the proposed
ALPD system by deploying a pair of WiFi access points (APs) for collecting CSI
dataset, which is further compared with several benchmarks. The results
demonstrate that our ALPD system outperforms the benchmarks in terms of
accuracy, especially in the presence of interference. Moreover, bidirectional
transmission data is beneficial to training improving stability and accuracy,
as well as reducing the costs of data collection for training. Overall, our
proposed ALPD system shows promising results for human presence detection using
WiFi CSI signals.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: TABLET: Learning From Instructions For Tabular Data. (arXiv:2304.13188v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13188">http://arxiv.org/abs/2304.13188</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13188] TABLET: Learning From Instructions For Tabular Data](http://arxiv.org/abs/2304.13188) #privacy</code></li>
<li>Summary: <p>Acquiring high-quality data is often a significant challenge in training
machine learning (ML) models for tabular prediction, particularly in
privacy-sensitive and costly domains like medicine and finance. Providing
natural language instructions to large language models (LLMs) offers an
alternative solution. However, it is unclear how effectively instructions
leverage the knowledge in LLMs for solving tabular prediction problems. To
address this gap, we introduce TABLET, a benchmark of 20 diverse tabular
datasets annotated with instructions that vary in their phrasing, granularity,
and technicality. Additionally, TABLET includes the instructions' logic and
structured modifications to the instructions. We find in-context instructions
increase zero-shot F1 performance for Flan-T5 11b by 44% on average and 13% for
ChatGPT on TABLET. Also, we explore the limitations of using LLMs for tabular
prediction in our benchmark by evaluating instruction faithfulness. We find
LLMs often ignore instructions and fail to predict specific instances
correctly, even with examples. Our analysis on TABLET shows that, while
instructions help LLM performance, learning from instructions for tabular data
requires new capabilities.
</p></li>
</ul>

<h3>Title: SHIELD: Thwarting Code Authorship Attribution. (arXiv:2304.13255v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13255">http://arxiv.org/abs/2304.13255</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13255] SHIELD: Thwarting Code Authorship Attribution](http://arxiv.org/abs/2304.13255) #privacy</code></li>
<li>Summary: <p>Authorship attribution has become increasingly accurate, posing a serious
privacy risk for programmers who wish to remain anonymous. In this paper, we
introduce SHIELD to examine the robustness of different code authorship
attribution approaches against adversarial code examples. We define four
attacks on attribution techniques, which include targeted and non-targeted
attacks, and realize them using adversarial code perturbation. We experiment
with a dataset of 200 programmers from the Google Code Jam competition to
validate our methods targeting six state-of-the-art authorship attribution
methods that adopt a variety of techniques for extracting authorship traits
from source-code, including RNN, CNN, and code stylometry. Our experiments
demonstrate the vulnerability of current authorship attribution methods against
adversarial attacks. For the non-targeted attack, our experiments demonstrate
the vulnerability of current authorship attribution methods against the attack
with an attack success rate exceeds 98.5\% accompanied by a degradation of the
identification confidence that exceeds 13\%. For the targeted attacks, we show
the possibility of impersonating a programmer using targeted-adversarial
perturbations with a success rate ranging from 66\% to 88\% for different
authorship attribution techniques under several adversarial scenarios.
</p></li>
</ul>

<h3>Title: C2PI: An Efficient Crypto-Clear Two-Party Neural Network Private Inference. (arXiv:2304.13266v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13266">http://arxiv.org/abs/2304.13266</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13266] C2PI: An Efficient Crypto-Clear Two-Party Neural Network Private Inference](http://arxiv.org/abs/2304.13266) #privacy</code></li>
<li>Summary: <p>Recently, private inference (PI) has addressed the rising concern over data
and model privacy in machine learning inference as a service. However, existing
PI frameworks suffer from high computational and communication costs due to the
expensive multi-party computation (MPC) protocols. Existing literature has
developed lighter MPC protocols to yield more efficient PI schemes. We, in
contrast, propose to lighten them by introducing an empirically-defined privacy
evaluation. To that end, we reformulate the threat model of PI and use
inference data privacy attacks (IDPAs) to evaluate data privacy. We then
present an enhanced IDPA, named distillation-based inverse-network attack
(DINA), for improved privacy evaluation. Finally, we leverage the findings from
DINA and propose C2PI, a two-party PI framework presenting an efficient
partitioning of the neural network model and requiring only the initial few
layers to be performed with MPC protocols. Based on our experimental
evaluations, relaxing the formal data privacy guarantees C2PI can speed up
existing PI frameworks, including Delphi [1] and Cheetah [2], up to 2.89x and
3.88x under LAN and WAN settings, respectively, and save up to 2.75x
communication costs.
</p></li>
</ul>

<h3>Title: FedVS: Straggler-Resilient and Privacy-Preserving Vertical Federated Learning for Split Models. (arXiv:2304.13407v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13407">http://arxiv.org/abs/2304.13407</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13407] FedVS: Straggler-Resilient and Privacy-Preserving Vertical Federated Learning for Split Models](http://arxiv.org/abs/2304.13407) #privacy</code></li>
<li>Summary: <p>In a vertical federated learning (VFL) system consisting of a central server
and many distributed clients, the training data are vertically partitioned such
that different features are privately stored on different clients. The problem
of split VFL is to train a model split between the server and the clients. This
paper aims to address two major challenges in split VFL: 1) performance
degradation due to straggling clients during training; and 2) data and model
privacy leakage from clients' uploaded data embeddings. We propose FedVS to
simultaneously address these two challenges. The key idea of FedVS is to design
secret sharing schemes for the local data and models, such that
information-theoretical privacy against colluding clients and curious server is
guaranteed, and the aggregation of all clients' embeddings is reconstructed
losslessly, via decrypting computation shares from the non-straggling clients.
Extensive experiments on various types of VFL datasets (including tabular, CV,
and multi-view) demonstrate the universal advantages of FedVS in straggler
mitigation and privacy protection over baseline protocols.
</p></li>
</ul>

<h3>Title: Killing Two Birds with One Stone: Quantization Achieves Privacy in Distributed Learning. (arXiv:2304.13545v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13545">http://arxiv.org/abs/2304.13545</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13545] Killing Two Birds with One Stone: Quantization Achieves Privacy in Distributed Learning](http://arxiv.org/abs/2304.13545) #privacy</code></li>
<li>Summary: <p>Communication efficiency and privacy protection are two critical issues in
distributed machine learning. Existing methods tackle these two issues
separately and may have a high implementation complexity that constrains their
application in a resource-limited environment. We propose a comprehensive
quantization-based solution that could simultaneously achieve communication
efficiency and privacy protection, providing new insights into the correlated
nature of communication and privacy. Specifically, we demonstrate the
effectiveness of our proposed solutions in the distributed stochastic gradient
descent (SGD) framework by adding binomial noise to the uniformly quantized
gradients to reach the desired differential privacy level but with a minor
sacrifice in communication efficiency. We theoretically capture the new
trade-offs between communication, privacy, and learning performance.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Autoencoder-based Radio Frequency Interference Mitigation For SMAP Passive Radiometer. (arXiv:2304.13158v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13158">http://arxiv.org/abs/2304.13158</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13158] Autoencoder-based Radio Frequency Interference Mitigation For SMAP Passive Radiometer](http://arxiv.org/abs/2304.13158) #protect</code></li>
<li>Summary: <p>Passive space-borne radiometers operating in the 1400-1427 MHz protected
frequency band face radio frequency interference (RFI) from terrestrial
sources. With the growth of wireless devices and the appearance of new
technologies, the possibility of sharing this spectrum with other technologies
would introduce more RFI to these radiometers. This band could be an ideal
mid-band frequency for 5G and Beyond, as it offers high capacity and good
coverage. Current RFI detection and mitigation techniques at SMAP (Soil
Moisture Active Passive) depend on correctly detecting and discarding or
filtering the contaminated data leading to the loss of valuable information,
especially in severe RFI cases. In this paper, we propose an autoencoder-based
RFI mitigation method to remove the dominant RFI caused by potential coexistent
terrestrial users (i.e., 5G base station) from the received contaminated signal
at the passive receiver side, potentially preserving valuable information and
preventing the contaminated data from being discarded.
</p></li>
</ul>

<h2>defense</h2>
<h2>attack</h2>
<h3>Title: Generating Adversarial Examples with Task Oriented Multi-Objective Optimization. (arXiv:2304.13229v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13229">http://arxiv.org/abs/2304.13229</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13229] Generating Adversarial Examples with Task Oriented Multi-Objective Optimization](http://arxiv.org/abs/2304.13229) #attack</code></li>
<li>Summary: <p>Deep learning models, even the-state-of-the-art ones, are highly vulnerable
to adversarial examples. Adversarial training is one of the most efficient
methods to improve the model's robustness. The key factor for the success of
adversarial training is the capability to generate qualified and divergent
adversarial examples which satisfy some objectives/goals (e.g., finding
adversarial examples that maximize the model losses for simultaneously
attacking multiple models). Therefore, multi-objective optimization (MOO) is a
natural tool for adversarial example generation to achieve multiple
objectives/goals simultaneously. However, we observe that a naive application
of MOO tends to maximize all objectives/goals equally, without caring if an
objective/goal has been achieved yet. This leads to useless effort to further
improve the goal-achieved tasks, while putting less focus on the
goal-unachieved tasks. In this paper, we propose \emph{Task Oriented MOO} to
address this issue, in the context where we can explicitly define the goal
achievement for a task. Our principle is to only maintain the goal-achieved
tasks, while letting the optimizer spend more effort on improving the
goal-unachieved tasks. We conduct comprehensive experiments for our Task
Oriented MOO on various adversarial example generation schemes. The
experimental results firmly demonstrate the merit of our proposed approach. Our
code is available at \url{https://github.com/tuananhbui89/TAMOO}.
</p></li>
</ul>

<h3>Title: Improving Adversarial Transferability by Intermediate-level Perturbation Decay. (arXiv:2304.13410v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13410">http://arxiv.org/abs/2304.13410</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13410] Improving Adversarial Transferability by Intermediate-level Perturbation Decay](http://arxiv.org/abs/2304.13410) #attack</code></li>
<li>Summary: <p>Intermediate-level attacks that attempt to perturb feature representations
following an adversarial direction drastically have shown favorable performance
in crafting transferable adversarial examples. Existing methods in this
category are normally formulated with two separate stages, where a directional
guide is required to be determined at first and the scalar projection of the
intermediate-level perturbation onto the directional guide is enlarged
thereafter. The obtained perturbation deviates from the guide inevitably in the
feature space, and it is revealed in this paper that such a deviation may lead
to sub-optimal attack. To address this issue, we develop a novel
intermediate-level method that crafts adversarial examples within a single
stage of optimization. In particular, the proposed method, named
intermediate-level perturbation decay (ILPD), encourages the intermediate-level
perturbation to be in an effective adversarial direction and to possess a great
magnitude simultaneously. In-depth discussion verifies the effectiveness of our
method. Experimental results show that it outperforms state-of-the-arts by
large margins in attacking various victim models on ImageNet (+10.07% on
average) and CIFAR-10 (+3.88% on average). Our code is at
https://github.com/qizhangli/ILPD-attack.
</p></li>
</ul>

<h3>Title: Are Explainability Tools Gender Biased? A Case Study on Face Presentation Attack Detection. (arXiv:2304.13419v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13419">http://arxiv.org/abs/2304.13419</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13419] Are Explainability Tools Gender Biased? A Case Study on Face Presentation Attack Detection](http://arxiv.org/abs/2304.13419) #attack</code></li>
<li>Summary: <p>Face recognition (FR) systems continue to spread in our daily lives with an
increasing demand for higher explainability and interpretability of FR systems
that are mainly based on deep learning. While bias across demographic groups in
FR systems has already been studied, the bias of explainability tools has not
yet been investigated. As such tools aim at steering further development and
enabling a better understanding of computer vision problems, the possible
existence of bias in their outcome can lead to a chain of biased decisions. In
this paper, we explore the existence of bias in the outcome of explainability
tools by investigating the use case of face presentation attack detection. By
utilizing two different explainability tools on models with different levels of
bias, we investigate the bias in the outcome of such tools. Our study shows
that these tools show clear signs of gender bias in the quality of their
explanations.
</p></li>
</ul>

<h3>Title: Model Extraction Attacks Against Reinforcement Learning Based Controllers. (arXiv:2304.13090v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13090">http://arxiv.org/abs/2304.13090</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13090] Model Extraction Attacks Against Reinforcement Learning Based Controllers](http://arxiv.org/abs/2304.13090) #attack</code></li>
<li>Summary: <p>We introduce the problem of model-extraction attacks in cyber-physical
systems in which an attacker attempts to estimate (or extract) the feedback
controller of the system. Extracting (or estimating) the controller provides an
unmatched edge to attackers since it allows them to predict the future control
actions of the system and plan their attack accordingly. Hence, it is important
to understand the ability of the attackers to perform such an attack. In this
paper, we focus on the setting when a Deep Neural Network (DNN) controller is
trained using Reinforcement Learning (RL) algorithms and is used to control a
stochastic system. We play the role of the attacker that aims to estimate such
an unknown DNN controller, and we propose a two-phase algorithm. In the first
phase, also called the offline phase, the attacker uses side-channel
information about the RL-reward function and the system dynamics to identify a
set of candidate estimates of the unknown DNN. In the second phase, also called
the online phase, the attacker observes the behavior of the unknown DNN and
uses these observations to shortlist the set of final policy estimates. We
provide theoretical analysis of the error between the unknown DNN and the
estimated one. We also provide numerical results showing the effectiveness of
the proposed algorithm.
</p></li>
</ul>

<h3>Title: LSTM-based Load Forecasting Robustness Against Noise Injection Attack in Microgrid. (arXiv:2304.13104v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13104">http://arxiv.org/abs/2304.13104</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13104] LSTM-based Load Forecasting Robustness Against Noise Injection Attack in Microgrid](http://arxiv.org/abs/2304.13104) #attack</code></li>
<li>Summary: <p>In this paper, we investigate the robustness of an LSTM neural network
against noise injection attacks for electric load forecasting in an ideal
microgrid. The performance of the LSTM model is investigated under a black-box
Gaussian noise attack with different SNRs. It is assumed that attackers have
just access to the input data of the LSTM model. The results show that the
noise attack affects the performance of the LSTM model. The load prediction
means absolute error (MAE) is 0.047 MW for a healthy prediction, while this
value increases up to 0.097 MW for a Gaussian noise insertion with SNR= 6 dB.
To robustify the LSTM model against noise attack, a low-pass filter with
optimal cut-off frequency is applied at the model's input to remove the noise
attack. The filter performs better in case of noise with lower SNR and is less
promising for small noises.
</p></li>
</ul>

<h3>Title: Analyzing In-browser Cryptojacking. (arXiv:2304.13253v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13253">http://arxiv.org/abs/2304.13253</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13253] Analyzing In-browser Cryptojacking](http://arxiv.org/abs/2304.13253) #attack</code></li>
<li>Summary: <p>Cryptojacking is the permissionless use of a target device to covertly mine
cryptocurrencies. With cryptojacking, attackers use malicious JavaScript codes
to force web browsers into solving proof-of-work puzzles, thus making money by
exploiting the resources of the website visitors. To understand and counter
such attacks, we systematically analyze the static, dynamic, and economic
aspects of in-browser cryptojacking. For static analysis, we perform content,
currency, and code-based categorization of cryptojacking samples to 1) measure
their distribution across websites, 2) highlight their platform affinities, and
3) study their code complexities. We apply machine learning techniques to
distinguish cryptojacking scripts from benign and malicious JavaScript samples
with 100\% accuracy. For dynamic analysis, we analyze the effect of
cryptojacking on critical system resources, such as CPU and battery usage. We
also perform web browser fingerprinting to analyze the information exchange
between the victim node and the dropzone cryptojacking server. We also build an
analytical model to empirically evaluate the feasibility of cryptojacking as an
alternative to online advertisement. Our results show a sizeable negative
profit and loss gap, indicating that the model is economically infeasible.
Finally, leveraging insights from our analyses, we build countermeasures for
in-browser cryptojacking that improve the existing remedies.
</p></li>
</ul>

<h3>Title: Blockchain-based Federated Learning with SMPC Model Verification Against Poisoning Attack for Healthcare Systems. (arXiv:2304.13360v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13360">http://arxiv.org/abs/2304.13360</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13360] Blockchain-based Federated Learning with SMPC Model Verification Against Poisoning Attack for Healthcare Systems](http://arxiv.org/abs/2304.13360) #attack</code></li>
<li>Summary: <p>Due to the rising awareness of privacy and security in machine learning
applications, federated learning (FL) has received widespread attention and
applied to several areas, e.g., intelligence healthcare systems, IoT-based
industries, and smart cities. FL enables clients to train a global model
collaboratively without accessing their local training data. However, the
current FL schemes are vulnerable to adversarial attacks. Its architecture
makes detecting and defending against malicious model updates difficult. In
addition, most recent studies to detect FL from malicious updates while
maintaining the model's privacy have not been sufficiently explored. This paper
proposed blockchain-based federated learning with SMPC model verification
against poisoning attacks for healthcare systems. First, we check the machine
learning model from the FL participants through an encrypted inference process
and remove the compromised model. Once the participants' local models have been
verified, the models are sent to the blockchain node to be securely aggregated.
We conducted several experiments with different medical datasets to evaluate
our proposed framework.
</p></li>
</ul>

<h3>Title: Thwarting Code-Reuse and Side-Channel Attacks in Embedded Systems. (arXiv:2304.13458v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13458">http://arxiv.org/abs/2304.13458</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13458] Thwarting Code-Reuse and Side-Channel Attacks in Embedded Systems](http://arxiv.org/abs/2304.13458) #attack</code></li>
<li>Summary: <p>Nowadays, embedded devices are increasingly present in everyday life, often
controlling and processing critical information. For this reason, these devices
make use of cryptographic protocols. However, embedded devices are particularly
vulnerable to attackers seeking to hijack their operation and extract sensitive
information. Code-Reuse Attacks (CRAs) can steer the execution of a program to
malicious outcomes, leveraging existing on-board code without direct access to
the device memory. Moreover, Side-Channel Attacks (SCAs) may reveal secret
information to the attacker based on mere observation of the device. In this
paper, we are particularly concerned with thwarting CRAs and SCAs against
embedded devices, while taking into account their resource limitations.
Fine-grained code diversification can hinder CRAs by introducing uncertainty to
the binary code; while software mechanisms can thwart timing or power SCAs. The
resilience to either attack may come at the price of the overall efficiency.
Moreover, a unified approach that preserves these mitigations against both CRAs
and SCAs is not available. This is the main novelty of our approach, Secure
Diversity by Construction (SecDivCon); a combinatorial compiler-based approach
that combines software diversification against CRAs with software mitigations
against SCAs. SecDivCon restricts the performance overhead in the generated
code, offering a secure-by-design control on the performance-security
trade-off. Our experiments show that SCA-aware diversification is effective
against CRAs, while preserving SCA mitigation properties at a low, controllable
overhead. Given the combinatorial nature of our approach, SecDivCon is suitable
for small, performance-critical functions that are sensitive to SCAs. SecDivCon
may be used as a building block to whole-program code diversification or in a
re-randomization scheme of cryptographic code.
</p></li>
</ul>

<h3>Title: Uncovering the Representation of Spiking Neural Networks Trained with Surrogate Gradient. (arXiv:2304.13098v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13098">http://arxiv.org/abs/2304.13098</a></li>
<li>Code URL: <a href="https://github.com/intelligent-computing-lab-yale/snncka">https://github.com/intelligent-computing-lab-yale/snncka</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13098] Uncovering the Representation of Spiking Neural Networks Trained with Surrogate Gradient](http://arxiv.org/abs/2304.13098) #attack</code></li>
<li>Summary: <p>Spiking Neural Networks (SNNs) are recognized as the candidate for the
next-generation neural networks due to their bio-plausibility and energy
efficiency. Recently, researchers have demonstrated that SNNs are able to
achieve nearly state-of-the-art performance in image recognition tasks using
surrogate gradient training. However, some essential questions exist pertaining
to SNNs that are little studied: Do SNNs trained with surrogate gradient learn
different representations from traditional Artificial Neural Networks (ANNs)?
Does the time dimension in SNNs provide unique representation power? In this
paper, we aim to answer these questions by conducting a representation
similarity analysis between SNNs and ANNs using Centered Kernel Alignment
(CKA). We start by analyzing the spatial dimension of the networks, including
both the width and the depth. Furthermore, our analysis of residual connections
shows that SNNs learn a periodic pattern, which rectifies the representations
in SNNs to be ANN-like. We additionally investigate the effect of the time
dimension on SNN representation, finding that deeper layers encourage more
dynamics along the time dimension. We also investigate the impact of input data
such as event-stream data and adversarial attacks. Our work uncovers a host of
new findings of representations in SNNs. We hope this work will inspire future
research to fully comprehend the representation power of SNNs. Code is released
at https://github.com/Intelligent-Computing-Lab-Yale/SNNCKA.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: AVFace: Towards Detailed Audio-Visual 4D Face Reconstruction. (arXiv:2304.13115v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13115">http://arxiv.org/abs/2304.13115</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13115] AVFace: Towards Detailed Audio-Visual 4D Face Reconstruction](http://arxiv.org/abs/2304.13115) #robust</code></li>
<li>Summary: <p>In this work, we present a multimodal solution to the problem of 4D face
reconstruction from monocular videos. 3D face reconstruction from 2D images is
an under-constrained problem due to the ambiguity of depth. State-of-the-art
methods try to solve this problem by leveraging visual information from a
single image or video, whereas 3D mesh animation approaches rely more on audio.
However, in most cases (e.g. AR/VR applications), videos include both visual
and speech information. We propose AVFace that incorporates both modalities and
accurately reconstructs the 4D facial and lip motion of any speaker, without
requiring any 3D ground truth for training. A coarse stage estimates the
per-frame parameters of a 3D morphable model, followed by a lip refinement, and
then a fine stage recovers facial geometric details. Due to the temporal audio
and video information captured by transformer-based modules, our method is
robust in cases when either modality is insufficient (e.g. face occlusions).
Extensive qualitative and quantitative evaluation demonstrates the superiority
of our method over the current state-of-the-art.
</p></li>
</ul>

<h3>Title: Neural-PBIR Reconstruction of Shape, Material, and Illumination. (arXiv:2304.13445v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13445">http://arxiv.org/abs/2304.13445</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13445] Neural-PBIR Reconstruction of Shape, Material, and Illumination](http://arxiv.org/abs/2304.13445) #robust</code></li>
<li>Summary: <p>Reconstructing the shape and spatially varying surface appearances of a
physical-world object as well as its surrounding illumination based on 2D
images (e.g., photographs) of the object has been a long-standing problem in
computer vision and graphics. In this paper, we introduce a robust object
reconstruction pipeline combining neural based object reconstruction and
physics-based inverse rendering (PBIR). Specifically, our pipeline firstly
leverages a neural stage to produce high-quality but potentially imperfect
predictions of object shape, reflectance, and illumination. Then, in the later
stage, initialized by the neural predictions, we perform PBIR to refine the
initial results and obtain the final high-quality reconstruction. Experimental
results demonstrate our pipeline significantly outperforms existing
reconstruction methods quality-wise and performance-wise.
</p></li>
</ul>

<h3>Title: ESimCSE Unsupervised Contrastive Learning Jointly with UDA Semi-Supervised Learning for Large Label System Text Classification Mode. (arXiv:2304.13140v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13140">http://arxiv.org/abs/2304.13140</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13140] ESimCSE Unsupervised Contrastive Learning Jointly with UDA Semi-Supervised Learning for Large Label System Text Classification Mode](http://arxiv.org/abs/2304.13140) #robust</code></li>
<li>Summary: <p>The challenges faced by text classification with large tag systems in natural
language processing tasks include multiple tag systems, uneven data
distribution, and high noise. To address these problems, the ESimCSE
unsupervised comparative learning and UDA semi-supervised comparative learning
models are combined through the use of joint training techniques in the
models.The ESimCSE model efficiently learns text vector representations using
unlabeled data to achieve better classification results, while UDA is trained
using unlabeled data through semi-supervised learning methods to improve the
prediction performance of the models and stability, and further improve the
generalization ability of the model. In addition, adversarial training
techniques FGM and PGD are used in the model training process to improve the
robustness and reliability of the model. The experimental results show that
there is an 8% and 10% accuracy improvement relative to Baseline on the public
dataset Ruesters as well as on the operational dataset, respectively, and a 15%
improvement in manual validation accuracy can be achieved on the operational
dataset, indicating that the method is effective.
</p></li>
</ul>

<h3>Title: Implicit Counterfactual Data Augmentation for Deep Neural Networks. (arXiv:2304.13431v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13431">http://arxiv.org/abs/2304.13431</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13431] Implicit Counterfactual Data Augmentation for Deep Neural Networks](http://arxiv.org/abs/2304.13431) #robust</code></li>
<li>Summary: <p>Machine-learning models are prone to capturing the spurious correlations
between non-causal attributes and classes, with counterfactual data
augmentation being a promising direction for breaking these spurious
associations. However, explicitly generating counterfactual data is
challenging, with the training efficiency declining. Therefore, this study
proposes an implicit counterfactual data augmentation (ICDA) method to remove
spurious correlations and make stable predictions. Specifically, first, a novel
sample-wise augmentation strategy is developed that generates semantically and
counterfactually meaningful deep features with distinct augmentation strength
for each sample. Second, we derive an easy-to-compute surrogate loss on the
augmented feature set when the number of augmented samples becomes infinite.
Third, two concrete schemes are proposed, including direct quantification and
meta-learning, to derive the key parameters for the robust loss. In addition,
ICDA is explained from a regularization aspect, with extensive experiments
indicating that our method consistently improves the generalization performance
of popular depth networks on multiple typical learning scenarios that require
out-of-distribution generalization.
</p></li>
</ul>

<h3>Title: CROP: Towards Distributional-Shift Robust Reinforcement Learning using Compact Reshaped Observation Processing. (arXiv:2304.13616v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13616">http://arxiv.org/abs/2304.13616</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13616] CROP: Towards Distributional-Shift Robust Reinforcement Learning using Compact Reshaped Observation Processing](http://arxiv.org/abs/2304.13616) #robust</code></li>
<li>Summary: <p>The safe application of reinforcement learning (RL) requires generalization
from limited training data to unseen scenarios. Yet, fulfilling tasks under
changing circumstances is a key challenge in RL. Current state-of-the-art
approaches for generalization apply data augmentation techniques to increase
the diversity of training data. Even though this prevents overfitting to the
training environment(s), it hinders policy optimization. Crafting a suitable
observation, only containing crucial information, has been shown to be a
challenging task itself. To improve data efficiency and generalization
capabilities, we propose Compact Reshaped Observation Processing (CROP) to
reduce the state information used for policy optimization. By providing only
relevant information, overfitting to a specific training layout is precluded
and generalization to unseen environments is improved. We formulate three CROPs
that can be applied to fully observable observation- and action-spaces and
provide methodical foundation. We empirically show the improvements of CROP in
a distributionally shifted safety gridworld. We furthermore provide benchmark
comparisons to full observability and data-augmentation in two different-sized
procedurally generated mazes.
</p></li>
</ul>

<h3>Title: Sparsified Model Zoo Twins: Investigating Populations of Sparsified Neural Network Models. (arXiv:2304.13718v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13718">http://arxiv.org/abs/2304.13718</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13718] Sparsified Model Zoo Twins: Investigating Populations of Sparsified Neural Network Models](http://arxiv.org/abs/2304.13718) #robust</code></li>
<li>Summary: <p>With growing size of Neural Networks (NNs), model sparsification to reduce
the computational cost and memory demand for model inference has become of
vital interest for both research and production. While many sparsification
methods have been proposed and successfully applied on individual models, to
the best of our knowledge their behavior and robustness has not yet been
studied on large populations of models. With this paper, we address that gap by
applying two popular sparsification methods on populations of models (so called
model zoos) to create sparsified versions of the original zoos. We investigate
the performance of these two methods for each zoo, compare sparsification
layer-wise, and analyse agreement between original and sparsified populations.
We find both methods to be very robust with magnitude pruning able outperform
variational dropout with the exception of high sparsification ratios above 80%.
Further, we find sparsified models agree to a high degree with their original
non-sparsified counterpart, and that the performance of original and sparsified
model is highly correlated. Finally, all models of the model zoos and their
sparsified model twins are publicly available: modelzoos.cc.
</p></li>
</ul>

<h2>biometric</h2>
<h3>Title: Bridging the Gap: Gaze Events as Interpretable Concepts to Explain Deep Neural Sequence Models. (arXiv:2304.13536v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13536">http://arxiv.org/abs/2304.13536</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13536] Bridging the Gap: Gaze Events as Interpretable Concepts to Explain Deep Neural Sequence Models](http://arxiv.org/abs/2304.13536) #biometric</code></li>
<li>Summary: <p>Recent work in XAI for eye tracking data has evaluated the suitability of
feature attribution methods to explain the output of deep neural sequence
models for the task of oculomotric biometric identification. These methods
provide saliency maps to highlight important input features of a specific eye
gaze sequence. However, to date, its localization analysis has been lacking a
quantitative approach across entire datasets. In this work, we employ
established gaze event detection algorithms for fixations and saccades and
quantitatively evaluate the impact of these events by determining their concept
influence. Input features that belong to saccades are shown to be substantially
more important than features that belong to fixations. By dissecting saccade
events into sub-events, we are able to show that gaze samples that are close to
the saccadic peak velocity are most influential. We further investigate the
effect of event properties like saccadic amplitude or fixational dispersion on
the resulting concept influence.
</p></li>
</ul>

<h3>Title: Measuring Bias in AI Models with Application to Face Biometrics: An Statistical Approach. (arXiv:2304.13680v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13680">http://arxiv.org/abs/2304.13680</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13680] Measuring Bias in AI Models with Application to Face Biometrics: An Statistical Approach](http://arxiv.org/abs/2304.13680) #biometric</code></li>
<li>Summary: <p>The new regulatory framework proposal on Artificial Intelligence (AI)
published by the European Commission establishes a new risk-based legal
approach. The proposal highlights the need to develop adequate risk assessments
for the different uses of AI. This risk assessment should address, among
others, the detection and mitigation of bias in AI. In this work we analyze
statistical approaches to measure biases in automatic decision-making systems.
We focus our experiments in face recognition technologies. We propose a novel
way to measure the biases in machine learning models using a statistical
approach based on the N-Sigma method. N-Sigma is a popular statistical approach
used to validate hypotheses in general science such as physics and social areas
and its application to machine learning is yet unexplored. In this work we
study how to apply this methodology to develop new risk assessment frameworks
based on bias analysis and we discuss the main advantages and drawbacks with
respect to other popular statistical tests.
</p></li>
</ul>

<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: ZRG: A High Resolution 3D Residential Rooftop Geometry Dataset for Machine Learning. (arXiv:2304.13219v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13219">http://arxiv.org/abs/2304.13219</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13219] ZRG: A High Resolution 3D Residential Rooftop Geometry Dataset for Machine Learning](http://arxiv.org/abs/2304.13219) #extraction</code></li>
<li>Summary: <p>In this paper we present the Zeitview Rooftop Geometry (ZRG) dataset. ZRG
contains thousands of samples of high resolution orthomosaics of aerial imagery
of residential rooftops with corresponding digital surface models (DSM), 3D
rooftop wireframes, and multiview imagery generated point clouds for the
purpose of residential rooftop geometry and scene understanding. We perform
thorough benchmarks to illustrate the numerous applications unlocked by this
dataset and provide baselines for the tasks of roof outline extraction,
monocular height estimation, and planar roof structure extraction.
</p></li>
</ul>

<h3>Title: Deep Lifelong Cross-modal Hashing. (arXiv:2304.13357v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13357">http://arxiv.org/abs/2304.13357</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13357] Deep Lifelong Cross-modal Hashing](http://arxiv.org/abs/2304.13357) #extraction</code></li>
<li>Summary: <p>Hashing methods have made significant progress in cross-modal retrieval tasks
with fast query speed and low storage cost. Among them, deep learning-based
hashing achieves better performance on large-scale data due to its excellent
extraction and representation ability for nonlinear heterogeneous features.
However, there are still two main challenges in catastrophic forgetting when
data with new categories arrive continuously, and time-consuming for
non-continuous hashing retrieval to retrain for updating. To this end, we, in
this paper, propose a novel deep lifelong cross-modal hashing to achieve
lifelong hashing retrieval instead of re-training hash function repeatedly when
new data arrive. Specifically, we design lifelong learning strategy to update
hash functions by directly training the incremental data instead of retraining
new hash functions using all the accumulated data, which significantly reduce
training time. Then, we propose lifelong hashing loss to enable original hash
codes participate in lifelong learning but remain invariant, and further
preserve the similarity and dis-similarity among original and incremental hash
codes to maintain performance. Additionally, considering distribution
heterogeneity when new data arriving continuously, we introduce multi-label
semantic similarity to supervise hash learning, and it has been proven that the
similarity improves performance with detailed analysis. Experimental results on
benchmark datasets show that the proposed methods achieves comparative
performance comparing with recent state-of-the-art cross-modal hashing methods,
and it yields substantial average increments over 20\% in retrieval accuracy
and almost reduces over 80\% training time when new data arrives continuously.
</p></li>
</ul>

<h3>Title: Group Equivariant BEV for 3D Object Detection. (arXiv:2304.13390v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13390">http://arxiv.org/abs/2304.13390</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13390] Group Equivariant BEV for 3D Object Detection](http://arxiv.org/abs/2304.13390) #extraction</code></li>
<li>Summary: <p>Recently, 3D object detection has attracted significant attention and
achieved continuous improvement in real road scenarios. The environmental
information is collected from a single sensor or multi-sensor fusion to detect
interested objects. However, most of the current 3D object detection approaches
focus on developing advanced network architectures to improve the detection
precision of the object rather than considering the dynamic driving scenes,
where data collected from sensors equipped in the vehicle contain various
perturbation features. As a result, existing work cannot still tackle the
perturbation issue. In order to solve this problem, we propose a group
equivariant bird's eye view network (GeqBevNet) based on the group equivariant
theory, which introduces the concept of group equivariant into the BEV fusion
object detection network. The group equivariant network is embedded into the
fused BEV feature map to facilitate the BEV-level rotational equivariant
feature extraction, thus leading to lower average orientation error. In order
to demonstrate the effectiveness of the GeqBevNet, the network is verified on
the nuScenes validation dataset in which mAOE can be decreased to 0.325.
Experimental results demonstrate that GeqBevNet can extract more rotational
equivariant features in the 3D object detection of the actual road scene and
improve the performance of object orientation prediction.
</p></li>
</ul>

<h3>Title: Key-value information extraction from full handwritten pages. (arXiv:2304.13530v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13530">http://arxiv.org/abs/2304.13530</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13530] Key-value information extraction from full handwritten pages](http://arxiv.org/abs/2304.13530) #extraction</code></li>
<li>Summary: <p>We propose a Transformer-based approach for information extraction from
digitized handwritten documents. Our approach combines, in a single model, the
different steps that were so far performed by separate models: feature
extraction, handwriting recognition and named entity recognition. We compare
this integrated approach with traditional two-stage methods that perform
handwriting recognition before named entity recognition, and present results at
different levels: line, paragraph, and page. Our experiments show that
attention-based models are especially interesting when applied on full pages,
as they do not require any prior segmentation step. Finally, we show that they
are able to learn from key-value annotations: a list of important words with
their corresponding named entities. We compare our models to state-of-the-art
methods on three public databases (IAM, ESPOSALLES, and POPP) and outperform
previous performances on all three datasets.
</p></li>
</ul>

<h3>Title: SIMARA: a database for key-value information extraction from full pages. (arXiv:2304.13606v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13606">http://arxiv.org/abs/2304.13606</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13606] SIMARA: a database for key-value information extraction from full pages](http://arxiv.org/abs/2304.13606) #extraction</code></li>
<li>Summary: <p>We propose a new database for information extraction from historical
handwritten documents. The corpus includes 5,393 finding aids from six
different series, dating from the 18th-20th centuries. Finding aids are
handwritten documents that contain metadata describing older archives. They are
stored in the National Archives of France and are used by archivists to
identify and find archival documents. Each document is annotated at page-level,
and contains seven fields to retrieve. The localization of each field is not
available in such a way that this dataset encourages research on
segmentation-free systems for information extraction. We propose a model based
on the Transformer architecture trained for end-to-end information extraction
and provide three sets for training, validation and testing, to ensure fair
comparison with future works. The database is freely accessible at
https://zenodo.org/record/7868059.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: SMPC-based Federated Learning for 6G enabled Internet of Medical Things. (arXiv:2304.13352v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13352">http://arxiv.org/abs/2304.13352</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13352] SMPC-based Federated Learning for 6G enabled Internet of Medical Things](http://arxiv.org/abs/2304.13352) #federate</code></li>
<li>Summary: <p>Rapidly developing intelligent healthcare systems are underpinned by Sixth
Generation (6G) connectivity, ubiquitous Internet of Things (IoT), and Deep
Learning (DL) techniques. This portends a future where 6G powers the Internet
of Medical Things (IoMT) with seamless, large-scale, and real-time connectivity
amongst entities. This article proposes a Convolutional Neural Network (CNN)
based Federated Learning framework that combines Secure Multi-Party Computation
(SMPC) based aggregation and Encrypted Inference methods, all within the
context of 6G and IoMT. We consider multiple hospitals with clusters of mixed
IoMT and edge devices that encrypt locally trained models. Subsequently, each
hospital sends the encrypted local models for SMPC-based encrypted aggregation
in the cloud, which generates the encrypted global model. Ultimately, the
encrypted global model is returned to each edge server for more localized
training, further improving model accuracy. Moreover, hospitals can perform
encrypted inference on their edge servers or the cloud while maintaining data
and model privacy. Multiple experiments were conducted with varying CNN models
and datasets to evaluate the proposed framework's performance.
</p></li>
</ul>

<h3>Title: Bayesian Federated Learning: A Survey. (arXiv:2304.13267v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13267">http://arxiv.org/abs/2304.13267</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13267] Bayesian Federated Learning: A Survey](http://arxiv.org/abs/2304.13267) #federate</code></li>
<li>Summary: <p>Federated learning (FL) demonstrates its advantages in integrating
distributed infrastructure, communication, computing and learning in a
privacy-preserving manner. However, the robustness and capabilities of existing
FL methods are challenged by limited and dynamic data and conditions,
complexities including heterogeneities and uncertainties, and analytical
explainability. Bayesian federated learning (BFL) has emerged as a promising
approach to address these issues. This survey presents a critical overview of
BFL, including its basic concepts, its relations to Bayesian learning in the
context of FL, and a taxonomy of BFL from both Bayesian and federated
perspectives. We categorize and discuss client- and server-side and FL-based
BFL methods and their pros and cons. The limitations of the existing BFL
methods and the future directions of BFL research further address the intricate
requirements of real-life FL applications.
</p></li>
</ul>

<h3>Title: Federated Learning with Uncertainty-Based Client Clustering for Fleet-Wide Fault Diagnosis. (arXiv:2304.13275v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13275">http://arxiv.org/abs/2304.13275</a></li>
<li>Code URL: <a href="https://github.com/salierilu/fl_fault_diagnosis">https://github.com/salierilu/fl_fault_diagnosis</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13275] Federated Learning with Uncertainty-Based Client Clustering for Fleet-Wide Fault Diagnosis](http://arxiv.org/abs/2304.13275) #federate</code></li>
<li>Summary: <p>Operators from various industries have been pushing the adoption of wireless
sensing nodes for industrial monitoring, and such efforts have produced
sizeable condition monitoring datasets that can be used to build diagnosis
algorithms capable of warning maintenance engineers of impending failure or
identifying current system health conditions. However, single operators may not
have sufficiently large fleets of systems or component units to collect
sufficient data to develop data-driven algorithms. Collecting a satisfactory
quantity of fault patterns for safety-critical systems is particularly
difficult due to the rarity of faults. Federated learning (FL) has emerged as a
promising solution to leverage datasets from multiple operators to train a
decentralized asset fault diagnosis model while maintaining data
confidentiality. However, there are still considerable obstacles to overcome
when it comes to optimizing the federation strategy without leaking sensitive
data and addressing the issue of client dataset heterogeneity. This is
particularly prevalent in fault diagnosis applications due to the high
diversity of operating conditions and system configurations. To address these
two challenges, we propose a novel clustering-based FL algorithm where clients
are clustered for federating based on dataset similarity. To quantify dataset
similarity between clients without explicitly sharing data, each client sets
aside a local test dataset and evaluates the other clients' model prediction
accuracy and uncertainty on this test dataset. Clients are then clustered for
FL based on relative prediction accuracy and uncertainty.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Structure Diagram Recognition in Financial Announcements. (arXiv:2304.13240v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13240">http://arxiv.org/abs/2304.13240</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13240] Structure Diagram Recognition in Financial Announcements](http://arxiv.org/abs/2304.13240) #fair</code></li>
<li>Summary: <p>Accurately extracting structured data from structure diagrams in financial
announcements is of great practical importance for building financial knowledge
graphs and further improving the efficiency of various financial applications.
First, we proposed a new method for recognizing structure diagrams in financial
announcements, which can better detect and extract different types of
connecting lines, including straight lines, curves, and polylines of different
orientations and angles. Second, we developed a two-stage method to efficiently
generate the industry's first benchmark of structure diagrams from Chinese
financial announcements, where a large number of diagrams were synthesized and
annotated using an automated tool to train a preliminary recognition model with
fairly good performance, and then a high-quality benchmark can be obtained by
automatically annotating the real-world structure diagrams using the
preliminary model and then making few manual corrections. Finally, we
experimentally verified the significant performance advantage of our structure
diagram recognition method over previous methods.
</p></li>
</ul>

<h2>interpretability</h2>
<h2>explainability</h2>
<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Single-View Height Estimation with Conditional Diffusion Probabilistic Models. (arXiv:2304.13214v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13214">http://arxiv.org/abs/2304.13214</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13214] Single-View Height Estimation with Conditional Diffusion Probabilistic Models](http://arxiv.org/abs/2304.13214) #diffusion</code></li>
<li>Summary: <p>Digital Surface Models (DSM) offer a wealth of height information for
understanding the Earth's surface as well as monitoring the existence or change
in natural and man-made structures. Classical height estimation requires
multi-view geospatial imagery or LiDAR point clouds which can be expensive to
acquire. Single-view height estimation using neural network based models shows
promise however it can struggle with reconstructing high resolution features.
The latest advancements in diffusion models for high resolution image synthesis
and editing have yet to be utilized for remote sensing imagery, particularly
height estimation. Our approach involves training a generative diffusion model
to learn the joint distribution of optical and DSM images across both domains
as a Markov chain. This is accomplished by minimizing a denoising score
matching objective while being conditioned on the source image to generate
realistic high resolution 3D surfaces. In this paper we experiment with
conditional denoising diffusion probabilistic models (DDPM) for height
estimation from a single remotely sensed image and show promising results on
the Vaihingen benchmark dataset.
</p></li>
</ul>

<h3>Title: Training-Free Location-Aware Text-to-Image Synthesis. (arXiv:2304.13427v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13427">http://arxiv.org/abs/2304.13427</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13427] Training-Free Location-Aware Text-to-Image Synthesis](http://arxiv.org/abs/2304.13427) #diffusion</code></li>
<li>Summary: <p>Current large-scale generative models have impressive efficiency in
generating high-quality images based on text prompts. However, they lack the
ability to precisely control the size and position of objects in the generated
image. In this study, we analyze the generative mechanism of the stable
diffusion model and propose a new interactive generation paradigm that allows
users to specify the position of generated objects without additional training.
Moreover, we propose an object detection-based evaluation metric to assess the
control capability of location aware generation task. Our experimental results
show that our method outperforms state-of-the-art methods on both control
capacity and image quality.
</p></li>
</ul>

<h3>Title: Diffusion Probabilistic Model Based Accurate and High-Degree-of-Freedom Metasurface Inverse Design. (arXiv:2304.13038v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13038">http://arxiv.org/abs/2304.13038</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13038] Diffusion Probabilistic Model Based Accurate and High-Degree-of-Freedom Metasurface Inverse Design](http://arxiv.org/abs/2304.13038) #diffusion</code></li>
<li>Summary: <p>Conventional meta-atom designs rely heavily on researchers' prior knowledge
and trial-and-error searches using full-wave simulations, resulting in
time-consuming and inefficient processes. Inverse design methods based on
optimization algorithms, such as evolutionary algorithms, and topological
optimizations, have been introduced to design metamaterials. However, none of
these algorithms are general enough to fulfill multi-objective tasks. Recently,
deep learning methods represented by Generative Adversarial Networks (GANs)
have been applied to inverse design of metamaterials, which can directly
generate high-degree-of-freedom meta-atoms based on S-parameter requirements.
However, the adversarial training process of GANs makes the network unstable
and results in high modeling costs. This paper proposes a novel metamaterial
inverse design method based on the diffusion probability theory. By learning
the Markov process that transforms the original structure into a Gaussian
distribution, the proposed method can gradually remove the noise starting from
the Gaussian distribution and generate new high-degree-of-freedom meta-atoms
that meet S-parameter conditions, which avoids the model instability introduced
by the adversarial training process of GANs and ensures more accurate and
high-quality generation results. Experiments have proven that our method is
superior to representative methods of GANs in terms of model convergence speed,
generation accuracy, and quality.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: iMixer: hierarchical Hopfield network implies an invertible, implicit and iterative MLP-Mixer. (arXiv:2304.13061v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13061">http://arxiv.org/abs/2304.13061</a></li>
<li>Code URL: <a href="https://github.com/toshihiro-ota/imixer">https://github.com/toshihiro-ota/imixer</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13061] iMixer: hierarchical Hopfield network implies an invertible, implicit and iterative MLP-Mixer](http://arxiv.org/abs/2304.13061) #transformer</code></li>
<li>Summary: <p>In the last few years, the success of Transformers in computer vision has
stimulated the discovery of many alternative models that compete with
Transformers, such as the MLP-Mixer. Despite their weak induced bias, these
models have achieved performance comparable to well-studied convolutional
neural networks. Recent studies on modern Hopfield networks suggest the
correspondence between certain energy-based associative memory models and
Transformers or MLP-Mixer, and shed some light on the theoretical background of
the Transformer-type architectures design. In this paper we generalize the
correspondence to the recently introduced hierarchical Hopfield network, and
find iMixer, a novel generalization of MLP-Mixer model. Unlike ordinary
feedforward neural networks, iMixer involves MLP layers that propagate forward
from the output side to the input side. We characterize the module as an
example of invertible, implicit, and iterative mixing module. We evaluate the
model performance with various datasets on image classification tasks, and find
that iMixer reasonably achieves the improvement compared to the baseline
vanilla MLP-Mixer. The results imply that the correspondence between the
Hopfield networks and the Mixer models serves as a principle for understanding
a broader class of Transformer-like architecture designs.
</p></li>
</ul>

<h3>Title: Objectives Matter: Understanding the Impact of Self-Supervised Objectives on Vision Transformer Representations. (arXiv:2304.13089v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13089">http://arxiv.org/abs/2304.13089</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13089] Objectives Matter: Understanding the Impact of Self-Supervised Objectives on Vision Transformer Representations](http://arxiv.org/abs/2304.13089) #transformer</code></li>
<li>Summary: <p>Joint-embedding based learning (e.g., SimCLR, MoCo, DINO) and
reconstruction-based learning (e.g., BEiT, SimMIM, MAE) are the two leading
paradigms for self-supervised learning of vision transformers, but they differ
substantially in their transfer performance. Here, we aim to explain these
differences by analyzing the impact of these objectives on the structure and
transferability of the learned representations. Our analysis reveals that
reconstruction-based learning features are significantly dissimilar to
joint-embedding based learning features and that models trained with similar
objectives learn similar features even across architectures. These differences
arise early in the network and are primarily driven by attention and
normalization layers. We find that joint-embedding features yield better linear
probe transfer for classification because the different objectives drive
different distributions of information and invariances in the learned
representation. These differences explain opposite trends in transfer
performance for downstream tasks that require spatial specificity in features.
Finally, we address how fine-tuning changes reconstructive representations to
enable better transfer, showing that fine-tuning re-organizes the information
to be more similar to pre-trained joint embedding models.
</p></li>
</ul>

<h3>Title: LEMaRT: Label-Efficient Masked Region Transform for Image Harmonization. (arXiv:2304.13166v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13166">http://arxiv.org/abs/2304.13166</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13166] LEMaRT: Label-Efficient Masked Region Transform for Image Harmonization](http://arxiv.org/abs/2304.13166) #transformer</code></li>
<li>Summary: <p>We present a simple yet effective self-supervised pre-training method for
image harmonization which can leverage large-scale unannotated image datasets.
To achieve this goal, we first generate pre-training data online with our
Label-Efficient Masked Region Transform (LEMaRT) pipeline. Given an image,
LEMaRT generates a foreground mask and then applies a set of transformations to
perturb various visual attributes, e.g., defocus blur, contrast, saturation, of
the region specified by the generated mask. We then pre-train image
harmonization models by recovering the original image from the perturbed image.
Secondly, we introduce an image harmonization model, namely SwinIH, by
retrofitting the Swin Transformer [27] with a combination of local and global
self-attention mechanisms. Pre-training SwinIH with LEMaRT results in a new
state of the art for image harmonization, while being label-efficient, i.e.,
consuming less annotated data for fine-tuning than existing methods. Notably,
on iHarmony4 dataset [8], SwinIH outperforms the state of the art, i.e., SCS-Co
[16] by a margin of 0.4 dB when it is fine-tuned on only 50% of the training
data, and by 1.0 dB when it is trained on the full training dataset.
</p></li>
</ul>

<h3>Title: StepFormer: Self-supervised Step Discovery and Localization in Instructional Videos. (arXiv:2304.13265v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13265">http://arxiv.org/abs/2304.13265</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13265] StepFormer: Self-supervised Step Discovery and Localization in Instructional Videos](http://arxiv.org/abs/2304.13265) #transformer</code></li>
<li>Summary: <p>Instructional videos are an important resource to learn procedural tasks from
human demonstrations. However, the instruction steps in such videos are
typically short and sparse, with most of the video being irrelevant to the
procedure. This motivates the need to temporally localize the instruction steps
in such videos, i.e. the task called key-step localization. Traditional methods
for key-step localization require video-level human annotations and thus do not
scale to large datasets. In this work, we tackle the problem with no human
supervision and introduce StepFormer, a self-supervised model that discovers
and localizes instruction steps in a video. StepFormer is a transformer decoder
that attends to the video with learnable queries, and produces a sequence of
slots capturing the key-steps in the video. We train our system on a large
dataset of instructional videos, using their automatically-generated subtitles
as the only source of supervision. In particular, we supervise our system with
a sequence of text narrations using an order-aware loss function that filters
out irrelevant phrases. We show that our model outperforms all previous
unsupervised and weakly-supervised approaches on step detection and
localization by a large margin on three challenging benchmarks. Moreover, our
model demonstrates an emergent property to solve zero-shot multi-step
localization and outperforms all relevant baselines at this task.
</p></li>
</ul>

<h3>Title: Learnable Ophthalmology SAM. (arXiv:2304.13425v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13425">http://arxiv.org/abs/2304.13425</a></li>
<li>Code URL: <a href="https://github.com/qsingle/learnablepromptsam">https://github.com/qsingle/learnablepromptsam</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13425] Learnable Ophthalmology SAM](http://arxiv.org/abs/2304.13425) #transformer</code></li>
<li>Summary: <p>Segmentation is vital for ophthalmology image analysis. But its various modal
images hinder most of the existing segmentation algorithms applications, as
they rely on training based on a large number of labels or hold weak
generalization ability. Based on Segment Anything (SAM), we propose a simple
but effective learnable prompt layer suitable for multiple target segmentation
in ophthalmology multi-modal images, named Learnable Ophthalmology Segment
Anything (SAM). The learnable prompt layer learns medical prior knowledge from
each transformer layer. During training, we only train the prompt layer and
task head based on a one-shot mechanism. We demonstrate the effectiveness of
our thought based on four medical segmentation tasks based on nine publicly
available datasets. Moreover, we only provide a new improvement thought for
applying the existing fundamental CV models in the medical field. Our codes are
available at \href{https://github.com/Qsingle/LearnablePromptSAM}{website}.
</p></li>
</ul>

<h3>Title: PVP: Pre-trained Visual Parameter-Efficient Tuning. (arXiv:2304.13639v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13639">http://arxiv.org/abs/2304.13639</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13639] PVP: Pre-trained Visual Parameter-Efficient Tuning](http://arxiv.org/abs/2304.13639) #transformer</code></li>
<li>Summary: <p>Large-scale pre-trained transformers have demonstrated remarkable success in
various computer vision tasks. However, it is still highly challenging to fully
fine-tune these models for downstream tasks due to their high computational and
storage costs. Recently, Parameter-Efficient Tuning (PETuning) techniques,
e.g., Visual Prompt Tuning (VPT) and Low-Rank Adaptation (LoRA), have
significantly reduced the computation and storage cost by inserting lightweight
prompt modules into the pre-trained models and tuning these prompt modules with
a small number of trainable parameters, while keeping the transformer backbone
frozen. Although only a few parameters need to be adjusted, most PETuning
methods still require a significant amount of downstream task training data to
achieve good results. The performance is inadequate on low-data regimes,
especially when there are only one or two examples per class. To this end, we
first empirically identify the poor performance is mainly due to the
inappropriate way of initializing prompt modules, which has also been verified
in the pre-trained language models. Next, we propose a Pre-trained Visual
Parameter-efficient (PVP) Tuning framework, which pre-trains the
parameter-efficient tuning modules first and then leverages the pre-trained
modules along with the pre-trained transformer backbone to perform
parameter-efficient tuning on downstream tasks. Experiment results on five
Fine-Grained Visual Classification (FGVC) and VTAB-1k datasets demonstrate that
our proposed method significantly outperforms state-of-the-art PETuning
methods.
</p></li>
</ul>

<h3>Title: UniNeXt: Exploring A Unified Architecture for Vision Recognition. (arXiv:2304.13700v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13700">http://arxiv.org/abs/2304.13700</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13700] UniNeXt: Exploring A Unified Architecture for Vision Recognition](http://arxiv.org/abs/2304.13700) #transformer</code></li>
<li>Summary: <p>Vision Transformers have shown great potential in computer vision tasks. Most
recent works have focused on elaborating the spatial token mixer for
performance gains. However, we observe that a well-designed general
architecture can significantly improve the performance of the entire backbone,
regardless of which spatial token mixer is equipped. In this paper, we propose
UniNeXt, an improved general architecture for the vision backbone. To verify
its effectiveness, we instantiate the spatial token mixer with various typical
and modern designs, including both convolution and attention modules. Compared
with the architecture in which they are first proposed, our UniNeXt
architecture can steadily boost the performance of all the spatial token
mixers, and narrows the performance gap among them. Surprisingly, our UniNeXt
equipped with naive local window attention even outperforms the previous
state-of-the-art. Interestingly, the ranking of these spatial token mixers also
changes under our UniNeXt, suggesting that an excellent spatial token mixer may
be stifled due to a suboptimal general architecture, which further shows the
importance of the study on the general architecture of vision backbone. All
models and codes will be publicly available.
</p></li>
</ul>

<h3>Title: Pretrain on just structure: Understanding linguistic inductive biases using transfer learning. (arXiv:2304.13060v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13060">http://arxiv.org/abs/2304.13060</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13060] Pretrain on just structure: Understanding linguistic inductive biases using transfer learning](http://arxiv.org/abs/2304.13060) #transformer</code></li>
<li>Summary: <p>Both humans and transformer language models are able to learn language
without explicit structural supervision. What inductive learning biases make
this learning possible? In this study, we examine the effect of different
inductive learning biases by predisposing language models with structural
biases through pretraining on artificial structured data, and then evaluating
by fine-tuning on English. Our experimental setup gives us the ability to
actively control the inductive bias of language models. With our experiments,
we investigate the comparative success of three types of inductive bias: 1) an
inductive bias for recursive, hierarchical processing 2) an inductive bias for
unrestricted token-token dependencies that can't be modeled by context-free
grammars, and 3) an inductive bias for a Zipfian power-law vocabulary
distribution. We show that complex token-token interactions form the best
inductive biases, and that this is strongest in the non-context-free case. We
also show that a Zipfian vocabulary distribution forms a good inductive bias
independently from grammatical structure. Our study leverages the capabilities
of transformer models to run controlled language learning experiments that are
not possible to run in humans, and surfaces hypotheses about the structures
that facilitate language learning in both humans and machines.
</p></li>
</ul>

<h3>Title: The Closeness of In-Context Learning and Weight Shifting for Softmax Regression. (arXiv:2304.13276v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13276">http://arxiv.org/abs/2304.13276</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13276] The Closeness of In-Context Learning and Weight Shifting for Softmax Regression](http://arxiv.org/abs/2304.13276) #transformer</code></li>
<li>Summary: <p>Large language models (LLMs) are known for their exceptional performance in
natural language processing, making them highly effective in many human
life-related or even job-related tasks. The attention mechanism in the
Transformer architecture is a critical component of LLMs, as it allows the
model to selectively focus on specific input parts. The softmax unit, which is
a key part of the attention mechanism, normalizes the attention scores. Hence,
the performance of LLMs in various NLP tasks depends significantly on the
crucial role played by the attention mechanism with the softmax unit.
</p></li>
</ul>

<p>In-context learning, as one of the celebrated abilities of recent LLMs, is an
important concept in querying LLMs such as ChatGPT. Without further parameter
updates, Transformers can learn to predict based on few in-context examples.
However, the reason why Transformers becomes in-context learners is not well
understood. Recently, several works [ASA+22,GTLV22,ONR+22] have studied the
in-context learning from a mathematical perspective based on a linear
regression formulation $\min_x\| Ax - b \|_2$, which show Transformers'
capability of learning linear functions in context.
</p>
<p>In this work, we study the in-context learning based on a softmax regression
formulation $\min_{x} \| \langle \exp(Ax), {\bf 1}_n \rangle^{-1} \exp(Ax) - b
\|_2$ of Transformer's attention mechanism. We show the upper bounds of the
data transformations induced by a single self-attention layer and by
gradient-descent on a $\ell_2$ regression loss for softmax prediction function,
which imply that when training self-attention-only Transformers for fundamental
regression tasks, the models learned by gradient-descent and Transformers show
great similarity.
</p>

<h3>Title: Impact of Position Bias on Language Models in Token Classification. (arXiv:2304.13567v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13567">http://arxiv.org/abs/2304.13567</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13567] Impact of Position Bias on Language Models in Token Classification](http://arxiv.org/abs/2304.13567) #transformer</code></li>
<li>Summary: <p>Language Models (LMs) have shown state-of-the-art performance in Natural
Language Processing (NLP) tasks. Downstream tasks such as Named Entity
Recognition (NER) or Part-of-Speech (POS) tagging are known to suffer from data
imbalance issues, specifically in terms of the ratio of positive to negative
examples, and class imbalance. In this paper, we investigate an additional
specific issue for language models, namely the position bias of positive
examples in token classification tasks. Therefore, we conduct an in-depth
evaluation of the impact of position bias on the performance of LMs when
fine-tuned on Token Classification benchmarks. Our study includes CoNLL03 and
OntoNote5.0 for NER, English Tree Bank UD_en and TweeBank for POS tagging. We
propose an evaluation approach to investigate position bias in Transformer
models. We show that encoders like BERT, ERNIE, ELECTRA, and decoders such as
GPT2 and BLOOM can suffer from this bias with an average drop of 3\% and 9\% in
their performance. To mitigate this effect, we propose two methods: Random
Position Shifting and Context Perturbation, that we apply on batches during the
training process. The results show an improvement of $\approx$ 2\% in the
performance of the model on CoNLL03, UD_en, and TweeBank.
</p></li>
</ul>

<h3>Title: Tensor Decomposition for Model Reduction in Neural Networks: A Review. (arXiv:2304.13539v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13539">http://arxiv.org/abs/2304.13539</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13539] Tensor Decomposition for Model Reduction in Neural Networks: A Review](http://arxiv.org/abs/2304.13539) #transformer</code></li>
<li>Summary: <p>Modern neural networks have revolutionized the fields of computer vision (CV)
and Natural Language Processing (NLP). They are widely used for solving complex
CV tasks and NLP tasks such as image classification, image generation, and
machine translation. Most state-of-the-art neural networks are
over-parameterized and require a high computational cost. One straightforward
solution is to replace the layers of the networks with their low-rank tensor
approximations using different tensor decomposition methods. This paper reviews
six tensor decomposition methods and illustrates their ability to compress
model parameters of convolutional neural networks (CNNs), recurrent neural
networks (RNNs) and Transformers. The accuracy of some compressed models can be
higher than the original versions. Evaluations indicate that tensor
decompositions can achieve significant reductions in model size, run-time and
energy consumption, and are well suited for implementing neural networks on
edge devices.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: LumiGAN: Unconditional Generation of Relightable 3D Human Faces. (arXiv:2304.13153v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13153">http://arxiv.org/abs/2304.13153</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13153] LumiGAN: Unconditional Generation of Relightable 3D Human Faces](http://arxiv.org/abs/2304.13153) #generative</code></li>
<li>Summary: <p>Unsupervised learning of 3D human faces from unstructured 2D image data is an
active research area. While recent works have achieved an impressive level of
photorealism, they commonly lack control of lighting, which prevents the
generated assets from being deployed in novel environments. To this end, we
introduce LumiGAN, an unconditional Generative Adversarial Network (GAN) for 3D
human faces with a physically based lighting module that enables relighting
under novel illumination at inference time. Unlike prior work, LumiGAN can
create realistic shadow effects using an efficient visibility formulation that
is learned in a self-supervised manner. LumiGAN generates plausible physical
properties for relightable faces, including surface normals, diffuse albedo,
and specular tint without any ground truth data. In addition to relightability,
we demonstrate significantly improved geometry generation compared to
state-of-the-art non-relightable 3D GANs and notably better photorealism than
existing relightable GANs.
</p></li>
</ul>

<h3>Title: Controllable Image Generation via Collage Representations. (arXiv:2304.13722v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13722">http://arxiv.org/abs/2304.13722</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13722] Controllable Image Generation via Collage Representations](http://arxiv.org/abs/2304.13722) #generative</code></li>
<li>Summary: <p>Recent advances in conditional generative image models have enabled
impressive results. On the one hand, text-based conditional models have
achieved remarkable generation quality, by leveraging large-scale datasets of
image-text pairs. To enable fine-grained controllability, however, text-based
models require long prompts, whose details may be ignored by the model. On the
other hand, layout-based conditional models have also witnessed significant
advances. These models rely on bounding boxes or segmentation maps for precise
spatial conditioning in combination with coarse semantic labels. The semantic
labels, however, cannot be used to express detailed appearance characteristics.
In this paper, we approach fine-grained scene controllability through image
collages which allow a rich visual description of the desired scene as well as
the appearance and location of the objects therein, without the need of class
nor attribute labels. We introduce "mixing and matching scenes" (M&amp;Ms), an
approach that consists of an adversarially trained generative image model which
is conditioned on appearance features and spatial positions of the different
elements in a collage, and integrates these into a coherent image. We train our
model on the OpenImages (OI) dataset and evaluate it on collages derived from
OI and MS-COCO datasets. Our experiments on the OI dataset show that M&amp;Ms
outperforms baselines in terms of fine-grained scene controllability while
being very competitive in terms of image quality and sample diversity. On the
MS-COCO dataset, we highlight the generalization ability of our model by
outperforming DALL-E in terms of the zero-shot FID metric, despite using two
magnitudes fewer parameters and data. Collage based generative models have the
potential to advance content creation in an efficient and effective way as they
are intuitive to use and yield high quality generations.
</p></li>
</ul>

<h3>Title: Directed Chain Generative Adversarial Networks. (arXiv:2304.13131v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13131">http://arxiv.org/abs/2304.13131</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13131] Directed Chain Generative Adversarial Networks](http://arxiv.org/abs/2304.13131) #generative</code></li>
<li>Summary: <p>Real-world data can be multimodal distributed, e.g., data describing the
opinion divergence in a community, the interspike interval distribution of
neurons, and the oscillators natural frequencies. Generating multimodal
distributed real-world data has become a challenge to existing generative
adversarial networks (GANs). For example, neural stochastic differential
equations (Neural SDEs), treated as infinite-dimensional GANs, have
demonstrated successful performance mainly in generating unimodal time series
data. In this paper, we propose a novel time series generator, named directed
chain GANs (DC-GANs), which inserts a time series dataset (called a
neighborhood process of the directed chain or input) into the drift and
diffusion coefficients of the directed chain SDEs with distributional
constraints. DC-GANs can generate new time series of the same distribution as
the neighborhood process, and the neighborhood process will provide the key
step in learning and generating multimodal distributed time series. The
proposed DC-GANs are examined on four datasets, including two stochastic models
from social sciences and computational neuroscience, and two real-world
datasets on stock prices and energy consumption. To our best knowledge, DC-GANs
are the first work that can generate multimodal time series data and
consistently outperforms state-of-the-art benchmarks with respect to measures
of distribution, data similarity, and predictive ability.
</p></li>
</ul>

<h3>Title: Score-based Generative Modeling Through Backward Stochastic Differential Equations: Inversion and Generation. (arXiv:2304.13224v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13224">http://arxiv.org/abs/2304.13224</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13224] Score-based Generative Modeling Through Backward Stochastic Differential Equations: Inversion and Generation](http://arxiv.org/abs/2304.13224) #generative</code></li>
<li>Summary: <p>The proposed BSDE-based diffusion model represents a novel approach to
diffusion modeling, which extends the application of stochastic differential
equations (SDEs) in machine learning. Unlike traditional SDE-based diffusion
models, our model can determine the initial conditions necessary to reach a
desired terminal distribution by adapting an existing score function. We
demonstrate the theoretical guarantees of the model, the benefits of using
Lipschitz networks for score matching, and its potential applications in
various areas such as diffusion inversion, conditional diffusion, and
uncertainty quantification. Our work represents a contribution to the field of
score-based generative learning and offers a promising direction for solving
real-world problems.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Zero-Shot Slot and Intent Detection in Low-Resource Languages. (arXiv:2304.13292v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13292">http://arxiv.org/abs/2304.13292</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13292] Zero-Shot Slot and Intent Detection in Low-Resource Languages](http://arxiv.org/abs/2304.13292) #large language model</code></li>
<li>Summary: <p>Intent detection and slot filling are critical tasks in spoken and natural
language understanding for task-oriented dialog systems. In this work we
describe our participation in the slot and intent detection for low-resource
language varieties (SID4LR; Aepli et al. (2023)). We investigate the slot and
intent detection (SID) tasks using a wide range of models and settings. Given
the recent success of multitask-prompted finetuning of large language models,
we also test the generalization capability of the recent encoder-decoder model
mT0 (Muennighoff et al., 2022) on new tasks (i.e., SID) in languages they have
never intentionally seen. We show that our best model outperforms the baseline
by a large margin (up to +30 F1 points) in both SID tasks
</p></li>
</ul>

<h3>Title: A Case-Based Reasoning Framework for Adaptive Prompting in Cross-Domain Text-to-SQL. (arXiv:2304.13301v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13301">http://arxiv.org/abs/2304.13301</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13301] A Case-Based Reasoning Framework for Adaptive Prompting in Cross-Domain Text-to-SQL](http://arxiv.org/abs/2304.13301) #large language model</code></li>
<li>Summary: <p>Recent advancements in Large Language Models (LLMs), such as Codex, ChatGPT
and GPT-4 have significantly impacted the AI community, including Text-to-SQL
tasks. Some evaluations and analyses on LLMs show their potential to generate
SQL queries but they point out poorly designed prompts (e.g. simplistic
construction or random sampling) limit LLMs' performance and may cause
unnecessary or irrelevant outputs. To address these issues, we propose
CBR-ApSQL, a Case-Based Reasoning (CBR)-based framework combined with GPT-3.5
for precise control over case-relevant and case-irrelevant knowledge in
Text-to-SQL tasks. We design adaptive prompts for flexibly adjusting inputs for
GPT-3.5, which involves (1) adaptively retrieving cases according to the
question intention by de-semantizing the input question, and (2) an adaptive
fallback mechanism to ensure the informativeness of the prompt, as well as the
relevance between cases and the prompt. In the de-semanticization phase, we
designed Semantic Domain Relevance Evaluator(SDRE), combined with Poincar\'e
detector(mining implicit semantics in hyperbolic space), TextAlign(discovering
explicit matches), and Positector (part-of-speech detector). SDRE semantically
and syntactically generates in-context exemplar annotations for the new case.
On the three cross-domain datasets, our framework outperforms the
state-of-the-art(SOTA) model in execution accuracy by 3.7\%, 2.5\%, and 8.2\%,
respectively.
</p></li>
</ul>

<h3>Title: Multidimensional Evaluation for Text Style Transfer Using ChatGPT. (arXiv:2304.13462v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13462">http://arxiv.org/abs/2304.13462</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13462] Multidimensional Evaluation for Text Style Transfer Using ChatGPT](http://arxiv.org/abs/2304.13462) #large language model</code></li>
<li>Summary: <p>We investigate the potential of ChatGPT as a multidimensional evaluator for
the task of \emph{Text Style Transfer}, alongside, and in comparison to,
existing automatic metrics as well as human judgements. We focus on a zero-shot
setting, i.e. prompting ChatGPT with specific task instructions, and test its
performance on three commonly-used dimensions of text style transfer
evaluation: style strength, content preservation, and fluency. We perform a
comprehensive correlation analysis for two transfer directions (and overall) at
different levels. Compared to existing automatic metrics, ChatGPT achieves
competitive correlations with human judgments. These preliminary results are
expected to provide a first glimpse into the role of large language models in
the multidimensional evaluation of stylized text generation.
</p></li>
</ul>

<h3>Title: Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond. (arXiv:2304.13712v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13712">http://arxiv.org/abs/2304.13712</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13712] Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond](http://arxiv.org/abs/2304.13712) #large language model</code></li>
<li>Summary: <p>This paper presents a comprehensive and practical guide for practitioners and
end-users working with Large Language Models (LLMs) in their downstream natural
language processing (NLP) tasks. We provide discussions and insights into the
usage of LLMs from the perspectives of models, data, and downstream tasks.
Firstly, we offer an introduction and brief summary of current GPT- and
BERT-style LLMs. Then, we discuss the influence of pre-training data, training
data, and test data. Most importantly, we provide a detailed discussion about
the use and non-use cases of large language models for various natural language
processing tasks, such as knowledge-intensive tasks, traditional natural
language understanding tasks, natural language generation tasks, emergent
abilities, and considerations for specific tasks.We present various use cases
and non-use cases to illustrate the practical applications and limitations of
LLMs in real-world scenarios. We also try to understand the importance of data
and the specific challenges associated with each NLP task. Furthermore, we
explore the impact of spurious biases on LLMs and delve into other essential
considerations, such as efficiency, cost, and latency, to ensure a
comprehensive understanding of deploying LLMs in practice. This comprehensive
guide aims to provide researchers and practitioners with valuable insights and
best practices for working with LLMs, thereby enabling the successful
implementation of these models in a wide range of NLP tasks. A curated list of
practical guide resources of LLMs, regularly updated, can be found at
\url{https://github.com/Mooler0410/LLMsPracticalGuide}.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Exploiting CNNs for Semantic Segmentation with Pascal VOC. (arXiv:2304.13216v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13216">http://arxiv.org/abs/2304.13216</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13216] Exploiting CNNs for Semantic Segmentation with Pascal VOC](http://arxiv.org/abs/2304.13216) #segmentation</code></li>
<li>Summary: <p>In this paper, we present a comprehensive study on semantic segmentation with
the Pascal VOC dataset. Here, we have to label each pixel with a class which in
turn segments the entire image based on the objects/entities present. To tackle
this, we firstly use a Fully Convolution Network (FCN) baseline which gave
71.31% pixel accuracy and 0.0527 mean IoU. We analyze its performance and
working and subsequently address the issues in the baseline with three
improvements: a) cosine annealing learning rate scheduler(pixel accuracy:
72.86%, IoU: 0.0529), b) data augmentation(pixel accuracy: 69.88%, IoU: 0.0585)
c) class imbalance weights(pixel accuracy: 68.98%, IoU: 0.0596). Apart from
these changes in training pipeline, we also explore three different
architectures: a) Our proposed model -- Advanced FCN (pixel accuracy: 67.20%,
IoU: 0.0602) b) Transfer Learning with ResNet (Best performance) (pixel
accuracy: 71.33%, IoU: 0.0926 ) c) U-Net(pixel accuracy: 72.15%, IoU: 0.0649).
We observe that the improvements help in greatly improving the performance, as
reflected both, in metrics and segmentation maps. Interestingly, we observe
that among the improvements, dataset augmentation has the greatest
contribution. Also, note that transfer learning model performs the best on the
pascal dataset. We analyse the performance of these using loss, accuracy and
IoU plots along with segmentation maps, which help us draw valuable insights
about the working of the models.
</p></li>
</ul>

<h3>Title: Compensation Learning in Semantic Segmentation. (arXiv:2304.13428v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13428">http://arxiv.org/abs/2304.13428</a></li>
<li>Code URL: <a href="https://github.com/tnt-LUH/compensation_learning">https://github.com/tnt-LUH/compensation_learning</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13428] Compensation Learning in Semantic Segmentation](http://arxiv.org/abs/2304.13428) #segmentation</code></li>
<li>Summary: <p>Label noise and ambiguities between similar classes are challenging problems
in developing new models and annotating new data for semantic segmentation. In
this paper, we propose Compensation Learning in Semantic Segmentation, a
framework to identify and compensate ambiguities as well as label noise. More
specifically, we add a ground truth depending and globally learned bias to the
classification logits and introduce a novel uncertainty branch for neural
networks to induce the compensation bias only to relevant regions. Our method
is employed into state-of-the-art segmentation frameworks and several
experiments demonstrate that our proposed compensation learns inter-class
relations that allow global identification of challenging ambiguities as well
as the exact localization of subsequent label noise. Additionally, it enlarges
robustness against label noise during training and allows target-oriented
manipulation during inference. We evaluate the proposed method on %the widely
used datasets Cityscapes, KITTI-STEP, ADE20k, and COCO-stuff10k.
</p></li>
</ul>

<h3>Title: Effect of latent space distribution on the segmentation of images with multiple annotations. (arXiv:2304.13476v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13476">http://arxiv.org/abs/2304.13476</a></li>
<li>Code URL: <a href="https://github.com/ishaanb92/generalizedprobabilisticunet">https://github.com/ishaanb92/generalizedprobabilisticunet</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13476] Effect of latent space distribution on the segmentation of images with multiple annotations](http://arxiv.org/abs/2304.13476) #segmentation</code></li>
<li>Summary: <p>We propose the Generalized Probabilistic U-Net, which extends the
Probabilistic U-Net by allowing more general forms of the Gaussian distribution
as the latent space distribution that can better approximate the uncertainty in
the reference segmentations. We study the effect the choice of latent space
distribution has on capturing the variation in the reference segmentations for
lung tumors and white matter hyperintensities in the brain. We show that the
choice of distribution affects the sample diversity of the predictions and
their overlap with respect to the reference segmentations. We have made our
implementation available at
https://github.com/ishaanb92/GeneralizedProbabilisticUNet
</p></li>
</ul>

<h3>Title: EasyPortrait - Face Parsing and Portrait Segmentation Dataset. (arXiv:2304.13509v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13509">http://arxiv.org/abs/2304.13509</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13509] EasyPortrait - Face Parsing and Portrait Segmentation Dataset](http://arxiv.org/abs/2304.13509) #segmentation</code></li>
<li>Summary: <p>Recently, due to COVID-19 and the growing demand for remote work, video
conferencing apps have become especially widespread. The most valuable features
of video chats are real-time background removal and face beautification. While
solving these tasks, computer vision researchers face the problem of having
relevant data for the training stage. There is no large dataset with
high-quality labeled and diverse images of people in front of a laptop or
smartphone camera to train a lightweight model without additional approaches.
To boost the progress in this area, we provide a new image dataset,
EasyPortrait, for portrait segmentation and face parsing tasks. It contains
20,000 primarily indoor photos of 8,377 unique users, and fine-grained
segmentation masks separated into 9 classes. Images are collected and labeled
from crowdsourcing platforms. Unlike most face parsing datasets, in
EasyPortrait, the beard is not considered part of the skin mask, and the inside
area of the mouth is separated from the teeth. These features allow using
EasyPortrait for skin enhancement and teeth whitening tasks. This paper
describes the pipeline for creating a large-scale and clean image segmentation
dataset using crowdsourcing platforms without additional synthetic data.
Moreover, we trained several models on EasyPortrait and showed experimental
results. Proposed dataset and trained models are publicly available.
</p></li>
</ul>

<h3>Title: Cluster Entropy: Active Domain Adaptation in Pathological Image Segmentation. (arXiv:2304.13513v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13513">http://arxiv.org/abs/2304.13513</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13513] Cluster Entropy: Active Domain Adaptation in Pathological Image Segmentation](http://arxiv.org/abs/2304.13513) #segmentation</code></li>
<li>Summary: <p>The domain shift in pathological segmentation is an important problem, where
a network trained by a source domain (collected at a specific hospital) does
not work well in the target domain (from different hospitals) due to the
different image features. Due to the problems of class imbalance and different
class prior of pathology, typical unsupervised domain adaptation methods do not
work well by aligning the distribution of source domain and target domain. In
this paper, we propose a cluster entropy for selecting an effective whole slide
image (WSI) that is used for semi-supervised domain adaptation. This approach
can measure how the image features of the WSI cover the entire distribution of
the target domain by calculating the entropy of each cluster and can
significantly improve the performance of domain adaptation. Our approach
achieved competitive results against the prior arts on datasets collected from
two hospitals.
</p></li>
</ul>

<h3>Title: Domain Adaptive and Generalizable Network Architectures and Training Strategies for Semantic Image Segmentation. (arXiv:2304.13615v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13615">http://arxiv.org/abs/2304.13615</a></li>
<li>Code URL: <a href="https://github.com/lhoyer/hrda">https://github.com/lhoyer/hrda</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13615] Domain Adaptive and Generalizable Network Architectures and Training Strategies for Semantic Image Segmentation](http://arxiv.org/abs/2304.13615) #segmentation</code></li>
<li>Summary: <p>Unsupervised domain adaptation (UDA) and domain generalization (DG) enable
machine learning models trained on a source domain to perform well on unlabeled
or even unseen target domains. As previous UDA&amp;DG semantic segmentation methods
are mostly based on outdated networks, we benchmark more recent architectures,
reveal the potential of Transformers, and design the DAFormer network tailored
for UDA&amp;DG. It is enabled by three training strategies to avoid overfitting to
the source domain: While (1) Rare Class Sampling mitigates the bias toward
common source domain classes, (2) a Thing-Class ImageNet Feature Distance and
(3) a learning rate warmup promote feature transfer from ImageNet pretraining.
As UDA&amp;DG are usually GPU memory intensive, most previous methods downscale or
crop images. However, low-resolution predictions often fail to preserve fine
details while models trained with cropped images fall short in capturing
long-range, domain-robust context information. Therefore, we propose HRDA, a
multi-resolution framework for UDA&amp;DG, that combines the strengths of small
high-resolution crops to preserve fine segmentation details and large
low-resolution crops to capture long-range context dependencies with a learned
scale attention. DAFormer and HRDA significantly improve the state-of-the-art
UDA&amp;DG by more than 10 mIoU on 5 different benchmarks. The implementation is
available at https://github.com/lhoyer/HRDA.
</p></li>
</ul>

<h3>Title: FVP: Fourier Visual Prompting for Source-Free Unsupervised Domain Adaptation of Medical Image Segmentation. (arXiv:2304.13672v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2304.13672">http://arxiv.org/abs/2304.13672</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2304.13672] FVP: Fourier Visual Prompting for Source-Free Unsupervised Domain Adaptation of Medical Image Segmentation](http://arxiv.org/abs/2304.13672) #segmentation</code></li>
<li>Summary: <p>Medical image segmentation methods normally perform poorly when there is a
domain shift between training and testing data. Unsupervised Domain Adaptation
(UDA) addresses the domain shift problem by training the model using both
labeled data from the source domain and unlabeled data from the target domain.
Source-Free UDA (SFUDA) was recently proposed for UDA without requiring the
source data during the adaptation, due to data privacy or data transmission
issues, which normally adapts the pre-trained deep model in the testing stage.
However, in real clinical scenarios of medical image segmentation, the trained
model is normally frozen in the testing stage. In this paper, we propose
Fourier Visual Prompting (FVP) for SFUDA of medical image segmentation.
Inspired by prompting learning in natural language processing, FVP steers the
frozen pre-trained model to perform well in the target domain by adding a
visual prompt to the input target data. In FVP, the visual prompt is
parameterized using only a small amount of low-frequency learnable parameters
in the input frequency space, and is learned by minimizing the segmentation
loss between the predicted segmentation of the prompted target image and
reliable pseudo segmentation label of the target image under the frozen model.
To our knowledge, FVP is the first work to apply visual prompts to SFUDA for
medical image segmentation. The proposed FVP is validated using three public
datasets, and experiments demonstrate that FVP yields better segmentation
results, compared with various existing methods.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
