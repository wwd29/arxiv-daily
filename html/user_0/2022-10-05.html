<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Blockchain-Based Decentralized Knowledge Marketplace Using Active Inference. (arXiv:2210.01688v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01688">http://arxiv.org/abs/2210.01688</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01688] Blockchain-Based Decentralized Knowledge Marketplace Using Active Inference](http://arxiv.org/abs/2210.01688)</code></li>
<li>Summary: <p>A knowledge market can be described as a type of market where there is a
consistent supply of data to satisfy the demand for information and is
responsible for the mapping of potential problem solvers with the entities
which need these solutions. It is possible to define them as value-exchange
systems in which the dynamic features of the creation and exchange of
intellectual assets serve as the fundamental drivers of the frequency, nature,
and outcomes of interactions among various stakeholders. Furthermore, the
provision of financial backing for research is an essential component in the
process of developing a knowledge market that is capable of enduring over time,
and it is also an essential driver of the progression of scientific
investigation. This paper underlines flaws associated with the conventional
knowledge-based market, including but not limited to excessive financing
concentration, ineffective information exchange, a lack of security, mapping of
entities, etc. The authors present a decentralized framework for the knowledge
marketplace incorporating technologies such as blockchain, active inference,
zero-knowledge proof, etc. The proposed decentralized framework provides not
only an efficient mapping mechanism to map entities in the marketplace but also
a more secure and controlled way to share knowledge and services among various
stakeholders.
</p></li>
</ul>

<h3>Title: SecureFedYJ: a safe feature Gaussianization protocol for Federated Learning. (arXiv:2210.01639v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01639">http://arxiv.org/abs/2210.01639</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01639] SecureFedYJ: a safe feature Gaussianization protocol for Federated Learning](http://arxiv.org/abs/2210.01639)</code></li>
<li>Summary: <p>The Yeo-Johnson (YJ) transformation is a standard parametrized per-feature
unidimensional transformation often used to Gaussianize features in machine
learning. In this paper, we investigate the problem of applying the YJ
transformation in a cross-silo Federated Learning setting under privacy
constraints. For the first time, we prove that the YJ negative log-likelihood
is in fact convex, which allows us to optimize it with exponential search. We
numerically show that the resulting algorithm is more stable than the
state-of-the-art approach based on the Brent minimization method. Building on
this simple algorithm and Secure Multiparty Computation routines, we propose
SecureFedYJ, a federated algorithm that performs a pooled-equivalent YJ
transformation without leaking more information than the final fitted
parameters do. Quantitative experiments on real data demonstrate that, in
addition to being secure, our approach reliably normalizes features across
silos as well as if data were pooled, making it a viable approach for safe
federated feature Gaussianization.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Enriching Vulnerability Reports Through Automated and Augmented Description Summarization. (arXiv:2210.01260v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01260">http://arxiv.org/abs/2210.01260</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01260] Enriching Vulnerability Reports Through Automated and Augmented Description Summarization](http://arxiv.org/abs/2210.01260)</code></li>
<li>Summary: <p>Security incidents and data breaches are increasing rapidly, and only a
fraction of them is being reported. Public vulnerability databases, e.g.,
national vulnerability database (NVD) and common vulnerability and exposure
(CVE), have been leading the effort in documenting vulnerabilities and sharing
them to aid defenses. Both are known for many issues, including brief
vulnerability descriptions. Those descriptions play an important role in
communicating the vulnerability information to security analysts in order to
develop the appropriate countermeasure. Many resources provide additional
information about vulnerabilities, however, they are not utilized to boost
public repositories. In this paper, we devise a pipeline to augment
vulnerability description through third party reference (hyperlink) scrapping.
To normalize the description, we build a natural language summarization
pipeline utilizing a pretrained language model that is fine-tuned using labeled
instances and evaluate its performance against both human evaluation (golden
standard) and computational metrics, showing initial promising results in terms
of summary fluency, completeness, correctness, and understanding.
</p></li>
</ul>

<h3>Title: Enabling a Zero Trust Architecture in a 5G-enabled Smart Grid. (arXiv:2210.01739v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01739">http://arxiv.org/abs/2210.01739</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01739] Enabling a Zero Trust Architecture in a 5G-enabled Smart Grid](http://arxiv.org/abs/2210.01739)</code></li>
<li>Summary: <p>One of the most promising applications of the IoT is the Smart Grid (SG).
Integrating SG's data communications network into the power grid allows
gathering and analyzing information from power lines, distribution power
stations, and end users. A smart grid (SG) requires a prompt and dependable
connection to provide real-time monitoring through the IoT. Hence 5G could be
considered a catalyst for upgrading the existing power grid systems.
Nonetheless, the additional attack surface of information infrastructure has
been brought about by the widespread adoption of ubiquitous connectivity in 5G,
to which the typical information security system in the smart grid cannot
respond promptly. Therefore, guaranteeing the Privacy and Security of a network
in a threatening, ever-changing environment requires groundbreaking
architectures that go well beyond the limitations of traditional, static
security measures. With "Continuous Identity Authentication and Dynamic Access
Control" as its foundation, this article analyzes the Zero Trust (ZT)
architecture specific to the power system of IoT and uses that knowledge to
develop a security protection architecture.
</p></li>
</ul>

<h3>Title: Lightweight Strategy for XOR PUFs as Security Primitives for Resource-constrained IoT device. (arXiv:2210.01749v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01749">http://arxiv.org/abs/2210.01749</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01749] Lightweight Strategy for XOR PUFs as Security Primitives for Resource-constrained IoT device](http://arxiv.org/abs/2210.01749)</code></li>
<li>Summary: <p>Physical Unclonable Functions (PUFs) are promising security primitives for
resource-constrained IoT devices. And the XOR Arbiter PUF (XOR-PUF) is one of
the most studied PUFs, out of an effort to improve the resistance against
machine learning attacks of probably the most lightweight delay-based PUFs -
the Arbiter PUFs. However, recent attack studies reveal that even XOR-PUFs with
large XOR sizes are still not safe against machine learning attacks. Increasing
PUF stages or components and using different challenges for different
components are two ways to improve the security of APUF-based PUFs, but more
stages or components lead to more hardware cost and higher operation power, and
different challenges for different components require the transmission of more
bits during operations, which also leads to higher power consumption. In this
paper, we present a strategy that combines the choice of XOR Arbiter PUF
(XOR-PUF) architecture parameters with the way XOR-PUFs are used to achieve
lightweights in hardware cost and energy consumption as well as security
against machine learning attacks. Experimental evaluations show that with the
proposed strategy, highly lightweight component-differentially challenged
XOR-PUFs can withstand the most powerful machine learning attacks developed so
far and maintain excellent intra-device and inter-device performance, rendering
this strategy a potential blueprint for the fabrication and use of XOR-PUFs for
resource-constrained IoT applications.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Knowledge Unlearning for Mitigating Privacy Risks in Language Models. (arXiv:2210.01504v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01504">http://arxiv.org/abs/2210.01504</a></li>
<li>Code URL: <a href="https://github.com/joeljang/knowledge-unlearning">https://github.com/joeljang/knowledge-unlearning</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01504] Knowledge Unlearning for Mitigating Privacy Risks in Language Models](http://arxiv.org/abs/2210.01504)</code></li>
<li>Summary: <p>Pretrained Language Models (LMs) memorize a vast amount of knowledge during
initial pretraining, including information that may violate the privacy of
personal lives and identities. Previous work addressing privacy issues for
language models has mostly focused on data preprocessing and differential
privacy methods, both requiring re-training the underlying LM. We propose
knowledge unlearning as an alternative method to reduce privacy risks for LMs
post hoc. We show that simply applying the unlikelihood training objective to
target token sequences is effective at forgetting them with little to no
degradation of general language modeling performances; it sometimes even
substantially improves the underlying LM with just a few iterations. We also
find that sequential unlearning is better than trying to unlearn all the data
at once and that unlearning is highly dependent on which kind of data (domain)
is forgotten. By showing comparisons with a previous data preprocessing method
known to mitigate privacy risks for LMs, we show that unlearning can give a
stronger empirical privacy guarantee in scenarios where the data vulnerable to
extraction attacks are known a priori while being orders of magnitude more
computationally efficient. We release the code and dataset needed to replicate
our results at https://github.com/joeljang/knowledge-unlearning .
</p></li>
</ul>

<h3>Title: Privacy-Preserving Link Prediction. (arXiv:2210.01297v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01297">http://arxiv.org/abs/2210.01297</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01297] Privacy-Preserving Link Prediction](http://arxiv.org/abs/2210.01297)</code></li>
<li>Summary: <p>Consider two data holders, ABC and XYZ, with graph data (e.g., social
networks, e-commerce, telecommunication, and bio-informatics). ABC can see that
node A is linked to node B, and XYZ can see node B is linked to node C. Node B
is the common neighbour of A and C but neither network can discover this fact
on their own. In this paper, we provide a two party computation that ABC and
XYZ can run to discover the common neighbours in the union of their graph data,
however neither party has to reveal their plaintext graph to the other. Based
on private set intersection, we implement our solution, provide measurements,
and quantify partial leaks of privacy. We also propose a heavyweight solution
that leaks zero information based on additively homomorphic encryption.
</p></li>
</ul>

<h3>Title: Certified Data Removal in Sum-Product Networks. (arXiv:2210.01451v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01451">http://arxiv.org/abs/2210.01451</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01451] Certified Data Removal in Sum-Product Networks](http://arxiv.org/abs/2210.01451)</code></li>
<li>Summary: <p>Data protection regulations like the GDPR or the California Consumer Privacy
Act give users more control over the data that is collected about them.
Deleting the collected data is often insufficient to guarantee data privacy
since it is often used to train machine learning models, which can expose
information about the training data. Thus, a guarantee that a trained model
does not expose information about its training data is additionally needed. In
this paper, we present UnlearnSPN -- an algorithm that removes the influence of
single data points from a trained sum-product network and thereby allows
fulfilling data privacy requirements on demand.
</p></li>
</ul>

<h3>Title: Semantics-based Privacy by Design for Internet of Things Applications. (arXiv:2210.01778v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01778">http://arxiv.org/abs/2210.01778</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01778] Semantics-based Privacy by Design for Internet of Things Applications](http://arxiv.org/abs/2210.01778)</code></li>
<li>Summary: <p>As Internet of Things (IoT) technologies become more widespread in everyday
life, privacy issues are becoming more prominent. The aim of this research is
to develop a personal assistant that can answer software engineers' questions
about Privacy by Design (PbD) practices during the design phase of IoT system
development. Semantic web technologies are used to model the knowledge
underlying PbD measurements, their intersections with privacy patterns, IoT
system requirements and the privacy patterns that should be applied across IoT
systems. This is achieved through the development of the PARROT ontology,
developed through a set of representative IoT use cases relevant for software
developers. This was supported by gathering Competency Questions (CQs) through
a series of workshops, resulting in 81 curated CQs. These CQs were then
recorded as SPARQL queries, and the developed ontology was evaluated using the
Common Pitfalls model with the help of the Prot\'eg\'e HermiT Reasoner and the
Ontology Pitfall Scanner (OOPS!), as well as evaluation by external experts.
The ontology was assessed within a user study that identified that the PARROT
ontology can answer up to 58\% of privacy-related questions from software
engineers.
</p></li>
</ul>

<h3>Title: Energy Consumption of Neural Networks on NVIDIA Edge Boards: an Empirical Model. (arXiv:2210.01625v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01625">http://arxiv.org/abs/2210.01625</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01625] Energy Consumption of Neural Networks on NVIDIA Edge Boards: an Empirical Model](http://arxiv.org/abs/2210.01625)</code></li>
<li>Summary: <p>Recently, there has been a trend of shifting the execution of deep learning
inference tasks toward the edge of the network, closer to the user, to reduce
latency and preserve data privacy. At the same time, growing interest is being
devoted to the energetic sustainability of machine learning. At the
intersection of these trends, we hence find the energetic characterization of
machine learning at the edge, which is attracting increasing attention.
Unfortunately, calculating the energy consumption of a given neural network
during inference is complicated by the heterogeneity of the possible underlying
hardware implementation. In this work, we hence aim at profiling the energetic
consumption of inference tasks for some modern edge nodes and deriving simple
but realistic models. To this end, we performed a large number of experiments
to collect the energy consumption of convolutional and fully connected layers
on two well-known edge boards by NVIDIA, namely Jetson TX2 and Xavier. From the
measurements, we have then distilled a simple, practical model that can provide
an estimate of the energy consumption of a certain inference task on the
considered boards. We believe that this model can be used in many contexts as,
for instance, to guide the search for efficient architectures in Neural
Architecture Search, as a heuristic in Neural Network pruning, or to find
energy-efficient offloading strategies in a Split computing context, or simply
to evaluate the energetic performance of Deep Neural Network architectures.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Vision-based Warning System for Maintenance Personnel on Short-Term Roadwork Site. (arXiv:2210.01689v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01689">http://arxiv.org/abs/2210.01689</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01689] Vision-based Warning System for Maintenance Personnel on Short-Term Roadwork Site](http://arxiv.org/abs/2210.01689)</code></li>
<li>Summary: <p>We propose a vision-based warning system for the maintenance personnel
working on short-term construction sites. Traditional solutions use passive
protection, like setting up traffic cones, safety beacons, or even nothing.
However, such methods cannot function as physical safety barriers to separate
working areas from used lanes. In contrast, our system provides active
protection, leveraging acoustic and visual warning signals to help road workers
be cautious of approaching vehicles before they pass the working area. To
decrease too many warnings to relieve a disturbance of road workers, we
implemented our traffic flow check algorithm, by which about 80% of the useless
notices can be filtered. We conduct the evaluations in laboratory conditions
and the real world, proving our system's applicability and reliability.
</p></li>
</ul>

<h2>defense</h2>
<h2>attack</h2>
<h3>Title: GANTouch: An Attack-Resilient Framework for Touch-based Continuous Authentication System. (arXiv:2210.01594v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01594">http://arxiv.org/abs/2210.01594</a></li>
<li>Code URL: <a href="https://github.com/midas-research/gantouch-tbiom">https://github.com/midas-research/gantouch-tbiom</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01594] GANTouch: An Attack-Resilient Framework for Touch-based Continuous Authentication System](http://arxiv.org/abs/2210.01594)</code></li>
<li>Summary: <p>Previous studies have shown that commonly studied (vanilla) implementations
of touch-based continuous authentication systems (V-TCAS) are susceptible to
active adversarial attempts. This study presents a novel Generative Adversarial
Network assisted TCAS (G-TCAS) framework and compares it to the V-TCAS under
three active adversarial environments viz. Zero-effort, Population, and
Random-vector. The Zero-effort environment was implemented in two variations
viz. Zero-effort (same-dataset) and Zero-effort (cross-dataset). The first
involved a Zero-effort attack from the same dataset, while the second used
three different datasets. G-TCAS showed more resilience than V-TCAS under the
Population and Random-vector, the more damaging adversarial scenarios than the
Zero-effort. On average, the increase in the false accept rates (FARs) for
V-TCAS was much higher (27.5% and 21.5%) than for G-TCAS (14% and 12.5%) for
Population and Random-vector attacks, respectively. Moreover, we performed a
fairness analysis of TCAS for different genders and found TCAS to be fair
across genders. The findings suggest that we should evaluate TCAS under active
adversarial environments and affirm the usefulness of GANs in the TCAS
pipeline.
</p></li>
</ul>

<h3>Title: Strength-Adaptive Adversarial Training. (arXiv:2210.01288v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01288">http://arxiv.org/abs/2210.01288</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01288] Strength-Adaptive Adversarial Training](http://arxiv.org/abs/2210.01288)</code></li>
<li>Summary: <p>Adversarial training (AT) is proved to reliably improve network's robustness
against adversarial data. However, current AT with a pre-specified perturbation
budget has limitations in learning a robust network. Firstly, applying a
pre-specified perturbation budget on networks of various model capacities will
yield divergent degree of robustness disparity between natural and robust
accuracies, which deviates from robust network's desideratum. Secondly, the
attack strength of adversarial training data constrained by the pre-specified
perturbation budget fails to upgrade as the growth of network robustness, which
leads to robust overfitting and further degrades the adversarial robustness. To
overcome these limitations, we propose \emph{Strength-Adaptive Adversarial
Training} (SAAT). Specifically, the adversary employs an adversarial loss
constraint to generate adversarial training data. Under this constraint, the
perturbation budget will be adaptively adjusted according to the training state
of adversarial data, which can effectively avoid robust overfitting. Besides,
SAAT explicitly constrains the attack strength of training data through the
adversarial loss, which manipulates model capacity scheduling during training,
and thereby can flexibly control the degree of robustness disparity and adjust
the tradeoff between natural accuracy and robustness. Extensive experiments
show that our proposal boosts the robustness of adversarial training.
</p></li>
</ul>

<h3>Title: Backdoor Attacks in the Supply Chain of Masked Image Modeling. (arXiv:2210.01632v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01632">http://arxiv.org/abs/2210.01632</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01632] Backdoor Attacks in the Supply Chain of Masked Image Modeling](http://arxiv.org/abs/2210.01632)</code></li>
<li>Summary: <p>Masked image modeling (MIM) revolutionizes self-supervised learning (SSL) for
image pre-training. In contrast to previous dominating self-supervised methods,
i.e., contrastive learning, MIM attains state-of-the-art performance by masking
and reconstructing random patches of the input image. However, the associated
security and privacy risks of this novel generative method are unexplored. In
this paper, we perform the first security risk quantification of MIM through
the lens of backdoor attacks. Different from previous work, we are the first to
systematically threat modeling on SSL in every phase of the model supply chain,
i.e., pre-training, release, and downstream phases. Our evaluation shows that
models built with MIM are vulnerable to existing backdoor attacks in release
and downstream phases and are compromised by our proposed method in
pre-training phase. For instance, on CIFAR10, the attack success rate can reach
99.62%, 96.48%, and 98.89% in the downstream phase, release phase, and
pre-training phase, respectively. We also take the first step to investigate
the success factors of backdoor attacks in the pre-training phase and find the
trigger number and trigger pattern play key roles in the success of backdoor
attacks while trigger location has only tiny effects. In the end, our empirical
study of the defense mechanisms across three detection-level on model supply
chain phases indicates that different defenses are suitable for backdoor
attacks in different phases. However, backdoor attacks in the release phase
cannot be detected by all three detection-level methods, calling for more
effective defenses in future research.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Supervised Contrastive Regression. (arXiv:2210.01189v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01189">http://arxiv.org/abs/2210.01189</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01189] Supervised Contrastive Regression](http://arxiv.org/abs/2210.01189)</code></li>
<li>Summary: <p>Deep regression models typically learn in an end-to-end fashion and do not
explicitly try to learn a regression-aware representation. Their
representations tend to be fragmented and fail to capture the continuous nature
of regression tasks. In this paper, we propose Supervised Contrastive
Regression (SupCR), a framework that learns a regression-aware representation
by contrasting samples against each other based on their target distance. SupCR
is orthogonal to existing regression models, and can be used in combination
with such models to improve performance. Extensive experiments using five
real-world regression datasets that span computer vision, human-computer
interaction, and healthcare show that using SupCR achieves the state-of-the-art
performance and consistently improves prior regression baselines on all
datasets, tasks, and input modalities. SupCR also improves robustness to data
corruptions, resilience to reduced training data, performance on transfer
learning, and generalization to unseen targets.
</p></li>
</ul>

<h3>Title: Probabilistic Volumetric Fusion for Dense Monocular SLAM. (arXiv:2210.01276v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01276">http://arxiv.org/abs/2210.01276</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01276] Probabilistic Volumetric Fusion for Dense Monocular SLAM](http://arxiv.org/abs/2210.01276)</code></li>
<li>Summary: <p>We present a novel method to reconstruct 3D scenes from images by leveraging
deep dense monocular SLAM and fast uncertainty propagation. The proposed
approach is able to 3D reconstruct scenes densely, accurately, and in real-time
while being robust to extremely noisy depth estimates coming from dense
monocular SLAM. Differently from previous approaches, that either use ad-hoc
depth filters, or that estimate the depth uncertainty from RGB-D cameras'
sensor models, our probabilistic depth uncertainty derives directly from the
information matrix of the underlying bundle adjustment problem in SLAM. We show
that the resulting depth uncertainty provides an excellent signal to weight the
depth-maps for volumetric fusion. Without our depth uncertainty, the resulting
mesh is noisy and with artifacts, while our approach generates an accurate 3D
mesh with significantly fewer artifacts. We provide results on the challenging
Euroc dataset, and show that our approach achieves 92% better accuracy than
directly fusing depths from monocular SLAM, and up to 90% improvements compared
to the best competing approach.
</p></li>
</ul>

<h3>Title: Nuisances via Negativa: Adjusting for Spurious Correlations via Data Augmentation. (arXiv:2210.01302v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01302">http://arxiv.org/abs/2210.01302</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01302] Nuisances via Negativa: Adjusting for Spurious Correlations via Data Augmentation](http://arxiv.org/abs/2210.01302)</code></li>
<li>Summary: <p>There exist features that are related to the label in the same way across
different settings for that task; these are semantic features or semantics.
Features with varying relationships to the label are nuisances. For example, in
detecting cows from natural images, the shape of the head is a semantic and
because images of cows often have grass backgrounds but only in certain
settings, the background is a nuisance. Relationships between a nuisance and
the label are unstable across settings and, consequently, models that exploit
nuisance-label relationships face performance degradation when these
relationships change. Direct knowledge of a nuisance helps build models that
are robust to such changes, but knowledge of a nuisance requires extra
annotations beyond the label and the covariates. In this paper, we develop an
alternative way to produce robust models by data augmentation. These data
augmentations corrupt semantic information to produce models that identify and
adjust for where nuisances drive predictions. We study semantic corruptions in
powering different robust-modeling methods for multiple out-of distribution
(OOD) tasks like classifying waterbirds, natural language inference, and
detecting Cardiomegaly in chest X-rays.
</p></li>
</ul>

<h3>Title: Learning to Collocate Visual-Linguistic Neural Modules for Image Captioning. (arXiv:2210.01338v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01338">http://arxiv.org/abs/2210.01338</a></li>
<li>Code URL: <a href="https://github.com/gcyzsl/cvlmn">https://github.com/gcyzsl/cvlmn</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01338] Learning to Collocate Visual-Linguistic Neural Modules for Image Captioning](http://arxiv.org/abs/2210.01338)</code></li>
<li>Summary: <p>Humans tend to decompose a sentence into different parts like \textsc{sth do
sth at someplace} and then fill each part with certain content. Inspired by
this, we follow the \textit{principle of modular design} to propose a novel
image captioner: learning to Collocate Visual-Linguistic Neural Modules
(CVLNM). Unlike the \re{widely used} neural module networks in VQA, where the
language (\ie, question) is fully observable, \re{the task of collocating
visual-linguistic modules is more challenging.} This is because the language is
only partially observable, for which we need to dynamically collocate the
modules during the process of image captioning. To sum up, we make the
following technical contributions to design and train our CVLNM: 1)
\textit{distinguishable module design} -- \re{four modules in the encoder}
including one linguistic module for function words and three visual modules for
different content words (\ie, noun, adjective, and verb) and another linguistic
one in the decoder for commonsense reasoning, 2) a self-attention based
\textit{module controller} for robustifying the visual reasoning, 3) a
part-of-speech based \textit{syntax loss} imposed on the module controller for
further regularizing the training of our CVLNM. Extensive experiments on the
MS-COCO dataset show that our CVLNM is more effective, \eg, achieving a new
state-of-the-art 129.5 CIDEr-D, and more robust, \eg, being less likely to
overfit to dataset bias and suffering less when fewer training samples are
available. Codes are available at \url{https://github.com/GCYZSL/CVLMN}
</p></li>
</ul>

<h3>Title: ImmFusion: Robust mmWave-RGB Fusion for 3D Human Body Reconstruction in All Weather Conditions. (arXiv:2210.01346v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01346">http://arxiv.org/abs/2210.01346</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01346] ImmFusion: Robust mmWave-RGB Fusion for 3D Human Body Reconstruction in All Weather Conditions](http://arxiv.org/abs/2210.01346)</code></li>
<li>Summary: <p>3D human reconstruction from RGB images achieves decent results in good
weather conditions but degrades dramatically in rough weather. Complementary,
mmWave radars have been employed to reconstruct 3D human joints and meshes in
rough weather. However, combining RGB and mmWave signals for robust all-weather
3D human reconstruction is still an open challenge, given the sparse nature of
mmWave and the vulnerability of RGB images. In this paper, we present
ImmFusion, the first mmWave-RGB fusion solution to reconstruct 3D human bodies
in all weather conditions robustly. Specifically, our ImmFusion consists of
image and point backbones for token feature extraction and a Transformer module
for token fusion. The image and point backbones refine global and local
features from original data, and the Fusion Transformer Module aims for
effective information fusion of two modalities by dynamically selecting
informative tokens. Extensive experiments on a large-scale dataset, mmBody,
captured in various environments demonstrate that ImmFusion can efficiently
utilize the information of two modalities to achieve a robust 3D human body
reconstruction in all weather conditions. In addition, our method's accuracy is
significantly superior to that of state-of-the-art Transformer-based
LiDAR-camera fusion methods.
</p></li>
</ul>

<h3>Title: Cross-identity Video Motion Retargeting with Joint Transformation and Synthesis. (arXiv:2210.01559v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01559">http://arxiv.org/abs/2210.01559</a></li>
<li>Code URL: <a href="https://github.com/nihaomiao/wacv23_tsnet">https://github.com/nihaomiao/wacv23_tsnet</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01559] Cross-identity Video Motion Retargeting with Joint Transformation and Synthesis](http://arxiv.org/abs/2210.01559)</code></li>
<li>Summary: <p>In this paper, we propose a novel dual-branch Transformation-Synthesis
network (TS-Net), for video motion retargeting. Given one subject video and one
driving video, TS-Net can produce a new plausible video with the subject
appearance of the subject video and motion pattern of the driving video. TS-Net
consists of a warp-based transformation branch and a warp-free synthesis
branch. The novel design of dual branches combines the strengths of
deformation-grid-based transformation and warp-free generation for better
identity preservation and robustness to occlusion in the synthesized videos. A
mask-aware similarity module is further introduced to the transformation branch
to reduce computational overhead. Experimental results on face and dance
datasets show that TS-Net achieves better performance in video motion
retargeting than several state-of-the-art models as well as its single-branch
variants. Our code is available at https://github.com/nihaomiao/WACV23_TSNet.
</p></li>
</ul>

<h3>Title: Cross-Geography Generalization of Machine Learning Methods for Classification of Flooded Regions in Aerial Images. (arXiv:2210.01588v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01588">http://arxiv.org/abs/2210.01588</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01588] Cross-Geography Generalization of Machine Learning Methods for Classification of Flooded Regions in Aerial Images](http://arxiv.org/abs/2210.01588)</code></li>
<li>Summary: <p>Identification of regions affected by floods is a crucial piece of
information required for better planning and management of post-disaster relief
and rescue efforts. Traditionally, remote sensing images are analysed to
identify the extent of damage caused by flooding. The data acquired from
sensors onboard earth observation satellites are analyzed to detect the flooded
regions, which can be affected by low spatial and temporal resolution. However,
in recent years, the images acquired from Unmanned Aerial Vehicles (UAVs) have
also been utilized to assess post-disaster damage. Indeed, a UAV based platform
can be rapidly deployed with a customized flight plan and minimum dependence on
the ground infrastructure. This work proposes two approaches for identifying
flooded regions in UAV aerial images. The first approach utilizes texture-based
unsupervised segmentation to detect flooded areas, while the second uses an
artificial neural network on the texture features to classify images as flooded
and non-flooded. Unlike the existing works where the models are trained and
tested on images of the same geographical regions, this work studies the
performance of the proposed model in identifying flooded regions across
geographical regions. An F1-score of 0.89 is obtained using the proposed
segmentation-based approach which is higher than existing classifiers. The
robustness of the proposed approach demonstrates that it can be utilized to
identify flooded regions of any region with minimum or no user intervention.
</p></li>
</ul>

<h3>Title: Positive Pair Distillation Considered Harmful: Continual Meta Metric Learning for Lifelong Object Re-Identification. (arXiv:2210.01600v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01600">http://arxiv.org/abs/2210.01600</a></li>
<li>Code URL: <a href="https://github.com/wangkai930418/dwopp_code">https://github.com/wangkai930418/dwopp_code</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01600] Positive Pair Distillation Considered Harmful: Continual Meta Metric Learning for Lifelong Object Re-Identification](http://arxiv.org/abs/2210.01600)</code></li>
<li>Summary: <p>Lifelong object re-identification incrementally learns from a stream of
re-identification tasks. The objective is to learn a representation that can be
applied to all tasks and that generalizes to previously unseen
re-identification tasks. The main challenge is that at inference time the
representation must generalize to previously unseen identities. To address this
problem, we apply continual meta metric learning to lifelong object
re-identification. To prevent forgetting of previous tasks, we use knowledge
distillation and explore the roles of positive and negative pairs. Based on our
observation that the distillation and metric losses are antagonistic, we
propose to remove positive pairs from distillation to robustify model updates.
Our method, called Distillation without Positive Pairs (DwoPP), is evaluated on
extensive intra-domain experiments on person and vehicle re-identification
datasets, as well as inter-domain experiments on the LReID benchmark. Our
experiments demonstrate that DwoPP significantly outperforms the
state-of-the-art. The code is here: https://github.com/wangkai930418/DwoPP_code
</p></li>
</ul>

<h3>Title: Robust Target Training for Multi-Source Domain Adaptation. (arXiv:2210.01676v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01676">http://arxiv.org/abs/2210.01676</a></li>
<li>Code URL: <a href="https://github.com/zhongying-deng/bort2">https://github.com/zhongying-deng/bort2</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01676] Robust Target Training for Multi-Source Domain Adaptation](http://arxiv.org/abs/2210.01676)</code></li>
<li>Summary: <p>Given multiple labeled source domains and a single target domain, most
existing multi-source domain adaptation (MSDA) models are trained on data from
all domains jointly in one step. Such an one-step approach limits their ability
to adapt to the target domain. This is because the training set is dominated by
the more numerous and labeled source domain data. The source-domain-bias can
potentially be alleviated by introducing a second training step, where the
model is fine-tuned with the unlabeled target domain data only using pseudo
labels as supervision. However, the pseudo labels are inevitably noisy and when
used unchecked can negatively impact the model performance. To address this
problem, we propose a novel Bi-level Optimization based Robust Target Training
(BORT$^2$) method for MSDA. Given any existing fully-trained one-step MSDA
model, BORT$^2$ turns it to a labeling function to generate pseudo-labels for
the target data and trains a target model using pseudo-labeled target data
only. Crucially, the target model is a stochastic CNN which is designed to be
intrinsically robust against label noise generated by the labeling function.
Such a stochastic CNN models each target instance feature as a Gaussian
distribution with an entropy maximization regularizer deployed to measure the
label uncertainty, which is further exploited to alleviate the negative impact
of noisy pseudo labels. Training the labeling function and the target model
poses a nested bi-level optimization problem, for which we formulate an elegant
solution based on implicit differentiation. Extensive experiments demonstrate
that our proposed method achieves the state of the art performance on three
MSDA benchmarks, including the large-scale DomainNet dataset. Our code will be
available at \url{https://github.com/Zhongying-Deng/BORT2}
</p></li>
</ul>

<h3>Title: Understanding Prior Bias and Choice Paralysis in Transformer-based Language Representation Models through Four Experimental Probes. (arXiv:2210.01258v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01258">http://arxiv.org/abs/2210.01258</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01258] Understanding Prior Bias and Choice Paralysis in Transformer-based Language Representation Models through Four Experimental Probes](http://arxiv.org/abs/2210.01258)</code></li>
<li>Summary: <p>Recent work on transformer-based neural networks has led to impressive
advances on multiple-choice natural language understanding (NLU) problems, such
as Question Answering (QA) and abductive reasoning. Despite these advances,
there is limited work still on understanding whether these models respond to
perturbed multiple-choice instances in a sufficiently robust manner that would
allow them to be trusted in real-world situations. We present four confusion
probes, inspired by similar phenomena first identified in the behavioral
science community, to test for problems such as prior bias and choice
paralysis. Experimentally, we probe a widely used transformer-based
multiple-choice NLU system using four established benchmark datasets. Here we
show that the model exhibits significant prior bias and to a lesser, but still
highly significant degree, choice paralysis, in addition to other problems. Our
results suggest that stronger testing protocols and additional benchmarks may
be necessary before the language models are used in front-facing systems or
decision making with real world consequences.
</p></li>
</ul>

<h3>Title: Robust Active Distillation. (arXiv:2210.01213v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01213">http://arxiv.org/abs/2210.01213</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01213] Robust Active Distillation](http://arxiv.org/abs/2210.01213)</code></li>
<li>Summary: <p>Distilling knowledge from a large teacher model to a lightweight one is a
widely successful approach for generating compact, powerful models in the
semi-supervised learning setting where a limited amount of labeled data is
available. In large-scale applications, however, the teacher tends to provide a
large number of incorrect soft-labels that impairs student performance. The
sheer size of the teacher additionally constrains the number of soft-labels
that can be queried due to prohibitive computational and/or financial costs.
The difficulty in achieving simultaneous \emph{efficiency} (i.e., minimizing
soft-label queries) and \emph{robustness} (i.e., avoiding student inaccuracies
due to incorrect labels) hurts the widespread application of knowledge
distillation to many modern tasks. In this paper, we present a parameter-free
approach with provable guarantees to query the soft-labels of points that are
simultaneously informative and correctly labeled by the teacher. At the core of
our work lies a game-theoretic formulation that explicitly considers the
inherent trade-off between the informativeness and correctness of input
instances. We establish bounds on the expected performance of our approach that
hold even in worst-case distillation instances. We present empirical
evaluations on popular benchmarks that demonstrate the improved distillation
performance enabled by our work relative to that of state-of-the-art active
learning and active distillation methods.
</p></li>
</ul>

<h3>Title: Interpretable Option Discovery using Deep Q-Learning and Variational Autoencoders. (arXiv:2210.01231v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01231">http://arxiv.org/abs/2210.01231</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01231] Interpretable Option Discovery using Deep Q-Learning and Variational Autoencoders](http://arxiv.org/abs/2210.01231)</code></li>
<li>Summary: <p>Deep Reinforcement Learning (RL) is unquestionably a robust framework to
train autonomous agents in a wide variety of disciplines. However, traditional
deep and shallow model-free RL algorithms suffer from low sample efficiency and
inadequate generalization for sparse state spaces. The options framework with
temporal abstractions is perhaps the most promising method to solve these
problems, but it still has noticeable shortcomings. It only guarantees local
convergence, and it is challenging to automate initiation and termination
conditions, which in practice are commonly hand-crafted.
</p></li>
</ul>

<p>Our proposal, the Deep Variational Q-Network (DVQN), combines deep
generative- and reinforcement learning. The algorithm finds good policies from
a Gaussian distributed latent-space, which is especially useful for defining
options. The DVQN algorithm uses MSE with KL-divergence as regularization,
combined with traditional Q-Learning updates. The algorithm learns a
latent-space that represents good policies with state clusters for options. We
show that the DVQN algorithm is a promising approach for identifying initiation
and termination conditions for option-based reinforcement learning. Experiments
show that the DVQN algorithm, with automatic initiation and termination, has
comparable performance to Rainbow and can maintain stability when trained for
extended periods after convergence.
</p>

<h3>Title: SAM as an Optimal Relaxation of Bayes. (arXiv:2210.01620v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01620">http://arxiv.org/abs/2210.01620</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01620] SAM as an Optimal Relaxation of Bayes](http://arxiv.org/abs/2210.01620)</code></li>
<li>Summary: <p>Sharpness-aware minimization (SAM) and related adversarial deep-learning
methods can drastically improve generalization, but their underlying mechanisms
are not yet fully understood. Here, we establish SAM as a relaxation of the
Bayes objective where the expected negative-loss is replaced by the optimal
convex lower bound, obtained by using the so-called Fenchel biconjugate. The
connection enables a new Adam-like extension of SAM to automatically obtain
reasonable uncertainty estimates, while sometimes also improving its accuracy.
By connecting adversarial and Bayesian methods, our work opens a new path to
robustness.
</p></li>
</ul>

<h3>Title: Rethinking Lipschitz Neural Networks for Certified L-infinity Robustness. (arXiv:2210.01787v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01787">http://arxiv.org/abs/2210.01787</a></li>
<li>Code URL: <a href="https://github.com/zbh2047/sortnet">https://github.com/zbh2047/sortnet</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01787] Rethinking Lipschitz Neural Networks for Certified L-infinity Robustness](http://arxiv.org/abs/2210.01787)</code></li>
<li>Summary: <p>Designing neural networks with bounded Lipschitz constant is a promising way
to obtain certifiably robust classifiers against adversarial examples. However,
the relevant progress for the important $\ell_\infty$ perturbation setting is
rather limited, and a principled understanding of how to design expressive
$\ell_\infty$ Lipschitz networks is still lacking. In this paper, we bridge the
gap by studying certified $\ell_\infty$ robustness from a novel perspective of
representing Boolean functions. We derive two fundamental impossibility results
that hold for any standard Lipschitz network: one for robust classification on
finite datasets, and the other for Lipschitz function approximation. These
results identify that networks built upon norm-bounded affine layers and
Lipschitz activations intrinsically lose expressive power even in the
two-dimensional case, and shed light on how recently proposed Lipschitz
networks (e.g., GroupSort and $\ell_\infty$-distance nets) bypass these
impossibilities by leveraging order statistic functions. Finally, based on
these insights, we develop a unified Lipschitz network that generalizes prior
works, and design a practical version that can be efficiently trained (making
certified robust training free). Extensive experiments show that our approach
is scalable, efficient, and consistently yields better certified robustness
across multiple datasets and perturbation radii than prior Lipschitz networks.
</p></li>
</ul>

<h3>Title: TPGNN: Learning High-order Information in Dynamic Graphs via Temporal Propagation. (arXiv:2210.01171v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01171">http://arxiv.org/abs/2210.01171</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01171] TPGNN: Learning High-order Information in Dynamic Graphs via Temporal Propagation](http://arxiv.org/abs/2210.01171)</code></li>
<li>Summary: <p>Temporal graph is an abstraction for modeling dynamic systems that consist of
evolving interaction elements. In this paper, we aim to solve an important yet
neglected problem -- how to learn information from high-order neighbors in
temporal graphs? -- to enhance the informativeness and discriminativeness for
the learned node representations. We argue that when learning high-order
information from temporal graphs, we encounter two challenges, i.e.,
computational inefficiency and over-smoothing, that cannot be solved by
conventional techniques applied on static graphs. To remedy these deficiencies,
we propose a temporal propagation-based graph neural network, namely TPGNN. To
be specific, the model consists of two distinct components, i.e., propagator
and node-wise encoder. The propagator is leveraged to propagate messages from
the anchor node to its temporal neighbors within $k$-hop, and then
simultaneously update the state of neighborhoods, which enables efficient
computation, especially for a deep model. In addition, to prevent
over-smoothing, the model compels the messages from $n$-hop neighbors to update
the $n$-hop memory vector preserved on the anchor. The node-wise encoder adopts
transformer architecture to learn node representations by explicitly learning
the importance of memory vectors preserved on the node itself, that is,
implicitly modeling the importance of messages from neighbors at different
layers, thus mitigating the over-smoothing. Since the encoding process will not
query temporal neighbors, we can dramatically save time consumption in
inference. Extensive experiments on temporal link prediction and node
classification demonstrate the superiority of TPGNN over state-of-the-art
baselines in efficiency and robustness.
</p></li>
</ul>

<h3>Title: Convolutional networks inherit frequency sensitivity from image statistics. (arXiv:2210.01257v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01257">http://arxiv.org/abs/2210.01257</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01257] Convolutional networks inherit frequency sensitivity from image statistics](http://arxiv.org/abs/2210.01257)</code></li>
<li>Summary: <p>It is widely acknowledged that trained convolutional neural networks (CNNs)
have different levels of sensitivity to signals of different frequency. In
particular, a number of empirical studies have documented CNNs sensitivity to
low-frequency signals. In this work we show with theory and experiments that
this observed sensitivity is a consequence of the frequency distribution of
natural images, which is known to have most of its power concentrated in
low-to-mid frequencies. Our theoretical analysis relies on representations of
the layers of a CNN in frequency space, an idea that has previously been used
to accelerate computations and study implicit bias of network training
algorithms, but to the best of our knowledge has not been applied in the domain
of model robustness.
</p></li>
</ul>

<h3>Title: RAP: Risk-Aware Prediction for Robust Planning. (arXiv:2210.01368v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01368">http://arxiv.org/abs/2210.01368</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01368] RAP: Risk-Aware Prediction for Robust Planning](http://arxiv.org/abs/2210.01368)</code></li>
<li>Summary: <p>Robust planning in interactive scenarios requires predicting the uncertain
future to make risk-aware decisions. Unfortunately, due to long-tail
safety-critical events, the risk is often under-estimated by finite-sampling
approximations of probabilistic motion forecasts. This can lead to
overconfident and unsafe robot behavior, even with robust planners. Instead of
assuming full prediction coverage that robust planners require, we propose to
make prediction itself risk-aware. We introduce a new prediction objective to
learn a risk-biased distribution over trajectories, so that risk evaluation
simplifies to an expected cost estimation under this biased distribution. This
reduces the sample complexity of the risk estimation during online planning,
which is needed for safe real-time performance. Evaluation results in a
didactic simulation environment and on a real-world dataset demonstrate the
effectiveness of our approach.
</p></li>
</ul>

<h3>Title: Robust self-healing prediction model for high dimensional data. (arXiv:2210.01788v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01788">http://arxiv.org/abs/2210.01788</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01788] Robust self-healing prediction model for high dimensional data](http://arxiv.org/abs/2210.01788)</code></li>
<li>Summary: <p>Owing to the advantages of increased accuracy and the potential to detect
unseen patterns, provided by data mining techniques they have been widely
incorporated for standard classification problems. They have often been used
for high precision disease prediction in the medical field, and several hybrid
prediction models capable of achieving high accuracies have been proposed.
Though this stands true most of the previous models fail to efficiently address
the recurring issue of bad data quality which plagues most high dimensional
data, and especially proves troublesome in the highly sensitive medical data.
This work proposes a robust self healing (RSH) hybrid prediction model which
functions by using the data in its entirety by removing errors and
inconsistencies from it rather than discarding any data. Initial processing
involves data preparation followed by cleansing or scrubbing through
context-dependent attribute correction, which ensures that there is no
significant loss of relevant information before the feature selection and
prediction phases. An ensemble of heterogeneous classifiers, subjected to local
boosting, is utilized to build the prediction model and genetic algorithm based
wrapper feature selection technique wrapped on the respective classifiers is
employed to select the corresponding optimal set of features, which warrant
higher accuracy. The proposed method is compared with some of the existing high
performing models and the results are analyzed.
</p></li>
</ul>

<h3>Title: Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals. (arXiv:2210.01790v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01790">http://arxiv.org/abs/2210.01790</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01790] Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals](http://arxiv.org/abs/2210.01790)</code></li>
<li>Summary: <p>The field of AI alignment is concerned with AI systems that pursue unintended
goals. One commonly studied mechanism by which an unintended goal might arise
is specification gaming, in which the designer-provided specification is flawed
in a way that the designers did not foresee. However, an AI system may pursue
an undesired goal even when the specification is correct, in the case of goal
misgeneralization. Goal misgeneralization is a specific form of robustness
failure for learning algorithms in which the learned program competently
pursues an undesired goal that leads to good performance in training situations
but bad performance in novel test situations. We demonstrate that goal
misgeneralization can occur in practical systems by providing several examples
in deep learning systems across a variety of domains. Extrapolating forward to
more capable systems, we provide hypotheticals that illustrate how goal
misgeneralization could lead to catastrophic risk. We suggest several research
directions that could reduce the risk of goal misgeneralization for future
systems.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Guiding the PLMs with Semantic Anchors as Intermediate Supervision: Towards Interpretable Semantic Parsing. (arXiv:2210.01425v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01425">http://arxiv.org/abs/2210.01425</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01425] Guiding the PLMs with Semantic Anchors as Intermediate Supervision: Towards Interpretable Semantic Parsing](http://arxiv.org/abs/2210.01425)</code></li>
<li>Summary: <p>The recent prevalence of pretrained language models (PLMs) has dramatically
shifted the paradigm of semantic parsing, where the mapping from natural
language utterances to structured logical forms is now formulated as a Seq2Seq
task. Despite the promising performance, previous PLM-based approaches often
suffer from hallucination problems due to their negligence of the structural
information contained in the sentence, which essentially constitutes the key
semantics of the logical forms. Furthermore, most works treat PLM as a black
box in which the generation process of the target logical form is hidden
beneath the decoder modules, which greatly hinders the model's intrinsic
interpretability. To address these two issues, we propose to incorporate the
current PLMs with a hierarchical decoder network. By taking the first-principle
structures as the semantic anchors, we propose two novel intermediate
supervision tasks, namely Semantic Anchor Extraction and Semantic Anchor
Alignment, for training the hierarchical decoders and probing the model
intermediate representations in a self-adaptive manner alongside the
fine-tuning process. We conduct intensive experiments on several semantic
parsing benchmarks and demonstrate that our approach can consistently
outperform the baselines. More importantly, by analyzing the intermediate
representations of the hierarchical decoders, our approach also makes a huge
step toward the intrinsic interpretability of PLMs in the domain of semantic
parsing.
</p></li>
</ul>

<h3>Title: Causal Intervention-based Prompt Debiasing for Event Argument Extraction. (arXiv:2210.01561v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01561">http://arxiv.org/abs/2210.01561</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01561] Causal Intervention-based Prompt Debiasing for Event Argument Extraction](http://arxiv.org/abs/2210.01561)</code></li>
<li>Summary: <p>Prompt-based methods have become increasingly popular among information
extraction tasks, especially in low-data scenarios. By formatting a finetune
task into a pre-training objective, prompt-based methods resolve the data
scarce problem effectively. However, seldom do previous research investigate
the discrepancy among different prompt formulating strategies. In this work, we
compare two kinds of prompts, name-based prompt and ontology-base prompt, and
reveal how ontology-base prompt methods exceed its counterpart in zero-shot
event argument extraction (EAE) . Furthermore, we analyse the potential risk in
ontology-base prompts via a causal view and propose a debias method by causal
intervention. Experiments on two benchmarks demonstrate that modified by our
debias method, the baseline model becomes both more effective and robust, with
significant improvement in the resistance to adversarial attacks.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Exploring Parameter-Efficient Fine-tuning for Improving Communication Efficiency in Federated Learning. (arXiv:2210.01708v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01708">http://arxiv.org/abs/2210.01708</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01708] Exploring Parameter-Efficient Fine-tuning for Improving Communication Efficiency in Federated Learning](http://arxiv.org/abs/2210.01708)</code></li>
<li>Summary: <p>Federated learning (FL) has emerged as a promising paradigm for enabling the
collaborative training of models without centralized access to the raw data on
local devices. In the typical FL paradigm (e.g., FedAvg), model weights are
sent to and from the server each round to participating clients. However, this
can quickly put a massive communication burden on the system, especially if
more capable models beyond very small MLPs are employed. Recently, the use of
pre-trained models has been shown effective in federated learning optimization
and improving convergence. This opens the door for new research questions. Can
we adjust the weight-sharing paradigm in federated learning, leveraging strong
and readily-available pre-trained models, to significantly reduce the
communication burden while simultaneously achieving excellent performance? To
this end, we investigate the use of parameter-efficient fine-tuning in
federated learning. Specifically, we systemically evaluate the performance of
several parameter-efficient fine-tuning methods across a variety of client
stability, data distribution, and differential privacy settings. By only
locally tuning and globally sharing a small portion of the model weights,
significant reductions in the total communication overhead can be achieved
while maintaining competitive performance in a wide range of federated learning
scenarios, providing insight into a new paradigm for practical and effective
federated systems.
</p></li>
</ul>

<h3>Title: OpBoost: A Vertical Federated Tree Boosting Framework Based on Order-Preserving Desensitization. (arXiv:2210.01318v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01318">http://arxiv.org/abs/2210.01318</a></li>
<li>Code URL: <a href="https://github.com/alibaba-edu/mpc4j">https://github.com/alibaba-edu/mpc4j</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01318] OpBoost: A Vertical Federated Tree Boosting Framework Based on Order-Preserving Desensitization](http://arxiv.org/abs/2210.01318)</code></li>
<li>Summary: <p>Vertical Federated Learning (FL) is a new paradigm that enables users with
non-overlapping attributes of the same data samples to jointly train a model
without directly sharing the raw data. Nevertheless, recent works show that
it's still not sufficient to prevent privacy leakage from the training process
or the trained model. This paper focuses on studying the privacy-preserving
tree boosting algorithms under the vertical FL. The existing solutions based on
cryptography involve heavy computation and communication overhead and are
vulnerable to inference attacks. Although the solution based on Local
Differential Privacy (LDP) addresses the above problems, it leads to the low
accuracy of the trained model.
</p></li>
</ul>

<p>This paper explores to improve the accuracy of the widely deployed tree
boosting algorithms satisfying differential privacy under vertical FL.
Specifically, we introduce a framework called OpBoost. Three order-preserving
desensitization algorithms satisfying a variant of LDP called distance-based
LDP (dLDP) are designed to desensitize the training data. In particular, we
optimize the dLDP definition and study efficient sampling distributions to
further improve the accuracy and efficiency of the proposed algorithms. The
proposed algorithms provide a trade-off between the privacy of pairs with large
distance and the utility of desensitized values. Comprehensive evaluations show
that OpBoost has a better performance on prediction accuracy of trained models
compared with existing LDP approaches on reasonable settings. Our code is open
source.
</p>

<h3>Title: Data Leakage in Tabular Federated Learning. (arXiv:2210.01785v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01785">http://arxiv.org/abs/2210.01785</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01785] Data Leakage in Tabular Federated Learning](http://arxiv.org/abs/2210.01785)</code></li>
<li>Summary: <p>While federated learning (FL) promises to preserve privacy in distributed
training of deep learning models, recent work in the image and NLP domains
showed that training updates leak private data of participating clients. At the
same time, most high-stakes applications of FL (e.g., legal and financial) use
tabular data. Compared to the NLP and image domains, reconstruction of tabular
data poses several unique challenges: (i) categorical features introduce a
significantly more difficult mixed discrete-continuous optimization problem,
(ii) the mix of categorical and continuous features causes high variance in the
final reconstructions, and (iii) structured data makes it difficult for the
adversary to judge reconstruction quality. In this work, we tackle these
challenges and propose the first comprehensive reconstruction attack on tabular
data, called TabLeak. TabLeak is based on three key ingredients: (i) a softmax
structural prior, implicitly converting the mixed discrete-continuous
optimization problem into an easier fully continuous one, (ii) a way to reduce
the variance of our reconstructions through a pooled ensembling scheme
exploiting the structure of tabular data, and (iii) an entropy measure which
can successfully assess reconstruction quality. Our experimental evaluation
demonstrates the effectiveness of TabLeak, reaching a state-of-the-art on four
popular tabular datasets. For instance, on the Adult dataset, we improve attack
accuracy by 10% compared to the baseline on the practically relevant batch size
of 32 and further obtain non-trivial reconstructions for batch sizes as large
as 128. Our findings are important as they show that performing FL on tabular
data, which often poses high privacy risks, is highly vulnerable.
</p></li>
</ul>

<h3>Title: Unbounded Gradients in Federated Leaning with Buffered Asynchronous Aggregation. (arXiv:2210.01161v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01161">http://arxiv.org/abs/2210.01161</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01161] Unbounded Gradients in Federated Leaning with Buffered Asynchronous Aggregation](http://arxiv.org/abs/2210.01161)</code></li>
<li>Summary: <p>Synchronous updates may compromise the efficiency of cross-device federated
learning once the number of active clients increases. The \textit{FedBuff}
algorithm (Nguyen et al., 2022) alleviates this problem by allowing
asynchronous updates (staleness), which enhances the scalability of training
while preserving privacy via secure aggregation. We revisit the
\textit{FedBuff} algorithm for asynchronous federated learning and extend the
existing analysis by removing the boundedness assumptions from the gradient
norm. This paper presents a theoretical analysis of the convergence rate of
this algorithm when heterogeneity in data, batch size, and delay are
considered.
</p></li>
</ul>

<h3>Title: PersA-FL: Personalized Asynchronous Federated Learning. (arXiv:2210.01176v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01176">http://arxiv.org/abs/2210.01176</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01176] PersA-FL: Personalized Asynchronous Federated Learning](http://arxiv.org/abs/2210.01176)</code></li>
<li>Summary: <p>We study the personalized federated learning problem under asynchronous
updates. In this problem, each client seeks to obtain a personalized model that
simultaneously outperforms local and global models. We consider two
optimization-based frameworks for personalization: (i) Model-Agnostic
Meta-Learning (MAML) and (ii) Moreau Envelope (ME). MAML involves learning a
joint model adapted for each client through fine-tuning, whereas ME requires a
bi-level optimization problem with implicit gradients to enforce
personalization via regularized losses. We focus on improving the scalability
of personalized federated learning by removing the synchronous communication
assumption. Moreover, we extend the studied function class by removing
boundedness assumptions on the gradient norm. Our main technical contribution
is a unified proof for asynchronous federated learning with bounded staleness
that we apply to MAML and ME personalization frameworks. For the smooth and
non-convex functions class, we show the convergence of our method to a
first-order stationary point. We illustrate the performance of our method and
its tolerance to staleness through experiments for classification tasks over
heterogeneous datasets.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: A Reproducible and Realistic Evaluation of Partial Domain Adaptation Methods. (arXiv:2210.01210v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01210">http://arxiv.org/abs/2210.01210</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01210] A Reproducible and Realistic Evaluation of Partial Domain Adaptation Methods](http://arxiv.org/abs/2210.01210)</code></li>
<li>Summary: <p>Unsupervised Domain Adaptation (UDA) aims at classifying unlabeled target
images leveraging source labeled ones. In this work, we consider the Partial
Domain Adaptation (PDA) variant, where we have extra source classes not present
in the target domain. Most successful algorithms use model selection strategies
that rely on target labels to find the best hyper-parameters and/or models
along training. However, these strategies violate the main assumption in PDA:
only unlabeled target domain samples are available. Moreover, there are also
inconsistencies in the experimental settings - architecture, hyper-parameter
tuning, number of runs - yielding unfair comparisons. The main goal of this
work is to provide a realistic evaluation of PDA methods with the different
model selection strategies under a consistent evaluation protocol. We evaluate
7 representative PDA algorithms on 2 different real-world datasets using 7
different model selection strategies. Our two main findings are: (i) without
target labels for model selection, the accuracy of the methods decreases up to
30 percentage points; (ii) only one method and model selection pair performs
well on both datasets. Experiments were performed with our PyTorch framework,
BenchmarkPDA, which we open source.
</p></li>
</ul>

<h3>Title: MEDFAIR: Benchmarking Fairness for Medical Imaging. (arXiv:2210.01725v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01725">http://arxiv.org/abs/2210.01725</a></li>
<li>Code URL: <a href="https://github.com/ys-zong/medfair">https://github.com/ys-zong/medfair</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01725] MEDFAIR: Benchmarking Fairness for Medical Imaging](http://arxiv.org/abs/2210.01725)</code></li>
<li>Summary: <p>A multitude of work has shown that machine learning-based medical diagnosis
systems can be biased against certain subgroups of people. This has motivated a
growing number of bias mitigation algorithms that aim to address fairness
issues in machine learning. However, it is difficult to compare their
effectiveness in medical imaging for two reasons. First, there is little
consensus on the criteria to assess fairness. Second, existing bias mitigation
algorithms are developed under different settings, e.g., datasets, model
selection strategies, backbones, and fairness metrics, making a direct
comparison and evaluation based on existing results impossible. In this work,
we introduce MEDFAIR, a framework to benchmark the fairness of machine learning
models for medical imaging. MEDFAIR covers eleven algorithms from various
categories, nine datasets from different imaging modalities, and three model
selection criteria. Through extensive experiments, we find that the
under-studied issue of model selection criterion can have a significant impact
on fairness outcomes; while in contrast, state-of-the-art bias mitigation
algorithms do not significantly improve fairness outcomes over empirical risk
minimization (ERM) in both in-distribution and out-of-distribution settings. We
evaluate fairness from various perspectives and make recommendations for
different medical application scenarios that require different ethical
principles. Our framework provides a reproducible and easy-to-use entry point
for the development and evaluation of future bias mitigation algorithms in deep
learning. Code is available at https://github.com/ys-zong/MEDFAIR.
</p></li>
</ul>

<h3>Title: Evaluating Disentanglement in Generative Models Without Knowledge of Latent Factors. (arXiv:2210.01760v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01760">http://arxiv.org/abs/2210.01760</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01760] Evaluating Disentanglement in Generative Models Without Knowledge of Latent Factors](http://arxiv.org/abs/2210.01760)</code></li>
<li>Summary: <p>Probabilistic generative models provide a flexible and systematic framework
for learning the underlying geometry of data. However, model selection in this
setting is challenging, particularly when selecting for ill-defined qualities
such as disentanglement or interpretability. In this work, we address this gap
by introducing a method for ranking generative models based on the training
dynamics exhibited during learning. Inspired by recent theoretical
characterizations of disentanglement, our method does not require supervision
of the underlying latent factors. We evaluate our approach by demonstrating the
need for disentanglement metrics which do not require labels\textemdash the
underlying generative factors. We additionally demonstrate that our approach
correlates with baseline supervised methods for evaluating disentanglement.
Finally, we show that our method can be used as an unsupervised indicator for
downstream performance on reinforcement learning and fairness-classification
problems.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Concise and interpretable multi-label rule sets. (arXiv:2210.01533v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2210.01533">http://arxiv.org/abs/2210.01533</a></li>
<li>Code URL: <a href="https://github.com/diversemultilabelclassificationrules/corset">https://github.com/diversemultilabelclassificationrules/corset</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2210.01533] Concise and interpretable multi-label rule sets](http://arxiv.org/abs/2210.01533)</code></li>
<li>Summary: <p>Multi-label classification is becoming increasingly ubiquitous, but not much
attention has been paid to interpretability. In this paper, we develop a
multi-label classifier that can be represented as a concise set of simple
"if-then" rules, and thus, it offers better interpretability compared to
black-box models. Notably, our method is able to find a small set of relevant
patterns that lead to accurate multi-label classification, while existing
rule-based classifiers are myopic and wasteful in searching rules,requiring a
large number of rules to achieve high accuracy. In particular, we formulate the
problem of choosing multi-label rules to maximize a target function, which
considers not only discrimination ability with respect to labels, but also
diversity. Accounting for diversity helps to avoid redundancy, and thus, to
control the number of rules in the solution set. To tackle the said
maximization problem we propose a 2-approximation algorithm, which relies on a
novel technique to sample high-quality rules. In addition to our theoretical
analysis, we provide a thorough experimental evaluation, which indicates that
our approach offers a trade-off between predictive performance and
interpretability that is unmatched in previous work.
</p></li>
</ul>

<h2>exlainability</h2>
<h2>watermark</h2>
<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
