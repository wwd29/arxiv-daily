<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h2>security</h2>
<h3>Title: Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities. (arXiv:2308.12833v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12833">http://arxiv.org/abs/2308.12833</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12833]] Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities(http://arxiv.org/abs/2308.12833)</code></li>
<li>Summary: <p>Spurred by the recent rapid increase in the development and distribution of
large language models (LLMs) across industry and academia, much recent work has
drawn attention to safety- and security-related threats and vulnerabilities of
LLMs, including in the context of potentially criminal activities.
Specifically, it has been shown that LLMs can be misused for fraud,
impersonation, and the generation of malware; while other authors have
considered the more general problem of AI alignment. It is important that
developers and practitioners alike are aware of security-related problems with
such models. In this paper, we provide an overview of existing - predominantly
scientific - efforts on identifying and mitigating threats and vulnerabilities
arising from LLMs. We present a taxonomy describing the relationship between
threats caused by the generative capabilities of LLMs, prevention measures
intended to address such threats, and vulnerabilities arising from imperfect
prevention measures. With our work, we hope to raise awareness of the
limitations of LLMs in light of such security concerns, among both experienced
developers and novel users of such technologies.
</p></li>
</ul>

<h3>Title: Trend and Emerging Types of 419 Scams. (arXiv:2308.12448v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12448">http://arxiv.org/abs/2308.12448</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12448]] Trend and Emerging Types of 419 Scams(http://arxiv.org/abs/2308.12448)</code></li>
<li>Summary: <p>Technological advancements have revolutionized various aspects of human life,
facilitating communication, business operations, healthcare, education, and
environmental monitoring. However, this increased reliance on technology has
also led to a surge in cybercrime, including cyber scams. The "419 scam" or
Nigerian scam has been a persistent problem for decades, encompassing frauds
like advance fee scams, fake lotteries, and black money scams. Initially
prevalent through postal mail and later via fax, the scam has now transitioned
to email. This study aims to identify recent types of 419 scam emails,
particularly after the covid 19 pandemic, and explore commonly used email
subjects. Analysis of the sample 419 scam emails revealed trending scams like
lucky winner, threat of exposure, business/partnership proposals, investment,
cancer/long-term illness, fund, and compensation scams. Emerging scams included
COVID-related, cryptocurrency, marketing contact, and software development
scams. Irrespective of the scam type, scammers commonly employed email subjects
such as 'Re', 'Good day', 'Greetings', 'Dear friend', 'Confirm', 'Attention',
and 'Hello dear'. The severity of cybercrime, especially the 419 scams, cannot
be overstated, as it erodes trust, causes financial losses, and hampers
Nigeria's reputation and economic progress. Combatting cyber scams and
enhancing cybersecurity measures are crucial to protect individuals and
organizations from falling victim to these fraudulent schemes.
</p></li>
</ul>

<h3>Title: Security Assessment and Hardening of Fog Computing Systems. (arXiv:2308.12707v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12707">http://arxiv.org/abs/2308.12707</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12707]] Security Assessment and Hardening of Fog Computing Systems(http://arxiv.org/abs/2308.12707)</code></li>
<li>Summary: <p>In recent years, there has been a shift in computing architectures, moving
away from centralized cloud computing towards decentralized edge and fog
computing. This shift is driven by factors such as the increasing volume of
data generated at the edge, the growing demand for real-time processing and
low-latency applications, and the need for improved privacy and data locality.
Although this new paradigm offers numerous advantages, it also introduces
significant security and reliability challenges. This paper aims to review the
architectures and technologies employed in fog computing and identify
opportunities for developing novel security assessment and security hardening
techniques. These techniques include secure configuration and debloating to
enhance the security of middleware, testing techniques to assess secure
communication mechanisms, and automated rehosting to speed up the security
testing of embedded firmware.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Source-Free Collaborative Domain Adaptation via Multi-Perspective Feature Enrichment for Functional MRI Analysis. (arXiv:2308.12495v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12495">http://arxiv.org/abs/2308.12495</a></li>
<li>Code URL: https://github.com/yqfang9199/scda</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12495]] Source-Free Collaborative Domain Adaptation via Multi-Perspective Feature Enrichment for Functional MRI Analysis(http://arxiv.org/abs/2308.12495)</code></li>
<li>Summary: <p>Resting-state functional MRI (rs-fMRI) is increasingly employed in multi-site
research to aid neurological disorder analysis. Existing studies usually suffer
from significant cross-site/domain data heterogeneity caused by site effects
such as differences in scanners/protocols. Many methods have been proposed to
reduce fMRI heterogeneity between source and target domains, heavily relying on
the availability of source data. But acquiring source data is challenging due
to privacy concerns and/or data storage burdens in multi-site studies. To this
end, we design a source-free collaborative domain adaptation (SCDA) framework
for fMRI analysis, where only a pretrained source model and unlabeled target
data are accessible. Specifically, a multi-perspective feature enrichment
method (MFE) is developed for target fMRI analysis, consisting of multiple
collaborative branches to dynamically capture fMRI features of unlabeled target
data from multiple views. Each branch has a data-feeding module, a
spatiotemporal feature encoder, and a class predictor. A mutual-consistency
constraint is designed to encourage pair-wise consistency of latent features of
the same input generated from these branches for robust representation
learning. To facilitate efficient cross-domain knowledge transfer without
source data, we initialize MFE using parameters of a pretrained source model.
We also introduce an unsupervised pretraining strategy using 3,806 unlabeled
fMRIs from three large-scale auxiliary databases, aiming to obtain a general
feature encoder. Experimental results on three public datasets and one private
dataset demonstrate the efficacy of our method in cross-scanner and cross-study
prediction tasks. The model pretrained on large-scale rs-fMRI data has been
released to the public.
</p></li>
</ul>

<h3>Title: Privacy engineering through obfuscation. (arXiv:2308.12514v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12514">http://arxiv.org/abs/2308.12514</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12514]] Privacy engineering through obfuscation(http://arxiv.org/abs/2308.12514)</code></li>
<li>Summary: <p>Obfuscation in privacy engineering denotes a diverse set of data operations
aimed at reducing the privacy loss that users incur in by participating in
digital systems. Obfuscation's domain of application is vast:
privacy-preserving database analysis, location-based privacy, private web
search or privacy-friendly recommender systems are but a few examples of the
contexts in which privacy engineers have resorted to obfuscation. Yet an
understanding of the role that obfuscation, in general, plays in the
engineering of privacy has so far proved elusive. Similarly, we lack a cohesive
view of the wide array of privacy measures that assist the evaluation of
obfuscation technologies. This paper contributes to closing these research
gaps. First, we provide a general analysis framework that brings together a
multiplicity of obfuscation methods under the same analytical umbrella. Second,
we distinguish between mechanism-centred and attack-centred evaluation, making
explicit a hierarchy of assumptions behind privacy measures that assists and
demystifies obfuscation tools' evaluation. Finally, we examine the role that
obfuscation technology plays in privacy engineering by introducing the concepts
of personal and public utility and distinguishing between utility-degrading and
utility-preserving obfuscation. We observe that public utility requirements
require us to resort to utility-degrading obfuscation to arbitrarily reduce
privacy loss. Conversely, personal utility requirements do not, in theory,
impose such a privacy-utility trade-off, and we illustrate how to perform
utility-preserving obfuscation through chaff.
</p></li>
</ul>

<h3>Title: Privacy-Preserving Discretized Spiking Neural Networks. (arXiv:2308.12529v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12529">http://arxiv.org/abs/2308.12529</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12529]] Privacy-Preserving Discretized Spiking Neural Networks(http://arxiv.org/abs/2308.12529)</code></li>
<li>Summary: <p>The rapid development of artificial intelligence has brought considerable
convenience, yet also introduces significant security risks. One of the
research hotspots is to balance data privacy and utility in the real world of
artificial intelligence. The present second-generation artificial neural
networks have made tremendous advances, but some big models could have really
high computational costs. The third-generation neural network, SNN (Spiking
Neural Network), mimics real neurons by using discrete spike signals, whose
sequences exhibit strong sparsity, providing advantages such as low energy
consumption and high efficiency. In this paper, we construct a framework to
evaluate the homomorphic computation of SNN named FHE-DiSNN that enables SNN to
achieve good prediction performance on encrypted data. First, benefitting from
the discrete nature of spike signals, our proposed model avoids the errors
introduced by discretizing activation functions. Second, by applying
bootstrapping, we design new private preserving functions FHE-Fire and
FHE-Reset, through which noise can be refreshed, allowing us to evaluate SNN
for an arbitrary number of operations. Furthermore, We improve the
computational efficiency of FHE-DiSNN while maintaining a high level of
accuracy. Finally, we evaluate our model on the MNIST dataset. The experiments
show that FHE-DiSNN with 30 neurons in the hidden layer achieves a minimum
prediction accuracy of 94.4%. Under optimal parameters, it achieves a 95.1%
accuracy, with only a 0.6% decrease compared to the original SNN (95.7%). These
results demonstrate the superiority of SNN over second-generation neural
networks for homomorphic evaluation.
</p></li>
</ul>

<h3>Title: Trustworthy Representation Learning Across Domains. (arXiv:2308.12315v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12315">http://arxiv.org/abs/2308.12315</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12315]] Trustworthy Representation Learning Across Domains(http://arxiv.org/abs/2308.12315)</code></li>
<li>Summary: <p>As AI systems have obtained significant performance to be deployed widely in
our daily live and human society, people both enjoy the benefits brought by
these technologies and suffer many social issues induced by these systems. To
make AI systems good enough and trustworthy, plenty of researches have been
done to build guidelines for trustworthy AI systems. Machine learning is one of
the most important parts for AI systems and representation learning is the
fundamental technology in machine learning. How to make the representation
learning trustworthy in real-world application, e.g., cross domain scenarios,
is very valuable and necessary for both machine learning and AI system fields.
Inspired by the concepts in trustworthy AI, we proposed the first trustworthy
representation learning across domains framework which includes four concepts,
i.e, robustness, privacy, fairness, and explainability, to give a comprehensive
literature review on this research direction. Specifically, we first introduce
the details of the proposed trustworthy framework for representation learning
across domains. Second, we provide basic notions and comprehensively summarize
existing methods for the trustworthy framework from four concepts. Finally, we
conclude this survey with insights and discussions on future research
directions.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: I3DOD: Towards Incremental 3D Object Detection via Prompting. (arXiv:2308.12512v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12512">http://arxiv.org/abs/2308.12512</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12512]] I3DOD: Towards Incremental 3D Object Detection via Prompting(http://arxiv.org/abs/2308.12512)</code></li>
<li>Summary: <p>3D object detection has achieved significant performance in many fields,
e.g., robotics system, autonomous driving, and augmented reality. However, most
existing methods could cause catastrophic forgetting of old classes when
performing on the class-incremental scenarios. Meanwhile, the current
class-incremental 3D object detection methods neglect the relationships between
the object localization information and category semantic information and
assume all the knowledge of old model is reliable. To address the above
challenge, we present a novel Incremental 3D Object Detection framework with
the guidance of prompting, i.e., I3DOD. Specifically, we propose a task-shared
prompts mechanism to learn the matching relationships between the object
localization information and category semantic information. After training on
the current task, these prompts will be stored in our prompt pool, and perform
the relationship of old classes in the next task. Moreover, we design a
reliable distillation strategy to transfer knowledge from two aspects: a
reliable dynamic distillation is developed to filter out the negative knowledge
and transfer the reliable 3D knowledge to new detection model; the relation
feature is proposed to capture the responses relation in feature space and
protect plasticity of the model when learning novel 3D classes. To the end, we
conduct comprehensive experiments on two benchmark datasets and our method
outperforms the state-of-the-art object detection methods by 0.6% - 2.7% in
terms of mAP@0.25.
</p></li>
</ul>

<h2>defense</h2>
<h3>Title: BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection. (arXiv:2308.12439v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12439">http://arxiv.org/abs/2308.12439</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12439]] BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection(http://arxiv.org/abs/2308.12439)</code></li>
<li>Summary: <p>We present a novel defense, against backdoor attacks on Deep Neural Networks
(DNNs), wherein adversaries covertly implant malicious behaviors (backdoors)
into DNNs. Our defense falls within the category of post-development defenses
that operate independently of how the model was generated. The proposed defense
is built upon a novel reverse engineering approach that can directly extract
backdoor functionality of a given backdoored model to a backdoor expert model.
The approach is straightforward -- finetuning the backdoored model over a small
set of intentionally mislabeled clean samples, such that it unlearns the normal
functionality while still preserving the backdoor functionality, and thus
resulting in a model (dubbed a backdoor expert model) that can only recognize
backdoor inputs. Based on the extracted backdoor expert model, we show the
feasibility of devising highly accurate backdoor input detectors that filter
out the backdoor inputs during model inference. Further augmented by an
ensemble strategy with a finetuned auxiliary model, our defense, BaDExpert
(Backdoor Input Detection with Backdoor Expert), effectively mitigates 16 SOTA
backdoor attacks while minimally impacting clean utility. The effectiveness of
BaDExpert has been verified on multiple datasets (CIFAR10, GTSRB and ImageNet)
across various model architectures (ResNet, VGG, MobileNetV2 and Vision
Transformer).
</p></li>
</ul>

<h3>Title: VetIoT: On Vetting IoT Defenses Enforcing Policies at Runtime. (arXiv:2308.12417v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12417">http://arxiv.org/abs/2308.12417</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12417]] VetIoT: On Vetting IoT Defenses Enforcing Policies at Runtime(http://arxiv.org/abs/2308.12417)</code></li>
<li>Summary: <p>Smart homes are powered by numerous programmable IoT platforms. Despite
tremendous innovations, these platforms often suffer from safety and security
issues. One class of defense solutions dynamically enforces safety and security
policies, which essentially capture the expected behavior of the IoT system.
While many proposed works were built on this runtime approach, they all are
under-vetted. The primary reason lies in their evaluation approach. They are
mostly self-evaluated in isolation using a virtual testbed combined with
manually orchestrated test scenarios that rely on user interactions with the
platform's UI. Such hand-crafted and non-uniform evaluation setups are limiting
not only the reproducibility but also a comparative analysis of their efficacy
results. Closing this gap in the traditional way requires a huge upfront manual
effort, which causes the researchers turn away from any large-scale comparative
empirical evaluation. Therefore, in this paper, we propose a highly-automated
uniform evaluation platform, dubbed VetIoT, to vet the defense solutions that
hinge on runtime policy enforcement. Given a defense solution, VetIoT easily
instantiates a virtual testbed inside which the solution is empirically
evaluated. VetIoT replaces manual UI-based interactions with an automated event
simulator and manual inspection of test outcomes with an automated comparator.
We developed a fully-functional prototype of VetIoT and applied it on three
runtime policy enforcement solutions: Expat, Patriot, and IoTguard. VetIoT
reproduced their individual prior results and assessed their efficacy results
via stress testing and differential testing. We believe VetIoT can foster
future research/evaluation.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: RemovalNet: DNN Fingerprint Removal Attacks. (arXiv:2308.12319v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12319">http://arxiv.org/abs/2308.12319</a></li>
<li>Code URL: https://github.com/grasses/removalnet</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12319]] RemovalNet: DNN Fingerprint Removal Attacks(http://arxiv.org/abs/2308.12319)</code></li>
<li>Summary: <p>With the performance of deep neural networks (DNNs) remarkably improving,
DNNs have been widely used in many areas. Consequently, the DNN model has
become a valuable asset, and its intellectual property is safeguarded by
ownership verification techniques (e.g., DNN fingerprinting). However, the
feasibility of the DNN fingerprint removal attack and its potential influence
remains an open problem. In this paper, we perform the first comprehensive
investigation of DNN fingerprint removal attacks. Generally, the knowledge
contained in a DNN model can be categorized into general semantic and
fingerprint-specific knowledge. To this end, we propose a min-max bilevel
optimization-based DNN fingerprint removal attack named RemovalNet, to evade
model ownership verification. The lower-level optimization is designed to
remove fingerprint-specific knowledge. While in the upper-level optimization,
we distill the victim model's general semantic knowledge to maintain the
surrogate model's performance. We conduct extensive experiments to evaluate the
fidelity, effectiveness, and efficiency of the RemovalNet against four advanced
defense methods on six metrics. The empirical results demonstrate that (1) the
RemovalNet is effective. After our DNN fingerprint removal attack, the model
distance between the target and surrogate models is x100 times higher than that
of the baseline attacks, (2) the RemovalNet is efficient. It uses only 0.2%
(400 samples) of the substitute dataset and 1,000 iterations to conduct our
attack. Besides, compared with advanced model stealing attacks, the RemovalNet
saves nearly 85% of computational resources at most, (3) the RemovalNet
achieves high fidelity that the created surrogate model maintains high accuracy
after the DNN fingerprint removal process. Our code is available at:
https://github.com/grasses/RemovalNet.
</p></li>
</ul>

<h3>Title: Saliency-based Video Summarization for Face Anti-spoofing. (arXiv:2308.12364v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12364">http://arxiv.org/abs/2308.12364</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12364]] Saliency-based Video Summarization for Face Anti-spoofing(http://arxiv.org/abs/2308.12364)</code></li>
<li>Summary: <p>Due to the growing availability of face anti-spoofing databases, researchers
are increasingly focusing on video-based methods that use hundreds to thousands
of images to assess their impact on performance. However, there is no clear
consensus on the exact number of frames in a video required to improve the
performance of face anti-spoofing tasks. Inspired by the visual saliency
theory, we present a video summarization method for face anti-spoofing tasks
that aims to enhance the performance and efficiency of deep learning models by
leveraging visual saliency. In particular, saliency information is extracted
from the differences between the Laplacian and Wiener filter outputs of the
source images, enabling identification of the most visually salient regions
within each frame. Subsequently, the source images are decomposed into base and
detail layers, enhancing representation of important information. The weighting
maps are then computed based on the saliency information, indicating the
importance of each pixel in the image. By linearly combining the base and
detail layers using the weighting maps, the method fuses the source images to
create a single representative image that summarizes the entire video. The key
contribution of our proposed method lies in demonstrating how visual saliency
can be used as a data-centric approach to improve the performance and
efficiency of face presentation attack detection models. By focusing on the
most salient images or regions within the images, a more representative and
diverse training set can be created, potentially leading to more effective
models. To validate the method's effectiveness, a simple deep learning
architecture (CNN-RNN) was used, and the experimental results showcased
state-of-the-art performance on five challenging face anti-spoofing datasets.
</p></li>
</ul>

<h3>Title: Don't Look into the Sun: Adversarial Solarization Attacks on Image Classifiers. (arXiv:2308.12661v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12661">http://arxiv.org/abs/2308.12661</a></li>
<li>Code URL: https://github.com/paulgavrikov/adversarial_solarization</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12661]] Don't Look into the Sun: Adversarial Solarization Attacks on Image Classifiers(http://arxiv.org/abs/2308.12661)</code></li>
<li>Summary: <p>Assessing the robustness of deep neural networks against out-of-distribution
inputs is crucial, especially in safety-critical domains like autonomous
driving, but also in safety systems where malicious actors can digitally alter
inputs to circumvent safety guards. However, designing effective
out-of-distribution tests that encompass all possible scenarios while
preserving accurate label information is a challenging task. Existing
methodologies often entail a compromise between variety and constraint levels
for attacks and sometimes even both. In a first step towards a more holistic
robustness evaluation of image classification models, we introduce an attack
method based on image solarization that is conceptually straightforward yet
avoids jeopardizing the global structure of natural images independent of the
intensity. Through comprehensive evaluations of multiple ImageNet models, we
demonstrate the attack's capacity to degrade accuracy significantly, provided
it is not integrated into the training augmentations. Interestingly, even then,
no full immunity to accuracy deterioration is achieved. In other settings, the
attack can often be simplified into a black-box attack with model-independent
parameters. Defenses against other corruptions do not consistently extend to be
effective against our specific attack.
</p>
<p>Project website: https://github.com/paulgavrikov/adversarial_solarization
</p></li>
</ul>

<h3>Title: Introducing a New Alert Data Set for Multi-Step Attack Analysis. (arXiv:2308.12627v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12627">http://arxiv.org/abs/2308.12627</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12627]] Introducing a New Alert Data Set for Multi-Step Attack Analysis(http://arxiv.org/abs/2308.12627)</code></li>
<li>Summary: <p>Intrusion detection systems (IDS) reinforce cyber defense by autonomously
monitoring various data sources for traces of attacks. However, IDSs are also
infamous for frequently raising false positives and alerts that are difficult
to interpret without context. This results in high workloads on security
operators who need to manually verify all reported alerts, often leading to
fatigue and incorrect decisions. To generate more meaningful alerts and
alleviate these issues, the research domain focused on multi-step attack
analysis proposes approaches for filtering, clustering, and correlating IDS
alerts, as well as generation of attack graphs. Unfortunately, existing data
sets are outdated, unreliable, narrowly focused, or only suitable for IDS
evaluation. Since hardly any suitable benchmark data sets are publicly
available, researchers often resort to private data sets that prevent
reproducibility of evaluations. We therefore generate a new alert data set that
we publish alongside this paper. The data set contains alerts from three
distinct IDSs monitoring eight executions of a multi-step attack as well as
simulations of normal user behavior. To illustrate the potential of our data
set, we experiment with alert prioritization as well as two open-source tools
for meta-alert generation and attack graph extraction.
</p></li>
</ul>

<h3>Title: Fast Adversarial Training with Smooth Convergence. (arXiv:2308.12857v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12857">http://arxiv.org/abs/2308.12857</a></li>
<li>Code URL: https://github.com/fat-cs/convergesmooth</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12857]] Fast Adversarial Training with Smooth Convergence(http://arxiv.org/abs/2308.12857)</code></li>
<li>Summary: <p>Fast adversarial training (FAT) is beneficial for improving the adversarial
robustness of neural networks. However, previous FAT work has encountered a
significant issue known as catastrophic overfitting when dealing with large
perturbation budgets, \ie the adversarial robustness of models declines to near
zero during training.
</p>
<p>To address this, we analyze the training process of prior FAT work and
observe that catastrophic overfitting is accompanied by the appearance of loss
convergence outliers.
</p>
<p>Therefore, we argue a moderately smooth loss convergence process will be a
stable FAT process that solves catastrophic overfitting.
</p>
<p>To obtain a smooth loss convergence process, we propose a novel oscillatory
constraint (dubbed ConvergeSmooth) to limit the loss difference between
adjacent epochs. The convergence stride of ConvergeSmooth is introduced to
balance convergence and smoothing. Likewise, we design weight centralization
without introducing additional hyperparameters other than the loss balance
coefficient.
</p>
<p>Our proposed methods are attack-agnostic and thus can improve the training
stability of various FAT techniques.
</p>
<p>Extensive experiments on popular datasets show that the proposed methods
efficiently avoid catastrophic overfitting and outperform all previous FAT
methods. Code is available at \url{https://github.com/FAT-CS/ConvergeSmooth}.
</p></li>
</ul>

<h3>Title: Evaluating the Vulnerabilities in ML systems in terms of adversarial attacks. (arXiv:2308.12918v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12918">http://arxiv.org/abs/2308.12918</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12918]] Evaluating the Vulnerabilities in ML systems in terms of adversarial attacks(http://arxiv.org/abs/2308.12918)</code></li>
<li>Summary: <p>There have been recent adversarial attacks that are difficult to find. These
new adversarial attacks methods may pose challenges to current deep learning
cyber defense systems and could influence the future defense of cyberattacks.
The authors focus on this domain in this research paper. They explore the
consequences of vulnerabilities in AI systems. This includes discussing how
they might arise, differences between randomized and adversarial examples and
also potential ethical implications of vulnerabilities. Moreover, it is
important to train the AI systems appropriately when they are in testing phase
and getting them ready for broader use.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Uniformly Distributed Category Prototype-Guided Vision-Language Framework for Long-Tail Recognition. (arXiv:2308.12522v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12522">http://arxiv.org/abs/2308.12522</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12522]] Uniformly Distributed Category Prototype-Guided Vision-Language Framework for Long-Tail Recognition(http://arxiv.org/abs/2308.12522)</code></li>
<li>Summary: <p>Recently, large-scale pre-trained vision-language models have presented
benefits for alleviating class imbalance in long-tailed recognition. However,
the long-tailed data distribution can corrupt the representation space, where
the distance between head and tail categories is much larger than the distance
between two tail categories. This uneven feature space distribution causes the
model to exhibit unclear and inseparable decision boundaries on the uniformly
distributed test set, which lowers its performance. To address these
challenges, we propose the uniformly category prototype-guided vision-language
framework to effectively mitigate feature space bias caused by data imbalance.
Especially, we generate a set of category prototypes uniformly distributed on a
hypersphere. Category prototype-guided mechanism for image-text matching makes
the features of different classes converge to these distinct and uniformly
distributed category prototypes, which maintain a uniform distribution in the
feature space, and improve class boundaries. Additionally, our proposed
irrelevant text filtering and attribute enhancement module allows the model to
ignore irrelevant noisy text and focus more on key attribute information,
thereby enhancing the robustness of our framework. In the image recognition
fine-tuning stage, to address the positive bias problem of the learnable
classifier, we design the class feature prototype-guided classifier, which
compensates for the performance of tail classes while maintaining the
performance of head classes. Our method outperforms previous vision-language
methods for long-tailed learning work by a large margin and achieves
state-of-the-art performance.
</p></li>
</ul>

<h3>Title: PoseSync: Robust pose based video synchronization. (arXiv:2308.12600v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12600">http://arxiv.org/abs/2308.12600</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12600]] PoseSync: Robust pose based video synchronization(http://arxiv.org/abs/2308.12600)</code></li>
<li>Summary: <p>Pose based video sychronization can have applications in multiple domains
such as gameplay performance evaluation, choreography or guiding athletes. The
subject's actions could be compared and evaluated against those performed by
professionals side by side. In this paper, we propose an end to end pipeline
for synchronizing videos based on pose. The first step crops the region where
the person present in the image followed by pose detection on the cropped
image. This is followed by application of Dynamic Time Warping(DTW) on angle/
distance measures between the pose keypoints leading to a scale and shift
invariant pose matching pipeline.
</p></li>
</ul>

<h3>Title: HR-Pro: Point-supervised Temporal Action Localization via Hierarchical Reliability Propagation. (arXiv:2308.12608v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12608">http://arxiv.org/abs/2308.12608</a></li>
<li>Code URL: https://github.com/pipixin321/hr-pro</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12608]] HR-Pro: Point-supervised Temporal Action Localization via Hierarchical Reliability Propagation(http://arxiv.org/abs/2308.12608)</code></li>
<li>Summary: <p>Point-supervised Temporal Action Localization (PSTAL) is an emerging research
direction for label-efficient learning. However, current methods mainly focus
on optimizing the network either at the snippet-level or the instance-level,
neglecting the inherent reliability of point annotations at both levels. In
this paper, we propose a Hierarchical Reliability Propagation (HR-Pro)
framework, which consists of two reliability-aware stages: Snippet-level
Discrimination Learning and Instance-level Completeness Learning, both stages
explore the efficient propagation of high-confidence cues in point annotations.
For snippet-level learning, we introduce an online-updated memory to store
reliable snippet prototypes for each class. We then employ a Reliability-aware
Attention Block to capture both intra-video and inter-video dependencies of
snippets, resulting in more discriminative and robust snippet representation.
For instance-level learning, we propose a point-based proposal generation
approach as a means of connecting snippets and instances, which produces
high-confidence proposals for further optimization at the instance level.
Through multi-level reliability-aware learning, we obtain more reliable
confidence scores and more accurate temporal boundaries of predicted proposals.
Our HR-Pro achieves state-of-the-art performance on multiple challenging
benchmarks, including an impressive average mAP of 60.3% on THUMOS14. Notably,
our HR-Pro largely surpasses all previous point-supervised methods, and even
outperforms several competitive fully supervised methods. Code will be
available at https://github.com/pipixin321/HR-Pro.
</p></li>
</ul>

<h3>Title: Cross-Video Contextual Knowledge Exploration and Exploitation for Ambiguity Reduction in Weakly Supervised Temporal Action Localization. (arXiv:2308.12609v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12609">http://arxiv.org/abs/2308.12609</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12609]] Cross-Video Contextual Knowledge Exploration and Exploitation for Ambiguity Reduction in Weakly Supervised Temporal Action Localization(http://arxiv.org/abs/2308.12609)</code></li>
<li>Summary: <p>Weakly supervised temporal action localization (WSTAL) aims to localize
actions in untrimmed videos using video-level labels. Despite recent advances,
existing approaches mainly follow a localization-by-classification pipeline,
generally processing each segment individually, thereby exploiting only limited
contextual information. As a result, the model will lack a comprehensive
understanding (e.g. appearance and temporal structure) of various action
patterns, leading to ambiguity in classification learning and temporal
localization. Our work addresses this from a novel perspective, by exploring
and exploiting the cross-video contextual knowledge within the dataset to
recover the dataset-level semantic structure of action instances via weak
labels only, thereby indirectly improving the holistic understanding of
fine-grained action patterns and alleviating the aforementioned ambiguities.
Specifically, an end-to-end framework is proposed, including a Robust
Memory-Guided Contrastive Learning (RMGCL) module and a Global Knowledge
Summarization and Aggregation (GKSA) module. First, the RMGCL module explores
the contrast and consistency of cross-video action features, assisting in
learning more structured and compact embedding space, thus reducing ambiguity
in classification learning. Further, the GKSA module is used to efficiently
summarize and propagate the cross-video representative action knowledge in a
learnable manner to promote holistic action patterns understanding, which in
turn allows the generation of high-confidence pseudo-labels for self-learning,
thus alleviating ambiguity in temporal localization. Extensive experiments on
THUMOS14, ActivityNet1.3, and FineAction demonstrate that our method
outperforms the state-of-the-art methods, and can be easily plugged into other
WSTAL methods.
</p></li>
</ul>

<h3>Title: LISTER: Neighbor Decoding for Length-Insensitive Scene Text Recognition. (arXiv:2308.12774v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12774">http://arxiv.org/abs/2308.12774</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12774]] LISTER: Neighbor Decoding for Length-Insensitive Scene Text Recognition(http://arxiv.org/abs/2308.12774)</code></li>
<li>Summary: <p>The diversity in length constitutes a significant characteristic of text. Due
to the long-tail distribution of text lengths, most existing methods for scene
text recognition (STR) only work well on short or seen-length text, lacking the
capability of recognizing longer text or performing length extrapolation. This
is a crucial issue, since the lengths of the text to be recognized are usually
not given in advance in real-world applications, but it has not been adequately
investigated in previous works. Therefore, we propose in this paper a method
called Length-Insensitive Scene TExt Recognizer (LISTER), which remedies the
limitation regarding the robustness to various text lengths. Specifically, a
Neighbor Decoder is proposed to obtain accurate character attention maps with
the assistance of a novel neighbor matrix regardless of the text lengths.
Besides, a Feature Enhancement Module is devised to model the long-range
dependency with low computation cost, which is able to perform iterations with
the neighbor decoder to enhance the feature map progressively. To the best of
our knowledge, we are the first to achieve effective length-insensitive scene
text recognition. Extensive experiments demonstrate that the proposed LISTER
algorithm exhibits obvious superiority on long text recognition and the ability
for length extrapolation, while comparing favourably with the previous
state-of-the-art methods on standard benchmarks for STR (mainly short text).
</p></li>
</ul>

<h3>Title: Implicit Obstacle Map-driven Indoor Navigation Model for Robust Obstacle Avoidance. (arXiv:2308.12845v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12845">http://arxiv.org/abs/2308.12845</a></li>
<li>Code URL: https://github.com/xwaiyy123/object-navigation</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12845]] Implicit Obstacle Map-driven Indoor Navigation Model for Robust Obstacle Avoidance(http://arxiv.org/abs/2308.12845)</code></li>
<li>Summary: <p>Robust obstacle avoidance is one of the critical steps for successful
goal-driven indoor navigation tasks.Due to the obstacle missing in the visual
image and the possible missed detection issue, visual image-based obstacle
avoidance techniques still suffer from unsatisfactory robustness. To mitigate
it, in this paper, we propose a novel implicit obstacle map-driven indoor
navigation framework for robust obstacle avoidance, where an implicit obstacle
map is learned based on the historical trial-and-error experience rather than
the visual image. In order to further improve the navigation efficiency, a
non-local target memory aggregation module is designed to leverage a non-local
network to model the intrinsic relationship between the target semantic and the
target orientation clues during the navigation process so as to mine the most
target-correlated object clues for the navigation decision. Extensive
experimental results on AI2-Thor and RoboTHOR benchmarks verify the excellent
obstacle avoidance and navigation efficiency of our proposed method. The core
source code is available at https://github.com/xwaiyy123/object-navigation.
</p></li>
</ul>

<h3>Title: CDAN: Convolutional Dense Attention-guided Network for Low-light Image Enhancement. (arXiv:2308.12902v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12902">http://arxiv.org/abs/2308.12902</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12902]] CDAN: Convolutional Dense Attention-guided Network for Low-light Image Enhancement(http://arxiv.org/abs/2308.12902)</code></li>
<li>Summary: <p>Low-light images, characterized by inadequate illumination, pose challenges
of diminished clarity, muted colors, and reduced details. Low-light image
enhancement, an essential task in computer vision, aims to rectify these issues
by improving brightness, contrast, and overall perceptual quality, thereby
facilitating accurate analysis and interpretation. This paper introduces the
Convolutional Dense Attention-guided Network (CDAN), a novel solution for
enhancing low-light images. CDAN integrates an autoencoder-based architecture
with convolutional and dense blocks, complemented by an attention mechanism and
skip connections. This architecture ensures efficient information propagation
and feature learning. Furthermore, a dedicated post-processing phase refines
color balance and contrast. Our approach demonstrates notable progress compared
to state-of-the-art results in low-light image enhancement, showcasing its
robustness across a wide range of challenging scenarios. Our model performs
remarkably on benchmark datasets, effectively mitigating under-exposure and
proficiently restoring textures and colors in diverse low-light scenarios. This
achievement underscores CDAN's potential for diverse computer vision tasks,
notably enabling robust object detection and recognition in challenging
low-light conditions.
</p></li>
</ul>

<h3>Title: Label Budget Allocation in Multi-Task Learning. (arXiv:2308.12949v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12949">http://arxiv.org/abs/2308.12949</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12949]] Label Budget Allocation in Multi-Task Learning(http://arxiv.org/abs/2308.12949)</code></li>
<li>Summary: <p>The cost of labeling data often limits the performance of machine learning
systems. In multi-task learning, related tasks provide information to each
other and improve overall performance, but the label cost can vary among tasks.
How should the label budget (i.e. the amount of money spent on labeling) be
allocated among different tasks to achieve optimal multi-task performance? We
are the first to propose and formally define the label budget allocation
problem in multi-task learning and to empirically show that different budget
allocation strategies make a big difference to its performance. We propose a
Task-Adaptive Budget Allocation algorithm to robustly generate the optimal
budget allocation adaptive to different multi-task learning settings.
Specifically, we estimate and then maximize the extent of new information
obtained from the allocated budget as a proxy for multi-task learning
performance. Experiments on PASCAL VOC and Taskonomy demonstrate the efficacy
of our approach over other widely used heuristic labeling strategies.
</p></li>
</ul>

<h3>Title: ROAM: Robust and Object-aware Motion Generation using Neural Pose Descriptors. (arXiv:2308.12969v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12969">http://arxiv.org/abs/2308.12969</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12969]] ROAM: Robust and Object-aware Motion Generation using Neural Pose Descriptors(http://arxiv.org/abs/2308.12969)</code></li>
<li>Summary: <p>Existing automatic approaches for 3D virtual character motion synthesis
supporting scene interactions do not generalise well to new objects outside
training distributions, even when trained on extensive motion capture datasets
with diverse objects and annotated interactions. This paper addresses this
limitation and shows that robustness and generalisation to novel scene objects
in 3D object-aware character synthesis can be achieved by training a motion
model with as few as one reference object. We leverage an implicit feature
representation trained on object-only datasets, which encodes an
SE(3)-equivariant descriptor field around the object. Given an unseen object
and a reference pose-object pair, we optimise for the object-aware pose that is
closest in the feature space to the reference pose. Finally, we use l-NSM,
i.e., our motion generation model that is trained to seamlessly transition from
locomotion to object interaction with the proposed bidirectional pose blending
scheme. Through comprehensive numerical comparisons to state-of-the-art methods
and in a user study, we demonstrate substantial improvements in 3D virtual
character motion and interaction quality and robustness to scenarios with
unseen objects. Our project page is available at
https://vcai.mpi-inf.mpg.de/projects/ROAM/.
</p></li>
</ul>

<h3>Title: MultiPA: a multi-task speech pronunciation assessment system for a closed and open response scenario. (arXiv:2308.12490v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12490">http://arxiv.org/abs/2308.12490</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12490]] MultiPA: a multi-task speech pronunciation assessment system for a closed and open response scenario(http://arxiv.org/abs/2308.12490)</code></li>
<li>Summary: <p>The design of automatic speech pronunciation assessment can be categorized
into closed and open response scenarios, each with strengths and limitations. A
system with the ability to function in both scenarios can cater to diverse
learning needs and provide a more precise and holistic assessment of
pronunciation skills. In this study, we propose a Multi-task Pronunciation
Assessment model called MultiPA. MultiPA provides an alternative to Kaldi-based
systems in that it has simpler format requirements and better compatibility
with other neural network models. Compared with previous open response systems,
MultiPA provides a wider range of evaluations, encompassing assessments at both
the sentence and word-level. Our experimental results show that MultiPA
achieves comparable performance when working in closed response scenarios and
maintains more robust performance when directly used for open responses.
</p></li>
</ul>

<h3>Title: FOSA: Full Information Maximum Likelihood (FIML) Optimized Self-Attention Imputation for Missing Data. (arXiv:2308.12388v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12388">http://arxiv.org/abs/2308.12388</a></li>
<li>Code URL: https://github.com/oudeng/fosa</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12388]] FOSA: Full Information Maximum Likelihood (FIML) Optimized Self-Attention Imputation for Missing Data(http://arxiv.org/abs/2308.12388)</code></li>
<li>Summary: <p>In data imputation, effectively addressing missing values is pivotal,
especially in intricate datasets. This paper delves into the FIML Optimized
Self-attention (FOSA) framework, an innovative approach that amalgamates the
strengths of Full Information Maximum Likelihood (FIML) estimation with the
capabilities of self-attention neural networks. Our methodology commences with
an initial estimation of missing values via FIML, subsequently refining these
estimates by leveraging the self-attention mechanism. Our comprehensive
experiments on both simulated and real-world datasets underscore FOSA's
pronounced advantages over traditional FIML techniques, encapsulating facets of
accuracy, computational efficiency, and adaptability to diverse data
structures. Intriguingly, even in scenarios where the Structural Equation Model
(SEM) might be mis-specified, leading to suboptimal FIML estimates, the robust
architecture of FOSA's self-attention component adeptly rectifies and optimizes
the imputation outcomes. Our empirical tests reveal that FOSA consistently
delivers commendable predictions, even in the face of up to 40% random
missingness, highlighting its robustness and potential for wide-scale
applications in data imputation.
</p></li>
</ul>

<h3>Title: Machine learning in parameter estimation of nonlinear systems. (arXiv:2308.12393v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12393">http://arxiv.org/abs/2308.12393</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12393]] Machine learning in parameter estimation of nonlinear systems(http://arxiv.org/abs/2308.12393)</code></li>
<li>Summary: <p>Accurately estimating parameters in complex nonlinear systems is crucial
across scientific and engineering fields. We present a novel approach for
parameter estimation using a neural network with the Huber loss function. This
method taps into deep learning's abilities to uncover parameters governing
intricate behaviors in nonlinear equations. We validate our approach using
synthetic data and predefined functions that model system dynamics. By training
the neural network with noisy time series data, it fine-tunes the Huber loss
function to converge to accurate parameters. We apply our method to damped
oscillators, Van der Pol oscillators, Lotka-Volterra systems, and Lorenz
systems under multiplicative noise. The trained neural network accurately
estimates parameters, evident from closely matching latent dynamics. Comparing
true and estimated trajectories visually reinforces our method's precision and
robustness. Our study underscores the Huber loss-guided neural network as a
versatile tool for parameter estimation, effectively uncovering complex
relationships in nonlinear systems. The method navigates noise and uncertainty
adeptly, showcasing its adaptability to real-world challenges.
</p></li>
</ul>

<h3>Title: A Co-training Approach for Noisy Time Series Learning. (arXiv:2308.12551v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12551">http://arxiv.org/abs/2308.12551</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12551]] A Co-training Approach for Noisy Time Series Learning(http://arxiv.org/abs/2308.12551)</code></li>
<li>Summary: <p>In this work, we focus on robust time series representation learning. Our
assumption is that real-world time series is noisy and complementary
information from different views of the same time series plays an important
role while analyzing noisy input. Based on this, we create two views for the
input time series through two different encoders. We conduct co-training based
contrastive learning iteratively to learn the encoders. Our experiments
demonstrate that this co-training approach leads to a significant improvement
in performance. Especially, by leveraging the complementary information from
different views, our proposed TS-CoT method can mitigate the impact of data
noise and corruption. Empirical evaluations on four time series benchmarks in
unsupervised and semi-supervised settings reveal that TS-CoT outperforms
existing methods. Furthermore, the representations learned by TS-CoT can
transfer well to downstream tasks through fine-tuning.
</p></li>
</ul>

<h3>Title: Hypergraph Convolutional Networks for Fine-grained ICU Patient Similarity Analysis and Risk Prediction. (arXiv:2308.12575v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12575">http://arxiv.org/abs/2308.12575</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12575]] Hypergraph Convolutional Networks for Fine-grained ICU Patient Similarity Analysis and Risk Prediction(http://arxiv.org/abs/2308.12575)</code></li>
<li>Summary: <p>The Intensive Care Unit (ICU) is one of the most important parts of a
hospital, which admits critically ill patients and provides continuous
monitoring and treatment. Various patient outcome prediction methods have been
attempted to assist healthcare professionals in clinical decision-making.
Existing methods focus on measuring the similarity between patients using deep
neural networks to capture the hidden feature structures. However, the
higher-order relationships are ignored, such as patient characteristics (e.g.,
diagnosis codes) and their causal effects on downstream clinical predictions.
</p>
<p>In this paper, we propose a novel Hypergraph Convolutional Network that
allows the representation of non-pairwise relationships among diagnosis codes
in a hypergraph to capture the hidden feature structures so that fine-grained
patient similarity can be calculated for personalized mortality risk
prediction. Evaluation using a publicly available eICU Collaborative Research
Database indicates that our method achieves superior performance over the
state-of-the-art models on mortality risk prediction. Moreover, the results of
several case studies demonstrated the effectiveness of constructing graph
networks in providing good transparency and robustness in decision-making.
</p></li>
</ul>

<h3>Title: A Huber Loss Minimization Approach to Byzantine Robust Federated Learning. (arXiv:2308.12581v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12581">http://arxiv.org/abs/2308.12581</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12581]] A Huber Loss Minimization Approach to Byzantine Robust Federated Learning(http://arxiv.org/abs/2308.12581)</code></li>
<li>Summary: <p>Federated learning systems are susceptible to adversarial attacks. To combat
this, we introduce a novel aggregator based on Huber loss minimization, and
provide a comprehensive theoretical analysis. Under independent and identically
distributed (i.i.d) assumption, our approach has several advantages compared to
existing methods. Firstly, it has optimal dependence on $\epsilon$, which
stands for the ratio of attacked clients. Secondly, our approach does not need
precise knowledge of $\epsilon$. Thirdly, it allows different clients to have
unequal data sizes. We then broaden our analysis to include non-i.i.d data,
such that clients have slightly different distributions.
</p></li>
</ul>

<h3>Title: Disentanglement Learning via Topology. (arXiv:2308.12696v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12696">http://arxiv.org/abs/2308.12696</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12696]] Disentanglement Learning via Topology(http://arxiv.org/abs/2308.12696)</code></li>
<li>Summary: <p>We propose TopDis (Topological Disentanglement), a method for learning
disentangled representations via adding multi-scale topological loss term.
Disentanglement is a crucial property of data representations substantial for
the explainability and robustness of deep learning models and a step towards
high-level cognition. The state-of-the-art method based on VAE minimizes the
total correlation of the joint distribution of latent variables. We take a
different perspective on disentanglement by analyzing topological properties of
data manifolds. In particular, we optimize the topological similarity for data
manifolds traversals. To the best of our knowledge, our paper is the first one
to propose a differentiable topological loss for disentanglement. Our
experiments have shown that the proposed topological loss improves
disentanglement scores such as MIG, FactorVAE score, SAP score and DCI
disentanglement score with respect to state-of-the-art results. Our method
works in an unsupervised manner, permitting to apply it for problems without
labeled factors of variation. Additionally, we show how to use the proposed
topological loss to find disentangled directions in a trained GAN.
</p></li>
</ul>

<h3>Title: Prediction without Preclusion: Recourse Verification with Reachable Sets. (arXiv:2308.12820v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12820">http://arxiv.org/abs/2308.12820</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12820]] Prediction without Preclusion: Recourse Verification with Reachable Sets(http://arxiv.org/abs/2308.12820)</code></li>
<li>Summary: <p>Machine learning models are often used to decide who will receive a loan, a
job interview, or a public benefit. Standard techniques to build these models
use features about people but overlook their actionability. In turn, models can
assign predictions that are fixed, meaning that consumers who are denied loans,
interviews, or benefits may be permanently locked out from access to credit,
employment, or assistance. In this work, we introduce a formal testing
procedure to flag models that assign fixed predictions that we call recourse
verification. We develop machinery to reliably determine if a given model can
provide recourse to its decision subjects from a set of user-specified
actionability constraints. We demonstrate how our tools can ensure recourse and
adversarial robustness in real-world datasets and use them to study the
infeasibility of recourse in real-world lending datasets. Our results highlight
how models can inadvertently assign fixed predictions that permanently bar
access, and we provide tools to design algorithms that account for
actionability when developing models.
</p></li>
</ul>

<h3>Title: Auto-weighted Bayesian Physics-Informed Neural Networks and robust estimations for multitask inverse problems in pore-scale imaging of dissolution. (arXiv:2308.12864v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12864">http://arxiv.org/abs/2308.12864</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12864]] Auto-weighted Bayesian Physics-Informed Neural Networks and robust estimations for multitask inverse problems in pore-scale imaging of dissolution(http://arxiv.org/abs/2308.12864)</code></li>
<li>Summary: <p>In this article, we present a novel data assimilation strategy in pore-scale
imaging and demonstrate that this makes it possible to robustly address
reactive inverse problems incorporating Uncertainty Quantification (UQ).
Pore-scale modeling of reactive flow offers a valuable opportunity to
investigate the evolution of macro-scale properties subject to dynamic
processes. Yet, they suffer from imaging limitations arising from the
associated X-ray microtomography (X-ray microCT) process, which induces
discrepancies in the properties estimates. Assessment of the kinetic parameters
also raises challenges, as reactive coefficients are critical parameters that
can cover a wide range of values. We account for these two issues and ensure
reliable calibration of pore-scale modeling, based on dynamical microCT images,
by integrating uncertainty quantification in the workflow.
</p>
<p>The present method is based on a multitasking formulation of reactive inverse
problems combining data-driven and physics-informed techniques in calcite
dissolution. This allows quantifying morphological uncertainties on the
porosity field and estimating reactive parameter ranges through prescribed PDE
models with a latent concentration field and dynamical microCT. The data
assimilation strategy relies on sequential reinforcement incorporating
successively additional PDE constraints. We guarantee robust and unbiased
uncertainty quantification by straightforward adaptive weighting of Bayesian
Physics-Informed Neural Networks (BPINNs), ensuring reliable micro-porosity
changes during geochemical transformations. We demonstrate successful Bayesian
Inference in 1D+Time and 2D+Time calcite dissolution based on synthetic microCT
images with meaningful posterior distribution on the reactive parameters and
dimensionless numbers.
</p></li>
</ul>

<h3>Title: Collect, Measure, Repeat: Reliability Factors for Responsible AI Data Collection. (arXiv:2308.12885v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12885">http://arxiv.org/abs/2308.12885</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12885]] Collect, Measure, Repeat: Reliability Factors for Responsible AI Data Collection(http://arxiv.org/abs/2308.12885)</code></li>
<li>Summary: <p>The rapid entry of machine learning approaches in our daily activities and
high-stakes domains demands transparency and scrutiny of their fairness and
reliability. To help gauge machine learning models' robustness, research
typically focuses on the massive datasets used for their deployment, e.g.,
creating and maintaining documentation for understanding their origin, process
of development, and ethical considerations. However, data collection for AI is
still typically a one-off practice, and oftentimes datasets collected for a
certain purpose or application are reused for a different problem.
Additionally, dataset annotations may not be representative over time, contain
ambiguous or erroneous annotations, or be unable to generalize across issues or
domains. Recent research has shown these practices might lead to unfair,
biased, or inaccurate outcomes. We argue that data collection for AI should be
performed in a responsible manner where the quality of the data is thoroughly
scrutinized and measured through a systematic set of appropriate metrics. In
this paper, we propose a Responsible AI (RAI) methodology designed to guide the
data collection with a set of metrics for an iterative in-depth analysis of the
factors influencing the quality and reliability} of the generated data. We
propose a granular set of measurements to inform on the internal reliability of
a dataset and its external stability over time. We validate our approach across
nine existing datasets and annotation tasks and four content modalities. This
approach impacts the assessment of data robustness used for AI applied in the
real world, where diversity of users and content is eminent. Furthermore, it
deals with fairness and accountability aspects in data collection by providing
systematic and transparent quality analysis for data collections.
</p></li>
</ul>

<h3>Title: Unified Data Management and Comprehensive Performance Evaluation for Urban Spatial-Temporal Prediction [Experiment, Analysis & Benchmark]. (arXiv:2308.12899v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12899">http://arxiv.org/abs/2308.12899</a></li>
<li>Code URL: https://github.com/libcity/bigscity-libcity</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12899]] Unified Data Management and Comprehensive Performance Evaluation for Urban Spatial-Temporal Prediction [Experiment, Analysis & Benchmark](http://arxiv.org/abs/2308.12899)</code></li>
<li>Summary: <p>The field of urban spatial-temporal prediction is advancing rapidly with the
development of deep learning techniques and the availability of large-scale
datasets. However, challenges persist in accessing and utilizing diverse urban
spatial-temporal datasets from different sources and stored in different
formats, as well as determining effective model structures and components with
the proliferation of deep learning models. This work addresses these challenges
and provides three significant contributions. Firstly, we introduce "atomic
files", a unified storage format designed for urban spatial-temporal big data,
and validate its effectiveness on 40 diverse datasets, simplifying data
management. Secondly, we present a comprehensive overview of technological
advances in urban spatial-temporal prediction models, guiding the development
of robust models. Thirdly, we conduct extensive experiments using diverse
models and datasets, establishing a performance leaderboard and identifying
promising research directions. Overall, this work effectively manages urban
spatial-temporal data, guides future efforts, and facilitates the development
of accurate and efficient urban spatial-temporal prediction models. It can
potentially make long-term contributions to urban spatial-temporal data
management and prediction, ultimately leading to improved urban living
standards.
</p></li>
</ul>

<h2>biometric</h2>
<h3>Title: Open-set Face Recognition with Neural Ensemble, Maximal Entropy Loss and Feature Augmentation. (arXiv:2308.12371v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12371">http://arxiv.org/abs/2308.12371</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12371]] Open-set Face Recognition with Neural Ensemble, Maximal Entropy Loss and Feature Augmentation(http://arxiv.org/abs/2308.12371)</code></li>
<li>Summary: <p>Open-set face recognition refers to a scenario in which biometric systems
have incomplete knowledge of all existing subjects. Therefore, they are
expected to prevent face samples of unregistered subjects from being identified
as previously enrolled identities. This watchlist context adds an arduous
requirement that calls for the dismissal of irrelevant faces by focusing mainly
on subjects of interest. As a response, this work introduces a novel method
that associates an ensemble of compact neural networks with a margin-based cost
function that explores additional samples. Supplementary negative samples can
be obtained from external databases or synthetically built at the
representation level in training time with a new mix-up feature augmentation
approach. Deep neural networks pre-trained on large face datasets serve as the
preliminary feature extraction module. We carry out experiments on well-known
LFW and IJB-C datasets where results show that the approach is able to boost
closed and open-set identification rates.
</p></li>
</ul>

<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Mutual-Guided Dynamic Network for Image Fusion. (arXiv:2308.12538v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12538">http://arxiv.org/abs/2308.12538</a></li>
<li>Code URL: https://github.com/guanys-dar/mgdn</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12538]] Mutual-Guided Dynamic Network for Image Fusion(http://arxiv.org/abs/2308.12538)</code></li>
<li>Summary: <p>Image fusion aims to generate a high-quality image from multiple images
captured under varying conditions. The key problem of this task is to preserve
complementary information while filtering out irrelevant information for the
fused result. However, existing methods address this problem by leveraging
static convolutional neural networks (CNNs), suffering two inherent limitations
during feature extraction, i.e., being unable to handle spatial-variant
contents and lacking guidance from multiple inputs. In this paper, we propose a
novel mutual-guided dynamic network (MGDN) for image fusion, which allows for
effective information utilization across different locations and inputs.
Specifically, we design a mutual-guided dynamic filter (MGDF) for adaptive
feature extraction, composed of a mutual-guided cross-attention (MGCA) module
and a dynamic filter predictor, where the former incorporates additional
guidance from different inputs and the latter generates spatial-variant kernels
for different locations. In addition, we introduce a parallel feature fusion
(PFF) module to effectively fuse local and global information of the extracted
features. To further reduce the redundancy among the extracted features while
simultaneously preserving their shared structural information, we devise a
novel loss function that combines the minimization of normalized mutual
information (NMI) with an estimated gradient mask. Experimental results on five
benchmark datasets demonstrate that our proposed method outperforms existing
methods on four image fusion tasks. The code and model are publicly available
at: https://github.com/Guanys-dar/MGDN.
</p></li>
</ul>

<h3>Title: Learning Heavily-Degraded Prior for Underwater Object Detection. (arXiv:2308.12738v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12738">http://arxiv.org/abs/2308.12738</a></li>
<li>Code URL: https://github.com/xiaodetection/learning-heavily-degraed-prior</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12738]] Learning Heavily-Degraded Prior for Underwater Object Detection(http://arxiv.org/abs/2308.12738)</code></li>
<li>Summary: <p>Underwater object detection suffers from low detection performance because
the distance and wavelength dependent imaging process yield evident image
quality degradations such as haze-like effects, low visibility, and color
distortions. Therefore, we commit to resolving the issue of underwater object
detection with compounded environmental degradations. Typical approaches
attempt to develop sophisticated deep architecture to generate high-quality
images or features. However, these methods are only work for limited ranges
because imaging factors are either unstable, too sensitive, or compounded.
Unlike these approaches catering for high-quality images or features, this
paper seeks transferable prior knowledge from detector-friendly images. The
prior guides detectors removing degradations that interfere with detection. It
is based on statistical observations that, the heavily degraded regions of
detector-friendly (DFUI) and underwater images have evident feature
distribution gaps while the lightly degraded regions of them overlap each
other. Therefore, we propose a residual feature transference module (RFTM) to
learn a mapping between deep representations of the heavily degraded patches of
DFUI- and underwater- images, and make the mapping as a heavily degraded prior
(HDP) for underwater detection. Since the statistical properties are
independent to image content, HDP can be learned without the supervision of
semantic labels and plugged into popular CNNbased feature extraction networks
to improve their performance on underwater object detection. Without bells and
whistles, evaluations on URPC2020 and UODD show that our methods outperform
CNN-based detectors by a large margin. Our method with higher speeds and less
parameters still performs better than transformer-based detectors. Our code and
DFUI dataset can be found in
https://github.com/xiaoDetection/Learning-Heavily-Degraed-Prior.
</p></li>
</ul>

<h3>Title: SkipcrossNets: Adaptive Skip-cross Fusion for Road Detection. (arXiv:2308.12863v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12863">http://arxiv.org/abs/2308.12863</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12863]] SkipcrossNets: Adaptive Skip-cross Fusion for Road Detection(http://arxiv.org/abs/2308.12863)</code></li>
<li>Summary: <p>Multi-modal fusion is increasingly being used for autonomous driving tasks,
as images from different modalities provide unique information for feature
extraction. However, the existing two-stream networks are only fused at a
specific network layer, which requires a lot of manual attempts to set up. As
the CNN goes deeper, the two modal features become more and more advanced and
abstract, and the fusion occurs at the feature level with a large gap, which
can easily hurt the performance. In this study, we propose a novel fusion
architecture called skip-cross networks (SkipcrossNets), which combines
adaptively LiDAR point clouds and camera images without being bound to a
certain fusion epoch. Specifically, skip-cross connects each layer to each
layer in a feed-forward manner, and for each layer, the feature maps of all
previous layers are used as input and its own feature maps are used as input to
all subsequent layers for the other modality, enhancing feature propagation and
multi-modal features fusion. This strategy facilitates selection of the most
similar feature layers from two data pipelines, providing a complementary
effect for sparse point cloud features during fusion processes. The network is
also divided into several blocks to reduce the complexity of feature fusion and
the number of model parameters. The advantages of skip-cross fusion were
demonstrated through application to the KITTI and A2D2 datasets, achieving a
MaxF score of 96.85% on KITTI and an F1 score of 84.84% on A2D2. The model
parameters required only 2.33 MB of memory at a speed of 68.24 FPS, which could
be viable for mobile terminals and embedded devices.
</p></li>
</ul>

<h3>Title: Perspective-aware Convolution for Monocular 3D Object Detection. (arXiv:2308.12938v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12938">http://arxiv.org/abs/2308.12938</a></li>
<li>Code URL: https://github.com/KenYu910645/perspective-aware-convolution</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12938]] Perspective-aware Convolution for Monocular 3D Object Detection(http://arxiv.org/abs/2308.12938)</code></li>
<li>Summary: <p>Monocular 3D object detection is a crucial and challenging task for
autonomous driving vehicle, while it uses only a single camera image to infer
3D objects in the scene. To address the difficulty of predicting depth using
only pictorial clue, we propose a novel perspective-aware convolutional layer
that captures long-range dependencies in images. By enforcing convolutional
kernels to extract features along the depth axis of every image pixel, we
incorporates perspective information into network architecture. We integrate
our perspective-aware convolutional layer into a 3D object detector and
demonstrate improved performance on the KITTI3D dataset, achieving a 23.9\%
average precision in the easy benchmark. These results underscore the
importance of modeling scene clues for accurate depth inference and highlight
the benefits of incorporating scene structure in network design. Our
perspective-aware convolutional layer has the potential to enhance object
detection accuracy by providing more precise and context-aware feature
extraction.
</p></li>
</ul>

<h3>Title: CARE: Co-Attention Network for Joint Entity and Relation Extraction. (arXiv:2308.12531v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12531">http://arxiv.org/abs/2308.12531</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12531]] CARE: Co-Attention Network for Joint Entity and Relation Extraction(http://arxiv.org/abs/2308.12531)</code></li>
<li>Summary: <p>Joint entity and relation extraction is the fundamental task of information
extraction, consisting of two subtasks: named entity recognition and relation
extraction. Most existing joint extraction methods suffer from issues of
feature confusion or inadequate interaction between two subtasks. In this work,
we propose a Co-Attention network for joint entity and Relation Extraction
(CARE). Our approach involves learning separate representations for each
subtask, aiming to avoid feature overlap. At the core of our approach is the
co-attention module that captures two-way interaction between two subtasks,
allowing the model to leverage entity information for relation prediction and
vice versa, thus promoting mutual enhancement. Extensive experiments on three
joint entity-relation extraction benchmark datasets (NYT, WebNLG and SciERC)
show that our proposed model achieves superior performance, surpassing existing
baseline models.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: FedSoL: Bridging Global Alignment and Local Generality in Federated Learning. (arXiv:2308.12532v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12532">http://arxiv.org/abs/2308.12532</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12532]] FedSoL: Bridging Global Alignment and Local Generality in Federated Learning(http://arxiv.org/abs/2308.12532)</code></li>
<li>Summary: <p>Federated Learning (FL) aggregates locally trained models from individual
clients to construct a global model. While FL enables learning a model with
data privacy, it often suffers from significant performance degradation when
client data distributions are heterogeneous. Many previous FL algorithms have
addressed this issue by introducing various proximal restrictions. These
restrictions aim to encourage global alignment by constraining the deviation of
local learning from the global objective. However, they inherently limit local
learning by interfering with the original local objectives. Recently, an
alternative approach has emerged to improve local learning generality. By
obtaining local models within a smooth loss landscape, this approach mitigates
conflicts among different local objectives of the clients. Yet, it does not
ensure stable global alignment, as local learning does not take the global
objective into account. In this study, we propose Federated Stability on
Learning (FedSoL), which combines both the concepts of global alignment and
local generality. In FedSoL, the local learning seeks a parameter region robust
against proximal perturbations. This strategy introduces an implicit proximal
restriction effect in local learning while maintaining the original local
objective for parameter update. Our experiments show that FedSoL consistently
achieves state-of-the-art performance on various setups.
</p></li>
</ul>

<h3>Title: FedDAT: An Approach for Foundation Model Finetuning in Multi-Modal Heterogeneous Federated Learning. (arXiv:2308.12305v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12305">http://arxiv.org/abs/2308.12305</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12305]] FedDAT: An Approach for Foundation Model Finetuning in Multi-Modal Heterogeneous Federated Learning(http://arxiv.org/abs/2308.12305)</code></li>
<li>Summary: <p>Recently, foundation models have exhibited remarkable advancements in
multi-modal learning. These models, equipped with millions (or billions) of
parameters, typically require a substantial amount of data for finetuning.
However, collecting and centralizing training data from diverse sectors becomes
challenging due to distinct privacy regulations. Federated Learning (FL)
emerges as a promising solution, enabling multiple clients to collaboratively
train neural networks without centralizing their local data. To alleviate
client computation burdens and communication overheads, previous works have
adapted Parameter-efficient Finetuning (PEFT) methods for FL. Hereby, only a
small fraction of the model parameters are optimized and communicated during
federated communications. Nevertheless, most previous works have focused on a
single modality and neglected one common phenomenon, i.e., the presence of data
heterogeneity across the clients. Therefore, in this work, we propose a
finetuning framework tailored to heterogeneous multi-modal FL, called Federated
Dual-Aadapter Teacher (FedDAT). Specifically, our approach leverages a
Dual-Adapter Teacher (DAT) to address data heterogeneity by regularizing the
client local updates and applying Mutual Knowledge Distillation (MKD) for an
efficient knowledge transfer. FedDAT is the first approach that enables an
efficient distributed finetuning of foundation models for a variety of
heterogeneous Vision-Language tasks. To demonstrate its effectiveness, we
conduct extensive experiments on four multi-modality FL benchmarks with
different types of data heterogeneity, where FedDAT substantially outperforms
the existing centralized PEFT methods adapted for FL.
</p></li>
</ul>

<h3>Title: PFL-GAN: When Client Heterogeneity Meets Generative Models in Personalized Federated Learning. (arXiv:2308.12454v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12454">http://arxiv.org/abs/2308.12454</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12454]] PFL-GAN: When Client Heterogeneity Meets Generative Models in Personalized Federated Learning(http://arxiv.org/abs/2308.12454)</code></li>
<li>Summary: <p>Recent advances of generative learning models are accompanied by the growing
interest in federated learning (FL) based on generative adversarial network
(GAN) models. In the context of FL, GAN can capture the underlying client data
structure, and regenerate samples resembling the original data distribution
without compromising the private raw data. Although most existing GAN-based FL
works focus on training a global model, Personalized FL (PFL) sometimes can be
more effective in view of client data heterogeneity in terms of distinct data
sample distributions, feature spaces, and labels. To cope with client
heterogeneity in GAN-based FL, we propose a novel GAN sharing and aggregation
strategy for PFL. The proposed PFL-GAN addresses the client heterogeneity in
different scenarios. More specially, we first learn the similarity among
clients and then develop an weighted collaborative data aggregation. The
empirical results through the rigorous experimentation on several well-known
datasets demonstrate the effectiveness of PFL-GAN.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: StreamMapNet: Streaming Mapping Network for Vectorized Online HD Map Construction. (arXiv:2308.12570v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12570">http://arxiv.org/abs/2308.12570</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12570]] StreamMapNet: Streaming Mapping Network for Vectorized Online HD Map Construction(http://arxiv.org/abs/2308.12570)</code></li>
<li>Summary: <p>High-Definition (HD) maps are essential for the safety of autonomous driving
systems. While existing techniques employ camera images and onboard sensors to
generate vectorized high-precision maps, they are constrained by their reliance
on single-frame input. This approach limits their stability and performance in
complex scenarios such as occlusions, largely due to the absence of temporal
information. Moreover, their performance diminishes when applied to broader
perception ranges. In this paper, we present StreamMapNet, a novel online
mapping pipeline adept at long-sequence temporal modeling of videos.
StreamMapNet employs multi-point attention and temporal information which
empowers the construction of large-range local HD maps with high stability and
further addresses the limitations of existing methods. Furthermore, we
critically examine widely used online HD Map construction benchmark and
datasets, Argoverse2 and nuScenes, revealing significant bias in the existing
evaluation protocols. We propose to resplit the benchmarks according to
geographical spans, promoting fair and precise evaluations. Experimental
results validate that StreamMapNet significantly outperforms existing methods
across all settings while maintaining an online inference speed of $14.2$ FPS.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Variational Information Pursuit with Large Language and Multimodal Models for Interpretable Predictions. (arXiv:2308.12562v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12562">http://arxiv.org/abs/2308.12562</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12562]] Variational Information Pursuit with Large Language and Multimodal Models for Interpretable Predictions(http://arxiv.org/abs/2308.12562)</code></li>
<li>Summary: <p>Variational Information Pursuit (V-IP) is a framework for making
interpretable predictions by design by sequentially selecting a short chain of
task-relevant, user-defined and interpretable queries about the data that are
most informative for the task. While this allows for built-in interpretability
in predictive models, applying V-IP to any task requires data samples with
dense concept-labeling by domain experts, limiting the application of V-IP to
small-scale tasks where manual data annotation is feasible. In this work, we
extend the V-IP framework with Foundational Models (FMs) to address this
limitation. More specifically, we use a two-step process, by first leveraging
Large Language Models (LLMs) to generate a sufficiently large candidate set of
task-relevant interpretable concepts, then using Large Multimodal Models to
annotate each data sample by semantic similarity with each concept in the
generated concept set. While other interpretable-by-design frameworks such as
Concept Bottleneck Models (CBMs) require an additional step of removing
repetitive and non-discriminative concepts to have good interpretability and
test performance, we mathematically and empirically justify that, with a
sufficiently informative and task-relevant query (concept) set, the proposed
FM+V-IP method does not require any type of concept filtering. In addition, we
show that FM+V-IP with LLM generated concepts can achieve better test
performance than V-IP with human annotated concepts, demonstrating the
effectiveness of LLMs at generating efficient query sets. Finally, when
compared to other interpretable-by-design frameworks such as CBMs, FM+V-IP can
achieve competitive test performance using fewer number of concepts/queries in
both cases with filtered or unfiltered concept sets.
</p></li>
</ul>

<h3>Title: Uncertainty and Explainable Analysis of Machine Learning Model for Reconstruction of Sonic Slowness Logs. (arXiv:2308.12625v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12625">http://arxiv.org/abs/2308.12625</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12625]] Uncertainty and Explainable Analysis of Machine Learning Model for Reconstruction of Sonic Slowness Logs(http://arxiv.org/abs/2308.12625)</code></li>
<li>Summary: <p>Logs are valuable information for oil and gas fields as they help to
determine the lithology of the formations surrounding the borehole and the
location and reserves of subsurface oil and gas reservoirs. However, important
logs are often missing in horizontal or old wells, which poses a challenge in
field applications. In this paper, we utilize data from the 2020 machine
learning competition of the SPWLA, which aims to predict the missing
compressional wave slowness and shear wave slowness logs using other logs in
the same borehole. We employ the NGBoost algorithm to construct an Ensemble
Learning model that can predicate the results as well as their uncertainty.
Furthermore, we combine the SHAP method to investigate the interpretability of
the machine learning model. We compare the performance of the NGBosst model
with four other commonly used Ensemble Learning methods, including Random
Forest, GBDT, XGBoost, LightGBM. The results show that the NGBoost model
performs well in the testing set and can provide a probability distribution for
the prediction results. In addition, the variance of the probability
distribution of the predicted log can be used to justify the quality of the
constructed log. Using the SHAP explainable machine learning model, we
calculate the importance of each input log to the predicted results as well as
the coupling relationship among input logs. Our findings reveal that the
NGBoost model tends to provide greater slowness prediction results when the
neutron porosity and gamma ray are large, which is consistent with the
cognition of petrophysical models. Furthermore, the machine learning model can
capture the influence of the changing borehole caliper on slowness, where the
influence of borehole caliper on slowness is complex and not easy to establish
a direct relationship. These findings are in line with the physical principle
of borehole acoustics.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: Asymmetric Co-Training with Explainable Cell Graph Ensembling for Histopathological Image Classification. (arXiv:2308.12737v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12737">http://arxiv.org/abs/2308.12737</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12737]] Asymmetric Co-Training with Explainable Cell Graph Ensembling for Histopathological Image Classification(http://arxiv.org/abs/2308.12737)</code></li>
<li>Summary: <p>Convolutional neural networks excel in histopathological image
classification, yet their pixel-level focus hampers explainability. Conversely,
emerging graph convolutional networks spotlight cell-level features and medical
implications. However, limited by their shallowness and suboptimal use of
high-dimensional pixel data, GCNs underperform in multi-class histopathological
image classification. To make full use of pixel-level and cell-level features
dynamically, we propose an asymmetric co-training framework combining a deep
graph convolutional network and a convolutional neural network for multi-class
histopathological image classification. To improve the explainability of the
entire framework by embedding morphological and topological distribution of
cells, we build a 14-layer deep graph convolutional network to handle cell
graph data. For the further utilization and dynamic interactions between
pixel-level and cell-level information, we also design a co-training strategy
to integrate the two asymmetric branches. Notably, we collect a private
clinically acquired dataset termed LUAD7C, including seven subtypes of lung
adenocarcinoma, which is rare and more challenging. We evaluated our approach
on the private LUAD7C and public colorectal cancer datasets, showcasing its
superior performance, explainability, and generalizability in multi-class
histopathological image classification.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Diffusion-based Image Translation with Label Guidance for Domain Adaptive Semantic Segmentation. (arXiv:2308.12350v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12350">http://arxiv.org/abs/2308.12350</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12350]] Diffusion-based Image Translation with Label Guidance for Domain Adaptive Semantic Segmentation(http://arxiv.org/abs/2308.12350)</code></li>
<li>Summary: <p>Translating images from a source domain to a target domain for learning
target models is one of the most common strategies in domain adaptive semantic
segmentation (DASS). However, existing methods still struggle to preserve
semantically-consistent local details between the original and translated
images. In this work, we present an innovative approach that addresses this
challenge by using source-domain labels as explicit guidance during image
translation. Concretely, we formulate cross-domain image translation as a
denoising diffusion process and utilize a novel Semantic Gradient Guidance
(SGG) method to constrain the translation process, conditioning it on the
pixel-wise source labels. Additionally, a Progressive Translation Learning
(PTL) strategy is devised to enable the SGG method to work reliably across
domains with large gaps. Extensive experiments demonstrate the superiority of
our approach over state-of-the-art methods.
</p></li>
</ul>

<h3>Title: Augmenting medical image classifiers with synthetic data from latent diffusion models. (arXiv:2308.12453v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12453">http://arxiv.org/abs/2308.12453</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12453]] Augmenting medical image classifiers with synthetic data from latent diffusion models(http://arxiv.org/abs/2308.12453)</code></li>
<li>Summary: <p>While hundreds of artificial intelligence (AI) algorithms are now approved or
cleared by the US Food and Drugs Administration (FDA), many studies have shown
inconsistent generalization or latent bias, particularly for underrepresented
populations. Some have proposed that generative AI could reduce the need for
real data, but its utility in model development remains unclear. Skin disease
serves as a useful case study in synthetic image generation due to the
diversity of disease appearance, particularly across the protected attribute of
skin tone. Here we show that latent diffusion models can scalably generate
images of skin disease and that augmenting model training with these data
improves performance in data-limited settings. These performance gains saturate
at synthetic-to-real image ratios above 10:1 and are substantially smaller than
the gains obtained from adding real images. As part of our analysis, we
generate and analyze a new dataset of 458,920 synthetic images produced using
several generation strategies. Our results suggest that synthetic data could
serve as a force-multiplier for model development, but the collection of
diverse real-world data remains the most important step to improve medical AI
algorithms.
</p></li>
</ul>

<h3>Title: Diffuse, Attend, and Segment: Unsupervised Zero-Shot Segmentation using Stable Diffusion. (arXiv:2308.12469v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12469">http://arxiv.org/abs/2308.12469</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12469]] Diffuse, Attend, and Segment: Unsupervised Zero-Shot Segmentation using Stable Diffusion(http://arxiv.org/abs/2308.12469)</code></li>
<li>Summary: <p>Producing quality segmentation masks for images is a fundamental problem in
computer vision. Recent research has explored large-scale supervised training
to enable zero-shot segmentation on virtually any image style and unsupervised
training to enable segmentation without dense annotations. However,
constructing a model capable of segmenting anything in a zero-shot manner
without any annotations is still challenging. In this paper, we propose to
utilize the self-attention layers in stable diffusion models to achieve this
goal because the pre-trained stable diffusion model has learned inherent
concepts of objects within its attention layers. Specifically, we introduce a
simple yet effective iterative merging process based on measuring KL divergence
among attention maps to merge them into valid segmentation masks. The proposed
method does not require any training or language dependency to extract quality
segmentation for any images. On COCO-Stuff-27, our method surpasses the prior
unsupervised zero-shot SOTA method by an absolute 26% in pixel accuracy and 17%
in mean IoU.
</p></li>
</ul>

<h3>Title: DD-GCN: Directed Diffusion Graph Convolutional Network for Skeleton-based Human Action Recognition. (arXiv:2308.12501v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12501">http://arxiv.org/abs/2308.12501</a></li>
<li>Code URL: https://github.com/shiyin-lc/dd-gcn</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12501]] DD-GCN: Directed Diffusion Graph Convolutional Network for Skeleton-based Human Action Recognition(http://arxiv.org/abs/2308.12501)</code></li>
<li>Summary: <p>Graph Convolutional Networks (GCNs) have been widely used in skeleton-based
human action recognition. In GCN-based methods, the spatio-temporal graph is
fundamental for capturing motion patterns. However, existing approaches ignore
the physical dependency and synchronized spatio-temporal correlations between
joints, which limits the representation capability of GCNs. To solve these
problems, we construct the directed diffusion graph for action modeling and
introduce the activity partition strategy to optimize the weight sharing
mechanism of graph convolution kernels. In addition, we present the
spatio-temporal synchronization encoder to embed synchronized spatio-temporal
semantics. Finally, we propose Directed Diffusion Graph Convolutional Network
(DD-GCN) for action recognition, and the experiments on three public datasets:
NTU-RGB+D, NTU-RGB+D 120, and NW-UCLA, demonstrate the state-of-the-art
performance of our method.
</p></li>
</ul>

<h3>Title: APLA: Additional Perturbation for Latent Noise with Adversarial Training Enables Consistency. (arXiv:2308.12605v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12605">http://arxiv.org/abs/2308.12605</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12605]] APLA: Additional Perturbation for Latent Noise with Adversarial Training Enables Consistency(http://arxiv.org/abs/2308.12605)</code></li>
<li>Summary: <p>Diffusion models have exhibited promising progress in video generation.
However, they often struggle to retain consistent details within local regions
across frames. One underlying cause is that traditional diffusion models
approximate Gaussian noise distribution by utilizing predictive noise, without
fully accounting for the impact of inherent information within the input
itself. Additionally, these models emphasize the distinction between
predictions and references, neglecting information intrinsic to the videos. To
address this limitation, inspired by the self-attention mechanism, we propose a
novel text-to-video (T2V) generation network structure based on diffusion
models, dubbed Additional Perturbation for Latent noise with Adversarial
training (APLA). Our approach only necessitates a single video as input and
builds upon pre-trained stable diffusion networks. Notably, we introduce an
additional compact network, known as the Video Generation Transformer (VGT).
This auxiliary component is designed to extract perturbations from the inherent
information contained within the input, thereby refining inconsistent pixels
during temporal predictions. We leverage a hybrid architecture of transformers
and convolutions to compensate for temporal intricacies, enhancing consistency
between different frames within the video. Experiments demonstrate a noticeable
improvement in the consistency of the generated videos both qualitatively and
quantitatively.
</p></li>
</ul>

<h3>Title: Dense Text-to-Image Generation with Attention Modulation. (arXiv:2308.12964v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12964">http://arxiv.org/abs/2308.12964</a></li>
<li>Code URL: https://github.com/naver-ai/densediffusion</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12964]] Dense Text-to-Image Generation with Attention Modulation(http://arxiv.org/abs/2308.12964)</code></li>
<li>Summary: <p>Existing text-to-image diffusion models struggle to synthesize realistic
images given dense captions, where each text prompt provides a detailed
description for a specific image region. To address this, we propose
DenseDiffusion, a training-free method that adapts a pre-trained text-to-image
model to handle such dense captions while offering control over the scene
layout. We first analyze the relationship between generated images' layouts and
the pre-trained model's intermediate attention maps. Next, we develop an
attention modulation method that guides objects to appear in specific regions
according to layout guidance. Without requiring additional fine-tuning or
datasets, we improve image generation performance given dense captions
regarding both automatic and human evaluation scores. In addition, we achieve
similar-quality visual results with models specifically trained with layout
conditions.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: AdVerb: Visually Guided Audio Dereverberation. (arXiv:2308.12370v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12370">http://arxiv.org/abs/2308.12370</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12370]] AdVerb: Visually Guided Audio Dereverberation(http://arxiv.org/abs/2308.12370)</code></li>
<li>Summary: <p>We present AdVerb, a novel audio-visual dereverberation framework that uses
visual cues in addition to the reverberant sound to estimate clean audio.
Although audio-only dereverberation is a well-studied problem, our approach
incorporates the complementary visual modality to perform audio
dereverberation. Given an image of the environment where the reverberated sound
signal has been recorded, AdVerb employs a novel geometry-aware cross-modal
transformer architecture that captures scene geometry and audio-visual
cross-modal relationship to generate a complex ideal ratio mask, which, when
applied to the reverberant audio predicts the clean sound. The effectiveness of
our method is demonstrated through extensive quantitative and qualitative
evaluations. Our approach significantly outperforms traditional audio-only and
audio-visual baselines on three downstream tasks: speech enhancement, speech
recognition, and speaker verification, with relative improvements in the range
of 18% - 82% on the LibriSpeech test-clean set. We also achieve highly
satisfactory RT60 error scores on the AVSpeech dataset.
</p></li>
</ul>

<h3>Title: Vision Transformer Adapters for Generalizable Multitask Learning. (arXiv:2308.12372v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12372">http://arxiv.org/abs/2308.12372</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12372]] Vision Transformer Adapters for Generalizable Multitask Learning(http://arxiv.org/abs/2308.12372)</code></li>
<li>Summary: <p>We introduce the first multitasking vision transformer adapters that learn
generalizable task affinities which can be applied to novel tasks and domains.
Integrated into an off-the-shelf vision transformer backbone, our adapters can
simultaneously solve multiple dense vision tasks in a parameter-efficient
manner, unlike existing multitasking transformers that are parametrically
expensive. In contrast to concurrent methods, we do not require retraining or
fine-tuning whenever a new task or domain is added. We introduce a task-adapted
attention mechanism within our adapter framework that combines gradient-based
task similarities with attention-based ones. The learned task affinities
generalize to the following settings: zero-shot task transfer, unsupervised
domain adaptation, and generalization without fine-tuning to novel domains. We
demonstrate that our approach outperforms not only the existing convolutional
neural network-based multitasking methods but also the vision transformer-based
ones. Our project page is at \url{https://ivrl.github.io/VTAGML}.
</p></li>
</ul>

<h3>Title: With a Little Help from your own Past: Prototypical Memory Networks for Image Captioning. (arXiv:2308.12383v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12383">http://arxiv.org/abs/2308.12383</a></li>
<li>Code URL: https://github.com/aimagelab/pma-net</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12383]] With a Little Help from your own Past: Prototypical Memory Networks for Image Captioning(http://arxiv.org/abs/2308.12383)</code></li>
<li>Summary: <p>Image captioning, like many tasks involving vision and language, currently
relies on Transformer-based architectures for extracting the semantics in an
image and translating it into linguistically coherent descriptions. Although
successful, the attention operator only considers a weighted summation of
projections of the current input sample, therefore ignoring the relevant
semantic information which can come from the joint observation of other
samples. In this paper, we devise a network which can perform attention over
activations obtained while processing other training samples, through a
prototypical memory model. Our memory models the distribution of past keys and
values through the definition of prototype vectors which are both
discriminative and compact. Experimentally, we assess the performance of the
proposed model on the COCO dataset, in comparison with carefully designed
baselines and state-of-the-art approaches, and by investigating the role of
each of the proposed components. We demonstrate that our proposal can increase
the performance of an encoder-decoder Transformer by 3.7 CIDEr points both when
training in cross-entropy only and when fine-tuning with self-critical sequence
training. Source code and trained models are available at:
https://github.com/aimagelab/PMA-Net.
</p></li>
</ul>

<h3>Title: MOFO: MOtion FOcused Self-Supervision for Video Understanding. (arXiv:2308.12447v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12447">http://arxiv.org/abs/2308.12447</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12447]] MOFO: MOtion FOcused Self-Supervision for Video Understanding(http://arxiv.org/abs/2308.12447)</code></li>
<li>Summary: <p>Self-supervised learning (SSL) techniques have recently produced outstanding
results in learning visual representations from unlabeled videos. Despite the
importance of motion in supervised learning techniques for action recognition,
SSL methods often do not explicitly consider motion information in videos. To
address this issue, we propose MOFO (MOtion FOcused), a novel SSL method for
focusing representation learning on the motion area of a video, for action
recognition. MOFO automatically detects motion areas in videos and uses these
to guide the self-supervision task. We use a masked autoencoder which randomly
masks out a high proportion of the input sequence; we force a specified
percentage of the inside of the motion area to be masked and the remainder from
outside. We further incorporate motion information into the finetuning step to
emphasise motion in the downstream task. We demonstrate that our motion-focused
innovations can significantly boost the performance of the currently leading
SSL method (VideoMAE) for action recognition. Our method improves the recent
self-supervised Vision Transformer (ViT), VideoMAE, by achieving +2.6%, +2.1%,
+1.3% accuracy on Epic-Kitchens verb, noun and action classification,
respectively, and +4.7% accuracy on Something-Something V2 action
classification. Our proposed approach significantly improves the performance of
the current SSL method for action recognition, indicating the importance of
explicitly encoding motion in SSL.
</p></li>
</ul>

<h3>Title: American Stories: A Large-Scale Structured Text Dataset of Historical U.S. Newspapers. (arXiv:2308.12477v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12477">http://arxiv.org/abs/2308.12477</a></li>
<li>Code URL: https://github.com/dell-research-harvard/americanstories</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12477]] American Stories: A Large-Scale Structured Text Dataset of Historical U(http://arxiv.org/abs/2308.12477)</code></li>
<li>Summary: <p>Existing full text datasets of U.S. public domain newspapers do not recognize
the often complex layouts of newspaper scans, and as a result the digitized
content scrambles texts from articles, headlines, captions, advertisements, and
other layout regions. OCR quality can also be low. This study develops a novel,
deep learning pipeline for extracting full article texts from newspaper images
and applies it to the nearly 20 million scans in Library of Congress's public
domain Chronicling America collection. The pipeline includes layout detection,
legibility classification, custom OCR, and association of article texts
spanning multiple bounding boxes. To achieve high scalability, it is built with
efficient architectures designed for mobile phones. The resulting American
Stories dataset provides high quality data that could be used for pre-training
a large language model to achieve better understanding of historical English
and historical world knowledge. The dataset could also be added to the external
database of a retrieval-augmented language model to make historical information
- ranging from interpretations of political events to minutiae about the lives
of people's ancestors - more widely accessible. Furthermore, structured article
texts facilitate using transformer-based methods for popular social science
applications like topic classification, detection of reproduced content, and
news story clustering. Finally, American Stories provides a massive silver
quality dataset for innovating multimodal layout analysis models and other
multimodal applications.
</p></li>
</ul>

<h3>Title: SieveNet: Selecting Point-Based Features for Mesh Networks. (arXiv:2308.12530v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12530">http://arxiv.org/abs/2308.12530</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12530]] SieveNet: Selecting Point-Based Features for Mesh Networks(http://arxiv.org/abs/2308.12530)</code></li>
<li>Summary: <p>Meshes are widely used in 3D computer vision and graphics, but their
irregular topology poses challenges in applying them to existing neural network
architectures. Recent advances in mesh neural networks turn to remeshing and
push the boundary of pioneer methods that solely take the raw meshes as input.
Although the remeshing offers a regular topology that significantly facilitates
the design of mesh network architectures, features extracted from such remeshed
proxies may struggle to retain the underlying geometry faithfully, limiting the
subsequent neural network's capacity. To address this issue, we propose
SieveNet, a novel paradigm that takes into account both the regular topology
and the exact geometry. Specifically, this method utilizes structured mesh
topology from remeshing and accurate geometric information from
distortion-aware point sampling on the surface of the original mesh.
Furthermore, our method eliminates the need for hand-crafted feature
engineering and can leverage off-the-shelf network architectures such as the
vision transformer. Comprehensive experimental results on classification and
segmentation tasks well demonstrate the effectiveness and superiority of our
method.
</p></li>
</ul>

<h3>Title: Synchronize Feature Extracting and Matching: A Single Branch Framework for 3D Object Tracking. (arXiv:2308.12549v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12549">http://arxiv.org/abs/2308.12549</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12549]] Synchronize Feature Extracting and Matching: A Single Branch Framework for 3D Object Tracking(http://arxiv.org/abs/2308.12549)</code></li>
<li>Summary: <p>Siamese network has been a de facto benchmark framework for 3D LiDAR object
tracking with a shared-parametric encoder extracting features from template and
search region, respectively. This paradigm relies heavily on an additional
matching network to model the cross-correlation/similarity of the template and
search region. In this paper, we forsake the conventional Siamese paradigm and
propose a novel single-branch framework, SyncTrack, synchronizing the feature
extracting and matching to avoid forwarding encoder twice for template and
search region as well as introducing extra parameters of matching network. The
synchronization mechanism is based on the dynamic affinity of the Transformer,
and an in-depth analysis of the relevance is provided theoretically. Moreover,
based on the synchronization, we introduce a novel Attentive Points-Sampling
strategy into the Transformer layers (APST), replacing the random/Farthest
Points Sampling (FPS) method with sampling under the supervision of attentive
relations between the template and search region. It implies connecting
point-wise sampling with the feature learning, beneficial to aggregating more
distinctive and geometric features for tracking with sparse points. Extensive
experiments on two benchmark datasets (KITTI and NuScenes) show that SyncTrack
achieves state-of-the-art performance in real-time tracking.
</p></li>
</ul>

<h3>Title: Towards Hierarchical Regional Transformer-based Multiple Instance Learning. (arXiv:2308.12634v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12634">http://arxiv.org/abs/2308.12634</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12634]] Towards Hierarchical Regional Transformer-based Multiple Instance Learning(http://arxiv.org/abs/2308.12634)</code></li>
<li>Summary: <p>The classification of gigapixel histopathology images with deep multiple
instance learning models has become a critical task in digital pathology and
precision medicine. In this work, we propose a Transformer-based multiple
instance learning approach that replaces the traditional learned attention
mechanism with a regional, Vision Transformer inspired self-attention
mechanism. We present a method that fuses regional patch information to derive
slide-level predictions and show how this regional aggregation can be stacked
to hierarchically process features on different distance levels. To increase
predictive accuracy, especially for datasets with small, local morphological
features, we introduce a method to focus the image processing on high attention
regions during inference. Our approach is able to significantly improve
performance over the baseline on two histopathology datasets and points towards
promising directions for further research.
</p></li>
</ul>

<h3>Title: A Parse-Then-Place Approach for Generating Graphic Layouts from Textual Descriptions. (arXiv:2308.12700v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12700">http://arxiv.org/abs/2308.12700</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12700]] A Parse-Then-Place Approach for Generating Graphic Layouts from Textual Descriptions(http://arxiv.org/abs/2308.12700)</code></li>
<li>Summary: <p>Creating layouts is a fundamental step in graphic design. In this work, we
propose to use text as the guidance to create graphic layouts, i.e.,
Text-to-Layout, aiming to lower the design barriers. Text-to-Layout is a
challenging task, because it needs to consider the implicit, combined, and
incomplete layout constraints from text, each of which has not been studied in
previous work. To address this, we present a two-stage approach, named
parse-then-place. The approach introduces an intermediate representation (IR)
between text and layout to represent diverse layout constraints. With IR,
Text-to-Layout is decomposed into a parse stage and a place stage. The parse
stage takes a textual description as input and generates an IR, in which the
implicit constraints from the text are transformed into explicit ones. The
place stage generates layouts based on the IR. To model combined and incomplete
constraints, we use a Transformer-based layout generation model and carefully
design a way to represent constraints and layouts as sequences. Besides, we
adopt the pretrain-then-finetune strategy to boost the performance of the
layout generation model with large-scale unlabeled layouts. To evaluate our
approach, we construct two Text-to-Layout datasets and conduct experiments on
them. Quantitative results, qualitative analysis, and user studies demonstrate
the effectiveness of our approach.
</p></li>
</ul>

<h3>Title: DeepLOC: Deep Learning-based Bone Pathology Localization and Classification in Wrist X-ray Images. (arXiv:2308.12727v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12727">http://arxiv.org/abs/2308.12727</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12727]] DeepLOC: Deep Learning-based Bone Pathology Localization and Classification in Wrist X-ray Images(http://arxiv.org/abs/2308.12727)</code></li>
<li>Summary: <p>In recent years, computer-aided diagnosis systems have shown great potential
in assisting radiologists with accurate and efficient medical image analysis.
This paper presents a novel approach for bone pathology localization and
classification in wrist X-ray images using a combination of YOLO (You Only Look
Once) and the Shifted Window Transformer (Swin) with a newly proposed block.
The proposed methodology addresses two critical challenges in wrist X-ray
analysis: accurate localization of bone pathologies and precise classification
of abnormalities. The YOLO framework is employed to detect and localize bone
pathologies, leveraging its real-time object detection capabilities.
Additionally, the Swin, a transformer-based module, is utilized to extract
contextual information from the localized regions of interest (ROIs) for
accurate classification.
</p></li>
</ul>

<h3>Title: MixNet: Toward Accurate Detection of Challenging Scene Text in the Wild. (arXiv:2308.12817v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12817">http://arxiv.org/abs/2308.12817</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12817]] MixNet: Toward Accurate Detection of Challenging Scene Text in the Wild(http://arxiv.org/abs/2308.12817)</code></li>
<li>Summary: <p>Detecting small scene text instances in the wild is particularly challenging,
where the influence of irregular positions and nonideal lighting often leads to
detection errors. We present MixNet, a hybrid architecture that combines the
strengths of CNNs and Transformers, capable of accurately detecting small text
from challenging natural scenes, regardless of the orientations, styles, and
lighting conditions. MixNet incorporates two key modules: (1) the Feature
Shuffle Network (FSNet) to serve as the backbone and (2) the Central
Transformer Block (CTBlock) to exploit the 1D manifold constraint of the scene
text. We first introduce a novel feature shuffling strategy in FSNet to
facilitate the exchange of features across multiple scales, generating
high-resolution features superior to popular ResNet and HRNet. The FSNet
backbone has achieved significant improvements over many existing text
detection methods, including PAN, DB, and FAST. Then we design a complementary
CTBlock to leverage center line based features similar to the medial axis of
text regions and show that it can outperform contour-based approaches in
challenging cases when small scene texts appear closely. Extensive experimental
results show that MixNet, which mixes FSNet with CTBlock, achieves
state-of-the-art results on multiple scene text detection datasets.
</p></li>
</ul>

<h3>Title: EFormer: Enhanced Transformer towards Semantic-Contour Features of Foreground for Portraits Matting. (arXiv:2308.12831v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12831">http://arxiv.org/abs/2308.12831</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12831]] EFormer: Enhanced Transformer towards Semantic-Contour Features of Foreground for Portraits Matting(http://arxiv.org/abs/2308.12831)</code></li>
<li>Summary: <p>The portrait matting task aims to extract an alpha matte with complete
semantics and finely-detailed contours. In comparison to CNN-based approaches,
transformers with self-attention allow a larger receptive field, enabling it to
better capture long-range dependencies and low-frequency semantic information
of a portrait. However, the recent research shows that self-attention mechanism
struggle with modeling high-frequency information and capturing fine contour
details, which can lead to bias while predicting the portrait's contours. To
address the problem, we propose EFormer to enhance the model's attention
towards semantic and contour features. Especially the latter, which is
surrounded by a large amount of high-frequency details. We build a semantic and
contour detector (SCD) to accurately capture the distribution of semantic and
contour features. And we further design contour-edge extraction branch and
semantic extraction branch for refining contour features and complete semantic
information. Finally, we fuse the two kinds of features and leverage the
segmentation head to generate the predicted portrait matte. Remarkably, EFormer
is an end-to-end trimap-free method and boasts a simple structure. Experiments
conducted on VideoMatte240K-JPEGSD and AIM datasets demonstrate that EFormer
outperforms previous portrait matte methods.
</p></li>
</ul>

<h3>Title: ToonTalker: Cross-Domain Face Reenactment. (arXiv:2308.12866v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12866">http://arxiv.org/abs/2308.12866</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12866]] ToonTalker: Cross-Domain Face Reenactment(http://arxiv.org/abs/2308.12866)</code></li>
<li>Summary: <p>We target cross-domain face reenactment in this paper, i.e., driving a
cartoon image with the video of a real person and vice versa. Recently, many
works have focused on one-shot talking face generation to drive a portrait with
a real video, i.e., within-domain reenactment. Straightforwardly applying those
methods to cross-domain animation will cause inaccurate expression transfer,
blur effects, and even apparent artifacts due to the domain shift between
cartoon and real faces. Only a few works attempt to settle cross-domain face
reenactment. The most related work AnimeCeleb requires constructing a dataset
with pose vector and cartoon image pairs by animating 3D characters, which
makes it inapplicable anymore if no paired data is available. In this paper, we
propose a novel method for cross-domain reenactment without paired data.
Specifically, we propose a transformer-based framework to align the motions
from different domains into a common latent space where motion transfer is
conducted via latent code addition. Two domain-specific motion encoders and two
learnable motion base memories are used to capture domain properties. A source
query transformer and a driving one are exploited to project domain-specific
motion to the canonical space. The edited motion is projected back to the
domain of the source with a transformer. Moreover, since no paired data is
provided, we propose a novel cross-domain training scheme using data from two
domains with the designed analogy constraint. Besides, we contribute a cartoon
dataset in Disney style. Extensive evaluations demonstrate the superiority of
our method over competing methods.
</p></li>
</ul>

<h3>Title: DS4DH at #SMM4H 2023: Zero-Shot Adverse Drug Events Normalization using Sentence Transformers and Reciprocal-Rank Fusion. (arXiv:2308.12877v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12877">http://arxiv.org/abs/2308.12877</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12877]] DS4DH at #SMM4H 2023: Zero-Shot Adverse Drug Events Normalization using Sentence Transformers and Reciprocal-Rank Fusion(http://arxiv.org/abs/2308.12877)</code></li>
<li>Summary: <p>This paper outlines the performance evaluation of a system for adverse drug
event normalization, developed by the Data Science for Digital Health group for
the Social Media Mining for Health Applications 2023 shared task 5. Shared task
5 targeted the normalization of adverse drug event mentions in Twitter to
standard concepts from the Medical Dictionary for Regulatory Activities
terminology. Our system hinges on a two-stage approach: BERT fine-tuning for
entity recognition, followed by zero-shot normalization using sentence
transformers and reciprocal-rank fusion. The approach yielded a precision of
44.9%, recall of 40.5%, and an F1-score of 42.6%. It outperformed the median
performance in shared task 5 by 10% and demonstrated the highest performance
among all participants. These results substantiate the effectiveness of our
approach and its potential application for adverse drug event normalization in
the realm of social media text mining.
</p></li>
</ul>

<h3>Title: Easy attention: A simple self-attention mechanism for Transformers. (arXiv:2308.12874v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12874">http://arxiv.org/abs/2308.12874</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12874]] Easy attention: A simple self-attention mechanism for Transformers(http://arxiv.org/abs/2308.12874)</code></li>
<li>Summary: <p>To improve the robustness of transformer neural networks used for
temporal-dynamics prediction of chaotic systems, we propose a novel attention
mechanism called easy attention. Due to the fact that self attention only makes
usage of the inner product of queries and keys, it is demonstrated that the
keys, queries and softmax are not necessary for obtaining the attention score
required to capture long-term dependencies in temporal sequences. Through
implementing singular-value decomposition (SVD) on the softmax attention score,
we further observe that the self attention compresses contribution from both
queries and keys in the spanned space of the attention score. Therefore, our
proposed easy-attention method directly treats the attention scores as
learnable parameters. This approach produces excellent results when
reconstructing and predicting the temporal dynamics of chaotic systems
exhibiting more robustness and less complexity than the self attention or the
widely-used long short-term memory (LSTM) network. Our results show great
potential for applications in more complex high-dimensional dynamical systems.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Continual Zero-Shot Learning through Semantically Guided Generative Random Walks. (arXiv:2308.12366v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12366">http://arxiv.org/abs/2308.12366</a></li>
<li>Code URL: https://github.com/wx-zhang/igczsl</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12366]] Continual Zero-Shot Learning through Semantically Guided Generative Random Walks(http://arxiv.org/abs/2308.12366)</code></li>
<li>Summary: <p>Learning novel concepts, remembering previous knowledge, and adapting it to
future tasks occur simultaneously throughout a human's lifetime. To model such
comprehensive abilities, continual zero-shot learning (CZSL) has recently been
introduced. However, most existing methods overused unseen semantic information
that may not be continually accessible in realistic settings. In this paper, we
address the challenge of continual zero-shot learning where unseen information
is not provided during training, by leveraging generative modeling. The heart
of the generative-based methods is to learn quality representations from seen
classes to improve the generative understanding of the unseen visual space.
Motivated by this, we introduce generalization-bound tools and provide the
first theoretical explanation for the benefits of generative modeling to CZSL
tasks. Guided by the theoretical analysis, we then propose our learning
algorithm that employs a novel semantically guided Generative Random Walk (GRW)
loss. The GRW loss augments the training by continually encouraging the model
to generate realistic and characterized samples to represent the unseen space.
Our algorithm achieves state-of-the-art performance on AWA1, AWA2, CUB, and SUN
datasets, surpassing existing CZSL methods by 3-7\%. The code has been made
available here \url{https://github.com/wx-zhang/IGCZSL}
</p></li>
</ul>

<h3>Title: FG-Net: Facial Action Unit Detection with Generalizable Pyramidal Features. (arXiv:2308.12380v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12380">http://arxiv.org/abs/2308.12380</a></li>
<li>Code URL: https://github.com/ihp-lab/fg-net</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12380]] FG-Net: Facial Action Unit Detection with Generalizable Pyramidal Features(http://arxiv.org/abs/2308.12380)</code></li>
<li>Summary: <p>Automatic detection of facial Action Units (AUs) allows for objective facial
expression analysis. Due to the high cost of AU labeling and the limited size
of existing benchmarks, previous AU detection methods tend to overfit the
dataset, resulting in a significant performance loss when evaluated across
corpora. To address this problem, we propose FG-Net for generalizable facial
action unit detection. Specifically, FG-Net extracts feature maps from a
StyleGAN2 model pre-trained on a large and diverse face image dataset. Then,
these features are used to detect AUs with a Pyramid CNN Interpreter, making
the training efficient and capturing essential local features. The proposed
FG-Net achieves a strong generalization ability for heatmap-based AU detection
thanks to the generalizable and semantic-rich features extracted from the
pre-trained generative model. Extensive experiments are conducted to evaluate
within- and cross-corpus AU detection with the widely-used DISFA and BP4D
datasets. Compared with the state-of-the-art, the proposed method achieves
superior cross-domain performance while maintaining competitive within-domain
performance. In addition, FG-Net is data-efficient and achieves competitive
performance even when trained on 1000 samples. Our code will be released at
\url{https://github.com/ihp-lab/FG-Net}
</p></li>
</ul>

<h3>Title: MapPrior: Bird's-Eye View Map Layout Estimation with Generative Models. (arXiv:2308.12963v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12963">http://arxiv.org/abs/2308.12963</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12963]] MapPrior: Bird's-Eye View Map Layout Estimation with Generative Models(http://arxiv.org/abs/2308.12963)</code></li>
<li>Summary: <p>Despite tremendous advancements in bird's-eye view (BEV) perception, existing
models fall short in generating realistic and coherent semantic map layouts,
and they fail to account for uncertainties arising from partial sensor
information (such as occlusion or limited coverage). In this work, we introduce
MapPrior, a novel BEV perception framework that combines a traditional
discriminative BEV perception model with a learned generative model for
semantic map layouts. Our MapPrior delivers predictions with better accuracy,
realism, and uncertainty awareness. We evaluate our model on the large-scale
nuScenes benchmark. At the time of submission, MapPrior outperforms the
strongest competing method, with significantly improved MMD and ECE scores in
camera- and LiDAR-based BEV perception.
</p></li>
</ul>

<h3>Title: Low-count Time Series Anomaly Detection. (arXiv:2308.12925v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12925">http://arxiv.org/abs/2308.12925</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12925]] Low-count Time Series Anomaly Detection(http://arxiv.org/abs/2308.12925)</code></li>
<li>Summary: <p>Low-count time series describe sparse or intermittent events, which are
prevalent in large-scale online platforms that capture and monitor diverse data
types. Several distinct challenges surface when modelling low-count time
series, particularly low signal-to-noise ratios (when anomaly signatures are
provably undetectable), and non-uniform performance (when average metrics are
not representative of local behaviour). The time series anomaly detection
community currently lacks explicit tooling and processes to model and reliably
detect anomalies in these settings. We address this gap by introducing a novel
generative procedure for creating benchmark datasets comprising of low-count
time series with anomalous segments. Via a mixture of theoretical and empirical
analysis, our work explains how widely-used algorithms struggle with the
distribution overlap between normal and anomalous segments. In order to
mitigate this shortcoming, we then leverage our findings to demonstrate how
anomaly score smoothing consistently improves performance. The practical
utility of our analysis and recommendation is validated on a real-world dataset
containing sales data for retail stores.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: VIGC: Visual Instruction Generation and Correction. (arXiv:2308.12714v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12714">http://arxiv.org/abs/2308.12714</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12714]] VIGC: Visual Instruction Generation and Correction(http://arxiv.org/abs/2308.12714)</code></li>
<li>Summary: <p>The integration of visual encoders and large language models (LLMs) has
driven recent progress in multimodal large language models (MLLMs). However,
the scarcity of high-quality instruction-tuning data for vision-language tasks
remains a challenge. The current leading paradigm, such as LLaVA, relies on
language-only GPT-4 to generate data, which requires pre-annotated image
captions and detection bounding boxes, suffering from understanding image
details. A practical solution to this problem would be to utilize the available
multimodal large language models (MLLMs) to generate instruction data for
vision-language tasks. However, it's worth noting that the currently accessible
MLLMs are not as powerful as their LLM counterparts, as they tend to produce
inadequate responses and generate false information. As a solution for
addressing the current issue, this paper proposes the Visual Instruction
Generation and Correction (VIGC) framework that enables multimodal large
language models to generate instruction-tuning data and progressively enhance
its quality on-the-fly. Specifically, Visual Instruction Generation (VIG)
guides the vision-language model to generate diverse instruction-tuning data.
To ensure generation quality, Visual Instruction Correction (VIC) adopts an
iterative update mechanism to correct any inaccuracies in data produced by VIG,
effectively reducing the risk of hallucination. Leveraging the diverse,
high-quality data generated by VIGC, we finetune mainstream models and validate
data quality based on various evaluations. Experimental results demonstrate
that VIGC not only compensates for the shortcomings of language-only data
generation methods, but also effectively enhances the benchmark performance.
The models, datasets, and code will be made publicly available.
</p></li>
</ul>

<h3>Title: Towards Realistic Zero-Shot Classification via Self Structural Semantic Alignment. (arXiv:2308.12960v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12960">http://arxiv.org/abs/2308.12960</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12960]] Towards Realistic Zero-Shot Classification via Self Structural Semantic Alignment(http://arxiv.org/abs/2308.12960)</code></li>
<li>Summary: <p>Large-scale pre-trained Vision Language Models (VLMs) have proven effective
for zero-shot classification. Despite the success, most traditional VLMs-based
methods are restricted by the assumption of partial source supervision or ideal
vocabularies, which rarely satisfy the open-world scenario. In this paper, we
aim at a more challenging setting, Realistic Zero-Shot Classification, which
assumes no annotation but instead a broad vocabulary. To address this
challenge, we propose the Self Structural Semantic Alignment (S^3A) framework,
which extracts the structural semantic information from unlabeled data while
simultaneously self-learning. Our S^3A framework adopts a unique
Cluster-Vote-Prompt-Realign (CVPR) algorithm, which iteratively groups
unlabeled data to derive structural semantics for pseudo-supervision. Our CVPR
process includes iterative clustering on images, voting within each cluster to
identify initial class candidates from the vocabulary, generating
discriminative prompts with large language models to discern confusing
candidates, and realigning images and the vocabulary as structural semantic
alignment. Finally, we propose to self-learn the CLIP image encoder with both
individual and structural semantic alignment through a teacher-student learning
strategy. Our comprehensive experiments across various generic and fine-grained
benchmarks demonstrate that the S^3A method offers substantial improvements
over existing VLMs-based approaches, achieving a more than 15% accuracy
improvement over CLIP on average. Our codes, models, and prompts are publicly
released at https://github.com/sheng-eatamath/S3A.
</p></li>
</ul>

<h3>Title: Large Language Model as Autonomous Decision Maker. (arXiv:2308.12519v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12519">http://arxiv.org/abs/2308.12519</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12519]] Large Language Model as Autonomous Decision Maker(http://arxiv.org/abs/2308.12519)</code></li>
<li>Summary: <p>While large language models (LLMs) exhibit impressive language understanding
and in-context learning abilities, their decision-making ability still heavily
relies on the guidance of task-specific expert knowledge when solving
real-world tasks. To unleash the potential of LLMs as autonomous decision
makers, this paper presents an approach JuDec to endow LLMs with the
self-judgment ability, enabling LLMs to achieve autonomous judgment and
exploration for decision making. Specifically, in JuDec, Elo-based
Self-Judgment Mechanism is designed to assign Elo scores to decision steps to
judge their values and utilities via pairwise comparisons between two solutions
and then guide the decision-searching process toward the optimal solution
accordingly. Experimental results on the ToolBench dataset demonstrate JuDec's
superiority over baselines, achieving over 10% improvement in Pass Rate on
diverse tasks. It offers higher-quality solutions and reduces costs (ChatGPT
API calls), highlighting its effectiveness and efficiency.
</p></li>
</ul>

<h3>Title: CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias. (arXiv:2308.12539v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12539">http://arxiv.org/abs/2308.12539</a></li>
<li>Code URL: https://github.com/vipulgupta1011/calm</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12539]] CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias(http://arxiv.org/abs/2308.12539)</code></li>
<li>Summary: <p>As language models (LMs) become increasingly powerful, it is important to
quantify and compare them for sociodemographic bias with potential for harm.
Prior bias measurement datasets are sensitive to perturbations in their
manually designed templates, therefore unreliable. To achieve reliability, we
introduce the Comprehensive Assessment of Language Model bias (CALM), a
benchmark dataset to quantify bias in LMs across three tasks. We integrate 16
existing datasets across different domains, such as Wikipedia and news
articles, to filter 224 templates from which we construct a dataset of 78,400
examples. We compare the diversity of CALM with prior datasets on metrics such
as average semantic similarity, and variation in template length, and test the
sensitivity to small perturbations. We show that our dataset is more diverse
and reliable than previous datasets, thus better capture the breadth of
linguistic variation required to reliably evaluate model bias. We evaluate 20
large language models including six prominent families of LMs such as Llama-2.
In two LM series, OPT and Bloom, we found that larger parameter models are more
biased than lower parameter models. We found the T0 series of models to be the
least biased. Furthermore, we noticed a tradeoff between gender and racial bias
with increasing model size in some model series. The code is available at
https://github.com/vipulgupta1011/CALM.
</p></li>
</ul>

<h3>Title: Mind vs. Mouth: On Measuring Re-judge Inconsistency of Social Bias in Large Language Models. (arXiv:2308.12578v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12578">http://arxiv.org/abs/2308.12578</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12578]] Mind vs(http://arxiv.org/abs/2308.12578)</code></li>
<li>Summary: <p>Recent researches indicate that Pre-trained Large Language Models (LLMs)
possess cognitive constructs similar to those observed in humans, prompting
researchers to investigate the cognitive aspects of LLMs. This paper focuses on
explicit and implicit social bias, a distinctive two-level cognitive construct
in psychology. It posits that individuals' explicit social bias, which is their
conscious expression of bias in the statements, may differ from their implicit
social bias, which represents their unconscious bias. We propose a two-stage
approach and discover a parallel phenomenon in LLMs known as "re-judge
inconsistency" in social bias. In the initial stage, the LLM is tasked with
automatically completing statements, potentially incorporating implicit social
bias. However, in the subsequent stage, the same LLM re-judges the biased
statement generated by itself but contradicts it. We propose that this re-judge
inconsistency can be similar to the inconsistency between human's unaware
implicit social bias and their aware explicit social bias. Experimental
investigations on ChatGPT and GPT-4 concerning common gender biases examined in
psychology corroborate the highly stable nature of the re-judge inconsistency.
This finding may suggest that diverse cognitive constructs emerge as LLMs'
capabilities strengthen. Consequently, leveraging psychological theories can
provide enhanced insights into the underlying mechanisms governing the
expressions of explicit and implicit constructs in LLMs.
</p></li>
</ul>

<h3>Title: Improving Translation Faithfulness of Large Language Models via Augmenting Instructions. (arXiv:2308.12674v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12674">http://arxiv.org/abs/2308.12674</a></li>
<li>Code URL: https://github.com/pppa2019/swie_overmiss_llm4mt</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12674]] Improving Translation Faithfulness of Large Language Models via Augmenting Instructions(http://arxiv.org/abs/2308.12674)</code></li>
<li>Summary: <p>Large Language Models (LLMs) present strong general capabilities, and a
current compelling challenge is stimulating their specialized capabilities,
such as machine translation, through low-cost instruction tuning. The standard
instruction-following data is sequentially organized as the concatenation of an
instruction, an input, and a response. As the attention mechanism of LLMs has
limitations on local focus, LLMs tend to focus more on the words or sentences
nearby at each position. This leads to a high risk of instruction forgetting
during decoding. To alleviate the above issues, We propose SWIE
(Segment-Weighted Instruction Embedding) and an instruction-following dataset
OVERMISS. SWIE improves the model instruction understanding by adding a global
instruction representation on the following input and response representations.
OVERMISS improves model faithfulness by comparing over-translation and
miss-translation results with the correct translation. We apply our methods to
two main-stream open-source LLMs, BLOOM and LLaMA. The experimental results
demonstrate significant improvements in translation performance with SWIE based
on BLOOMZ-3b, particularly in zero-shot and long text translations due to
reduced instruction forgetting risk. Additionally, OVERMISS outperforms the
baseline in translation performance (e.g. an increase in BLEU scores from 0.69
to 3.12 and an average improvement of 0.48 percentage comet scores for
LLaMA-7b) with further enhancements seen in models combining OVERMISS and SWIE
(e.g. the BLUE scores increase up to 0.56 from English to German across three
different backbones), and both exhibit improvements in the faithfulness metric
based on word alignment.
</p></li>
</ul>

<h3>Title: Harnessing the Power of David against Goliath: Exploring Instruction Data Generation without Using Closed-Source Models. (arXiv:2308.12711v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12711">http://arxiv.org/abs/2308.12711</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12711]] Harnessing the Power of David against Goliath: Exploring Instruction Data Generation without Using Closed-Source Models(http://arxiv.org/abs/2308.12711)</code></li>
<li>Summary: <p>Instruction tuning is instrumental in enabling Large Language Models~(LLMs)
to follow user instructions to complete various open-domain tasks. The success
of instruction tuning depends on the availability of high-quality instruction
data. Owing to the exorbitant cost and substandard quality of human annotation,
recent works have been deeply engaged in the exploration of the utilization of
powerful closed-source models to generate instruction data automatically.
However, these methods carry potential risks arising from the usage
requirements of powerful closed-source models, which strictly forbid the
utilization of their outputs to develop machine learning models. To deal with
this problem, in this work, we explore alternative approaches to generate
high-quality instruction data that do not rely on closed-source models. Our
exploration includes an investigation of various existing instruction
generation methods, culminating in the integration of the most efficient
variant with two novel strategies to enhance the quality further. Evaluation
results from two benchmarks and the GPT-4 model demonstrate the effectiveness
of our generated instruction data, which can outperform Alpaca, a method
reliant on closed-source models. We hope that more progress can be achieved in
generating high-quality instruction data without using closed-source models.
</p></li>
</ul>

<h3>Title: Large Language Models Vote: Prompting for Rare Disease Identification. (arXiv:2308.12890v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12890">http://arxiv.org/abs/2308.12890</a></li>
<li>Code URL: https://github.com/oniani/llms-vote</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12890]] Large Language Models Vote: Prompting for Rare Disease Identification(http://arxiv.org/abs/2308.12890)</code></li>
<li>Summary: <p>The emergence of generative Large Language Models (LLMs) emphasizes the need
for accurate and efficient prompting approaches. LLMs are often applied in
Few-Shot Learning (FSL) contexts, where tasks are executed with minimal
training data. FSL has become popular in many Artificial Intelligence (AI)
subdomains, including AI for health. Rare diseases, affecting a small fraction
of the population, inherently require FSL techniques due to limited data
availability, though manual data collection and annotation is costly and
time-consuming. In this paper, we propose Models-Vote Prompting (MVP), a
flexible prompting approach for improving the performance of LLM queries in FSL
settings. MVP works by prompting numerous LLMs to perform the same tasks and
then conducting a majority vote on the resulting outputs. This method achieves
improved results to any one model in the ensemble on one-shot rare disease
identification and classification tasks. We also release a novel rare disease
dataset for FSL, available to those who agreed to the MIMIC-IV Data Use
Agreement (DUA). Furthermore, in using MVP, each model is prompted multiple
times, substantially increasing the time needed for manual annotation, and to
address this, we assess the feasibility of using JSON for automating generative
LLM evaluation.
</p></li>
</ul>

<h3>Title: Code Llama: Open Foundation Models for Code. (arXiv:2308.12950v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12950">http://arxiv.org/abs/2308.12950</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12950]] Code Llama: Open Foundation Models for Code(http://arxiv.org/abs/2308.12950)</code></li>
<li>Summary: <p>We release Code Llama, a family of large language models for code based on
Llama 2 providing state-of-the-art performance among open models, infilling
capabilities, support for large input contexts, and zero-shot instruction
following ability for programming tasks. We provide multiple flavors to cover a
wide range of applications: foundation models (Code Llama), Python
specializations (Code Llama - Python), and instruction-following models (Code
Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained
on sequences of 16k tokens and show improvements on inputs with up to 100k
tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support
infilling based on surrounding content. Code Llama reaches state-of-the-art
performance among open models on several code benchmarks, with scores of up to
53% and 55% on HumanEval and MBPP, respectively. Notably, Code Llama - Python
7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform
every other publicly available model on MultiPL-E. We release Code Llama under
a permissive license that allows for both research and commercial use.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: A Spatiotemporal Correspondence Approach to Unsupervised LiDAR Segmentation with Traffic Applications. (arXiv:2308.12433v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12433">http://arxiv.org/abs/2308.12433</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12433]] A Spatiotemporal Correspondence Approach to Unsupervised LiDAR Segmentation with Traffic Applications(http://arxiv.org/abs/2308.12433)</code></li>
<li>Summary: <p>We address the problem of unsupervised semantic segmentation of outdoor LiDAR
point clouds in diverse traffic scenarios. The key idea is to leverage the
spatiotemporal nature of a dynamic point cloud sequence and introduce
drastically stronger augmentation by establishing spatiotemporal
correspondences across multiple frames. We dovetail clustering and pseudo-label
learning in this work. Essentially, we alternate between clustering points into
semantic groups and optimizing models using point-wise pseudo-spatiotemporal
labels with a simple learning objective. Therefore, our method can learn
discriminative features in an unsupervised learning fashion. We show promising
segmentation performance on Semantic-KITTI, SemanticPOSS, and FLORIDA benchmark
datasets covering scenarios in autonomous vehicle and intersection
infrastructure, which is competitive when compared against many existing fully
supervised learning methods. This general framework can lead to a unified
representation learning approach for LiDAR point clouds incorporating domain
knowledge.
</p></li>
</ul>

<h3>Title: Channel and Spatial Relation-Propagation Network for RGB-Thermal Semantic Segmentation. (arXiv:2308.12534v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12534">http://arxiv.org/abs/2308.12534</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12534]] Channel and Spatial Relation-Propagation Network for RGB-Thermal Semantic Segmentation(http://arxiv.org/abs/2308.12534)</code></li>
<li>Summary: <p>RGB-Thermal (RGB-T) semantic segmentation has shown great potential in
handling low-light conditions where RGB-based segmentation is hindered by poor
RGB imaging quality. The key to RGB-T semantic segmentation is to effectively
leverage the complementarity nature of RGB and thermal images. Most existing
algorithms fuse RGB and thermal information in feature space via concatenation,
element-wise summation, or attention operations in either unidirectional
enhancement or bidirectional aggregation manners. However, they usually
overlook the modality gap between RGB and thermal images during feature fusion,
resulting in modality-specific information from one modality contaminating the
other. In this paper, we propose a Channel and Spatial Relation-Propagation
Network (CSRPNet) for RGB-T semantic segmentation, which propagates only
modality-shared information across different modalities and alleviates the
modality-specific information contamination issue. Our CSRPNet first performs
relation-propagation in channel and spatial dimensions to capture the
modality-shared features from the RGB and thermal features. CSRPNet then
aggregates the modality-shared features captured from one modality with the
input feature from the other modality to enhance the input feature without the
contamination issue. While being fused together, the enhanced RGB and thermal
features will be also fed into the subsequent RGB or thermal feature extraction
layers for interactive feature fusion, respectively. We also introduce a
dual-path cascaded feature refinement module that aggregates multi-layer
features to produce two refined features for semantic and boundary prediction.
Extensive experimental results demonstrate that CSRPNet performs favorably
against state-of-the-art algorithms.
</p></li>
</ul>

<h3>Title: Logic-induced Diagnostic Reasoning for Semi-supervised Semantic Segmentation. (arXiv:2308.12595v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12595">http://arxiv.org/abs/2308.12595</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12595]] Logic-induced Diagnostic Reasoning for Semi-supervised Semantic Segmentation(http://arxiv.org/abs/2308.12595)</code></li>
<li>Summary: <p>Recent advances in semi-supervised semantic segmentation have been heavily
reliant on pseudo labeling to compensate for limited labeled data, disregarding
the valuable relational knowledge among semantic concepts. To bridge this gap,
we devise LogicDiag, a brand new neural-logic semi-supervised learning
framework. Our key insight is that conflicts within pseudo labels, identified
through symbolic knowledge, can serve as strong yet commonly ignored learning
signals. LogicDiag resolves such conflicts via reasoning with logic-induced
diagnoses, enabling the recovery of (potentially) erroneous pseudo labels,
ultimately alleviating the notorious error accumulation problem. We showcase
the practical application of LogicDiag in the data-hungry segmentation
scenario, where we formalize the structured abstraction of semantic concepts as
a set of logic rules. Extensive experiments on three standard semi-supervised
semantic segmentation benchmarks demonstrate the effectiveness and generality
of LogicDiag. Moreover, LogicDiag highlights the promising opportunities
arising from the systematic integration of symbolic reasoning into the
prevalent statistical, neural learning approaches.
</p></li>
</ul>

<h3>Title: FastSurfer-HypVINN: Automated sub-segmentation of the hypothalamus and adjacent structures on high-resolutional brain MRI. (arXiv:2308.12736v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12736">http://arxiv.org/abs/2308.12736</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12736]] FastSurfer-HypVINN: Automated sub-segmentation of the hypothalamus and adjacent structures on high-resolutional brain MRI(http://arxiv.org/abs/2308.12736)</code></li>
<li>Summary: <p>The hypothalamus plays a crucial role in the regulation of a broad range of
physiological, behavioural, and cognitive functions. However, despite its
importance, only a few small-scale neuroimaging studies have investigated its
substructures, likely due to the lack of fully automated segmentation tools to
address scalability and reproducibility issues of manual segmentation. While
the only previous attempt to automatically sub-segment the hypothalamus with a
neural network showed promise for 1.0 mm isotropic T1-weighted (T1w) MRI, there
is a need for an automated tool to sub-segment also high-resolutional (HiRes)
MR scans, as they are becoming widely available, and include structural detail
also from multi-modal MRI. We, therefore, introduce a novel, fast, and fully
automated deep learning method named HypVINN for sub-segmentation of the
hypothalamus and adjacent structures on 0.8 mm isotropic T1w and T2w brain MR
images that is robust to missing modalities. We extensively validate our model
with respect to segmentation accuracy, generalizability, in-session test-retest
reliability, and sensitivity to replicate hypothalamic volume effects (e.g.
sex-differences). The proposed method exhibits high segmentation performance
both for standalone T1w images as well as for T1w/T2w image pairs. Even with
the additional capability to accept flexible inputs, our model matches or
exceeds the performance of state-of-the-art methods with fixed inputs. We,
further, demonstrate the generalizability of our method in experiments with 1.0
mm MR scans from both the Rhineland Study and the UK Biobank. Finally, HypVINN
can perform the segmentation in less than a minute (GPU) and will be available
in the open source FastSurfer neuroimaging software suite, offering a
validated, efficient, and scalable solution for evaluating imaging-derived
phenotypes of the hypothalamus.
</p></li>
</ul>

<h3>Title: PartSeg: Few-shot Part Segmentation via Part-aware Prompt Learning. (arXiv:2308.12757v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12757">http://arxiv.org/abs/2308.12757</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12757]] PartSeg: Few-shot Part Segmentation via Part-aware Prompt Learning(http://arxiv.org/abs/2308.12757)</code></li>
<li>Summary: <p>In this work, we address the task of few-shot part segmentation, which aims
to segment the different parts of an unseen object using very few labeled
examples. It is found that leveraging the textual space of a powerful
pre-trained image-language model (such as CLIP) can be beneficial in learning
visual features. Therefore, we develop a novel method termed PartSeg for
few-shot part segmentation based on multimodal learning. Specifically, we
design a part-aware prompt learning method to generate part-specific prompts
that enable the CLIP model to better understand the concept of ``part'' and
fully utilize its textual space. Furthermore, since the concept of the same
part under different object categories is general, we establish relationships
between these parts during the prompt learning process. We conduct extensive
experiments on the PartImageNet and Pascal$\_$Part datasets, and the
experimental results demonstrated that our proposed method achieves
state-of-the-art performance.
</p></li>
</ul>

<h3>Title: Robotic Scene Segmentation with Memory Network for Runtime Surgical Context Inference. (arXiv:2308.12789v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12789">http://arxiv.org/abs/2308.12789</a></li>
<li>Code URL: https://github.com/uva-dsa/runtime_robscene_seg_2context</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12789]] Robotic Scene Segmentation with Memory Network for Runtime Surgical Context Inference(http://arxiv.org/abs/2308.12789)</code></li>
<li>Summary: <p>Surgical context inference has recently garnered significant attention in
robot-assisted surgery as it can facilitate workflow analysis, skill
assessment, and error detection. However, runtime context inference is
challenging since it requires timely and accurate detection of the interactions
among the tools and objects in the surgical scene based on the segmentation of
video data. On the other hand, existing state-of-the-art video segmentation
methods are often biased against infrequent classes and fail to provide
temporal consistency for segmented masks. This can negatively impact the
context inference and accurate detection of critical states. In this study, we
propose a solution to these challenges using a Space Time Correspondence
Network (STCN). STCN is a memory network that performs binary segmentation and
minimizes the effects of class imbalance. The use of a memory bank in STCN
allows for the utilization of past image and segmentation information, thereby
ensuring consistency of the masks. Our experiments using the publicly available
JIGSAWS dataset demonstrate that STCN achieves superior segmentation
performance for objects that are difficult to segment, such as needle and
thread, and improves context inference compared to the state-of-the-art. We
also demonstrate that segmentation and context inference can be performed at
runtime without compromising performance.
</p></li>
</ul>

<h3>Title: Boosting Semantic Segmentation from the Perspective of Explicit Class Embeddings. (arXiv:2308.12894v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12894">http://arxiv.org/abs/2308.12894</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12894]] Boosting Semantic Segmentation from the Perspective of Explicit Class Embeddings(http://arxiv.org/abs/2308.12894)</code></li>
<li>Summary: <p>Semantic segmentation is a computer vision task that associates a label with
each pixel in an image. Modern approaches tend to introduce class embeddings
into semantic segmentation for deeply utilizing category semantics, and regard
supervised class masks as final predictions. In this paper, we explore the
mechanism of class embeddings and have an insight that more explicit and
meaningful class embeddings can be generated based on class masks purposely.
Following this observation, we propose ECENet, a new segmentation paradigm, in
which class embeddings are obtained and enhanced explicitly during interacting
with multi-stage image features. Based on this, we revisit the traditional
decoding process and explore inverted information flow between segmentation
masks and class embeddings. Furthermore, to ensure the discriminability and
informativity of features from backbone, we propose a Feature Reconstruction
module, which combines intrinsic and diverse branches together to ensure the
concurrence of diversity and redundancy in features. Experiments show that our
ECENet outperforms its counterparts on the ADE20K dataset with much less
computational cost and achieves new state-of-the-art results on PASCAL-Context
dataset. The code will be released at https://gitee.com/mindspore/models and
https://github.com/Carol-lyh/ECENet.
</p></li>
</ul>

<h3>Title: Panoptic-Depth Color Map for Combination of Depth and Image Segmentation. (arXiv:2308.12937v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12937">http://arxiv.org/abs/2308.12937</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12937]] Panoptic-Depth Color Map for Combination of Depth and Image Segmentation(http://arxiv.org/abs/2308.12937)</code></li>
<li>Summary: <p>Image segmentation and depth estimation are crucial tasks in computer vision,
especially in autonomous driving scenarios. Although these tasks are typically
addressed separately, we propose an innovative approach to combine them in our
novel deep learning network, Panoptic-DepthLab. By incorporating an additional
depth estimation branch into the segmentation network, it can predict the depth
of each instance segment. Evaluating on Cityscape dataset, we demonstrate the
effectiveness of our method in achieving high-quality segmentation results with
depth and visualize it with a color map. Our proposed method demonstrates a new
possibility of combining different tasks and networks to generate a more
comprehensive image recognition result to facilitate the safety of autonomous
driving vehicles.
</p></li>
</ul>

<h3>Title: Less is More: Towards Efficient Few-shot 3D Semantic Segmentation via Training-free Networks. (arXiv:2308.12961v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12961">http://arxiv.org/abs/2308.12961</a></li>
<li>Code URL: https://github.com/yangyangyang127/tfs3d</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12961]] Less is More: Towards Efficient Few-shot 3D Semantic Segmentation via Training-free Networks(http://arxiv.org/abs/2308.12961)</code></li>
<li>Summary: <p>To reduce the reliance on large-scale datasets, recent works in 3D
segmentation resort to few-shot learning. Current 3D few-shot semantic
segmentation methods first pre-train the models on `seen' classes, and then
evaluate their generalization performance on `unseen' classes. However, the
prior pre-training stage not only introduces excessive time overhead, but also
incurs a significant domain gap on `unseen' classes. To tackle these issues, we
propose an efficient Training-free Few-shot 3D Segmentation netwrok, TFS3D, and
a further training-based variant, TFS3D-T. Without any learnable parameters,
TFS3D extracts dense representations by trigonometric positional encodings, and
achieves comparable performance to previous training-based methods. Due to the
elimination of pre-training, TFS3D can alleviate the domain gap issue and save
a substantial amount of time. Building upon TFS3D, TFS3D-T only requires to
train a lightweight query-support transferring attention (QUEST), which
enhances the interaction between the few-shot query and support data.
Experiments demonstrate TFS3D-T improves previous state-of-the-art methods by
+6.93% and +17.96% mIoU respectively on S3DIS and ScanNet, while reducing the
training time by -90%, indicating superior effectiveness and efficiency.
</p></li>
</ul>

<h3>Title: Scenimefy: Learning to Craft Anime Scene via Semi-Supervised Image-to-Image Translation. (arXiv:2308.12968v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.12968">http://arxiv.org/abs/2308.12968</a></li>
<li>Code URL: https://github.com/yuxinn-j/scenimefy</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.12968]] Scenimefy: Learning to Craft Anime Scene via Semi-Supervised Image-to-Image Translation(http://arxiv.org/abs/2308.12968)</code></li>
<li>Summary: <p>Automatic high-quality rendering of anime scenes from complex real-world
images is of significant practical value. The challenges of this task lie in
the complexity of the scenes, the unique features of anime style, and the lack
of high-quality datasets to bridge the domain gap. Despite promising attempts,
previous efforts are still incompetent in achieving satisfactory results with
consistent semantic preservation, evident stylization, and fine details. In
this study, we propose Scenimefy, a novel semi-supervised image-to-image
translation framework that addresses these challenges. Our approach guides the
learning with structure-consistent pseudo paired data, simplifying the pure
unsupervised setting. The pseudo data are derived uniquely from a
semantic-constrained StyleGAN leveraging rich model priors like CLIP. We
further apply segmentation-guided data selection to obtain high-quality pseudo
supervision. A patch-wise contrastive style loss is introduced to improve
stylization and fine details. Besides, we contribute a high-resolution anime
scene dataset to facilitate future research. Our extensive experiments
demonstrate the superiority of our method over state-of-the-art baselines in
terms of both perceptual quality and quantitative performance.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
