<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-01-05</h1>
<h2>secure</h2>
<h2>security</h2>
<h3>Title: Evasive Hardware Trojan through Adversarial Power Trace. (arXiv:2401.02342v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02342">http://arxiv.org/abs/2401.02342</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02342]] Evasive Hardware Trojan through Adversarial Power Trace(http://arxiv.org/abs/2401.02342)</code></li>
<li>Summary: <p>The globalization of the Integrated Circuit (IC) supply chain, driven by
time-to-market and cost considerations, has made ICs vulnerable to hardware
Trojans (HTs). Against this threat, a promising approach is to use Machine
Learning (ML)-based side-channel analysis, which has the advantage of being a
non-intrusive method, along with efficiently detecting HTs under golden
chip-free settings. In this paper, we question the trustworthiness of ML-based
HT detection via side-channel analysis. We introduce a HT obfuscation (HTO)
approach to allow HTs to bypass this detection method. Rather than
theoretically misleading the model by simulated adversarial traces, a key
aspect of our approach is the design and implementation of adversarial noise as
part of the circuitry, alongside the HT. We detail HTO methodologies for ASICs
and FPGAs, and evaluate our approach using TrustHub benchmark. Interestingly,
we found that HTO can be implemented with only a single transistor for ASIC
designs to generate adversarial power traces that can fool the defense with
100% efficiency. We also efficiently implemented our approach on a Spartan 6
Xilinx FPGA using 2 different variants: (i) DSP slices-based, and (ii)
ring-oscillator-based design. Additionally, we assess the efficiency of
countermeasures like spectral domain analysis, and we show that an adaptive
attacker can still design evasive HTOs by constraining the design with a
spectral noise budget. In addition, while adversarial training (AT) offers
higher protection against evasive HTs, AT models suffer from a considerable
utility loss, potentially rendering them unsuitable for such security
application. We believe this research represents a significant step in
understanding and exploiting ML vulnerabilities in a hardware security context,
and we make all resources and designs openly available online:
https://dev.d18uu4lqwhbmka.amplifyapp.com
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: CLAPP: Contrastive Language-Audio Pre-training in Passive Underwater Vessel Classification. (arXiv:2401.02099v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02099">http://arxiv.org/abs/2401.02099</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02099]] CLAPP: Contrastive Language-Audio Pre-training in Passive Underwater Vessel Classification(http://arxiv.org/abs/2401.02099)</code></li>
<li>Summary: <p>Existing research on audio classification faces challenges in recognizing
attributes of passive underwater vessel scenarios and lacks well-annotated
datasets due to data privacy concerns. In this study, we introduce CLAPP
(Contrastive Language-Audio Pre-training in Passive Underwater Vessel
Classification), a novel model. Our aim is to train a neural network using a
wide range of vessel audio and vessel state text pairs obtained from an
oceanship dataset. CLAPP is capable of directly learning from raw vessel audio
data and, when available, from carefully curated labels, enabling improved
recognition of vessel attributes in passive underwater vessel scenarios.
Model's zero-shot capability allows predicting the most relevant vessel state
description for a given vessel audio, without directly optimizing for the task.
Our approach aims to solve 2 challenges: vessel audio-text classification and
passive underwater vessel audio attribute recognition. The proposed method
achieves new state-of-the-art results on both Deepship and Shipsear public
datasets, with a notable margin of about 7%-13% for accuracy compared to prior
methods on zero-shot task.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Lightweight Fish Classification Model for Sustainable Marine Management: Indonesian Case. (arXiv:2401.02278v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02278">http://arxiv.org/abs/2401.02278</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02278]] Lightweight Fish Classification Model for Sustainable Marine Management: Indonesian Case(http://arxiv.org/abs/2401.02278)</code></li>
<li>Summary: <p>The enormous demand for seafood products has led to exploitation of marine
resources and near-extinction of some species. In particular, overfishing is
one the main issues in sustainable marine development. In alignment with the
protection of marine resources and sustainable fishing, this study proposes to
advance fish classification techniques that support identifying protected fish
species using state-of-the-art machine learning. We use a custom modification
of the MobileNet model to design a lightweight classifier called M-MobileNet
that is capable of running on limited hardware. As part of the study, we
compiled a labeled dataset of 37,462 images of fish found in the waters of the
Indonesian archipelago. The proposed model is trained on the dataset to
classify images of the captured fish into their species and give
recommendations on whether they are consumable or not. Our modified MobileNet
model uses only 50\% of the top layer parameters with about 42% GTX 860M
utility and achieves up to 97% accuracy in fish classification and determining
its consumability. Given the limited computing capacity available on many
fishing vessels, the proposed model provides a practical solution to on-site
fish classification. In addition, synchronized implementation of the proposed
model on multiple vessels can supply valuable information about the movement
and location of different species of fish.
</p></li>
</ul>

<h2>defense</h2>
<h3>Title: Reputation-Based Federated Learning Defense to Mitigate Threats in EEG Signal Classification. (arXiv:2401.01896v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01896">http://arxiv.org/abs/2401.01896</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01896]] Reputation-Based Federated Learning Defense to Mitigate Threats in EEG Signal Classification(http://arxiv.org/abs/2401.01896)</code></li>
<li>Summary: <p>This paper presents a reputation-based threat mitigation framework that
defends potential security threats in electroencephalogram (EEG) signal
classification during model aggregation of Federated Learning. While EEG signal
analysis has attracted attention because of the emergence of brain-computer
interface (BCI) technology, it is difficult to create efficient learning models
for EEG analysis because of the distributed nature of EEG data and related
privacy and security concerns. To address these challenges, the proposed
defending framework leverages the Federated Learning paradigm to preserve
privacy by collaborative model training with localized data from dispersed
sources and introduces a reputation-based mechanism to mitigate the influence
of data poisoning attacks and identify compromised participants. To assess the
efficiency of the proposed reputation-based federated learning defense
framework, data poisoning attacks based on the risk level of training data
derived by Explainable Artificial Intelligence (XAI) techniques are conducted
on both publicly available EEG signal datasets and the self-established EEG
signal dataset. Experimental results on the poisoned datasets show that the
proposed defense methodology performs well in EEG signal classification while
reducing the risks associated with security threats.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Backdoor Attack on Unpaired Medical Image-Text Foundation Models: A Pilot Study on MedCLIP. (arXiv:2401.01911v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01911">http://arxiv.org/abs/2401.01911</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01911]] Backdoor Attack on Unpaired Medical Image-Text Foundation Models: A Pilot Study on MedCLIP(http://arxiv.org/abs/2401.01911)</code></li>
<li>Summary: <p>In recent years, foundation models (FMs) have solidified their role as
cornerstone advancements in the deep learning domain. By extracting intricate
patterns from vast datasets, these models consistently achieve state-of-the-art
results across a spectrum of downstream tasks, all without necessitating
extensive computational resources. Notably, MedCLIP, a vision-language
contrastive learning-based medical FM, has been designed using unpaired
image-text training. While the medical domain has often adopted unpaired
training to amplify data, the exploration of potential security concerns linked
to this approach hasn't kept pace with its practical usage. Notably, the
augmentation capabilities inherent in unpaired training also indicate that
minor label discrepancies can result in significant model deviations. In this
study, we frame this label discrepancy as a backdoor attack problem. We further
analyze its impact on medical FMs throughout the FM supply chain. Our
evaluation primarily revolves around MedCLIP, emblematic of medical FM
employing the unpaired strategy. We begin with an exploration of
vulnerabilities in MedCLIP stemming from unpaired image-text matching, termed
BadMatch. BadMatch is achieved using a modest set of wrongly labeled data.
Subsequently, we disrupt MedCLIP's contrastive learning through
BadDist-assisted BadMatch by introducing a Bad-Distance between the embeddings
of clean and poisoned data. Additionally, combined with BadMatch and BadDist,
the attacking pipeline consistently fends off backdoor assaults across diverse
model designs, datasets, and triggers. Also, our findings reveal that current
defense strategies are insufficient in detecting these latent threats in
medical FMs' supply chains.
</p></li>
</ul>

<h3>Title: Spy-Watermark: Robust Invisible Watermarking for Backdoor Attack. (arXiv:2401.02031v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02031">http://arxiv.org/abs/2401.02031</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02031]] Spy-Watermark: Robust Invisible Watermarking for Backdoor Attack(http://arxiv.org/abs/2401.02031)</code></li>
<li>Summary: <p>Backdoor attack aims to deceive a victim model when facing backdoor instances
while maintaining its performance on benign data. Current methods use manual
patterns or special perturbations as triggers, while they often overlook the
robustness against data corruption, making backdoor attacks easy to defend in
practice. To address this issue, we propose a novel backdoor attack method
named Spy-Watermark, which remains effective when facing data collapse and
backdoor defense. Therein, we introduce a learnable watermark embedded in the
latent domain of images, serving as the trigger. Then, we search for a
watermark that can withstand collapse during image decoding, cooperating with
several anti-collapse operations to further enhance the resilience of our
trigger against data corruption. Extensive experiments are conducted on
CIFAR10, GTSRB, and ImageNet datasets, demonstrating that Spy-Watermark
overtakes ten state-of-the-art methods in terms of robustness and stealthiness.
</p></li>
</ul>

<h3>Title: Shadow Blade: A tool to interact with attack vectors. (arXiv:2401.01960v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01960">http://arxiv.org/abs/2401.01960</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01960]] Shadow Blade: A tool to interact with attack vectors(http://arxiv.org/abs/2401.01960)</code></li>
<li>Summary: <p>The increased demand of cyber security professionals has also increased the
development of new platforms and tools that help those professionals to improve
their offensive skills. One of these platforms is HackTheBox, an online cyber
security training platform that delivers a controlled and safe environment for
those professionals to explore virtual machines in a Capture the Flag (CTF)
competition style.
</p>
<p>Most of the tools used in a CTF, or even on real-world Penetration Testing
(Pentest), were developed for specific reasons so each tool usually has
different input and output formats. These different formats make it hard for
cyber security professionals and CTF competitors to develop an attack graph. In
order to help cyber security professionals and CTF competitors to discover,
select and exploit an attack vector, this paper presents Shadow Blade, a tool
to aid users to interact with their attack vectors.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Towards Truly Zero-shot Compositional Visual Reasoning with LLMs as Programmers. (arXiv:2401.01974v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01974">http://arxiv.org/abs/2401.01974</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01974]] Towards Truly Zero-shot Compositional Visual Reasoning with LLMs as Programmers(http://arxiv.org/abs/2401.01974)</code></li>
<li>Summary: <p>Visual reasoning is dominated by end-to-end neural networks scaled to
billions of model parameters and training examples. However, even the largest
models struggle with compositional reasoning, generalization, fine-grained
spatial and temporal reasoning, and counting. Visual reasoning with large
language models (LLMs) as controllers can, in principle, address these
limitations by decomposing the task and solving subtasks by orchestrating a set
of (visual) tools. Recently, these models achieved great performance on tasks
such as compositional visual question answering, visual grounding, and video
temporal reasoning. Nevertheless, in their current form, these models heavily
rely on human engineering of in-context examples in the prompt, which are often
dataset- and task-specific and require significant labor by highly skilled
programmers. In this work, we present a framework that mitigates these issues
by introducing spatially and temporally abstract routines and by leveraging a
small number of labeled examples to automatically generate in-context examples,
thereby avoiding human-created in-context examples. On a number of visual
reasoning tasks, we show that our framework leads to consistent gains in
performance, makes LLMs as controllers setup more robust, and removes the need
for human engineering of in-context examples.
</p></li>
</ul>

<h3>Title: AUPIMO: Redefining Visual Anomaly Detection Benchmarks with High Speed and Low Tolerance. (arXiv:2401.01984v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01984">http://arxiv.org/abs/2401.01984</a></li>
<li>Code URL: <a href="https://github.com/jpcbertoldo/aupimo">https://github.com/jpcbertoldo/aupimo</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01984]] AUPIMO: Redefining Visual Anomaly Detection Benchmarks with High Speed and Low Tolerance(http://arxiv.org/abs/2401.01984)</code></li>
<li>Summary: <p>Recent advances in visual anomaly detection research have seen AUROC and
AUPRO scores on public benchmark datasets such as MVTec and VisA converge
towards perfect recall, giving the impression that these benchmarks are
near-solved. However, high AUROC and AUPRO scores do not always reflect
qualitative performance, which limits the validity of these metrics in
real-world applications. We argue that the artificial ceiling imposed by the
lack of an adequate evaluation metric restrains progression of the field, and
it is crucial that we revisit the evaluation metrics used to rate our
algorithms. In response, we introduce Per-IMage Overlap (PIMO), a novel metric
that addresses the shortcomings of AUROC and AUPRO. PIMO retains the
recall-based nature of the existing metrics but introduces two distinctions:
the assignment of curves (and respective area under the curve) is per-image,
and its X-axis relies solely on normal images. Measuring recall per image
simplifies instance score indexing and is more robust to noisy annotations. As
we show, it also accelerates computation and enables the usage of statistical
tests to compare models. By imposing low tolerance for false positives on
normal images, PIMO provides an enhanced model validation procedure and
highlights performance variations across datasets. Our experiments demonstrate
that PIMO offers practical advantages and nuanced performance insights that
redefine anomaly detection benchmarks -- notably challenging the perception
that MVTec AD and VisA datasets have been solved by contemporary models.
Available on GitHub: https://github.com/jpcbertoldo/aupimo.
</p></li>
</ul>

<h3>Title: Explore Human Parsing Modality for Action Recognition. (arXiv:2401.02138v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02138">http://arxiv.org/abs/2401.02138</a></li>
<li>Code URL: <a href="https://github.com/liujf69/EPP-Net-Action">https://github.com/liujf69/EPP-Net-Action</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02138]] Explore Human Parsing Modality for Action Recognition(http://arxiv.org/abs/2401.02138)</code></li>
<li>Summary: <p>Multimodal-based action recognition methods have achieved high success using
pose and RGB modality. However, skeletons sequences lack appearance depiction
and RGB images suffer irrelevant noise due to modality limitations. To address
this, we introduce human parsing feature map as a novel modality, since it can
selectively retain effective semantic features of the body parts, while
filtering out most irrelevant noise. We propose a new dual-branch framework
called Ensemble Human Parsing and Pose Network (EPP-Net), which is the first to
leverage both skeletons and human parsing modalities for action recognition.
The first human pose branch feeds robust skeletons in graph convolutional
network to model pose features, while the second human parsing branch also
leverages depictive parsing feature maps to model parsing festures via
convolutional backbones. The two high-level features will be effectively
combined through a late fusion strategy for better action recognition.
Extensive experiments on NTU RGB+D and NTU RGB+D 120 benchmarks consistently
verify the effectiveness of our proposed EPP-Net, which outperforms the
existing action recognition methods. Our code is available at:
https://github.com/liujf69/EPP-Net-Action.
</p></li>
</ul>

<h3>Title: Prompt Decoupling for Text-to-Image Person Re-identification. (arXiv:2401.02173v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02173">http://arxiv.org/abs/2401.02173</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02173]] Prompt Decoupling for Text-to-Image Person Re-identification(http://arxiv.org/abs/2401.02173)</code></li>
<li>Summary: <p>Text-to-image person re-identification (TIReID) aims to retrieve the target
person from an image gallery via a textual description query. Recently,
pre-trained vision-language models like CLIP have attracted significant
attention and have been widely utilized for this task due to their robust
capacity for semantic concept learning and rich multi-modal knowledge. However,
recent CLIP-based TIReID methods commonly rely on direct fine-tuning of the
entire network to adapt the CLIP model for the TIReID task. Although these
methods show competitive performance on this topic, they are suboptimal as they
necessitate simultaneous domain adaptation and task adaptation. To address this
issue, we attempt to decouple these two processes during the training stage.
Specifically, we introduce the prompt tuning strategy to enable domain
adaptation and propose a two-stage training approach to disentangle domain
adaptation from task adaptation. In the first stage, we freeze the two encoders
from CLIP and solely focus on optimizing the prompts to alleviate domain gap
between the original training data of CLIP and downstream tasks. In the second
stage, we maintain the fixed prompts and fine-tune the CLIP model to prioritize
capturing fine-grained information, which is more suitable for TIReID task.
Finally, we evaluate the effectiveness of our method on three widely used
datasets. Compared to the directly fine-tuned approach, our method achieves
significant improvements.
</p></li>
</ul>

<h3>Title: Distillation-based fabric anomaly detection. (arXiv:2401.02287v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02287">http://arxiv.org/abs/2401.02287</a></li>
<li>Code URL: <a href="https://github.com/simonthomine/industrialtextiledataset">https://github.com/simonthomine/industrialtextiledataset</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02287]] Distillation-based fabric anomaly detection(http://arxiv.org/abs/2401.02287)</code></li>
<li>Summary: <p>Unsupervised texture anomaly detection has been a concerning topic in a vast
amount of industrial processes. Patterned textures inspection, particularly in
the context of fabric defect detection, is indeed a widely encountered use
case. This task involves handling a diverse spectrum of colors and textile
types, encompassing a wide range of fabrics. Given the extensive variability in
colors, textures, and defect types, fabric defect detection poses a complex and
challenging problem in the field of patterned textures inspection. In this
article, we propose a knowledge distillation-based approach tailored
specifically for addressing the challenge of unsupervised anomaly detection in
textures resembling fabrics. Our method aims to redefine the recently
introduced reverse distillation approach, which advocates for an
encoder-decoder design to mitigate classifier bias and to prevent the student
from reconstructing anomalies. In this study, we present a new reverse
distillation technique for the specific task of fabric defect detection. Our
approach involves a meticulous design selection that strategically highlights
high-level features. To demonstrate the capabilities of our approach both in
terms of performance and inference speed, we conducted a series of experiments
on multiple texture datasets, including MVTEC AD, AITEX, and TILDA, alongside
conducting experiments on a dataset acquired from a textile manufacturing
facility. The main contributions of this paper are the following: a robust
texture anomaly detector utilizing a reverse knowledge-distillation technique
suitable for both anomaly detection and domain generalization and a novel
dataset encompassing a diverse range of fabrics and defects.
</p></li>
</ul>

<h3>Title: Fit-NGP: Fitting Object Models to Neural Graphics Primitives. (arXiv:2401.02357v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02357">http://arxiv.org/abs/2401.02357</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02357]] Fit-NGP: Fitting Object Models to Neural Graphics Primitives(http://arxiv.org/abs/2401.02357)</code></li>
<li>Summary: <p>Accurate 3D object pose estimation is key to enabling many robotic
applications that involve challenging object interactions. In this work, we
show that the density field created by a state-of-the-art efficient radiance
field reconstruction method is suitable for highly accurate and robust pose
estimation for objects with known 3D models, even when they are very small and
with challenging reflective surfaces. We present a fully automatic object pose
estimation system based on a robot arm with a single wrist-mounted camera,
which can scan a scene from scratch, detect and estimate the 6-Degrees of
Freedom (DoF) poses of multiple objects within a couple of minutes of
operation. Small objects such as bolts and nuts are estimated with accuracy on
order of 1mm.
</p></li>
</ul>

<h3>Title: Generalist embedding models are better at short-context clinical semantic search than specialized embedding models. (arXiv:2401.01943v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01943">http://arxiv.org/abs/2401.01943</a></li>
<li>Code URL: <a href="https://github.com/kaduceo/icd10cm_embedding_benchmark">https://github.com/kaduceo/icd10cm_embedding_benchmark</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01943]] Generalist embedding models are better at short-context clinical semantic search than specialized embedding models(http://arxiv.org/abs/2401.01943)</code></li>
<li>Summary: <p>The increasing use of tools and solutions based on Large Language Models
(LLMs) for various tasks in the medical domain has become a prominent trend.
Their use in this highly critical and sensitive domain has thus raised
important questions about their robustness, especially in response to
variations in input, and the reliability of the generated outputs. This study
addresses these questions by constructing a textual dataset based on the
ICD-10-CM code descriptions, widely used in US hospitals and containing many
clinical terms, and their easily reproducible rephrasing. We then benchmarked
existing embedding models, either generalist or specialized in the clinical
domain, in a semantic search task where the goal was to correctly match the
rephrased text to the original description. Our results showed that generalist
models performed better than clinical models, suggesting that existing clinical
specialized models are more sensitive to small changes in input that confuse
them. The highlighted problem of specialized models may be due to the fact that
they have not been trained on sufficient data, and in particular on datasets
that are not diverse enough to have a reliable global language understanding,
which is still necessary for accurate handling of medical documents.
</p></li>
</ul>

<h3>Title: Are LLMs Robust for Spoken Dialogues?. (arXiv:2401.02297v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02297">http://arxiv.org/abs/2401.02297</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02297]] Are LLMs Robust for Spoken Dialogues?(http://arxiv.org/abs/2401.02297)</code></li>
<li>Summary: <p>Large Pre-Trained Language Models have demonstrated state-of-the-art
performance in different downstream tasks, including dialogue state tracking
and end-to-end response generation. Nevertheless, most of the publicly
available datasets and benchmarks on task-oriented dialogues focus on written
conversations. Consequently, the robustness of the developed models to spoken
interactions is unknown. In this work, we have evaluated the performance of
LLMs for spoken task-oriented dialogues on the DSTC11 test sets. Due to the
lack of proper spoken dialogue datasets, we have automatically transcribed a
development set of spoken dialogues with a state-of-the-art ASR engine. We have
characterized the ASR-error types and their distributions and simulated these
errors in a large dataset of dialogues. We report the intrinsic (perplexity)
and extrinsic (human evaluation) performance of fine-tuned GPT-2 and T5 models
in two subtasks of response generation and dialogue state tracking,
respectively. The results show that LLMs are not robust to spoken noise by
default, however, fine-tuning/training such models on a proper dataset of
spoken TODs can result in a more robust performance.
</p></li>
</ul>

<h3>Title: A Robust Adversary Detection-Deactivation Method for Metaverse-oriented Collaborative Deep Learning. (arXiv:2401.01895v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01895">http://arxiv.org/abs/2401.01895</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01895]] A Robust Adversary Detection-Deactivation Method for Metaverse-oriented Collaborative Deep Learning(http://arxiv.org/abs/2401.01895)</code></li>
<li>Summary: <p>Metaverse is trending to create a digital circumstance that can transfer the
real world to an online platform supported by large quantities of real-time
interactions. Pre-trained Artificial Intelligence (AI) models are demonstrating
their increasing capability in aiding the metaverse to achieve an excellent
response with negligible delay, and nowadays, many large models are
collaboratively trained by various participants in a manner named collaborative
deep learning (CDL). However, several security weaknesses can threaten the
safety of the CDL training process, which might result in fatal attacks to
either the pre-trained large model or the local sensitive data sets possessed
by an individual entity. In CDL, malicious participants can hide within the
major innocent and silently uploads deceptive parameters to degenerate the
model performance, or they can abuse the downloaded parameters to construct a
Generative Adversarial Network (GAN) to acquire the private information of
others illegally. To compensate for these vulnerabilities, this paper proposes
an adversary detection-deactivation method, which can limit and isolate the
access of potential malicious participants, quarantine and disable the
GAN-attack or harmful backpropagation of received threatening gradients. A
detailed protection analysis has been conducted on a Multiview CDL case, and
results show that the protocol can effectively prevent harmful access by
heuristic manner analysis and can protect the existing model by swiftly
checking received gradients using only one low-cost branch with an embedded
firewall.
</p></li>
</ul>

<h3>Title: Representation Learning of Multivariate Time Series using Attention and Adversarial Training. (arXiv:2401.01987v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01987">http://arxiv.org/abs/2401.01987</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01987]] Representation Learning of Multivariate Time Series using Attention and Adversarial Training(http://arxiv.org/abs/2401.01987)</code></li>
<li>Summary: <p>A critical factor in trustworthy machine learning is to develop robust
representations of the training data. Only under this guarantee methods are
legitimate to artificially generate data, for example, to counteract imbalanced
datasets or provide counterfactual explanations for blackbox decision-making
systems. In recent years, Generative Adversarial Networks (GANs) have shown
considerable results in forming stable representations and generating realistic
data. While many applications focus on generating image data, less effort has
been made in generating time series data, especially multivariate signals. In
this work, a Transformer-based autoencoder is proposed that is regularized
using an adversarial training scheme to generate artificial multivariate time
series signals. The representation is evaluated using t-SNE visualizations,
Dynamic Time Warping (DTW) and Entropy scores. Our results indicate that the
generated signals exhibit higher similarity to an exemplary dataset than using
a convolutional network approach.
</p></li>
</ul>

<h3>Title: Decentralized Multi-Task Online Convex Optimization Under Random Link Failures. (arXiv:2401.02011v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02011">http://arxiv.org/abs/2401.02011</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02011]] Decentralized Multi-Task Online Convex Optimization Under Random Link Failures(http://arxiv.org/abs/2401.02011)</code></li>
<li>Summary: <p>Decentralized optimization methods often entail information exchange between
neighbors. Transmission failures can happen due to network congestion,
hardware/software issues, communication outage, and other factors. In this
paper, we investigate the random link failure problem in decentralized
multi-task online convex optimization, where agents have individual decisions
that are coupled with each other via pairwise constraints. Although widely used
in constrained optimization, conventional saddle-point algorithms are not
directly applicable here because of random packet dropping. To address this
issue, we develop a robust decentralized saddle-point algorithm against random
link failures with heterogeneous probabilities by replacing the missing
decisions of neighbors with their latest received values. Then, by judiciously
bounding the accumulated deviation stemming from this replacement, we first
establish that our algorithm achieves $\mathcal{O}(\sqrt{T})$ regret and
$\mathcal{O}(T^\frac{3}{4})$ constraint violations for the full information
scenario, where the complete information on the local cost function is revealed
to each agent at the end of each time slot. These two bounds match, in order
sense, the performance bounds of algorithms with perfect communications.
Further, we extend our algorithm and analysis to the two-point bandit feedback
scenario, where only the values of the local cost function at two random points
are disclosed to each agent sequentially. Performance bounds of the same orders
as the full information case are derived. Finally, we corroborate the efficacy
of the proposed algorithms and the analytical results through numerical
simulations.
</p></li>
</ul>

<h3>Title: Fast & Fair: Efficient Second-Order Robust Optimization for Fairness in Machine Learning. (arXiv:2401.02012v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02012">http://arxiv.org/abs/2401.02012</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02012]] Fast & Fair: Efficient Second-Order Robust Optimization for Fairness in Machine Learning(http://arxiv.org/abs/2401.02012)</code></li>
<li>Summary: <p>This project explores adversarial training techniques to develop fairer Deep
Neural Networks (DNNs) to mitigate the inherent bias they are known to exhibit.
DNNs are susceptible to inheriting bias with respect to sensitive attributes
such as race and gender, which can lead to life-altering outcomes (e.g.,
demographic bias in facial recognition software used to arrest a suspect). We
propose a robust optimization problem, which we demonstrate can improve
fairness in several datasets, both synthetic and real-world, using an affine
linear model. Leveraging second order information, we are able to find a
solution to our optimization problem more efficiently than a purely first order
method.
</p></li>
</ul>

<h3>Title: U-Mixer: An Unet-Mixer Architecture with Stationarity Correction for Time Series Forecasting. (arXiv:2401.02236v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02236">http://arxiv.org/abs/2401.02236</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02236]] U-Mixer: An Unet-Mixer Architecture with Stationarity Correction for Time Series Forecasting(http://arxiv.org/abs/2401.02236)</code></li>
<li>Summary: <p>Time series forecasting is a crucial task in various domains. Caused by
factors such as trends, seasonality, or irregular fluctuations, time series
often exhibits non-stationary. It obstructs stable feature propagation through
deep layers, disrupts feature distributions, and complicates learning data
distribution changes. As a result, many existing models struggle to capture the
underlying patterns, leading to degraded forecasting performance. In this
study, we tackle the challenge of non-stationarity in time series forecasting
with our proposed framework called U-Mixer. By combining Unet and Mixer,
U-Mixer effectively captures local temporal dependencies between different
patches and channels separately to avoid the influence of distribution
variations among channels, and merge low- and high-levels features to obtain
comprehensive data representations. The key contribution is a novel
stationarity correction method, explicitly restoring data distribution by
constraining the difference in stationarity between the data before and after
model processing to restore the non-stationarity information, while ensuring
the temporal dependencies are preserved. Through extensive experiments on
various real-world time series datasets, U-Mixer demonstrates its effectiveness
and robustness, and achieves 14.5\% and 7.7\% improvements over
state-of-the-art (SOTA) methods.
</p></li>
</ul>

<h3>Title: Robust Physics Informed Neural Networks. (arXiv:2401.02300v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02300">http://arxiv.org/abs/2401.02300</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02300]] Robust Physics Informed Neural Networks(http://arxiv.org/abs/2401.02300)</code></li>
<li>Summary: <p>We introduce a Robust version of the Physics-Informed Neural Networks
(RPINNs) to approximate the Partial Differential Equations (PDEs) solution.
Standard Physics Informed Neural Networks (PINN) takes into account the
governing physical laws described by PDE during the learning process. The
network is trained on a data set that consists of randomly selected points in
the physical domain and its boundary. PINNs have been successfully applied to
solve various problems described by PDEs with boundary conditions. The loss
function in traditional PINNs is based on the strong residuals of the PDEs.
This loss function in PINNs is generally not robust with respect to the true
error. The loss function in PINNs can be far from the true error, which makes
the training process more difficult. In particular, we do not know if the
training process has already converged to the solution with the required
accuracy. This is especially true if we do not know the exact solution, so we
cannot estimate the true error during the training. This paper introduces a
different way of defining the loss function. It incorporates the residual and
the inverse of the Gram matrix, computed using the energy norm. We test our
RPINN algorithm on two Laplace problems and one advection-diffusion problem in
two spatial dimensions. We conclude that RPINN is a robust method. The proposed
loss coincides well with the true error of the solution, as measured in the
energy norm. Thus, we know if our training process goes well, and we know when
to stop the training to obtain the neural network approximation of the solution
of the PDE with the true error of required accuracy.
</p></li>
</ul>

<h3>Title: A Robust Quantile Huber Loss With Interpretable Parameter Adjustment In Distributional Reinforcement Learning. (arXiv:2401.02325v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02325">http://arxiv.org/abs/2401.02325</a></li>
<li>Code URL: <a href="https://github.com/parvin95/generalized-quantile-huber-loss">https://github.com/parvin95/generalized-quantile-huber-loss</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02325]] A Robust Quantile Huber Loss With Interpretable Parameter Adjustment In Distributional Reinforcement Learning(http://arxiv.org/abs/2401.02325)</code></li>
<li>Summary: <p>Distributional Reinforcement Learning (RL) estimates return distribution
mainly by learning quantile values via minimizing the quantile Huber loss
function, entailing a threshold parameter often selected heuristically or via
hyperparameter search, which may not generalize well and can be suboptimal.
This paper introduces a generalized quantile Huber loss function derived from
Wasserstein distance (WD) calculation between Gaussian distributions, capturing
noise in predicted (current) and target (Bellman-updated) quantile values.
Compared to the classical quantile Huber loss, this innovative loss function
enhances robustness against outliers. Notably, the classical Huber loss
function can be seen as an approximation of our proposed loss, enabling
parameter adjustment by approximating the amount of noise in the data during
the learning process. Empirical tests on Atari games, a common application in
distributional RL, and a recent hedging strategy using distributional RL,
validate the effectiveness of our proposed loss function and its potential for
parameter adjustments in distributional RL.
</p></li>
</ul>

<h3>Title: A Survey Analyzing Generalization in Deep Reinforcement Learning. (arXiv:2401.02349v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02349">http://arxiv.org/abs/2401.02349</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02349]] A Survey Analyzing Generalization in Deep Reinforcement Learning(http://arxiv.org/abs/2401.02349)</code></li>
<li>Summary: <p>Reinforcement learning research obtained significant success and attention
with the utilization of deep neural networks to solve problems in high
dimensional state or action spaces. While deep reinforcement learning policies
are currently being deployed in many different fields from medical applications
to self driving vehicles, there are still ongoing questions the field is trying
to answer on the generalization capabilities of deep reinforcement learning
policies. In this paper, we will outline the fundamental reasons why deep
reinforcement learning policies encounter overfitting problems that limit their
robustness and generalization capabilities. Furthermore, we will formalize and
unify the diverse solution approaches to increase generalization, and overcome
overfitting in state-action value functions. We believe our study can provide a
compact systematic unified analysis for the current advancements in deep
reinforcement learning, and help to construct robust deep neural policies with
improved generalization abilities.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Text2MDT: Extracting Medical Decision Trees from Medical Texts. (arXiv:2401.02034v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02034">http://arxiv.org/abs/2401.02034</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02034]] Text2MDT: Extracting Medical Decision Trees from Medical Texts(http://arxiv.org/abs/2401.02034)</code></li>
<li>Summary: <p>Knowledge of the medical decision process, which can be modeled as medical
decision trees (MDTs), is critical to build clinical decision support systems.
However, the current MDT construction methods rely heavily on time-consuming
and laborious manual annotation. In this work, we propose a novel task,
Text2MDT, to explore the automatic extraction of MDTs from medical texts such
as medical guidelines and textbooks. We normalize the form of the MDT and
create an annotated Text-to-MDT dataset in Chinese with the participation of
medical experts. We investigate two different methods for the Text2MDT tasks:
(a) an end-to-end framework which only relies on a GPT style large language
models (LLM) instruction tuning to generate all the node information and tree
structures. (b) The pipeline framework which decomposes the Text2MDT task to
three subtasks. Experiments on our Text2MDT dataset demonstrate that: (a) the
end-to-end method basd on LLMs (7B parameters or larger) show promising
results, and successfully outperform the pipeline methods. (b) The
chain-of-thought (COT) prompting method \cite{Wei2022ChainOT} can improve the
performance of the fine-tuned LLMs on the Text2MDT test set. (c) the
lightweight pipelined method based on encoder-based pretrained models can
perform comparably with LLMs with model complexity two magnititudes smaller.
Our Text2MDT dataset is open-sourced at
\url{https://tianchi.aliyun.com/dataset/95414}, and the source codes are
open-sourced at \url{https://github.com/michael-wzhu/text2dt}.
</p></li>
</ul>

<h3>Title: Beyond Extraction: Contextualising Tabular Data for Efficient Summarisation by Language Models. (arXiv:2401.02333v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02333">http://arxiv.org/abs/2401.02333</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02333]] Beyond Extraction: Contextualising Tabular Data for Efficient Summarisation by Language Models(http://arxiv.org/abs/2401.02333)</code></li>
<li>Summary: <p>The conventional use of the Retrieval-Augmented Generation (RAG) architecture
has proven effective for retrieving information from diverse documents.
However, challenges arise in handling complex table queries, especially within
PDF documents containing intricate tabular structures.This research introduces
an innovative approach to enhance the accuracy of complex table queries in
RAG-based systems. Our methodology involves storing PDFs in the retrieval
database and extracting tabular content separately. The extracted tables
undergo a process of context enrichment, concatenating headers with
corresponding values. To ensure a comprehensive understanding of the enriched
data, we employ a fine-tuned version of the Llama-2-chat language model for
summarisation within the RAG architecture. Furthermore, we augment the tabular
data with contextual sense using the ChatGPT 3.5 API through a one-shot prompt.
This enriched data is then fed into the retrieval database alongside other
PDFs. Our approach aims to significantly improve the precision of complex table
queries, offering a promising solution to a longstanding challenge in
information retrieval.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Federated Class-Incremental Learning with Prototype Guided Transformer. (arXiv:2401.02094v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02094">http://arxiv.org/abs/2401.02094</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02094]] Federated Class-Incremental Learning with Prototype Guided Transformer(http://arxiv.org/abs/2401.02094)</code></li>
<li>Summary: <p>Existing federated learning methods have effectively addressed decentralized
learning in scenarios involving data privacy and non-IID data. However, in
real-world situations, each client dynamically learns new classes, requiring
the global model to maintain discriminative capabilities for both new and old
classes. To effectively mitigate the effects of catastrophic forgetting and
data heterogeneity under low communication costs, we designed a simple and
effective method named PLoRA. On the one hand, we adopt prototype learning to
learn better feature representations and leverage the heuristic information
between prototypes and class features to design a prototype re-weight module to
solve the classifier bias caused by data heterogeneity without retraining the
classification layer. On the other hand, our approach utilizes a pre-trained
model as the backbone and utilizes LoRA to fine-tune with a tiny amount of
parameters when learning new classes. Moreover, PLoRA does not rely on
similarity-based module selection strategies, thereby further reducing
communication overhead. Experimental results on standard datasets indicate that
our method outperforms the state-of-the-art approaches significantly. More
importantly, our method exhibits strong robustness and superiority in various
scenarios and degrees of data heterogeneity. Our code will be publicly
available.
</p></li>
</ul>

<h3>Title: Not all Minorities are Equal: Empty-Class-Aware Distillation for Heterogeneous Federated Learning. (arXiv:2401.02329v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02329">http://arxiv.org/abs/2401.02329</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02329]] Not all Minorities are Equal: Empty-Class-Aware Distillation for Heterogeneous Federated Learning(http://arxiv.org/abs/2401.02329)</code></li>
<li>Summary: <p>Data heterogeneity, characterized by disparities in local data distribution
across clients, poses a significant challenge in federated learning.
Substantial efforts have been devoted to addressing the heterogeneity in local
label distribution. As minority classes suffer from worse accuracy due to
overfitting on local imbalanced data, prior methods often incorporate
class-balanced learning techniques during local training. Despite the improved
mean accuracy across all classes, we observe that empty classes-referring to
categories absent from a client's data distribution-are still not well
recognized. This paper introduces FedED, a novel approach in heterogeneous
federated learning that integrates both empty-class distillation and logit
suppression simultaneously. Specifically, empty-class distillation leverages
knowledge distillation during local training on each client to retain essential
information related to empty classes from the global model. Moreover, logit
suppression directly penalizes network logits for non-label classes,
effectively addressing misclassifications in minority classes that may be
biased toward majority classes. Extensive experiments validate the efficacy of
FedED, surpassing previous state-of-the-art methods across diverse datasets
with varying degrees of label distribution shift.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Marginal Debiased Network for Fair Visual Recognition. (arXiv:2401.02150v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02150">http://arxiv.org/abs/2401.02150</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02150]] Marginal Debiased Network for Fair Visual Recognition(http://arxiv.org/abs/2401.02150)</code></li>
<li>Summary: <p>Deep neural networks (DNNs) are often prone to learn the spurious
correlations between target classes and bias attributes, like gender and race,
inherent in a major portion of training data (bias-aligned samples), thus
showing unfair behavior and arising controversy in the modern pluralistic and
egalitarian society. In this paper, we propose a novel marginal debiased
network (MDN) to learn debiased representations. More specifically, a marginal
softmax loss (MSL) is designed by introducing the idea of margin penalty into
the fairness problem, which assigns a larger margin for bias-conflicting
samples (data without spurious correlations) than for bias-aligned ones, so as
to deemphasize the spurious correlations and improve generalization on unbiased
test criteria. To determine the margins, our MDN is optimized through a meta
learning framework. We propose a meta equalized loss (MEL) to perceive the
model fairness, and adaptively update the margin parameters by metaoptimization
which requires the trained model guided by the optimal margins should minimize
MEL computed on an unbiased meta-validation set. Extensive experiments on
BiasedMNIST, Corrupted CIFAR-10, CelebA and UTK-Face datasets demonstrate that
our MDN can achieve a remarkable performance on under-represented samples and
obtain superior debiased results against the previous approaches.
</p></li>
</ul>

<h3>Title: Travelers: A scalable fair ordering BFT system. (arXiv:2401.02030v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02030">http://arxiv.org/abs/2401.02030</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02030]] Travelers: A scalable fair ordering BFT system(http://arxiv.org/abs/2401.02030)</code></li>
<li>Summary: <p>Many blockchain platform are subject to maximal value extraction (MEV), and
users on the platform are losing money while sending transactions because the
transaction order can be manipulated to extract value from them. Consensus
protocols have been augmented with different notion of fair ordering in order
to counter the problem. Out of all practical protocols, the most efficient BFT
consensus requires $O(nTL + n^2T)$ communication complexity, where $n$ is
number node, $T$ is number of transactions and $L$ is average transaction size.
In this work, we propose a new system of BFT fair ordering protocols,
Travelers, that substantially reduce the communication complexity. The proposed
system of protocols satisfy a new notion of fair ordering, called probabilistic
fair ordering, which is an extension to some existing notions of fairness. The
new notion allows a small probability of error $\epsilon$, that adversary can
insert some transactions at any location in a block, but for the remaining
$1-\epsilon$ the a modified version of ordering linearizability holds. Our
mechanism neither require a dissemination network nor direct submissions to all
consensus nodes. The key innovation comes from a routing protocol, that is both
flexible and efficient. We construct a protocol with $O(c\log({n})TL + n^2)$
communication complexity with $\epsilon = 1/n^c$ for some system parameter
$c\ge 1$.
</p></li>
</ul>

<h3>Title: FairGridSearch: A Framework to Compare Fairness-Enhancing Models. (arXiv:2401.02183v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02183">http://arxiv.org/abs/2401.02183</a></li>
<li>Code URL: <a href="https://github.com/dorisscma/fairgridsearch">https://github.com/dorisscma/fairgridsearch</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02183]] FairGridSearch: A Framework to Compare Fairness-Enhancing Models(http://arxiv.org/abs/2401.02183)</code></li>
<li>Summary: <p>Machine learning models are increasingly used in critical decision-making
applications. However, these models are susceptible to replicating or even
amplifying bias present in real-world data. While there are various bias
mitigation methods and base estimators in the literature, selecting the optimal
model for a specific application remains challenging.
</p>
<p>This paper focuses on binary classification and proposes FairGridSearch, a
novel framework for comparing fairness-enhancing models. FairGridSearch enables
experimentation with different model parameter combinations and recommends the
best one. The study applies FairGridSearch to three popular datasets (Adult,
COMPAS, and German Credit) and analyzes the impacts of metric selection, base
estimator choice, and classification threshold on model fairness.
</p>
<p>The results highlight the significance of selecting appropriate accuracy and
fairness metrics for model evaluation. Additionally, different base estimators
and classification threshold values affect the effectiveness of bias mitigation
methods and fairness stability respectively, but the effects are not consistent
across all datasets. Based on these findings, future research on fairness in
machine learning should consider a broader range of factors when building fair
models, going beyond bias mitigation methods alone.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Bayesian Intrinsic Groupwise Image Registration: Unsupervised Disentanglement of Anatomy and Geometry. (arXiv:2401.02141v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02141">http://arxiv.org/abs/2401.02141</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02141]] Bayesian Intrinsic Groupwise Image Registration: Unsupervised Disentanglement of Anatomy and Geometry(http://arxiv.org/abs/2401.02141)</code></li>
<li>Summary: <p>This article presents a general Bayesian learning framework for multi-modal
groupwise registration on medical images. The method builds on probabilistic
modelling of the image generative process, where the underlying common anatomy
and geometric variations of the observed images are explicitly disentangled as
latent variables. Thus, groupwise registration is achieved through the solution
to Bayesian inference. We propose a novel hierarchical variational
auto-encoding architecture to realize the inference procedure of the latent
variables, where the registration parameters can be calculated in a
mathematically interpretable fashion. Remarkably, this new paradigm can learn
groupwise registration in an unsupervised closed-loop self-reconstruction
process, sparing the burden of designing complex intensity-based similarity
measures. The computationally efficient disentangled architecture is also
inherently scalable and flexible, allowing for groupwise registration on
large-scale image groups with variable sizes. Furthermore, the inferred
structural representations from disentanglement learning are capable of
capturing the latent anatomy of the observations with visual semantics.
Extensive experiments were conducted to validate the proposed framework,
including four datasets from cardiac, brain and abdominal medical images. The
results have demonstrated the superiority of our method over conventional
similarity-based approaches in terms of accuracy, efficiency, scalability and
interpretability.
</p></li>
</ul>

<h3>Title: Path-based Explanation for Knowledge Graph Completion. (arXiv:2401.02290v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02290">http://arxiv.org/abs/2401.02290</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02290]] Path-based Explanation for Knowledge Graph Completion(http://arxiv.org/abs/2401.02290)</code></li>
<li>Summary: <p>Graph Neural Networks (GNNs) have achieved great success in Knowledge Graph
Completion (KGC) by modelling how entities and relations interact in recent
years. However, the explanation of the predicted facts has not caught the
necessary attention. Proper explanations for the results of GNN-based KGC
models increase model transparency and help researchers develop more reliable
models. Existing practices for explaining KGC tasks rely on
instance/subgraph-based approaches, while in some scenarios, paths can provide
more user-friendly and interpretable explanations. Nonetheless, the methods for
generating path-based explanations for KGs have not been well-explored. To
address this gap, we propose Power-Link, the first path-based KGC explainer
that explores GNN-based models. We design a novel simplified graph-powering
technique, which enables the generation of path-based explanations with a fully
parallelisable and memory-efficient training scheme. We further introduce three
new metrics for quantitative evaluation of the explanations, together with a
qualitative human evaluation. Extensive experiments demonstrate that Power-Link
outperforms the SOTA baselines in interpretability, efficiency, and
scalability.
</p></li>
</ul>

<h2>explainability</h2>
<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Can We Generate Realistic Hands Only Using Convolution?. (arXiv:2401.01951v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01951">http://arxiv.org/abs/2401.01951</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01951]] Can We Generate Realistic Hands Only Using Convolution?(http://arxiv.org/abs/2401.01951)</code></li>
<li>Summary: <p>The enduring inability of image generative models to recreate intricate
geometric features, such as those present in human hands and fingers has been
an ongoing problem in image generation for nearly a decade. While strides have
been made by increasing model sizes and diversifying training datasets, this
issue remains prevalent across all models, from denoising diffusion models to
Generative Adversarial Networks (GAN), pointing to a fundamental shortcoming in
the underlying architectures. In this paper, we demonstrate how this problem
can be mitigated by augmenting convolution layers geometric capabilities
through providing them with a single input channel incorporating the relative
$n$-dimensional Cartesian coordinate system. We show that this drastically
improves quality of hand and face images generated by GANs and Variational
AutoEncoders (VAE).
</p></li>
</ul>

<h3>Title: Instruct-Imagen: Image Generation with Multi-modal Instruction. (arXiv:2401.01952v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01952">http://arxiv.org/abs/2401.01952</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01952]] Instruct-Imagen: Image Generation with Multi-modal Instruction(http://arxiv.org/abs/2401.01952)</code></li>
<li>Summary: <p>This paper presents instruct-imagen, a model that tackles heterogeneous image
generation tasks and generalizes across unseen tasks. We introduce *multi-modal
instruction* for image generation, a task representation articulating a range
of generation intents with precision. It uses natural language to amalgamate
disparate modalities (e.g., text, edge, style, subject, etc.), such that
abundant generation intents can be standardized in a uniform format.
</p>
<p>We then build instruct-imagen by fine-tuning a pre-trained text-to-image
diffusion model with a two-stage framework. First, we adapt the model using the
retrieval-augmented training, to enhance model's capabilities to ground its
generation on external multimodal context. Subsequently, we fine-tune the
adapted model on diverse image generation tasks that requires vision-language
understanding (e.g., subject-driven generation, etc.), each paired with a
multi-modal instruction encapsulating the task's essence. Human evaluation on
various image generation datasets reveals that instruct-imagen matches or
surpasses prior task-specific models in-domain and demonstrates promising
generalization to unseen and more complex tasks.
</p></li>
</ul>

<h3>Title: Improving Diffusion-Based Image Synthesis with Context Prediction. (arXiv:2401.02015v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02015">http://arxiv.org/abs/2401.02015</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02015]] Improving Diffusion-Based Image Synthesis with Context Prediction(http://arxiv.org/abs/2401.02015)</code></li>
<li>Summary: <p>Diffusion models are a new class of generative models, and have dramatically
promoted image generation with unprecedented quality and diversity. Existing
diffusion models mainly try to reconstruct input image from a corrupted one
with a pixel-wise or feature-wise constraint along spatial axes. However, such
point-based reconstruction may fail to make each predicted pixel/feature fully
preserve its neighborhood context, impairing diffusion-based image synthesis.
As a powerful source of automatic supervisory signal, context has been well
studied for learning representations. Inspired by this, we for the first time
propose ConPreDiff to improve diffusion-based image synthesis with context
prediction. We explicitly reinforce each point to predict its neighborhood
context (i.e., multi-stride features/tokens/pixels) with a context decoder at
the end of diffusion denoising blocks in training stage, and remove the decoder
for inference. In this way, each point can better reconstruct itself by
preserving its semantic connections with neighborhood context. This new
paradigm of ConPreDiff can generalize to arbitrary discrete and continuous
diffusion backbones without introducing extra parameters in sampling procedure.
Extensive experiments are conducted on unconditional image generation,
text-to-image generation and image inpainting tasks. Our ConPreDiff
consistently outperforms previous methods and achieves a new SOTA text-to-image
generation results on MS-COCO, with a zero-shot FID score of 6.21.
</p></li>
</ul>

<h3>Title: DiffusionEdge: Diffusion Probabilistic Model for Crisp Edge Detection. (arXiv:2401.02032v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02032">http://arxiv.org/abs/2401.02032</a></li>
<li>Code URL: <a href="https://github.com/guhuangai/diffusionedge">https://github.com/guhuangai/diffusionedge</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02032]] DiffusionEdge: Diffusion Probabilistic Model for Crisp Edge Detection(http://arxiv.org/abs/2401.02032)</code></li>
<li>Summary: <p>Limited by the encoder-decoder architecture, learning-based edge detectors
usually have difficulty predicting edge maps that satisfy both correctness and
crispness. With the recent success of the diffusion probabilistic model (DPM),
we found it is especially suitable for accurate and crisp edge detection since
the denoising process is directly applied to the original image size.
Therefore, we propose the first diffusion model for the task of general edge
detection, which we call DiffusionEdge. To avoid expensive computational
resources while retaining the final performance, we apply DPM in the latent
space and enable the classic cross-entropy loss which is uncertainty-aware in
pixel level to directly optimize the parameters in latent space in a
distillation manner. We also adopt a decoupled architecture to speed up the
denoising process and propose a corresponding adaptive Fourier filter to adjust
the latent features of specific frequencies. With all the technical designs,
DiffusionEdge can be stably trained with limited resources, predicting crisp
and accurate edge maps with much fewer augmentation strategies. Extensive
experiments on four edge detection benchmarks demonstrate the superiority of
DiffusionEdge both in correctness and crispness. On the NYUDv2 dataset,
compared to the second best, we increase the ODS, OIS (without post-processing)
and AC by 30.2%, 28.1% and 65.1%, respectively. Code:
https://github.com/GuHuangAI/DiffusionEdge.
</p></li>
</ul>

<h3>Title: Preserving Image Properties Through Initializations in Diffusion Models. (arXiv:2401.02097v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02097">http://arxiv.org/abs/2401.02097</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02097]] Preserving Image Properties Through Initializations in Diffusion Models(http://arxiv.org/abs/2401.02097)</code></li>
<li>Summary: <p>Retail photography imposes specific requirements on images. For instance,
images may need uniform background colors, consistent model poses, centered
products, and consistent lighting. Minor deviations from these standards impact
a site's aesthetic appeal, making the images unsuitable for use. We show that
Stable Diffusion methods, as currently applied, do not respect these
requirements. The usual practice of training the denoiser with a very noisy
image and starting inference with a sample of pure noise leads to inconsistent
generated images during inference. This inconsistency occurs because it is easy
to tell the difference between samples of the training and inference
distributions. As a result, a network trained with centered retail product
images with uniform backgrounds generates images with erratic backgrounds. The
problem is easily fixed by initializing inference with samples from an
approximation of noisy images. However, in using such an approximation, the
joint distribution of text and noisy image at inference time still slightly
differs from that at training time. This discrepancy is corrected by training
the network with samples from the approximate noisy image distribution.
Extensive experiments on real application data show significant qualitative and
quantitative improvements in performance from adopting these procedures.
Finally, our procedure can interact well with other control-based methods to
further enhance the controllability of diffusion-based methods.
</p></li>
</ul>

<h3>Title: Unified Diffusion-Based Rigid and Non-Rigid Editing with Text and Image Guidance. (arXiv:2401.02126v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02126">http://arxiv.org/abs/2401.02126</a></li>
<li>Code URL: <a href="https://github.com/kihensarn/ti-guided-edit">https://github.com/kihensarn/ti-guided-edit</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02126]] Unified Diffusion-Based Rigid and Non-Rigid Editing with Text and Image Guidance(http://arxiv.org/abs/2401.02126)</code></li>
<li>Summary: <p>Existing text-to-image editing methods tend to excel either in rigid or
non-rigid editing but encounter challenges when combining both, resulting in
misaligned outputs with the provided text prompts. In addition, integrating
reference images for control remains challenging. To address these issues, we
present a versatile image editing framework capable of executing both rigid and
non-rigid edits, guided by either textual prompts or reference images. We
leverage a dual-path injection scheme to handle diverse editing scenarios and
introduce an integrated self-attention mechanism for fusion of appearance and
structural information. To mitigate potential visual artifacts, we further
employ latent fusion techniques to adjust intermediate latents. Compared to
previous work, our approach represents a significant advance in achieving
precise and versatile image editing. Comprehensive experiments validate the
efficacy of our method, showcasing competitive or superior results in
text-based editing and appearance transfer tasks, encompassing both rigid and
non-rigid settings.
</p></li>
</ul>

<h3>Title: GUESS:GradUally Enriching SyntheSis for Text-Driven Human Motion Generation. (arXiv:2401.02142v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02142">http://arxiv.org/abs/2401.02142</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02142]] GUESS:GradUally Enriching SyntheSis for Text-Driven Human Motion Generation(http://arxiv.org/abs/2401.02142)</code></li>
<li>Summary: <p>In this paper, we propose a novel cascaded diffusion-based generative
framework for text-driven human motion synthesis, which exploits a strategy
named GradUally Enriching SyntheSis (GUESS as its abbreviation). The strategy
sets up generation objectives by grouping body joints of detailed skeletons in
close semantic proximity together and then replacing each of such joint group
with a single body-part node. Such an operation recursively abstracts a human
pose to coarser and coarser skeletons at multiple granularity levels. With
gradually increasing the abstraction level, human motion becomes more and more
concise and stable, significantly benefiting the cross-modal motion synthesis
task. The whole text-driven human motion synthesis problem is then divided into
multiple abstraction levels and solved with a multi-stage generation framework
with a cascaded latent diffusion model: an initial generator first generates
the coarsest human motion guess from a given text description; then, a series
of successive generators gradually enrich the motion details based on the
textual description and the previous synthesized results. Notably, we further
integrate GUESS with the proposed dynamic multi-condition fusion mechanism to
dynamically balance the cooperative effects of the given textual condition and
synthesized coarse motion prompt in different generation stages. Extensive
experiments on large-scale datasets verify that GUESS outperforms existing
state-of-the-art methods by large margins in terms of accuracy, realisticness,
and diversity. Code is available at https://github.com/Xuehao-Gao/GUESS.
</p></li>
</ul>

<h3>Title: Bring Metric Functions into Diffusion Models. (arXiv:2401.02414v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02414">http://arxiv.org/abs/2401.02414</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02414]] Bring Metric Functions into Diffusion Models(http://arxiv.org/abs/2401.02414)</code></li>
<li>Summary: <p>We introduce a Cascaded Diffusion Model (Cas-DM) that improves a Denoising
Diffusion Probabilistic Model (DDPM) by effectively incorporating additional
metric functions in training. Metric functions such as the LPIPS loss have been
proven highly effective in consistency models derived from the score matching.
However, for the diffusion counterparts, the methodology and efficacy of adding
extra metric functions remain unclear. One major challenge is the mismatch
between the noise predicted by a DDPM at each step and the desired clean image
that the metric function works well on. To address this problem, we propose
Cas-DM, a network architecture that cascades two network modules to effectively
apply metric functions to the diffusion model training. The first module,
similar to a standard DDPM, learns to predict the added noise and is unaffected
by the metric function. The second cascaded module learns to predict the clean
image, thereby facilitating the metric function computation. Experiment results
show that the proposed diffusion model backbone enables the effective use of
the LPIPS loss, leading to state-of-the-art image quality (FID, sFID, IS) on
various established benchmarks.
</p></li>
</ul>

<h3>Title: Energy based diffusion generator for efficient sampling of Boltzmann distributions. (arXiv:2401.02080v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02080">http://arxiv.org/abs/2401.02080</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02080]] Energy based diffusion generator for efficient sampling of Boltzmann distributions(http://arxiv.org/abs/2401.02080)</code></li>
<li>Summary: <p>We introduce a novel sampler called the energy based diffusion generator for
generating samples from arbitrary target distributions. The sampling model
employs a structure similar to a variational autoencoder, utilizing a decoder
to transform latent variables from a simple distribution into random variables
approximating the target distribution, and we design an encoder based on the
diffusion model. Leveraging the powerful modeling capacity of the diffusion
model for complex distributions, we can obtain an accurate variational estimate
of the Kullback-Leibler divergence between the distributions of the generated
samples and the target. Moreover, we propose a decoder based on generalized
Hamiltonian dynamics to further enhance sampling performance. Through empirical
evaluation, we demonstrate the effectiveness of our method across various
complex distribution functions, showcasing its superiority compared to existing
methods.
</p></li>
</ul>

<h3>Title: Integration of physics-informed operator learning and finite element method for parametric learning of partial differential equations. (arXiv:2401.02363v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02363">http://arxiv.org/abs/2401.02363</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02363]] Integration of physics-informed operator learning and finite element method for parametric learning of partial differential equations(http://arxiv.org/abs/2401.02363)</code></li>
<li>Summary: <p>We present a method that employs physics-informed deep learning techniques
for parametrically solving partial differential equations. The focus is on the
steady-state heat equations within heterogeneous solids exhibiting significant
phase contrast. Similar equations manifest in diverse applications like
chemical diffusion, electrostatics, and Darcy flow. The neural network aims to
establish the link between the complex thermal conductivity profiles and
temperature distributions, as well as heat flux components within the
microstructure, under fixed boundary conditions. A distinctive aspect is our
independence from classical solvers like finite element methods for data. A
noteworthy contribution lies in our novel approach to defining the loss
function, based on the discretized weak form of the governing equation. This
not only reduces the required order of derivatives but also eliminates the need
for automatic differentiation in the construction of loss terms, accepting
potential numerical errors from the chosen discretization method. As a result,
the loss function in this work is an algebraic equation that significantly
enhances training efficiency. We benchmark our methodology against the standard
finite element method, demonstrating accurate yet faster predictions using the
trained neural network for temperature and flux profiles. We also show higher
accuracy by using the proposed method compared to purely data-driven approaches
for unforeseen scenarios.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Shrinking Your TimeStep: Towards Low-Latency Neuromorphic Object Recognition with Spiking Neural Network. (arXiv:2401.01912v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01912">http://arxiv.org/abs/2401.01912</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01912]] Shrinking Your TimeStep: Towards Low-Latency Neuromorphic Object Recognition with Spiking Neural Network(http://arxiv.org/abs/2401.01912)</code></li>
<li>Summary: <p>Neuromorphic object recognition with spiking neural networks (SNNs) is the
cornerstone of low-power neuromorphic computing. However, existing SNNs suffer
from significant latency, utilizing 10 to 40 timesteps or more, to recognize
neuromorphic objects. At low latencies, the performance of existing SNNs is
drastically degraded. In this work, we propose the Shrinking SNN (SSNN) to
achieve low-latency neuromorphic object recognition without reducing
performance. Concretely, we alleviate the temporal redundancy in SNNs by
dividing SNNs into multiple stages with progressively shrinking timesteps,
which significantly reduces the inference latency. During timestep shrinkage,
the temporal transformer smoothly transforms the temporal scale and preserves
the information maximally. Moreover, we add multiple early classifiers to the
SNN during training to mitigate the mismatch between the surrogate gradient and
the true gradient, as well as the gradient vanishing/exploding, thus
eliminating the performance degradation at low latency. Extensive experiments
on neuromorphic datasets, CIFAR10-DVS, N-Caltech101, and DVS-Gesture have
revealed that SSNN is able to improve the baseline accuracy by 6.55% ~ 21.41%.
With only 5 average timesteps and without any data augmentation, SSNN is able
to achieve an accuracy of 73.63% on CIFAR10-DVS. This work presents a
heterogeneous temporal scale SNN and provides valuable insights into the
development of high-performance, low-latency SNNs.
</p></li>
</ul>

<h3>Title: Exploring Boundary of GPT-4V on Marine Analysis: A Preliminary Case Study. (arXiv:2401.02147v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02147">http://arxiv.org/abs/2401.02147</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02147]] Exploring Boundary of GPT-4V on Marine Analysis: A Preliminary Case Study(http://arxiv.org/abs/2401.02147)</code></li>
<li>Summary: <p>Large language models (LLMs) have demonstrated a powerful ability to answer
various queries as a general-purpose assistant. The continuous multi-modal
large language models (MLLM) empower LLMs with the ability to perceive visual
signals. The launch of GPT-4 (Generative Pre-trained Transformers) has
generated significant interest in the research communities. GPT-4V(ison) has
demonstrated significant power in both academia and industry fields, as a focal
point in a new artificial intelligence generation. Though significant success
was achieved by GPT-4V, exploring MLLMs in domain-specific analysis (e.g.,
marine analysis) that required domain-specific knowledge and expertise has
gained less attention. In this study, we carry out the preliminary and
comprehensive case study of utilizing GPT-4V for marine analysis. This report
conducts a systematic evaluation of existing GPT-4V, assessing the performance
of GPT-4V on marine research and also setting a new standard for future
developments in MLLMs. The experimental results of GPT-4V show that the
responses generated by GPT-4V are still far away from satisfying the
domain-specific requirements of the marine professions. All images and prompts
used in this study will be available at
https://github.com/hkust-vgd/Marine_GPT-4V_Eval
</p></li>
</ul>

<h3>Title: Slot-guided Volumetric Object Radiance Fields. (arXiv:2401.02241v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02241">http://arxiv.org/abs/2401.02241</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02241]] Slot-guided Volumetric Object Radiance Fields(http://arxiv.org/abs/2401.02241)</code></li>
<li>Summary: <p>We present a novel framework for 3D object-centric representation learning.
Our approach effectively decomposes complex scenes into individual objects from
a single image in an unsupervised fashion. This method, called slot-guided
Volumetric Object Radiance Fields (sVORF), composes volumetric object radiance
fields with object slots as a guidance to implement unsupervised 3D scene
decomposition. Specifically, sVORF obtains object slots from a single image via
a transformer module, maps these slots to volumetric object radiance fields
with a hypernetwork and composes object radiance fields with the guidance of
object slots at a 3D location. Moreover, sVORF significantly reduces memory
requirement due to small-sized pixel rendering during training. We demonstrate
the effectiveness of our approach by showing top results in scene decomposition
and generation tasks of complex synthetic datasets (e.g., Room-Diverse).
Furthermore, we also confirm the potential of sVORF to segment objects in
real-world scenes (e.g., the LLFF dataset). We hope our approach can provide
preliminary understanding of the physical world and help ease future research
in 3D object-centric representation learning.
</p></li>
</ul>

<h3>Title: GridFormer: Point-Grid Transformer for Surface Reconstruction. (arXiv:2401.02292v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02292">http://arxiv.org/abs/2401.02292</a></li>
<li>Code URL: <a href="https://github.com/list17/gridformer">https://github.com/list17/gridformer</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02292]] GridFormer: Point-Grid Transformer for Surface Reconstruction(http://arxiv.org/abs/2401.02292)</code></li>
<li>Summary: <p>Implicit neural networks have emerged as a crucial technology in 3D surface
reconstruction. To reconstruct continuous surfaces from discrete point clouds,
encoding the input points into regular grid features (plane or volume) has been
commonly employed in existing approaches. However, these methods typically use
the grid as an index for uniformly scattering point features. Compared with the
irregular point features, the regular grid features may sacrifice some
reconstruction details but improve efficiency. To take full advantage of these
two types of features, we introduce a novel and high-efficiency attention
mechanism between the grid and point features named Point-Grid Transformer
(GridFormer). This mechanism treats the grid as a transfer point connecting the
space and point cloud. Our method maximizes the spatial expressiveness of grid
features and maintains computational efficiency. Furthermore, optimizing
predictions over the entire space could potentially result in blurred
boundaries. To address this issue, we further propose a boundary optimization
strategy incorporating margin binary cross-entropy loss and boundary sampling.
This approach enables us to achieve a more precise representation of the object
structure. Our experiments validate that our method is effective and
outperforms the state-of-the-art approaches under widely used benchmarks by
producing more precise geometry reconstructions. The code is available at
https://github.com/list17/GridFormer.
</p></li>
</ul>

<h3>Title: TR-DETR: Task-Reciprocal Transformer for Joint Moment Retrieval and Highlight Detection. (arXiv:2401.02309v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02309">http://arxiv.org/abs/2401.02309</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02309]] TR-DETR: Task-Reciprocal Transformer for Joint Moment Retrieval and Highlight Detection(http://arxiv.org/abs/2401.02309)</code></li>
<li>Summary: <p>Video moment retrieval (MR) and highlight detection (HD) based on natural
language queries are two highly related tasks, which aim to obtain relevant
moments within videos and highlight scores of each video clip. Recently,
several methods have been devoted to building DETR-based networks to solve both
MR and HD jointly. These methods simply add two separate task heads after
multi-modal feature extraction and feature interaction, achieving good
performance. Nevertheless, these approaches underutilize the reciprocal
relationship between two tasks. In this paper, we propose a task-reciprocal
transformer based on DETR (TR-DETR) that focuses on exploring the inherent
reciprocity between MR and HD. Specifically, a local-global multi-modal
alignment module is first built to align features from diverse modalities into
a shared latent space. Subsequently, a visual feature refinement is designed to
eliminate query-irrelevant information from visual features for modal
interaction. Finally, a task cooperation module is constructed to refine the
retrieval pipeline and the highlight score prediction process by utilizing the
reciprocity between MR and HD. Comprehensive experiments on QVHighlights,
Charades-STA and TVSum datasets demonstrate that TR-DETR outperforms existing
state-of-the-art methods. Codes are available at
\url{https://github.com/mingyao1120/TR-DETR}.
</p></li>
</ul>

<h3>Title: ODIN: A Single Model for 2D and 3D Perception. (arXiv:2401.02416v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02416">http://arxiv.org/abs/2401.02416</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02416]] ODIN: A Single Model for 2D and 3D Perception(http://arxiv.org/abs/2401.02416)</code></li>
<li>Summary: <p>State-of-the-art models on contemporary 3D perception benchmarks like ScanNet
consume and label dataset-provided 3D point clouds, obtained through post
processing of sensed multiview RGB-D images. They are typically trained
in-domain, forego large-scale 2D pre-training and outperform alternatives that
featurize the posed RGB-D multiview images instead. The gap in performance
between methods that consume posed images versus post-processed 3D point clouds
has fueled the belief that 2D and 3D perception require distinct model
architectures. In this paper, we challenge this view and propose ODIN
(Omni-Dimensional INstance segmentation), a model that can segment and label
both 2D RGB images and 3D point clouds, using a transformer architecture that
alternates between 2D within-view and 3D cross-view information fusion. Our
model differentiates 2D and 3D feature operations through the positional
encodings of the tokens involved, which capture pixel coordinates for 2D patch
tokens and 3D coordinates for 3D feature tokens. ODIN achieves state-of-the-art
performance on ScanNet200, Matterport3D and AI2THOR 3D instance segmentation
benchmarks, and competitive performance on ScanNet, S3DIS and COCO. It
outperforms all previous works by a wide margin when the sensed 3D point cloud
is used in place of the point cloud sampled from 3D mesh. When used as the 3D
perception engine in an instructable embodied agent architecture, it sets a new
state-of-the-art on the TEACh action-from-dialogue benchmark. Our code and
checkpoints can be found at the project website: https://odin-seg.github.io.
</p></li>
</ul>

<h3>Title: ICE-GRT: Instruction Context Enhancement by Generative Reinforcement based Transformers. (arXiv:2401.02072v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02072">http://arxiv.org/abs/2401.02072</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02072]] ICE-GRT: Instruction Context Enhancement by Generative Reinforcement based Transformers(http://arxiv.org/abs/2401.02072)</code></li>
<li>Summary: <p>The emergence of Large Language Models (LLMs) such as ChatGPT and LLaMA
encounter limitations in domain-specific tasks, with these models often lacking
depth and accuracy in specialized areas, and exhibiting a decrease in general
capabilities when fine-tuned, particularly analysis ability in small sized
models. To address these gaps, we introduce ICE-GRT, utilizing Reinforcement
Learning from Human Feedback (RLHF) grounded in Proximal Policy Optimization
(PPO), demonstrating remarkable ability in in-domain scenarios without
compromising general task performance. Our exploration of ICE-GRT highlights
its understanding and reasoning ability to not only generate robust answers but
also to provide detailed analyses of the reasons behind the answer. This
capability marks a significant progression beyond the scope of Supervised
Fine-Tuning models. The success of ICE-GRT is dependent on several crucial
factors, including Appropriate Data, Reward Size Scaling, KL-Control, Advantage
Normalization, etc. The ICE-GRT model exhibits state-of-the-art performance in
domain-specific tasks and across 12 general Language tasks against equivalent
size and even larger size LLMs, highlighting the effectiveness of our approach.
We provide a comprehensive analysis of the ICE-GRT, underscoring the
significant advancements it brings to the field of LLM.
</p></li>
</ul>

<h3>Title: Re-evaluating the Memory-balanced Pipeline Parallelism: BPipe. (arXiv:2401.02088v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02088">http://arxiv.org/abs/2401.02088</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02088]] Re-evaluating the Memory-balanced Pipeline Parallelism: BPipe(http://arxiv.org/abs/2401.02088)</code></li>
<li>Summary: <p>Pipeline parallelism is an essential technique in the training of large-scale
Transformer models. However, it suffers from imbalanced memory consumption,
leading to insufficient memory utilization. The BPipe technique was proposed to
address this issue and has proven effective in the GPT-3 model. Nevertheless,
our experiments have not yielded similar benefits for LLaMA training.
Additionally, BPipe only yields negligible benefits for GPT-3 training when
applying flash attention. We analyze the underlying causes of the divergent
performance of BPipe on GPT-3 and LLaMA. Furthermore, we introduce a novel
method to estimate the performance of BPipe.
</p></li>
</ul>

<h3>Title: Shayona@SMM4H23: COVID-19 Self diagnosis classification using BERT and LightGBM models. (arXiv:2401.02158v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02158">http://arxiv.org/abs/2401.02158</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02158]] Shayona@SMM4H23: COVID-19 Self diagnosis classification using BERT and LightGBM models(http://arxiv.org/abs/2401.02158)</code></li>
<li>Summary: <p>This paper describes approaches and results for shared Task 1 and 4 of
SMMH4-23 by Team Shayona. Shared Task-1 was binary classification of english
tweets self-reporting a COVID-19 diagnosis, and Shared Task-4 was Binary
classification of English Reddit posts self-reporting a social anxiety disorder
diagnosis. Our team has achieved the highest f1-score 0.94 in Task-1 among all
participants. We have leveraged the Transformer model (BERT) in combination
with the LightGBM model for both tasks.
</p></li>
</ul>

<h3>Title: LLaMA Pro: Progressive LLaMA with Block Expansion. (arXiv:2401.02415v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02415">http://arxiv.org/abs/2401.02415</a></li>
<li>Code URL: <a href="https://github.com/tencentarc/llama-pro">https://github.com/tencentarc/llama-pro</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02415]] LLaMA Pro: Progressive LLaMA with Block Expansion(http://arxiv.org/abs/2401.02415)</code></li>
<li>Summary: <p>Humans generally acquire new skills without compromising the old; however,
the opposite holds for Large Language Models (LLMs), e.g., from LLaMA to
CodeLLaMA. To this end, we propose a new post-pretraining method for LLMs with
an expansion of Transformer blocks. We tune the expanded blocks using only new
corpus, efficiently and effectively improving the model's knowledge without
catastrophic forgetting. In this paper, we experiment on the corpus of code and
math, yielding LLaMA Pro-8.3B, a versatile foundation model initialized from
LLaMA2-7B, excelling in general tasks, programming, and mathematics. LLaMA Pro
and its instruction-following counterpart (LLaMA Pro-Instruct) achieve advanced
performance among various benchmarks, demonstrating superiority over existing
open models in the LLaMA family and the immense potential of reasoning and
addressing diverse tasks as an intelligent agent. Our findings provide valuable
insights into integrating natural and programming languages, laying a solid
foundation for developing advanced language agents that operate effectively in
various environments.
</p></li>
</ul>

<h3>Title: Multi-Source Domain Adaptation with Transformer-based Feature Generation for Subject-Independent EEG-based Emotion Recognition. (arXiv:2401.02344v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02344">http://arxiv.org/abs/2401.02344</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02344]] Multi-Source Domain Adaptation with Transformer-based Feature Generation for Subject-Independent EEG-based Emotion Recognition(http://arxiv.org/abs/2401.02344)</code></li>
<li>Summary: <p>Although deep learning-based algorithms have demonstrated excellent
performance in automated emotion recognition via electroencephalogram (EEG)
signals, variations across brain signal patterns of individuals can diminish
the model's effectiveness when applied across different subjects. While
transfer learning techniques have exhibited promising outcomes, they still
encounter challenges related to inadequate feature representations and may
overlook the fact that source subjects themselves can possess distinct
characteristics. In this work, we propose a multi-source domain adaptation
approach with a transformer-based feature generator (MSDA-TF) designed to
leverage information from multiple sources. The proposed feature generator
retains convolutional layers to capture shallow spatial, temporal, and spectral
EEG data representations, while self-attention mechanisms extract global
dependencies within these features. During the adaptation process, we group the
source subjects based on correlation values and aim to align the moments of the
target subject with each source as well as within the sources. MSDA-TF is
validated on the SEED dataset and is shown to yield promising results.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Unsupervised Object-Centric Learning from Multiple Unspecified Viewpoints. (arXiv:2401.01922v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01922">http://arxiv.org/abs/2401.01922</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01922]] Unsupervised Object-Centric Learning from Multiple Unspecified Viewpoints(http://arxiv.org/abs/2401.01922)</code></li>
<li>Summary: <p>Visual scenes are extremely diverse, not only because there are infinite
possible combinations of objects and backgrounds but also because the
observations of the same scene may vary greatly with the change of viewpoints.
When observing a multi-object visual scene from multiple viewpoints, humans can
perceive the scene compositionally from each viewpoint while achieving the
so-called ``object constancy'' across different viewpoints, even though the
exact viewpoints are untold. This ability is essential for humans to identify
the same object while moving and to learn from vision efficiently. It is
intriguing to design models that have a similar ability. In this paper, we
consider a novel problem of learning compositional scene representations from
multiple unspecified (i.e., unknown and unrelated) viewpoints without using any
supervision and propose a deep generative model which separates latent
representations into a viewpoint-independent part and a viewpoint-dependent
part to solve this problem. During the inference, latent representations are
randomly initialized and iteratively updated by integrating the information in
different viewpoints with neural networks. Experiments on several specifically
designed synthetic datasets have shown that the proposed method can effectively
learn from multiple unspecified viewpoints.
</p></li>
</ul>

<h3>Title: Linguistic Profiling of Deepfakes: An Open Database for Next-Generation Deepfake Detection. (arXiv:2401.02335v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02335">http://arxiv.org/abs/2401.02335</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02335]] Linguistic Profiling of Deepfakes: An Open Database for Next-Generation Deepfake Detection(http://arxiv.org/abs/2401.02335)</code></li>
<li>Summary: <p>The emergence of text-to-image generative models has revolutionized the field
of deepfakes, enabling the creation of realistic and convincing visual content
directly from textual descriptions. However, this advancement presents
considerably greater challenges in detecting the authenticity of such content.
Existing deepfake detection datasets and methods often fall short in
effectively capturing the extensive range of emerging deepfakes and offering
satisfactory explanatory information for detection. To address the significant
issue, this paper introduces a deepfake database (DFLIP-3K) for the development
of convincing and explainable deepfake detection. It encompasses about 300K
diverse deepfake samples from approximately 3K generative models, which boasts
the largest number of deepfake models in the literature. Moreover, it collects
around 190K linguistic footprints of these deepfakes. The two distinguished
features enable DFLIP-3K to develop a benchmark that promotes progress in
linguistic profiling of deepfakes, which includes three sub-tasks namely
deepfake detection, model identification, and prompt prediction. The deepfake
model and prompt are two essential components of each deepfake, and thus
dissecting them linguistically allows for an invaluable exploration of
trustworthy and interpretable evidence in deepfake detection, which we believe
is the key for the next-generation deepfake detection. Furthermore, DFLIP-3K is
envisioned as an open database that fosters transparency and encourages
collaborative efforts to further enhance its growth. Our extensive experiments
on the developed benchmark verify that our DFLIP-3K database is capable of
serving as a standardized resource for evaluating and comparing
linguistic-based deepfake detection, identification, and prompt prediction
techniques.
</p></li>
</ul>

<h3>Title: What You See is What You GAN: Rendering Every Pixel for High-Fidelity Geometry in 3D GANs. (arXiv:2401.02411v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02411">http://arxiv.org/abs/2401.02411</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02411]] What You See is What You GAN: Rendering Every Pixel for High-Fidelity Geometry in 3D GANs(http://arxiv.org/abs/2401.02411)</code></li>
<li>Summary: <p>3D-aware Generative Adversarial Networks (GANs) have shown remarkable
progress in learning to generate multi-view-consistent images and 3D geometries
of scenes from collections of 2D images via neural volume rendering. Yet, the
significant memory and computational costs of dense sampling in volume
rendering have forced 3D GANs to adopt patch-based training or employ
low-resolution rendering with post-processing 2D super resolution, which
sacrifices multiview consistency and the quality of resolved geometry.
Consequently, 3D GANs have not yet been able to fully resolve the rich 3D
geometry present in 2D images. In this work, we propose techniques to scale
neural volume rendering to the much higher resolution of native 2D images,
thereby resolving fine-grained 3D geometry with unprecedented detail. Our
approach employs learning-based samplers for accelerating neural rendering for
3D GAN training using up to 5 times fewer depth samples. This enables us to
explicitly "render every pixel" of the full-resolution image during training
and inference without post-processing superresolution in 2D. Together with our
strategy to learn high-quality surface geometry, our method synthesizes
high-resolution 3D geometry and strictly view-consistent images while
maintaining image quality on par with baselines relying on post-processing
super resolution. We demonstrate state-of-the-art 3D gemetric quality on FFHQ
and AFHQ, setting a new standard for unsupervised learning of 3D shapes in 3D
GANs.
</p></li>
</ul>

<h3>Title: From Function to Distribution Modeling: A PAC-Generative Approach to Offline Optimization. (arXiv:2401.02019v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02019">http://arxiv.org/abs/2401.02019</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02019]] From Function to Distribution Modeling: A PAC-Generative Approach to Offline Optimization(http://arxiv.org/abs/2401.02019)</code></li>
<li>Summary: <p>This paper considers the problem of offline optimization, where the objective
function is unknown except for a collection of ``offline" data examples. While
recent years have seen a flurry of work on applying various machine learning
techniques to the offline optimization problem, the majority of these work
focused on learning a surrogate of the unknown objective function and then
applying existing optimization algorithms. While the idea of modeling the
unknown objective function is intuitive and appealing, from the learning point
of view it also makes it very difficult to tune the objective of the learner
according to the objective of optimization. Instead of learning and then
optimizing the unknown objective function, in this paper we take on a less
intuitive but more direct view that optimization can be thought of as a process
of sampling from a generative model. To learn an effective generative model
from the offline data examples, we consider the standard technique of
``re-weighting", and our main technical contribution is a probably
approximately correct (PAC) lower bound on the natural optimization objective,
which allows us to jointly learn a weight function and a score-based generative
model. The robustly competitive performance of the proposed approach is
demonstrated via empirical studies using the standard offline optimization
benchmarks.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Learning to Prompt with Text Only Supervision for Vision-Language Models. (arXiv:2401.02418v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02418">http://arxiv.org/abs/2401.02418</a></li>
<li>Code URL: <a href="https://github.com/muzairkhattak/protext">https://github.com/muzairkhattak/protext</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02418]] Learning to Prompt with Text Only Supervision for Vision-Language Models(http://arxiv.org/abs/2401.02418)</code></li>
<li>Summary: <p>Foundational vision-language models such as CLIP are becoming a new paradigm
in vision, due to their excellent generalization abilities. However, adapting
these models for downstream tasks while maintaining their generalization
remains a challenge. In literature, one branch of methods adapts CLIP by
learning prompts using visual information. While effective, most of these works
require labeled data which is not practical, and often struggle to generalize
towards new datasets due to over-fitting on the source data. An alternative
approach resorts to training-free methods by generating class descriptions from
large language models (LLMs) and perform prompt ensembling. However, these
methods often generate class specific prompts that cannot be transferred to
other classes, which incur higher costs by generating LLM descriptions for each
class separately. In this work, we propose to combine the strengths of these
both streams of methods by learning prompts using only text data derived from
LLMs. As supervised training of prompts is not trivial due to absence of
images, we develop a training approach that allows prompts to extract rich
contextual knowledge from LLM data. Moreover, with LLM contextual data mapped
within the learned prompts, it enables zero-shot transfer of prompts to new
classes and datasets potentially cutting the LLM prompt engineering cost. To
the best of our knowledge, this is the first work that learns generalized
prompts using text only data. We perform extensive evaluations on 4 benchmarks
where our method improves over prior ensembling works while being competitive
to those utilizing labeled images. Our code and pre-trained models are
available at https://github.com/muzairkhattak/ProText.
</p></li>
</ul>

<h3>Title: Revisiting Zero-Shot Abstractive Summarization in the Era of Large Language Models from the Perspective of Position Bias. (arXiv:2401.01989v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.01989">http://arxiv.org/abs/2401.01989</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.01989]] Revisiting Zero-Shot Abstractive Summarization in the Era of Large Language Models from the Perspective of Position Bias(http://arxiv.org/abs/2401.01989)</code></li>
<li>Summary: <p>We characterize and study zero-shot abstractive summarization in Large
Language Models (LLMs) by measuring position bias, which we propose as a
general formulation of the more restrictive lead bias phenomenon studied
previously in the literature. Position bias captures the tendency of a model
unfairly prioritizing information from certain parts of the input text over
others, leading to undesirable behavior. Through numerous experiments on four
diverse real-world datasets, we study position bias in multiple LLM models such
as GPT 3.5-Turbo, Llama-2, and Dolly-v2, as well as state-of-the-art pretrained
encoder-decoder abstractive summarization models such as Pegasus and BART. Our
findings lead to novel insights and discussion on performance and position bias
of models for zero-shot summarization tasks.
</p></li>
</ul>

<h3>Title: Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives. (arXiv:2401.02009v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02009">http://arxiv.org/abs/2401.02009</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02009]] Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives(http://arxiv.org/abs/2401.02009)</code></li>
<li>Summary: <p>The reflection capacity of Large Language Model (LLM) has garnered extensive
attention. A post-hoc prompting strategy, e.g., reflexion and self-refine,
refines LLM's response based on self-evaluated or external feedback. However,
recent research indicates without external feedback, LLM's intrinsic reflection
is unstable. Our investigation unveils that the key bottleneck is the quality
of the self-evaluated feedback. We find LLMs often exhibit overconfidence or
high randomness when self-evaluate, offering stubborn or inconsistent feedback,
which causes poor reflection. To remedy this, we advocate Self-Contrast: It
adaptively explores diverse solving perspectives tailored to the request,
contrasts the differences, and summarizes these discrepancies into a checklist
which could be used to re-examine and eliminate discrepancies. Our method
endows LLM with diverse perspectives to alleviate stubborn biases. Moreover,
their discrepancies indicate potential errors or inherent uncertainties that
LLM often overlooks. Reflecting upon these can catalyze more accurate and
stable reflection. Experiments conducted on a series of reasoning and
translation tasks with different LLMs serve to underscore the effectiveness and
generality of our strategy.
</p></li>
</ul>

<h3>Title: Understanding LLMs: A Comprehensive Overview from Training to Inference. (arXiv:2401.02038v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02038">http://arxiv.org/abs/2401.02038</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02038]] Understanding LLMs: A Comprehensive Overview from Training to Inference(http://arxiv.org/abs/2401.02038)</code></li>
<li>Summary: <p>The introduction of ChatGPT has led to a significant increase in the
utilization of Large Language Models (LLMs) for addressing downstream tasks.
There's an increasing focus on cost-efficient training and deployment within
this context. Low-cost training and deployment of LLMs represent the future
development trend. This paper reviews the evolution of large language model
training techniques and inference deployment technologies aligned with this
emerging trend. The discussion on training includes various aspects, including
data preprocessing, training architecture, pre-training tasks, parallel
training, and relevant content related to model fine-tuning. On the inference
side, the paper covers topics such as model compression, parallel computation,
memory scheduling, and structural optimization. It also explores LLMs'
utilization and provides insights into their future development.
</p></li>
</ul>

<h3>Title: DCR-Consistency: Divide-Conquer-Reasoning for Consistency Evaluation and Improvement of Large Language Models. (arXiv:2401.02132v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02132">http://arxiv.org/abs/2401.02132</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02132]] DCR-Consistency: Divide-Conquer-Reasoning for Consistency Evaluation and Improvement of Large Language Models(http://arxiv.org/abs/2401.02132)</code></li>
<li>Summary: <p>Evaluating the quality and variability of text generated by Large Language
Models (LLMs) poses a significant, yet unresolved research challenge.
Traditional evaluation methods, such as ROUGE and BERTScore, which measure
token similarity, often fail to capture the holistic semantic equivalence. This
results in a low correlation with human judgments and intuition, which is
especially problematic in high-stakes applications like healthcare and finance
where reliability, safety, and robust decision-making are highly critical. This
work proposes DCR, an automated framework for evaluating and improving the
consistency of LLM-generated texts using a divide-conquer-reasoning approach.
Unlike existing LLM-based evaluators that operate at the paragraph level, our
method employs a divide-and-conquer evaluator (DCE) that breaks down the
paragraph-to-paragraph comparison between two generated responses into
individual sentence-to-paragraph comparisons, each evaluated based on
predefined criteria. To facilitate this approach, we introduce an automatic
metric converter (AMC) that translates the output from DCE into an
interpretable numeric score. Beyond the consistency evaluation, we further
present a reason-assisted improver (RAI) that leverages the analytical reasons
with explanations identified by DCE to generate new responses aimed at reducing
these inconsistencies. Through comprehensive and systematic empirical analysis,
we show that our approach outperforms state-of-the-art methods by a large
margin (e.g., +19.3% and +24.3% on the SummEval dataset) in evaluating the
consistency of LLM generation across multiple benchmarks in semantic, factual,
and summarization consistency tasks. Our approach also substantially reduces
nearly 90% of output inconsistencies, showing promise for effective
hallucination mitigation.
</p></li>
</ul>

<h3>Title: DIALIGHT: Lightweight Multilingual Development and Evaluation of Task-Oriented Dialogue Systems with Large Language Models. (arXiv:2401.02208v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02208">http://arxiv.org/abs/2401.02208</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02208]] DIALIGHT: Lightweight Multilingual Development and Evaluation of Task-Oriented Dialogue Systems with Large Language Models(http://arxiv.org/abs/2401.02208)</code></li>
<li>Summary: <p>We present DIALIGHT, a toolkit for developing and evaluating multilingual
Task-Oriented Dialogue (ToD) systems which facilitates systematic evaluations
and comparisons between ToD systems using fine-tuning of Pretrained Language
Models (PLMs) and those utilising the zero-shot and in-context learning
capabilities of Large Language Models (LLMs). In addition to automatic
evaluation, this toolkit features (i) a secure, user-friendly web interface for
fine-grained human evaluation at both local utterance level and global dialogue
level, and (ii) a microservice-based backend, improving efficiency and
scalability. Our evaluations reveal that while PLM fine-tuning leads to higher
accuracy and coherence, LLM-based systems excel in producing diverse and
likeable responses. However, we also identify significant challenges of LLMs in
adherence to task-specific instructions and generating outputs in multiple
languages, highlighting areas for future research. We hope this open-sourced
toolkit will serve as a valuable resource for researchers aiming to develop and
properly evaluate multilingual ToD systems and will lower, currently still
high, entry barriers in the field.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Leveraging SAM for Single-Source Domain Generalization in Medical Image Segmentation. (arXiv:2401.02076v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02076">http://arxiv.org/abs/2401.02076</a></li>
<li>Code URL: <a href="https://github.com/sarihust/sammed">https://github.com/sarihust/sammed</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02076]] Leveraging SAM for Single-Source Domain Generalization in Medical Image Segmentation(http://arxiv.org/abs/2401.02076)</code></li>
<li>Summary: <p>Domain Generalization (DG) aims to reduce domain shifts between domains to
achieve promising performance on the unseen target domain, which has been
widely practiced in medical image segmentation. Single-source domain
generalization (SDG) is the most challenging setting that trains on only one
source domain. Although existing methods have made considerable progress on SDG
of medical image segmentation, the performances are still far from the
applicable standards when faced with a relatively large domain shift. In this
paper, we leverage the Segment Anything Model (SAM) to SDG to greatly improve
the ability of generalization. Specifically, we introduce a parallel framework,
the source images are sent into the SAM module and normal segmentation module
respectively. To reduce the calculation resources, we apply a merging strategy
before sending images to the SAM module. We extract the bounding boxes from the
segmentation module and send the refined version as prompts to the SAM module.
We evaluate our model on a classic DG dataset and achieve competitive results
compared to other state-of-the-art DG methods. Furthermore, We conducted a
series of ablation experiments to prove the effectiveness of the proposed
method. The code is publicly available at https://github.com/SARIHUST/SAMMed.
</p></li>
</ul>

<h3>Title: Source-Free Online Domain Adaptive Semantic Segmentation of Satellite Images under Image Degradation. (arXiv:2401.02113v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02113">http://arxiv.org/abs/2401.02113</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02113]] Source-Free Online Domain Adaptive Semantic Segmentation of Satellite Images under Image Degradation(http://arxiv.org/abs/2401.02113)</code></li>
<li>Summary: <p>Online adaptation to distribution shifts in satellite image segmentation
stands as a crucial yet underexplored problem. In this paper, we address
source-free and online domain adaptation, i.e., test-time adaptation (TTA), for
satellite images, with the focus on mitigating distribution shifts caused by
various forms of image degradation. Towards achieving this goal, we propose a
novel TTA approach involving two effective strategies. First, we progressively
estimate the global Batch Normalization (BN) statistics of the target
distribution with incoming data stream. Leveraging these statistics during
inference has the ability to effectively reduce domain gap. Furthermore, we
enhance prediction quality by refining the predicted masks using global class
centers. Both strategies employ dynamic momentum for fast and stable
convergence. Notably, our method is backpropagation-free and hence fast and
lightweight, making it highly suitable for on-the-fly adaptation to new domain.
Through comprehensive experiments across various domain adaptation scenarios,
we demonstrate the robust performance of our method.
</p></li>
</ul>

<h3>Title: ClassWise-SAM-Adapter: Parameter Efficient Fine-tuning Adapts Segment Anything to SAR Domain for Semantic Segmentation. (arXiv:2401.02326v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02326">http://arxiv.org/abs/2401.02326</a></li>
<li>Code URL: <a href="https://github.com/xypu98/cwsam">https://github.com/xypu98/cwsam</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02326]] ClassWise-SAM-Adapter: Parameter Efficient Fine-tuning Adapts Segment Anything to SAR Domain for Semantic Segmentation(http://arxiv.org/abs/2401.02326)</code></li>
<li>Summary: <p>In the realm of artificial intelligence, the emergence of foundation models,
backed by high computing capabilities and extensive data, has been
revolutionary. Segment Anything Model (SAM), built on the Vision Transformer
(ViT) model with millions of parameters and vast training dataset SA-1B, excels
in various segmentation scenarios relying on its significance of semantic
information and generalization ability. Such achievement of visual foundation
model stimulates continuous researches on specific downstream tasks in computer
vision. The ClassWise-SAM-Adapter (CWSAM) is designed to adapt the
high-performing SAM for landcover classification on space-borne Synthetic
Aperture Radar (SAR) images. The proposed CWSAM freezes most of SAM's
parameters and incorporates lightweight adapters for parameter efficient
fine-tuning, and a classwise mask decoder is designed to achieve semantic
segmentation task. This adapt-tuning method allows for efficient landcover
classification of SAR images, balancing the accuracy with computational demand.
In addition, the task specific input module injects low frequency information
of SAR images by MLP-based layers to improve the model performance. Compared to
conventional state-of-the-art semantic segmentation algorithms by extensive
experiments, CWSAM showcases enhanced performance with fewer computing
resources, highlighting the potential of leveraging foundational models like
SAM for specific downstream tasks in the SAR domain. The source code is
available at: https://github.com/xypu98/CWSAM.
</p></li>
</ul>

<h3>Title: 3D Open-Vocabulary Panoptic Segmentation with 2D-3D Vision-Language Distillation. (arXiv:2401.02402v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2401.02402">http://arxiv.org/abs/2401.02402</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2401.02402]] 3D Open-Vocabulary Panoptic Segmentation with 2D-3D Vision-Language Distillation(http://arxiv.org/abs/2401.02402)</code></li>
<li>Summary: <p>3D panoptic segmentation is a challenging perception task, which aims to
predict both semantic and instance annotations for 3D points in a scene.
Although prior 3D panoptic segmentation approaches have achieved great
performance on closed-set benchmarks, generalizing to novel categories remains
an open problem. For unseen object categories, 2D open-vocabulary segmentation
has achieved promising results that solely rely on frozen CLIP backbones and
ensembling multiple classification outputs. However, we find that simply
extending these 2D models to 3D does not achieve good performance due to poor
per-mask classification quality on novel categories. In this paper, we propose
the first method to tackle 3D open-vocabulary panoptic segmentation. Our model
takes advantage of the fusion between learnable LiDAR features and dense frozen
vision CLIP features, using a single classification head to make predictions
for both base and novel classes. To further improve the classification
performance on novel classes and leverage the CLIP model, we propose two novel
loss functions: object-level distillation loss and voxel-level distillation
loss. Our experiments on the nuScenes and SemanticKITTI datasets show that our
method outperforms strong baselines by a large margin.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
