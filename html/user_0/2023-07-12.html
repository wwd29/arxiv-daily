<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Let's shake on it: Extracting secure shared keys from Wi-Fi CSI. (arXiv:2307.05423v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05423">http://arxiv.org/abs/2307.05423</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05423] Let's shake on it: Extracting secure shared keys from Wi-Fi CSI](http://arxiv.org/abs/2307.05423) #secure</code></li>
<li>Summary: <p>A shared secret key is necessary for encrypted communications. Since Wi-Fi
relies on OFDM, we suggest a method to generate such a key by utilizing Wi-Fi's
channel state information (CSI). CSI is typically reciprocal but very sensitive
to location: While the legitimate Alice and Bob observe the same CSI, an
eavesdropper Eve observes an uncorrelated CSI when positioned over 0.5
wavelength away. We show that if endpoint Bob is shaken, sufficient diversity
is induced in the CSI so that it can serve as a source of true randomness. Then
we show that the CSI among neighboring sub-carriers is correlated, so we select
a small set of judiciously-spaced sub-carriers, and use a majority rule around
each. We demonstrate that Alice and Bob observe a 5-15\% bit mismatch rate
(BMR) in the extracted bitstream while Eve observes a BMR of around 50\% even
when placed within 10cm of Alice. We employ the cryptography-oriented
definition of min-entropy to estimate the number of secure bits within the
bitstream, and use the Cascade algorithm of quantum-key-distribution to
reconcile Alice and Bob's bitstreams, while quantifying the number of bits
leaked by the algorithm. Accounting for both the min-entropy and the cascade
leakage we quantify the Secured Bit Generation Rate of our method.
</p></li>
</ul>

<p>We conducted extensive tests in an indoor environment. Our system exhibits a
secure bit generation rate of 1.2--1.6 %secure bits per packet, at distances
ranging from 0.5m--9m, and can generate a secure shared 128-bit key with 20sec
of device shaking.
</p>

<h2>security</h2>
<h3>Title: Disentangled Contrastive Image Translation for Nighttime Surveillance. (arXiv:2307.05038v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05038">http://arxiv.org/abs/2307.05038</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05038] Disentangled Contrastive Image Translation for Nighttime Surveillance](http://arxiv.org/abs/2307.05038) #security</code></li>
<li>Summary: <p>Nighttime surveillance suffers from degradation due to poor illumination and
arduous human annotations. It is challengable and remains a security risk at
night. Existing methods rely on multi-spectral images to perceive objects in
the dark, which are troubled by low resolution and color absence. We argue that
the ultimate solution for nighttime surveillance is night-to-day translation,
or Night2Day, which aims to translate a surveillance scene from nighttime to
the daytime while maintaining semantic consistency. To achieve this, this paper
presents a Disentangled Contrastive (DiCo) learning method. Specifically, to
address the poor and complex illumination in the nighttime scenes, we propose a
learnable physical prior, i.e., the color invariant, which provides a stable
perception of a highly dynamic night environment and can be incorporated into
the learning pipeline of neural networks. Targeting the surveillance scenes, we
develop a disentangled representation, which is an auxiliary pretext task that
separates surveillance scenes into the foreground and background with
contrastive learning. Such a strategy can extract the semantics without
supervision and boost our model to achieve instance-aware translation. Finally,
we incorporate all the modules above into generative adversarial networks and
achieve high-fidelity translation. This paper also contributes a new
surveillance dataset called NightSuR. It includes six scenes to support the
study on nighttime surveillance. This dataset collects nighttime images with
different properties of nighttime environments, such as flare and extreme
darkness. Extensive experiments demonstrate that our method outperforms
existing works significantly. The dataset and source code will be released on
GitHub soon.
</p></li>
</ul>

<h3>Title: Unveiling the invisible: Enhanced detection and analysis deteriorated areas in solar PV modules using unsupervised sensing algorithms and 3D augmented reality. (arXiv:2307.05136v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05136">http://arxiv.org/abs/2307.05136</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05136] Unveiling the invisible: Enhanced detection and analysis deteriorated areas in solar PV modules using unsupervised sensing algorithms and 3D augmented reality](http://arxiv.org/abs/2307.05136) #security</code></li>
<li>Summary: <p>Solar Photovoltaic (PV) is increasingly being used to address the global
concern of energy security. However, hot spot and snail trails in PV modules
caused mostly by crakes reduce their efficiency and power capacity. This
article presents a groundbreaking methodology for automatically identifying and
analyzing anomalies like hot spots and snail trails in Solar Photovoltaic (PV)
modules, leveraging unsupervised sensing algorithms and 3D Augmented Reality
(AR) visualization. By transforming the traditional methods of diagnosis and
repair, our approach not only enhances efficiency but also substantially cuts
down the cost of PV system maintenance. Validated through computer simulations
and real-world image datasets, the proposed framework accurately identifies
dirty regions, emphasizing the critical role of regular maintenance in
optimizing the power capacity of solar PV modules. Our immediate objective is
to leverage drone technology for real-time, automatic solar panel detection,
significantly boosting the efficacy of PV maintenance. The proposed methodology
could revolutionize solar PV maintenance, enabling swift, precise anomaly
detection without human intervention. This could result in significant cost
savings, heightened energy production, and improved overall performance of
solar PV systems. Moreover, the novel combination of unsupervised sensing
algorithms with 3D AR visualization heralds new opportunities for further
research and development in solar PV maintenance.
</p></li>
</ul>

<h3>Title: SecFlow: Adaptive Security-Aware Workflow Management System in Multi-Cloud Environments. (arXiv:2307.05137v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05137">http://arxiv.org/abs/2307.05137</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05137] SecFlow: Adaptive Security-Aware Workflow Management System in Multi-Cloud Environments](http://arxiv.org/abs/2307.05137) #security</code></li>
<li>Summary: <p>In this paper, we propose an architecture for a security-aware workflow
management system (WfMS) we call SecFlow in answer to the recent developments
of combining workflow management systems with Cloud environments and the still
lacking abilities of such systems to ensure the security and privacy of
cloud-based workflows. The SecFlow architecture focuses on full workflow life
cycle coverage as, in addition to the existing approaches to design
security-aware processes, there is a need to fill in the gap of maintaining
security properties of workflows during their execution phase. To address this
gap, we derive the requirements for such a security-aware WfMS and design a
system architecture that meets these requirements. SecFlow integrates key
functional components such as secure model construction, security-aware service
selection, security violation detection, and adaptive response mechanisms while
considering all potential malicious parties in multi-tenant and cloud-based
WfMS.
</p></li>
</ul>

<h3>Title: Smart Environment for Adaptive Learning of Cybersecurity Skills. (arXiv:2307.05281v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05281">http://arxiv.org/abs/2307.05281</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05281] Smart Environment for Adaptive Learning of Cybersecurity Skills](http://arxiv.org/abs/2307.05281) #security</code></li>
<li>Summary: <p>Hands-on computing education requires a realistic learning environment that
enables students to gain and deepen their skills. Available learning
environments, including virtual and physical labs, provide students with
real-world computer systems but rarely adapt the learning environment to
individual students of various proficiency and background. We designed a unique
and novel smart environment for adaptive training of cybersecurity skills. The
environment collects a variety of student data to assign a suitable learning
path through the training. To enable such adaptiveness, we proposed, developed,
and deployed a new tutor model and a training format. We evaluated the learning
environment using two different adaptive trainings attended by 114 students of
various proficiency. The results show students were assigned tasks with a more
appropriate difficulty, which enabled them to successfully complete the
training. Students reported that they enjoyed the training, felt the training
difficulty was appropriately designed, and would attend more training sessions
like these. Instructors can use the environment for teaching any topic
involving real-world computer networks and systems because it is not tailored
to particular training. We freely released the software along with exemplary
training so that other instructors can adopt the innovations in their teaching
practice.
</p></li>
</ul>

<h3>Title: Improving the Security of Smartwatch Payment with Deep Learning. (arXiv:2307.05437v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05437">http://arxiv.org/abs/2307.05437</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05437] Improving the Security of Smartwatch Payment with Deep Learning](http://arxiv.org/abs/2307.05437) #security</code></li>
<li>Summary: <p>Making contactless payments using a smartwatch is increasingly popular, but
this payment medium lacks traditional biometric security measures such as
facial or fingerprint recognition. In 2022, Sturgess et al. proposed WatchAuth,
a system for authenticating smartwatch payments using the physical gesture of
reaching towards a payment terminal. While effective, the system requires the
user to undergo a burdensome enrolment period to achieve acceptable error
levels. In this dissertation, we explore whether applications of deep learning
can reduce the number of gestures a user must provide to enrol into an
authentication system for smartwatch payment. We firstly construct a
deep-learned authentication system that outperforms the current
state-of-the-art, including in a scenario where the target user has provided a
limited number of gestures. We then develop a regularised autoencoder model for
generating synthetic user-specific gestures. We show that using these gestures
in training improves classification ability for an authentication system.
Through this technique we can reduce the number of gestures required to enrol a
user into a WatchAuth-like system without negatively impacting its error rates.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: A Modular Multimodal Architecture for Gaze Target Prediction: Application to Privacy-Sensitive Settings. (arXiv:2307.05158v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05158">http://arxiv.org/abs/2307.05158</a></li>
<li>Code URL: <a href="https://github.com/idiap/multimodal_gaze_target_prediction">https://github.com/idiap/multimodal_gaze_target_prediction</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05158] A Modular Multimodal Architecture for Gaze Target Prediction: Application to Privacy-Sensitive Settings](http://arxiv.org/abs/2307.05158) #privacy</code></li>
<li>Summary: <p>Predicting where a person is looking is a complex task, requiring to
understand not only the person's gaze and scene content, but also the 3D scene
structure and the person's situation (are they manipulating? interacting or
observing others? attentive?) to detect obstructions in the line of sight or
apply attention priors that humans typically have when observing others. In
this paper, we hypothesize that identifying and leveraging such priors can be
better achieved through the exploitation of explicitly derived multimodal cues
such as depth and pose. We thus propose a modular multimodal architecture
allowing to combine these cues using an attention mechanism. The architecture
can naturally be exploited in privacy-sensitive situations such as surveillance
and health, where personally identifiable information cannot be released. We
perform extensive experiments on the GazeFollow and VideoAttentionTarget public
datasets, obtaining state-of-the-art performance and demonstrating very
competitive results in the privacy setting case.
</p></li>
</ul>

<h3>Title: MentalHealthAI: Utilizing Personal Health Device Data to Optimize Psychiatry Treatment. (arXiv:2307.04777v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04777">http://arxiv.org/abs/2307.04777</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04777] MentalHealthAI: Utilizing Personal Health Device Data to Optimize Psychiatry Treatment](http://arxiv.org/abs/2307.04777) #privacy</code></li>
<li>Summary: <p>Mental health disorders remain a significant challenge in modern healthcare,
with diagnosis and treatment often relying on subjective patient descriptions
and past medical history. To address this issue, we propose a personalized
mental health tracking and mood prediction system that utilizes patient
physiological data collected through personal health devices. Our system
leverages a decentralized learning mechanism that combines transfer and
federated machine learning concepts using smart contracts, allowing data to
remain on users' devices and enabling effective tracking of mental health
conditions for psychiatric treatment and management in a privacy-aware and
accountable manner. We evaluate our model using a popular mental health dataset
that demonstrates promising results. By utilizing connected health systems and
machine learning models, our approach offers a novel solution to the challenge
of providing psychiatrists with further insight into their patients' mental
health outside of traditional office visits.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h3>Title: ATWM: Defense against adversarial malware based on adversarial training. (arXiv:2307.05095v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05095">http://arxiv.org/abs/2307.05095</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05095] ATWM: Defense against adversarial malware based on adversarial training](http://arxiv.org/abs/2307.05095) #defense</code></li>
<li>Summary: <p>Deep learning technology has made great achievements in the field of image.
In order to defend against malware attacks, researchers have proposed many
Windows malware detection models based on deep learning. However, deep learning
models are vulnerable to adversarial example attacks. Malware can generate
adversarial malware with the same malicious function to attack the malware
detection model and evade detection of the model. Currently, many adversarial
defense studies have been proposed, but existing adversarial defense studies
are based on image sample and cannot be directly applied to malware sample.
Therefore, this paper proposes an adversarial malware defense method based on
adversarial training. This method uses preprocessing to defend simple
adversarial examples to reduce the difficulty of adversarial training.
Moreover, this method improves the adversarial defense capability of the model
through adversarial training. We experimented with three attack methods in two
sets of datasets, and the results show that the method in this paper can
improve the adversarial defense capability of the model without reducing the
accuracy of the model.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: On the Vulnerability of DeepFake Detectors to Attacks Generated by Denoising Diffusion Models. (arXiv:2307.05397v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05397">http://arxiv.org/abs/2307.05397</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05397] On the Vulnerability of DeepFake Detectors to Attacks Generated by Denoising Diffusion Models](http://arxiv.org/abs/2307.05397) #attack</code></li>
<li>Summary: <p>The detection of malicious Deepfakes is a constantly evolving problem, that
requires continuous monitoring of detectors, to ensure they are able to detect
image manipulations generated by the latest emerging models. In this paper, we
present a preliminary study that investigates the vulnerability of single-image
Deepfake detectors to attacks created by a representative of the newest
generation of generative methods, i.e. Denoising Diffusion Models (DDMs). Our
experiments are run on FaceForensics++, a commonly used benchmark dataset,
consisting of Deepfakes generated with various techniques for face swapping and
face reenactment. The analysis shows, that reconstructing existing Deepfakes
with only one denoising diffusion step significantly decreases the accuracy of
all tested detectors, without introducing visually perceptible image changes.
</p></li>
</ul>

<h3>Title: A Blockchain-based two Factor Honeytoken Authentication System. (arXiv:2307.05047v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05047">http://arxiv.org/abs/2307.05047</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05047] A Blockchain-based two Factor Honeytoken Authentication System](http://arxiv.org/abs/2307.05047) #attack</code></li>
<li>Summary: <p>This paper extends and advances our recently introduced two-factor Honeytoken
authentication method by incorporating blockchain technology. This novel
approach strengthens the authentication method to prevent many attacks
including tampering attacks. Evaluation results show that integrating
blockchain into the Honeytoken method could improve performance and operational
efficiency.
</p></li>
</ul>

<h3>Title: Membership Inference Attacks on DNNs using Adversarial Perturbations. (arXiv:2307.05193v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05193">http://arxiv.org/abs/2307.05193</a></li>
<li>Code URL: <a href="https://github.com/hassanalikhatim/amia">https://github.com/hassanalikhatim/amia</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05193] Membership Inference Attacks on DNNs using Adversarial Perturbations](http://arxiv.org/abs/2307.05193) #attack</code></li>
<li>Summary: <p>Several membership inference (MI) attacks have been proposed to audit a
target DNN. Given a set of subjects, MI attacks tell which subjects the target
DNN has seen during training. This work focuses on the post-training MI attacks
emphasizing high confidence membership detection -- True Positive Rates (TPR)
at low False Positive Rates (FPR). Current works in this category -- likelihood
ratio attack (LiRA) and enhanced MI attack (EMIA) -- only perform well on
complex datasets (e.g., CIFAR-10 and Imagenet) where the target DNN overfits
its train set, but perform poorly on simpler datasets (0% TPR by both attacks
on Fashion-MNIST, 2% and 0% TPR respectively by LiRA and EMIA on MNIST at 1%
FPR). To address this, firstly, we unify current MI attacks by presenting a
framework divided into three stages -- preparation, indication and decision.
Secondly, we utilize the framework to propose two novel attacks: (1)
Adversarial Membership Inference Attack (AMIA) efficiently utilizes the
membership and the non-membership information of the subjects while
adversarially minimizing a novel loss function, achieving 6% TPR on both
Fashion-MNIST and MNIST datasets; and (2) Enhanced AMIA (E-AMIA) combines EMIA
and AMIA to achieve 8% and 4% TPRs on Fashion-MNIST and MNIST datasets
respectively, at 1% FPR. Thirdly, we introduce two novel augmented indicators
that positively leverage the loss information in the Gaussian neighborhood of a
subject. This improves TPR of all four attacks on average by 2.5% and 0.25%
respectively on Fashion-MNIST and MNIST datasets at 1% FPR. Finally, we propose
simple, yet novel, evaluation metric, the running TPR average (RTA) at a given
FPR, that better distinguishes different MI attacks in the low FPR region. We
also show that AMIA and E-AMIA are more transferable to the unknown DNNs (other
than the target DNN) and are more robust to DP-SGD training as compared to LiRA
and EMIA.
</p></li>
</ul>

<h3>Title: Application-aware Energy Attack Mitigation in the Battery-less Internet of Things. (arXiv:2307.05206v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05206">http://arxiv.org/abs/2307.05206</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05206] Application-aware Energy Attack Mitigation in the Battery-less Internet of Things](http://arxiv.org/abs/2307.05206) #attack</code></li>
<li>Summary: <p>We study how to mitigate the effects of energy attacks in the batteryless
Internet of Things (IoT). Battery-less IoT devices live and die with ambient
energy, as they use energy harvesting to power their operation. They are
employed in a multitude of applications, including safety-critical ones such as
biomedical implants. Due to scarce energy intakes and limited energy buffers,
their executions become intermittent, alternating periods of active operation
with periods of recharging their energy buffers. Experimental evidence exists
that shows how controlling ambient energy allows an attacker to steer a device
execution in unintended ways: energy provisioning effectively becomes an attack
vector. We design, implement, and evaluate a mitigation system for energy
attacks. By taking into account the specific application requirements and the
output of an attack detection module, we tune task execution rates and optimize
energy management. This ensures continued application execution in the event of
an energy attack. When a device is under attack, our solution ensures the
execution of 23.3% additional application cycles compared to the baselines we
consider and increases task schedulability by at least 21%, while enabling a
34% higher peripheral availability.
</p></li>
</ul>

<h3>Title: Differential Analysis of Triggers and Benign Features for Black-Box DNN Backdoor Detection. (arXiv:2307.05422v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05422">http://arxiv.org/abs/2307.05422</a></li>
<li>Code URL: <a href="https://github.com/fu1001hao/five-metrics-detector">https://github.com/fu1001hao/five-metrics-detector</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05422] Differential Analysis of Triggers and Benign Features for Black-Box DNN Backdoor Detection](http://arxiv.org/abs/2307.05422) #attack</code></li>
<li>Summary: <p>This paper proposes a data-efficient detection method for deep neural
networks against backdoor attacks under a black-box scenario. The proposed
approach is motivated by the intuition that features corresponding to triggers
have a higher influence in determining the backdoored network output than any
other benign features. To quantitatively measure the effects of triggers and
benign features on determining the backdoored network output, we introduce five
metrics. To calculate the five-metric values for a given input, we first
generate several synthetic samples by injecting the input's partial contents
into clean validation samples. Then, the five metrics are computed by using the
output labels of the corresponding synthetic samples. One contribution of this
work is the use of a tiny clean validation dataset. Having the computed five
metrics, five novelty detectors are trained from the validation dataset. A meta
novelty detector fuses the output of the five trained novelty detectors to
generate a meta confidence score. During online testing, our method determines
if online samples are poisoned or not via assessing their meta confidence
scores output by the meta novelty detector. We show the efficacy of our
methodology through a broad range of backdoor attacks, including ablation
studies and comparison to existing approaches. Our methodology is promising
since the proposed five metrics quantify the inherent differences between clean
and poisoned samples. Additionally, our detection method can be incrementally
improved by appending more metrics that may be proposed to address future
advanced attacks.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Unleashing the Potential of Regularization Strategies in Learning with Noisy Labels. (arXiv:2307.05025v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05025">http://arxiv.org/abs/2307.05025</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05025] Unleashing the Potential of Regularization Strategies in Learning with Noisy Labels](http://arxiv.org/abs/2307.05025) #robust</code></li>
<li>Summary: <p>In recent years, research on learning with noisy labels has focused on
devising novel algorithms that can achieve robustness to noisy training labels
while generalizing to clean data. These algorithms often incorporate
sophisticated techniques, such as noise modeling, label correction, and
co-training. In this study, we demonstrate that a simple baseline using
cross-entropy loss, combined with widely used regularization strategies like
learning rate decay, model weights average, and data augmentations, can
outperform state-of-the-art methods. Our findings suggest that employing a
combination of regularization strategies can be more effective than intricate
algorithms in tackling the challenges of learning with noisy labels. While some
of these regularization strategies have been utilized in previous noisy label
learning research, their full potential has not been thoroughly explored. Our
results encourage a reevaluation of benchmarks for learning with noisy labels
and prompt reconsideration of the role of specialized learning algorithms
designed for training with noisy labels.
</p></li>
</ul>

<h3>Title: One-Shot Learning for Periocular Recognition: Exploring the Effect of Domain Adaptation and Data Bias on Deep Representations. (arXiv:2307.05128v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05128">http://arxiv.org/abs/2307.05128</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05128] One-Shot Learning for Periocular Recognition: Exploring the Effect of Domain Adaptation and Data Bias on Deep Representations](http://arxiv.org/abs/2307.05128) #robust</code></li>
<li>Summary: <p>One weakness of machine-learning algorithms is the need to train the models
for a new task. This presents a specific challenge for biometric recognition
due to the dynamic nature of databases and, in some instances, the reliance on
subject collaboration for data collection. In this paper, we investigate the
behavior of deep representations in widely used CNN models under extreme data
scarcity for One-Shot periocular recognition, a biometric recognition task. We
analyze the outputs of CNN layers as identity-representing feature vectors. We
examine the impact of Domain Adaptation on the network layers' output for
unseen data and evaluate the method's robustness concerning data normalization
and generalization of the best-performing layer. We improved state-of-the-art
results that made use of networks trained with biometric datasets with millions
of images and fine-tuned for the target periocular dataset by utilizing
out-of-the-box CNNs trained for the ImageNet Recognition Challenge and standard
computer vision algorithms. For example, for the Cross-Eyed dataset, we could
reduce the EER by 67% and 79% (from 1.70% and 3.41% to 0.56% and 0.71%) in the
Close-World and Open-World protocols, respectively, for the periocular case. We
also demonstrate that traditional algorithms like SIFT can outperform CNNs in
situations with limited data or scenarios where the network has not been
trained with the test classes like the Open-World mode. SIFT alone was able to
reduce the EER by 64% and 71.6% (from 1.7% and 3.41% to 0.6% and 0.97%) for
Cross-Eyed in the Close-World and Open-World protocols, respectively, and a
reduction of 4.6% (from 3.94% to 3.76%) in the PolyU database for the
Open-World and single biometric case.
</p></li>
</ul>

<h3>Title: Co-Attention Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery. (arXiv:2307.05182v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05182">http://arxiv.org/abs/2307.05182</a></li>
<li>Code URL: <a href="https://github.com/longbai1006/cat-vil">https://github.com/longbai1006/cat-vil</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05182] Co-Attention Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery](http://arxiv.org/abs/2307.05182) #robust</code></li>
<li>Summary: <p>Medical students and junior surgeons often rely on senior surgeons and
specialists to answer their questions when learning surgery. However, experts
are often busy with clinical and academic work, and have little time to give
guidance. Meanwhile, existing deep learning (DL)-based surgical Visual Question
Answering (VQA) systems can only provide simple answers without the location of
the answers. In addition, vision-language (ViL) embedding is still a less
explored research in these kinds of tasks. Therefore, a surgical Visual
Question Localized-Answering (VQLA) system would be helpful for medical
students and junior surgeons to learn and understand from recorded surgical
videos. We propose an end-to-end Transformer with Co-Attention gaTed
Vision-Language (CAT-ViL) for VQLA in surgical scenarios, which does not
require feature extraction through detection models. The CAT-ViL embedding
module is designed to fuse heterogeneous features from visual and textual
sources. The fused embedding will feed a standard Data-Efficient Image
Transformer (DeiT) module, before the parallel classifier and detector for
joint prediction. We conduct the experimental validation on public surgical
videos from MICCAI EndoVis Challenge 2017 and 2018. The experimental results
highlight the superior performance and robustness of our proposed model
compared to the state-of-the-art approaches. Ablation studies further prove the
outstanding performance of all the proposed components. The proposed method
provides a promising solution for surgical scene understanding, and opens up a
primary step in the Artificial Intelligence (AI)-based VQLA system for surgical
training. Our code is publicly available.
</p></li>
</ul>

<h3>Title: Differentiable Blocks World: Qualitative 3D Decomposition by Rendering Primitives. (arXiv:2307.05473v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05473">http://arxiv.org/abs/2307.05473</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05473] Differentiable Blocks World: Qualitative 3D Decomposition by Rendering Primitives](http://arxiv.org/abs/2307.05473) #robust</code></li>
<li>Summary: <p>Given a set of calibrated images of a scene, we present an approach that
produces a simple, compact, and actionable 3D world representation by means of
3D primitives. While many approaches focus on recovering high-fidelity 3D
scenes, we focus on parsing a scene into mid-level 3D representations made of a
small set of textured primitives. Such representations are interpretable, easy
to manipulate and suited for physics-based simulations. Moreover, unlike
existing primitive decomposition methods that rely on 3D input data, our
approach operates directly on images through differentiable rendering.
Specifically, we model primitives as textured superquadric meshes and optimize
their parameters from scratch with an image rendering loss. We highlight the
importance of modeling transparency for each primitive, which is critical for
optimization and also enables handling varying numbers of primitives. We show
that the resulting textured primitives faithfully reconstruct the input images
and accurately model the visible 3D points, while providing amodal shape
completions of unseen object regions. We compare our approach to the state of
the art on diverse scenes from DTU, and demonstrate its robustness on real-life
captures from BlendedMVS and Nerfstudio. We also showcase how our results can
be used to effortlessly edit a scene or perform physical simulations. Code and
video results are available at https://www.tmonnier.com/DBW .
</p></li>
</ul>

<h3>Title: SigOpt Mulch: An Intelligent System for AutoML of Gradient Boosted Trees. (arXiv:2307.04849v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04849">http://arxiv.org/abs/2307.04849</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04849] SigOpt Mulch: An Intelligent System for AutoML of Gradient Boosted Trees](http://arxiv.org/abs/2307.04849) #robust</code></li>
<li>Summary: <p>Gradient boosted trees (GBTs) are ubiquitous models used by researchers,
machine learning (ML) practitioners, and data scientists because of their
robust performance, interpretable behavior, and ease-of-use. One critical
challenge in training GBTs is the tuning of their hyperparameters. In practice,
selecting these hyperparameters is often done manually. Recently, the ML
community has advocated for tuning hyperparameters through black-box
optimization and developed state-of-the-art systems to do so. However, applying
such systems to tune GBTs suffers from two drawbacks. First, these systems are
not \textit{model-aware}, rather they are designed to apply to a
\textit{generic} model; this leaves significant optimization performance on the
table. Second, using these systems requires \textit{domain knowledge} such as
the choice of hyperparameter search space, which is an antithesis to the
automatic experimentation that black-box optimization aims to provide. In this
paper, we present SigOpt Mulch, a model-aware hyperparameter tuning system
specifically designed for automated tuning of GBTs that provides two
improvements over existing systems. First, Mulch leverages powerful techniques
in metalearning and multifidelity optimization to perform model-aware
hyperparameter optimization. Second, it automates the process of learning
performant hyperparameters by making intelligent decisions about the
optimization search space, thus reducing the need for user domain knowledge.
These innovations allow Mulch to identify good GBT hyperparameters far more
efficiently -- and in a more seamless and user-friendly way -- than existing
black-box hyperparameter tuning systems.
</p></li>
</ul>

<h3>Title: Benchmarking Bayesian Causal Discovery Methods for Downstream Treatment Effect Estimation. (arXiv:2307.04988v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04988">http://arxiv.org/abs/2307.04988</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04988] Benchmarking Bayesian Causal Discovery Methods for Downstream Treatment Effect Estimation](http://arxiv.org/abs/2307.04988) #robust</code></li>
<li>Summary: <p>The practical utility of causality in decision-making is widely recognized,
with causal discovery and inference being inherently intertwined. Nevertheless,
a notable gap exists in the evaluation of causal discovery methods, where
insufficient emphasis is placed on downstream inference. To address this gap,
we evaluate six established baseline causal discovery methods and a newly
proposed method based on GFlowNets, on the downstream task of treatment effect
estimation. Through the implementation of a robust evaluation procedure, we
offer valuable insights into the efficacy of these causal discovery methods for
treatment effect estimation, considering both synthetic and real-world
scenarios, as well as low-data scenarios. Furthermore, the results of our study
demonstrate that GFlowNets possess the capability to effectively capture a wide
range of useful and diverse ATE modes.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Entity Identifier: A Natural Text Parsing-based Framework For Entity Relation Extraction. (arXiv:2307.04892v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04892">http://arxiv.org/abs/2307.04892</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04892] Entity Identifier: A Natural Text Parsing-based Framework For Entity Relation Extraction](http://arxiv.org/abs/2307.04892) #extraction</code></li>
<li>Summary: <p>The field of programming has a diversity of paradigms that are used according
to the working framework. While current neural code generation methods are able
to learn and generate code directly from text, we believe that this approach is
not optimal for certain code tasks, particularly the generation of classes in
an object-oriented project. Specifically, we use natural language processing
techniques to extract structured information from requirements descriptions, in
order to automate the generation of CRUD (Create, Read, Update, Delete) class
code. To facilitate this process, we introduce a pipeline for extracting entity
and relation information, as well as a representation called an "Entity Tree"
to model this information. We also create a dataset to evaluate the
effectiveness of our approach.
</p></li>
</ul>

<h3>Title: CareFall: Automatic Fall Detection through Wearable Devices and AI Methods. (arXiv:2307.05275v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05275">http://arxiv.org/abs/2307.05275</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05275] CareFall: Automatic Fall Detection through Wearable Devices and AI Methods](http://arxiv.org/abs/2307.05275) #extraction</code></li>
<li>Summary: <p>The aging population has led to a growing number of falls in our society,
affecting global public health worldwide. This paper presents CareFall, an
automatic Fall Detection System (FDS) based on wearable devices and Artificial
Intelligence (AI) methods. CareFall considers the accelerometer and gyroscope
time signals extracted from a smartwatch. Two different approaches are used for
feature extraction and classification: i) threshold-based, and ii) machine
learning-based. Experimental results on two public databases show that the
machine learning-based approach, which combines accelerometer and gyroscope
information, outperforms the threshold-based approach in terms of accuracy,
sensitivity, and specificity. This research contributes to the design of smart
and user-friendly solutions to mitigate the negative consequences of falls
among older people.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Fed-CPrompt: Contrastive Prompt for Rehearsal-Free Federated Continual Learning. (arXiv:2307.04869v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04869">http://arxiv.org/abs/2307.04869</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04869] Fed-CPrompt: Contrastive Prompt for Rehearsal-Free Federated Continual Learning](http://arxiv.org/abs/2307.04869) #federate</code></li>
<li>Summary: <p>Federated continual learning (FCL) learns incremental tasks over time from
confidential datasets distributed across clients. This paper focuses on
rehearsal-free FCL, which has severe forgetting issues when learning new tasks
due to the lack of access to historical task data. To address this issue, we
propose Fed-CPrompt based on prompt learning techniques to obtain task-specific
prompts in a communication-efficient way. Fed-CPrompt introduces two key
components, asynchronous prompt learning, and contrastive continual loss, to
handle asynchronous task arrival and heterogeneous data distributions in FCL,
respectively. Extensive experiments demonstrate the effectiveness of
Fed-CPrompt in achieving SOTA rehearsal-free FCL performance.
</p></li>
</ul>

<h3>Title: FedYolo: Augmenting Federated Learning with Pretrained Transformers. (arXiv:2307.04905v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04905">http://arxiv.org/abs/2307.04905</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04905] FedYolo: Augmenting Federated Learning with Pretrained Transformers](http://arxiv.org/abs/2307.04905) #federate</code></li>
<li>Summary: <p>The growth and diversity of machine learning applications motivate a
rethinking of learning with mobile and edge devices. How can we address diverse
client goals and learn with scarce heterogeneous data? While federated learning
aims to address these issues, it has challenges hindering a unified solution.
Large transformer models have been shown to work across a variety of tasks
achieving remarkable few-shot adaptation. This raises the question: Can clients
use a single general-purpose model, rather than custom models for each task,
while obeying device and network constraints? In this work, we investigate
pretrained transformers (PTF) to achieve these on-device learning goals and
thoroughly explore the roles of model size and modularity, where the latter
refers to adaptation through modules such as prompts or adapters. Focusing on
federated learning, we demonstrate that: (1) Larger scale shrinks the accuracy
gaps between alternative approaches and improves heterogeneity robustness.
Scale allows clients to run more local SGD epochs which can significantly
reduce the number of communication rounds. At the extreme, clients can achieve
respectable accuracy locally highlighting the potential of fully-local
learning. (2) Modularity, by design, enables $>$100$\times$ less communication
in bits. Surprisingly, it also boosts the generalization capability of local
adaptation methods and the robustness of smaller PTFs. Finally, it enables
clients to solve multiple unrelated tasks simultaneously using a single PTF,
whereas full updates are prone to catastrophic forgetting. These insights on
scale and modularity motivate a new federated learning approach we call "You
Only Load Once" (FedYolo): The clients load a full PTF model once and all
future updates are accomplished through communication-efficient modules with
limited catastrophic-forgetting, where each task is assigned to its own module.
</p></li>
</ul>

<h3>Title: Benchmarking Algorithms for Federated Domain Generalization. (arXiv:2307.04942v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04942">http://arxiv.org/abs/2307.04942</a></li>
<li>Code URL: <a href="https://github.com/inouye-lab/feddg_benchmark">https://github.com/inouye-lab/feddg_benchmark</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04942] Benchmarking Algorithms for Federated Domain Generalization](http://arxiv.org/abs/2307.04942) #federate</code></li>
<li>Summary: <p>While prior domain generalization (DG) benchmarks consider train-test dataset
heterogeneity, we evaluate Federated DG which introduces federated learning
(FL) specific challenges. Additionally, we explore domain-based heterogeneity
in clients' local datasets - a realistic Federated DG scenario. Prior Federated
DG evaluations are limited in terms of the number or heterogeneity of clients
and dataset diversity. To address this gap, we propose an Federated DG
benchmark methodology that enables control of the number and heterogeneity of
clients and provides metrics for dataset difficulty. We then apply our
methodology to evaluate 13 Federated DG methods, which include centralized DG
methods adapted to the FL context, FL methods that handle client heterogeneity,
and methods designed specifically for Federated DG. Our results suggest that
despite some progress, there remain significant performance gaps in Federated
DG particularly when evaluating with a large number of clients, high client
heterogeneity, or more realistic datasets. Please check our extendable
benchmark code here: https://github.com/inouye-lab/FedDG_Benchmark.
</p></li>
</ul>

<h3>Title: Combating Data Imbalances in Federated Semi-supervised Learning with Dual Regulators. (arXiv:2307.05358v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05358">http://arxiv.org/abs/2307.05358</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05358] Combating Data Imbalances in Federated Semi-supervised Learning with Dual Regulators](http://arxiv.org/abs/2307.05358) #federate</code></li>
<li>Summary: <p>Federated learning has become a popular method to learn from decentralized
heterogeneous data. Federated semi-supervised learning (FSSL) emerges to train
models from a small fraction of labeled data due to label scarcity on
decentralized clients. Existing FSSL methods assume independent and identically
distributed (IID) labeled data across clients and consistent class distribution
between labeled and unlabeled data within a client. This work studies a more
practical and challenging scenario of FSSL, where data distribution is
different not only across clients but also within a client between labeled and
unlabeled data. To address this challenge, we propose a novel FSSL framework
with dual regulators, FedDure.} FedDure lifts the previous assumption with a
coarse-grained regulator (C-reg) and a fine-grained regulator (F-reg): C-reg
regularizes the updating of the local model by tracking the learning effect on
labeled data distribution; F-reg learns an adaptive weighting scheme tailored
for unlabeled instances in each client. We further formulate the client model
training as bi-level optimization that adaptively optimizes the model in the
client with two regulators. Theoretically, we show the convergence guarantee of
the dual regulators. Empirically, we demonstrate that FedDure is superior to
the existing methods across a wide range of settings, notably by more than 11%
on CIFAR-10 and CINIC-10 datasets.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: SAM-U: Multi-box prompts triggered uncertainty estimation for reliable SAM in medical image. (arXiv:2307.04973v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04973">http://arxiv.org/abs/2307.04973</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04973] SAM-U: Multi-box prompts triggered uncertainty estimation for reliable SAM in medical image](http://arxiv.org/abs/2307.04973) #fair</code></li>
<li>Summary: <p>Recently, Segmenting Anything has taken an important step towards general
artificial intelligence. At the same time, its reliability and fairness have
also attracted great attention, especially in the field of health care. In this
study, we propose multi-box prompts triggered uncertainty estimation for SAM
cues to demonstrate the reliability of segmented lesions or tissues. We
estimate the distribution of SAM predictions via Monte Carlo with prior
distribution parameters, which employs different prompts as formulation of
test-time augmentation. Our experimental results found that multi-box prompts
augmentation improve the SAM performance, and endowed each pixel with
uncertainty. This provides the first paradigm for a reliable SAM.
</p></li>
</ul>

<h3>Title: Improving Fairness of Graph Neural Networks: A Graph Counterfactual Perspective. (arXiv:2307.04937v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04937">http://arxiv.org/abs/2307.04937</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04937] Improving Fairness of Graph Neural Networks: A Graph Counterfactual Perspective](http://arxiv.org/abs/2307.04937) #fair</code></li>
<li>Summary: <p>Graph neural networks have shown great ability in representation (GNNs)
learning on graphs, facilitating various tasks. Despite their great performance
in modeling graphs, recent works show that GNNs tend to inherit and amplify the
bias from training data, causing concerns of the adoption of GNNs in high-stake
scenarios. Hence, many efforts have been taken for fairness-aware GNNs.
However, most existing fair GNNs learn fair node representations by adopting
statistical fairness notions, which may fail to alleviate bias in the presence
of statistical anomalies. Motivated by causal theory, there are several
attempts utilizing graph counterfactual fairness to mitigate root causes of
unfairness. However, these methods suffer from non-realistic counterfactuals
obtained by perturbation or generation. In this paper, we take a causal view on
fair graph learning problem. Guided by the casual analysis, we propose a novel
framework CAF, which can select counterfactuals from training data to avoid
non-realistic counterfactuals and adopt selected counterfactuals to learn fair
node representations for node classification task. Extensive experiments on
synthetic and real-world datasets show the effectiveness of CAF.
</p></li>
</ul>

<h3>Title: FairLay-ML: Intuitive Remedies for Unfairness in Data-Driven Social-Critical Algorithms. (arXiv:2307.05029v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05029">http://arxiv.org/abs/2307.05029</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05029] FairLay-ML: Intuitive Remedies for Unfairness in Data-Driven Social-Critical Algorithms](http://arxiv.org/abs/2307.05029) #fair</code></li>
<li>Summary: <p>This thesis explores open-sourced machine learning (ML) model explanation
tools to understand whether these tools can allow a layman to visualize,
understand, and suggest intuitive remedies to unfairness in ML-based
decision-support systems. Machine learning models trained on datasets biased
against minority groups are increasingly used to guide life-altering social
decisions, prompting the urgent need to study their logic for unfairness. Due
to this problem's impact on vast populations of the general public, it is
critical for the layperson -- not just subject matter experts in social justice
or machine learning experts -- to understand the nature of unfairness within
these algorithms and the potential trade-offs. Existing research on fairness in
machine learning focuses mostly on the mathematical definitions and tools to
understand and remedy unfair models, with some directly citing user-interactive
tools as necessary for future work. This thesis presents FairLay-ML, a
proof-of-concept GUI integrating some of the most promising tools to provide
intuitive explanations for unfair logic in ML models by integrating existing
research tools (e.g. Local Interpretable Model-Agnostic Explanations) with
existing ML-focused GUI (e.g. Python Streamlit). We test FairLay-ML using
models of various accuracy and fairness generated by an unfairness detector
tool, Parfait-ML, and validate our results using Themis. Our study finds that
the technology stack used for FairLay-ML makes it easy to install and provides
real-time black-box explanations of pre-trained models to users. Furthermore,
the explanations provided translate to actionable remedies.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Scale Alone Does not Improve Mechanistic Interpretability in Vision Models. (arXiv:2307.05471v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05471">http://arxiv.org/abs/2307.05471</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05471] Scale Alone Does not Improve Mechanistic Interpretability in Vision Models](http://arxiv.org/abs/2307.05471) #interpretability</code></li>
<li>Summary: <p>In light of the recent widespread adoption of AI systems, understanding the
internal information processing of neural networks has become increasingly
critical. Most recently, machine vision has seen remarkable progress by scaling
neural networks to unprecedented levels in dataset and model size. We here ask
whether this extraordinary increase in scale also positively impacts the field
of mechanistic interpretability. In other words, has our understanding of the
inner workings of scaled neural networks improved as well? We here use a
psychophysical paradigm to quantify mechanistic interpretability for a diverse
suite of models and find no scaling effect for interpretability - neither for
model nor dataset size. Specifically, none of the nine investigated
state-of-the-art models are easier to interpret than the GoogLeNet model from
almost a decade ago. Latest-generation vision models appear even less
interpretable than older architectures, hinting at a regression rather than
improvement, with modern models sacrificing interpretability for accuracy.
These results highlight the need for models explicitly designed to be
mechanistically interpretable and the need for more helpful interpretability
methods to increase our understanding of networks at an atomic level. We
release a dataset containing more than 120'000 human responses from our
psychophysical evaluation of 767 units across nine models. This dataset is
meant to facilitate research on automated instead of human-based
interpretability evaluations that can ultimately be leveraged to directly
optimize the mechanistic interpretability of models.
</p></li>
</ul>

<h3>Title: A Deep Dive into Perturbations as Evaluation Technique for Time Series XAI. (arXiv:2307.05104v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05104">http://arxiv.org/abs/2307.05104</a></li>
<li>Code URL: <a href="https://github.com/visual-xai-for-time-series/time-series-xai-perturbation-analysis">https://github.com/visual-xai-for-time-series/time-series-xai-perturbation-analysis</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05104] A Deep Dive into Perturbations as Evaluation Technique for Time Series XAI](http://arxiv.org/abs/2307.05104) #interpretability</code></li>
<li>Summary: <p>Explainable Artificial Intelligence (XAI) has gained significant attention
recently as the demand for transparency and interpretability of machine
learning models has increased. In particular, XAI for time series data has
become increasingly important in finance, healthcare, and climate science.
However, evaluating the quality of explanations, such as attributions provided
by XAI techniques, remains challenging. This paper provides an in-depth
analysis of using perturbations to evaluate attributions extracted from time
series models. A perturbation analysis involves systematically modifying the
input data and evaluating the impact on the attributions generated by the XAI
method. We apply this approach to several state-of-the-art XAI techniques and
evaluate their performance on three time series classification datasets. Our
results demonstrate that the perturbation analysis approach can effectively
evaluate the quality of attributions and provide insights into the strengths
and limitations of XAI techniques. Such an approach can guide the selection of
XAI methods for time series data, e.g., focusing on return time rather than
precision, and facilitate the development of more reliable and interpretable
machine learning models for time series analysis.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: Route, Interpret, Repeat: Blurring the line between post hoc explainability and interpretable models. (arXiv:2307.05350v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05350">http://arxiv.org/abs/2307.05350</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05350] Route, Interpret, Repeat: Blurring the line between post hoc explainability and interpretable models](http://arxiv.org/abs/2307.05350) #explainability</code></li>
<li>Summary: <p>The current approach to ML model design is either to choose a flexible
Blackbox model and explain it post hoc or to start with an interpretable model.
Blackbox models are flexible but difficult to explain, whereas interpretable
models are designed to be explainable. However, developing interpretable models
necessitates extensive ML knowledge, and the resulting models tend to be less
flexible, offering potentially subpar performance compared to their Blackbox
equivalents. This paper aims to blur the distinction between a post hoc
explanation of a BlackBox and constructing interpretable models. We propose
beginning with a flexible BlackBox model and gradually \emph{carving out} a
mixture of interpretable models and a \emph{residual network}. Our design
identifies a subset of samples and \emph{routes} them through the interpretable
models. The remaining samples are routed through a flexible residual network.
We adopt First Order Logic (FOL) as the interpretable model's backbone, which
provides basic reasoning on concepts retrieved from the BlackBox model. On the
residual network, we repeat the method until the proportion of data explained
by the residual network falls below a desired threshold. Our approach offers
several advantages. First, the mixture of interpretable and flexible residual
networks results in almost no compromise in performance. Second, the route,
interpret, and repeat approach yields a highly flexible interpretable model.
Our extensive experiment demonstrates the performance of the model on various
datasets. We show that by editing the FOL model, we can fix the shortcut
learned by the original BlackBox model. Finally, our method provides a
framework for a hybrid symbolic-connectionist network that is simple to train
and adaptable to many applications.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Collaborative Score Distillation for Consistent Visual Synthesis. (arXiv:2307.04787v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04787">http://arxiv.org/abs/2307.04787</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04787] Collaborative Score Distillation for Consistent Visual Synthesis](http://arxiv.org/abs/2307.04787) #diffusion</code></li>
<li>Summary: <p>Generative priors of large-scale text-to-image diffusion models enable a wide
range of new generation and editing applications on diverse visual modalities.
However, when adapting these priors to complex visual modalities, often
represented as multiple images (e.g., video), achieving consistency across a
set of images is challenging. In this paper, we address this challenge with a
novel method, Collaborative Score Distillation (CSD). CSD is based on the Stein
Variational Gradient Descent (SVGD). Specifically, we propose to consider
multiple samples as "particles" in the SVGD update and combine their score
functions to distill generative priors over a set of images synchronously.
Thus, CSD facilitates seamless integration of information across 2D images,
leading to a consistent visual synthesis across multiple samples. We show the
effectiveness of CSD in a variety of tasks, encompassing the visual editing of
panorama images, videos, and 3D scenes. Our results underline the competency of
CSD as a versatile method for enhancing inter-sample consistency, thereby
broadening the applicability of text-to-image diffusion models.
</p></li>
</ul>

<h3>Title: Articulated 3D Head Avatar Generation using Text-to-Image Diffusion Models. (arXiv:2307.04859v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04859">http://arxiv.org/abs/2307.04859</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04859] Articulated 3D Head Avatar Generation using Text-to-Image Diffusion Models](http://arxiv.org/abs/2307.04859) #diffusion</code></li>
<li>Summary: <p>The ability to generate diverse 3D articulated head avatars is vital to a
plethora of applications, including augmented reality, cinematography, and
education. Recent work on text-guided 3D object generation has shown great
promise in addressing these needs. These methods directly leverage pre-trained
2D text-to-image diffusion models to generate 3D-multi-view-consistent radiance
fields of generic objects. However, due to the lack of geometry and texture
priors, these methods have limited control over the generated 3D objects,
making it difficult to operate inside a specific domain, e.g., human heads. In
this work, we develop a new approach to text-guided 3D head avatar generation
to address this limitation. Our framework directly operates on the geometry and
texture of an articulable 3D morphable model (3DMM) of a head, and introduces
novel optimization procedures to update the geometry and texture while keeping
the 2D and 3D facial features aligned. The result is a 3D head avatar that is
consistent with the text description and can be readily articulated using the
deformation model of the 3DMM. We show that our diffusion-based articulated
head avatars outperform state-of-the-art approaches for this task. The latter
are typically based on CLIP, which is known to provide limited diversity of
generation and accuracy for 3D object generation.
</p></li>
</ul>

<h3>Title: DDGM: Solving inverse problems by Diffusive Denoising of Gradient-based Minimization. (arXiv:2307.04946v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04946">http://arxiv.org/abs/2307.04946</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04946] DDGM: Solving inverse problems by Diffusive Denoising of Gradient-based Minimization](http://arxiv.org/abs/2307.04946) #diffusion</code></li>
<li>Summary: <p>Inverse problems generally require a regularizer or prior for a good
solution. A recent trend is to train a convolutional net to denoise images, and
use this net as a prior when solving the inverse problem. Several proposals
depend on a singular value decomposition of the forward operator, and several
others backpropagate through the denoising net at runtime. Here we propose a
simpler approach that combines the traditional gradient-based minimization of
reconstruction error with denoising. Noise is also added at each step, so the
iterative dynamics resembles a Langevin or diffusion process. Both the level of
added noise and the size of the denoising step decay exponentially with time.
We apply our method to the problem of tomographic reconstruction from electron
micrographs acquired at multiple tilt angles. With empirical studies using
simulated tilt views, we find parameter settings for our method that produce
good results. We show that high accuracy can be achieved with as few as 50
denoising steps. We also compare with DDRM and DPS, more complex diffusion
methods of the kinds mentioned above. These methods are less accurate (as
measured by MSE and SSIM) for our tomography problem, even after the generation
hyperparameters are optimized. Finally we extend our method to reconstruction
of arbitrary-sized images and show results on 128 $\times$ 1568 pixel images
</p></li>
</ul>

<h3>Title: Diffusion idea exploration for art generation. (arXiv:2307.04978v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04978">http://arxiv.org/abs/2307.04978</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04978] Diffusion idea exploration for art generation](http://arxiv.org/abs/2307.04978) #diffusion</code></li>
<li>Summary: <p>Cross-Modal learning tasks have picked up pace in recent times. With plethora
of applications in diverse areas, generation of novel content using multiple
modalities of data has remained a challenging problem. To address the same,
various generative modelling techniques have been proposed for specific tasks.
Novel and creative image generation is one important aspect for industrial
application which could help as an arm for novel content generation. Techniques
proposed previously used Generative Adversarial Network(GAN), autoregressive
models and Variational Autoencoders (VAE) for accomplishing similar tasks.
These approaches are limited in their capability to produce images guided by
either text instructions or rough sketch images decreasing the overall
performance of image generator. We used state of the art diffusion models to
generate creative art by primarily leveraging text with additional support of
rough sketches. Diffusion starts with a pattern of random dots and slowly
converts that pattern into a design image using the guiding information fed
into the model. Diffusion models have recently outperformed other generative
models in image generation tasks using cross modal data as guiding information.
The initial experiments for this task of novel image generation demonstrated
promising qualitative results.
</p></li>
</ul>

<h3>Title: AutoDecoding Latent 3D Diffusion Models. (arXiv:2307.05445v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05445">http://arxiv.org/abs/2307.05445</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05445] AutoDecoding Latent 3D Diffusion Models](http://arxiv.org/abs/2307.05445) #diffusion</code></li>
<li>Summary: <p>We present a novel approach to the generation of static and articulated 3D
assets that has a 3D autodecoder at its core. The 3D autodecoder framework
embeds properties learned from the target dataset in the latent space, which
can then be decoded into a volumetric representation for rendering
view-consistent appearance and geometry. We then identify the appropriate
intermediate volumetric latent space, and introduce robust normalization and
de-normalization operations to learn a 3D diffusion from 2D images or monocular
videos of rigid or articulated objects. Our approach is flexible enough to use
either existing camera supervision or no camera information at all -- instead
efficiently learning it during training. Our evaluations demonstrate that our
generation results outperform state-of-the-art alternatives on various
benchmark datasets and metrics, including multi-view image datasets of
synthetic objects, real in-the-wild videos of moving people, and a large-scale,
real video dataset of static objects.
</p></li>
</ul>

<h3>Title: Metropolis Sampling for Constrained Diffusion Models. (arXiv:2307.05439v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05439">http://arxiv.org/abs/2307.05439</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05439] Metropolis Sampling for Constrained Diffusion Models](http://arxiv.org/abs/2307.05439) #diffusion</code></li>
<li>Summary: <p>Denoising diffusion models have recently emerged as the predominant paradigm
for generative modelling. Their extension to Riemannian manifolds has
facilitated their application to an array of problems in the natural sciences.
Yet, in many practical settings, such manifolds are defined by a set of
constraints and are not covered by the existing (Riemannian) diffusion model
methodology. Recent work has attempted to address this issue by employing novel
noising processes based on logarithmic barrier methods or reflected Brownian
motions. However, the associated samplers are computationally burdensome as the
complexity of the constraints increases. In this paper, we introduce an
alternative simple noising scheme based on Metropolis sampling that affords
substantial gains in computational efficiency and empirical performance
compared to the earlier samplers. Of independent interest, we prove that this
new process corresponds to a valid discretisation of the reflected Brownian
motion. We demonstrate the scalability and flexibility of our approach on a
range of problem settings with convex and non-convex constraints, including
applications from geospatial modelling, robotics and protein design.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: SimpleMTOD: A Simple Language Model for Multimodal Task-Oriented Dialogue with Symbolic Scene Representation. (arXiv:2307.04907v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04907">http://arxiv.org/abs/2307.04907</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04907] SimpleMTOD: A Simple Language Model for Multimodal Task-Oriented Dialogue with Symbolic Scene Representation](http://arxiv.org/abs/2307.04907) #transformer</code></li>
<li>Summary: <p>SimpleMTOD is a simple language model which recasts several sub-tasks in
multimodal task-oriented dialogues as sequence prediction tasks. SimpleMTOD is
built on a large-scale transformer-based auto-regressive architecture, which
has already proven to be successful in uni-modal task-oriented dialogues, and
effectively leverages transfer learning from pre-trained GPT-2. In-order to
capture the semantics of visual scenes, we introduce both local and
de-localized tokens for objects within a scene. De-localized tokens represent
the type of an object rather than the specific object itself and so possess a
consistent meaning across the dataset. SimpleMTOD achieves a state-of-the-art
BLEU score (0.327) in the Response Generation sub-task of the SIMMC 2.0
test-std dataset while performing on par in other multimodal sub-tasks:
Disambiguation, Coreference Resolution, and Dialog State Tracking. This is
despite taking a minimalist approach for extracting visual (and non-visual)
information. In addition the model does not rely on task-specific architectural
changes such as classification heads.
</p></li>
</ul>

<h3>Title: Vacaspati: A Diverse Corpus of Bangla Literature. (arXiv:2307.05083v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05083">http://arxiv.org/abs/2307.05083</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05083] Vacaspati: A Diverse Corpus of Bangla Literature](http://arxiv.org/abs/2307.05083) #transformer</code></li>
<li>Summary: <p>Bangla (or Bengali) is the fifth most spoken language globally; yet, the
state-of-the-art NLP in Bangla is lagging for even simple tasks such as
lemmatization, POS tagging, etc. This is partly due to lack of a varied quality
corpus. To alleviate this need, we build Vacaspati, a diverse corpus of Bangla
literature. The literary works are collected from various websites; only those
works that are publicly available without copyright violations or restrictions
are collected. We believe that published literature captures the features of a
language much better than newspapers, blogs or social media posts which tend to
follow only a certain literary pattern and, therefore, miss out on language
variety. Our corpus Vacaspati is varied from multiple aspects, including type
of composition, topic, author, time, space, etc. It contains more than 11
million sentences and 115 million words. We also built a word embedding model,
Vac-FT, using FastText from Vacaspati as well as trained an Electra model,
Vac-BERT, using the corpus. Vac-BERT has far fewer parameters and requires only
a fraction of resources compared to other state-of-the-art transformer models
and yet performs either better or similar on various downstream tasks. On
multiple downstream tasks, Vac-FT outperforms other FastText-based models. We
also demonstrate the efficacy of Vacaspati as a corpus by showing that similar
models built from other corpora are not as effective. The models are available
at https://bangla.iitk.ac.in/.
</p></li>
</ul>

<h3>Title: ISLTranslate: Dataset for Translating Indian Sign Language. (arXiv:2307.05440v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05440">http://arxiv.org/abs/2307.05440</a></li>
<li>Code URL: <a href="https://github.com/exploration-lab/isltranslate">https://github.com/exploration-lab/isltranslate</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05440] ISLTranslate: Dataset for Translating Indian Sign Language](http://arxiv.org/abs/2307.05440) #transformer</code></li>
<li>Summary: <p>Sign languages are the primary means of communication for many
hard-of-hearing people worldwide. Recently, to bridge the communication gap
between the hard-of-hearing community and the rest of the population, several
sign language translation datasets have been proposed to enable the development
of statistical sign language translation systems. However, there is a dearth of
sign language resources for the Indian sign language. This resource paper
introduces ISLTranslate, a translation dataset for continuous Indian Sign
Language (ISL) consisting of 31k ISL-English sentence/phrase pairs. To the best
of our knowledge, it is the largest translation dataset for continuous Indian
Sign Language. We provide a detailed analysis of the dataset. To validate the
performance of existing end-to-end Sign language to spoken language translation
systems, we benchmark the created dataset with a transformer-based model for
ISL translation.
</p></li>
</ul>

<h3>Title: Transaction Fraud Detection via Spatial-Temporal-Aware Graph Transformer. (arXiv:2307.05121v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05121">http://arxiv.org/abs/2307.05121</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05121] Transaction Fraud Detection via Spatial-Temporal-Aware Graph Transformer](http://arxiv.org/abs/2307.05121) #transformer</code></li>
<li>Summary: <p>How to obtain informative representations of transactions and then perform
the identification of fraudulent transactions is a crucial part of ensuring
financial security. Recent studies apply Graph Neural Networks (GNNs) to the
transaction fraud detection problem. Nevertheless, they encounter challenges in
effectively learning spatial-temporal information due to structural
limitations. Moreover, few prior GNN-based detectors have recognized the
significance of incorporating global information, which encompasses similar
behavioral patterns and offers valuable insights for discriminative
representation learning. Therefore, we propose a novel heterogeneous graph
neural network called Spatial-Temporal-Aware Graph Transformer (STA-GT) for
transaction fraud detection problems. Specifically, we design a temporal
encoding strategy to capture temporal dependencies and incorporate it into the
graph neural network framework, enhancing spatial-temporal information modeling
and improving expressive ability. Furthermore, we introduce a transformer
module to learn local and global information. Pairwise node-node interactions
overcome the limitation of the GNN structure and build up the interactions with
the target node and long-distance ones. Experimental results on two financial
datasets compared to general GNN models and GNN-based fraud detectors
demonstrate that our proposed method STA-GT is effective on the transaction
fraud detection task.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Uni-Removal: A Semi-Supervised Framework for Simultaneously Addressing Multiple Degradations in Real-World Images. (arXiv:2307.05075v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05075">http://arxiv.org/abs/2307.05075</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05075] Uni-Removal: A Semi-Supervised Framework for Simultaneously Addressing Multiple Degradations in Real-World Images](http://arxiv.org/abs/2307.05075) #generative</code></li>
<li>Summary: <p>Removing multiple degradations, such as haze, rain, and blur, from real-world
images poses a challenging and illposed problem. Recently, unified models that
can handle different degradations have been proposed and yield promising
results. However, these approaches focus on synthetic images and experience a
significant performance drop when applied to realworld images. In this paper,
we introduce Uni-Removal, a twostage semi-supervised framework for addressing
the removal of multiple degradations in real-world images using a unified model
and parameters. In the knowledge transfer stage, Uni-Removal leverages a
supervised multi-teacher and student architecture in the knowledge transfer
stage to facilitate learning from pretrained teacher networks specialized in
different degradation types. A multi-grained contrastive loss is introduced to
enhance learning from feature and image spaces. In the domain adaptation stage,
unsupervised fine-tuning is performed by incorporating an adversarial
discriminator on real-world images. The integration of an extended
multi-grained contrastive loss and generative adversarial loss enables the
adaptation of the student network from synthetic to real-world domains.
Extensive experiments on real-world degraded datasets demonstrate the
effectiveness of our proposed method. We compare our Uni-Removal framework with
state-of-the-art supervised and unsupervised methods, showcasing its promising
results in real-world image dehazing, deraining, and deblurring simultaneously.
</p></li>
</ul>

<h3>Title: ExFaceGAN: Exploring Identity Directions in GAN's Learned Latent Space for Synthetic Identity Generation. (arXiv:2307.05151v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05151">http://arxiv.org/abs/2307.05151</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05151] ExFaceGAN: Exploring Identity Directions in GAN's Learned Latent Space for Synthetic Identity Generation](http://arxiv.org/abs/2307.05151) #generative</code></li>
<li>Summary: <p>Deep generative models have recently presented impressive results in
generating realistic face images of random synthetic identities. To generate
multiple samples of a certain synthetic identity, several previous works
proposed to disentangle the latent space of GANs by incorporating additional
supervision or regularization, enabling the manipulation of certain attributes,
e.g. identity, hairstyle, pose, or expression. Most of these works require
designing special loss functions and training dedicated network architectures.
Others proposed to disentangle specific factors in unconditional pretrained
GANs latent spaces to control their output, which also requires supervision by
attribute classifiers. Moreover, these attributes are entangled in GAN's latent
space, making it difficult to manipulate them without affecting the identity
information. We propose in this work a framework, ExFaceGAN, to disentangle
identity information in state-of-the-art pretrained GANs latent spaces,
enabling the generation of multiple samples of any synthetic identity. The
variations in our generated images are not limited to specific attributes as
ExFaceGAN explicitly aims at disentangling identity information, while other
visual attributes are randomly drawn from a learned GAN latent space. As an
example of the practical benefit of our ExFaceGAN, we empirically prove that
data generated by ExFaceGAN can be successfully used to train face recognition
models.
</p></li>
</ul>

<h3>Title: Generative Pretraining in Multimodality. (arXiv:2307.05222v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05222">http://arxiv.org/abs/2307.05222</a></li>
<li>Code URL: <a href="https://github.com/baaivision/emu">https://github.com/baaivision/emu</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05222] Generative Pretraining in Multimodality](http://arxiv.org/abs/2307.05222) #generative</code></li>
<li>Summary: <p>We present Emu, a Transformer-based multimodal foundation model, which can
seamlessly generate images and texts in multimodal context. This omnivore model
can take in any single-modality or multimodal data input indiscriminately
(e.g., interleaved image, text and video) through a one-model-for-all
autoregressive training process. First, visual signals are encoded into
embeddings, and together with text tokens form an interleaved input sequence.
Emu is then end-to-end trained with a unified objective of classifying the next
text token or regressing the next visual embedding in the multimodal sequence.
This versatile multimodality empowers the exploration of diverse pretraining
data sources at scale, such as videos with interleaved frames and text,
webpages with interleaved images and text, as well as web-scale image-text
pairs and video-text pairs. Emu can serve as a generalist multimodal interface
for both image-to-text and text-to-image tasks, and supports in-context image
and text generation. Across a broad range of zero-shot/few-shot tasks including
image captioning, visual question answering, video question answering and
text-to-image generation, Emu demonstrates superb performance compared to
state-of-the-art large multimodal models. Extended capabilities such as
multimodal assistants via instruction tuning are also demonstrated with
impressive performance.
</p></li>
</ul>

<h3>Title: Efficient 3D Articulated Human Generation with Layered Surface Volumes. (arXiv:2307.05462v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05462">http://arxiv.org/abs/2307.05462</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05462] Efficient 3D Articulated Human Generation with Layered Surface Volumes](http://arxiv.org/abs/2307.05462) #generative</code></li>
<li>Summary: <p>Access to high-quality and diverse 3D articulated digital human assets is
crucial in various applications, ranging from virtual reality to social
platforms. Generative approaches, such as 3D generative adversarial networks
(GANs), are rapidly replacing laborious manual content creation tools. However,
existing 3D GAN frameworks typically rely on scene representations that
leverage either template meshes, which are fast but offer limited quality, or
volumes, which offer high capacity but are slow to render, thereby limiting the
3D fidelity in GAN settings. In this work, we introduce layered surface volumes
(LSVs) as a new 3D object representation for articulated digital humans. LSVs
represent a human body using multiple textured mesh layers around a
conventional template. These layers are rendered using alpha compositing with
fast differentiable rasterization, and they can be interpreted as a volumetric
representation that allocates its capacity to a manifold of finite thickness
around the template. Unlike conventional single-layer templates that struggle
with representing fine off-surface details like hair or accessories, our
surface volumes naturally capture such details. LSVs can be articulated, and
they exhibit exceptional efficiency in GAN settings, where a 2D generator
learns to synthesize the RGBA textures for the individual layers. Trained on
unstructured, single-view 2D image datasets, our LSV-GAN generates high-quality
and view-consistent 3D articulated digital humans without the need for
view-inconsistent 2D upsampling networks.
</p></li>
</ul>

<h3>Title: My3DGen: Building Lightweight Personalized 3D Generative Model. (arXiv:2307.05468v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05468">http://arxiv.org/abs/2307.05468</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05468] My3DGen: Building Lightweight Personalized 3D Generative Model](http://arxiv.org/abs/2307.05468) #generative</code></li>
<li>Summary: <p>Our paper presents My3DGen, a practical system for creating a personalized
and lightweight 3D generative prior using as few as 10 images. My3DGen can
reconstruct multi-view consistent images from an input test image, and generate
novel appearances by interpolating between any two images of the same
individual. While recent studies have demonstrated the effectiveness of
personalized generative priors in producing high-quality 2D portrait
reconstructions and syntheses, to the best of our knowledge, we are the first
to develop a personalized 3D generative prior. Instead of fine-tuning a large
pre-trained generative model with millions of parameters to achieve
personalization, we propose a parameter-efficient approach. Our method involves
utilizing a pre-trained model with fixed weights as a generic prior, while
training a separate personalized prior through low-rank decomposition of the
weights in each convolution and fully connected layer. However,
parameter-efficient few-shot fine-tuning on its own often leads to overfitting.
To address this, we introduce a regularization technique based on symmetry of
human faces. This regularization enforces that novel view renderings of a
training sample, rendered from symmetric poses, exhibit the same identity. By
incorporating this symmetry prior, we enhance the quality of reconstruction and
synthesis, particularly for non-frontal (profile) faces. Our final system
combines low-rank fine-tuning with symmetry regularization and significantly
surpasses the performance of pre-trained models, e.g. EG3D. It introduces only
approximately 0.6 million additional parameters per identity compared to 31
million for full finetuning of the original model. As a result, our system
achieves a 50-fold reduction in model size without sacrificing the quality of
the generated 3D faces. Code will be available at our project page:
https://luchaoqi.github.io/my3dgen.
</p></li>
</ul>

<h3>Title: Comparison of Point Cloud and Image-based Models for Calorimeter Fast Simulation. (arXiv:2307.04780v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04780">http://arxiv.org/abs/2307.04780</a></li>
<li>Code URL: <a href="https://github.com/viniciusmikuni/calo4eic">https://github.com/viniciusmikuni/calo4eic</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04780] Comparison of Point Cloud and Image-based Models for Calorimeter Fast Simulation](http://arxiv.org/abs/2307.04780) #generative</code></li>
<li>Summary: <p>Score based generative models are a new class of generative models that have
been shown to accurately generate high dimensional calorimeter datasets. Recent
advances in generative models have used images with 3D voxels to represent and
model complex calorimeter showers. Point clouds, however, are likely a more
natural representation of calorimeter showers, particularly in calorimeters
with high granularity. Point clouds preserve all of the information of the
original simulation, more naturally deal with sparse datasets, and can be
implemented with more compact models and data files. In this work, two
state-of-the-art score based models are trained on the same set of calorimeter
simulation and directly compared.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Amplifying Limitations, Harms and Risks of Large Language Models. (arXiv:2307.04821v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04821">http://arxiv.org/abs/2307.04821</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04821] Amplifying Limitations, Harms and Risks of Large Language Models](http://arxiv.org/abs/2307.04821) #large language model</code></li>
<li>Summary: <p>We present this article as a small gesture in an attempt to counter what
appears to be exponentially growing hype around Artificial Intelligence (AI)
and its capabilities, and the distraction provided by the associated talk of
science-fiction scenarios that might arise if AI should become sentient and
super-intelligent. It may also help those outside of the field to become more
informed about some of the limitations of AI technology. In the current context
of popular discourse AI defaults to mean foundation and large language models
(LLMs) such as those used to create ChatGPT. This in itself is a
misrepresentation of the diversity, depth and volume of research, researchers,
and technology that truly represents the field of AI. AI being a field of
research that has existed in software artefacts since at least the 1950's. We
set out to highlight a number of limitations of LLMs, and in so doing highlight
that harms have already arisen and will continue to arise due to these
limitations. Along the way we also highlight some of the associated risks for
individuals and organisations in using this technology.
</p></li>
</ul>

<h3>Title: Secrets of RLHF in Large Language Models Part I: PPO. (arXiv:2307.04964v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04964">http://arxiv.org/abs/2307.04964</a></li>
<li>Code URL: <a href="https://github.com/openlmlab/moss-rlhf">https://github.com/openlmlab/moss-rlhf</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04964] Secrets of RLHF in Large Language Models Part I: PPO](http://arxiv.org/abs/2307.04964) #large language model</code></li>
<li>Summary: <p>Large language models (LLMs) have formulated a blueprint for the advancement
of artificial general intelligence. Its primary objective is to function as a
human-centric (helpful, honest, and harmless) assistant. Alignment with humans
assumes paramount significance, and reinforcement learning with human feedback
(RLHF) emerges as the pivotal technological paradigm underpinning this pursuit.
Current technical routes usually include \textbf{reward models} to measure
human preferences, \textbf{Proximal Policy Optimization} (PPO) to optimize
policy model outputs, and \textbf{process supervision} to improve step-by-step
reasoning capabilities. However, due to the challenges of reward design,
environment interaction, and agent training, coupled with huge trial and error
cost of large language models, there is a significant barrier for AI
researchers to motivate the development of technical alignment and safe landing
of LLMs. The stable training of RLHF has still been a puzzle. In the first
report, we dissect the framework of RLHF, re-evaluate the inner workings of
PPO, and explore how the parts comprising PPO algorithms impact policy agent
training. We identify policy constraints being the key factor for the effective
implementation of the PPO algorithm. Therefore, we explore the PPO-max, an
advanced version of PPO algorithm, to efficiently improve the training
stability of the policy model. Based on our main results, we perform a
comprehensive analysis of RLHF abilities compared with SFT models and ChatGPT.
The absence of open-source implementations has posed significant challenges to
the investigation of LLMs alignment. Therefore, we are eager to release
technical reports, reward models and PPO codes
</p></li>
</ul>

<h3>Title: Towards Understanding In-Context Learning with Contrastive Demonstrations and Saliency Maps. (arXiv:2307.05052v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05052">http://arxiv.org/abs/2307.05052</a></li>
<li>Code URL: <a href="https://github.com/paihengxu/xicl">https://github.com/paihengxu/xicl</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05052] Towards Understanding In-Context Learning with Contrastive Demonstrations and Saliency Maps](http://arxiv.org/abs/2307.05052) #large language model</code></li>
<li>Summary: <p>We investigate the role of various demonstration components in the in-context
learning (ICL) performance of large language models (LLMs). Specifically, we
explore the impacts of ground-truth labels, input distribution, and
complementary explanations, particularly when these are altered or perturbed.
We build on previous work, which offers mixed findings on how these elements
influence ICL. To probe these questions, we employ explainable NLP (XNLP)
methods and utilize saliency maps of contrastive demonstrations for both
qualitative and quantitative analysis. Our findings reveal that flipping
ground-truth labels significantly affects the saliency, though it's more
noticeable in larger LLMs. Our analysis of the input distribution at a granular
level reveals that changing sentiment-indicative terms in a sentiment analysis
task to neutral ones does not have as substantial an impact as altering
ground-truth labels. Finally, we find that the effectiveness of complementary
explanations in boosting ICL performance is task-dependent, with limited
benefits seen in sentiment analysis tasks compared to symbolic reasoning tasks.
These insights are critical for understanding the functionality of LLMs and
guiding the development of effective demonstrations, which is increasingly
relevant in light of the growing use of LLMs in applications such as ChatGPT.
Our research code is publicly available at https://github.com/paihengxu/XICL.
</p></li>
</ul>

<h3>Title: SuryaKiran at MEDIQA-Sum 2023: Leveraging LoRA for Clinical Dialogue Summarization. (arXiv:2307.05162v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05162">http://arxiv.org/abs/2307.05162</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05162] SuryaKiran at MEDIQA-Sum 2023: Leveraging LoRA for Clinical Dialogue Summarization](http://arxiv.org/abs/2307.05162) #large language model</code></li>
<li>Summary: <p>Finetuning Large Language Models helps improve the results for
domain-specific use cases. End-to-end finetuning of large language models is
time and resource intensive and has high storage requirements to store the
finetuned version of the large language model. Parameter Efficient Fine Tuning
(PEFT) methods address the time and resource challenges by keeping the large
language model as a fixed base and add additional layers, which the PEFT
methods finetune. This paper demonstrates the evaluation results for one such
PEFT method Low Rank Adaptation (LoRA), for Clinical Dialogue Summarization.
The evaluation results show that LoRA works at par with end-to-end finetuning
for a large language model. The paper presents the evaluations done for solving
both the Subtask A and B from ImageCLEFmedical
{https://www.imageclef.org/2023/medical}
</p></li>
</ul>

<h3>Title: GujiBERT and GujiGPT: Construction of Intelligent Information Processing Foundation Language Models for Ancient Texts. (arXiv:2307.05354v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05354">http://arxiv.org/abs/2307.05354</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05354] GujiBERT and GujiGPT: Construction of Intelligent Information Processing Foundation Language Models for Ancient Texts](http://arxiv.org/abs/2307.05354) #large language model</code></li>
<li>Summary: <p>In the context of the rapid development of large language models, we have
meticulously trained and introduced the GujiBERT and GujiGPT language models,
which are foundational models specifically designed for intelligent information
processing of ancient texts. These models have been trained on an extensive
dataset that encompasses both simplified and traditional Chinese characters,
allowing them to effectively handle various natural language processing tasks
related to ancient books, including but not limited to automatic sentence
segmentation, punctuation, word segmentation, part-of-speech tagging, entity
recognition, and automatic translation. Notably, these models have exhibited
exceptional performance across a range of validation tasks using publicly
available datasets. Our research findings highlight the efficacy of employing
self-supervised methods to further train the models using classical text
corpora, thus enhancing their capability to tackle downstream tasks. Moreover,
it is worth emphasizing that the choice of font, the scale of the corpus, and
the initial model selection all exert significant influence over the ultimate
experimental outcomes. To cater to the diverse text processing preferences of
researchers in digital humanities and linguistics, we have developed three
distinct categories comprising a total of nine model variations. We believe
that by sharing these foundational language models specialized in the domain of
ancient texts, we can facilitate the intelligent processing and scholarly
exploration of ancient literary works and, consequently, contribute to the
global dissemination of China's rich and esteemed traditional culture in this
new era.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: Rapid Deforestation and Burned Area Detection using Deep Multimodal Learning on Satellite Imagery. (arXiv:2307.04916v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04916">http://arxiv.org/abs/2307.04916</a></li>
<li>Code URL: <a href="https://github.com/h2oai/cvpr-multiearth-deforestation-segmentation">https://github.com/h2oai/cvpr-multiearth-deforestation-segmentation</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04916] Rapid Deforestation and Burned Area Detection using Deep Multimodal Learning on Satellite Imagery](http://arxiv.org/abs/2307.04916) #segmentation</code></li>
<li>Summary: <p>Deforestation estimation and fire detection in the Amazon forest poses a
significant challenge due to the vast size of the area and the limited
accessibility. However, these are crucial problems that lead to severe
environmental consequences, including climate change, global warming, and
biodiversity loss. To effectively address this problem, multimodal satellite
imagery and remote sensing offer a promising solution for estimating
deforestation and detecting wildfire in the Amazonia region. This research
paper introduces a new curated dataset and a deep learning-based approach to
solve these problems using convolutional neural networks (CNNs) and
comprehensive data processing techniques. Our dataset includes curated images
and diverse channel bands from Sentinel, Landsat, VIIRS, and MODIS satellites.
We design the dataset considering different spatial and temporal resolution
requirements. Our method successfully achieves high-precision deforestation
estimation and burned area detection on unseen images from the region. Our
code, models and dataset are open source:
https://github.com/h2oai/cvpr-multiearth-deforestation-segmentation
</p></li>
</ul>

<h3>Title: PKU-GoodsAD: A Supermarket Goods Dataset for Unsupervised Anomaly Detection and Segmentation. (arXiv:2307.04956v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.04956">http://arxiv.org/abs/2307.04956</a></li>
<li>Code URL: <a href="https://github.com/jianzhang96/goodsad">https://github.com/jianzhang96/goodsad</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.04956] PKU-GoodsAD: A Supermarket Goods Dataset for Unsupervised Anomaly Detection and Segmentation](http://arxiv.org/abs/2307.04956) #segmentation</code></li>
<li>Summary: <p>Visual anomaly detection is essential and commonly used for many tasks in the
field of computer vision. Recent anomaly detection datasets mainly focus on
industrial automated inspection, medical image analysis and video surveillance.
In order to broaden the application and research of anomaly detection in
unmanned supermarkets and smart manufacturing, we introduce the supermarket
goods anomaly detection (GoodsAD) dataset. It contains 6124 high-resolution
images of 484 different appearance goods divided into 6 categories. Each
category contains several common different types of anomalies such as
deformation, surface damage and opened. Anomalies contain both texture changes
and structural changes. It follows the unsupervised setting and only normal
(defect-free) images are used for training. Pixel-precise ground truth regions
are provided for all anomalies. Moreover, we also conduct a thorough evaluation
of current state-of-the-art unsupervised anomaly detection methods. This
initial benchmark indicates that some methods which perform well on the
industrial anomaly detection dataset (e.g., MVTec AD), show poor performance on
our dataset. This is a comprehensive, multi-object dataset for supermarket
goods anomaly detection that focuses on real-world applications.
</p></li>
</ul>

<h3>Title: Test-Time Training on Video Streams. (arXiv:2307.05014v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05014">http://arxiv.org/abs/2307.05014</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05014] Test-Time Training on Video Streams](http://arxiv.org/abs/2307.05014) #segmentation</code></li>
<li>Summary: <p>Prior work has established test-time training (TTT) as a general framework to
further improve a trained model at test time. Before making a prediction on
each test instance, the model is trained on the same instance using a
self-supervised task, such as image reconstruction with masked autoencoders. We
extend TTT to the streaming setting, where multiple test instances - video
frames in our case - arrive in temporal order. Our extension is online TTT: The
current model is initialized from the previous model, then trained on the
current frame and a small window of frames immediately before. Online TTT
significantly outperforms the fixed-model baseline for four tasks, on three
real-world datasets. The relative improvement is 45% and 66% for instance and
panoptic segmentation. Surprisingly, online TTT also outperforms its offline
variant that accesses more information, training on all frames from the entire
test video regardless of temporal order. This differs from previous findings
using synthetic videos. We conceptualize locality as the advantage of online
over offline TTT. We analyze the role of locality with ablations and a theory
based on bias-variance trade-off.
</p></li>
</ul>

<h3>Title: TRansPose: Large-Scale Multispectral Dataset for Transparent Object. (arXiv:2307.05016v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05016">http://arxiv.org/abs/2307.05016</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05016] TRansPose: Large-Scale Multispectral Dataset for Transparent Object](http://arxiv.org/abs/2307.05016) #segmentation</code></li>
<li>Summary: <p>Transparent objects are encountered frequently in our daily lives, yet
recognizing them poses challenges for conventional vision sensors due to their
unique material properties, not being well perceived from RGB or depth cameras.
Overcoming this limitation, thermal infrared cameras have emerged as a
solution, offering improved visibility and shape information for transparent
objects. In this paper, we present TRansPose, the first large-scale
multispectral dataset that combines stereo RGB-D, thermal infrared (TIR)
images, and object poses to promote transparent object research. The dataset
includes 99 transparent objects, encompassing 43 household items, 27 recyclable
trashes, 29 chemical laboratory equivalents, and 12 non-transparent objects. It
comprises a vast collection of 333,819 images and 4,000,056 annotations,
providing instance-level segmentation masks, ground-truth poses, and completed
depth information. The data was acquired using a FLIR A65 thermal infrared
(TIR) camera, two Intel RealSense L515 RGB-D cameras, and a Franka Emika Panda
robot manipulator. Spanning 87 sequences, TRansPose covers various challenging
real-life scenarios, including objects filled with water, diverse lighting
conditions, heavy clutter, non-transparent or translucent containers, objects
in plastic bags, and multi-stacked objects. TRansPose dataset can be accessed
from the following link: https://sites.google.com/view/transpose-dataset
</p></li>
</ul>

<h3>Title: Estimating label quality and errors in semantic segmentation data via any model. (arXiv:2307.05080v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05080">http://arxiv.org/abs/2307.05080</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05080] Estimating label quality and errors in semantic segmentation data via any model](http://arxiv.org/abs/2307.05080) #segmentation</code></li>
<li>Summary: <p>The labor-intensive annotation process of semantic segmentation datasets is
often prone to errors, since humans struggle to label every pixel correctly. We
study algorithms to automatically detect such annotation errors, in particular
methods to score label quality, such that the images with the lowest scores are
least likely to be correctly labeled. This helps prioritize what data to review
in order to ensure a high-quality training/evaluation dataset, which is
critical in sensitive applications such as medical imaging and autonomous
vehicles. Widely applicable, our label quality scores rely on probabilistic
predictions from a trained segmentation model -- any model architecture and
training procedure can be utilized. Here we study 7 different label quality
scoring methods used in conjunction with a DeepLabV3+ or a FPN segmentation
model to detect annotation errors in a version of the SYNTHIA dataset.
Precision-recall evaluations reveal a score -- the soft-minimum of the
model-estimated likelihoods of each pixel's annotated class -- that is
particularly effective to identify images that are mislabeled, across multiple
types of annotation error.
</p></li>
</ul>

<h3>Title: Automatic Generation of Semantic Parts for Face Image Synthesis. (arXiv:2307.05317v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05317">http://arxiv.org/abs/2307.05317</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05317] Automatic Generation of Semantic Parts for Face Image Synthesis](http://arxiv.org/abs/2307.05317) #segmentation</code></li>
<li>Summary: <p>Semantic image synthesis (SIS) refers to the problem of generating realistic
imagery given a semantic segmentation mask that defines the spatial layout of
object classes. Most of the approaches in the literature, other than the
quality of the generated images, put effort in finding solutions to increase
the generation diversity in terms of style i.e. texture. However, they all
neglect a different feature, which is the possibility of manipulating the
layout provided by the mask. Currently, the only way to do so is manually by
means of graphical users interfaces. In this paper, we describe a network
architecture to address the problem of automatically manipulating or generating
the shape of object classes in semantic segmentation masks, with specific focus
on human faces. Our proposed model allows embedding the mask class-wise into a
latent space where each class embedding can be independently edited. Then, a
bi-directional LSTM block and a convolutional decoder output a new, locally
manipulated mask. We report quantitative and qualitative results on the
CelebMask-HQ dataset, which show our model can both faithfully reconstruct and
modify a segmentation mask at the class level. Also, we show our model can be
put before a SIS generator, opening the way to a fully automatic generation
control of both shape and texture. Code available at
https://github.com/TFonta/Semantic-VAE.
</p></li>
</ul>

<h3>Title: 3D detection of roof sections from a single satellite image and application to LOD2-building reconstruction. (arXiv:2307.05409v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05409">http://arxiv.org/abs/2307.05409</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05409] 3D detection of roof sections from a single satellite image and application to LOD2-building reconstruction](http://arxiv.org/abs/2307.05409) #segmentation</code></li>
<li>Summary: <p>Reconstructing urban areas in 3D out of satellite raster images has been a
long-standing and challenging goal of both academical and industrial research.
The rare methods today achieving this objective at a Level Of Details $2$ rely
on procedural approaches based on geometry, and need stereo images and/or LIDAR
data as input. We here propose a method for urban 3D reconstruction named
KIBS(\textit{Keypoints Inference By Segmentation}), which comprises two novel
features: i) a full deep learning approach for the 3D detection of the roof
sections, and ii) only one single (non-orthogonal) satellite raster image as
model input. This is achieved in two steps: i) by a Mask R-CNN model performing
a 2D segmentation of the buildings' roof sections, and after blending these
latter segmented pixels within the RGB satellite raster image, ii) by another
identical Mask R-CNN model inferring the heights-to-ground of the roof
sections' corners via panoptic segmentation, unto full 3D reconstruction of the
buildings and city. We demonstrate the potential of the KIBS method by
reconstructing different urban areas in a few minutes, with a Jaccard index for
the 2D segmentation of individual roof sections of $88.55\%$ and $75.21\%$ on
our two data sets resp., and a height's mean error of such correctly segmented
pixels for the 3D reconstruction of $1.60$ m and $2.06$ m on our two data sets
resp., hence within the LOD2 precision range.
</p></li>
</ul>

<h3>Title: Argumentative Segmentation Enhancement for Legal Summarization. (arXiv:2307.05081v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.05081">http://arxiv.org/abs/2307.05081</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.05081] Argumentative Segmentation Enhancement for Legal Summarization](http://arxiv.org/abs/2307.05081) #segmentation</code></li>
<li>Summary: <p>We use the combination of argumentative zoning [1] and a legal argumentative
scheme to create legal argumentative segments. Based on the argumentative
segmentation, we propose a novel task of classifying argumentative segments of
legal case decisions. GPT-3.5 is used to generate summaries based on
argumentative segments. In terms of automatic evaluation metrics, our method
generates higher quality argumentative summaries while leaving out less
relevant context as compared to GPT-4 and non-GPT models.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
