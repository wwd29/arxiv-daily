<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-10-14</h1>
<h3>Title: Self-Attention Mechanism in Multimodal Context for Banking Transaction Flow</h3>
<ul>
<li><strong>Authors: </strong>Cyrile Delestre, Yoann Sola</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08243">https://arxiv.org/abs/2410.08243</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08243">https://arxiv.org/pdf/2410.08243</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08243]] Self-Attention Mechanism in Multimodal Context for Banking Transaction Flow(https://arxiv.org/abs/2410.08243)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Banking Transaction Flow (BTF) is a sequential data found in a number of banking activities such as marketing, credit risk or banking fraud. It is a multimodal data composed of three modalities: a date, a numerical value and a wording. We propose in this work an application of self-attention mechanism to the processing of BTFs. We trained two general models on a large amount of BTFs in a self-supervised way: one RNN-based model and one Transformer-based model. We proposed a specific tokenization in order to be able to process BTFs. The performance of these two models was evaluated on two banking downstream tasks: a transaction categorization task and a credit risk task. The results show that fine-tuning these two pre-trained models allowed to perform better than the state-of-the-art approaches for both tasks.</li>
</ul>

<h3>Title: RAB$^2$-DEF: Dynamic and explainable defense against adversarial attacks in Federated Learning to fair poor clients</h3>
<ul>
<li><strong>Authors: </strong>Nuria Rodríguez-Barroso, M. Victoria Luzón, Francisco Herrera</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08244">https://arxiv.org/abs/2410.08244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08244">https://arxiv.org/pdf/2410.08244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08244]] RAB$^2$-DEF: Dynamic and explainable defense against adversarial attacks in Federated Learning to fair poor clients(https://arxiv.org/abs/2410.08244)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, attack, federate, fair, explainability</a></li>
<li><strong>Abstract: </strong>At the same time that artificial intelligence is becoming popular, concern and the need for regulation is growing, including among other requirements the data privacy. In this context, Federated Learning is proposed as a solution to data privacy concerns derived from different source data scenarios due to its distributed learning. The defense mechanisms proposed in literature are just focused on defending against adversarial attacks and the performance, leaving aside other important qualities such as explainability, fairness to poor quality clients, dynamism in terms of attacks configuration and generality in terms of being resilient against different kinds of attacks. In this work, we propose RAB$^2$-DEF, a $\textbf{r}$esilient $\textbf{a}$gainst $\textbf{b}\text{yzantine}$ and $\textbf{b}$ackdoor attacks which is $\textbf{d}$ynamic, $\textbf{e}$xplainable and $\textbf{f}$air to poor clients using local linear explanations. We test the performance of RAB$^2$-DEF in image datasets and both byzantine and backdoor attacks considering the state-of-the-art defenses and achieve that RAB$^2$-DEF is a proper defense at the same time that it boosts the other qualities towards trustworthy artificial intelligence.</li>
</ul>

<h3>Title: Flex-MoE: Modeling Arbitrary Modality Combination via the Flexible Mixture-of-Experts</h3>
<ul>
<li><strong>Authors: </strong>Sukwon Yun, Inyoung Choi, Jie Peng, Yangfan Wu, Jingxuan Bao, Qiyiwen Zhang, Jiayi Xin, Qi Long, Tianlong Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08245">https://arxiv.org/abs/2410.08245</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08245">https://arxiv.org/pdf/2410.08245</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08245]] Flex-MoE: Modeling Arbitrary Modality Combination via the Flexible Mixture-of-Experts(https://arxiv.org/abs/2410.08245)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multimodal learning has gained increasing importance across various fields, offering the ability to integrate data from diverse sources such as images, text, and personalized records, which are frequently observed in medical domains. However, in scenarios where some modalities are missing, many existing frameworks struggle to accommodate arbitrary modality combinations, often relying heavily on a single modality or complete data. This oversight of potential modality combinations limits their applicability in real-world situations. To address this challenge, we propose Flex-MoE (Flexible Mixture-of-Experts), a new framework designed to flexibly incorporate arbitrary modality combinations while maintaining robustness to missing data. The core idea of Flex-MoE is to first address missing modalities using a new missing modality bank that integrates observed modality combinations with the corresponding missing ones. This is followed by a uniquely designed Sparse MoE framework. Specifically, Flex-MoE first trains experts using samples with all modalities to inject generalized knowledge through the generalized router ($\mathcal{G}$-Router). The $\mathcal{S}$-Router then specializes in handling fewer modality combinations by assigning the top-1 gate to the expert corresponding to the observed modality combination. We evaluate Flex-MoE on the ADNI dataset, which encompasses four modalities in the Alzheimer's Disease domain, as well as on the MIMIC-IV dataset. The results demonstrate the effectiveness of Flex-MoE highlighting its ability to model arbitrary modality combinations in diverse missing modality scenarios. Code is available at this https URL.</li>
</ul>

<h3>Title: Federated Graph Learning for Cross-Domain Recommendation</h3>
<ul>
<li><strong>Authors: </strong>Ziqi Yang, Zhaopeng Peng, Zihui Wang, Jianzhong Qi, Chaochao Chen, Weike Pan, Chenglu Wen, Cheng Wang, Xiaoliang Fan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08249">https://arxiv.org/abs/2410.08249</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08249">https://arxiv.org/pdf/2410.08249</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08249]] Federated Graph Learning for Cross-Domain Recommendation(https://arxiv.org/abs/2410.08249)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, extraction, federate</a></li>
<li><strong>Abstract: </strong>Cross-domain recommendation (CDR) offers a promising solution to the data sparsity problem by enabling knowledge transfer across source and target domains. However, many recent CDR models overlook crucial issues such as privacy as well as the risk of negative transfer (which negatively impact model performance), especially in multi-domain settings. To address these challenges, we propose FedGCDR, a novel federated graph learning framework that securely and effectively leverages positive knowledge from multiple source domains. First, we design a positive knowledge transfer module that ensures privacy during inter-domain knowledge transmission. This module employs differential privacy-based knowledge extraction combined with a feature mapping mechanism, transforming source domain embeddings from federated graph attention networks into reliable domain knowledge. Second, we design a knowledge activation module to filter out potential harmful or conflicting knowledge from source domains, addressing the issues of negative transfer. This module enhances target domain training by expanding the graph of the target domain to generate reliable domain attentions and fine-tunes the target model for improved negative knowledge filtering and more accurate predictions. We conduct extensive experiments on 16 popular domains of the Amazon dataset, demonstrating that FedGCDR significantly outperforms state-of-the-art methods.</li>
</ul>

<h3>Title: Generalization from Starvation: Hints of Universality in LLM Knowledge Graph Learning</h3>
<ul>
<li><strong>Authors: </strong>David D. Baek, Yuxiao Li, Max Tegmark</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08255">https://arxiv.org/abs/2410.08255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08255">https://arxiv.org/pdf/2410.08255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08255]] Generalization from Starvation: Hints of Universality in LLM Knowledge Graph Learning(https://arxiv.org/abs/2410.08255)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Motivated by interpretability and reliability, we investigate how neural networks represent knowledge during graph learning, We find hints of universality, where equivalent representations are learned across a range of model sizes (from $10^2$ to $10^9$ parameters) and contexts (MLP toy models, LLM in-context learning and LLM training). We show that these attractor representations optimize generalization to unseen examples by exploiting properties of knowledge graph relations (e.g. symmetry and meta-transitivity). We find experimental support for such universality by showing that LLMs and simpler neural networks can be stitched, i.e., by stitching the first part of one model to the last part of another, mediated only by an affine or almost affine transformation. We hypothesize that this dynamic toward simplicity and generalization is driven by "intelligence from starvation": where overfitting is minimized by pressure to minimize the use of resources that are either scarce or competed for against other tasks.</li>
</ul>

<h3>Title: Neural Material Adaptor for Visual Grounding of Intrinsic Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Junyi Cao, Shanyan Guan, Yanhao Ge, Wei Li, Xiaokang Yang, Chao Ma</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08257">https://arxiv.org/abs/2410.08257</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08257">https://arxiv.org/pdf/2410.08257</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08257]] Neural Material Adaptor for Visual Grounding of Intrinsic Dynamics(https://arxiv.org/abs/2410.08257)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>While humans effortlessly discern intrinsic dynamics and adapt to new scenarios, modern AI systems often struggle. Current methods for visual grounding of dynamics either use pure neural-network-based simulators (black box), which may violate physical laws, or traditional physical simulators (white box), which rely on expert-defined equations that may not fully capture actual dynamics. We propose the Neural Material Adaptor (NeuMA), which integrates existing physical laws with learned corrections, facilitating accurate learning of actual dynamics while maintaining the generalizability and interpretability of physical priors. Additionally, we propose Particle-GS, a particle-driven 3D Gaussian Splatting variant that bridges simulation and observed images, allowing back-propagate image gradients to optimize the simulator. Comprehensive experiments on various dynamics in terms of grounded particle accuracy, dynamic rendering quality, and generalization ability demonstrate that NeuMA can accurately capture intrinsic dynamics.</li>
</ul>

<h3>Title: In Search of Forgotten Domain Generalization</h3>
<ul>
<li><strong>Authors: </strong>Prasanna Mayilvahanan, Roland S. Zimmermann, Thaddäus Wiedemer, Evgenia Rusak, Attila Juhos, Matthias Bethge, Wieland Brendel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08258">https://arxiv.org/abs/2410.08258</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08258">https://arxiv.org/pdf/2410.08258</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08258]] In Search of Forgotten Domain Generalization(https://arxiv.org/abs/2410.08258)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Out-of-Domain (OOD) generalization is the ability of a model trained on one or more domains to generalize to unseen domains. In the ImageNet era of computer vision, evaluation sets for measuring a model's OOD performance were designed to be strictly OOD with respect to style. However, the emergence of foundation models and expansive web-scale datasets has obfuscated this evaluation process, as datasets cover a broad range of domains and risk test domain contamination. In search of the forgotten domain generalization, we create large-scale datasets subsampled from LAION -- LAION-Natural and LAION-Rendition -- that are strictly OOD to corresponding ImageNet and DomainNet test sets in terms of style. Training CLIP models on these datasets reveals that a significant portion of their performance is explained by in-domain examples. This indicates that the OOD generalization challenges from the ImageNet era still prevail and that training on web-scale data merely creates the illusion of OOD generalization. Furthermore, through a systematic exploration of combining natural and rendition datasets in varying proportions, we identify optimal mixing ratios for model generalization across these domains. Our datasets and results re-enable meaningful assessment of OOD robustness at scale -- a crucial prerequisite for improving model robustness.</li>
</ul>

<h3>Title: Quantifying Jitter Transfer for Differential Measurement: Enhancing Security of Oscillator-Based TRNGs</h3>
<ul>
<li><strong>Authors: </strong>David Lubicz, Maciej Skorski</a></li>
<li><strong>Subjects: </strong>cs.CR, math.ST, stat.CO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08259">https://arxiv.org/abs/2410.08259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08259">https://arxiv.org/pdf/2410.08259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08259]] Quantifying Jitter Transfer for Differential Measurement: Enhancing Security of Oscillator-Based TRNGs(https://arxiv.org/abs/2410.08259)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>The aim of this paper is to describe a way to improve the reliability of the measurement of the statistical parameters of the phase noise in a multi-ring oscillator-based TRNG. This is necessary to guarantee that the entropy rate is within the bounds prescribed by standards or security specifications. According to the literature, to filter out global noises which may strongly affect the measurement of the phase noise parameters, it is necessary to perform a differential measure. But a differential measurement only returns the parameters of the phase noise resulting of the composition of the noises of two oscillators whereas jitters parameters of individual oscillators are required to compute the entropy rate of a multi-ring oscillator-based TRNG. In this paper, we revisit the "jitter transfer principle" in conjunction with a tweaked design of an oscillator based TRNG to enjoy the precision of differential measures and, at the same time, obtain jitter parameters of individual oscillators. We show the relevance of our method with simulations and experiments with hardware implementations.</li>
</ul>

<h3>Title: Meissonic: Revitalizing Masked Generative Transformers for Efficient High-Resolution Text-to-Image Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Jinbin Bai, Tian Ye, Wei Chow, Enxin Song, Qing-Guo Chen, Xiangtai Li, Zhen Dong, Lei Zhu, Shuicheng Yan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08261">https://arxiv.org/abs/2410.08261</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08261">https://arxiv.org/pdf/2410.08261</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08261]] Meissonic: Revitalizing Masked Generative Transformers for Efficient High-Resolution Text-to-Image Synthesis(https://arxiv.org/abs/2410.08261)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models, such as Stable Diffusion, have made significant strides in visual generation, yet their paradigm remains fundamentally different from autoregressive language models, complicating the development of unified language-vision models. Recent efforts like LlamaGen have attempted autoregressive image generation using discrete VQVAE tokens, but the large number of tokens involved renders this approach inefficient and slow. In this work, we present Meissonic, which elevates non-autoregressive masked image modeling (MIM) text-to-image to a level comparable with state-of-the-art diffusion models like SDXL. By incorporating a comprehensive suite of architectural innovations, advanced positional encoding strategies, and optimized sampling conditions, Meissonic substantially improves MIM's performance and efficiency. Additionally, we leverage high-quality training data, integrate micro-conditions informed by human preference scores, and employ feature compression layers to further enhance image fidelity and resolution. Our model not only matches but often exceeds the performance of existing models like SDXL in generating high-quality, high-resolution images. Extensive experiments validate Meissonic's capabilities, demonstrating its potential as a new standard in text-to-image synthesis. We release a model checkpoint capable of producing $1024 \times 1024$ resolution images.</li>
</ul>

<h3>Title: Can Looped Transformers Learn to Implement Multi-step Gradient Descent for In-context Learning?</h3>
<ul>
<li><strong>Authors: </strong>Khashayar Gatmiry, Nikunj Saunshi, Sashank J. Reddi, Stefanie Jegelka, Sanjiv Kumar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08292">https://arxiv.org/abs/2410.08292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08292">https://arxiv.org/pdf/2410.08292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08292]] Can Looped Transformers Learn to Implement Multi-step Gradient Descent for In-context Learning?(https://arxiv.org/abs/2410.08292)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The remarkable capability of Transformers to do reasoning and few-shot learning, without any fine-tuning, is widely conjectured to stem from their ability to implicitly simulate a multi-step algorithms -- such as gradient descent -- with their weights in a single forward pass. Recently, there has been progress in understanding this complex phenomenon from an expressivity point of view, by demonstrating that Transformers can express such multi-step algorithms. However, our knowledge about the more fundamental aspect of its learnability, beyond single layer models, is very limited. In particular, can training Transformers enable convergence to algorithmic solutions? In this work we resolve this for in-context linear regression with linear looped Transformers -- a multi-layer model with weight sharing that is conjectured to have an inductive bias to learn fix-point iterative algorithms. More specifically, for this setting we show that the global minimizer of the population training loss implements multi-step preconditioned gradient descent, with a preconditioner that adapts to the data distribution. Furthermore, we show a fast convergence for gradient flow on the regression loss, despite the non-convexity of the landscape, by proving a novel gradient dominance condition. To our knowledge, this is the first theoretical analysis for multi-layer Transformer in this setting. We further validate our theoretical findings through synthetic experiments.</li>
</ul>

<h3>Title: Impact of Missing Values in Machine Learning: A Comprehensive Analysis</h3>
<ul>
<li><strong>Authors: </strong>Abu Fuad Ahmad, Md Shohel Sayeed, Khaznah Alshammari, Istiaque Ahmed</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08295">https://arxiv.org/abs/2410.08295</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08295">https://arxiv.org/pdf/2410.08295</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08295]] Impact of Missing Values in Machine Learning: A Comprehensive Analysis(https://arxiv.org/abs/2410.08295)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Machine learning (ML) has become a ubiquitous tool across various domains of data mining and big data analysis. The efficacy of ML models depends heavily on high-quality datasets, which are often complicated by the presence of missing values. Consequently, the performance and generalization of ML models are at risk in the face of such datasets. This paper aims to examine the nuanced impact of missing values on ML workflows, including their types, causes, and consequences. Our analysis focuses on the challenges posed by missing values, including biased inferences, reduced predictive power, and increased computational burdens. The paper further explores strategies for handling missing values, including imputation techniques and removal strategies, and investigates how missing values affect model evaluation metrics and introduces complexities in cross-validation and model selection. The study employs case studies and real-world examples to illustrate the practical implications of addressing missing values. Finally, the discussion extends to future research directions, emphasizing the need for handling missing values ethically and transparently. The primary goal of this paper is to provide insights into the pervasive impact of missing values on ML models and guide practitioners toward effective strategies for achieving robust and reliable model outcomes.</li>
</ul>

<h3>Title: Privately Learning from Graphs with Applications in Fine-tuning Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haoteng Yin, Rongzhe Wei, Eli Chien, Pan Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08299">https://arxiv.org/abs/2410.08299</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08299">https://arxiv.org/pdf/2410.08299</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08299]] Privately Learning from Graphs with Applications in Fine-tuning Large Language Models(https://arxiv.org/abs/2410.08299)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, large language model</a></li>
<li><strong>Abstract: </strong>Graphs offer unique insights into relationships and interactions between entities, complementing data modalities like text, images, and videos. By incorporating relational information from graph data, AI models can extend their capabilities beyond traditional tasks. However, relational data in sensitive domains such as finance and healthcare often contain private information, making privacy preservation crucial. Existing privacy-preserving methods, such as DP-SGD, which rely on gradient decoupling assumptions, are not well-suited for relational learning due to the inherent dependencies between coupled training samples. To address this challenge, we propose a privacy-preserving relational learning pipeline that decouples dependencies in sampled relations during training, ensuring differential privacy through a tailored application of DP-SGD. We apply this method to fine-tune large language models (LLMs) on sensitive graph data, and tackle the associated computational complexities. Our approach is evaluated on LLMs of varying sizes (e.g., BERT, Llama2) using real-world relational data from four text-attributed graphs. The results demonstrate significant improvements in relational learning tasks, all while maintaining robust privacy guarantees during training. Additionally, we explore the trade-offs between privacy, utility, and computational efficiency, offering insights into the practical deployment of our approach. Code is available at this https URL.</li>
</ul>

<h3>Title: Global Lyapunov functions: a long-standing open problem in mathematics, with symbolic transformers</h3>
<ul>
<li><strong>Authors: </strong>Alberto Alfarano, François Charton, Amaury Hayat</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08304">https://arxiv.org/abs/2410.08304</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08304">https://arxiv.org/pdf/2410.08304</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08304]] Global Lyapunov functions: a long-standing open problem in mathematics, with symbolic transformers(https://arxiv.org/abs/2410.08304)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Despite their spectacular progress, language models still struggle on complex reasoning tasks, such as advanced mathematics. We consider a long-standing open problem in mathematics: discovering a Lyapunov function that ensures the global stability of a dynamical system. This problem has no known general solution, and algorithmic solvers only exist for some small polynomial systems. We propose a new method for generating synthetic training samples from random solutions, and show that sequence-to-sequence transformers trained on such datasets perform better than algorithmic solvers and humans on polynomial systems, and can discover new Lyapunov functions for non-polynomial systems.</li>
</ul>

<h3>Title: Randomized Asymmetric Chain of LoRA: The First Meaningful Theoretical Framework for Low-Rank Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Grigory Malinovsky, Umberto Michieli, Hasan Abed Al Kader Hammoud, Taha Ceritli, Hayder Elesedy, Mete Ozay, Peter Richtárik</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08305">https://arxiv.org/abs/2410.08305</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08305">https://arxiv.org/pdf/2410.08305</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08305]] Randomized Asymmetric Chain of LoRA: The First Meaningful Theoretical Framework for Low-Rank Adaptation(https://arxiv.org/abs/2410.08305)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Fine-tuning has become a popular approach to adapting large foundational models to specific tasks. As the size of models and datasets grows, parameter-efficient fine-tuning techniques are increasingly important. One of the most widely used methods is Low-Rank Adaptation (LoRA), with adaptation update expressed as the product of two low-rank matrices. While LoRA was shown to possess strong performance in fine-tuning, it often under-performs when compared to full-parameter fine-tuning (FPFT). Although many variants of LoRA have been extensively studied empirically, their theoretical optimization analysis is heavily under-explored. The starting point of our work is a demonstration that LoRA and its two extensions, Asymmetric LoRA and Chain of LoRA, indeed encounter convergence issues. To address these issues, we propose Randomized Asymmetric Chain of LoRA (RAC-LoRA) -- a general optimization framework that rigorously analyzes the convergence rates of LoRA-based methods. Our approach inherits the empirical benefits of LoRA-style heuristics, but introduces several small but important algorithmic modifications which turn it into a provably convergent method. Our framework serves as a bridge between FPFT and low-rank adaptation. We provide provable guarantees of convergence to the same solution as FPFT, along with the rate of convergence. Additionally, we present a convergence analysis for smooth, non-convex loss functions, covering gradient descent, stochastic gradient descent, and federated learning settings. Our theoretical findings are supported by experimental results.</li>
</ul>

<h3>Title: Machine Learning for Missing Value Imputation</h3>
<ul>
<li><strong>Authors: </strong>Abu Fuad Ahmad, Khaznah Alshammari, Istiaque Ahmed, MD Shohel Sayed</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08308">https://arxiv.org/abs/2410.08308</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08308">https://arxiv.org/pdf/2410.08308</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08308]] Machine Learning for Missing Value Imputation(https://arxiv.org/abs/2410.08308)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In recent times, a considerable number of research studies have been carried out to address the issue of Missing Value Imputation (MVI). MVI aims to provide a primary solution for datasets that have one or more missing attribute values. The advancements in Artificial Intelligence (AI) drive the development of new and improved machine learning (ML) algorithms and methods. The advancements in ML have opened up significant opportunities for effectively imputing these missing values. The main objective of this article is to conduct a comprehensive and rigorous review, as well as analysis, of the state-of-the-art ML applications in MVI methods. This analysis seeks to enhance researchers' understanding of the subject and facilitate the development of robust and impactful interventions in data preprocessing for Data Analytics. The review is performed following the Preferred Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA) technique. More than 100 articles published between 2014 and 2023 are critically reviewed, considering the methods and findings. Furthermore, the latest literature is examined to scrutinize the trends in MVI methods and their evaluation. The accomplishments and limitations of the existing literature are discussed in detail. The survey concludes by identifying the current gaps in research and providing suggestions for future research directions and emerging trends in related fields of interest.</li>
</ul>

<h3>Title: Dynamics of Concept Learning and Compositional Generalization</h3>
<ul>
<li><strong>Authors: </strong>Yongyi Yang, Core Francisco Park, Ekdeep Singh Lubana, Maya Okawa, Wei Hu, Hidenori Tanaka</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08309">https://arxiv.org/abs/2410.08309</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08309">https://arxiv.org/pdf/2410.08309</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08309]] Dynamics of Concept Learning and Compositional Generalization(https://arxiv.org/abs/2410.08309)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Prior work has shown that text-conditioned diffusion models can learn to identify and manipulate primitive concepts underlying a compositional data-generating process, enabling generalization to entirely novel, out-of-distribution compositions. Beyond performance evaluations, these studies develop a rich empirical phenomenology of learning dynamics, showing that models generalize sequentially, respecting the compositional hierarchy of the data-generating process. Moreover, concept-centric structures within the data significantly influence a model's speed of learning the ability to manipulate a concept. In this paper, we aim to better characterize these empirical results from a theoretical standpoint. Specifically, we propose an abstraction of prior work's compositional generalization problem by introducing a structured identity mapping (SIM) task, where a model is trained to learn the identity mapping on a Gaussian mixture with structurally organized centroids. We mathematically analyze the learning dynamics of neural networks trained on this SIM task and show that, despite its simplicity, SIM's learning dynamics capture and help explain key empirical observations on compositional generalization with diffusion models identified in prior work. Our theory also offers several new insights -- e.g., we find a novel mechanism for non-monotonic learning dynamics of test loss in early phases of training. We validate our new predictions by training a text-conditioned diffusion model, bridging our simplified framework and complex generative models. Overall, this work establishes the SIM task as a meaningful theoretical abstraction of concept learning dynamics in modern generative models.</li>
</ul>

<h3>Title: HyperDPO: Hypernetwork-based Multi-Objective Fine-Tuning Framework</h3>
<ul>
<li><strong>Authors: </strong>Yinuo Ren, Tesi Xiao, Michael Shavlovsky, Lexing Ying, Holakou Rahmanian</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08316">https://arxiv.org/abs/2410.08316</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08316">https://arxiv.org/pdf/2410.08316</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08316]] HyperDPO: Hypernetwork-based Multi-Objective Fine-Tuning Framework(https://arxiv.org/abs/2410.08316)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In LLM alignment and many other ML applications, one often faces the Multi-Objective Fine-Tuning (MOFT) problem, i.e. fine-tuning an existing model with datasets labeled w.r.t. different objectives simultaneously. To address the challenge, we propose the HyperDPO framework, a hypernetwork-based approach that extends the Direct Preference Optimization (DPO) technique, originally developed for efficient LLM alignment with preference data, to accommodate the MOFT settings. By substituting the Bradley-Terry-Luce model in DPO with the Plackett-Luce model, our framework is capable of handling a wide range of MOFT tasks that involve listwise ranking datasets. Compared with previous approaches, HyperDPO enjoys an efficient one-shot training process for profiling the Pareto front of auxiliary objectives, and offers flexible post-training control over trade-offs. Additionally, we propose a novel Hyper Prompt Tuning design, that conveys continuous weight across objectives to transformer-based models without altering their architecture. We demonstrate the effectiveness and efficiency of the HyperDPO framework through its applications to various tasks, including Learning-to-Rank (LTR) and LLM alignment, highlighting its viability for large-scale ML deployments.</li>
</ul>

<h3>Title: Neural Architecture Search of Hybrid Models for NPU-CIM Heterogeneous AR/VR Devices</h3>
<ul>
<li><strong>Authors: </strong>Yiwei Zhao, Ziyun Li, Win-San Khwa, Xiaoyu Sun, Sai Qian Zhang, Syed Shakib Sarwar, Kleber Hugo Stangherlin, Yi-Lun Lu, Jorge Tomas Gomez, Jae-Sun Seo, Phillip B. Gibbons, Barbara De Salvo, Chiao Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AR, cs.LG, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08326">https://arxiv.org/abs/2410.08326</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08326">https://arxiv.org/pdf/2410.08326</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08326]] Neural Architecture Search of Hybrid Models for NPU-CIM Heterogeneous AR/VR Devices(https://arxiv.org/abs/2410.08326)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Low-Latency and Low-Power Edge AI is essential for Virtual Reality and Augmented Reality applications. Recent advances show that hybrid models, combining convolution layers (CNN) and transformers (ViT), often achieve superior accuracy/performance tradeoff on various computer vision and machine learning (ML) tasks. However, hybrid ML models can pose system challenges for latency and energy-efficiency due to their diverse nature in dataflow and memory access patterns. In this work, we leverage the architecture heterogeneity from Neural Processing Units (NPU) and Compute-In-Memory (CIM) and perform diverse execution schemas to efficiently execute these hybrid models. We also introduce H4H-NAS, a Neural Architecture Search framework to design efficient hybrid CNN/ViT models for heterogeneous edge systems with both NPU and CIM. Our H4H-NAS approach is powered by a performance estimator built with NPU performance results measured on real silicon, and CIM performance based on industry IPs. H4H-NAS searches hybrid CNN/ViT models with fine granularity and achieves significant (up to 1.34%) top-1 accuracy improvement on ImageNet dataset. Moreover, results from our Algo/HW co-design reveal up to 56.08% overall latency and 41.72% energy improvements by introducing such heterogeneous computing over baseline solutions. The framework guides the design of hybrid network architectures and system architectures of NPU+CIM heterogeneous systems.</li>
</ul>

<h3>Title: Evaluating Differentially Private Synthetic Data Generation in High-Stakes Domains</h3>
<ul>
<li><strong>Authors: </strong>Krithika Ramesh, Nupoor Gandhi, Pulkit Madaan, Lisa Bauer, Charith Peris, Anjalie Field</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08327">https://arxiv.org/abs/2410.08327</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08327">https://arxiv.org/pdf/2410.08327</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08327]] Evaluating Differentially Private Synthetic Data Generation in High-Stakes Domains(https://arxiv.org/abs/2410.08327)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, fair</a></li>
<li><strong>Abstract: </strong>The difficulty of anonymizing text data hinders the development and deployment of NLP in high-stakes domains that involve private data, such as healthcare and social services. Poorly anonymized sensitive data cannot be easily shared with annotators or external researchers, nor can it be used to train public models. In this work, we explore the feasibility of using synthetic data generated from differentially private language models in place of real data to facilitate the development of NLP in these domains without compromising privacy. In contrast to prior work, we generate synthetic data for real high-stakes domains, and we propose and conduct use-inspired evaluations to assess data quality. Our results show that prior simplistic evaluations have failed to highlight utility, privacy, and fairness issues in the synthetic data. Overall, our work underscores the need for further improvements to synthetic data generation for it to be a viable way to enable privacy-preserving data sharing.</li>
</ul>

<h3>Title: Level of agreement between emotions generated by Artificial Intelligence and human evaluation: a methodological proposal</h3>
<ul>
<li><strong>Authors: </strong>Miguel Carrasco, Cesar Gonzalez-Martin, Sonia Navajas-Torrente, Raul Dastres</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08332">https://arxiv.org/abs/2410.08332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08332">https://arxiv.org/pdf/2410.08332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08332]] Level of agreement between emotions generated by Artificial Intelligence and human evaluation: a methodological proposal(https://arxiv.org/abs/2410.08332)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Images are capable of conveying emotions, but emotional experience is highly subjective. Advances in artificial intelligence have enabled the generation of images based on emotional descriptions. However, the level of agreement between the generative images and human emotional responses has not yet been evaluated. To address this, 20 artistic landscapes were generated using StyleGAN2-ADA. Four variants evoking positive emotions (contentment, amusement) and negative emotions (fear, sadness) were created for each image, resulting in 80 pictures. An online questionnaire was designed using this material, in which 61 observers classified the generated images. Statistical analyses were performed on the collected data to determine the level of agreement among participants, between the observer's responses, and the AI-generated emotions. A generally good level of agreement was found, with better results for negative emotions. However, the study confirms the subjectivity inherent in emotional evaluation.</li>
</ul>

<h3>Title: Kernel Banzhaf: A Fast and Robust Estimator for Banzhaf Values</h3>
<ul>
<li><strong>Authors: </strong>Yurong Liu, R. Teal Witter, Flip Korn, Tarfah Alrashed, Dimitris Paparas, Juliana Freire</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08336">https://arxiv.org/abs/2410.08336</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08336">https://arxiv.org/pdf/2410.08336</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08336]] Kernel Banzhaf: A Fast and Robust Estimator for Banzhaf Values(https://arxiv.org/abs/2410.08336)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Banzhaf values offer a simple and interpretable alternative to the widely-used Shapley values. We introduce Kernel Banzhaf, a novel algorithm inspired by KernelSHAP, that leverages an elegant connection between Banzhaf values and linear regression. Through extensive experiments on feature attribution tasks, we demonstrate that Kernel Banzhaf substantially outperforms other algorithms for estimating Banzhaf values in both sample efficiency and robustness to noise. Furthermore, we prove theoretical guarantees on the algorithm's performance, establishing Kernel Banzhaf as a valuable tool for interpretable machine learning.</li>
</ul>

<h3>Title: Time Traveling to Defend Against Adversarial Example Attacks in Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Anthony Etim, Jakub Szefer</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08338">https://arxiv.org/abs/2410.08338</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08338">https://arxiv.org/pdf/2410.08338</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08338]] Time Traveling to Defend Against Adversarial Example Attacks in Image Classification(https://arxiv.org/abs/2410.08338)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>Adversarial example attacks have emerged as a critical threat to machine learning. Adversarial attacks in image classification abuse various, minor modifications to the image that confuse the image classification neural network -- while the image still remains recognizable to humans. One important domain where the attacks have been applied is in the automotive setting with traffic sign classification. Researchers have demonstrated that adding stickers, shining light, or adding shadows are all different means to make machine learning inference algorithms mis-classify the traffic signs. This can cause potentially dangerous situations as a stop sign is recognized as a speed limit sign causing vehicles to ignore it and potentially leading to accidents. To address these attacks, this work focuses on enhancing defenses against such adversarial attacks. This work shifts the advantage to the user by introducing the idea of leveraging historical images and majority voting. While the attacker modifies a traffic sign that is currently being processed by the victim's machine learning inference, the victim can gain advantage by examining past images of the same traffic sign. This work introduces the notion of ''time traveling'' and uses historical Street View images accessible to anybody to perform inference on different, past versions of the same traffic sign. In the evaluation, the proposed defense has 100% effectiveness against latest adversarial example attack on traffic sign classification algorithm.</li>
</ul>

<h3>Title: Intellectual Property Blockchain Odyssey: Navigating Challenges and Seizing Opportunities</h3>
<ul>
<li><strong>Authors: </strong>Rabia Bajwa, Farah Tasnur Meem</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08359">https://arxiv.org/abs/2410.08359</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08359">https://arxiv.org/pdf/2410.08359</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08359]] Intellectual Property Blockchain Odyssey: Navigating Challenges and Seizing Opportunities(https://arxiv.org/abs/2410.08359)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect</a></li>
<li><strong>Abstract: </strong>This paper investigates the evolving relationship between protecting Intellectual Property Rights (IPRs) and blockchain technology. We conducted a comprehensive literature review, supplemented by case study analyses and research paper reviews, to understand the scope and implications of blockchain about intellectual property rights. Our study demonstrates how applying blockchain technology for IPR could revolutionize transparency, security, and operational efficiency. It also identifies the primary challenges and openings in this area. We provide an extensive framework for integrating blockchain technology with intellectual property rights and other technical components (some of which already exist or are resolved by blockchain; some might need attention), drawing on current research and best practices. This framework has the potential to give a new perspective in a structured manner for the intellectual property landscape by providing 360-degree coverage across different layers of operation.</li>
</ul>

<h3>Title: Evaluating Transformer Models for Suicide Risk Detection on Social Media</h3>
<ul>
<li><strong>Authors: </strong>Jakub Pokrywka, Jeremi I. Kaczmarek, Edward J. Gorzelańczyk</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08375">https://arxiv.org/abs/2410.08375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08375">https://arxiv.org/pdf/2410.08375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08375]] Evaluating Transformer Models for Suicide Risk Detection on Social Media(https://arxiv.org/abs/2410.08375)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The detection of suicide risk in social media is a critical task with potential life-saving implications. This paper presents a study on leveraging state-of-the-art natural language processing solutions for identifying suicide risk in social media posts as a submission for the "IEEE BigData 2024 Cup: Detection of Suicide Risk on Social Media" conducted by the kubapok team. We experimented with the following configurations of transformer-based models: fine-tuned DeBERTa, GPT-4o with CoT and few-shot prompting, and fine-tuned GPT-4o. The task setup was to classify social media posts into four categories: indicator, ideation, behavior, and attempt. Our findings demonstrate that the fine-tuned GPT-4o model outperforms two other configurations, achieving high accuracy in identifying suicide risk. Notably, our model achieved second place in the competition. By demonstrating that straightforward, general-purpose models can achieve state-of-the-art results, we propose that these models, combined with minimal tuning, may have the potential to be effective solutions for automated suicide risk detection on social media.</li>
</ul>

<h3>Title: GUS-Net: Social Bias Classification in Text with Generalizations, Unfairness, and Stereotypes</h3>
<ul>
<li><strong>Authors: </strong>Maximus Powers, Hua Wei, Umang Mavani, Harshitha Reddy Jonala, Ansh Tiwari</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08388">https://arxiv.org/abs/2410.08388</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08388">https://arxiv.org/pdf/2410.08388</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08388]] GUS-Net: Social Bias Classification in Text with Generalizations, Unfairness, and Stereotypes(https://arxiv.org/abs/2410.08388)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, generative, large language model</a></li>
<li><strong>Abstract: </strong>The detection of bias in natural language processing (NLP) is a critical challenge, particularly with the increasing use of large language models (LLMs) in various domains. This paper introduces GUS-Net, an innovative approach to bias detection that focuses on three key types of biases: (G)eneralizations, (U)nfairness, and (S)tereotypes. GUS-Net leverages generative AI and automated agents to create a comprehensive synthetic dataset, enabling robust multi-label token classification. Our methodology enhances traditional bias detection methods by incorporating the contextual encodings of pre-trained models, resulting in improved accuracy and depth in identifying biased entities. Through extensive experiments, we demonstrate that GUS-Net outperforms state-of-the-art techniques, achieving superior performance in terms of accuracy, F1-score, and Hamming Loss. The findings highlight GUS-Net's effectiveness in capturing a wide range of biases across diverse contexts, making it a valuable tool for social bias detection in text. This study contributes to the ongoing efforts in NLP to address implicit bias, providing a pathway for future research and applications in various fields. The Jupyter notebooks used to create the dataset and model are available at: this https URL. Warning: This paper contains examples of harmful language, and reader discretion is recommended.</li>
</ul>

<h3>Title: Heating Up Quasi-Monte Carlo Graph Random Features: A Diffusion Kernel Perspective</h3>
<ul>
<li><strong>Authors: </strong>Brooke Feinberg, Aiwen Li</a></li>
<li><strong>Subjects: </strong>cs.LG, math.CO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08389">https://arxiv.org/abs/2410.08389</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08389">https://arxiv.org/pdf/2410.08389</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08389]] Heating Up Quasi-Monte Carlo Graph Random Features: A Diffusion Kernel Perspective(https://arxiv.org/abs/2410.08389)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We build upon a recently introduced class of quasi-graph random features (q-GRFs), which have demonstrated the ability to yield lower variance estimators of the 2-regularized Laplacian kernel (Choromanski 2023). Our research investigates whether similar results can be achieved with alternative kernel functions, specifically the Diffusion (or Heat), Matérn, and Inverse Cosine kernels. We find that the Diffusion kernel performs most similarly to the 2-regularized Laplacian, and we further explore graph types that benefit from the previously established antithetic termination procedure. Specifically, we explore Erdős-Rényi and Barabási-Albert random graph models, Binary Trees, and Ladder graphs, with the goal of identifying combinations of specific kernel and graph type that benefit from antithetic termination. We assert that q-GRFs achieve lower variance estimators of the Diffusion (or Heat) kernel on Ladder graphs. However, the number of rungs on the Ladder graphs impacts the algorithm's performance; further theoretical results supporting our experimentation are forthcoming. This work builds upon some of the earliest Quasi-Monte Carlo methods for kernels defined on combinatorial objects, paving the way for kernel-based learning algorithms and future real-world applications in various domains.</li>
</ul>

<h3>Title: KnowGraph: Knowledge-Enabled Anomaly Detection via Logical Reasoning on Graph Data</h3>
<ul>
<li><strong>Authors: </strong>Andy Zhou, Xiaojun Xu, Ramesh Raghunathan, Alok Lal, Xinze Guan, Bin Yu, Bo Li</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08390">https://arxiv.org/abs/2410.08390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08390">https://arxiv.org/pdf/2410.08390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08390]] KnowGraph: Knowledge-Enabled Anomaly Detection via Logical Reasoning on Graph Data(https://arxiv.org/abs/2410.08390)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Graph-based anomaly detection is pivotal in diverse security applications, such as fraud detection in transaction networks and intrusion detection for network traffic. Standard approaches, including Graph Neural Networks (GNNs), often struggle to generalize across shifting data distributions. Meanwhile, real-world domain knowledge is more stable and a common existing component of real-world detection strategies. To explicitly integrate such knowledge into data-driven models such as GCNs, we propose KnowGraph, which integrates domain knowledge with data-driven learning for enhanced graph-based anomaly detection. KnowGraph comprises two principal components: (1) a statistical learning component that utilizes a main model for the overarching detection task, augmented by multiple specialized knowledge models that predict domain-specific semantic entities; (2) a reasoning component that employs probabilistic graphical models to execute logical inferences based on model outputs, encoding domain knowledge through weighted first-order logic formulas. Extensive experiments on these large-scale real-world datasets show that KnowGraph consistently outperforms state-of-the-art baselines in both transductive and inductive settings, achieving substantial gains in average precision when generalizing to completely unseen test graphs. Further ablation studies demonstrate the effectiveness of the proposed reasoning component in improving detection performance, especially under extreme class imbalance. These results highlight the potential of integrating domain knowledge into data-driven models for high-stakes, graph-based security applications.</li>
</ul>

<h3>Title: KV Prediction for Improved Time to First Token</h3>
<ul>
<li><strong>Authors: </strong>Maxwell Horton, Qingqing Cao, Chenfan Sun, Yanzi Jin, Sachin Mehta, Mohammad Rastegari, Moin Nabi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08391">https://arxiv.org/abs/2410.08391</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08391">https://arxiv.org/pdf/2410.08391</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08391]] KV Prediction for Improved Time to First Token(https://arxiv.org/abs/2410.08391)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Inference with transformer-based language models begins with a prompt processing step. In this step, the model generates the first output token and stores the KV cache needed for future generation steps. This prompt processing step can be computationally expensive, taking 10s of seconds or more for billion-parameter models on edge devices when prompt lengths or batch sizes rise. This degrades user experience by introducing significant latency into the model's outputs. To reduce the time spent producing the first output (known as the ``time to first token'', or TTFT) of a pretrained model, we introduce a novel method called KV Prediction. In our method, a small auxiliary model is used to process the prompt and produce an approximation of the KV cache used by a base model. This approximated KV cache is then used with the base model for autoregressive generation without the need to query the auxiliary model again. We demonstrate that our method produces a pareto-optimal efficiency-accuracy trade-off when compared to baselines. On TriviaQA, we demonstrate relative accuracy improvements in the range of $15\%-50\%$ across a range of TTFT FLOPs budgets. We also demonstrate accuracy improvements of up to $30\%$ on HumanEval python code completion at fixed TTFT FLOPs budgets. Additionally, we benchmark models on an Apple M2 Pro CPU and demonstrate that our improvement in FLOPs translates to a TTFT speedup on hardware. We release our code at this https URL .</li>
</ul>

<h3>Title: The Effects of Hallucinations in Synthetic Training Data for Relation Extraction</h3>
<ul>
<li><strong>Authors: </strong>Steven Rogulsky, Nicholas Popovic, Michael Färber</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08393">https://arxiv.org/abs/2410.08393</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08393">https://arxiv.org/pdf/2410.08393</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08393]] The Effects of Hallucinations in Synthetic Training Data for Relation Extraction(https://arxiv.org/abs/2410.08393)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, generative</a></li>
<li><strong>Abstract: </strong>Relation extraction is crucial for constructing knowledge graphs, with large high-quality datasets serving as the foundation for training, fine-tuning, and evaluating models. Generative data augmentation (GDA) is a common approach to expand such datasets. However, this approach often introduces hallucinations, such as spurious facts, whose impact on relation extraction remains underexplored. In this paper, we examine the effects of hallucinations on the performance of relation extraction on the document and sentence levels. Our empirical study reveals that hallucinations considerably compromise the ability of models to extract relations from text, with recall reductions between 19.1% and 39.2%. We identify that relevant hallucinations impair the model's performance, while irrelevant hallucinations have a minimal impact. Additionally, we develop methods for the detection of hallucinations to improve data quality and model performance. Our approaches successfully classify texts as either 'hallucinated' or 'clean,' achieving high F1-scores of 83.8% and 92.2%. These methods not only assist in removing hallucinations but also help in estimating their prevalence within datasets, which is crucial for selecting high-quality data. Overall, our work confirms the profound impact of relevant hallucinations on the effectiveness of relation extraction models.</li>
</ul>

<h3>Title: AgroGPT: Efficient Agricultural Vision-Language Model with Expert Tuning</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Awais, Ali Husain Salem Abdulla Alharthi, Amandeep Kumar, Hisham Cholakkal, Rao Muhammad Anwer</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08405">https://arxiv.org/abs/2410.08405</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08405">https://arxiv.org/pdf/2410.08405</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08405]] AgroGPT: Efficient Agricultural Vision-Language Model with Expert Tuning(https://arxiv.org/abs/2410.08405)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Significant progress has been made in advancing large multimodal conversational models (LMMs), capitalizing on vast repositories of image-text data available online. Despite this progress, these models often encounter substantial domain gaps, hindering their ability to engage in complex conversations across new domains. Recent efforts have aimed to mitigate this issue, albeit relying on domain-specific image-text data to curate instruction-tuning data. However, many domains, such as agriculture, lack such vision-language data. In this work, we propose an approach to construct instruction-tuning data that harnesses vision-only data for the agriculture domain. We utilize diverse agricultural datasets spanning multiple domains, curate class-specific information, and employ large language models (LLMs) to construct an expert-tuning set, resulting in a 70k expert-tuning dataset called AgroInstruct. Subsequently, we expert-tuned and created AgroGPT, an efficient LMM that can hold complex agriculture-related conversations and provide useful insights. We also develop AgroEvals for evaluation and compare {AgroGPT's} performance with large open and closed-source models. {AgroGPT} excels at identifying fine-grained agricultural concepts, can act as an agriculture expert, and provides helpful information for multimodal agriculture questions. The code, datasets, and models are available at this https URL.</li>
</ul>

<h3>Title: What is Left After Distillation? How Knowledge Transfer Impacts Fairness and Bias</h3>
<ul>
<li><strong>Authors: </strong>Aida Mohammadshahi, Yani Ioannou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08407">https://arxiv.org/abs/2410.08407</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08407">https://arxiv.org/pdf/2410.08407</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08407]] What is Left After Distillation? How Knowledge Transfer Impacts Fairness and Bias(https://arxiv.org/abs/2410.08407)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Knowledge Distillation is a commonly used Deep Neural Network compression method, which often maintains overall generalization performance. However, we show that even for balanced image classification datasets, such as CIFAR-100, Tiny ImageNet and ImageNet, as many as 41% of the classes are statistically significantly affected by distillation when comparing class-wise accuracy (i.e. class bias) between a teacher/distilled student or distilled student/non-distilled student model. Changes in class bias are not necessarily an undesirable outcome when considered outside of the context of a model's usage. Using two common fairness metrics, Demographic Parity Difference (DPD) and Equalized Odds Difference (EOD) on models trained with the CelebA, Trifeature, and HateXplain datasets, our results suggest that increasing the distillation temperature improves the distilled student model's fairness -- for DPD, the distilled student even surpasses the fairness of the teacher model at high temperatures. This study highlights the uneven effects of Knowledge Distillation on certain classes and its potentially significant role in fairness, emphasizing that caution is warranted when using distilled models for sensitive application domains.</li>
</ul>

<h3>Title: Understanding the Interplay between Parametric and Contextual Knowledge for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sitao Cheng, Liangming Pan, Xunjian Yin, Xinyi Wang, William Yang Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08414">https://arxiv.org/abs/2410.08414</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08414">https://arxiv.org/pdf/2410.08414</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08414]] Understanding the Interplay between Parametric and Contextual Knowledge for Large Language Models(https://arxiv.org/abs/2410.08414)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) encode vast amounts of knowledge during pre-training (parametric knowledge, or PK) and can further be enhanced by incorporating contextual knowledge (CK). Can LLMs effectively integrate their internal PK with external CK to solve complex problems? In this paper, we investigate the dynamic interaction between PK and CK, categorizing their relationships into four types: Supportive, Complementary, Conflicting, and Irrelevant. To support this investigation, we introduce ECHOQA, a benchmark spanning scientific, factual, and commonsense knowledge. Our results show that LLMs tend to suppress their PK when contextual information is available, even when it is complementary or irrelevant. While tailored instructions can encourage LLMs to rely more on their PK, they still struggle to fully leverage it. These findings reveal a key vulnerability in LLMs, raising concerns about their reliability in knowledge-intensive tasks. Resources are available at this https URL Interplay.</li>
</ul>

<h3>Title: Bilinear MLPs enable weight-based mechanistic interpretability</h3>
<ul>
<li><strong>Authors: </strong>Michael T. Pearce, Thomas Dooms, Alice Rigg, Jose M. Oramas, Lee Sharkey</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08417">https://arxiv.org/abs/2410.08417</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08417">https://arxiv.org/pdf/2410.08417</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08417]] Bilinear MLPs enable weight-based mechanistic interpretability(https://arxiv.org/abs/2410.08417)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>A mechanistic understanding of how MLPs do computation in deep neural networks remains elusive. Current interpretability work can extract features from hidden activations over an input dataset but generally cannot explain how MLP weights construct features. One challenge is that element-wise nonlinearities introduce higher-order interactions and make it difficult to trace computations through the MLP layer. In this paper, we analyze bilinear MLPs, a type of Gated Linear Unit (GLU) without any element-wise nonlinearity that nevertheless achieves competitive performance. Bilinear MLPs can be fully expressed in terms of linear operations using a third-order tensor, allowing flexible analysis of the weights. Analyzing the spectra of bilinear MLP weights using eigendecomposition reveals interpretable low-rank structure across toy tasks, image classification, and language modeling. We use this understanding to craft adversarial examples, uncover overfitting, and identify small language model circuits directly from the weights alone. Our results demonstrate that bilinear layers serve as an interpretable drop-in replacement for current activation functions and that weight-based interpretability is viable for understanding deep-learning models.</li>
</ul>

<h3>Title: Generalizable autoregressive modeling of time series through functional narratives</h3>
<ul>
<li><strong>Authors: </strong>Ran Liu, Wenrui Ma, Ellen Zippi, Hadi Pouransari, Jingyun Xiao, Chris Sandino, Behrooz Mahasseni, Juri Minxha, Erdrin Azemi, Eva L. Dyer, Ali Moin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08421">https://arxiv.org/abs/2410.08421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08421">https://arxiv.org/pdf/2410.08421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08421]] Generalizable autoregressive modeling of time series through functional narratives(https://arxiv.org/abs/2410.08421)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Time series data are inherently functions of time, yet current transformers often learn time series by modeling them as mere concatenations of time periods, overlooking their functional properties. In this work, we propose a novel objective for transformers that learn time series by re-interpreting them as temporal functions. We build an alternative sequence of time series by constructing degradation operators of different intensity in the functional space, creating augmented variants of the original sample that are abstracted or simplified to different degrees. Based on the new set of generated sequence, we train an autoregressive transformer that progressively recovers the original sample from the most simplified variant. Analogous to the next word prediction task in languages that learns narratives by connecting different words, our autoregressive transformer aims to learn the Narratives of Time Series (NoTS) by connecting different functions in time. Theoretically, we justify the construction of the alternative sequence through its advantages in approximating functions. When learning time series data with transformers, constructing sequences of temporal functions allows for a broader class of approximable functions (e.g., differentiation) compared to sequences of time periods, leading to a 26\% performance improvement in synthetic feature regression experiments. Experimentally, we validate NoTS in 3 different tasks across 22 real-world datasets, where we show that NoTS significantly outperforms other pre-training methods by up to 6\%. Additionally, combining NoTS on top of existing transformer architectures can consistently boost the performance. Our results demonstrate the potential of NoTS as a general-purpose dynamic learner, offering a viable alternative for developing foundation models for time series analysis.</li>
</ul>

<h3>Title: Levels of Binary Equivalence for the Comparison of Binaries from Alternative Builds</h3>
<ul>
<li><strong>Authors: </strong>Jens Dietrich, Tim White, Behnaz Hassanshahi, Paddy Krishnan</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08427">https://arxiv.org/abs/2410.08427</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08427">https://arxiv.org/pdf/2410.08427</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08427]] Levels of Binary Equivalence for the Comparison of Binaries from Alternative Builds(https://arxiv.org/abs/2410.08427)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>In response to challenges in software supply chain security, several organisations have created infrastructures to independently build commodity open source projects and release the resulting binaries. Build platform variability can strengthen security as it facilitates the detection of compromised build environments. Furthermore, by improving the security posture of the build platform and collecting provenance information during the build, the resulting artifacts can be used with greater trust. Such offerings are now available from Google, Oracle and RedHat. The availability of multiple binaries built from the same sources creates new challenges and opportunities, and raises questions such as: 'Does build A confirm the integrity of build B?' or 'Can build A reveal a compromised build B?'. To answer such questions requires a notion of equivalence between binaries. We demonstrate that the obvious approach based on bitwise equality has significant shortcomings in practice, and that there is value in opting for alternative notions. We conceptualise this by introducing levels of equivalence, inspired by clone detection types. We demonstrate the value of these new levels through several experiments. We construct a dataset consisting of Java binaries built from the same sources independently by different providers, resulting in 14,156 pairs of binaries in total. We then compare the compiled class files in those jar files and find that for 3,750 pairs of jars (26.49%) there is at least one such file that is different, also forcing the jar files and their cryptographic hashes to be different. However, based on the new equivalence levels, we can still establish that many of them are practically equivalent. We evaluate several candidate equivalence relations on a semi-synthetic dataset that provides oracles consisting of pairs of binaries that either should be, or must not be equivalent.</li>
</ul>

<h3>Title: oRetrieval Augmented Generation for 10 Large Language Models and its Generalizability in Assessing Medical Fitness</h3>
<ul>
<li><strong>Authors: </strong>Yu He Ke, Liyuan Jin, Kabilan Elangovan, Hairil Rizal Abdullah, Nan Liu, Alex Tiong Heng Sia, Chai Rick Soh, Joshua Yi Min Tung, Jasmine Chiat Ling Ong, Chang-Fu Kuo, Shao-Chun Wu, Vesela P. Kovacheva, Daniel Shu Wei Ting</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08431">https://arxiv.org/abs/2410.08431</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08431">https://arxiv.org/pdf/2410.08431</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08431]] oRetrieval Augmented Generation for 10 Large Language Models and its Generalizability in Assessing Medical Fitness(https://arxiv.org/abs/2410.08431)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) show potential for medical applications but often lack specialized clinical knowledge. Retrieval Augmented Generation (RAG) allows customization with domain-specific information, making it suitable for healthcare. This study evaluates the accuracy, consistency, and safety of RAG models in determining fitness for surgery and providing preoperative instructions. We developed LLM-RAG models using 35 local and 23 international preoperative guidelines and tested them against human-generated responses. A total of 3,682 responses were evaluated. Clinical documents were processed using Llamaindex, and 10 LLMs, including GPT3.5, GPT4, and Claude-3, were assessed. Fourteen clinical scenarios were analyzed, focusing on seven aspects of preoperative instructions. Established guidelines and expert judgment were used to determine correct responses, with human-generated answers serving as comparisons. The LLM-RAG models generated responses within 20 seconds, significantly faster than clinicians (10 minutes). The GPT4 LLM-RAG model achieved the highest accuracy (96.4% vs. 86.6%, p=0.016), with no hallucinations and producing correct instructions comparable to clinicians. Results were consistent across both local and international guidelines. This study demonstrates the potential of LLM-RAG models for preoperative healthcare tasks, highlighting their efficiency, scalability, and reliability.</li>
</ul>

<h3>Title: MYCROFT: Towards Effective and Efficient External Data Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Zain Sarwar, Van Tran, Arjun Nitin Bhagoji, Nick Feamster, Ben Y. Zhao, Supriyo Chakraborty</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08432">https://arxiv.org/abs/2410.08432</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08432">https://arxiv.org/pdf/2410.08432</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08432]] MYCROFT: Towards Effective and Efficient External Data Augmentation(https://arxiv.org/abs/2410.08432)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust</a></li>
<li><strong>Abstract: </strong>Machine learning (ML) models often require large amounts of data to perform well. When the available data is limited, model trainers may need to acquire more data from external sources. Often, useful data is held by private entities who are hesitant to share their data due to propriety and privacy concerns. This makes it challenging and expensive for model trainers to acquire the data they need to improve model performance. To address this challenge, we propose Mycroft, a data-efficient method that enables model trainers to evaluate the relative utility of different data sources while working with a constrained data-sharing budget. By leveraging feature space distances and gradient matching, Mycroft identifies small but informative data subsets from each owner, allowing model trainers to maximize performance with minimal data exposure. Experimental results across four tasks in two domains show that Mycroft converges rapidly to the performance of the full-information baseline, where all data is shared. Moreover, Mycroft is robust to noise and can effectively rank data owners by utility. Mycroft can pave the way for democratized training of high performance ML models.</li>
</ul>

<h3>Title: Exploring the Role of Reasoning Structures for Constructing Proofs in Multi-Step Natural Language Reasoning with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zi'ou Zheng, Christopher Malon, Martin Renqiang Min, Xiaodan Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08436">https://arxiv.org/abs/2410.08436</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08436">https://arxiv.org/pdf/2410.08436</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08436]] Exploring the Role of Reasoning Structures for Constructing Proofs in Multi-Step Natural Language Reasoning with Large Language Models(https://arxiv.org/abs/2410.08436)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, large language model</a></li>
<li><strong>Abstract: </strong>When performing complex multi-step reasoning tasks, the ability of Large Language Models (LLMs) to derive structured intermediate proof steps is important for ensuring that the models truly perform the desired reasoning and for improving models' explainability. This paper is centred around a focused study: whether the current state-of-the-art generalist LLMs can leverage the structures in a few examples to better construct the proof structures with \textit{in-context learning}. Our study specifically focuses on structure-aware demonstration and structure-aware pruning. We demonstrate that they both help improve performance. A detailed analysis is provided to help understand the results.</li>
</ul>

<h3>Title: JurEE not Judges: safeguarding llm interactions with small, specialised Encoder Ensembles</h3>
<ul>
<li><strong>Authors: </strong>Dom Nasrabadi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08442">https://arxiv.org/abs/2410.08442</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08442">https://arxiv.org/pdf/2410.08442</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08442]] JurEE not Judges: safeguarding llm interactions with small, specialised Encoder Ensembles(https://arxiv.org/abs/2410.08442)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, transformer</a></li>
<li><strong>Abstract: </strong>We introduce JurEE, an ensemble of efficient, encoder-only transformer models designed to strengthen safeguards in AI-User interactions within LLM-based systems. Unlike existing LLM-as-Judge methods, which often struggle with generalization across risk taxonomies and only provide textual outputs, JurEE offers probabilistic risk estimates across a wide range of prevalent risks. Our approach leverages diverse data sources and employs progressive synthetic data generation techniques, including LLM-assisted augmentation, to enhance model robustness and performance. We create an in-house benchmark comprising of other reputable benchmarks such as the OpenAI Moderation Dataset and ToxicChat, where we find JurEE significantly outperforms baseline models, demonstrating superior accuracy, speed, and cost-efficiency. This makes it particularly suitable for applications requiring stringent content moderation, such as customer-facing chatbots. The encoder-ensemble's modular design allows users to set tailored risk thresholds, enhancing its versatility across various safety-related applications. JurEE's collective decision-making process, where each specialized encoder model contributes to the final output, not only improves predictive accuracy but also enhances interpretability. This approach provides a more efficient, performant, and economical alternative to traditional LLMs for large-scale implementations requiring robust content moderation.</li>
</ul>

<h3>Title: AdvDiffuser: Generating Adversarial Safety-Critical Driving Scenarios via Guided Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Yuting Xie, Xianda Guo, Cong Wang, Kunhua Liu, Long Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08453">https://arxiv.org/abs/2410.08453</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08453">https://arxiv.org/pdf/2410.08453</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08453]] AdvDiffuser: Generating Adversarial Safety-Critical Driving Scenarios via Guided Diffusion(https://arxiv.org/abs/2410.08453)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Safety-critical scenarios are infrequent in natural driving environments but hold significant importance for the training and testing of autonomous driving systems. The prevailing approach involves generating safety-critical scenarios automatically in simulation by introducing adversarial adjustments to natural environments. These adjustments are often tailored to specific tested systems, thereby disregarding their transferability across different systems. In this paper, we propose AdvDiffuser, an adversarial framework for generating safety-critical driving scenarios through guided diffusion. By incorporating a diffusion model to capture plausible collective behaviors of background vehicles and a lightweight guide model to effectively handle adversarial scenarios, AdvDiffuser facilitates transferability. Experimental results on the nuScenes dataset demonstrate that AdvDiffuser, trained on offline driving logs, can be applied to various tested systems with minimal warm-up episode data and outperform other existing methods in terms of realism, diversity, and adversarial performance.</li>
</ul>

<h3>Title: HorGait: Advancing Gait Recognition with Efficient High-Order Spatial Interactions in LiDAR Point Clouds</h3>
<ul>
<li><strong>Authors: </strong>Jiaxing Hao, Yanxi Wang, Zhigang Chang, Hongmin Gao, Zihao Cheng, Chen Wu, Xin Zhao, Peiye Fang, Rachmat Muwardi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08454">https://arxiv.org/abs/2410.08454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08454">https://arxiv.org/pdf/2410.08454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08454]] HorGait: Advancing Gait Recognition with Efficient High-Order Spatial Interactions in LiDAR Point Clouds(https://arxiv.org/abs/2410.08454)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, biometric, transformer</a></li>
<li><strong>Abstract: </strong>Gait recognition is a remote biometric technology that utilizes the dynamic characteristics of human movement to identify individuals even under various extreme lighting conditions. Due to the limitation in spatial perception capability inherent in 2D gait representations, LiDAR can directly capture 3D gait features and represent them as point clouds, reducing environmental and lighting interference in recognition while significantly advancing privacy protection. For complex 3D representations, shallow networks fail to achieve accurate recognition, making vision Transformers the foremost prevalent method. However, the prevalence of dumb patches has limited the widespread use of Transformer architecture in gait recognition. This paper proposes a method named HorGait, which utilizes a hybrid model with a Transformer architecture for gait recognition on the planar projection of 3D point clouds from LiDAR. Specifically, it employs a hybrid model structure called LHM Block to achieve input adaptation, long-range, and high-order spatial interaction of the Transformer architecture. Additionally, it uses large convolutional kernel CNNs to segment the input representation, replacing attention windows to reduce dumb patches. We conducted extensive experiments, and the results show that HorGait achieves state-of-the-art performance among Transformer architecture methods on the SUSTech1K dataset, verifying that the hybrid model can complete the full Transformer process and perform better in point cloud planar projection. The outstanding performance of HorGait offers new insights for the future application of the Transformer architecture in gait recognition.</li>
</ul>

<h3>Title: Simultaneous Reward Distillation and Preference Learning: Get You a Language Model Who Can Do Both</h3>
<ul>
<li><strong>Authors: </strong>Abhijnan Nath, Changsoo Jung, Ethan Seefried, Nikhil Krishnaswamy</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08458">https://arxiv.org/abs/2410.08458</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08458">https://arxiv.org/pdf/2410.08458</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08458]] Simultaneous Reward Distillation and Preference Learning: Get You a Language Model Who Can Do Both(https://arxiv.org/abs/2410.08458)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Reward modeling of human preferences is one of the cornerstones of building usable generative large language models (LLMs). While traditional RLHF-based alignment methods explicitly maximize the expected rewards from a separate reward model, more recent supervised alignment methods like Direct Preference Optimization (DPO) circumvent this phase to avoid problems including model drift and reward overfitting. Although popular due to its simplicity, DPO and similar direct alignment methods can still lead to degenerate policies, and rely heavily on the Bradley-Terry-based preference formulation to model reward differences between pairs of candidate outputs. This formulation is challenged by non-deterministic or noisy preference labels, for example human scoring of two candidate outputs is of low confidence. In this paper, we introduce DRDO (Direct Reward Distillation and policy-Optimization), a supervised knowledge distillation-based preference alignment method that simultaneously models rewards and preferences to avoid such degeneracy. DRDO directly mimics rewards assigned by an oracle while learning human preferences from a novel preference likelihood formulation. Our experimental results on the Ultrafeedback and TL;DR datasets demonstrate that policies trained using DRDO surpass previous methods such as DPO and e-DPO in terms of expected rewards and are more robust, on average, to noisy preference signals as well as out-of-distribution (OOD) settings.</li>
</ul>

<h3>Title: Driving Privacy Forward: Mitigating Information Leakage within Smart Vehicles through Synthetic Data Generation</h3>
<ul>
<li><strong>Authors: </strong>Krish Parikh</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08462">https://arxiv.org/abs/2410.08462</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08462">https://arxiv.org/pdf/2410.08462</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08462]] Driving Privacy Forward: Mitigating Information Leakage within Smart Vehicles through Synthetic Data Generation(https://arxiv.org/abs/2410.08462)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack</a></li>
<li><strong>Abstract: </strong>Smart vehicles produce large amounts of data, much of which is sensitive and at risk of privacy breaches. As attackers increasingly exploit anonymised metadata within these datasets to profile drivers, it's important to find solutions that mitigate this information leakage without hindering innovation and ongoing research. Synthetic data has emerged as a promising tool to address these privacy concerns, as it allows for the replication of real-world data relationships while minimising the risk of revealing sensitive information. In this paper, we examine the use of synthetic data to tackle these challenges. We start by proposing a comprehensive taxonomy of 14 in-vehicle sensors, identifying potential attacks and categorising their vulnerability. We then focus on the most vulnerable signals, using the Passive Vehicular Sensor (PVS) dataset to generate synthetic data with a Tabular Variational Autoencoder (TVAE) model, which included over 1 million data points. Finally, we evaluate this against 3 core metrics: fidelity, utility, and privacy. Our results show that we achieved 90.1% statistical similarity and 78% classification accuracy when tested on its original intent while also preventing the profiling of the driver. The code can be found at this https URL</li>
</ul>

<h3>Title: Aligned Divergent Pathways for Omni-Domain Generalized Person Re-Identification</h3>
<ul>
<li><strong>Authors: </strong>Eugene P.W. Ang, Shan Lin, Alex C. Kot</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08466">https://arxiv.org/abs/2410.08466</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08466">https://arxiv.org/pdf/2410.08466</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08466]] Aligned Divergent Pathways for Omni-Domain Generalized Person Re-Identification(https://arxiv.org/abs/2410.08466)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Person Re-identification (Person ReID) has advanced significantly in fully supervised and domain generalized Person R e ID. However, methods developed for one task domain transfer poorly to the other. An ideal Person ReID method should be effective regardless of the number of domains involved in training or testing. Furthermore, given training data from the target domain, it should perform at least as well as state-of-the-art (SOTA) fully supervised Person ReID methods. We call this paradigm Omni-Domain Generalization Person ReID, referred to as ODG-ReID, and propose a way to achieve this by expanding compatible backbone architectures into multiple diverse pathways. Our method, Aligned Divergent Pathways (ADP), first converts a base architecture into a multi-branch structure by copying the tail of the original backbone. We design our module Dynamic Max-Deviance Adaptive Instance Normalization (DyMAIN) that encourages learning of generalized features that are robust to omni-domain directions and apply DyMAIN to the branches of ADP. Our proposed Phased Mixture-of-Cosines (PMoC) coordinates a mix of stable and turbulent learning rate schedules among branches for further diversified learning. Finally, we realign the feature space between branches with our proposed Dimensional Consistency Metric Loss (DCML). ADP outperforms the state-of-the-art (SOTA) results for multi-source domain generalization and supervised ReID within the same domain. Furthermore, our method demonstrates improvement on a wide range of single-source domain generalization benchmarks, achieving Omni-Domain Generalization over Person ReID tasks.</li>
</ul>

<h3>Title: SPORTU: A Comprehensive Sports Understanding Benchmark for Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haotian Xia, Zhengbang Yang, Junbo Zou, Rhys Tracy, Yuqing Wang, Chi Lu, Christopher Lai, Yanjun He, Xun Shao, Zhuoqing Xie, Yuan-fang Wang, Weining Shen, Hanjie Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08474">https://arxiv.org/abs/2410.08474</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08474">https://arxiv.org/pdf/2410.08474</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08474]] SPORTU: A Comprehensive Sports Understanding Benchmark for Multimodal Large Language Models(https://arxiv.org/abs/2410.08474)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) are advancing the ability to reason about complex sports scenarios by integrating textual and visual information. To comprehensively evaluate their capabilities, we introduce SPORTU, a benchmark designed to assess MLLMs across multi-level sports reasoning tasks. SPORTU comprises two key components: SPORTU-text, featuring 900 multiple-choice questions with human-annotated explanations for rule comprehension and strategy understanding. This component focuses on testing models' ability to reason about sports solely through question-answering (QA), without requiring visual inputs; SPORTU-video, consisting of 1,701 slow-motion video clips across 7 different sports and 12,048 QA pairs, designed to assess multi-level reasoning, from simple sports recognition to complex tasks like foul detection and rule application. We evaluate four prevalent LLMs mainly utilizing few-shot learning paradigms supplemented by chain-of-thought (CoT) prompting on the SPORTU-text part. We evaluate four LLMs using few-shot learning and chain-of-thought (CoT) prompting on SPORTU-text. GPT-4o achieves the highest accuracy of 71%, but still falls short of human-level performance, highlighting room for improvement in rule comprehension and reasoning. The evaluation for the SPORTU-video part includes 7 proprietary and 6 open-source MLLMs. Experiments show that models fall short on hard tasks that require deep reasoning and rule-based understanding. Claude-3.5-Sonnet performs the best with only 52.6% accuracy on the hard task, showing large room for improvement. We hope that SPORTU will serve as a critical step toward evaluating models' capabilities in sports understanding and reasoning.</li>
</ul>

<h3>Title: Towards Sharper Risk Bounds for Minimax Problems</h3>
<ul>
<li><strong>Authors: </strong>Bowei Zhu, Shaojie Li, Yong Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08497">https://arxiv.org/abs/2410.08497</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08497">https://arxiv.org/pdf/2410.08497</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08497]] Towards Sharper Risk Bounds for Minimax Problems(https://arxiv.org/abs/2410.08497)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Minimax problems have achieved success in machine learning such as adversarial training, robust optimization, reinforcement learning. For theoretical analysis, current optimal excess risk bounds, which are composed by generalization error and optimization error, present 1/n-rates in strongly-convex-strongly-concave (SC-SC) settings. Existing studies mainly focus on minimax problems with specific algorithms for optimization error, with only a few studies on generalization performance, which limit better excess risk bounds. In this paper, we study the generalization bounds measured by the gradients of primal functions using uniform localized convergence. We obtain a sharper high probability generalization error bound for nonconvex-strongly-concave (NC-SC) stochastic minimax problems. Furthermore, we provide dimension-independent results under Polyak-Lojasiewicz condition for the outer layer. Based on our generalization error bound, we analyze some popular algorithms such as empirical saddle point (ESP), gradient descent ascent (GDA) and stochastic gradient descent ascent (SGDA). We derive better excess primal risk bounds with further reasonable assumptions, which, to the best of our knowledge, are n times faster than exist results in minimax problems.</li>
</ul>

<h3>Title: Adversarial Training Can Provably Improve Robustness: Theoretical Analysis of Feature Learning Process Under Structured Data</h3>
<ul>
<li><strong>Authors: </strong>Binghui Li, Yuanzhi Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08503">https://arxiv.org/abs/2410.08503</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08503">https://arxiv.org/pdf/2410.08503</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08503]] Adversarial Training Can Provably Improve Robustness: Theoretical Analysis of Feature Learning Process Under Structured Data(https://arxiv.org/abs/2410.08503)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Adversarial training is a widely-applied approach to training deep neural networks to be robust against adversarial perturbation. However, although adversarial training has achieved empirical success in practice, it still remains unclear why adversarial examples exist and how adversarial training methods improve model robustness. In this paper, we provide a theoretical understanding of adversarial examples and adversarial training algorithms from the perspective of feature learning theory. Specifically, we focus on a multiple classification setting, where the structured data can be composed of two types of features: the robust features, which are resistant to perturbation but sparse, and the non-robust features, which are susceptible to perturbation but dense. We train a two-layer smoothed ReLU convolutional neural network to learn our structured data. First, we prove that by using standard training (gradient descent over the empirical risk), the network learner primarily learns the non-robust feature rather than the robust feature, which thereby leads to the adversarial examples that are generated by perturbations aligned with negative non-robust feature directions. Then, we consider the gradient-based adversarial training algorithm, which runs gradient ascent to find adversarial examples and runs gradient descent over the empirical risk at adversarial examples to update models. We show that the adversarial training method can provably strengthen the robust feature learning and suppress the non-robust feature learning to improve the network robustness. Finally, we also empirically validate our theoretical findings with experiments on real-image datasets, including MNIST, CIFAR10 and SVHN.</li>
</ul>

<h3>Title: A Bayesian Approach to Weakly-supervised Laparoscopic Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Zhou Zheng, Yuichiro Hayashi, Masahiro Oda, Takayuki Kitasaka, Kensaku Mori</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08509">https://arxiv.org/abs/2410.08509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08509">https://arxiv.org/pdf/2410.08509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08509]] A Bayesian Approach to Weakly-supervised Laparoscopic Image Segmentation(https://arxiv.org/abs/2410.08509)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, segmentation</a></li>
<li><strong>Abstract: </strong>In this paper, we study weakly-supervised laparoscopic image segmentation with sparse annotations. We introduce a novel Bayesian deep learning approach designed to enhance both the accuracy and interpretability of the model's segmentation, founded upon a comprehensive Bayesian framework, ensuring a robust and theoretically validated method. Our approach diverges from conventional methods that directly train using observed images and their corresponding weak annotations. Instead, we estimate the joint distribution of both images and labels given the acquired data. This facilitates the sampling of images and their high-quality pseudo-labels, enabling the training of a generalizable segmentation model. Each component of our model is expressed through probabilistic formulations, providing a coherent and interpretable structure. This probabilistic nature benefits accurate and practical learning from sparse annotations and equips our model with the ability to quantify uncertainty. Extensive evaluations with two public laparoscopic datasets demonstrated the efficacy of our method, which consistently outperformed existing methods. Furthermore, our method was adapted for scribble-supervised cardiac multi-structure segmentation, presenting competitive performance compared to previous methods. The code is available at this https URL.</li>
</ul>

<h3>Title: Distributionally robust self-supervised learning for tabular data</h3>
<ul>
<li><strong>Authors: </strong>Shantanu Ghosh, Tiankang Xie, Mikhail Kuznetsov</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08511">https://arxiv.org/abs/2410.08511</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08511">https://arxiv.org/pdf/2410.08511</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08511]] Distributionally robust self-supervised learning for tabular data(https://arxiv.org/abs/2410.08511)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Machine learning (ML) models trained using Empirical Risk Minimization (ERM) often exhibit systematic errors on specific subpopulations of tabular data, known as error slices. Learning robust representation in presence of error slices is challenging, especially in self-supervised settings during the feature reconstruction phase, due to high cardinality features and the complexity of constructing error sets. Traditional robust representation learning methods are largely focused on improving worst group performance in supervised setting in computer vision, leaving a gap in approaches tailored for tabular data. We address this gap by developing a framework to learn robust representation in tabular data during self-supervised pre-training. Our approach utilizes an encoder-decoder model trained with Masked Language Modeling (MLM) loss to learn robust latent representations. This paper applies the Just Train Twice (JTT) and Deep Feature Reweighting (DFR) methods during the pre-training phase for tabular data. These methods fine-tune the ERM pre-trained model by up-weighting error-prone samples or creating balanced datasets for specific categorical features. This results in specialized models for each feature, which are then used in an ensemble approach to enhance downstream classification performance. This methodology improves robustness across slices, thus enhancing overall generalization performance. Extensive experiments across various datasets demonstrate the efficacy of our approach.</li>
</ul>

<h3>Title: Improving Legal Entity Recognition Using a Hybrid Transformer Model and Semantic Filtering Approach</h3>
<ul>
<li><strong>Authors: </strong>Duraimurugan Rajamanickam</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08521">https://arxiv.org/abs/2410.08521</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08521">https://arxiv.org/pdf/2410.08521</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08521]] Improving Legal Entity Recognition Using a Hybrid Transformer Model and Semantic Filtering Approach(https://arxiv.org/abs/2410.08521)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Legal Entity Recognition (LER) is critical in automating legal workflows such as contract analysis, compliance monitoring, and litigation support. Existing approaches, including rule-based systems and classical machine learning models, struggle with the complexity of legal documents and domain specificity, particularly in handling ambiguities and nested entity structures. This paper proposes a novel hybrid model that enhances the accuracy and precision of Legal-BERT, a transformer model fine-tuned for legal text processing, by introducing a semantic similarity-based filtering mechanism. We evaluate the model on a dataset of 15,000 annotated legal documents, achieving an F1 score of 93.4%, demonstrating significant improvements in precision and recall over previous methods.</li>
</ul>

<h3>Title: Scaling Laws for Predicting Downstream Performance in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yangyi Chen, Binxuan Huang, Yifan Gao, Zhengyang Wang, Jingfeng Yang, Heng Ji</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08527">https://arxiv.org/abs/2410.08527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08527">https://arxiv.org/pdf/2410.08527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08527]] Scaling Laws for Predicting Downstream Performance in LLMs(https://arxiv.org/abs/2410.08527)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Precise estimation of downstream performance in large language models (LLMs) prior to training is essential for guiding their development process. Scaling laws analysis utilizes the statistics of a series of significantly smaller sampling language models (LMs) to predict the performance of the target LLM. For downstream performance prediction, the critical challenge lies in the emergent abilities in LLMs that occur beyond task-specific computational thresholds. In this work, we focus on the pre-training loss as a more computation-efficient metric for performance estimation. Our two-stage approach consists of first estimating a function that maps computational resources (e.g., FLOPs) to the pre-training Loss using a series of sampling models, followed by mapping the pre-training loss to downstream task Performance after the critical "emergent phase". In preliminary experiments, this FLP solution accurately predicts the performance of LLMs with 7B and 13B parameters using a series of sampling LMs up to 3B, achieving error margins of 5% and 10%, respectively, and significantly outperforming the FLOPs-to-Performance approach. This motivates FLP-M, a fundamental approach for performance prediction that addresses the practical need to integrate datasets from multiple sources during pre-training, specifically blending general corpora with code data to accurately represent the common necessity. FLP-M extends the power law analytical function to predict domain-specific pre-training loss based on FLOPs across data sources, and employs a two-layer neural network to model the non-linear relationship between multiple domain-specific loss and downstream performance. By utilizing a 3B LLM trained on a specific ratio and a series of smaller sampling LMs, FLP-M can effectively forecast the performance of 3B and 7B LLMs across various data mixtures for most benchmarks within 10% error margins.</li>
</ul>

<h3>Title: Ego3DT: Tracking Every 3D Object in Ego-centric Videos</h3>
<ul>
<li><strong>Authors: </strong>Shengyu Hao, Wenhao Chai, Zhonghan Zhao, Meiqi Sun, Wendi Hu, Jieyang Zhou, Yixian Zhao, Qi Li, Yizhou Wang, Xi Li, Gaoang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08530">https://arxiv.org/abs/2410.08530</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08530">https://arxiv.org/pdf/2410.08530</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08530]] Ego3DT: Tracking Every 3D Object in Ego-centric Videos(https://arxiv.org/abs/2410.08530)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>The growing interest in embodied intelligence has brought ego-centric perspectives to contemporary research. One significant challenge within this realm is the accurate localization and tracking of objects in ego-centric videos, primarily due to the substantial variability in viewing angles. Addressing this issue, this paper introduces a novel zero-shot approach for the 3D reconstruction and tracking of all objects from the ego-centric video. We present Ego3DT, a novel framework that initially identifies and extracts detection and segmentation information of objects within the ego environment. Utilizing information from adjacent video frames, Ego3DT dynamically constructs a 3D scene of the ego view using a pre-trained 3D scene reconstruction model. Additionally, we have innovated a dynamic hierarchical association mechanism for creating stable 3D tracking trajectories of objects in ego-centric videos. Moreover, the efficacy of our approach is corroborated by extensive experiments on two newly compiled datasets, with 1.04x - 2.90x in HOTA, showcasing the robustness and accuracy of our method in diverse ego-centric scenarios.</li>
</ul>

<h3>Title: Diffusion Models Need Visual Priors for Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyu Yue, Zidong Wang, Zeyu Lu, Shuyang Sun, Meng Wei, Wanli Ouyang, Lei Bai, Luping Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08531">https://arxiv.org/abs/2410.08531</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08531">https://arxiv.org/pdf/2410.08531</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08531]] Diffusion Models Need Visual Priors for Image Generation(https://arxiv.org/abs/2410.08531)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Conventional class-guided diffusion models generally succeed in generating images with correct semantic content, but often struggle with texture details. This limitation stems from the usage of class priors, which only provide coarse and limited conditional information. To address this issue, we propose Diffusion on Diffusion (DoD), an innovative multi-stage generation framework that first extracts visual priors from previously generated samples, then provides rich guidance for the diffusion model leveraging visual priors from the early stages of diffusion sampling. Specifically, we introduce a latent embedding module that employs a compression-reconstruction approach to discard redundant detail information from the conditional samples in each stage, retaining only the semantic information for guidance. We evaluate DoD on the popular ImageNet-$256 \times 256$ dataset, reducing 7$\times$ training cost compared to SiT and DiT with even better performance in terms of the FID-50K score. Our largest model DoD-XL achieves an FID-50K score of 1.83 with only 1 million training steps, which surpasses other state-of-the-art methods without bells and whistles during inference.</li>
</ul>

<h3>Title: Quality Prediction of AI Generated Images and Videos: Emerging Trends and Opportunities</h3>
<ul>
<li><strong>Authors: </strong>Abhijay Ghildyal, Yuanhan Chen, Saman Zadtootaghaj, Nabajeet Barman, Alan C. Bovik</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08534">https://arxiv.org/abs/2410.08534</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08534">https://arxiv.org/pdf/2410.08534</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08534]] Quality Prediction of AI Generated Images and Videos: Emerging Trends and Opportunities(https://arxiv.org/abs/2410.08534)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The advent of AI has influenced many aspects of human life, from self-driving cars and intelligent chatbots to text-based image and video generation models capable of creating realistic images and videos based on user prompts (text-to-image, image-to-image, and image-to-video). AI-based methods for image and video super resolution, video frame interpolation, denoising, and compression have already gathered significant attention and interest in the industry and some solutions are already being implemented in real-world products and services. However, to achieve widespread integration and acceptance, AI-generated and enhanced content must be visually accurate, adhere to intended use, and maintain high visual quality to avoid degrading the end user's quality of experience (QoE). One way to monitor and control the visual "quality" of AI-generated and -enhanced content is by deploying Image Quality Assessment (IQA) and Video Quality Assessment (VQA) models. However, most existing IQA and VQA models measure visual fidelity in terms of "reconstruction" quality against a pristine reference content and were not designed to assess the quality of "generative" artifacts. To address this, newer metrics and models have recently been proposed, but their performance evaluation and overall efficacy have been limited by datasets that were too small or otherwise lack representative content and/or distortion capacity; and by performance measures that can accurately report the success of an IQA/VQA model for "GenAI". This paper examines the current shortcomings and possibilities presented by AI-generated and enhanced image and video content, with a particular focus on end-user perceived quality. Finally, we discuss open questions and make recommendations for future work on the "GenAI" quality assessment problems, towards further progressing on this interesting and relevant field of research.</li>
</ul>

<h3>Title: Robust Offline Policy Learning with Observational Data from Multiple Sources</h3>
<ul>
<li><strong>Authors: </strong>Aldo Gael Carranza, Susan Athey</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08537">https://arxiv.org/abs/2410.08537</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08537">https://arxiv.org/pdf/2410.08537</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08537]] Robust Offline Policy Learning with Observational Data from Multiple Sources(https://arxiv.org/abs/2410.08537)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We consider the problem of using observational bandit feedback data from multiple heterogeneous data sources to learn a personalized decision policy that robustly generalizes across diverse target settings. To achieve this, we propose a minimax regret optimization objective to ensure uniformly low regret under general mixtures of the source distributions. We develop a policy learning algorithm tailored to this objective, combining doubly robust offline policy evaluation techniques and no-regret learning algorithms for minimax optimization. Our regret analysis shows that this approach achieves the minimal worst-case mixture regret up to a moderated vanishing rate of the total data across all sources. Our analysis, extensions, and experimental results demonstrate the benefits of this approach for learning robust decision policies from multiple data sources.</li>
</ul>

<h3>Title: Humanity in AI: Detecting the Personality of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Baohua Zhan, Yongyi Huang, Wenyao Cui, Huaping Zhang, Jianyun Shang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08545">https://arxiv.org/abs/2410.08545</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08545">https://arxiv.org/pdf/2410.08545</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08545]] Humanity in AI: Detecting the Personality of Large Language Models(https://arxiv.org/abs/2410.08545)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Questionnaires are a common method for detecting the personality of Large Language Models (LLMs). However, their reliability is often compromised by two main issues: hallucinations (where LLMs produce inaccurate or irrelevant responses) and the sensitivity of responses to the order of the presented options. To address these issues, we propose combining text mining with questionnaires method. Text mining can extract psychological features from the LLMs' responses without being affected by the order of options. Furthermore, because this method does not rely on specific answers, it reduces the influence of hallucinations. By normalizing the scores from both methods and calculating the root mean square error, our experiment results confirm the effectiveness of this approach. To further investigate the origins of personality traits in LLMs, we conduct experiments on both pre-trained language models (PLMs), such as BERT and GPT, as well as conversational models (ChatLLMs), such as ChatGPT. The results show that LLMs do contain certain personalities, for example, ChatGPT and ChatGLM exhibit the personality traits of 'Conscientiousness'. Additionally, we find that the personalities of LLMs are derived from their pre-trained data. The instruction data used to train ChatLLMs can enhance the generation of data containing personalities and expose their hidden personality. We compare the results with the human average personality score, and we find that the personality of FLAN-T5 in PLMs and ChatGPT in ChatLLMs is more similar to that of a human, with score differences of 0.34 and 0.22, respectively.</li>
</ul>

<h3>Title: Score Neural Operator: A Generative Model for Learning and Generalizing Across Multiple Probability Distributions</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Liao, Aoyang Qin, Jacob Seidman, Junqi Wang, Wei Wang, Paris Perdikaris</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08549">https://arxiv.org/abs/2410.08549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08549">https://arxiv.org/pdf/2410.08549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08549]] Score Neural Operator: A Generative Model for Learning and Generalizing Across Multiple Probability Distributions(https://arxiv.org/abs/2410.08549)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Most existing generative models are limited to learning a single probability distribution from the training data and cannot generalize to novel distributions for unseen data. An architecture that can generate samples from both trained datasets and unseen probability distributions would mark a significant breakthrough. Recently, score-based generative models have gained considerable attention for their comprehensive mode coverage and high-quality image synthesis, as they effectively learn an operator that maps a probability distribution to its corresponding score function. In this work, we introduce the $\emph{Score Neural Operator}$, which learns the mapping from multiple probability distributions to their score functions within a unified framework. We employ latent space techniques to facilitate the training of score matching, which tends to over-fit in the original image pixel space, thereby enhancing sample generation quality. Our trained Score Neural Operator demonstrates the ability to predict score functions of probability measures beyond the training space and exhibits strong generalization performance in both 2-dimensional Gaussian Mixture Models and 1024-dimensional MNIST double-digit datasets. Importantly, our approach offers significant potential for few-shot learning applications, where a single image from a new distribution can be leveraged to generate multiple distinct images from that distribution.</li>
</ul>

<h3>Title: Context-Aware Full Body Anonymization using Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Pascl Zwick, Kevin Roesch, Marvin Klemp, Oliver Bringmann</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08551">https://arxiv.org/abs/2410.08551</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08551">https://arxiv.org/pdf/2410.08551</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08551]] Context-Aware Full Body Anonymization using Text-to-Image Diffusion Models(https://arxiv.org/abs/2410.08551)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Anonymization plays a key role in protecting sensible information of individuals in real world datasets. Self-driving cars for example need high resolution facial features to track people and their viewing direction to predict future behaviour and react accordingly. In order to protect people's privacy whilst keeping important features in the dataset, it is important to replace the full body of a person with a highly detailed anonymized one. In contrast to doing face anonymization, full body replacement decreases the ability of recognizing people by their hairstyle or clothes. In this paper, we propose a workflow for full body person anonymization utilizing Stable Diffusion as a generative backend. Text-to-image diffusion models, like Stable Diffusion, OpenAI's DALL-E or Midjourney, have become very popular in recent time, being able to create photorealistic images from a single text prompt. We show that our method outperforms state-of-the art anonymization pipelines with respect to image quality, resolution, Inception Score (IS) and Frechet Inception Distance (FID). Additionally, our method is invariant with respect to the image generator and thus able to be used with the latest models available.</li>
</ul>

<h3>Title: Balancing Innovation and Privacy: Data Security Strategies in Natural Language Processing Applications</h3>
<ul>
<li><strong>Authors: </strong>Shaobo Liu, Guiran Liu, Binrong Zhu, Yuanshuai Luo, Linxiao Wu, Rui Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08553">https://arxiv.org/abs/2410.08553</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08553">https://arxiv.org/pdf/2410.08553</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08553]] Balancing Innovation and Privacy: Data Security Strategies in Natural Language Processing Applications(https://arxiv.org/abs/2410.08553)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect</a></li>
<li><strong>Abstract: </strong>This research addresses privacy protection in Natural Language Processing (NLP) by introducing a novel algorithm based on differential privacy, aimed at safeguarding user data in common applications such as chatbots, sentiment analysis, and machine translation. With the widespread application of NLP technology, the security and privacy protection of user data have become important issues that need to be solved urgently. This paper proposes a new privacy protection algorithm designed to effectively prevent the leakage of user sensitive information. By introducing a differential privacy mechanism, our model ensures the accuracy and reliability of data analysis results while adding random noise. This method not only reduces the risk caused by data leakage but also achieves effective processing of data while protecting user privacy. Compared to traditional privacy methods like data anonymization and homomorphic encryption, our approach offers significant advantages in terms of computational efficiency and scalability while maintaining high accuracy in data analysis. The proposed algorithm's efficacy is demonstrated through performance metrics such as accuracy (0.89), precision (0.85), and recall (0.88), outperforming other methods in balancing privacy and utility. As privacy protection regulations become increasingly stringent, enterprises and developers must take effective measures to deal with privacy risks. Our research provides an important reference for the application of privacy protection technology in the field of NLP, emphasizing the need to achieve a balance between technological innovation and user privacy. In the future, with the continuous advancement of technology, privacy protection will become a core element of data-driven applications and promote the healthy development of the entire industry.</li>
</ul>

<h3>Title: Diffusion-Based Depth Inpainting for Transparent and Reflective Objects</h3>
<ul>
<li><strong>Authors: </strong>Tianyu Sun, Dingchang Hu, Yixiang Dai, Guijin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08567">https://arxiv.org/abs/2410.08567</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08567">https://arxiv.org/pdf/2410.08567</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08567]] Diffusion-Based Depth Inpainting for Transparent and Reflective Objects(https://arxiv.org/abs/2410.08567)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Transparent and reflective objects, which are common in our everyday lives, present a significant challenge to 3D imaging techniques due to their unique visual and optical properties. Faced with these types of objects, RGB-D cameras fail to capture the real depth value with their accurate spatial information. To address this issue, we propose DITR, a diffusion-based Depth Inpainting framework specifically designed for Transparent and Reflective objects. This network consists of two stages, including a Region Proposal stage and a Depth Inpainting stage. DITR dynamically analyzes the optical and geometric depth loss and inpaints them automatically. Furthermore, comprehensive experimental results demonstrate that DITR is highly effective in depth inpainting tasks of transparent and reflective objects with robust adaptability.</li>
</ul>

<h3>Title: DeBiFormer: Vision Transformer with Deformable Agent Bi-level Routing Attention</h3>
<ul>
<li><strong>Authors: </strong>Nguyen Huu Bao Long, Chenyu Zhang, Yuzhi Shi, Tsubasa Hirakawa, Takayoshi Yamashita, Tohgoroh Matsui, Hironobu Fujiyoshi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08582">https://arxiv.org/abs/2410.08582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08582">https://arxiv.org/pdf/2410.08582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08582]] DeBiFormer: Vision Transformer with Deformable Agent Bi-level Routing Attention(https://arxiv.org/abs/2410.08582)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Vision Transformers with various attention modules have demonstrated superior performance on vision tasks. While using sparsity-adaptive attention, such as in DAT, has yielded strong results in image classification, the key-value pairs selected by deformable points lack semantic relevance when fine-tuning for semantic segmentation tasks. The query-aware sparsity attention in BiFormer seeks to focus each query on top-k routed regions. However, during attention calculation, the selected key-value pairs are influenced by too many irrelevant queries, reducing attention on the more important ones. To address these issues, we propose the Deformable Bi-level Routing Attention (DBRA) module, which optimizes the selection of key-value pairs using agent queries and enhances the interpretability of queries in attention maps. Based on this, we introduce the Deformable Bi-level Routing Attention Transformer (DeBiFormer), a novel general-purpose vision transformer built with the DBRA module. DeBiFormer has been validated on various computer vision tasks, including image classification, object detection, and semantic segmentation, providing strong evidence of its this http URL is available at {this https URL}</li>
</ul>

<h3>Title: Retraining-Free Merging of Sparse Mixture-of-Experts via Hierarchical Clustering</h3>
<ul>
<li><strong>Authors: </strong>I-Chun Chen, Hsu-Shen Liu, Wei-Fang Sun, Chen-Hao Chao, Yen-Chang Hsu, Chun-Yi Lee</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08589">https://arxiv.org/abs/2410.08589</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08589">https://arxiv.org/pdf/2410.08589</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08589]] Retraining-Free Merging of Sparse Mixture-of-Experts via Hierarchical Clustering(https://arxiv.org/abs/2410.08589)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Sparse Mixture-of-Experts (SMoE) models represent a significant breakthrough in large language model development. These models enable performance improvements without a proportional increase in inference costs. By selectively activating a small set of parameters during task execution, SMoEs enhance model capacity. However, their deployment remains challenging due to the substantial memory footprint required to accommodate the growing number of experts. This constraint renders them less feasible in environments with limited hardware resources. To address this challenge, we propose Hierarchical Clustering for Sparsely activated Mixture of Experts (HC-SMoE), a task-agnostic expert merging framework that reduces SMoE model parameters without retraining. Unlike previous methods, HC-SMoE employs hierarchical clustering based on expert outputs. This approach ensures that the merging process remains unaffected by routing decisions. The output-based clustering strategy captures functional similarities between experts, offering an adaptable solution for models with numerous experts. We validate our approach through extensive experiments on eight zero-shot language tasks and demonstrate its effectiveness in large-scale SMoE models such as Qwen and Mixtral. Our comprehensive results demonstrate that HC-SMoE consistently achieves strong performance, which highlights its potential for real-world deployment.</li>
</ul>

<h3>Title: VERIFIED: A Video Corpus Moment Retrieval Benchmark for Fine-Grained Video Understanding</h3>
<ul>
<li><strong>Authors: </strong>Houlun Chen, Xin Wang, Hong Chen, Zeyang Zhang, Wei Feng, Bin Huang, Jia Jia, Wenwu Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08593">https://arxiv.org/abs/2410.08593</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08593">https://arxiv.org/pdf/2410.08593</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08593]] VERIFIED: A Video Corpus Moment Retrieval Benchmark for Fine-Grained Video Understanding(https://arxiv.org/abs/2410.08593)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Existing Video Corpus Moment Retrieval (VCMR) is limited to coarse-grained understanding, which hinders precise video moment localization when given fine-grained queries. In this paper, we propose a more challenging fine-grained VCMR benchmark requiring methods to localize the best-matched moment from the corpus with other partially matched candidates. To improve the dataset construction efficiency and guarantee high-quality data annotations, we propose VERIFIED, an automatic \underline{V}id\underline{E}o-text annotation pipeline to generate captions with \underline{R}el\underline{I}able \underline{FI}n\underline{E}-grained statics and \underline{D}ynamics. Specifically, we resort to large language models (LLM) and large multimodal models (LMM) with our proposed Statics and Dynamics Enhanced Captioning modules to generate diverse fine-grained captions for each video. To filter out the inaccurate annotations caused by the LLM hallucination, we propose a Fine-Granularity Aware Noise Evaluator where we fine-tune a video foundation model with disturbed hard-negatives augmented contrastive and matching losses. With VERIFIED, we construct a more challenging fine-grained VCMR benchmark containing Charades-FIG, DiDeMo-FIG, and ActivityNet-FIG which demonstrate a high level of annotation quality. We evaluate several state-of-the-art VCMR models on the proposed dataset, revealing that there is still significant scope for fine-grained video understanding in VCMR. Code and Datasets are in \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: Parameter-Efficient Fine-Tuning of Large Language Models using Semantic Knowledge Tuning</h3>
<ul>
<li><strong>Authors: </strong>Nusrat Jahan Prottasha, Asif Mahmud, Md. Shohanur Islam Sobuj, Prakash Bhat, Md Kowsher, Niloofar Yousefi, Ozlem Ozmen Garibay</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08598">https://arxiv.org/abs/2410.08598</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08598">https://arxiv.org/pdf/2410.08598</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08598]] Parameter-Efficient Fine-Tuning of Large Language Models using Semantic Knowledge Tuning(https://arxiv.org/abs/2410.08598)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are gaining significant popularity in recent years for specialized tasks using prompts due to their low computational cost. Standard methods like prefix tuning utilize special, modifiable tokens that lack semantic meaning and require extensive training for best performance, often falling short. In this context, we propose a novel method called Semantic Knowledge Tuning (SK-Tuning) for prompt and prefix tuning that employs meaningful words instead of random tokens. This method involves using a fixed LLM to understand and process the semantic content of the prompt through zero-shot capabilities. Following this, it integrates the processed prompt with the input text to improve the model's performance on particular tasks. Our experimental results show that SK-Tuning exhibits faster training times, fewer parameters, and superior performance on tasks such as text classification and understanding compared to other tuning methods. This approach offers a promising method for optimizing the efficiency and effectiveness of LLMs in processing language tasks.</li>
</ul>

<h3>Title: StraGo: Harnessing Strategic Guidance for Prompt Optimization</h3>
<ul>
<li><strong>Authors: </strong>Yurong Wu, Yan Gao, Bin Benjamin Zhu, Zineng Zhou, Xiaodi Sun, Sheng Yang, Jian-Guang Lou, Zhiming Ding, Linjun Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08601">https://arxiv.org/abs/2410.08601</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08601">https://arxiv.org/pdf/2410.08601</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08601]] StraGo: Harnessing Strategic Guidance for Prompt Optimization(https://arxiv.org/abs/2410.08601)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Prompt engineering is pivotal for harnessing the capabilities of large language models (LLMs) across diverse applications. While existing prompt optimization methods improve prompt effectiveness, they often lead to prompt drifting, where newly generated prompts can adversely impact previously successful cases while addressing failures. Furthermore, these methods tend to rely heavily on LLMs' intrinsic capabilities for prompt optimization tasks. In this paper, we introduce StraGo (Strategic-Guided Optimization), a novel approach designed to mitigate prompt drifting by leveraging insights from both successful and failed cases to identify critical factors for achieving optimization objectives. StraGo employs a how-to-do methodology, integrating in-context learning to formulate specific, actionable strategies that provide detailed, step-by-step guidance for prompt optimization. Extensive experiments conducted across a range of tasks, including reasoning, natural language understanding, domain-specific knowledge, and industrial applications, demonstrate StraGo's superior performance. It establishes a new state-of-the-art in prompt optimization, showcasing its ability to deliver stable and effective prompt improvements.</li>
</ul>

<h3>Title: MergePrint: Robust Fingerprinting against Merging Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shojiro Yamabe, Tsubasa Takahashi, Futa Waseda, Koki Wataoka</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08604">https://arxiv.org/abs/2410.08604</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08604">https://arxiv.org/pdf/2410.08604</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08604]] MergePrint: Robust Fingerprinting against Merging Large Language Models(https://arxiv.org/abs/2410.08604)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust, large language model</a></li>
<li><strong>Abstract: </strong>As the cost of training large language models (LLMs) rises, protecting their intellectual property has become increasingly critical. Model merging, which integrates multiple expert models into a single model capable of performing multiple tasks, presents a growing risk of unauthorized and malicious usage. While fingerprinting techniques have been studied for asserting model ownership, existing methods have primarily focused on fine-tuning, leaving model merging underexplored. To address this gap, we propose a novel fingerprinting method MergePrint that embeds robust fingerprints designed to preserve ownership claims even after model merging. By optimizing against a pseudo-merged model, which simulates post-merged model weights, MergePrint generates fingerprints that remain detectable after merging. Additionally, we optimize the fingerprint inputs to minimize performance degradation, enabling verification through specific outputs from targeted inputs. This approach provides a practical fingerprinting strategy for asserting ownership in cases of misappropriation through model merging.</li>
</ul>

<h3>Title: Text-To-Image with Generative Adversarial Networks</h3>
<ul>
<li><strong>Authors: </strong>Mehrshad Momen-Tayefeh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08608">https://arxiv.org/abs/2410.08608</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08608">https://arxiv.org/pdf/2410.08608</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08608]] Text-To-Image with Generative Adversarial Networks(https://arxiv.org/abs/2410.08608)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generating realistic images from human texts is one of the most challenging problems in the field of computer vision (CV). The meaning of descriptions given can be roughly reflected by existing text-to-image approaches. In this paper, our main purpose is to propose a brief comparison between five different methods base on the Generative Adversarial Networks (GAN) to make image from the text. In addition, each model architectures synthesis images with different resolution. Furthermore, the best and worst obtained resolutions is 64*64, 256*256 respectively. However, we checked and compared some metrics that introduce the accuracy of each model. Also, by doing this study, we found out the best model for this problem by comparing these different approaches essential metrics.</li>
</ul>

<h3>Title: Synth-SONAR: Sonar Image Synthesis with Enhanced Diversity and Realism via Dual Diffusion Models and GPT Prompting</h3>
<ul>
<li><strong>Authors: </strong>Purushothaman Natarajan, Kamal Basha, Athira Nambiar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08612">https://arxiv.org/abs/2410.08612</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08612">https://arxiv.org/pdf/2410.08612</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08612]] Synth-SONAR: Sonar Image Synthesis with Enhanced Diversity and Realism via Dual Diffusion Models and GPT Prompting(https://arxiv.org/abs/2410.08612)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Sonar image synthesis is crucial for advancing applications in underwater exploration, marine biology, and defence. Traditional methods often rely on extensive and costly data collection using sonar sensors, jeopardizing data quality and diversity. To overcome these limitations, this study proposes a new sonar image synthesis framework, Synth-SONAR leveraging diffusion models and GPT prompting. The key novelties of Synth-SONAR are threefold: First, by integrating Generative AI-based style injection techniques along with publicly available real/simulated data, thereby producing one of the largest sonar data corpus for sonar research. Second, a dual text-conditioning sonar diffusion model hierarchy synthesizes coarse and fine-grained sonar images with enhanced quality and diversity. Third, high-level (coarse) and low-level (detailed) text-based sonar generation methods leverage advanced semantic information available in visual language models (VLMs) and GPT-prompting. During inference, the method generates diverse and realistic sonar images from textual prompts, bridging the gap between textual descriptions and sonar image generation. This marks the application of GPT-prompting in sonar imagery for the first time, to the best of our knowledge. Synth-SONAR achieves state-of-the-art results in producing high-quality synthetic sonar datasets, significantly enhancing their diversity and realism.</li>
</ul>

<h3>Title: Cross-Modal Bidirectional Interaction Model for Referring Remote Sensing Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Zhe Dong, Yuzhe Sun, Yanfeng Gu, Tianzhu Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08613">https://arxiv.org/abs/2410.08613</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08613">https://arxiv.org/pdf/2410.08613</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08613]] Cross-Modal Bidirectional Interaction Model for Referring Remote Sensing Image Segmentation(https://arxiv.org/abs/2410.08613)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Given a natural language expression and a remote sensing image, the goal of referring remote sensing image segmentation (RRSIS) is to generate a pixel-level mask of the target object identified by the referring expression. In contrast to natural scenarios, expressions in RRSIS often involve complex geospatial relationships, with target objects of interest that vary significantly in scale and lack visual saliency, thereby increasing the difficulty of achieving precise segmentation. To address the aforementioned challenges, a novel RRSIS framework is proposed, termed the cross-modal bidirectional interaction model (CroBIM). Specifically, a context-aware prompt modulation (CAPM) module is designed to integrate spatial positional relationships and task-specific knowledge into the linguistic features, thereby enhancing the ability to capture the target object. Additionally, a language-guided feature aggregation (LGFA) module is introduced to integrate linguistic information into multi-scale visual features, incorporating an attention deficit compensation mechanism to enhance feature aggregation. Finally, a mutual-interaction decoder (MID) is designed to enhance cross-modal feature alignment through cascaded bidirectional cross-attention, thereby enabling precise segmentation mask prediction. To further forster the research of RRSIS, we also construct RISBench, a new large-scale benchmark dataset comprising 52,472 image-language-label triplets. Extensive benchmarking on RISBench and two other prevalent datasets demonstrates the superior performance of the proposed CroBIM over existing state-of-the-art (SOTA) methods. The source code for CroBIM and the RISBench dataset will be publicly available at this https URL</li>
</ul>

<h3>Title: Natural Language Induced Adversarial Images</h3>
<ul>
<li><strong>Authors: </strong>Xiaopei Zhu, Peiyang Xu, Guanning Zeng, Yingpeng Dong, Xiaolin Hu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08620">https://arxiv.org/abs/2410.08620</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08620">https://arxiv.org/pdf/2410.08620</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08620]] Natural Language Induced Adversarial Images(https://arxiv.org/abs/2410.08620)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>Research of adversarial attacks is important for AI security because it shows the vulnerability of deep learning models and helps to build more robust models. Adversarial attacks on images are most widely studied, which include noise-based attacks, image editing-based attacks, and latent space-based attacks. However, the adversarial examples crafted by these methods often lack sufficient semantic information, making it challenging for humans to understand the failure modes of deep learning models under natural conditions. To address this limitation, we propose a natural language induced adversarial image attack method. The core idea is to leverage a text-to-image model to generate adversarial images given input prompts, which are maliciously constructed to lead to misclassification for a target model. To adopt commercial text-to-image models for synthesizing more natural adversarial images, we propose an adaptive genetic algorithm (GA) for optimizing discrete adversarial prompts without requiring gradients and an adaptive word space reduction method for improving query efficiency. We further used CLIP to maintain the semantic consistency of the generated images. In our experiments, we found that some high-frequency semantic information such as "foggy", "humid", "stretching", etc. can easily cause classifier errors. This adversarial semantic information exists not only in generated images but also in photos captured in the real world. We also found that some adversarial semantic information can be transferred to unknown classification tasks. Furthermore, our attack method can transfer to different text-to-image models (e.g., Midjourney, DALL-E 3, etc.) and image classifiers. Our code is available at: this https URL.</li>
</ul>

<h3>Title: Transformers Provably Solve Parity Efficiently with Chain of Thought</h3>
<ul>
<li><strong>Authors: </strong>Juno Kim, Taiji Suzuki</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08633">https://arxiv.org/abs/2410.08633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08633">https://arxiv.org/pdf/2410.08633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08633]] Transformers Provably Solve Parity Efficiently with Chain of Thought(https://arxiv.org/abs/2410.08633)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This work provides the first theoretical analysis of training transformers to solve complex problems by recursively generating intermediate states, analogous to fine-tuning for chain-of-thought (CoT) reasoning. We consider training a one-layer transformer to solve the fundamental $k$-parity problem, extending the work on RNNs by Wies et al. (2023). We establish three key results: (1) any finite-precision gradient-based algorithm, without intermediate supervision, requires substantial iterations to solve parity with finite samples. (2) In contrast, when intermediate parities are incorporated into the loss function, our model can learn parity in one gradient update when aided by \emph{teacher forcing}, where ground-truth labels of the reasoning chain are provided at each generation step. (3) Even without teacher forcing, where the model must generate CoT chains end-to-end, parity can be learned efficiently if augmented data is employed to internally verify the soundness of intermediate steps. These results rigorously show that task decomposition and stepwise reasoning naturally arise from optimizing transformers with CoT; moreover, self-consistency checking can improve reasoning ability, aligning with empirical studies of CoT.</li>
</ul>

<h3>Title: GAI-Enabled Explainable Personalized Federated Semi-Supervised Learning</h3>
<ul>
<li><strong>Authors: </strong>Yubo Peng, Feibo Jiang, Li Dong, Kezhi Wang, Kun Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08634">https://arxiv.org/abs/2410.08634</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08634">https://arxiv.org/pdf/2410.08634</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08634]] GAI-Enabled Explainable Personalized Federated Semi-Supervised Learning(https://arxiv.org/abs/2410.08634)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate, explainability, generative</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) is a commonly distributed algorithm for mobile users (MUs) training artificial intelligence (AI) models, however, several challenges arise when applying FL to real-world scenarios, such as label scarcity, non-IID data, and unexplainability. As a result, we propose an explainable personalized FL framework, called XPFL. First, we introduce a generative AI (GAI) assisted personalized federated semi-supervised learning, called GFed. Particularly, in local training, we utilize a GAI model to learn from large unlabeled data and apply knowledge distillation-based semi-supervised learning to train the local FL model using the knowledge acquired from the GAI model. In global aggregation, we obtain the new local FL model by fusing the local and global FL models in specific proportions, allowing each local model to incorporate knowledge from others while preserving its personalized characteristics. Second, we propose an explainable AI mechanism for FL, named XFed. Specifically, in local training, we apply a decision tree to match the input and output of the local FL model. In global aggregation, we utilize t-distributed stochastic neighbor embedding (t-SNE) to visualize the local models before and after aggregation. Finally, simulation results validate the effectiveness of the proposed XPFL framework.</li>
</ul>

<h3>Title: E-Motion: Future Motion Simulation via Event Sequence Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Song Wu, Zhiyu Zhu, Junhui Hou, Guangming Shi, Jinjian Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08649">https://arxiv.org/abs/2410.08649</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08649">https://arxiv.org/pdf/2410.08649</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08649]] E-Motion: Future Motion Simulation via Event Sequence Diffusion(https://arxiv.org/abs/2410.08649)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Forecasting a typical object's future motion is a critical task for interpreting and interacting with dynamic environments in computer vision. Event-based sensors, which could capture changes in the scene with exceptional temporal granularity, may potentially offer a unique opportunity to predict future motion with a level of detail and precision previously unachievable. Inspired by that, we propose to integrate the strong learning capacity of the video diffusion model with the rich motion information of an event camera as a motion simulation framework. Specifically, we initially employ pre-trained stable video diffusion models to adapt the event sequence dataset. This process facilitates the transfer of extensive knowledge from RGB videos to an event-centric domain. Moreover, we introduce an alignment mechanism that utilizes reinforcement learning techniques to enhance the reverse generation trajectory of the diffusion model, ensuring improved performance and accuracy. Through extensive testing and validation, we demonstrate the effectiveness of our method in various complex scenarios, showcasing its potential to revolutionize motion flow prediction in computer vision applications such as autonomous vehicle guidance, robotic navigation, and interactive media. Our findings suggest a promising direction for future research in enhancing the interpretative power and predictive accuracy of computer vision systems.</li>
</ul>

<h3>Title: Finite Sample Complexity Analysis of Binary Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Toby Dylan Hocking</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.CO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08654">https://arxiv.org/abs/2410.08654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08654">https://arxiv.org/pdf/2410.08654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08654]] Finite Sample Complexity Analysis of Binary Segmentation(https://arxiv.org/abs/2410.08654)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Binary segmentation is the classic greedy algorithm which recursively splits a sequential data set by optimizing some loss or likelihood function. Binary segmentation is widely used for changepoint detection in data sets measured over space or time, and as a sub-routine for decision tree learning. In theory it should be extremely fast for $N$ data and $K$ splits, $O(N K)$ in the worst case, and $O(N \log K)$ in the best case. In this paper we describe new methods for analyzing the time and space complexity of binary segmentation for a given finite $N$, $K$, and minimum segment length parameter. First, we describe algorithms that can be used to compute the best and worst case number of splits the algorithm must consider. Second, we describe synthetic data that achieve the best and worst case and which can be used to test for correct implementation of the algorithm. Finally, we provide an empirical analysis of real data which suggests that binary segmentation is often close to optimal speed in practice.</li>
</ul>

<h3>Title: RePD: Defending Jailbreak Attack through a Retrieval-based Prompt Decomposition Process</h3>
<ul>
<li><strong>Authors: </strong>Peiran Wang, Xiaogeng Liu, Chaowei Xiao</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08660">https://arxiv.org/abs/2410.08660</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08660">https://arxiv.org/pdf/2410.08660</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08660]] RePD: Defending Jailbreak Attack through a Retrieval-based Prompt Decomposition Process(https://arxiv.org/abs/2410.08660)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>In this study, we introduce RePD, an innovative attack Retrieval-based Prompt Decomposition framework designed to mitigate the risk of jailbreak attacks on large language models (LLMs). Despite rigorous pretraining and finetuning focused on ethical alignment, LLMs are still susceptible to jailbreak exploits. RePD operates on a one-shot learning model, wherein it accesses a database of pre-collected jailbreak prompt templates to identify and decompose harmful inquiries embedded within user prompts. This process involves integrating the decomposition of the jailbreak prompt into the user's original query into a one-shot learning example to effectively teach the LLM to discern and separate malicious components. Consequently, the LLM is equipped to first neutralize any potentially harmful elements before addressing the user's prompt in a manner that aligns with its ethical guidelines. RePD is versatile and compatible with a variety of open-source LLMs acting as agents. Through comprehensive experimentation with both harmful and benign prompts, we have demonstrated the efficacy of our proposed RePD in enhancing the resilience of LLMs against jailbreak attacks, without compromising their performance in responding to typical user requests.</li>
</ul>

<h3>Title: QEFT: Quantization for Efficient Fine-Tuning of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Changhun Lee, Jun-gyu Jin, Younghyun Cho, Eunhyeok Park</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08661">https://arxiv.org/abs/2410.08661</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08661">https://arxiv.org/pdf/2410.08661</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08661]] QEFT: Quantization for Efficient Fine-Tuning of LLMs(https://arxiv.org/abs/2410.08661)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>With the rapid growth in the use of fine-tuning for large language models (LLMs), optimizing fine-tuning while keeping inference efficient has become highly important. However, this is a challenging task as it requires improvements in all aspects, including inference speed, fine-tuning speed, memory consumption, and, most importantly, model quality. Previous studies have attempted to achieve this by combining quantization with fine-tuning, but they have failed to enhance all four aspects simultaneously. In this study, we propose a new lightweight technique called Quantization for Efficient Fine-Tuning (QEFT). QEFT accelerates both inference and fine-tuning, is supported by robust theoretical foundations, offers high flexibility, and maintains good hardware compatibility. Our extensive experiments demonstrate that QEFT matches the quality and versatility of full-precision parameter-efficient fine-tuning, while using fewer resources. Our code is available at this https URL.</li>
</ul>

<h3>Title: DistDD: Distributed Data Distillation Aggregation through Gradient Matching</h3>
<ul>
<li><strong>Authors: </strong>Peiran Wang, Haohan Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08665">https://arxiv.org/abs/2410.08665</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08665">https://arxiv.org/pdf/2410.08665</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08665]] DistDD: Distributed Data Distillation Aggregation through Gradient Matching(https://arxiv.org/abs/2410.08665)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce DistDD, a novel approach within the federated learning framework that reduces the need for repetitive communication by distilling data directly on clients' devices. Unlike traditional federated learning that requires iterative model updates across nodes, DistDD facilitates a one-time distillation process that extracts a global distilled dataset, maintaining the privacy standards of federated learning while significantly cutting down communication costs. By leveraging the DistDD's distilled dataset, the developers of the FL can achieve just-in-time parameter tuning and neural architecture search over FL without repeating the whole FL process multiple times. We provide a detailed convergence proof of the DistDD algorithm, reinforcing its mathematical stability and reliability for practical applications. Our experiments demonstrate the effectiveness and robustness of DistDD, particularly in non-i.i.d. and mislabeled data scenarios, showcasing its potential to handle complex real-world data challenges distinctively from conventional federated learning methods. We also evaluate DistDD's application in the use case and prove its effectiveness and communication-savings in the NAS use case.</li>
</ul>

<h3>Title: DeltaDQ: Ultra-High Delta Compression for Fine-Tuned LLMs via Group-wise Dropout and Separate Quantization</h3>
<ul>
<li><strong>Authors: </strong>Yanfeng Jiang, Zelan Yang, Bohua Chen, Shen Li, Yong Li, Tao Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08666">https://arxiv.org/abs/2410.08666</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08666">https://arxiv.org/pdf/2410.08666</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08666]] DeltaDQ: Ultra-High Delta Compression for Fine-Tuned LLMs via Group-wise Dropout and Separate Quantization(https://arxiv.org/abs/2410.08666)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models achieve exceptional performance on various downstream tasks through supervised fine-tuning. However, the diversity of downstream tasks and practical requirements makes deploying multiple full-parameter fine-tuned models challenging. Current methods that compress the delta weight struggle to achieve ultra-high compression, failing to minimize the deployment overhead. To address the above issue, we propose a novel distribution-driven delta compression framework DeltaDQ, which utilizes Group-wise Dropout and Separate Quantization to achieve ultra-high compression for the delta weight. We have observed that the matrix-computed intermediate results for the delta weight exhibit extremely small variance and min-max range characteristics, referred to as Balanced Intermediate Results. Exploiting this phenomenon, we introduce Group-wise Dropout to perform dropout on the delta weight using an optimal group size. Furthermore, using Separate Quantization, sparse weights are quantized and decomposed to achieve a lower bit. Experimental results show that DeltaDQ achieves 16x compression with improved accuracy compared to baselines for WizardMath and WizardCoder models across different parameter scales. Moreover, DeltaDQ demonstrates the ability for ultra-high compression ratio, achieving 128x compression for the WizardMath-7B model and 512x compression for the WizardMath-70B model.</li>
</ul>

<h3>Title: SmartPretrain: Model-Agnostic and Dataset-Agnostic Representation Learning for Motion Prediction</h3>
<ul>
<li><strong>Authors: </strong>Yang Zhou, Hao Shao, Letian Wang, Steven L. Waslander, Hongsheng Li, Yu Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08669">https://arxiv.org/abs/2410.08669</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08669">https://arxiv.org/pdf/2410.08669</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08669]] SmartPretrain: Model-Agnostic and Dataset-Agnostic Representation Learning for Motion Prediction(https://arxiv.org/abs/2410.08669)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Predicting the future motion of surrounding agents is essential for autonomous vehicles (AVs) to operate safely in dynamic, human-robot-mixed environments. However, the scarcity of large-scale driving datasets has hindered the development of robust and generalizable motion prediction models, limiting their ability to capture complex interactions and road geometries. Inspired by recent advances in natural language processing (NLP) and computer vision (CV), self-supervised learning (SSL) has gained significant attention in the motion prediction community for learning rich and transferable scene representations. Nonetheless, existing pre-training methods for motion prediction have largely focused on specific model architectures and single dataset, limiting their scalability and generalizability. To address these challenges, we propose SmartPretrain, a general and scalable SSL framework for motion prediction that is both model-agnostic and dataset-agnostic. Our approach integrates contrastive and reconstructive SSL, leveraging the strengths of both generative and discriminative paradigms to effectively represent spatiotemporal evolution and interactions without imposing architectural constraints. Additionally, SmartPretrain employs a dataset-agnostic scenario sampling strategy that integrates multiple datasets, enhancing data volume, diversity, and robustness. Extensive experiments on multiple datasets demonstrate that SmartPretrain consistently improves the performance of state-of-the-art prediction models across datasets, data splits and main metrics. For instance, SmartPretrain significantly reduces the MissRate of Forecast-MAE by 10.6%. These results highlight SmartPretrain's effectiveness as a unified, scalable solution for motion prediction, breaking free from the limitations of the small-data regime. Codes are available at this https URL</li>
</ul>

<h3>Title: Gait Sequence Upsampling using Diffusion Models for single LiDAR sensors</h3>
<ul>
<li><strong>Authors: </strong>Jeongho Ahn, Kazuto Nakashima, Koki Yoshino, Yumi Iwashita, Ryo Kurazume</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08680">https://arxiv.org/abs/2410.08680</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08680">https://arxiv.org/pdf/2410.08680</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08680]] Gait Sequence Upsampling using Diffusion Models for single LiDAR sensors(https://arxiv.org/abs/2410.08680)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recently, 3D LiDAR has emerged as a promising technique in the field of gait-based person identification, serving as an alternative to traditional RGB cameras, due to its robustness under varying lighting conditions and its ability to capture 3D geometric information. However, long capture distances or the use of low-cost LiDAR sensors often result in sparse human point clouds, leading to a decline in identification performance. To address these challenges, we propose a sparse-to-dense upsampling model for pedestrian point clouds in LiDAR-based gait recognition, named LidarGSU, which is designed to improve the generalization capability of existing identification models. Our method utilizes diffusion probabilistic models (DPMs), which have shown high fidelity in generative tasks such as image completion. In this work, we leverage DPMs on sparse sequential pedestrian point clouds as conditional masks in a video-to-video translation approach, applied in an inpainting manner. We conducted extensive experiments on the SUSTeck1K dataset to evaluate the generative quality and recognition performance of the proposed method. Furthermore, we demonstrate the applicability of our upsampling model using a real-world dataset, captured with a low-resolution sensor across varying measurement distances.</li>
</ul>

<h3>Title: Uncertainty Estimation and Out-of-Distribution Detection for LiDAR Scene Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Hanieh Shojaei, Qianqian Zou, Max Mehltretter</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08687">https://arxiv.org/abs/2410.08687</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08687">https://arxiv.org/pdf/2410.08687</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08687]] Uncertainty Estimation and Out-of-Distribution Detection for LiDAR Scene Semantic Segmentation(https://arxiv.org/abs/2410.08687)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Safe navigation in new environments requires autonomous vehicles and robots to accurately interpret their surroundings, relying on LiDAR scene segmentation, out-of-distribution (OOD) obstacle detection, and uncertainty computation. We propose a method to distinguish in-distribution (ID) from OOD samples and quantify both epistemic and aleatoric uncertainties using the feature space of a single deterministic model. After training a semantic segmentation network, a Gaussian Mixture Model (GMM) is fitted to its feature space. OOD samples are detected by checking if their squared Mahalanobis distances to each Gaussian component conform to a chi-squared distribution, eliminating the need for an additional OOD training set. Given that the estimated mean and covariance matrix of a multivariate Gaussian distribution follow Gaussian and Inverse-Wishart distributions, multiple GMMs are generated by sampling from these distributions to assess epistemic uncertainty through classification variability. Aleatoric uncertainty is derived from the entropy of responsibility values within Gaussian components. Comparing our method with deep ensembles and logit-sampling for uncertainty computation demonstrates its superior performance in real-world applications for quantifying epistemic and aleatoric uncertainty, as well as detecting OOD samples. While deep ensembles miss some highly uncertain samples, our method successfully detects them and assigns high epistemic uncertainty.</li>
</ul>

<h3>Title: Dynamic Multimodal Evaluation with Flexible Complexity by Vision-Language Bootstrapping</h3>
<ul>
<li><strong>Authors: </strong>Yue Yang, Shuibai Zhang, Wenqi Shao, Kaipeng Zhang, Yi Bin, Yu Wang, Ping Luo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08695">https://arxiv.org/abs/2410.08695</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08695">https://arxiv.org/pdf/2410.08695</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08695]] Dynamic Multimodal Evaluation with Flexible Complexity by Vision-Language Bootstrapping(https://arxiv.org/abs/2410.08695)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities across multimodal tasks such as visual perception and reasoning, leading to good performance on various multimodal evaluation benchmarks. However, these benchmarks keep a static nature and overlap with the pre-training data, resulting in fixed complexity constraints and data contamination issues. This raises the concern regarding the validity of the evaluation. To address these two challenges, we introduce a dynamic multimodal evaluation protocol called Vision-Language Bootstrapping (VLB). VLB provides a robust and comprehensive assessment for LVLMs with reduced data contamination and flexible complexity. To this end, VLB dynamically generates new visual question-answering samples through a multimodal bootstrapping module that modifies both images and language, while ensuring that newly generated samples remain consistent with the original ones by a judge module. By composing various bootstrapping strategies, VLB offers dynamic variants of existing benchmarks with diverse complexities, enabling the evaluation to co-evolve with the ever-evolving capabilities of LVLMs. Extensive experimental results across multiple benchmarks, including SEEDBench, MMBench, and MME, show that VLB significantly reduces data contamination and exposes performance limitations of LVLMs.</li>
</ul>

<h3>Title: AMPO: Automatic Multi-Branched Prompt Optimization</h3>
<ul>
<li><strong>Authors: </strong>Sheng Yang, Yurong Wu, Yan Gao, Zineng Zhou, Bin Benjamin Zhu, Xiaodi Sun, Jian-Guang Lou, Zhiming Ding, Anbang Hu, Yuan Fang, Yunsong Li, Junyan Chen, Linjun Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08696">https://arxiv.org/abs/2410.08696</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08696">https://arxiv.org/pdf/2410.08696</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08696]] AMPO: Automatic Multi-Branched Prompt Optimization(https://arxiv.org/abs/2410.08696)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Prompt engineering is very important to enhance the performance of large language models (LLMs). When dealing with complex issues, prompt engineers tend to distill multiple patterns from examples and inject relevant solutions to optimize the prompts, achieving satisfying results. However, existing automatic prompt optimization techniques are only limited to producing single flow instructions, struggling with handling diverse patterns. In this paper, we present AMPO, an automatic prompt optimization method that can iteratively develop a multi-branched prompt using failure cases as feedback. Our goal is to explore a novel way of structuring prompts with multi-branches to better handle multiple patterns in complex tasks, for which we introduce three modules: Pattern Recognition, Branch Adjustment, and Branch Pruning. In experiments across five tasks, AMPO consistently achieves the best results. Additionally, our approach demonstrates significant optimization efficiency due to our adoption of a minimal search strategy.</li>
</ul>

<h3>Title: SocialGaze: Improving the Integration of Human Social Norms in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Anvesh Rao Vijjini, Rakesh R. Menon, Jiayi Fu, Shashank Srivastava, Snigdha Chaturvedi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08698">https://arxiv.org/abs/2410.08698</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08698">https://arxiv.org/pdf/2410.08698</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08698]] SocialGaze: Improving the Integration of Human Social Norms in Large Language Models(https://arxiv.org/abs/2410.08698)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>While much research has explored enhancing the reasoning capabilities of large language models (LLMs) in the last few years, there is a gap in understanding the alignment of these models with social values and norms. We introduce the task of judging social acceptance. Social acceptance requires models to judge and rationalize the acceptability of people's actions in social situations. For example, is it socially acceptable for a neighbor to ask others in the community to keep their pets indoors at night? We find that LLMs' understanding of social acceptance is often misaligned with human consensus. To alleviate this, we introduce SocialGaze, a multi-step prompting framework, in which a language model verbalizes a social situation from multiple perspectives before forming a judgment. Our experiments demonstrate that the SocialGaze approach improves the alignment with human judgments by up to 11 F1 points with the GPT-3.5 model. We also identify biases and correlations in LLMs in assigning blame that is related to features such as the gender (males are significantly more likely to be judged unfairly) and age (LLMs are more aligned with humans for older narrators).</li>
</ul>

<h3>Title: Progressive Pruning: Estimating Anonymity of Stream-Based Communication</h3>
<ul>
<li><strong>Authors: </strong>Christoph Döpmann, Maximilian Weisenseel, Florian Tschorsch</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08700">https://arxiv.org/abs/2410.08700</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08700">https://arxiv.org/pdf/2410.08700</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08700]] Progressive Pruning: Estimating Anonymity of Stream-Based Communication(https://arxiv.org/abs/2410.08700)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Streams of data have become the ubiquitous communication model on today's Internet. For strong anonymous communication, this conflicts with the traditional notion of single, independent messages, as assumed e.g. by many mixnet designs. In this work, we investigate the anonymity factors that are inherent to stream communication. We introduce Progressive Pruning}, a methodology suitable for estimating the anonymity level of streams. By mimicking an intersection attack, it captures the susceptibility of streams against traffic analysis attacks. We apply it to simulations of tailored examples of stream communication as well as to large-scale simulations of Tor using our novel TorFS simulator, finding that the stream length, the number of users, and how streams are distributed over the network have interdependent impacts on anonymity. Our work draws attention to challenges that need to be solved in order to provide strong anonymity for stream-based communication in the future.</li>
</ul>

<h3>Title: Distillation of Discrete Diffusion through Dimensional Correlations</h3>
<ul>
<li><strong>Authors: </strong>Satoshi Hayakawa, Yuhta Takida, Masaaki Imaizumi, Hiromi Wakaki, Yuki Mitsufuji</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08709">https://arxiv.org/abs/2410.08709</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08709">https://arxiv.org/pdf/2410.08709</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08709]] Distillation of Discrete Diffusion through Dimensional Correlations(https://arxiv.org/abs/2410.08709)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated exceptional performances in various fields of generative modeling. While they often outperform competitors including VAEs and GANs in sample quality and diversity, they suffer from slow sampling speed due to their iterative nature. Recently, distillation techniques and consistency models are mitigating this issue in continuous domains, but discrete diffusion models have some specific challenges towards faster generation. Most notably, in the current literature, correlations between different dimensions (pixels, locations) are ignored, both by its modeling and loss functions, due to computational limitations. In this paper, we propose "mixture" models in discrete diffusion that are capable of treating dimensional correlations while remaining scalable, and we provide a set of loss functions for distilling the iterations of existing models. Two primary theoretical insights underpin our approach: first, that dimensionally independent models can well approximate the data distribution if they are allowed to conduct many sampling steps, and second, that our loss functions enables mixture models to distill such many-step conventional models into just a few steps by learning the dimensional correlations. We empirically demonstrate that our proposed method for discrete diffusions work in practice, by distilling a continuous-time discrete diffusion model pretrained on the CIFAR-10 dataset.</li>
</ul>

<h3>Title: Preferential Normalizing Flows</h3>
<ul>
<li><strong>Authors: </strong>Petrus Mikkola, Luigi Acerbi, Arto Klami</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08710">https://arxiv.org/abs/2410.08710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08710">https://arxiv.org/pdf/2410.08710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08710]] Preferential Normalizing Flows(https://arxiv.org/abs/2410.08710)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Eliciting a high-dimensional probability distribution from an expert via noisy judgments is notoriously challenging, yet useful for many applications, such as prior elicitation and reward modeling. We introduce a method for eliciting the expert's belief density as a normalizing flow based solely on preferential questions such as comparing or ranking alternatives. This allows eliciting in principle arbitrarily flexible densities, but flow estimation is susceptible to the challenge of collapsing or diverging probability mass that makes it difficult in practice. We tackle this problem by introducing a novel functional prior for the flow, motivated by a decision-theoretic argument, and show empirically that the belief density can be inferred as the function-space maximum a posteriori estimate. We demonstrate our method by eliciting multivariate belief densities of simulated experts, including the prior belief of a general-purpose large language model over a real-world dataset.</li>
</ul>

<h3>Title: From N-grams to Pre-trained Multilingual Models For Language Identification</h3>
<ul>
<li><strong>Authors: </strong>Thapelo Sindane, Vukosi Marivate</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08728">https://arxiv.org/abs/2410.08728</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08728">https://arxiv.org/pdf/2410.08728</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08728]] From N-grams to Pre-trained Multilingual Models For Language Identification(https://arxiv.org/abs/2410.08728)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In this paper, we investigate the use of N-gram models and Large Pre-trained Multilingual models for Language Identification (LID) across 11 South African languages. For N-gram models, this study shows that effective data size selection remains crucial for establishing effective frequency distributions of the target languages, that efficiently model each language, thus, improving language ranking. For pre-trained multilingual models, we conduct extensive experiments covering a diverse set of massively pre-trained multilingual (PLM) models -- mBERT, RemBERT, XLM-r, and Afri-centric multilingual models -- AfriBERTa, Afro-XLMr, AfroLM, and Serengeti. We further compare these models with available large-scale Language Identification tools: Compact Language Detector v3 (CLD V3), AfroLID, GlotLID, and OpenLID to highlight the importance of focused-based LID. From these, we show that Serengeti is a superior model across models: N-grams to Transformers on average. Moreover, we propose a lightweight BERT-based LID model (za_BERT_lid) trained with NHCLT + Vukzenzele corpus, which performs on par with our best-performing Afri-centric models.</li>
</ul>

<h3>Title: Developing a Pragmatic Benchmark for Assessing Korean Legal Language Understanding in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yeeun Kim, Young Rok Choi, Eunkyung Choi, Jinhwan Choi, Hai Jin Park, Wonseok Hwang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08731">https://arxiv.org/abs/2410.08731</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08731">https://arxiv.org/pdf/2410.08731</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08731]] Developing a Pragmatic Benchmark for Assessing Korean Legal Language Understanding in Large Language Models(https://arxiv.org/abs/2410.08731)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated remarkable performance in the legal domain, with GPT-4 even passing the Uniform Bar Exam in the U.S. However their efficacy remains limited for non-standardized tasks and tasks in languages other than English. This underscores the need for careful evaluation of LLMs within each legal system before application. Here, we introduce KBL, a benchmark for assessing the Korean legal language understanding of LLMs, consisting of (1) 7 legal knowledge tasks (510 examples), (2) 4 legal reasoning tasks (288 examples), and (3) the Korean bar exam (4 domains, 53 tasks, 2,510 examples). First two datasets were developed in close collaboration with lawyers to evaluate LLMs in practical scenarios in a certified manner. Furthermore, considering legal practitioners' frequent use of extensive legal documents for research, we assess LLMs in both a closed book setting, where they rely solely on internal knowledge, and a retrieval-augmented generation (RAG) setting, using a corpus of Korean statutes and precedents. The results indicate substantial room and opportunities for improvement.</li>
</ul>

<h3>Title: Gradients Stand-in for Defending Deep Leakage in Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>H. Yi, H. Ren, C. Hu, Y. Li, J. Deng, X. Xie</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08734">https://arxiv.org/abs/2410.08734</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08734">https://arxiv.org/pdf/2410.08734</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08734]] Gradients Stand-in for Defending Deep Leakage in Federated Learning(https://arxiv.org/abs/2410.08734)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, defense, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) has become a cornerstone of privacy protection, shifting the paradigm towards localizing sensitive data while only sending model gradients to a central server. This strategy is designed to reinforce privacy protections and minimize the vulnerabilities inherent in centralized data storage systems. Despite its innovative approach, recent empirical studies have highlighted potential weaknesses in FL, notably regarding the exchange of gradients. In response, this study introduces a novel, efficacious method aimed at safeguarding against gradient leakage, namely, ``AdaDefense". Following the idea that model convergence can be achieved by using different types of optimization methods, we suggest using a local stand-in rather than the actual local gradient for global gradient aggregation on the central server. This proposed approach not only effectively prevents gradient leakage, but also ensures that the overall performance of the model remains largely unaffected. Delving into the theoretical dimensions, we explore how gradients may inadvertently leak private information and present a theoretical framework supporting the efficacy of our proposed method. Extensive empirical tests, supported by popular benchmark experiments, validate that our approach maintains model integrity and is robust against gradient leakage, marking an important step in our pursuit of safe and efficient FL.</li>
</ul>

<h3>Title: Bad Neighbors: On Understanding VPN Provider Networks</h3>
<ul>
<li><strong>Authors: </strong>Teemu Rytilahti (Ruhr University Bochum), Thorsten Holz (CISPA Helmholtz Center for Information Security)</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08737">https://arxiv.org/abs/2410.08737</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08737">https://arxiv.org/pdf/2410.08737</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08737]] Bad Neighbors: On Understanding VPN Provider Networks(https://arxiv.org/abs/2410.08737)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, protect</a></li>
<li><strong>Abstract: </strong>Virtual Private Network (VPN) solutions are used to connect private networks securely over the Internet. Besides their benefits in corporate environments, VPNs are also marketed to privacy-minded users to preserve their privacy, and to bypass geolocation-based content blocking and censorship. This has created a market for turnkey VPN services offering a multitude of vantage points all over the world for a monthly price. While VPN providers are heavily using privacy and security benefits in their marketing, such claims are generally hard to measure and substantiate. While there exist some studies on the VPN ecosystem, all prior works omit a critical part in their analyses: (i) How well do the providers configure and secure their own network infrastructure? and (ii) How well are they protecting their customers from other customers? To answer these questions, we have developed an automated measurement system with which we conduct a large-scale analysis of VPN providers and their thousands of VPN endpoints. Considering the fact that VPNs work internally using non-Internet-routable IP addresses, they might enable access to otherwise inaccessible networks. If not properly secured, this can inadvertently expose internal networks of these providers, or worse, even other clients connected to their services. Our results indicate a widespread lack of traffic filtering towards internally routable networks on the majority of tested VPN service providers, even in cases where no other VPN customers were directly exposed. We have disclosed our findings to the affected providers and other stakeholders, and offered guidance to improve the situation.</li>
</ul>

<h3>Title: Hespi: A pipeline for automatically detecting information from hebarium specimen sheets</h3>
<ul>
<li><strong>Authors: </strong>Robert Turnbull, Emily Fitzgerald, Karen Thompson, Joanne L. Birch</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08740">https://arxiv.org/abs/2410.08740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08740">https://arxiv.org/pdf/2410.08740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08740]] Hespi: A pipeline for automatically detecting information from hebarium specimen sheets(https://arxiv.org/abs/2410.08740)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Specimen associated biodiversity data are sought after for biological, environmental, climate, and conservation sciences. A rate shift is required for the extraction of data from specimen images to eliminate the bottleneck that the reliance on human-mediated transcription of these data represents. We applied advanced computer vision techniques to develop the `Hespi' (HErbarium Specimen sheet PIpeline), which extracts a pre-catalogue subset of collection data on the institutional labels on herbarium specimens from their digital images. The pipeline integrates two object detection models; the first detects bounding boxes around text-based labels and the second detects bounding boxes around text-based data fields on the primary institutional label. The pipeline classifies text-based institutional labels as printed, typed, handwritten, or a combination and applies Optical Character Recognition (OCR) and Handwritten Text Recognition (HTR) for data extraction. The recognized text is then corrected against authoritative databases of taxon names. The extracted text is also corrected with the aide of a multimodal Large Language Model (LLM). Hespi accurately detects and extracts text for test datasets including specimen sheet images from international herbaria. The components of the pipeline are modular and users can train their own models with their own data and use them in place of the models provided.</li>
</ul>

<h3>Title: PILLAR: an AI-Powered Privacy Threat Modeling Tool</h3>
<ul>
<li><strong>Authors: </strong>Majid Mollaeefar, Andrea Bissoli, Silvio Ranise</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08755">https://arxiv.org/abs/2410.08755</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08755">https://arxiv.org/pdf/2410.08755</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08755]] PILLAR: an AI-Powered Privacy Threat Modeling Tool(https://arxiv.org/abs/2410.08755)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, large language model</a></li>
<li><strong>Abstract: </strong>The rapid evolution of Large Language Models (LLMs) has unlocked new possibilities for applying artificial intelligence across a wide range of fields, including privacy engineering. As modern applications increasingly handle sensitive user data, safeguarding privacy has become more critical than ever. To protect privacy effectively, potential threats need to be identified and addressed early in the system development process. Frameworks like LINDDUN offer structured approaches for uncovering these risks, but despite their value, they often demand substantial manual effort, expert input, and detailed system knowledge. This makes the process time-consuming and prone to errors. Current privacy threat modeling methods, such as LINDDUN, typically rely on creating and analyzing complex data flow diagrams (DFDs) and system descriptions to pinpoint potential privacy issues. While these approaches are thorough, they can be cumbersome, relying heavily on the precision of the data provided by users. Moreover, they often generate a long list of threats without clear guidance on how to prioritize them, leaving developers unsure of where to focus their efforts. In response to these challenges, we introduce PILLAR (Privacy risk Identification with LINDDUN and LLM Analysis Report), a new tool that integrates LLMs with the LINDDUN framework to streamline and enhance privacy threat modeling. PILLAR automates key parts of the LINDDUN process, such as generating DFDs, classifying threats, and prioritizing risks. By leveraging the capabilities of LLMs, PILLAR can take natural language descriptions of systems and transform them into comprehensive threat models with minimal input from users, reducing the workload on developers and privacy experts while improving the efficiency and accuracy of the process.</li>
</ul>

<h3>Title: Unlocking FedNL: Self-Contained Compute-Optimized Implementation</h3>
<ul>
<li><strong>Authors: </strong>Konstantin Burlachenko, Peter Richtárik</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.MS, cs.PF, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08760">https://arxiv.org/abs/2410.08760</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08760">https://arxiv.org/pdf/2410.08760</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08760]] Unlocking FedNL: Self-Contained Compute-Optimized Implementation(https://arxiv.org/abs/2410.08760)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) is an emerging paradigm that enables intelligent agents to collaboratively train Machine Learning (ML) models in a distributed manner, eliminating the need for sharing their local data. The recent work (arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL) algorithms, marking a significant step towards applying second-order methods to FL and large-scale optimization. However, the reference FedNL prototype exhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch a single experiment in a sever-grade workstation; (ii) The prototype only simulates multi-node setting; (iii) Prototype integration into resource-constrained applications is challenging. To bridge the gap between theory and practice, we present a self-contained implementation of FedNL, FedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves the aforementioned issues and reduces the wall clock time by x1000. With this FedNL outperforms alternatives for training logistic regression in a single-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark (arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose two practical-orientated compressors for FedNL - adaptive TopLEK and cache-aware RandSeqK, which fulfill the theory of FedNL.</li>
</ul>

<h3>Title: Cross-chain Sharing of Personal Health Records: Heterogeneous and Interoperable Blockchains</h3>
<ul>
<li><strong>Authors: </strong>Yongyang Lv, Xiaohong Li, Yingwenbo Wang, Kui Chen, Zhe Hou, Ruitao Feng</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08762">https://arxiv.org/abs/2410.08762</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08762">https://arxiv.org/pdf/2410.08762</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08762]] Cross-chain Sharing of Personal Health Records: Heterogeneous and Interoperable Blockchains(https://arxiv.org/abs/2410.08762)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>With the widespread adoption of medical informatics, a wealth of valuable personal health records (PHR) has been generated. Concurrently, blockchain technology has enhanced the security of medical institutions. However, these institutions often function as isolated data silos, limiting the potential value of PHRs. As the demand for data sharing between hospitals on different blockchains grows, addressing the challenge of cross-chain data sharing becomes crucial. When sharing PHRs across blockchains, the limited storage and computational capabilities of medical Internet of Things (IoT) devices complicate the storage of large volumes of PHRs and the handling of complex calculations. Additionally, varying blockchain cryptosystems and the risk of internal attacks further complicate the cross-chain sharing of PHRs. This paper proposes a scheme for sharing PHRs across heterogeneous and interoperable blockchains. Medical IoT devices can encrypt and store real-time PHRs in an InterPlanetary File System, requiring only simple operations for data sharing. An enhanced proxy re-encryption(PRE) algorithm addresses the differences in blockchain cryptosystems. Multi-dimensional analysis demonstrates that this scheme offers robust security and excellent performance.</li>
</ul>

<h3>Title: Measuring the Groundedness of Legal Question-Answering Systems</h3>
<ul>
<li><strong>Authors: </strong>Dietrich Trautmann, Natalia Ostapuk, Quentin Grail, Adrian Alan Pol, Guglielmo Bonifazi, Shang Gao, Martin Gajek</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08764">https://arxiv.org/abs/2410.08764</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08764">https://arxiv.org/pdf/2410.08764</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08764]] Measuring the Groundedness of Legal Question-Answering Systems(https://arxiv.org/abs/2410.08764)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>In high-stakes domains like legal question-answering, the accuracy and trustworthiness of generative AI systems are of paramount importance. This work presents a comprehensive benchmark of various methods to assess the groundedness of AI-generated responses, aiming to significantly enhance their reliability. Our experiments include similarity-based metrics and natural language inference models to evaluate whether responses are well-founded in the given contexts. We also explore different prompting strategies for large language models to improve the detection of ungrounded responses. We validated the effectiveness of these methods using a newly created grounding classification corpus, designed specifically for legal queries and corresponding responses from retrieval-augmented prompting, focusing on their alignment with source material. Our results indicate potential in groundedness classification of generated responses, with the best method achieving a macro-F1 score of 0.8. Additionally, we evaluated the methods in terms of their latency to determine their suitability for real-world applications, as this step typically follows the generation process. This capability is essential for processes that may trigger additional manual verification or automated response regeneration. In summary, this study demonstrates the potential of various detection methods to improve the trustworthiness of generative AI in legal settings.</li>
</ul>

<h3>Title: Efficient Multi-Object Tracking on Edge Devices via Reconstruction-Based Channel Pruning</h3>
<ul>
<li><strong>Authors: </strong>Jan Müller, Adrian Pigors</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08769">https://arxiv.org/abs/2410.08769</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08769">https://arxiv.org/pdf/2410.08769</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08769]] Efficient Multi-Object Tracking on Edge Devices via Reconstruction-Based Channel Pruning(https://arxiv.org/abs/2410.08769)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>The advancement of multi-object tracking (MOT) technologies presents the dual challenge of maintaining high performance while addressing critical security and privacy concerns. In applications such as pedestrian tracking, where sensitive personal data is involved, the potential for privacy violations and data misuse becomes a significant issue if data is transmitted to external servers. To mitigate these risks, processing data directly on an edge device, such as a smart camera, has emerged as a viable solution. Edge computing ensures that sensitive information remains local, thereby aligning with stringent privacy principles and significantly reducing network latency. However, the implementation of MOT on edge devices is not without its challenges. Edge devices typically possess limited computational resources, necessitating the development of highly optimized algorithms capable of delivering real-time performance under these constraints. The disparity between the computational requirements of state-of-the-art MOT algorithms and the capabilities of edge devices emphasizes a significant obstacle. To address these challenges, we propose a neural network pruning method specifically tailored to compress complex networks, such as those used in modern MOT systems. This approach optimizes MOT performance by ensuring high accuracy and efficiency within the constraints of limited edge devices, such as NVIDIA's Jetson Orin Nano. By applying our pruning method, we achieve model size reductions of up to 70% while maintaining a high level of accuracy and further improving performance on the Jetson Orin Nano, demonstrating the effectiveness of our approach for edge computing applications.</li>
</ul>

<h3>Title: F2A: An Innovative Approach for Prompt Injection by Utilizing Feign Security Detection Agents</h3>
<ul>
<li><strong>Authors: </strong>Yupeng Ren</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08776">https://arxiv.org/abs/2410.08776</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08776">https://arxiv.org/pdf/2410.08776</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08776]] F2A: An Innovative Approach for Prompt Injection by Utilizing Feign Security Detection Agents(https://arxiv.org/abs/2410.08776)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>With the rapid development of Large Language Models (LLMs), numerous mature applications of LLMs have emerged in the field of content safety detection. However, we have found that LLMs exhibit blind trust in safety detection agents. The general LLMs can be compromised by hackers with this vulnerability. Hence, this paper proposed an attack named Feign Agent Attack (F2A).Through such malicious forgery methods, adding fake safety detection results into the prompt, the defense mechanism of LLMs can be bypassed, thereby obtaining harmful content and hijacking the normal this http URL, a series of experiments were conducted. In these experiments, the hijacking capability of F2A on LLMs was analyzed and demonstrated, exploring the fundamental reasons why LLMs blindly trust safety detection results. The experiments involved various scenarios where fake safety detection results were injected into prompts, and the responses were closely monitored to understand the extent of the vulnerability. Also, this paper provided a reasonable solution to this attack, emphasizing that it is important for LLMs to critically evaluate the results of augmented agents to prevent the generating harmful content. By doing so, the reliability and security can be significantly improved, protecting the LLMs from F2A.</li>
</ul>

<h3>Title: VideoSAM: Open-World Video Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Pinxue Guo, Zixu Zhao, Jianxiong Gao, Chongruo Wu, Tong He, Zheng Zhang, Tianjun Xiao, Wenqiang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08781">https://arxiv.org/abs/2410.08781</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08781">https://arxiv.org/pdf/2410.08781</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08781]] VideoSAM: Open-World Video Segmentation(https://arxiv.org/abs/2410.08781)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Video segmentation is essential for advancing robotics and autonomous driving, particularly in open-world settings where continuous perception and object association across video frames are critical. While the Segment Anything Model (SAM) has excelled in static image segmentation, extending its capabilities to video segmentation poses significant challenges. We tackle two major hurdles: a) SAM's embedding limitations in associating objects across frames, and b) granularity inconsistencies in object segmentation. To this end, we introduce VideoSAM, an end-to-end framework designed to address these challenges by improving object tracking and segmentation consistency in dynamic environments. VideoSAM integrates an agglomerated backbone, RADIO, enabling object association through similarity metrics and introduces Cycle-ack-Pairs Propagation with a memory mechanism for stable object tracking. Additionally, we incorporate an autoregressive object-token mechanism within the SAM decoder to maintain consistent granularity across frames. Our method is extensively evaluated on the UVO and BURST benchmarks, and robotic videos from RoboTAP, demonstrating its effectiveness and robustness in real-world scenarios. All codes will be available.</li>
</ul>

<h3>Title: Superpipeline: A Universal Approach for Reducing GPU Memory Usage in Large Models</h3>
<ul>
<li><strong>Authors: </strong>Reza Abbasi, Sernam Lim</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08791">https://arxiv.org/abs/2410.08791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08791">https://arxiv.org/pdf/2410.08791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08791]] Superpipeline: A Universal Approach for Reducing GPU Memory Usage in Large Models(https://arxiv.org/abs/2410.08791)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid growth in machine learning models, especially in natural language processing and computer vision, has led to challenges when running these models on hardware with limited resources. This paper introduces Superpipeline, a new framework designed to optimize the execution of large AI models on constrained hardware during both training and inference. Our approach involves dynamically managing model execution by dividing models into individual layers and efficiently transferring these layers between GPU and CPU memory. Superpipeline reduces GPU memory usage by up to 60% in our experiments while maintaining model accuracy and acceptable processing speeds. This allows models that would otherwise exceed available GPU memory to run effectively. Unlike existing solutions that focus mainly on inference or specific model types, Superpipeline can be applied to large language models (LLMs), vision-language models (VLMs), and vision-based models. We tested Superpipeline's performance across various models and hardware setups. The method includes two key parameters that allow fine-tuning the balance between GPU memory use and processing speed. Importantly, Superpipeline does not require retraining or changing model parameters, ensuring that the original model's output remains unchanged. Superpipeline's simplicity and flexibility make it useful for researchers and professionals working with advanced AI models on limited hardware. It enables the use of larger models or bigger batch sizes on existing hardware, potentially speeding up innovation across many machine learning applications. This work marks an important step toward making advanced AI models more accessible and optimizing their deployment in resource-limited environments. The code for Superpipeline is available at this https URL.</li>
</ul>

<h3>Title: On the State of NLP Approaches to Modeling Depression in Social Media: A Post-COVID-19 Outlook</h3>
<ul>
<li><strong>Authors: </strong>Ana-Maria Bucur, Andreea-Codrina Moldovan, Krutika Parvatikar, Marcos Zampieri, Ashiqur R. KhudaBukhsh, Liviu P. Dinu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08793">https://arxiv.org/abs/2410.08793</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08793">https://arxiv.org/pdf/2410.08793</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08793]] On the State of NLP Approaches to Modeling Depression in Social Media: A Post-COVID-19 Outlook(https://arxiv.org/abs/2410.08793)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Computational approaches to predicting mental health conditions in social media have been substantially explored in the past years. Multiple surveys have been published on this topic, providing the community with comprehensive accounts of the research in this area. Among all mental health conditions, depression is the most widely studied due to its worldwide prevalence. The COVID-19 global pandemic, starting in early 2020, has had a great impact on mental health worldwide. Harsh measures employed by governments to slow the spread of the virus (e.g., lockdowns) and the subsequent economic downturn experienced in many countries have significantly impacted people's lives and mental health. Studies have shown a substantial increase of above 50% in the rate of depression in the population. In this context, we present a survey on natural language processing (NLP) approaches to modeling depression in social media, providing the reader with a post-COVID-19 outlook. This survey contributes to the understanding of the impacts of the pandemic on modeling depression in social media. We outline how state-of-the-art approaches and new datasets have been used in the context of the COVID-19 pandemic. Finally, we also discuss ethical issues in collecting and processing mental health data, considering fairness, accountability, and ethics.</li>
</ul>

<h3>Title: CoTCoNet: An Optimized Coupled Transformer-Convolutional Network with an Adaptive Graph Reconstruction for Leukemia Detection</h3>
<ul>
<li><strong>Authors: </strong>Chandravardhan Singh Raghaw, Arnav Sharma, Shubhi Bansa, Mohammad Zia Ur Rehman, Nagendra Kumar</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08797">https://arxiv.org/abs/2410.08797</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08797">https://arxiv.org/pdf/2410.08797</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08797]] CoTCoNet: An Optimized Coupled Transformer-Convolutional Network with an Adaptive Graph Reconstruction for Leukemia Detection(https://arxiv.org/abs/2410.08797)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, explainability, transformer</a></li>
<li><strong>Abstract: </strong>Swift and accurate blood smear analysis is an effective diagnostic method for leukemia and other hematological malignancies. However, manual leukocyte count and morphological evaluation using a microscope is time-consuming and prone to errors. Conventional image processing methods also exhibit limitations in differentiating cells due to the visual similarity between malignant and benign cell morphology. This limitation is further compounded by the skewed training data that hinders the extraction of reliable and pertinent features. In response to these challenges, we propose an optimized Coupled Transformer Convolutional Network (CoTCoNet) framework for the classification of leukemia, which employs a well-designed transformer integrated with a deep convolutional network to effectively capture comprehensive global features and scalable spatial patterns, enabling the identification of complex and large-scale hematological features. Further, the framework incorporates a graph-based feature reconstruction module to reveal the hidden or unobserved hard-to-see biological features of leukocyte cells and employs a Population-based Meta-Heuristic Algorithm for feature selection and optimization. To mitigate data imbalance issues, we employ a synthetic leukocyte generator. In the evaluation phase, we initially assess CoTCoNet on a dataset containing 16,982 annotated cells, and it achieves remarkable accuracy and F1-Score rates of 0.9894 and 0.9893, respectively. To broaden the generalizability of our model, we evaluate it across four publicly available diverse datasets, which include the aforementioned dataset. This evaluation demonstrates that our method outperforms current state-of-the-art approaches. We also incorporate an explainability approach in the form of feature visualization closely aligned with cell annotations to provide a deeper understanding of the framework.</li>
</ul>

<h3>Title: Data Processing for the OpenGPT-X Model Family</h3>
<ul>
<li><strong>Authors: </strong>Nicolo' Brandizzi, Hammam Abdelwahab, Anirban Bhowmick, Lennard Helmer, Benny Jörg Stein, Pavel Denisov, Qasid Saleem, Michael Fromm, Mehdi Ali, Richard Rutmann, Farzad Naderi, Mohamad Saif Agy, Alexander Schwirjow, Fabian Küch, Luzian Hahn, Malte Ostendorff, Pedro Ortiz Suarez, Georg Rehm, Dennis Wegener, Nicolas Flores-Herr, Joachim Köhler, Johannes Leveling</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08800">https://arxiv.org/abs/2410.08800</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08800">https://arxiv.org/pdf/2410.08800</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08800]] Data Processing for the OpenGPT-X Model Family(https://arxiv.org/abs/2410.08800)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper presents a comprehensive overview of the data preparation pipeline developed for the OpenGPT-X project, a large-scale initiative aimed at creating open and high-performance multilingual large language models (LLMs). The project goal is to deliver models that cover all major European languages, with a particular focus on real-world applications within the European Union. We explain all data processing steps, starting with the data selection and requirement definition to the preparation of the final datasets for model training. We distinguish between curated data and web data, as each of these categories is handled by distinct pipelines, with curated data undergoing minimal filtering and web data requiring extensive filtering and deduplication. This distinction guided the development of specialized algorithmic solutions for both pipelines. In addition to describing the processing methodologies, we provide an in-depth analysis of the datasets, increasing transparency and alignment with European data regulations. Finally, we share key insights and challenges faced during the project, offering recommendations for future endeavors in large-scale multilingual data preparation for LLMs.</li>
</ul>

<h3>Title: Don't Transform the Code, Code the Transforms: Towards Precise Code Rewriting using LLMs</h3>
<ul>
<li><strong>Authors: </strong>Chris Cummins, Volker Seeker, Jordi Armengol-Estapé, Aram H. Markosyan, Gabriel Synnaeve, Hugh Leather</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08806">https://arxiv.org/abs/2410.08806</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08806">https://arxiv.org/pdf/2410.08806</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08806]] Don't Transform the Code, Code the Transforms: Towards Precise Code Rewriting using LLMs(https://arxiv.org/abs/2410.08806)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Tools for rewriting, refactoring and optimizing code should be fast and correct. Large language models (LLMs), by their nature, possess neither of these qualities. Yet, there remains tremendous opportunity in using LLMs to improve code. We explore the use of LLMs not to transform code, but to code transforms. We propose a chain-of-thought approach to synthesizing code transformations from a small number of input/output code examples that incorporates execution and feedback. Unlike the direct rewrite approach, LLM-generated transformations are easy to inspect, debug, and validate. The logic of the rewrite is explicitly coded and easy to adapt. The compute required to run code transformations is minute compared to that of LLM rewriting. We test our approach on 16 Python code transformations and find that LLM- generated transforms are perfectly precise for 7 of them and less imprecise than direct LLM rewriting on the others. We hope to encourage further research to improving the precision of LLM code rewriting.</li>
</ul>

<h3>Title: PoisonBench: Assessing Large Language Model Vulnerability to Data Poisoning</h3>
<ul>
<li><strong>Authors: </strong>Tingchen Fu, Mrinank Sharma, Philip Torr, Shay B. Cohen, David Krueger, Fazl Barez</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08811">https://arxiv.org/abs/2410.08811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08811">https://arxiv.org/pdf/2410.08811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08811]] PoisonBench: Assessing Large Language Model Vulnerability to Data Poisoning(https://arxiv.org/abs/2410.08811)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Preference learning is a central component for aligning current LLMs, but this process can be vulnerable to data poisoning attacks. To address this concern, we introduce PoisonBench, a benchmark for evaluating large language models' susceptibility to data poisoning during preference learning. Data poisoning attacks can manipulate large language model responses to include hidden malicious content or biases, potentially causing the model to generate harmful or unintended outputs while appearing to function normally. We deploy two distinct attack types across eight realistic scenarios, assessing 21 widely-used models. Our findings reveal concerning trends: (1) Scaling up parameter size does not inherently enhance resilience against poisoning attacks; (2) There exists a log-linear relationship between the effects of the attack and the data poison ratio; (3) The effect of data poisoning can generalize to extrapolated triggers that are not included in the poisoned data. These results expose weaknesses in current preference learning techniques, highlighting the urgent need for more robust defenses against malicious models and data manipulation.</li>
</ul>

<h3>Title: StructRAG: Boosting Knowledge Intensive Reasoning of LLMs via Inference-time Hybrid Information Structurization</h3>
<ul>
<li><strong>Authors: </strong>Zhuoqun Li, Xuanang Chen, Haiyang Yu, Hongyu Lin, Yaojie Lu, Qiaoyu Tang, Fei Huang, Xianpei Han, Le Sun, Yongbin Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08815">https://arxiv.org/abs/2410.08815</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08815">https://arxiv.org/pdf/2410.08815</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08815]] StructRAG: Boosting Knowledge Intensive Reasoning of LLMs via Inference-time Hybrid Information Structurization(https://arxiv.org/abs/2410.08815)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) is a key means to effectively enhance large language models (LLMs) in many knowledge-based tasks. However, existing RAG methods struggle with knowledge-intensive reasoning tasks, because useful information required to these tasks are badly scattered. This characteristic makes it difficult for existing RAG methods to accurately identify key information and perform global reasoning with such noisy augmentation. In this paper, motivated by the cognitive theories that humans convert raw information into various structured knowledge when tackling knowledge-intensive reasoning, we proposes a new framework, StructRAG, which can identify the optimal structure type for the task at hand, reconstruct original documents into this structured format, and infer answers based on the resulting structure. Extensive experiments across various knowledge-intensive tasks show that StructRAG achieves state-of-the-art performance, particularly excelling in challenging scenarios, demonstrating its potential as an effective solution for enhancing LLMs in complex real-world applications.</li>
</ul>

<h3>Title: Uncertainty-Aware Optimal Treatment Selection for Clinical Time Series</h3>
<ul>
<li><strong>Authors: </strong>Thomas Schwarz, Cecilia Casolo, Niki Kilbertus</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08816">https://arxiv.org/abs/2410.08816</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08816">https://arxiv.org/pdf/2410.08816</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08816]] Uncertainty-Aware Optimal Treatment Selection for Clinical Time Series(https://arxiv.org/abs/2410.08816)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In personalized medicine, the ability to predict and optimize treatment outcomes across various time frames is essential. Additionally, the ability to select cost-effective treatments within specific budget constraints is critical. Despite recent advancements in estimating counterfactual trajectories, a direct link to optimal treatment selection based on these estimates is missing. This paper introduces a novel method integrating counterfactual estimation techniques and uncertainty quantification to recommend personalized treatment plans adhering to predefined cost constraints. Our approach is distinctive in its handling of continuous treatment variables and its incorporation of uncertainty quantification to improve prediction reliability. We validate our method using two simulated datasets, one focused on the cardiovascular system and the other on COVID-19. Our findings indicate that our method has robust performance across different counterfactual estimation baselines, showing that introducing uncertainty quantification in these settings helps the current baselines in finding more reliable and accurate treatment selection. The robustness of our method across various settings highlights its potential for broad applicability in personalized healthcare solutions.</li>
</ul>

<h3>Title: Which Demographics do LLMs Default to During Annotation?</h3>
<ul>
<li><strong>Authors: </strong>Christopher Bagdon, Aidan Combs, Lynn Greschner, Roman Klinger, Jiahui Li, Sean Papay, Nadine Probol, Yarik Menchaca Resendiz, Johannes Schäfer, Aswathy Velutharambath, Sabine Weber, Amelie Wührl</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08820">https://arxiv.org/abs/2410.08820</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08820">https://arxiv.org/pdf/2410.08820</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08820]] Which Demographics do LLMs Default to During Annotation?(https://arxiv.org/abs/2410.08820)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Demographics and cultural background of annotators influence the labels they assign in text annotation -- for instance, an elderly woman might find it offensive to read a message addressed to a "bro", but a male teenager might find it appropriate. It is therefore important to acknowledge label variations to not under-represent members of a society. Two research directions developed out of this observation in the context of using large language models (LLM) for data annotations, namely (1) studying biases and inherent knowledge of LLMs and (2) injecting diversity in the output by manipulating the prompt with demographic information. We combine these two strands of research and ask the question to which demographics an LLM resorts to when no demographics is given. To answer this question, we evaluate which attributes of human annotators LLMs inherently mimic. Furthermore, we compare non-demographic conditioned prompts and placebo-conditioned prompts (e.g., "you are an annotator who lives in house number 5") to demographics-conditioned prompts ("You are a 45 year old man and an expert on politeness annotation. How do you rate {instance}"). We study these questions for politeness and offensiveness annotations on the POPQUORN data set, a corpus created in a controlled manner to investigate human label variations based on demographics which has not been used for LLM-based analyses so far. We observe notable influences related to gender, race, and age in demographic prompting, which contrasts with previous studies that found no such effects.</li>
</ul>

<h3>Title: Retriever-and-Memory: Towards Adaptive Note-Enhanced Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Ruobing Wang, Daren Zha, Shi Yu, Qingfei Zhao, Yuxuan Chen, Yixuan Wang, Shuo Wang, Yukun Yan, Zhenghao Liu, Xu Han, Zhiyuan Liu, Maosong Sun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08821">https://arxiv.org/abs/2410.08821</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08821">https://arxiv.org/pdf/2410.08821</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08821]] Retriever-and-Memory: Towards Adaptive Note-Enhanced Retrieval-Augmented Generation(https://arxiv.org/abs/2410.08821)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) mitigates issues of the factual errors and hallucinated outputs generated by Large Language Models (LLMs) in open-domain question-answering tasks (OpenQA) via introducing external knowledge. For complex QA, however, existing RAG methods use LLMs to actively predict retrieval timing and directly use the retrieved information for generation, regardless of whether the retrieval timing accurately reflects the actual information needs, or sufficiently considers prior retrieved knowledge, which may result in insufficient information gathering and interaction, yielding low-quality answers. To address these, we propose a generic RAG approach called Adaptive Note-Enhanced RAG (Adaptive-Note) for complex QA tasks, which includes the iterative information collector, adaptive memory reviewer, and task-oriented generator, while following a new Retriever-and-Memory paradigm. Specifically, Adaptive-Note introduces an overarching view of knowledge growth, iteratively gathering new information in the form of notes and updating them into the existing optimal knowledge structure, enhancing high-quality knowledge interactions. In addition, we employ an adaptive, note-based stop-exploration strategy to decide "what to retrieve and when to stop" to encourage sufficient knowledge exploration. We conduct extensive experiments on five complex QA datasets, and the results demonstrate the superiority and effectiveness of our method and its components. The code and data are at this https URL.</li>
</ul>

<h3>Title: SOLD: Reinforcement Learning with Slot Object-Centric Latent Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Malte Mosbach, Jan Niklas Ewertz, Angel Villar-Corrales, Sven Behnke</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08822">https://arxiv.org/abs/2410.08822</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08822">https://arxiv.org/pdf/2410.08822</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08822]] SOLD: Reinforcement Learning with Slot Object-Centric Latent Dynamics(https://arxiv.org/abs/2410.08822)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Learning a latent dynamics model provides a task-agnostic representation of an agent's understanding of its environment. Leveraging this knowledge for model-based reinforcement learning holds the potential to improve sample efficiency over model-free methods by learning inside imagined rollouts. Furthermore, because the latent space serves as input to behavior models, the informative representations learned by the world model facilitate efficient learning of desired skills. Most existing methods rely on holistic representations of the environment's state. In contrast, humans reason about objects and their interactions, forecasting how actions will affect specific parts of their surroundings. Inspired by this, we propose Slot-Attention for Object-centric Latent Dynamics (SOLD), a novel algorithm that learns object-centric dynamics models in an unsupervised manner from pixel inputs. We demonstrate that the structured latent space not only improves model interpretability but also provides a valuable input space for behavior models to reason over. Our results show that SOLD outperforms DreamerV3, a state-of-the-art model-based RL algorithm, across a range of benchmark robotic environments that evaluate for both relational reasoning and low-level manipulation capabilities. Videos are available at this https URL.</li>
</ul>

<h3>Title: One-shot Generative Domain Adaptation in 3D GANs</h3>
<ul>
<li><strong>Authors: </strong>Ziqiang Li, Yi Wu, Chaoyue Wang, Xue Rui, Bin Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08824">https://arxiv.org/abs/2410.08824</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08824">https://arxiv.org/pdf/2410.08824</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08824]] One-shot Generative Domain Adaptation in 3D GANs(https://arxiv.org/abs/2410.08824)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>3D-aware image generation necessitates extensive training data to ensure stable training and mitigate the risk of overfitting. This paper first considers a novel task known as One-shot 3D Generative Domain Adaptation (GDA), aimed at transferring a pre-trained 3D generator from one domain to a new one, relying solely on a single reference image. One-shot 3D GDA is characterized by the pursuit of specific attributes, namely, high fidelity, large diversity, cross-domain consistency, and multi-view consistency. Within this paper, we introduce 3D-Adapter, the first one-shot 3D GDA method, for diverse and faithful generation. Our approach begins by judiciously selecting a restricted weight set for fine-tuning, and subsequently leverages four advanced loss functions to facilitate adaptation. An efficient progressive fine-tuning strategy is also implemented to enhance the adaptation process. The synergy of these three technological components empowers 3D-Adapter to achieve remarkable performance, substantiated both quantitatively and qualitatively, across all desired properties of 3D GDA. Furthermore, 3D-Adapter seamlessly extends its capabilities to zero-shot scenarios, and preserves the potential for crucial tasks such as interpolation, reconstruction, and editing within the latent space of the pre-trained generator. Code will be available at this https URL.</li>
</ul>

<h3>Title: Towards virtual painting recolouring using Vision Transformer on X-Ray Fluorescence datacubes</h3>
<ul>
<li><strong>Authors: </strong>Alessandro Bombini, Fernando García-Avello Bofías, Francesca Giambi, Chiara Ruberto</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, physics.app-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08826">https://arxiv.org/abs/2410.08826</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08826">https://arxiv.org/pdf/2410.08826</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08826]] Towards virtual painting recolouring using Vision Transformer on X-Ray Fluorescence datacubes(https://arxiv.org/abs/2410.08826)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In this contribution, we define (and test) a pipeline to perform virtual painting recolouring using raw data of X-Ray Fluorescence (XRF) analysis on pictorial artworks. To circumvent the small dataset size, we generate a synthetic dataset, starting from a database of XRF spectra; furthermore, to ensure a better generalisation capacity (and to tackle the issue of in-memory size and inference time), we define a Deep Variational Embedding network to embed the XRF spectra into a lower dimensional, K-Means friendly, metric space. We thus train a set of models to assign coloured images to embedded XRF images. We report here the devised pipeline performances in terms of visual quality metrics, and we close on a discussion on the results.</li>
</ul>

<h3>Title: Do Unlearning Methods Remove Information from Language Model Weights?</h3>
<ul>
<li><strong>Authors: </strong>Aghyad Deeb, Fabien Roger</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08827">https://arxiv.org/abs/2410.08827</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08827">https://arxiv.org/pdf/2410.08827</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08827]] Do Unlearning Methods Remove Information from Language Model Weights?(https://arxiv.org/abs/2410.08827)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models' knowledge of how to perform cyber-security attacks, create bioweapons, and manipulate humans poses risks of misuse. Previous work has proposed methods to unlearn this knowledge. Historically, it has been unclear whether unlearning techniques are removing information from the model weights or just making it harder to access. To disentangle these two objectives, we propose an adversarial evaluation method to test for the removal of information from model weights: we give an attacker access to some facts that were supposed to be removed, and using those, the attacker tries to recover other facts from the same distribution that cannot be guessed from the accessible facts. We show that using fine-tuning on the accessible facts can recover 88% of the pre-unlearning accuracy when applied to current unlearning methods, revealing the limitations of these methods in removing information from the model weights.</li>
</ul>

<h3>Title: Unveiling Molecular Secrets: An LLM-Augmented Linear Model for Explainable and Calibratable Molecular Property Prediction</h3>
<ul>
<li><strong>Authors: </strong>Zhuoran Li, Xu Sun, Wanyu Lin, Jiannong Cao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08829">https://arxiv.org/abs/2410.08829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08829">https://arxiv.org/pdf/2410.08829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08829]] Unveiling Molecular Secrets: An LLM-Augmented Linear Model for Explainable and Calibratable Molecular Property Prediction(https://arxiv.org/abs/2410.08829)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, large language model</a></li>
<li><strong>Abstract: </strong>Explainable molecular property prediction is essential for various scientific fields, such as drug discovery and material science. Despite delivering intrinsic explainability, linear models struggle with capturing complex, non-linear patterns. Large language models (LLMs), on the other hand, yield accurate predictions through powerful inference capabilities yet fail to provide chemically meaningful explanations for their predictions. This work proposes a novel framework, called MoleX, which leverages LLM knowledge to build a simple yet powerful linear model for accurate molecular property prediction with faithful explanations. The core of MoleX is to model complicated molecular structure-property relationships using a simple linear model, augmented by LLM knowledge and a crafted calibration strategy. Specifically, to extract the maximum amount of task-relevant knowledge from LLM embeddings, we employ information bottleneck-inspired fine-tuning and sparsity-inducing dimensionality reduction. These informative embeddings are then used to fit a linear model for explainable inference. Moreover, we introduce residual calibration to address prediction errors stemming from linear models' insufficient expressiveness of complex LLM embeddings, thus recovering the LLM's predictive power and boosting overall accuracy. Theoretically, we provide a mathematical foundation to justify MoleX's explainability. Extensive experiments demonstrate that MoleX outperforms existing methods in molecular property prediction, establishing a new milestone in predictive performance, explainability, and efficiency. In particular, MoleX enables CPU inference and accelerates large-scale dataset processing, achieving comparable performance 300x faster with 100,000 fewer parameters than LLMs. Additionally, the calibration improves model performance by up to 12.7% without compromising explainability.</li>
</ul>

<h3>Title: Measuring the Inconsistency of Large Language Models in Preferential Ranking</h3>
<ul>
<li><strong>Authors: </strong>Xiutian Zhao, Ke Wang, Wei Peng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08851">https://arxiv.org/abs/2410.08851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08851">https://arxiv.org/pdf/2410.08851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08851]] Measuring the Inconsistency of Large Language Models in Preferential Ranking(https://arxiv.org/abs/2410.08851)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite large language models' (LLMs) recent advancements, their bias and hallucination issues persist, and their ability to offer consistent preferential rankings remains underexplored. This study investigates the capacity of LLMs to provide consistent ordinal preferences, a crucial aspect in scenarios with dense decision space or lacking absolute answers. We introduce a formalization of consistency based on order theory, outlining criteria such as transitivity, asymmetry, reversibility, and independence from irrelevant alternatives. Our diagnostic experiments on selected state-of-the-art LLMs reveal their inability to meet these criteria, indicating a strong positional bias and poor transitivity, with preferences easily swayed by irrelevant alternatives. These findings highlight a significant inconsistency in LLM-generated preferential rankings, underscoring the need for further research to address these limitations.</li>
</ul>

<h3>Title: Hybrid LLM-DDQN based Joint Optimization of V2I Communication and Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Zijiang Yan, Hao Zhou, Hina Tabassum, Xue Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NI, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08854">https://arxiv.org/abs/2410.08854</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08854">https://arxiv.org/pdf/2410.08854</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08854]] Hybrid LLM-DDQN based Joint Optimization of V2I Communication and Autonomous Driving(https://arxiv.org/abs/2410.08854)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have received considerable interest recently due to their outstanding reasoning and comprehension capabilities. This work explores applying LLMs to vehicular networks, aiming to jointly optimize vehicle-to-infrastructure (V2I) communications and autonomous driving (AD) policies. We deploy LLMs for AD decision-making to maximize traffic flow and avoid collisions for road safety, and a double deep Q-learning algorithm (DDQN) is used for V2I optimization to maximize the received data rate and reduce frequent handovers. In particular, for LLM-enabled AD, we employ the Euclidean distance to identify previously explored AD experiences, and then LLMs can learn from past good and bad decisions for further improvement. Then, LLM-based AD decisions will become part of states in V2I problems, and DDQN will optimize the V2I decisions accordingly. After that, the AD and V2I decisions are iteratively optimized until convergence. Such an iterative optimization approach can better explore the interactions between LLMs and conventional reinforcement learning techniques, revealing the potential of using LLMs for network optimization and management. Finally, the simulations demonstrate that our proposed hybrid LLM-DDQN approach outperforms the conventional DDQN algorithm, showing faster convergence and higher average rewards.</li>
</ul>

<h3>Title: Decoding Secret Memorization in Code LLMs Through Token-Level Characterization</h3>
<ul>
<li><strong>Authors: </strong>Yuqing Nie, Chong Wang, Kailong Wang, Guoai Xu, Guosheng Xu, Haoyu Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08858">https://arxiv.org/abs/2410.08858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08858">https://arxiv.org/pdf/2410.08858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08858]] Decoding Secret Memorization in Code LLMs Through Token-Level Characterization(https://arxiv.org/abs/2410.08858)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Code Large Language Models (LLMs) have demonstrated remarkable capabilities in generating, understanding, and manipulating programming code. However, their training process inadvertently leads to the memorization of sensitive information, posing severe privacy risks. Existing studies on memorization in LLMs primarily rely on prompt engineering techniques, which suffer from limitations such as widespread hallucination and inefficient extraction of the target sensitive information. In this paper, we present a novel approach to characterize real and fake secrets generated by Code LLMs based on token probabilities. We identify four key characteristics that differentiate genuine secrets from hallucinated ones, providing insights into distinguishing real and fake secrets. To overcome the limitations of existing works, we propose DESEC, a two-stage method that leverages token-level features derived from the identified characteristics to guide the token decoding process. DESEC consists of constructing an offline token scoring model using a proxy Code LLM and employing the scoring model to guide the decoding process by reassigning token likelihoods. Through extensive experiments on four state-of-the-art Code LLMs using a diverse dataset, we demonstrate the superior performance of DESEC in achieving a higher plausible rate and extracting more real secrets compared to existing baselines. Our findings highlight the effectiveness of our token-level approach in enabling an extensive assessment of the privacy leakage risks associated with Code LLMs.</li>
</ul>

<h3>Title: Audio Description Generation in the Era of LLMs and VLMs: A Review of Transferable Generative AI Technologies</h3>
<ul>
<li><strong>Authors: </strong>Yingqiang Gao, Lukas Fischer, Alexa Lintner, Sarah Ebling</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08860">https://arxiv.org/abs/2410.08860</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08860">https://arxiv.org/pdf/2410.08860</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08860]] Audio Description Generation in the Era of LLMs and VLMs: A Review of Transferable Generative AI Technologies(https://arxiv.org/abs/2410.08860)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Audio descriptions (ADs) function as acoustic commentaries designed to assist blind persons and persons with visual impairments in accessing digital media content on television and in movies, among other settings. As an accessibility service typically provided by trained AD professionals, the generation of ADs demands significant human effort, making the process both time-consuming and costly. Recent advancements in natural language processing (NLP) and computer vision (CV), particularly in large language models (LLMs) and vision-language models (VLMs), have allowed for getting a step closer to automatic AD generation. This paper reviews the technologies pertinent to AD generation in the era of LLMs and VLMs: we discuss how state-of-the-art NLP and CV technologies can be applied to generate ADs and identify essential research directions for the future.</li>
</ul>

<h3>Title: The Good, the Bad and the Ugly: Watermarks, Transferable Attacks and Adversarial Defenses</h3>
<ul>
<li><strong>Authors: </strong>Grzegorz Głuch, Berkant Turan, Sai Ganesh Nagarajan, Sebastian Pokutta</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08864">https://arxiv.org/abs/2410.08864</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08864">https://arxiv.org/pdf/2410.08864</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08864]] The Good, the Bad and the Ugly: Watermarks, Transferable Attacks and Adversarial Defenses(https://arxiv.org/abs/2410.08864)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, watermark</a></li>
<li><strong>Abstract: </strong>We formalize and extend existing definitions of backdoor-based watermarks and adversarial defenses as interactive protocols between two players. The existence of these schemes is inherently tied to the learning tasks for which they are designed. Our main result shows that for almost every discriminative learning task, at least one of the two -- a watermark or an adversarial defense -- exists. The term "almost every" indicates that we also identify a third, counterintuitive but necessary option, i.e., a scheme we call a transferable attack. By transferable attack, we refer to an efficient algorithm computing queries that look indistinguishable from the data distribution and fool all efficient defenders. To this end, we prove the necessity of a transferable attack via a construction that uses a cryptographic tool called homomorphic encryption. Furthermore, we show that any task that satisfies our notion of a transferable attack implies a cryptographic primitive, thus requiring the underlying task to be computationally complex. These two facts imply an "equivalence" between the existence of transferable attacks and cryptography. Finally, we show that the class of tasks of bounded VC-dimension has an adversarial defense, and a subclass of them has a watermark.</li>
</ul>

<h3>Title: Evolution of SAE Features Across Layers in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Daniel Balcells, Benjamin Lerner, Michael Oesterle, Ediz Ucar, Stefan Heimersheim</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08869">https://arxiv.org/abs/2410.08869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08869">https://arxiv.org/pdf/2410.08869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08869]] Evolution of SAE Features Across Layers in LLMs(https://arxiv.org/abs/2410.08869)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Sparse Autoencoders for transformer-based language models are typically defined independently per layer. In this work we analyze statistical relationships between features in adjacent layers to understand how features evolve through a forward pass. We provide a graph visualization interface for features and their most similar next-layer neighbors, and build communities of related features across layers. We find that a considerable amount of features are passed through from a previous layer, some features can be expressed as quasi-boolean combinations of previous features, and some features become more specialized in later layers.</li>
</ul>

<h3>Title: Fragile Giants: Understanding the Susceptibility of Models to Subpopulation Attacks</h3>
<ul>
<li><strong>Authors: </strong>Isha Gupta, Hidde Lycklama, Emanuel Opel, Evan Rose, Anwar Hithnawi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08872">https://arxiv.org/abs/2410.08872</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08872">https://arxiv.org/pdf/2410.08872</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08872]] Fragile Giants: Understanding the Susceptibility of Models to Subpopulation Attacks(https://arxiv.org/abs/2410.08872)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, steal</a></li>
<li><strong>Abstract: </strong>As machine learning models become increasingly complex, concerns about their robustness and trustworthiness have become more pressing. A critical vulnerability of these models is data poisoning attacks, where adversaries deliberately alter training data to degrade model performance. One particularly stealthy form of these attacks is subpopulation poisoning, which targets distinct subgroups within a dataset while leaving overall performance largely intact. The ability of these attacks to generalize within subpopulations poses a significant risk in real-world settings, as they can be exploited to harm marginalized or underrepresented groups within the dataset. In this work, we investigate how model complexity influences susceptibility to subpopulation poisoning attacks. We introduce a theoretical framework that explains how overparameterized models, due to their large capacity, can inadvertently memorize and misclassify targeted subpopulations. To validate our theory, we conduct extensive experiments on large-scale image and text datasets using popular model architectures. Our results show a clear trend: models with more parameters are significantly more vulnerable to subpopulation poisoning. Moreover, we find that attacks on smaller, human-interpretable subgroups often go undetected by these models. These results highlight the need to develop defenses that specifically address subpopulation vulnerabilities.</li>
</ul>

<h3>Title: RoRA-VLM: Robust Retrieval-Augmented Vision Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jingyuan Qi, Zhiyang Xu, Rulin Shao, Yang Chen, Jing Di, Yu Cheng, Qifan Wang, Lifu Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08876">https://arxiv.org/abs/2410.08876</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08876">https://arxiv.org/pdf/2410.08876</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08876]] RoRA-VLM: Robust Retrieval-Augmented Vision Language Models(https://arxiv.org/abs/2410.08876)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Current vision-language models (VLMs) still exhibit inferior performance on knowledge-intensive tasks, primarily due to the challenge of accurately encoding all the associations between visual objects and scenes to their corresponding entities and background knowledge. While retrieval augmentation methods offer an efficient way to integrate external knowledge, extending them to vision-language domain presents unique challenges in (1) precisely retrieving relevant information from external sources due to the inherent discrepancy within the multimodal queries, and (2) being resilient to the irrelevant, extraneous and noisy information contained in the retrieved multimodal knowledge snippets. In this work, we introduce RORA-VLM, a novel and robust retrieval augmentation framework specifically tailored for VLMs, with two key innovations: (1) a 2-stage retrieval process with image-anchored textual-query expansion to synergistically combine the visual and textual information in the query and retrieve the most relevant multimodal knowledge snippets; and (2) a robust retrieval augmentation method that strengthens the resilience of VLMs against irrelevant information in the retrieved multimodal knowledge by injecting adversarial noises into the retrieval-augmented training process, and filters out extraneous visual information, such as unrelated entities presented in images, via a query-oriented visual token refinement strategy. We conduct extensive experiments to validate the effectiveness and robustness of our proposed methods on three widely adopted benchmark datasets. Our results demonstrate that with a minimal amount of training instance, RORA-VLM enables the base model to achieve significant performance improvement and constantly outperform state-of-the-art retrieval-augmented VLMs on all benchmarks while also exhibiting a novel zero-shot domain transfer capability.</li>
</ul>

<h3>Title: Multi-modal Fusion based Q-distribution Prediction for Controlled Nuclear Fusion</h3>
<ul>
<li><strong>Authors: </strong>Shiao Wang, Yifeng Wang, Qingchuan Ma, Xiao Wang, Ning Yan, Qingquan Yang, Guosheng Xu, Jin Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08879">https://arxiv.org/abs/2410.08879</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08879">https://arxiv.org/pdf/2410.08879</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08879]] Multi-modal Fusion based Q-distribution Prediction for Controlled Nuclear Fusion(https://arxiv.org/abs/2410.08879)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Q-distribution prediction is a crucial research direction in controlled nuclear fusion, with deep learning emerging as a key approach to solving prediction challenges. In this paper, we leverage deep learning techniques to tackle the complexities of Q-distribution prediction. Specifically, we explore multimodal fusion methods in computer vision, integrating 2D line image data with the original 1D data to form a bimodal input. Additionally, we employ the Transformer's attention mechanism for feature extraction and the interactive fusion of bimodal information. Extensive experiments validate the effectiveness of our approach, significantly reducing prediction errors in Q-distribution.</li>
</ul>

<h3>Title: Federated Learning in Practice: Reflections and Projections</h3>
<ul>
<li><strong>Authors: </strong>Katharine Daly, Hubert Eichner, Peter Kairouz, H. Brendan McMahan, Daniel Ramage, Zheng Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08892">https://arxiv.org/abs/2410.08892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08892">https://arxiv.org/pdf/2410.08892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08892]] Federated Learning in Practice: Reflections and Projections(https://arxiv.org/abs/2410.08892)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) is a machine learning technique that enables multiple entities to collaboratively learn a shared model without exchanging their local data. Over the past decade, FL systems have achieved substantial progress, scaling to millions of devices across various learning domains while offering meaningful differential privacy (DP) guarantees. Production systems from organizations like Google, Apple, and Meta demonstrate the real-world applicability of FL. However, key challenges remain, including verifying server-side DP guarantees and coordinating training across heterogeneous devices, limiting broader adoption. Additionally, emerging trends such as large (multi-modal) models and blurred lines between training, inference, and personalization challenge traditional FL frameworks. In response, we propose a redefined FL framework that prioritizes privacy principles rather than rigid definitions. We also chart a path forward by leveraging trusted execution environments and open-source ecosystems to address these challenges and facilitate future advancements in FL.</li>
</ul>

<h3>Title: Drama: Mamba-Enabled Model-Based Reinforcement Learning Is Sample and Parameter Efficient</h3>
<ul>
<li><strong>Authors: </strong>Wenlong Wang, Ivana Dusparic, Yucheng Shi, Ke Zhang, Vinny Cahill</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08893">https://arxiv.org/abs/2410.08893</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08893">https://arxiv.org/pdf/2410.08893</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08893]] Drama: Mamba-Enabled Model-Based Reinforcement Learning Is Sample and Parameter Efficient(https://arxiv.org/abs/2410.08893)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Model-based reinforcement learning (RL) offers a solution to the data inefficiency that plagues most model-free RL algorithms. However, learning a robust world model often demands complex and deep architectures, which are expensive to compute and train. Within the world model, dynamics models are particularly crucial for accurate predictions, and various dynamics-model architectures have been explored, each with its own set of challenges. Currently, recurrent neural network (RNN) based world models face issues such as vanishing gradients and difficulty in capturing long-term dependencies effectively. In contrast, use of transformers suffers from the well-known issues of self-attention mechanisms, where both memory and computational complexity scale as $O(n^2)$, with $n$ representing the sequence length. To address these challenges we propose a state space model (SSM) based world model, specifically based on Mamba, that achieves $O(n)$ memory and computational complexity while effectively capturing long-term dependencies and facilitating the use of longer training sequences efficiently. We also introduce a novel sampling method to mitigate the suboptimality caused by an incorrect world model in the early stages of training, combining it with the aforementioned technique to achieve a normalised score comparable to other state-of-the-art model-based RL algorithms using only a 7 million trainable parameter world model. This model is accessible and can be trained on an off-the-shelf laptop. Our code is available at this https URL.</li>
</ul>

<h3>Title: A Benchmark for Cross-Domain Argumentative Stance Classification on Social Media</h3>
<ul>
<li><strong>Authors: </strong>Jiaqing Yuan, Ruijie Xi, Munindar P. Singh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08900">https://arxiv.org/abs/2410.08900</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08900">https://arxiv.org/pdf/2410.08900</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08900]] A Benchmark for Cross-Domain Argumentative Stance Classification on Social Media(https://arxiv.org/abs/2410.08900)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Argumentative stance classification plays a key role in identifying authors' viewpoints on specific topics. However, generating diverse pairs of argumentative sentences across various domains is challenging. Existing benchmarks often come from a single domain or focus on a limited set of topics. Additionally, manual annotation for accurate labeling is time-consuming and labor-intensive. To address these challenges, we propose leveraging platform rules, readily available expert-curated content, and large language models to bypass the need for human annotation. Our approach produces a multidomain benchmark comprising 4,498 topical claims and 30,961 arguments from three sources, spanning 21 domains. We benchmark the dataset in fully supervised, zero-shot, and few-shot settings, shedding light on the strengths and limitations of different methodologies. We release the dataset and code in this study at hidden for anonymity.</li>
</ul>

<h3>Title: Efficient Hyperparameter Importance Assessment for CNNs</h3>
<ul>
<li><strong>Authors: </strong>Ruinan Wang, Ian Nabney, Mohammad Golbabaee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08920">https://arxiv.org/abs/2410.08920</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08920">https://arxiv.org/pdf/2410.08920</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08920]] Efficient Hyperparameter Importance Assessment for CNNs(https://arxiv.org/abs/2410.08920)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Hyperparameter selection is an essential aspect of the machine learning pipeline, profoundly impacting models' robustness, stability, and generalization capabilities. Given the complex hyperparameter spaces associated with Neural Networks and the constraints of computational resources and time, optimizing all hyperparameters becomes impractical. In this context, leveraging hyperparameter importance assessment (HIA) can provide valuable guidance by narrowing down the search space. This enables machine learning practitioners to focus their optimization efforts on the hyperparameters with the most significant impact on model performance while conserving time and resources. This paper aims to quantify the importance weights of some hyperparameters in Convolutional Neural Networks (CNNs) with an algorithm called N-RReliefF, laying the groundwork for applying HIA methodologies in the Deep Learning field. We conduct an extensive study by training over ten thousand CNN models across ten popular image classification datasets, thereby acquiring a comprehensive dataset containing hyperparameter configuration instances and their corresponding performance metrics. It is demonstrated that among the investigated hyperparameters, the top five important hyperparameters of the CNN model are the number of convolutional layers, learning rate, dropout rate, optimizer and epoch.</li>
</ul>

<h3>Title: DiffPO: A causal diffusion model for learning distributions of potential outcomes</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Ma, Valentyn Melnychuk, Jonas Schweisthal, Stefan Feuerriegel</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08924">https://arxiv.org/abs/2410.08924</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08924">https://arxiv.org/pdf/2410.08924</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08924]] DiffPO: A causal diffusion model for learning distributions of potential outcomes(https://arxiv.org/abs/2410.08924)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Predicting potential outcomes of interventions from observational data is crucial for decision-making in medicine, but the task is challenging due to the fundamental problem of causal inference. Existing methods are largely limited to point estimates of potential outcomes with no uncertain quantification; thus, the full information about the distributions of potential outcomes is typically ignored. In this paper, we propose a novel causal diffusion model called DiffPO, which is carefully designed for reliable inferences in medicine by learning the distribution of potential outcomes. In our DiffPO, we leverage a tailored conditional denoising diffusion model to learn complex distributions, where we address the selection bias through a novel orthogonal diffusion loss. Another strength of our DiffPO method is that it is highly flexible (e.g., it can also be used to estimate different causal quantities such as CATE). Across a wide range of experiments, we show that our method achieves state-of-the-art performance.</li>
</ul>

<h3>Title: HyperPg -- Prototypical Gaussians on the Hypersphere for Interpretable Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Maximilian Xiling Li, Korbinian Franz Rudolf, Nils Blank, Rudolf Lioutikov</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08925">https://arxiv.org/abs/2410.08925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08925">https://arxiv.org/pdf/2410.08925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08925]] HyperPg -- Prototypical Gaussians on the Hypersphere for Interpretable Deep Learning(https://arxiv.org/abs/2410.08925)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, interpretability</a></li>
<li><strong>Abstract: </strong>Prototype Learning methods provide an interpretable alternative to black-box deep learning models. Approaches such as ProtoPNet learn, which part of a test image "look like" known prototypical parts from training images, combining predictive power with the inherent interpretability of case-based reasoning. However, existing approaches have two main drawbacks: A) They rely solely on deterministic similarity scores without statistical confidence. B) The prototypes are learned in a black-box manner without human input. This work introduces HyperPg, a new prototype representation leveraging Gaussian distributions on a hypersphere in latent space, with learnable mean and variance. HyperPg prototypes adapt to the spread of clusters in the latent space and output likelihood scores. The new architecture, HyperPgNet, leverages HyperPg to learn prototypes aligned with human concepts from pixel-level annotations. Consequently, each prototype represents a specific concept such as color, image texture, or part of the image subject. A concept extraction pipeline built on foundation models provides pixel-level annotations, significantly reducing human labeling effort. Experiments on CUB-200-2011 and Stanford Cars datasets demonstrate that HyperPgNet outperforms other prototype learning architectures while using fewer parameters and training steps. Additionally, the concept-aligned HyperPg prototypes are learned transparently, enhancing model interpretability.</li>
</ul>

<h3>Title: Zero-Shot Pupil Segmentation with SAM 2: A Case Study of Over 14 Million Images</h3>
<ul>
<li><strong>Authors: </strong>Virmarie Maquiling, Sean Anthony Byrne, Diederick C. Niehorster, Marco Carminati, Enkelejda Kasneci</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08926">https://arxiv.org/abs/2410.08926</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08926">https://arxiv.org/pdf/2410.08926</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08926]] Zero-Shot Pupil Segmentation with SAM 2: A Case Study of Over 14 Million Images(https://arxiv.org/abs/2410.08926)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We explore the transformative potential of SAM 2, a vision foundation model, in advancing gaze estimation and eye tracking technologies. By significantly reducing annotation time, lowering technical barriers through its ease of deployment, and enhancing segmentation accuracy, SAM 2 addresses critical challenges faced by researchers and practitioners. Utilizing its zero-shot segmentation capabilities with minimal user input-a single click per video-we tested SAM 2 on over 14 million eye images from diverse datasets, including virtual reality setups and the world's largest unified dataset recorded using wearable eye trackers. Remarkably, in pupil segmentation tasks, SAM 2 matches the performance of domain-specific models trained solely on eye images, achieving competitive mean Intersection over Union (mIoU) scores of up to 93% without fine-tuning. Additionally, we provide our code and segmentation masks for these widely used datasets to promote further research.</li>
</ul>

<h3>Title: Towards Cross-Lingual LLM Evaluation for European Languages</h3>
<ul>
<li><strong>Authors: </strong>Klaudia Thellmann, Bernhard Stadler, Michael Fromm, Jasper Schulze Buschhoff, Alex Jude, Fabio Barth, Johannes Leveling, Nicolas Flores-Herr, Joachim Köhler, René Jäkel, Mehdi Ali</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08928">https://arxiv.org/abs/2410.08928</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08928">https://arxiv.org/pdf/2410.08928</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08928]] Towards Cross-Lingual LLM Evaluation for European Languages(https://arxiv.org/abs/2410.08928)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rise of Large Language Models (LLMs) has revolutionized natural language processing across numerous languages and tasks. However, evaluating LLM performance in a consistent and meaningful way across multiple European languages remains challenging, especially due to the scarcity of multilingual benchmarks. We introduce a cross-lingual evaluation approach tailored for European languages. We employ translated versions of five widely-used benchmarks to assess the capabilities of 40 LLMs across 21 European languages. Our contributions include examining the effectiveness of translated benchmarks, assessing the impact of different translation services, and offering a multilingual evaluation framework for LLMs that includes newly created datasets: EU20-MMLU, EU20-HellaSwag, EU20-ARC, EU20-TruthfulQA, and EU20-GSM8K. The benchmarks and results are made publicly available to encourage further research in multilingual LLM evaluation.</li>
</ul>

<h3>Title: Maximizing the Potential of Synthetic Data: Insights from Random Matrix Theory</h3>
<ul>
<li><strong>Authors: </strong>Aymane El Firdoussi, Mohamed El Amine Seddik, Soufiane Hayou, Reda Alami, Ahmed Alzubaidi, Hakim Hacid</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.ST</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08942">https://arxiv.org/abs/2410.08942</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08942">https://arxiv.org/pdf/2410.08942</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08942]] Maximizing the Potential of Synthetic Data: Insights from Random Matrix Theory(https://arxiv.org/abs/2410.08942)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Synthetic data has gained attention for training large language models, but poor-quality data can harm performance (see, e.g., Shumailov et al. (2023); Seddik et al. (2024)). A potential solution is data pruning, which retains only high-quality data based on a score function (human or machine feedback). Previous work Feng et al. (2024) analyzed models trained on synthetic data as sample size increases. We extend this by using random matrix theory to derive the performance of a binary classifier trained on a mix of real and pruned synthetic data in a high dimensional setting. Our findings identify conditions where synthetic data could improve performance, focusing on the quality of the generative model and verification strategy. We also show a smooth phase transition in synthetic label noise, contrasting with prior sharp behavior in infinite sample limits. Experiments with toy models and large language models validate our theoretical results.</li>
</ul>

<h3>Title: Parallel Watershed Partitioning: GPU-Based Hierarchical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Varduhi Yeghiazaryan, Yeva Gabrielyan, Irina Voiculescu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08946">https://arxiv.org/abs/2410.08946</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08946">https://arxiv.org/pdf/2410.08946</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08946]] Parallel Watershed Partitioning: GPU-Based Hierarchical Image Segmentation(https://arxiv.org/abs/2410.08946)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Many image processing applications rely on partitioning an image into disjoint regions whose pixels are 'similar.' The watershed and waterfall transforms are established mathematical morphology pixel clustering techniques. They are both relevant to modern applications where groups of pixels are to be decided upon in one go, or where adjacency information is relevant. We introduce three new parallel partitioning algorithms for GPUs. By repeatedly applying watershed algorithms, we produce waterfall results which form a hierarchy of partition regions over an input image. Our watershed algorithms attain competitive execution times in both 2D and 3D, processing an 800 megavoxel image in less than 1.4 sec. We also show how to use this fully deterministic image partitioning as a pre-processing step to machine learning based semantic segmentation. This replaces the role of superpixel algorithms, and results in comparable accuracy and faster training times.</li>
</ul>

<h3>Title: On the Adversarial Transferability of Generalized "Skip Connections"</h3>
<ul>
<li><strong>Authors: </strong>Yisen Wang, Yichuan Mo, Dongxian Wu, Mingjie Li, Xingjun Ma, Zhouchen Lin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08950">https://arxiv.org/abs/2410.08950</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08950">https://arxiv.org/pdf/2410.08950</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08950]] On the Adversarial Transferability of Generalized "Skip Connections"(https://arxiv.org/abs/2410.08950)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, defense, attack, steal, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Skip connection is an essential ingredient for modern deep models to be deeper and more powerful. Despite their huge success in normal scenarios (state-of-the-art classification performance on natural examples), we investigate and identify an interesting property of skip connections under adversarial scenarios, namely, the use of skip connections allows easier generation of highly transferable adversarial examples. Specifically, in ResNet-like models (with skip connections), we find that using more gradients from the skip connections rather than the residual modules according to a decay factor during backpropagation allows one to craft adversarial examples with high transferability. The above method is termed as Skip Gradient Method (SGM). Although starting from ResNet-like models in vision domains, we further extend SGM to more advanced architectures, including Vision Transformers (ViTs) and models with length-varying paths and other domains, i.e. natural language processing. We conduct comprehensive transfer attacks against various models including ResNets, Transformers, Inceptions, Neural Architecture Search, and Large Language Models (LLMs). We show that employing SGM can greatly improve the transferability of crafted attacks in almost all cases. Furthermore, considering the big complexity for practical use, we further demonstrate that SGM can even improve the transferability on ensembles of models or targeted attacks and the stealthiness against current defenses. At last, we provide theoretical explanations and empirical insights on how SGM works. Our findings not only motivate new adversarial research into the architectural characteristics of models but also open up further challenges for secure model architecture design. Our code is available at this https URL.</li>
</ul>

<h3>Title: Evaluating Federated Kolmogorov-Arnold Networks on Non-IID Data</h3>
<ul>
<li><strong>Authors: </strong>Arthur Mendonça Sasse, Claudio Miceli de Farias</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08961">https://arxiv.org/abs/2410.08961</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08961">https://arxiv.org/pdf/2410.08961</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08961]] Evaluating Federated Kolmogorov-Arnold Networks on Non-IID Data(https://arxiv.org/abs/2410.08961)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Federated Kolmogorov-Arnold Networks (F-KANs) have already been proposed, but their assessment is at an initial stage. We present a comparison between KANs (using B-splines and Radial Basis Functions as activation functions) and Multi- Layer Perceptrons (MLPs) with a similar number of parameters for 100 rounds of federated learning in the MNIST classification task using non-IID partitions with 100 clients. After 15 trials for each model, we show that the best accuracies achieved by MLPs can be achieved by Spline-KANs in half of the time (in rounds), with just a moderate increase in computing time.</li>
</ul>

<h3>Title: Language Imbalance Driven Rewarding for Multilingual Self-improving</h3>
<ul>
<li><strong>Authors: </strong>Wen Yang, Junhong Wu, Chen Wang, Chengqing Zong, Jiajun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08964">https://arxiv.org/abs/2410.08964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08964">https://arxiv.org/pdf/2410.08964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08964]] Language Imbalance Driven Rewarding for Multilingual Self-improving(https://arxiv.org/abs/2410.08964)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved state-of-the-art performance across numerous tasks. However, these advancements have predominantly benefited "first-class" languages such as English and Chinese, leaving many other languages underrepresented. This imbalance, while limiting broader applications, generates a natural preference ranking between languages, offering an opportunity to bootstrap the multilingual capabilities of LLM in a self-improving manner. Thus, we propose $\textit{Language Imbalance Driven Rewarding}$, where the inherent imbalance between dominant and non-dominant languages within LLMs is leveraged as a reward signal. Iterative DPO training demonstrates that this approach not only enhances LLM performance in non-dominant languages but also improves the dominant language's capacity, thereby yielding an iterative reward signal. Fine-tuning Meta-Llama-3-8B-Instruct over two iterations of this approach results in continuous improvements in multilingual performance across instruction-following and arithmetic reasoning tasks, evidenced by an average improvement of 7.46% win rate on the X-AlpacaEval leaderboard and 13.9% accuracy on the MGSM benchmark. This work serves as an initial exploration, paving the way for multilingual self-improvement of LLMs.</li>
</ul>

<h3>Title: Controllable Safety Alignment: Inference-Time Adaptation to Diverse Safety Requirements</h3>
<ul>
<li><strong>Authors: </strong>Jingyu Zhang, Ahmed Elgohary, Ahmed Magooda, Daniel Khashabi, Benjamin Van Durme</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08968">https://arxiv.org/abs/2410.08968</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08968">https://arxiv.org/pdf/2410.08968</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08968]] Controllable Safety Alignment: Inference-Time Adaptation to Diverse Safety Requirements(https://arxiv.org/abs/2410.08968)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The current paradigm for safety alignment of large language models (LLMs) follows a one-size-fits-all approach: the model refuses to interact with any content deemed unsafe by the model provider. This approach lacks flexibility in the face of varying social norms across cultures and regions. In addition, users may have diverse safety needs, making a model with static safety standards too restrictive to be useful, as well as too costly to be re-aligned. We propose Controllable Safety Alignment (CoSA), a framework designed to adapt models to diverse safety requirements without re-training. Instead of aligning a fixed model, we align models to follow safety configs -- free-form natural language descriptions of the desired safety behaviors -- that are provided as part of the system prompt. To adjust model safety behavior, authorized users only need to modify such safety configs at inference time. To enable that, we propose CoSAlign, a data-centric method for aligning LLMs to easily adapt to diverse safety configs. Furthermore, we devise a novel controllability evaluation protocol that considers both helpfulness and configured safety, summarizing them into CoSA-Score, and construct CoSApien, a human-authored benchmark that consists of real-world LLM use cases with diverse safety requirements and corresponding evaluation prompts. We show that CoSAlign leads to substantial gains of controllability over strong baselines including in-context alignment. Our framework encourages better representation and adaptation to pluralistic human values in LLMs, and thereby increasing their practicality.</li>
</ul>

<h3>Title: NoVo: Norm Voting off Hallucinations with Attention Heads in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zheng Yi Ho, Siyuan Liang, Sen Zhang, Yibing Zhan, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08970">https://arxiv.org/abs/2410.08970</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08970">https://arxiv.org/pdf/2410.08970</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08970]] NoVo: Norm Voting off Hallucinations with Attention Heads in Large Language Models(https://arxiv.org/abs/2410.08970)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Hallucinations in Large Language Models (LLMs) remain a major obstacle, particularly in high-stakes applications where factual accuracy is critical. While representation editing and reading methods have made strides in reducing hallucinations, their heavy reliance on specialised tools and training on in-domain samples, makes them difficult to scale and prone to overfitting. This limits their accuracy gains and generalizability to diverse datasets. This paper presents a lightweight method, Norm Voting (NoVo), which harnesses the untapped potential of attention head norms to dramatically enhance factual accuracy in zero-shot multiple-choice questions (MCQs). NoVo begins by automatically selecting truth-correlated head norms with an efficient, inference-only algorithm using only 30 random samples, allowing NoVo to effortlessly scale to diverse datasets. Afterwards, selected head norms are employed in a simple voting algorithm, which yields significant gains in prediction accuracy. On TruthfulQA MC1, NoVo surpasses the current state-of-the-art and all previous methods by an astounding margin -- at least 19 accuracy points. NoVo demonstrates exceptional generalization to 20 diverse datasets, with significant gains in over 90\% of them, far exceeding all current representation editing and reading methods. NoVo also reveals promising gains to finetuning strategies and building textual adversarial defence. NoVo's effectiveness with head norms opens new frontiers in LLM interpretability, robustness and reliability.</li>
</ul>

<h3>Title: Extra Global Attention Designation Using Keyword Detection in Sparse Transformer Architectures</h3>
<ul>
<li><strong>Authors: </strong>Evan Lucas, Dylan Kangas, Timothy C Havens</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08971">https://arxiv.org/abs/2410.08971</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08971">https://arxiv.org/pdf/2410.08971</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08971]] Extra Global Attention Designation Using Keyword Detection in Sparse Transformer Architectures(https://arxiv.org/abs/2410.08971)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In this paper, we propose an extension to Longformer Encoder-Decoder, a popular sparse transformer architecture. One common challenge with sparse transformers is that they can struggle with encoding of long range context, such as connections between topics discussed at a beginning and end of a document. A method to selectively increase global attention is proposed and demonstrated for abstractive summarization tasks on several benchmark data sets. By prefixing the transcript with additional keywords and encoding global attention on these keywords, improvement in zero-shot, few-shot, and fine-tuned cases is demonstrated for some benchmark data sets.</li>
</ul>

<h3>Title: DEL: Discrete Element Learner for Learning 3D Particle Dynamics with Neural Rendering</h3>
<ul>
<li><strong>Authors: </strong>Jiaxu Wang, Jingkai Sun, Junhao He, Ziyi Zhang, Qiang Zhang, Mingyuan Sun, Renjing Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08983">https://arxiv.org/abs/2410.08983</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08983">https://arxiv.org/pdf/2410.08983</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08983]] DEL: Discrete Element Learner for Learning 3D Particle Dynamics with Neural Rendering(https://arxiv.org/abs/2410.08983)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Learning-based simulators show great potential for simulating particle dynamics when 3D groundtruth is available, but per-particle correspondences are not always accessible. The development of neural rendering presents a new solution to this field to learn 3D dynamics from 2D images by inverse rendering. However, existing approaches still suffer from ill-posed natures resulting from the 2D to 3D uncertainty, for example, specific 2D images can correspond with various 3D particle distributions. To mitigate such uncertainty, we consider a conventional, mechanically interpretable framework as the physical priors and extend it to a learning-based version. In brief, we incorporate the learnable graph kernels into the classic Discrete Element Analysis (DEA) framework to implement a novel mechanics-integrated learning system. In this case, the graph network kernels are only used for approximating some specific mechanical operators in the DEA framework rather than the whole dynamics mapping. By integrating the strong physics priors, our methods can effectively learn the dynamics of various materials from the partial 2D observations in a unified manner. Experiments show that our approach outperforms other learned simulators by a large margin in this context and is robust to different renderers, fewer training samples, and fewer camera views.</li>
</ul>

<h3>Title: SubZero: Random Subspace Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Ziming Yu, Pan Zhou, Sike Wang, Jia Li, Hua Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08989">https://arxiv.org/abs/2410.08989</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08989">https://arxiv.org/pdf/2410.08989</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08989]] SubZero: Random Subspace Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning(https://arxiv.org/abs/2410.08989)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning Large Language Models (LLMs) has proven effective for a variety of downstream tasks. However, as LLMs grow in size, the memory demands for backpropagation become increasingly prohibitive. Zeroth-order (ZO) optimization methods offer a memory-efficient alternative by using forward passes to estimate gradients, but the variance of gradient estimates typically scales linearly with the model's parameter dimension$\unicode{x2013}$a significant issue for LLMs. In this paper, we propose the random Subspace Zeroth-order (SubZero) optimization to address the challenges posed by LLMs' high dimensionality. We introduce a low-rank perturbation tailored for LLMs that significantly reduces memory consumption while improving training performance. Additionally, we prove that our gradient estimation closely approximates the backpropagation gradient, exhibits lower variance than traditional ZO methods, and ensures convergence when combined with SGD. Experimental results show that SubZero enhances fine-tuning performance and achieves faster convergence compared to standard ZO approaches like MeZO across various language modeling tasks.</li>
</ul>

<h3>Title: Science is Exploration: Computational Frontiers for Conceptual Metaphor Theory</h3>
<ul>
<li><strong>Authors: </strong>Rebecca M. M. Hicke, Ross Deans Kristensen-McLachlan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08991">https://arxiv.org/abs/2410.08991</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08991">https://arxiv.org/pdf/2410.08991</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08991]] Science is Exploration: Computational Frontiers for Conceptual Metaphor Theory(https://arxiv.org/abs/2410.08991)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Metaphors are everywhere. They appear extensively across all domains of natural language, from the most sophisticated poetry to seemingly dry academic prose. A significant body of research in the cognitive science of language argues for the existence of conceptual metaphors, the systematic structuring of one domain of experience in the language of another. Conceptual metaphors are not simply rhetorical flourishes but are crucial evidence of the role of analogical reasoning in human cognition. In this paper, we ask whether Large Language Models (LLMs) can accurately identify and explain the presence of such conceptual metaphors in natural language data. Using a novel prompting technique based on metaphor annotation guidelines, we demonstrate that LLMs are a promising tool for large-scale computational research on conceptual metaphors. Further, we show that LLMs are able to apply procedural guidelines designed for human annotators, displaying a surprising depth of linguistic knowledge.</li>
</ul>

<h3>Title: Hypothesis-only Biases in Large Language Model-Elicited Natural Language Inference</h3>
<ul>
<li><strong>Authors: </strong>Grace Proebsting, Adam Poliak</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.08996">https://arxiv.org/abs/2410.08996</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.08996">https://arxiv.org/pdf/2410.08996</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.08996]] Hypothesis-only Biases in Large Language Model-Elicited Natural Language Inference(https://arxiv.org/abs/2410.08996)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We test whether replacing crowdsource workers with LLMs to write Natural Language Inference (NLI) hypotheses similarly results in annotation artifacts. We recreate a portion of the Stanford NLI corpus using GPT-4, Llama-2 and Mistral 7b, and train hypothesis-only classifiers to determine whether LLM-elicited hypotheses contain annotation artifacts. On our LLM-elicited NLI datasets, BERT-based hypothesis-only classifiers achieve between 86-96% accuracy, indicating these datasets contain hypothesis-only artifacts. We also find frequent "give-aways" in LLM-generated hypotheses, e.g. the phrase "swimming in a pool" appears in more than 10,000 contradictions generated by GPT-4. Our analysis provides empirical evidence that well-attested biases in NLI can persist in LLM-generated data.</li>
</ul>

<h3>Title: SuperCorrect: Supervising and Correcting Language Models with Error-Driven Insights</h3>
<ul>
<li><strong>Authors: </strong>Ling Yang, Zhaochen Yu, Tianjun Zhang, Minkai Xu, Joseph E. Gonzalez, Bin Cui, Shuicheng Yan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09008">https://arxiv.org/abs/2410.09008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09008">https://arxiv.org/pdf/2410.09008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09008]] SuperCorrect: Supervising and Correcting Language Models with Error-Driven Insights(https://arxiv.org/abs/2410.09008)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) like GPT-4, PaLM, and LLaMA have shown significant improvements in various reasoning tasks. However, smaller models such as Llama-3-8B and DeepSeekMath-Base still struggle with complex mathematical reasoning because they fail to effectively identify and correct reasoning errors. Recent reflection-based methods aim to address these issues by enabling self-reflection and self-correction, but they still face challenges in independently detecting errors in their reasoning steps. To overcome these limitations, we propose SuperCorrect, a novel two-stage framework that uses a large teacher model to supervise and correct both the reasoning and reflection processes of a smaller student model. In the first stage, we extract hierarchical high-level and detailed thought templates from the teacher model to guide the student model in eliciting more fine-grained reasoning thoughts. In the second stage, we introduce cross-model collaborative direct preference optimization (DPO) to enhance the self-correction abilities of the student model by following the teacher's correction traces during training. This cross-model DPO approach teaches the student model to effectively locate and resolve erroneous thoughts with error-driven insights from the teacher model, breaking the bottleneck of its thoughts and acquiring new skills and knowledge to tackle challenging problems. Extensive experiments consistently demonstrate our superiority over previous methods. Notably, our SuperCorrect-7B model significantly surpasses powerful DeepSeekMath-7B by 7.8%/5.3% and Qwen2.5-Math-7B by 15.1%/6.3% on MATH/GSM8K benchmarks, achieving new SOTA performance among all 7B models. Code: this https URL</li>
</ul>

<h3>Title: Semantic Score Distillation Sampling for Compositional Text-to-3D Generation</h3>
<ul>
<li><strong>Authors: </strong>Ling Yang, Zixiang Zhang, Junlin Han, Bohan Zeng, Runjia Li, Philip Torr, Wentao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09009">https://arxiv.org/abs/2410.09009</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09009">https://arxiv.org/pdf/2410.09009</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09009]] Semantic Score Distillation Sampling for Compositional Text-to-3D Generation(https://arxiv.org/abs/2410.09009)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Generating high-quality 3D assets from textual descriptions remains a pivotal challenge in computer graphics and vision research. Due to the scarcity of 3D data, state-of-the-art approaches utilize pre-trained 2D diffusion priors, optimized through Score Distillation Sampling (SDS). Despite progress, crafting complex 3D scenes featuring multiple objects or intricate interactions is still difficult. To tackle this, recent methods have incorporated box or layout guidance. However, these layout-guided compositional methods often struggle to provide fine-grained control, as they are generally coarse and lack expressiveness. To overcome these challenges, we introduce a novel SDS approach, Semantic Score Distillation Sampling (SemanticSDS), designed to effectively improve the expressiveness and accuracy of compositional text-to-3D generation. Our approach integrates new semantic embeddings that maintain consistency across different rendering views and clearly differentiate between various objects and parts. These embeddings are transformed into a semantic map, which directs a region-specific SDS process, enabling precise optimization and compositional generation. By leveraging explicit semantic guidance, our method unlocks the compositional capabilities of existing pre-trained diffusion models, thereby achieving superior quality in 3D content generation, particularly for complex objects and scenes. Experimental results demonstrate that our SemanticSDS framework is highly effective for generating state-of-the-art complex 3D content. Code: this https URL</li>
</ul>

<h3>Title: CVAM-Pose: Conditional Variational Autoencoder for Multi-Object Monocular Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Jianyu Zhao, Wei Quan, Bogdan J. Matuszewski</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09010">https://arxiv.org/abs/2410.09010</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09010">https://arxiv.org/pdf/2410.09010</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09010]] CVAM-Pose: Conditional Variational Autoencoder for Multi-Object Monocular Pose Estimation(https://arxiv.org/abs/2410.09010)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Estimating rigid objects' poses is one of the fundamental problems in computer vision, with a range of applications across automation and augmented reality. Most existing approaches adopt one network per object class strategy, depend heavily on objects' 3D models, depth data, and employ a time-consuming iterative refinement, which could be impractical for some applications. This paper presents a novel approach, CVAM-Pose, for multi-object monocular pose estimation that addresses these limitations. The CVAM-Pose method employs a label-embedded conditional variational autoencoder network, to implicitly abstract regularised representations of multiple objects in a single low-dimensional latent space. This autoencoding process uses only images captured by a projective camera and is robust to objects' occlusion and scene clutter. The classes of objects are one-hot encoded and embedded throughout the network. The proposed label-embedded pose regression strategy interprets the learnt latent space representations utilising continuous pose representations. Ablation tests and systematic evaluations demonstrate the scalability and efficiency of the CVAM-Pose method for multi-object scenarios. The proposed CVAM-Pose outperforms competing latent space approaches. For example, it is respectively 25% and 20% better than AAE and Multi-Path methods, when evaluated using the $\mathrm{AR_{VSD}}$ metric on the Linemod-Occluded dataset. It also achieves results somewhat comparable to methods reliant on 3D models reported in BOP challenges. Code available: this https URL</li>
</ul>

<h3>Title: The Impact of Visual Information in Chinese Characters: Evaluating Large Models' Ability to Recognize and Utilize Radicals</h3>
<ul>
<li><strong>Authors: </strong>Xiaofeng Wu, Karl Stratos, Wei Xu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09013">https://arxiv.org/abs/2410.09013</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09013">https://arxiv.org/pdf/2410.09013</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09013]] The Impact of Visual Information in Chinese Characters: Evaluating Large Models' Ability to Recognize and Utilize Radicals(https://arxiv.org/abs/2410.09013)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The glyphic writing system of Chinese incorporates information-rich visual features in each character, such as radicals that provide hints about meaning or pronunciation. However, there has been no investigation into whether contemporary Large Language Models (LLMs) and Vision-Language Models (VLMs) can harness these sub-character features in Chinese through prompting. In this study, we establish a benchmark to evaluate LLMs' and VLMs' understanding of visual elements in Chinese characters, including radicals, composition structures, strokes, and stroke counts. Our results reveal that models surprisingly exhibit some, but still limited, knowledge of the visual information, regardless of whether images of characters are provided. To incite models' ability to use radicals, we further experiment with incorporating radicals into the prompts for Chinese language understanding tasks. We observe consistent improvement in Part-Of-Speech tagging when providing additional information about radicals, suggesting the potential to enhance CLP by integrating sub-character information.</li>
</ul>

<h3>Title: MedMobile: A mobile-sized language model with expert-level clinical capabilities</h3>
<ul>
<li><strong>Authors: </strong>Krithik Vishwanath, Jaden Stryker, Anton Alaykin, Daniel Alexander Alber, Eric Karl Oermann</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09019">https://arxiv.org/abs/2410.09019</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09019">https://arxiv.org/pdf/2410.09019</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09019]] MedMobile: A mobile-sized language model with expert-level clinical capabilities(https://arxiv.org/abs/2410.09019)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Language models (LMs) have demonstrated expert-level reasoning and recall abilities in medicine. However, computational costs and privacy concerns are mounting barriers to wide-scale implementation. We introduce a parsimonious adaptation of phi-3-mini, MedMobile, a 3.8 billion parameter LM capable of running on a mobile device, for medical applications. We demonstrate that MedMobile scores 75.7% on the MedQA (USMLE), surpassing the passing mark for physicians (~60%), and approaching the scores of models 100 times its size. We subsequently perform a careful set of ablations, and demonstrate that chain of thought, ensembling, and fine-tuning lead to the greatest performance gains, while unexpectedly retrieval augmented generation fails to demonstrate significant improvements</li>
</ul>

<h3>Title: AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents</h3>
<ul>
<li><strong>Authors: </strong>Maksym Andriushchenko, Alexandra Souly, Mateusz Dziemian, Derek Duenas, Maxwell Lin, Justin Wang, Dan Hendrycks, Andy Zou, Zico Kolter, Matt Fredrikson, Eric Winsor, Jerome Wynne, Yarin Gal, Xander Davies</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09024">https://arxiv.org/abs/2410.09024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09024">https://arxiv.org/pdf/2410.09024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09024]] AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents(https://arxiv.org/abs/2410.09024)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>The robustness of LLMs to jailbreak attacks, where users design prompts to circumvent safety measures and misuse model capabilities, has been studied primarily for LLMs acting as simple chatbots. Meanwhile, LLM agents -- which use external tools and can execute multi-stage tasks -- may pose a greater risk if misused, but their robustness remains underexplored. To facilitate research on LLM agent misuse, we propose a new benchmark called AgentHarm. The benchmark includes a diverse set of 110 explicitly malicious agent tasks (440 with augmentations), covering 11 harm categories including fraud, cybercrime, and harassment. In addition to measuring whether models refuse harmful agentic requests, scoring well on AgentHarm requires jailbroken agents to maintain their capabilities following an attack to complete a multi-step task. We evaluate a range of leading LLMs, and find (1) leading LLMs are surprisingly compliant with malicious agent requests without jailbreaking, (2) simple universal jailbreak templates can be adapted to effectively jailbreak agents, and (3) these jailbreaks enable coherent and malicious multi-step agent behavior and retain model capabilities. We publicly release AgentHarm to enable simple and reliable evaluation of attacks and defenses for LLM-based agents. We publicly release the benchmark at this https URL.</li>
</ul>

<h3>Title: Alberta Wells Dataset: Pinpointing Oil and Gas Wells from Satellite Imagery</h3>
<ul>
<li><strong>Authors: </strong>Pratinav Seth, Michelle Lin, Brefo Dwamena Yaw, Jade Boutot, Mary Kang, David Rolnick</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09032">https://arxiv.org/abs/2410.09032</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09032">https://arxiv.org/pdf/2410.09032</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09032]] Alberta Wells Dataset: Pinpointing Oil and Gas Wells from Satellite Imagery(https://arxiv.org/abs/2410.09032)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Millions of abandoned oil and gas wells are scattered across the world, leaching methane into the atmosphere and toxic compounds into the groundwater. Many of these locations are unknown, preventing the wells from being plugged and their polluting effects averted. Remote sensing is a relatively unexplored tool for pinpointing abandoned wells at scale. We introduce the first large-scale benchmark dataset for this problem, leveraging medium-resolution multi-spectral satellite imagery from Planet Labs. Our curated dataset comprises over 213,000 wells (abandoned, suspended, and active) from Alberta, a region with especially high well density, sourced from the Alberta Energy Regulator and verified by domain experts. We evaluate baseline algorithms for well detection and segmentation, showing the promise of computer vision approaches but also significant room for improvement.</li>
</ul>

<h3>Title: Mentor-KD: Making Small Language Models Better Multi-step Reasoners</h3>
<ul>
<li><strong>Authors: </strong>Hojae Lee, Junho Kim, SangKeun Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09037">https://arxiv.org/abs/2410.09037</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09037">https://arxiv.org/pdf/2410.09037</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09037]] Mentor-KD: Making Small Language Models Better Multi-step Reasoners(https://arxiv.org/abs/2410.09037)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have displayed remarkable performances across various complex tasks by leveraging Chain-of-Thought (CoT) prompting. Recently, studies have proposed a Knowledge Distillation (KD) approach, reasoning distillation, which transfers such reasoning ability of LLMs through fine-tuning language models of multi-step rationales generated by LLM teachers. However, they have inadequately considered two challenges regarding insufficient distillation sets from the LLM teacher model, in terms of 1) data quality and 2) soft label provision. In this paper, we propose Mentor-KD, which effectively distills the multi-step reasoning capability of LLMs to smaller LMs while addressing the aforementioned challenges. Specifically, we exploit a mentor, intermediate-sized task-specific fine-tuned model, to augment additional CoT annotations and provide soft labels for the student model during reasoning distillation. We conduct extensive experiments and confirm Mentor-KD's effectiveness across various models and complex reasoning tasks.</li>
</ul>

<h3>Title: SimpleStrat: Diversifying Language Model Generation with Stratification</h3>
<ul>
<li><strong>Authors: </strong>Justin Wong, Yury Orlovskiy, Michael Luo, Sanjit A. Seshia, Joseph E. Gonzalez</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09038">https://arxiv.org/abs/2410.09038</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09038">https://arxiv.org/pdf/2410.09038</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09038]] SimpleStrat: Diversifying Language Model Generation with Stratification(https://arxiv.org/abs/2410.09038)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Generating diverse responses from large language models (LLMs) is crucial for applications such as planning/search and synthetic data generation, where diversity provides distinct answers across generations. Prior approaches rely on increasing temperature to increase diversity. However, contrary to popular belief, we show not only does this approach produce lower quality individual generations as temperature increases, but it depends on model's next-token probabilities being similar to the true distribution of answers. We propose \method{}, an alternative approach that uses the language model itself to partition the space into strata. At inference, a random stratum is selected and a sample drawn from within the strata. To measure diversity, we introduce CoverageQA, a dataset of underspecified questions with multiple equally plausible answers, and assess diversity by measuring KL Divergence between the output distribution and uniform distribution over valid ground truth answers. As computing probability per response/solution for proprietary models is infeasible, we measure recall on ground truth solutions. Our evaluation show using SimpleStrat achieves higher recall by 0.05 compared to GPT-4o and 0.36 average reduction in KL Divergence compared to Llama 3.</li>
</ul>

<h3>Title: AttnGCG: Enhancing Jailbreaking Attacks on LLMs with Attention Manipulation</h3>
<ul>
<li><strong>Authors: </strong>Zijun Wang, Haoqin Tu, Jieru Mei, Bingchen Zhao, Yisen Wang, Cihang Xie</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09040">https://arxiv.org/abs/2410.09040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09040">https://arxiv.org/pdf/2410.09040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09040]] AttnGCG: Enhancing Jailbreaking Attacks on LLMs with Attention Manipulation(https://arxiv.org/abs/2410.09040)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>This paper studies the vulnerabilities of transformer-based Large Language Models (LLMs) to jailbreaking attacks, focusing specifically on the optimization-based Greedy Coordinate Gradient (GCG) strategy. We first observe a positive correlation between the effectiveness of attacks and the internal behaviors of the models. For instance, attacks tend to be less effective when models pay more attention to system prompts designed to ensure LLM safety alignment. Building on this discovery, we introduce an enhanced method that manipulates models' attention scores to facilitate LLM jailbreaking, which we term AttnGCG. Empirically, AttnGCG shows consistent improvements in attack efficacy across diverse LLMs, achieving an average increase of ~7% in the Llama-2 series and ~10% in the Gemma series. Our strategy also demonstrates robust attack transferability against both unseen harmful goals and black-box LLMs like GPT-3.5 and GPT-4. Moreover, we note our attention-score visualization is more interpretable, allowing us to gain better insights into how our targeted attention manipulation facilitates more effective jailbreaking. We release the code at this https URL.</li>
</ul>

<h3>Title: Transforming In-Vehicle Network Intrusion Detection: VAE-based Knowledge Distillation Meets Explainable AI</h3>
<ul>
<li><strong>Authors: </strong>Muhammet Anil Yagiz, Pedram MohajerAnsari, Mert D. Pese, Polat Goktas</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09043">https://arxiv.org/abs/2410.09043</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09043">https://arxiv.org/pdf/2410.09043</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09043]] Transforming In-Vehicle Network Intrusion Detection: VAE-based Knowledge Distillation Meets Explainable AI(https://arxiv.org/abs/2410.09043)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack, robust</a></li>
<li><strong>Abstract: </strong>In the evolving landscape of autonomous vehicles, ensuring robust in-vehicle network (IVN) security is paramount. This paper introduces an advanced intrusion detection system (IDS) called KD-XVAE that uses a Variational Autoencoder (VAE)-based knowledge distillation approach to enhance both performance and efficiency. Our model significantly reduces complexity, operating with just 1669 parameters and achieving an inference time of 0.3 ms per batch, making it highly suitable for resource-constrained automotive environments. Evaluations in the HCRL Car-Hacking dataset demonstrate exceptional capabilities, attaining perfect scores (Recall, Precision, F1 Score of 100%, and FNR of 0%) under multiple attack types, including DoS, Fuzzing, Gear Spoofing, and RPM Spoofing. Comparative analysis on the CICIoV2024 dataset further underscores its superiority over traditional machine learning models, achieving perfect detection metrics. We furthermore integrate Explainable AI (XAI) techniques to ensure transparency in the model's decisions. The VAE compresses the original feature space into a latent space, on which the distilled model is trained. SHAP(SHapley Additive exPlanations) values provide insights into the importance of each latent dimension, mapped back to original features for intuitive understanding. Our paper advances the field by integrating state-of-the-art techniques, addressing critical challenges in the deployment of efficient, trustworthy, and reliable IDSes for autonomous vehicles, ensuring enhanced protection against emerging cyber threats.</li>
</ul>

<h3>Title: SceneCraft: Layout-Guided 3D Scene Generation</h3>
<ul>
<li><strong>Authors: </strong>Xiuyu Yang, Yunze Man, Jun-Kun Chen, Yu-Xiong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.09049">https://arxiv.org/abs/2410.09049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.09049">https://arxiv.org/pdf/2410.09049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.09049]] SceneCraft: Layout-Guided 3D Scene Generation(https://arxiv.org/abs/2410.09049)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The creation of complex 3D scenes tailored to user specifications has been a tedious and challenging task with traditional 3D modeling tools. Although some pioneering methods have achieved automatic text-to-3D generation, they are generally limited to small-scale scenes with restricted control over the shape and texture. We introduce SceneCraft, a novel method for generating detailed indoor scenes that adhere to textual descriptions and spatial layout preferences provided by users. Central to our method is a rendering-based technique, which converts 3D semantic layouts into multi-view 2D proxy maps. Furthermore, we design a semantic and depth conditioned diffusion model to generate multi-view images, which are used to learn a neural radiance field (NeRF) as the final scene representation. Without the constraints of panorama image generation, we surpass previous methods in supporting complicated indoor space generation beyond a single room, even as complicated as a whole multi-bedroom apartment with irregular shapes and layouts. Through experimental analysis, we demonstrate that our method significantly outperforms existing approaches in complex indoor scene generation with diverse textures, consistent geometry, and realistic visual quality. Code and more results are available at: this https URL</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
