<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-06-16</h1>
<h3>Title: TeleEval-OS: Performance evaluations of large language models for operations scheduling</h3>
<ul>
<li><strong>Authors: </strong>Yanyan Wang, Yingying Wang, Junli Liang, Yin Xu, Yunlong Liu, Yiming Xu, Zhengwang Jiang, Zhehe Li, Fei Li, Long Zhao, Kuang Xu, Qi Song, Xiangyang Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11017">https://arxiv.org/abs/2506.11017</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11017">https://arxiv.org/pdf/2506.11017</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11017]] TeleEval-OS: Performance evaluations of large language models for operations scheduling(https://arxiv.org/abs/2506.11017)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancement of large language models (LLMs) has significantly propelled progress in artificial intelligence, demonstrating substantial application potential across multiple specialized domains. Telecommunications operation scheduling (OS) is a critical aspect of the telecommunications industry, involving the coordinated management of networks, services, risks, and human resources to optimize production scheduling and ensure unified service control. However, the inherent complexity and domain-specific nature of OS tasks, coupled with the absence of comprehensive evaluation benchmarks, have hindered thorough exploration of LLMs' application potential in this critical field. To address this research gap, we propose the first Telecommunications Operation Scheduling Evaluation Benchmark (TeleEval-OS). Specifically, this benchmark comprises 15 datasets across 13 subtasks, comprehensively simulating four key operational stages: intelligent ticket creation, intelligent ticket handling, intelligent ticket closure, and intelligent evaluation. To systematically assess the performance of LLMs on tasks of varying complexity, we categorize their capabilities in telecommunications operation scheduling into four hierarchical levels, arranged in ascending order of difficulty: basic NLP, knowledge Q&A, report generation, and report analysis. On TeleEval-OS, we leverage zero-shot and few-shot evaluation methods to comprehensively assess 10 open-source LLMs (e.g., DeepSeek-V3) and 4 closed-source LLMs (e.g., GPT-4o) across diverse scenarios. Experimental results demonstrate that open-source LLMs can outperform closed-source LLMs in specific scenarios, highlighting their significant potential and value in the field of telecommunications operation scheduling.</li>
</ul>

<h3>Title: Not All Clients Are Equal: Personalized Federated Learning on Heterogeneous Multi-Modal Clients</h3>
<ul>
<li><strong>Authors: </strong>Minhyuk Seo, Taeheon Kim, Hankook Lee, Jonghyun Choi, Tinne Tuytelaars</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11024">https://arxiv.org/abs/2506.11024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11024">https://arxiv.org/pdf/2506.11024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11024]] Not All Clients Are Equal: Personalized Federated Learning on Heterogeneous Multi-Modal Clients(https://arxiv.org/abs/2506.11024)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Foundation models have shown remarkable capabilities across diverse multi-modal tasks, but their centralized training raises privacy concerns and induces high transmission costs. In contrast, federated learning (FL) offers a distributed alternative without the need to share data. Recently, for the growing demand for personalizing AI models for different user purposes, personalized federated learning (PFL) has emerged. PFL allows each client to leverage the knowledge of other clients for further adaptation to individual user preferences, again without the need to share data. Despite its potential, most PFL studies remain confined to simulated environments, overlooking the data and model heterogeneity that arise in real-world scenarios. In contrast, we first consider large data heterogeneity, evaluating on a new benchmark for multi-modal PFL, spanning 40 distinct tasks with realistic data distribution shifts. We then consider model heterogeneity in that we do not assume that all clients share similar model architectures. To address data heterogeneity, we propose a task-similarity-aware model aggregation method that provides customized global models to each client. For model heterogeneity, we propose a dimension-invariant module that enables knowledge sharing across heterogeneous models. Empirical validations demonstrate that the proposed approach outperforms the state-of-the-art, excelling in both personalization and generalization capabilities.</li>
</ul>

<h3>Title: When Algorithms Play Favorites: Lookism in the Generation and Perception of Faces</h3>
<ul>
<li><strong>Authors: </strong>Miriam Doh, Aditya Gulati, Matei Mancas, Nuria Oliver</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11025">https://arxiv.org/abs/2506.11025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11025">https://arxiv.org/pdf/2506.11025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11025]] When Algorithms Play Favorites: Lookism in the Generation and Perception of Faces(https://arxiv.org/abs/2506.11025)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>This paper examines how synthetically generated faces and machine learning-based gender classification algorithms are affected by algorithmic lookism, the preferential treatment based on appearance. In experiments with 13,200 synthetically generated faces, we find that: (1) text-to-image (T2I) systems tend to associate facial attractiveness to unrelated positive traits like intelligence and trustworthiness; and (2) gender classification models exhibit higher error rates on "less-attractive" faces, especially among non-White women. These result raise fairness concerns regarding digital identity systems.</li>
</ul>

<h3>Title: Evaluating Privacy-Utility Tradeoffs in Synthetic Smart Grid Data</h3>
<ul>
<li><strong>Authors: </strong>Andre Catarino, Rui Melo, Rui Abreu, Luis Cruz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11026">https://arxiv.org/abs/2506.11026</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11026">https://arxiv.org/pdf/2506.11026</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11026]] Evaluating Privacy-Utility Tradeoffs in Synthetic Smart Grid Data(https://arxiv.org/abs/2506.11026)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, diffusion, generative</a></li>
<li><strong>Abstract: </strong>The widespread adoption of dynamic Time-of-Use (dToU) electricity tariffs requires accurately identifying households that would benefit from such pricing structures. However, the use of real consumption data poses serious privacy concerns, motivating the adoption of synthetic alternatives. In this study, we conduct a comparative evaluation of four synthetic data generation methods, Wasserstein-GP Generative Adversarial Networks (WGAN), Conditional Tabular GAN (CTGAN), Diffusion Models, and Gaussian noise augmentation, under different synthetic regimes. We assess classification utility, distribution fidelity, and privacy leakage. Our results show that architectural design plays a key role: diffusion models achieve the highest utility (macro-F1 up to 88.2%), while CTGAN provide the strongest resistance to reconstruction attacks. These findings highlight the potential of structured generative models for developing privacy-preserving, data-driven energy systems.</li>
</ul>

<h3>Title: From Reasoning to Code: GRPO Optimization for Underrepresented Languages</h3>
<ul>
<li><strong>Authors: </strong>Federico Pennino, Bianca Raimondi, Massimo Rondelli, Andrea Gurioli, Maurizio Gabbrielli</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.PL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11027">https://arxiv.org/abs/2506.11027</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11027">https://arxiv.org/pdf/2506.11027</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11027]] From Reasoning to Code: GRPO Optimization for Underrepresented Languages(https://arxiv.org/abs/2506.11027)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Generating accurate and executable code using large language models (LLMs) is challenging for languages with limited public training data compared to popular languages such as Python. This paper introduces a generalizable approach that uses small-scale code versions of the Qwen 2.5 model combined with Group Relative Policy Optimization (GRPO) to enable effective code generation through explicit reasoning steps, which is particularly beneficial for languages with smaller source code databases. Using Prolog as a representative use case -- given its limited online presence -- the initial model faced challenges in generating executable code. After some training steps, the model successfully produces logically consistent and syntactically accurate code by directly integrating reasoning-driven feedback into the reinforcement learning loop. Experimental evaluations using mathematical logic problem benchmarks illustrate significant improvements in reasoning quality, code accuracy, and logical correctness, underscoring the potential of this approach to benefit a wide range of programming languages lacking extensive training resources.</li>
</ul>

<h3>Title: Enhancing Epidemic Forecasting: Evaluating the Role of Mobility Data and Graph Convolutional Networks</h3>
<ul>
<li><strong>Authors: </strong>Suhan Guo, Zhenghao Xu, Furao Shen, Jian Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11028">https://arxiv.org/abs/2506.11028</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11028">https://arxiv.org/pdf/2506.11028</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11028]] Enhancing Epidemic Forecasting: Evaluating the Role of Mobility Data and Graph Convolutional Networks(https://arxiv.org/abs/2506.11028)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Accurate prediction of contagious disease outbreaks is vital for informed decision-making. Our study addresses the gap between machine learning algorithms and their epidemiological applications, noting that methods optimal for benchmark datasets often underperform with real-world data due to difficulties in incorporating mobility information. We adopt a two-phase approach: first, assessing the significance of mobility data through a pilot study, then evaluating the impact of Graph Convolutional Networks (GCNs) on a transformer backbone. Our findings reveal that while mobility data and GCN modules do not significantly enhance forecasting performance, the inclusion of mortality and hospitalization data markedly improves model accuracy. Additionally, a comparative analysis between GCN-derived spatial maps and lockdown orders suggests a notable correlation, highlighting the potential of spatial maps as sensitive indicators for mobility. Our research offers a novel perspective on mobility representation in predictive modeling for contagious diseases, empowering decision-makers to better prepare for future outbreaks.</li>
</ul>

<h3>Title: Output Scaling: YingLong-Delayed Chain of Thought in a Large Pretrained Time Series Forecasting Model</h3>
<ul>
<li><strong>Authors: </strong>Xue Wang, Tian Zhou, Jinyang Gao, Bolin Ding, Jingren Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11029">https://arxiv.org/abs/2506.11029</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11029">https://arxiv.org/pdf/2506.11029</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11029]] Output Scaling: YingLong-Delayed Chain of Thought in a Large Pretrained Time Series Forecasting Model(https://arxiv.org/abs/2506.11029)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We present a joint forecasting framework for time series prediction that contrasts with traditional direct or recursive methods. This framework achieves state-of-the-art performance for our designed foundation model, YingLong, and reveals a novel scaling effect: longer outputs significantly enhance model accuracy due to delayed chain-of-thought reasoning in our non-causal approach. YingLong is a non-causal, bidirectional attention encoder-only transformer trained through masked token recovery, aligning more effectively with language understanding tasks than with generation tasks. Additionally, we boost performance by tackling output variance with a multi-input ensemble. We release four foundation models ranging from 6M to 300M parameters, demonstrating superior results in zero-shot tasks on the ETT and Weather datasets. YingLong achieves more than 60% best performance. To ensure generalizability, we assessed the models using the GIFT-Eval benchmark, which comprises 23 time series datasets across 7 domains. Yinglong significantly outperformed the best time-series foundation models, end-to-end trained models by 14% and 44% in rank this http URL pretrained 300M model is available at this https URL</li>
</ul>

<h3>Title: Task-aligned prompting improves zero-shot detection of AI-generated images by Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zoher Kachwala, Danishjeet Singh, Danielle Yang, Filippo Menczer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11031">https://arxiv.org/abs/2506.11031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11031">https://arxiv.org/pdf/2506.11031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11031]] Task-aligned prompting improves zero-shot detection of AI-generated images by Vision-Language Models(https://arxiv.org/abs/2506.11031)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>As image generators produce increasingly realistic images, concerns about potential misuse continue to grow. Supervised detection relies on large, curated datasets and struggles to generalize across diverse generators. In this work, we investigate the use of pre-trained Vision-Language Models (VLMs) for zero-shot detection of AI-generated images. While off-the-shelf VLMs exhibit some task-specific reasoning and chain-of-thought prompting offers gains, we show that task-aligned prompting elicits more focused reasoning and significantly improves performance without fine-tuning. Specifically, prefixing the model's response with the phrase ``Let's examine the style and the synthesis artifacts'' -- a method we call zero-shot-s$^2$ -- boosts Macro F1 scores by 8%-29% for two widely used open-source models. These gains are consistent across three recent, diverse datasets spanning human faces, objects, and animals with images generated by 16 different models -- demonstrating strong generalization. We further evaluate the approach across three additional model sizes and observe improvements in most dataset-model combinations -- suggesting robustness to model scale. Surprisingly, self-consistency, a behavior previously observed in language reasoning, where aggregating answers from diverse reasoning paths improves performance, also holds in this setting. Even here, zero-shot-s$^2$ scales better than chain-of-thought in most cases -- indicating that it elicits more useful diversity. Our findings show that task-aligned prompts elicit more focused reasoning and enhance latent capabilities in VLMs, like the detection of AI-generated images -- offering a simple, generalizable, and explainable alternative to supervised methods. Our code is publicly available on github: this https URL.</li>
</ul>

<h3>Title: CausalVLBench: Benchmarking Visual Causal Reasoning in Large Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Aneesh Komanduri, Karuna Bhaila, Xintao Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11034">https://arxiv.org/abs/2506.11034</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11034">https://arxiv.org/pdf/2506.11034</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11034]] CausalVLBench: Benchmarking Visual Causal Reasoning in Large Vision-Language Models(https://arxiv.org/abs/2506.11034)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown remarkable ability in various language tasks, especially with their emergent in-context learning capability. Extending LLMs to incorporate visual inputs, large vision-language models (LVLMs) have shown impressive performance in tasks such as recognition and visual question answering (VQA). Despite increasing interest in the utility of LLMs in causal reasoning tasks such as causal discovery and counterfactual reasoning, there has been relatively little work showcasing the abilities of LVLMs on visual causal reasoning tasks. We take this opportunity to formally introduce a comprehensive causal reasoning benchmark for multi-modal in-context learning from LVLMs. Our CausalVLBench encompasses three representative tasks: causal structure inference, intervention target prediction, and counterfactual prediction. We evaluate the ability of state-of-the-art open-source LVLMs on our causal reasoning tasks across three causal representation learning datasets and demonstrate their fundamental strengths and weaknesses. We hope that our benchmark elucidates the drawbacks of existing vision-language models and motivates new directions and paradigms in improving the visual causal reasoning abilities of LVLMs.</li>
</ul>

<h3>Title: Tversky Neural Networks: Psychologically Plausible Deep Learning with Differentiable Tversky Similarity</h3>
<ul>
<li><strong>Authors: </strong>Moussa Koulako Bala Doumbouya, Dan Jurafsky, Christopher D. Manning</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11035">https://arxiv.org/abs/2506.11035</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11035">https://arxiv.org/pdf/2506.11035</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11035]] Tversky Neural Networks: Psychologically Plausible Deep Learning with Differentiable Tversky Similarity(https://arxiv.org/abs/2506.11035)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Work in psychology has highlighted that the geometric model of similarity standard in deep learning is not psychologically plausible because its metric properties such as symmetry do not align with human perception. In contrast, Tversky (1977) proposed an axiomatic theory of similarity based on a representation of objects as sets of features, and their similarity as a function of common and distinctive features. However, this model has not been used in deep learning before, partly due to the challenge of incorporating discrete set operations. We develop a differentiable parameterization of Tversky's similarity that is learnable through gradient descent, and derive neural network building blocks such as the Tversky projection layer, which unlike the linear projection layer can model non-linear functions such as XOR. Through experiments with image recognition and language modeling, we show that the Tversky projection layer is a beneficial replacement for the linear projection layer, which employs geometric similarity. On the NABirds image classification task, a frozen ResNet-50 adapted with a Tversky projection layer achieves a 24.7% relative accuracy improvement over the linear layer adapter baseline. With Tversky projection layers, GPT-2's perplexity on PTB decreases by 7.5%, and its parameter count by 34.8%. Finally, we propose a unified interpretation of both projection layers as computing similarities of input stimuli to learned prototypes, for which we also propose a novel visualization technique highlighting the interpretability of Tversky projection layers. Our work offers a new paradigm for thinking about the similarity model implicit in deep learning, and designing networks that are interpretable under an established theory of psychological similarity.</li>
</ul>

<h3>Title: Human-centered Interactive Learning via MLLMs for Text-to-Image Person Re-identification</h3>
<ul>
<li><strong>Authors: </strong>Yang Qin, Chao Chen, Zhihang Fu, Dezhong Peng, Xi Peng, Peng Hu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11036">https://arxiv.org/abs/2506.11036</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11036">https://arxiv.org/pdf/2506.11036</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11036]] Human-centered Interactive Learning via MLLMs for Text-to-Image Person Re-identification(https://arxiv.org/abs/2506.11036)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite remarkable advancements in text-to-image person re-identification (TIReID) facilitated by the breakthrough of cross-modal embedding models, existing methods often struggle to distinguish challenging candidate images due to intrinsic limitations, such as network architecture and data quality. To address these issues, we propose an Interactive Cross-modal Learning framework (ICL), which leverages human-centered interaction to enhance the discriminability of text queries through external multimodal knowledge. To achieve this, we propose a plug-and-play Test-time Humane-centered Interaction (THI) module, which performs visual question answering focused on human characteristics, facilitating multi-round interactions with a multimodal large language model (MLLM) to align query intent with latent target images. Specifically, THI refines user queries based on the MLLM responses to reduce the gap to the best-matching images, thereby boosting ranking accuracy. Additionally, to address the limitation of low-quality training texts, we introduce a novel Reorganization Data Augmentation (RDA) strategy based on information enrichment and diversity enhancement to enhance query discriminability by enriching, decomposing, and reorganizing person descriptions. Extensive experiments on four TIReID benchmarks, i.e., CUHK-PEDES, ICFG-PEDES, RSTPReid, and UFine6926, demonstrate that our method achieves remarkable performance with substantial improvement.</li>
</ul>

<h3>Title: Angle Domain Guidance: Latent Diffusion Requires Rotation Rather Than Extrapolation</h3>
<ul>
<li><strong>Authors: </strong>Cheng Jin, Zhenyu Xiao, Chutao Liu, Yuantao Gu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11039">https://arxiv.org/abs/2506.11039</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11039">https://arxiv.org/pdf/2506.11039</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11039]] Angle Domain Guidance: Latent Diffusion Requires Rotation Rather Than Extrapolation(https://arxiv.org/abs/2506.11039)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Classifier-free guidance (CFG) has emerged as a pivotal advancement in text-to-image latent diffusion models, establishing itself as a cornerstone technique for achieving high-quality image synthesis. However, under high guidance weights, where text-image alignment is significantly enhanced, CFG also leads to pronounced color distortions in the generated images. We identify that these distortions stem from the amplification of sample norms in the latent space. We present a theoretical framework that elucidates the mechanisms of norm amplification and anomalous diffusion phenomena induced by classifier-free guidance. Leveraging our theoretical insights and the latent space structure, we propose an Angle Domain Guidance (ADG) algorithm. ADG constrains magnitude variations while optimizing angular alignment, thereby mitigating color distortions while preserving the enhanced text-image alignment achieved at higher guidance weights. Experimental results demonstrate that ADG significantly outperforms existing methods, generating images that not only maintain superior text alignment but also exhibit improved color fidelity and better alignment with human perceptual preferences.</li>
</ul>

<h3>Title: Large Language models for Time Series Analysis: Techniques, Applications, and Challenges</h3>
<ul>
<li><strong>Authors: </strong>Feifei Shi, Xueyan Yin, Kang Wang, Wanyu Tu, Qifu Sun, Huansheng Ning</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11040">https://arxiv.org/abs/2506.11040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11040">https://arxiv.org/pdf/2506.11040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11040]] Large Language models for Time Series Analysis: Techniques, Applications, and Challenges(https://arxiv.org/abs/2506.11040)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Time series analysis is pivotal in domains like financial forecasting and biomedical monitoring, yet traditional methods are constrained by limited nonlinear feature representation and long-term dependency capture. The emergence of Large Language Models (LLMs) offers transformative potential by leveraging their cross-modal knowledge integration and inherent attention mechanisms for time series analysis. However, the development of general-purpose LLMs for time series from scratch is still hindered by data diversity, annotation scarcity, and computational requirements. This paper presents a systematic review of pre-trained LLM-driven time series analysis, focusing on enabling techniques, potential applications, and open challenges. First, it establishes an evolutionary roadmap of AI-driven time series analysis, from the early machine learning era, through the emerging LLM-driven paradigm, to the development of native temporal foundation models. Second, it organizes and systematizes the technical landscape of LLM-driven time series analysis from a workflow perspective, covering LLMs' input, optimization, and lightweight stages. Finally, it critically examines novel real-world applications and highlights key open challenges that can guide future research and innovation. The work not only provides valuable insights into current advances but also outlines promising directions for future development. It serves as a foundational reference for both academic and industrial researchers, paving the way for the development of more efficient, generalizable, and interpretable systems of LLM-driven time series analysis.</li>
</ul>

<h3>Title: ChemHGNN: A Hierarchical Hypergraph Neural Network for Reaction Virtual Screening and Discovery</h3>
<ul>
<li><strong>Authors: </strong>Xiaobao Huang, Yihong Ma, Anjali Gurajapu, Jules Schleinitz, Zhichun Guo, Sarah E. Reisman, Nitesh V. Chawla</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11041">https://arxiv.org/abs/2506.11041</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11041">https://arxiv.org/pdf/2506.11041</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11041]] ChemHGNN: A Hierarchical Hypergraph Neural Network for Reaction Virtual Screening and Discovery(https://arxiv.org/abs/2506.11041)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Reaction virtual screening and discovery are fundamental challenges in chemistry and materials science, where traditional graph neural networks (GNNs) struggle to model multi-reactant interactions. In this work, we propose ChemHGNN, a hypergraph neural network (HGNN) framework that effectively captures high-order relationships in reaction networks. Unlike GNNs, which require constructing complete graphs for multi-reactant reactions, ChemHGNN naturally models multi-reactant reactions through hyperedges, enabling more expressive reaction representations. To address key challenges, such as combinatorial explosion, model collapse, and chemically invalid negative samples, we introduce a reaction center-aware negative sampling strategy (RCNS) and a hierarchical embedding approach combining molecule, reaction and hypergraph level features. Experiments on the USPTO dataset demonstrate that ChemHGNN significantly outperforms HGNN and GNN baselines, particularly in large-scale settings, while maintaining interpretability and chemical plausibility. Our work establishes HGNNs as a superior alternative to GNNs for reaction virtual screening and discovery, offering a chemically informed framework for accelerating reaction discovery.</li>
</ul>

<h3>Title: GenFT: A Generative Parameter-Efficient Fine-Tuning Method for Pretrained Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Baoquan Zhang, Guangning Xu, Michael. K. Ng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11042">https://arxiv.org/abs/2506.11042</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11042">https://arxiv.org/pdf/2506.11042</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11042]] GenFT: A Generative Parameter-Efficient Fine-Tuning Method for Pretrained Foundation Models(https://arxiv.org/abs/2506.11042)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Pretrained Foundation Models (PFMs) have transformed numerous applications by enabling efficient adaptation to customized tasks. Parameter-Efficient Fine-Tuning (PEFT) has emerged as a resource-efficient alternative to full fine-tuning, especially leveraging reparameterized weights $\Delta W$ to adapt models for downstream tasks. However, a critical yet underexplored question remains: can we utilize well-pretrained weights $W_0$ to guide the update of task-specific $\Delta W$, avoiding inefficient training it from scratch? To end this, we propose Generative Parameter-Efficient Fine-Tuning (GenFT), a novel method that extracts structured, transferable information from $W_0$ for efficient $\Delta W$ training. To extract row and column structure information, GenFT applies row and column transformations to distill essential patterns from $W_0$. A tailored policy further decomposes $\Delta W$ into layer-shared and layer-specific components, balancing information reuse and individualized flexibility. GenFT is simple yet effective, achieving superior performance across CV and NLP tasks. Extensive experiments on VTAB-1K, FGVC, and GLUE benchmarks demonstrate that GenFT outperforms state-of-the-art PEFT methods, offering a new perspective for efficient model adaptation.</li>
</ul>

<h3>Title: Boost Post-Training Quantization via Null Space Optimization for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiaqi Zhao, Miao Zhang, Weili Guan, Liqiang Nie</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11044">https://arxiv.org/abs/2506.11044</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11044">https://arxiv.org/pdf/2506.11044</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11044]] Boost Post-Training Quantization via Null Space Optimization for Large Language Models(https://arxiv.org/abs/2506.11044)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Existing post-training quantization methods for large language models (LLMs) offer remarkable success. However, the increasingly marginal performance gains suggest that existing quantization strategies are insufficient to support the development of more compressed models. To inspire new directions for future research, this paper introduces the concept of null space into LLMs quantization. We argue that the quantization error can be effectively alleviated by constraining the post-quantization weight perturbation to lie within the null space of input activations. To prove this idea, we propose a plug-and-play null space projection module for existing milestone PTQ baselines named Q2N. Specifically, we first design an efficient and accurate null space projection approximation method tailored to the characteristics of LLMs. Subsequently, we theoretically derive a closed-form solution for an equivalent vector of the obtained projection matrix, which satisfies practical inference condition while avoiding additional memory overhead. Extensive experiments are conducted on various state-of-the-art LLMs (LLaMA3, DeepSeek, Qwen3) and baselines, demonstrating the effectiveness of both our Q2N and the perspective of null space optimization for LLMs quantization. We view this paper the first step to further alleviate the quantization error based on the insights of null space, hoping it inspiring future researchers to design more advanced quantization methods. Codes are available at this https URL.</li>
</ul>

<h3>Title: The Effects of Data Augmentation on Confidence Estimation for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Rui Wang, Renyu Zhu, Minmin Lin, Runze Wu, Tangjie Lv, Changjie Fan, Haobo Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11046">https://arxiv.org/abs/2506.11046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11046">https://arxiv.org/pdf/2506.11046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11046]] The Effects of Data Augmentation on Confidence Estimation for LLMs(https://arxiv.org/abs/2506.11046)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Confidence estimation is crucial for reflecting the reliability of large language models (LLMs), particularly in the widely used closed-source models. Utilizing data augmentation for confidence estimation is viable, but discussions focus on specific augmentation techniques, limiting its potential. We study the impact of different data augmentation methods on confidence estimation. Our findings indicate that data augmentation strategies can achieve better performance and mitigate the impact of overconfidence. We investigate the influential factors related to this and discover that, while preserving semantic information, greater data diversity enhances the effectiveness of augmentation. Furthermore, the impact of different augmentation strategies varies across different range of application. Considering parameter transferability and usability, the random combination of augmentations is a promising choice.</li>
</ul>

<h3>Title: Perception-Driven Bias Detection in Machine Learning via Crowdsourced Visual Judgment</h3>
<ul>
<li><strong>Authors: </strong>Chirudeep Tupakula, Rittika Shamsuddin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11047">https://arxiv.org/abs/2506.11047</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11047">https://arxiv.org/pdf/2506.11047</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11047]] Perception-Driven Bias Detection in Machine Learning via Crowdsourced Visual Judgment(https://arxiv.org/abs/2506.11047)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Machine learning systems are increasingly deployed in high-stakes domains, yet they remain vulnerable to bias systematic disparities that disproportionately impact specific demographic groups. Traditional bias detection methods often depend on access to sensitive labels or rely on rigid fairness metrics, limiting their applicability in real-world settings. This paper introduces a novel, perception-driven framework for bias detection that leverages crowdsourced human judgment. Inspired by reCAPTCHA and other crowd-powered systems, we present a lightweight web platform that displays stripped-down visualizations of numeric data (for example-salary distributions across demographic clusters) and collects binary judgments on group similarity. We explore how users' visual perception-shaped by layout, spacing, and question phrasing can signal potential disparities. User feedback is aggregated to flag data segments as biased, which are then validated through statistical tests and machine learning cross-evaluations. Our findings show that perceptual signals from non-expert users reliably correlate with known bias cases, suggesting that visual intuition can serve as a powerful, scalable proxy for fairness auditing. This approach offers a label-efficient, interpretable alternative to conventional fairness diagnostics, paving the way toward human-aligned, crowdsourced bias detection pipelines.</li>
</ul>

<h3>Title: I Can't Believe It's Not Real: CV-MuSeNet: Complex-Valued Multi-Signal Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Sangwon Shin, Mehmet C. Vuran</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11048">https://arxiv.org/abs/2506.11048</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11048">https://arxiv.org/pdf/2506.11048</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11048]] I Can't Believe It's Not Real: CV-MuSeNet: Complex-Valued Multi-Signal Segmentation(https://arxiv.org/abs/2506.11048)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The increasing congestion of the radio frequency spectrum presents challenges for efficient spectrum utilization. Cognitive radio systems enable dynamic spectrum access with the aid of recent innovations in neural networks. However, traditional real-valued neural networks (RVNNs) face difficulties in low signal-to-noise ratio (SNR) environments, as they were not specifically developed to capture essential wireless signal properties such as phase and amplitude. This work presents CMuSeNet, a complex-valued multi-signal segmentation network for wideband spectrum sensing, to address these limitations. Extensive hyperparameter analysis shows that a naive conversion of existing RVNNs into their complex-valued counterparts is ineffective. Built on complex-valued neural networks (CVNNs) with a residual architecture, CMuSeNet introduces a complexvalued Fourier spectrum focal loss (CFL) and a complex plane intersection over union (CIoU) similarity metric to enhance training performance. Extensive evaluations on synthetic, indoor overthe-air, and real-world datasets show that CMuSeNet achieves an average accuracy of 98.98%-99.90%, improving by up to 9.2 percentage points over its real-valued counterpart and consistently outperforms state of the art. Strikingly, CMuSeNet achieves the accuracy level of its RVNN counterpart in just two epochs, compared to the 27 epochs required for RVNN, while reducing training time by up to a 92.2% over the state of the art. The results highlight the effectiveness of complex-valued architectures in improving weak signal detection and training efficiency for spectrum sensing in challenging low-SNR environments. The dataset is available at: this https URL</li>
</ul>

<h3>Title: 15,500 Seconds: Lean UAV Classification Leveraging PEFT and Pre-Trained Networks</h3>
<ul>
<li><strong>Authors: </strong>Andrew P. Berg, Qian Zhang, Mia Y. Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11049">https://arxiv.org/abs/2506.11049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11049">https://arxiv.org/pdf/2506.11049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11049]] 15,500 Seconds: Lean UAV Classification Leveraging PEFT and Pre-Trained Networks(https://arxiv.org/abs/2506.11049)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Unmanned Aerial Vehicles (UAVs) pose an escalating security concerns as the market for consumer and military UAVs grows. This paper address the critical data scarcity challenges in deep UAV audio classification. We build upon our previous work expanding novel approaches such as: parameter efficient fine-tuning, data augmentation, and pre-trained networks. We achieve performance upwards of 95\% validation accuracy with EfficientNet-B0.</li>
</ul>

<h3>Title: NSW-EPNews: A News-Augmented Benchmark for Electricity Price Forecasting with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Zhaoge Bi, Linghan Huang, Haolin Jin, Qingwen Zeng, Huaming Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11050">https://arxiv.org/abs/2506.11050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11050">https://arxiv.org/pdf/2506.11050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11050]] NSW-EPNews: A News-Augmented Benchmark for Electricity Price Forecasting with LLMs(https://arxiv.org/abs/2506.11050)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Electricity price forecasting is a critical component of modern energy-management systems, yet existing approaches heavily rely on numerical histories and ignore contemporaneous textual signals. We introduce NSW-EPNews, the first benchmark that jointly evaluates time-series models and large language models (LLMs) on real-world electricity-price prediction. The dataset includes over 175,000 half-hourly spot prices from New South Wales, Australia (2015-2024), daily temperature readings, and curated market-news summaries from WattClarity. We frame the task as 48-step-ahead forecasting, using multimodal input, including lagged prices, vectorized news and weather features for classical models, and prompt-engineered structured contexts for LLMs. Our datasets yields 3.6k multimodal prompt-output pairs for LLM evaluation using specific templates. Through compresive benchmark design, we identify that for traditional statistical and machine learning models, the benefits gain is marginal from news feature. For state-of-the-art LLMs, such as GPT-4o and Gemini 1.5 Pro, we observe modest performance increase while it also produce frequent hallucinations such as fabricated and malformed price sequences. NSW-EPNews provides a rigorous testbed for evaluating grounded numerical reasoning in multimodal settings, and highlights a critical gap between current LLM capabilities and the demands of high-stakes energy forecasting.</li>
</ul>

<h3>Title: ACCORD: Autoregressive Constraint-satisfying Generation for COmbinatorial Optimization with Routing and Dynamic attention</h3>
<ul>
<li><strong>Authors: </strong>Henrik Abgaryan, Tristan Cazenave, Ararat Harutyunyan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11052">https://arxiv.org/abs/2506.11052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11052">https://arxiv.org/pdf/2506.11052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11052]] ACCORD: Autoregressive Constraint-satisfying Generation for COmbinatorial Optimization with Routing and Dynamic attention(https://arxiv.org/abs/2506.11052)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, yet their direct application to NP-hard combinatorial problems (CPs) remains underexplored. In this work, we systematically investigate the reasoning abilities of LLMs on a variety of NP-hard combinatorial optimization tasks and introduce ACCORD: Autoregressive Constraint-satisfying generation for COmbinatorial optimization with Routing and Dynamic attention. ACCORD features a novel dataset representation and model architecture that leverage the autoregressive nature of LLMs to dynamically enforce feasibility constraints, coupled with attention-based routing to activate problem-specific LoRA modules. We also present the ACCORD-90k supervised dataset, covering six NP-hard combinatorial problems: TSP, VRP, Knapsack, FlowShop, JSSP, and BinPacking. Extensive experiments demonstrate that our ACCORD model, built on an 8B-parameter Llama backbone, consistently outperforms standard prompting and input-output methods, even when compared to much larger LLMs, such as gpt-4. Ablation studies further show that our output structure enhances solution feasibility. To the best of our knowledge, this is the first large-scale, end-to-end framework for exploring the applications of LLMs to a broad spectrum of combinatorial optimization problems. The codes are publicly available at this https URL</li>
</ul>

<h3>Title: PolyMicros: Bootstrapping a Foundation Model for Polycrystalline Material Structure</h3>
<ul>
<li><strong>Authors: </strong>Michael Buzzy, Andreas Robertson, Peng Chen, Surya Kalidindi</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11055">https://arxiv.org/abs/2506.11055</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11055">https://arxiv.org/pdf/2506.11055</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11055]] PolyMicros: Bootstrapping a Foundation Model for Polycrystalline Material Structure(https://arxiv.org/abs/2506.11055)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in Foundation Models for Materials Science are poised to revolutionize the discovery, manufacture, and design of novel materials with tailored properties and responses. Although great strides have been made, successes have been restricted to materials classes where multi-million sample data repositories can be readily curated (e.g., atomistic structures). Unfortunately, for many structural and functional materials (e.g., mesoscale structured metal alloys), such datasets are too costly or prohibitive to construct; instead, datasets are limited to very few examples. To address this challenge, we introduce a novel machine learning approach for learning from hyper-sparse, complex spatial data in scientific domains. Our core contribution is a physics-driven data augmentation scheme that leverages an ensemble of local generative models, trained on as few as five experimental observations, and coordinates them through a novel diversity curation strategy to generate a large-scale, physically diverse dataset. We utilize this framework to construct PolyMicros, the first Foundation Model for polycrystalline materials (a structural material class important across a broad range of industrial and scientific applications). We demonstrate the utility of PolyMicros by zero-shot solving several long standing challenges related to accelerating 3D experimental microscopy. Finally, we make both our models and datasets openly available to the community.</li>
</ul>

<h3>Title: xInv: Explainable Optimization of Inverse Problems</h3>
<ul>
<li><strong>Authors: </strong>Sean Memery, Kevin Denamganai, Anna Kapron-King, Kartic Subr</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11056">https://arxiv.org/abs/2506.11056</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11056">https://arxiv.org/pdf/2506.11056</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11056]] xInv: Explainable Optimization of Inverse Problems(https://arxiv.org/abs/2506.11056)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Inverse problems are central to a wide range of fields, including healthcare, climate science, and agriculture. They involve the estimation of inputs, typically via iterative optimization, to some known forward model so that it produces a desired outcome. Despite considerable development in the explainability and interpretability of forward models, the iterative optimization of inverse problems remains largely cryptic to domain experts. We propose a methodology to produce explanations, from traces produced by an optimizer, that are interpretable by humans at the abstraction of the domain. The central idea in our approach is to instrument a differentiable simulator so that it emits natural language events during its forward and backward passes. In a post-process, we use a Language Model to create an explanation from the list of events. We demonstrate the effectiveness of our approach with an illustrative optimization problem and an example involving the training of a neural network.</li>
</ul>

<h3>Title: STRCMP: Integrating Graph Structural Priors with Language Models for Combinatorial Optimization</h3>
<ul>
<li><strong>Authors: </strong>Xijun Li, Jiexiang Yang, Jinghao Wang, Bo Peng, Jianguo Yao, Haibing Guan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11057">https://arxiv.org/abs/2506.11057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11057">https://arxiv.org/pdf/2506.11057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11057]] STRCMP: Integrating Graph Structural Priors with Language Models for Combinatorial Optimization(https://arxiv.org/abs/2506.11057)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Combinatorial optimization (CO) problems, central to operation research and theoretical computer science, present significant computational challenges due to their NP-hard nature. While large language models (LLMs) have emerged as promising tools for CO--either by directly generating solutions or synthesizing solver-specific codes--existing approaches often neglect critical structural priors inherent to CO problems, leading to suboptimality and iterative inefficiency. Inspired by human experts' success in leveraging CO structures for algorithm design, we propose STRCMP, a novel structure-aware LLM-based algorithm discovery framework that systematically integrates structure priors to enhance solution quality and solving efficiency. Our framework combines a graph neural network (GNN) for extracting structural embeddings from CO instances with an LLM conditioned on these embeddings to identify high-performing algorithms in the form of solver-specific codes. This composite architecture ensures syntactic correctness, preserves problem topology, and aligns with natural language objectives, while an evolutionary refinement process iteratively optimizes generated algorithm. Extensive evaluations across Mixed Integer Linear Programming and Boolean Satisfiability problems, using nine benchmark datasets, demonstrate that our proposed STRCMP outperforms five strong neural and LLM-based methods by a large margin, in terms of both solution optimality and computational efficiency. The code and learned model will be publicly available upon the acceptance of the paper.</li>
</ul>

<h3>Title: Who is in the Spotlight: The Hidden Bias Undermining Multimodal Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Jiayu Yao, Shenghua Liu, Yiwei Wang, Lingrui Mei, Baolong Bi, Yuyao Ge, Zhecheng Li, Xueqi Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11063">https://arxiv.org/abs/2506.11063</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11063">https://arxiv.org/pdf/2506.11063</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11063]] Who is in the Spotlight: The Hidden Bias Undermining Multimodal Retrieval-Augmented Generation(https://arxiv.org/abs/2506.11063)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Multimodal Retrieval-Augmented Generation (RAG) systems have become essential in knowledge-intensive and open-domain tasks. As retrieval complexity increases, ensuring the robustness of these systems is critical. However, current RAG models are highly sensitive to the order in which evidence is presented, often resulting in unstable performance and biased reasoning, particularly as the number of retrieved items or modality diversity grows. This raises a central question: How does the position of retrieved evidence affect multimodal RAG performance? To answer this, we present the first comprehensive study of position bias in multimodal RAG systems. Through controlled experiments across text-only, image-only, and mixed-modality tasks, we observe a consistent U-shaped accuracy curve with respect to evidence position. To quantify this bias, we introduce the Position Sensitivity Index ($PSI_p$) and develop a visualization framework to trace attention allocation patterns across decoder layers. Our results reveal that multimodal interactions intensify position bias compared to unimodal settings, and that this bias increases logarithmically with retrieval range. These findings offer both theoretical and empirical foundations for position-aware analysis in RAG, highlighting the need for evidence reordering or debiasing strategies to build more reliable and equitable generation systems.</li>
</ul>

<h3>Title: Smotrom tvoja pa ander drogoj verden! Resurrecting Dead Pidgin with Generative Models: Russenorsk Case Study</h3>
<ul>
<li><strong>Authors: </strong>Alexey Tikhonov, Sergei Shteiner, Anna Bykova, Ivan P. Yamshchikov</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11065">https://arxiv.org/abs/2506.11065</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11065">https://arxiv.org/pdf/2506.11065</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11065]] Smotrom tvoja pa ander drogoj verden! Resurrecting Dead Pidgin with Generative Models: Russenorsk Case Study(https://arxiv.org/abs/2506.11065)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Russenorsk, a pidgin language historically used in trade interactions between Russian and Norwegian speakers, represents a unique linguistic phenomenon. In this paper, we attempt to analyze its lexicon using modern large language models (LLMs), based on surviving literary sources. We construct a structured dictionary of the language, grouped by synonyms and word origins. Subsequently, we use this dictionary to formulate hypotheses about the core principles of word formation and grammatical structure in Russenorsk and show which hypotheses generated by large language models correspond to the hypotheses previously proposed ones in the academic literature. We also develop a "reconstruction" translation agent that generates hypothetical Russenorsk renderings of contemporary Russian and Norwegian texts.</li>
</ul>

<h3>Title: A Large Language Model Based Pipeline for Review of Systems Entity Recognition from Clinical Notes</h3>
<ul>
<li><strong>Authors: </strong>Hieu Nghiem, Hemanth Reddy Singareddy, Zhuqi Miao, Jivan Lamichhane, Abdulaziz Ahmed, Johnson Thomas, Dursun Delen, William Paiva</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11067">https://arxiv.org/abs/2506.11067</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11067">https://arxiv.org/pdf/2506.11067</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11067]] A Large Language Model Based Pipeline for Review of Systems Entity Recognition from Clinical Notes(https://arxiv.org/abs/2506.11067)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Objective: Develop a cost-effective, large language model (LLM)-based pipeline for automatically extracting Review of Systems (ROS) entities from clinical notes. Materials and Methods: The pipeline extracts ROS sections using SecTag, followed by few-shot LLMs to identify ROS entity spans, their positive/negative status, and associated body systems. We implemented the pipeline using open-source LLMs (Mistral, Llama, Gemma) and ChatGPT. The evaluation was conducted on 36 general medicine notes containing 341 annotated ROS entities. Results: When integrating ChatGPT, the pipeline achieved the lowest error rates in detecting ROS entity spans and their corresponding statuses/systems (28.2% and 14.5%, respectively). Open-source LLMs enable local, cost-efficient execution of the pipeline while delivering promising performance with similarly low error rates (span: 30.5-36.7%; status/system: 24.3-27.3%). Discussion and Conclusion: Our pipeline offers a scalable and locally deployable solution to reduce ROS documentation burden. Open-source LLMs present a viable alternative to commercial models in resource-limited healthcare environments.</li>
</ul>

<h3>Title: Deontological Keyword Bias: The Impact of Modal Expressions on Normative Judgments of Language Models</h3>
<ul>
<li><strong>Authors: </strong>Bumjin Park, Jinsil Lee, Jaesik Choi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11068">https://arxiv.org/abs/2506.11068</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11068">https://arxiv.org/pdf/2506.11068</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11068]] Deontological Keyword Bias: The Impact of Modal Expressions on Normative Judgments of Language Models(https://arxiv.org/abs/2506.11068)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly engaging in moral and ethical reasoning, where criteria for judgment are often unclear, even for humans. While LLM alignment studies cover many areas, one important yet underexplored area is how LLMs make judgments about obligations. This work reveals a strong tendency in LLMs to judge non-obligatory contexts as obligations when prompts are augmented with modal expressions such as must or ought to. We introduce this phenomenon as Deontological Keyword Bias (DKB). We find that LLMs judge over 90\% of commonsense scenarios as obligations when modal expressions are present. This tendency is consist across various LLM families, question types, and answer formats. To mitigate DKB, we propose a judgment strategy that integrates few-shot examples with reasoning prompts. This study sheds light on how modal expressions, as a form of linguistic framing, influence the normative decisions of LLMs and underscores the importance of addressing such biases to ensure judgment alignment.</li>
</ul>

<h3>Title: Targeted control of fast prototyping through domain-specific interface</h3>
<ul>
<li><strong>Authors: </strong>Yu-Zhe Shi, Mingchen Liu, Hanlu Ma, Qiao Xu, Huamin Qu, Kun He, Lecheng Ruan, Qining Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11070">https://arxiv.org/abs/2506.11070</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11070">https://arxiv.org/pdf/2506.11070</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11070]] Targeted control of fast prototyping through domain-specific interface(https://arxiv.org/abs/2506.11070)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Industrial designers have long sought a natural and intuitive way to achieve the targeted control of prototype models -- using simple natural language instructions to configure and adjust the models seamlessly according to their intentions, without relying on complex modeling commands. While Large Language Models have shown promise in this area, their potential for controlling prototype models through language remains partially underutilized. This limitation stems from gaps between designers' languages and modeling languages, including mismatch in abstraction levels, fluctuation in semantic precision, and divergence in lexical scopes. To bridge these gaps, we propose an interface architecture that serves as a medium between the two languages. Grounded in design principles derived from a systematic investigation of fast prototyping practices, we devise the interface's operational mechanism and develop an algorithm for its automated domain specification. Both machine-based evaluations and human studies on fast prototyping across various product design domains demonstrate the interface's potential to function as an auxiliary module for Large Language Models, enabling precise and effective targeted control of prototype models.</li>
</ul>

<h3>Title: RoE-FND: A Case-Based Reasoning Approach with Dual Verification for Fake News Detection via LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yuzhou Yang, Yangming Zhou, Zhiying Zhu, Zhenxing Qian, Xinpeng Zhang, Sheng Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11078">https://arxiv.org/abs/2506.11078</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11078">https://arxiv.org/pdf/2506.11078</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11078]] RoE-FND: A Case-Based Reasoning Approach with Dual Verification for Fake News Detection via LLMs(https://arxiv.org/abs/2506.11078)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The proliferation of deceptive content online necessitates robust Fake News Detection (FND) systems. While evidence-based approaches leverage external knowledge to verify claims, existing methods face critical limitations: noisy evidence selection, generalization bottlenecks, and unclear decision-making processes. Recent efforts to harness Large Language Models (LLMs) for FND introduce new challenges, including hallucinated rationales and conclusion bias. To address these issues, we propose \textbf{RoE-FND} (\textbf{\underline{R}}eason \textbf{\underline{o}}n \textbf{\underline{E}}xperiences FND), a framework that reframes evidence-based FND as a logical deduction task by synergizing LLMs with experiential learning. RoE-FND encompasses two stages: (1) \textit{self-reflective knowledge building}, where a knowledge base is curated by analyzing past reasoning errors, namely the exploration stage, and (2) \textit{dynamic criterion retrieval}, which synthesizes task-specific reasoning guidelines from historical cases as experiences during deployment. It further cross-checks rationales against internal experience through a devised dual-channel procedure. Key contributions include: a case-based reasoning framework for FND that addresses multiple existing challenges, a training-free approach enabling adaptation to evolving situations, and empirical validation of the framework's superior generalization and effectiveness over state-of-the-art methods across three datasets.</li>
</ul>

<h3>Title: MANBench: Is Your Multimodal Model Smarter than Human?</h3>
<ul>
<li><strong>Authors: </strong>Han Zhou, Qitong Xu, Yiheng Dong, Xin Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11080">https://arxiv.org/abs/2506.11080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11080">https://arxiv.org/pdf/2506.11080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11080]] MANBench: Is Your Multimodal Model Smarter than Human?(https://arxiv.org/abs/2506.11080)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancement of Multimodal Large Language Models (MLLMs) has ignited discussions regarding their potential to surpass human performance in multimodal tasks. In response, we introduce MANBench (Multimodal Ability Norms Benchmark), a bilingual benchmark (English and Chinese) comprising 1,314 questions across nine tasks, spanning knowledge-based and non-knowledge-based domains. MANBench emphasizes intuitive reasoning, seamless cross-modal integration, and real-world complexity, providing a rigorous evaluation framework. Through extensive human experiments involving diverse participants, we compared human performance against state-of-the-art MLLMs. The results indicate that while MLLMs excel in tasks like Knowledge and Text-Image Understanding, they struggle with deeper cross-modal reasoning tasks such as Transmorphic Understanding, Image Consistency, and Multi-image Understanding. Moreover, both humans and MLLMs face challenges in highly complex tasks like Puzzles and Spatial Imagination. MANBench highlights the strengths and limitations of MLLMs, revealing that even advanced models fall short of achieving human-level performance across many domains. We hope MANBench will inspire efforts to bridge the gap between MLLMs and human multimodal capabilities. The code and dataset are available at this https URL.</li>
</ul>

<h3>Title: SAGE:Specification-Aware Grammar Extraction for Automated Test Case Generation with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Aditi, Hyunwoo Park, Sicheol Sung, Yo-Sub Han, Sang-Ki Ko</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11081">https://arxiv.org/abs/2506.11081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11081">https://arxiv.org/pdf/2506.11081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11081]] SAGE:Specification-Aware Grammar Extraction for Automated Test Case Generation with LLMs(https://arxiv.org/abs/2506.11081)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Grammar-based test case generation has proven effective for competitive programming problems, but generating valid and general grammars from natural language specifications remains a key challenge, especially under limited supervision. Context-Free Grammars with Counters (CCFGs) have recently been introduced as a formalism to represent such specifications with logical constraints by storing and reusing counter values during derivation. In this work, we explore the use of open-source large language models (LLMs) to induce CCFGs from specifications using a small number of labeled examples and verifiable reward-guided reinforcement learning. Our approach first fine-tunes an open-source LLM to perform specification-to-grammar translation, and further applies Group Relative Policy Optimization (GRPO) to enhance grammar validity and generality. We also examine the effectiveness of iterative feedback for open and closed-source LLMs in correcting syntactic and semantic errors in generated grammars. Experimental results show that our approach SAGE achieves stronger generalization and outperforms 17 open and closed-source LLMs in both grammar quality and test effectiveness, improving over the state-of-the-art by 15.92%p in grammar validity and 12.34%p in test effectiveness. We provide our implementation and dataset at the following anonymous repository:this https URL</li>
</ul>

<h3>Title: PRISM: A Transformer-based Language Model of Structured Clinical Event Data</h3>
<ul>
<li><strong>Authors: </strong>Lionel Levine, John Santerre, Alex S. Young, T. Barry Levine, Francis Campion, Majid Sarrafzadeh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11082">https://arxiv.org/abs/2506.11082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11082">https://arxiv.org/pdf/2506.11082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11082]] PRISM: A Transformer-based Language Model of Structured Clinical Event Data(https://arxiv.org/abs/2506.11082)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>We introduce PRISM (Predictive Reasoning in Sequential Medicine), a transformer-based architecture designed to model the sequential progression of clinical decision-making processes. Unlike traditional approaches that rely on isolated diagnostic classification, PRISM frames clinical trajectories as tokenized sequences of events - including diagnostic tests, laboratory results, and diagnoses - and learns to predict the most probable next steps in the patient diagnostic journey. Leveraging a large custom clinical vocabulary and an autoregressive training objective, PRISM demonstrates the ability to capture complex dependencies across longitudinal patient timelines. Experimental results show substantial improvements over random baselines in next-token prediction tasks, with generated sequences reflecting realistic diagnostic pathways, laboratory result progressions, and clinician ordering behaviors. These findings highlight the feasibility of applying generative language modeling techniques to structured medical event data, enabling applications in clinical decision support, simulation, and education. PRISM establishes a foundation for future advancements in sequence-based healthcare modeling, bridging the gap between machine learning architectures and real-world diagnostic reasoning.</li>
</ul>

<h3>Title: RedDebate: Safer Responses through Multi-Agent Red Teaming Debates</h3>
<ul>
<li><strong>Authors: </strong>Ali Asad, Stephen Obadinma, Radin Shayanfar, Xiaodan Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11083">https://arxiv.org/abs/2506.11083</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11083">https://arxiv.org/pdf/2506.11083</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11083]] RedDebate: Safer Responses through Multi-Agent Red Teaming Debates(https://arxiv.org/abs/2506.11083)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We propose RedDebate, a novel multi-agent debate framework that leverages adversarial argumentation among Large Language Models (LLMs) to proactively identify and mitigate their own unsafe behaviours. Existing AI safety methods often depend heavily on costly human evaluations or isolated single-model assessment, both subject to scalability constraints and oversight risks. RedDebate instead embraces collaborative disagreement, enabling multiple LLMs to critically examine one another's reasoning, and systematically uncovering unsafe blind spots through automated red-teaming, and iteratively improve their responses. We further integrate distinct types of long-term memory that retain learned safety insights from debate interactions. Evaluating on established safety benchmarks such as HarmBench, we demonstrate the proposed method's effectiveness. Debate alone can reduce unsafe behaviours by 17.7%, and when combined with long-term memory modules, achieves reductions exceeding 23.5%. To our knowledge, RedDebate constitutes the first fully automated framework that combines multi-agent debates with red-teaming to progressively enhance AI safety without direct human intervention.(Github Repository: this https URL)</li>
</ul>

<h3>Title: ADAMIX: Adaptive Mixed-Precision Delta-Compression with Quantization Error Optimization for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Boya Xiong, Shuo Wang, Weifeng Ge, Guanhua Chen, Yun Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11087">https://arxiv.org/abs/2506.11087</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11087">https://arxiv.org/pdf/2506.11087</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11087]] ADAMIX: Adaptive Mixed-Precision Delta-Compression with Quantization Error Optimization for Large Language Models(https://arxiv.org/abs/2506.11087)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) achieve impressive performance on various knowledge-intensive and complex reasoning tasks in different domains. In certain scenarios like multi-tenant serving, a large number of LLMs finetuned from the same base model are deployed to meet complex requirements for users. Recent works explore delta-compression approaches to quantize and compress the delta parameters between the customized LLM and the corresponding base model. However, existing works either exhibit unsatisfactory performance at high compression ratios or depend on empirical bit allocation schemes. In this work, we propose ADAMIX, an effective adaptive mixed-precision delta-compression framework. We provide a mathematical derivation of quantization error to motivate our mixed-precision compression strategy and formulate the optimal mixed-precision bit allocation scheme as the solution to a 0/1 integer linear programming problem. Our derived bit allocation strategy minimizes the quantization error while adhering to a predefined compression ratio requirement. Experimental results on various models and benchmarks demonstrate that our approach surpasses the best baseline by a considerable margin. On tasks like AIME2024 and GQA, where the norm of $\Delta \mathbf{W}$ is large and the base model lacks sufficient ability, ADAMIX outperforms the best baseline Delta-CoMe by 22.3% and 6.1% with 7B models, respectively.</li>
</ul>

<h3>Title: Customizing Speech Recognition Model with Large Language Model Feedback</h3>
<ul>
<li><strong>Authors: </strong>Shaoshi Ling, Guoli Ye</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11091">https://arxiv.org/abs/2506.11091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11091">https://arxiv.org/pdf/2506.11091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11091]] Customizing Speech Recognition Model with Large Language Model Feedback(https://arxiv.org/abs/2506.11091)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Automatic speech recognition (ASR) systems have achieved strong performance on general transcription tasks. However, they continue to struggle with recognizing rare named entities and adapting to domain mismatches. In contrast, large language models (LLMs), trained on massive internet-scale datasets, are often more effective across a wide range of domains. In this work, we propose a reinforcement learning based approach for unsupervised domain adaptation, leveraging unlabeled data to enhance transcription quality, particularly the named entities affected by domain mismatch, through feedback from a LLM. Given contextual information, our framework employs a LLM as the reward model to score the hypotheses from the ASR model. These scores serve as reward signals to fine-tune the ASR model via reinforcement learning. Our method achieves a 21\% improvement on entity word error rate over conventional self-training methods.</li>
</ul>

<h3>Title: Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing Multi-Turn Planning and Tool Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Jubin Abhishek Soni, Amit Anand, Rajesh Kumar Pandey, Aniket Abhishek Soni</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11092">https://arxiv.org/abs/2506.11092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11092">https://arxiv.org/pdf/2506.11092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11092]] Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing Multi-Turn Planning and Tool Adaptation(https://arxiv.org/abs/2506.11092)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) has significantly advanced large language models (LLMs) by grounding their outputs in external tools and knowledge sources. However, existing RAG systems are typically constrained to static, single-turn interactions with fixed toolsets, making them ill-suited for dynamic domains such as healthcare and smart homes, where user intent, available tools, and contextual factors evolve over time. We present Dynamic Context Tuning (DCT), a lightweight framework that extends RAG to support multi-turn dialogue and evolving tool environments without requiring retraining. DCT integrates an attention-based context cache to track relevant past information, LoRA-based retrieval to dynamically select domain-specific tools, and efficient context compression to maintain inputs within LLM context limits. Experiments on both synthetic and real-world benchmarks show that DCT improves plan accuracy by 14% and reduces hallucinations by 37%, while matching GPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to previously unseen tools, enabling scalable and adaptable AI assistants across a wide range of dynamic environments.</li>
</ul>

<h3>Title: EfficientQuant: An Efficient Post-Training Quantization for CNN-Transformer Hybrid Models on Edge Devices</h3>
<ul>
<li><strong>Authors: </strong>Shaibal Saha, Lanyu Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11093">https://arxiv.org/abs/2506.11093</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11093">https://arxiv.org/pdf/2506.11093</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11093]] EfficientQuant: An Efficient Post-Training Quantization for CNN-Transformer Hybrid Models on Edge Devices(https://arxiv.org/abs/2506.11093)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Hybrid models that combine convolutional and transformer blocks offer strong performance in computer vision (CV) tasks but are resource-intensive for edge deployment. Although post-training quantization (PTQ) can help reduce resource demand, its application to hybrid models remains limited. We propose EfficientQuant, a novel structure-aware PTQ approach that applies uniform quantization to convolutional blocks and $log_2$ quantization to transformer blocks. EfficientQuant achieves $2.5 \times - 8.7 \times$ latency reduction with minimal accuracy loss on the ImageNet-1K dataset. It further demonstrates low latency and memory efficiency on edge devices, making it practical for real-world deployment.</li>
</ul>

<h3>Title: The Scales of Justitia: A Comprehensive Survey on Safety Evaluation of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Songyang Liu, Chaozhuo Li, Jiameng Qiu, Xi Zhang, Feiran Huang, Litian Zhang, Yiming Hei, Philip S. Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11094">https://arxiv.org/abs/2506.11094</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11094">https://arxiv.org/pdf/2506.11094</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11094]] The Scales of Justitia: A Comprehensive Survey on Safety Evaluation of LLMs(https://arxiv.org/abs/2506.11094)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, large language model</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of artificial intelligence technology, Large Language Models (LLMs) have demonstrated remarkable potential in the field of Natural Language Processing (NLP), including areas such as content generation, human-computer interaction, machine translation, and code generation, among others. However, their widespread deployment has also raised significant safety concerns. In recent years, LLM-generated content has occasionally exhibited unsafe elements like toxicity and bias, particularly in adversarial scenarios, which has garnered extensive attention from both academia and industry. While numerous efforts have been made to evaluate the safety risks associated with LLMs, there remains a lack of systematic reviews summarizing these research endeavors. This survey aims to provide a comprehensive and systematic overview of recent advancements in LLMs safety evaluation, focusing on several key aspects: (1) "Why evaluate" that explores the background of LLMs safety evaluation, how they differ from general LLMs evaluation, and the significance of such evaluation; (2) "What to evaluate" that examines and categorizes existing safety evaluation tasks based on key capabilities, including dimensions such as toxicity, robustness, ethics, bias and fairness, truthfulness, and so on; (3) "Where to evaluate" that summarizes the evaluation metrics, datasets and benchmarks currently used in safety evaluations; (4) "How to evaluate" that reviews existing evaluation toolkit, and categorizing mainstream evaluation methods based on the roles of the evaluators. Finally, we identify the challenges in LLMs safety evaluation and propose potential research directions to promote further advancement in this field. We emphasize the importance of prioritizing LLMs safety evaluation to ensure the safe deployment of these models in real-world applications.</li>
</ul>

<h3>Title: C-SEO Bench: Does Conversational SEO Work?</h3>
<ul>
<li><strong>Authors: </strong>Haritz Puerto, Martin Gubri, Tommaso Green, Seong Joon Oh, Sangdoo Yun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11097">https://arxiv.org/abs/2506.11097</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11097">https://arxiv.org/pdf/2506.11097</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11097]] C-SEO Bench: Does Conversational SEO Work?(https://arxiv.org/abs/2506.11097)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are transforming search engines into Conversational Search Engines (CSE). Consequently, Search Engine Optimization (SEO) is being shifted into Conversational Search Engine Optimization (C-SEO). We are beginning to see dedicated C-SEO methods for modifying web documents to increase their visibility in CSE responses. However, they are often tested only for a limited breadth of application domains; we do not understand whether certain C-SEO methods would be effective for a broad range of domains. Moreover, existing evaluations consider only a single-actor scenario where only one web document adopts a C-SEO method; in reality, multiple players are likely to competitively adopt the cutting-edge C-SEO techniques, drawing an analogy from the dynamics we have seen in SEO. We present C-SEO Bench, the first benchmark designed to evaluate C-SEO methods across multiple tasks, domains, and number of actors. We consider two search tasks, question answering and product recommendation, with three domains each. We also formalize a new evaluation protocol with varying adoption rates among involved actors. Our experiments reveal that most current C-SEO methods are largely ineffective, contrary to reported results in the literature. Instead, traditional SEO strategies, those aiming to improve the ranking of the source in the LLM context, are significantly more effective. We also observe that as we increase the number of C-SEO adopters, the overall gains decrease, depicting a congested and zero-sum nature of the problem. Our code and data are available at this https URL and this https URL.</li>
</ul>

<h3>Title: Debiasing Online Preference Learning via Preference Feature Preservation</h3>
<ul>
<li><strong>Authors: </strong>Dongyoung Kim, Jinsung Yoon, Jinwoo Shin, Jaehyung Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11098">https://arxiv.org/abs/2506.11098</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11098">https://arxiv.org/pdf/2506.11098</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11098]] Debiasing Online Preference Learning via Preference Feature Preservation(https://arxiv.org/abs/2506.11098)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent preference learning frameworks for large language models (LLMs) simplify human preferences with binary pairwise comparisons and scalar rewards. This simplification could make LLMs' responses biased to mostly preferred features, and would be exacerbated during the iterations of online preference learning steps. To address these challenges, we propose a novel framework coined PFP (Preference Feature Preservation). The key idea of PFP is maintaining the distribution of human preference features and utilizing such rich signals throughout the online preference learning process. Specifically, PFP first extract preference features from offline pairwise human preference data and trains a feature classifier. Then, using trained classifier and the distribution preserving optimization, PFP maps appropriate preference features for a new input instruction during online learning. Lastly, PFP trains LLM using the existing preference learning method, by incorporating the preference feature into system prompts and enabling LLM to explicitly handle various human preferences. Our experiments demonstrate that PFP successfully mitigates the bias in preference features during online learning, and hence achieves superior performance compared to previous preference learning methods on standard benchmarks to evaluate LLM alignment.</li>
</ul>

<h3>Title: Evolutionary Perspectives on the Evaluation of LLM-Based AI Agents: A Comprehensive Survey</h3>
<ul>
<li><strong>Authors: </strong>Jiachen Zhu, Menghui Zhu, Renting Rui, Rong Shan, Congmin Zheng, Bo Chen, Yunjia Xi, Jianghao Lin, Weiwen Liu, Ruiming Tang, Yong Yu, Weinan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11102">https://arxiv.org/abs/2506.11102</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11102">https://arxiv.org/pdf/2506.11102</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11102]] Evolutionary Perspectives on the Evaluation of LLM-Based AI Agents: A Comprehensive Survey(https://arxiv.org/abs/2506.11102)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The advent of large language models (LLMs), such as GPT, Gemini, and DeepSeek, has significantly advanced natural language processing, giving rise to sophisticated chatbots capable of diverse language-related tasks. The transition from these traditional LLM chatbots to more advanced AI agents represents a pivotal evolutionary step. However, existing evaluation frameworks often blur the distinctions between LLM chatbots and AI agents, leading to confusion among researchers selecting appropriate benchmarks. To bridge this gap, this paper introduces a systematic analysis of current evaluation approaches, grounded in an evolutionary perspective. We provide a detailed analytical framework that clearly differentiates AI agents from LLM chatbots along five key aspects: complex environment, multi-source instructor, dynamic feedback, multi-modal perception, and advanced capability. Further, we categorize existing evaluation benchmarks based on external environments driving forces, and resulting advanced internal capabilities. For each category, we delineate relevant evaluation attributes, presented comprehensively in practical reference tables. Finally, we synthesize current trends and outline future evaluation methodologies through four critical lenses: environment, agent, evaluator, and metrics. Our findings offer actionable guidance for researchers, facilitating the informed selection and application of benchmarks in AI agent evaluation, thus fostering continued advancement in this rapidly evolving research domain.</li>
</ul>

<h3>Title: You Only Fine-tune Once: Many-Shot In-Context Fine-Tuning for Large Language Model</h3>
<ul>
<li><strong>Authors: </strong>Wenchong He, Liqian Peng, Zhe Jiang, Alex Go</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11103">https://arxiv.org/abs/2506.11103</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11103">https://arxiv.org/pdf/2506.11103</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11103]] You Only Fine-tune Once: Many-Shot In-Context Fine-Tuning for Large Language Model(https://arxiv.org/abs/2506.11103)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) possess a remarkable ability to perform in-context learning (ICL), which enables them to handle multiple downstream tasks simultaneously without requiring task-specific fine-tuning. Recent studies have shown that even moderately sized LLMs, such as Mistral 7B, Gemma 7B and Llama-3 8B, can achieve ICL through few-shot in-context fine-tuning of all tasks at once. However, this approach still lags behind dedicated fine-tuning, where a separate model is trained for each individual task. In this paper, we propose a novel approach, Many-Shot In-Context Fine-tuning (ManyICL), which significantly narrows this performance gap by extending the principles of ICL to a many-shot setting. To unlock the full potential of ManyICL and address the inherent inefficiency of processing long sequences with numerous in-context examples, we propose a novel training objective. Instead of solely predicting the final answer, our approach treats every answer within the context as a supervised training target. This effectively shifts the role of many-shot examples from prompts to targets for autoregressive learning. Through extensive experiments on diverse downstream tasks, including classification, summarization, question answering, natural language inference, and math, we demonstrate that ManyICL substantially outperforms zero/few-shot fine-tuning and approaches the performance of dedicated fine-tuning. Furthermore, ManyICL significantly mitigates catastrophic forgetting issues observed in zero/few-shot fine-tuning. The code will be made publicly available upon publication.</li>
</ul>

<h3>Title: DAM: Dynamic Attention Mask for Long-Context Large Language Model Inference Acceleration</h3>
<ul>
<li><strong>Authors: </strong>Hanzhi Zhang, Heng Fan, Kewei Sha, Yan Huang, Yunhe Feng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11104">https://arxiv.org/abs/2506.11104</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11104">https://arxiv.org/pdf/2506.11104</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11104]] DAM: Dynamic Attention Mask for Long-Context Large Language Model Inference Acceleration(https://arxiv.org/abs/2506.11104)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Long-context understanding is crucial for many NLP applications, yet transformers struggle with efficiency due to the quadratic complexity of self-attention. Sparse attention methods alleviate this cost but often impose static, predefined masks, failing to capture heterogeneous attention patterns. This results in suboptimal token interactions, limiting adaptability and retrieval accuracy in long-sequence tasks. This work introduces a dynamic sparse attention mechanism that assigns adaptive masks at the attention-map level, preserving heterogeneous patterns across layers and heads. Unlike existing approaches, our method eliminates the need for fine-tuning and predefined mask structures while maintaining computational efficiency. By learning context-aware attention structures, it achieves high alignment with full-attention models, ensuring minimal performance degradation while reducing memory and compute overhead. This approach provides a scalable alternative to full attention, enabling the practical deployment of large-scale Large Language Models (LLMs) without sacrificing retrieval performance. DAM is available at: this https URL.</li>
</ul>

<h3>Title: Enabling On-Device Medical AI Assistants via Input-Driven Saliency Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Uttej Kallakurik, Edward Humes, Rithvik Jonna, Xiaomin Lin, Tinoosh Mohsenin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.AR, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11105">https://arxiv.org/abs/2506.11105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11105">https://arxiv.org/pdf/2506.11105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11105]] Enabling On-Device Medical AI Assistants via Input-Driven Saliency Adaptation(https://arxiv.org/abs/2506.11105)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have significant impact on the healthcare scenarios but remain prohibitively large for deployment in real-time, resource-constrained environments such as edge devices. In this work, we introduce a novel medical assistant system, optimized through our general-purpose compression framework, which tailors Large Language Models (LLMs) for deployment in specialized domains. By measuring neuron saliency on domain-specific data, our method can aggressively prune irrelevant neurons, reducing model size while preserving performance. Following pruning, we apply post-training quantization to further reduce the memory footprint, and evaluate the compressed model across medical benchmarks including MedMCQA, MedQA, and PubMedQA. We also deploy the 50\% compressed Gemma and the 67\% compressed LLaMA3 models on Jetson Orin Nano (18.7W peak) and Raspberry Pi 5 (6.3W peak), achieving real-time, energy-efficient inference under hardware constraints.</li>
</ul>

<h3>Title: Graph-based RAG Enhancement via Global Query Disambiguation and Dependency-Aware Reranking</h3>
<ul>
<li><strong>Authors: </strong>Ningyuan Li, Junrui Liu, Yi Shan, Minghui Huang, Tong Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11106">https://arxiv.org/abs/2506.11106</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11106">https://arxiv.org/pdf/2506.11106</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11106]] Graph-based RAG Enhancement via Global Query Disambiguation and Dependency-Aware Reranking(https://arxiv.org/abs/2506.11106)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, large language model</a></li>
<li><strong>Abstract: </strong>Contemporary graph-based retrieval-augmented generation (RAG) methods typically begin by extracting entities from user queries and then leverage pre-constructed knowledge graphs to retrieve related relationships and metadata. However, this pipeline's exclusive reliance on entity-level extraction can lead to the misinterpretation or omission of latent yet critical information and relations. As a result, retrieved content may be irrelevant or contradictory, and essential knowledge may be excluded, exacerbating hallucination risks and degrading the fidelity of generated responses. To address these limitations, we introduce PankRAG, a framework that combines a globally aware, hierarchical query-resolution strategy with a novel dependency-aware reranking mechanism. PankRAG first constructs a multi-level resolution path that captures both parallel and sequential interdependencies within a query, guiding large language models (LLMs) through structured reasoning. It then applies its dependency-aware reranker to exploit the dependency structure among resolved sub-questions, enriching and validating retrieval results for subsequent sub-questions. Empirical evaluations demonstrate that PankRAG consistently outperforms state-of-the-art approaches across multiple benchmarks, underscoring its robustness and generalizability.</li>
</ul>

<h3>Title: Enhancing Large Language Models for Mobility Analytics with Semantic Location Tokenization</h3>
<ul>
<li><strong>Authors: </strong>Yile Chen, Yicheng Tao, Yue Jiang, Shuai Liu, Han Yu, Gao Cong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11109">https://arxiv.org/abs/2506.11109</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11109">https://arxiv.org/pdf/2506.11109</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11109]] Enhancing Large Language Models for Mobility Analytics with Semantic Location Tokenization(https://arxiv.org/abs/2506.11109)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The widespread adoption of location-based services has led to the generation of vast amounts of mobility data, providing significant opportunities to model user movement dynamics within urban environments. Recent advancements have focused on adapting Large Language Models (LLMs) for mobility analytics. However, existing methods face two primary limitations: inadequate semantic representation of locations (i.e., discrete IDs) and insufficient modeling of mobility signals within LLMs (i.e., single templated instruction fine-tuning). To address these issues, we propose QT-Mob, a novel framework that significantly enhances LLMs for mobility analytics. QT-Mob introduces a location tokenization module that learns compact, semantically rich tokens to represent locations, preserving contextual information while ensuring compatibility with LLMs. Furthermore, QT-Mob incorporates a series of complementary fine-tuning objectives that align the learned tokens with the internal representations in LLMs, improving the model's comprehension of sequential movement patterns and location semantics. The proposed QT-Mob framework not only enhances LLMs' ability to interpret mobility data but also provides a more generalizable approach for various mobility analytics tasks. Experiments on three real-world dataset demonstrate the superior performance in both next-location prediction and mobility recovery tasks, outperforming existing deep learning and LLM-based methods.</li>
</ul>

<h3>Title: AssertBench: A Benchmark for Evaluating Self-Assertion in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jaeho Lee, Atharv Chowdhary</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11110">https://arxiv.org/abs/2506.11110</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11110">https://arxiv.org/pdf/2506.11110</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11110]] AssertBench: A Benchmark for Evaluating Self-Assertion in Large Language Models(https://arxiv.org/abs/2506.11110)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Recent benchmarks have probed factual consistency and rhetorical robustness in Large Language Models (LLMs). However, a knowledge gap exists regarding how directional framing of factually true statements influences model agreement, a common scenario for LLM users. AssertBench addresses this by sampling evidence-supported facts from FEVEROUS, a fact verification dataset. For each (evidence-backed) fact, we construct two framing prompts: one where the user claims the statement is factually correct, and another where the user claims it is incorrect. We then record the model's agreement and reasoning. The desired outcome is that the model asserts itself, maintaining consistent truth evaluation across both framings, rather than switching its evaluation to agree with the user. AssertBench isolates framing-induced variability from the model's underlying factual knowledge by stratifying results based on the model's accuracy on the same claims when presented neutrally. In doing so, this benchmark aims to measure an LLM's ability to "stick to its guns" when presented with contradictory user assertions about the same fact. The complete source code is available at this https URL.</li>
</ul>

<h3>Title: Evaluating and Improving Robustness in Large Language Models: A Survey and Future Directions</h3>
<ul>
<li><strong>Authors: </strong>Kun Zhang, Le Wu, Kui Yu, Guangyi Lv, Dacao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11111">https://arxiv.org/abs/2506.11111</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11111">https://arxiv.org/pdf/2506.11111</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11111]] Evaluating and Improving Robustness in Large Language Models: A Survey and Future Directions(https://arxiv.org/abs/2506.11111)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have gained enormous attention in recent years due to their capability of understanding and generating natural languages. With the rapid development and wild-range applications (e.g., Agents, Embodied Intelligence), the robustness of LLMs has received increased attention. As the core brain of many AI applications, the robustness of LLMs requires that models should not only generate consistent contents, but also ensure the correctness and stability of generated content when dealing with unexpeted application scenarios (e.g., toxic prompts, limited noise domain data, outof-distribution (OOD) applications, etc). In this survey paper, we conduct a thorough review of the robustness of LLMs, aiming to provide a comprehensive terminology of concepts and methods around this field and facilitate the community. Specifically, we first give a formal definition of LLM robustness and present the collection protocol of this survey paper. Then, based on the types of perturbated inputs, we organize this survey from the following perspectives: 1) Adversarial Robustness: tackling the problem that prompts are manipulated intentionally, such as noise prompts, long context, data attack, etc; 2) OOD Robustness: dealing with the unexpected real-world application scenarios, such as OOD detection, zero-shot transferring, hallucinations, etc; 3) Evaluation of Robustness: summarizing the new evaluation datasets, metrics, and tools for verifying the robustness of LLMs. After reviewing the representative work from each perspective, we discuss and highlight future opportunities and research directions in this field. Meanwhile, we also organize related works and provide an easy-to-search project (this https URL) to support the community.</li>
</ul>

<h3>Title: Breaking the Reviewer: Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks</h3>
<ul>
<li><strong>Authors: </strong>Tzu-Ling Lin, Wei-Chih Chen, Teng-Fang Hsiao, Hou-I Liu, Ya-Hsin Yeh, Yu Kai Chan, Wen-Sheng Lien, Po-Yen Kuo, Philip S. Yu, Hong-Han Shuai</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11113">https://arxiv.org/abs/2506.11113</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11113">https://arxiv.org/pdf/2506.11113</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11113]] Breaking the Reviewer: Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks(https://arxiv.org/abs/2506.11113)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Peer review is essential for maintaining academic quality, but the increasing volume of submissions places a significant burden on reviewers. Large language models (LLMs) offer potential assistance in this process, yet their susceptibility to textual adversarial attacks raises reliability concerns. This paper investigates the robustness of LLMs used as automated reviewers in the presence of such attacks. We focus on three key questions: (1) The effectiveness of LLMs in generating reviews compared to human reviewers. (2) The impact of adversarial attacks on the reliability of LLM-generated reviews. (3) Challenges and potential mitigation strategies for LLM-based review. Our evaluation reveals significant vulnerabilities, as text manipulations can distort LLM assessments. We offer a comprehensive evaluation of LLM performance in automated peer reviewing and analyze its robustness against adversarial attacks. Our findings emphasize the importance of addressing adversarial risks to ensure AI strengthens, rather than compromises, the integrity of scholarly communication.</li>
</ul>

<h3>Title: KokushiMD-10: Benchmark for Evaluating Large Language Models on Ten Japanese National Healthcare Licensing Examinations</h3>
<ul>
<li><strong>Authors: </strong>Junyu Liu, Kaiqi Yan, Tianyang Wang, Qian Niu, Momoko Nagai-Tanima, Tomoki Aoyama</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11114">https://arxiv.org/abs/2506.11114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11114">https://arxiv.org/pdf/2506.11114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11114]] KokushiMD-10: Benchmark for Evaluating Large Language Models on Ten Japanese National Healthcare Licensing Examinations(https://arxiv.org/abs/2506.11114)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have demonstrated notable performance in medical licensing exams. However, comprehensive evaluation of LLMs across various healthcare roles, particularly in high-stakes clinical scenarios, remains a challenge. Existing benchmarks are typically text-based, English-centric, and focus primarily on medicines, which limits their ability to assess broader healthcare knowledge and multimodal reasoning. To address these gaps, we introduce KokushiMD-10, the first multimodal benchmark constructed from ten Japanese national healthcare licensing exams. This benchmark spans multiple fields, including Medicine, Dentistry, Nursing, Pharmacy, and allied health professions. It contains over 11588 real exam questions, incorporating clinical images and expert-annotated rationales to evaluate both textual and visual reasoning. We benchmark over 30 state-of-the-art LLMs, including GPT-4o, Claude 3.5, and Gemini, across both text and image-based settings. Despite promising results, no model consistently meets passing thresholds across domains, highlighting the ongoing challenges in medical AI. KokushiMD-10 provides a comprehensive and linguistically grounded resource for evaluating and advancing reasoning-centric medical AI across multilingual and multimodal clinical tasks.</li>
</ul>

<h3>Title: Infinity Instruct: Scaling Instruction Selection and Synthesis to Enhance Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jijie Li, Li Du, Hanyu Zhao, Bo-wen Zhang, Liangdong Wang, Boyan Gao, Guang Liu, Yonghua Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11116">https://arxiv.org/abs/2506.11116</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11116">https://arxiv.org/pdf/2506.11116</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11116]] Infinity Instruct: Scaling Instruction Selection and Synthesis to Enhance Language Models(https://arxiv.org/abs/2506.11116)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) demonstrate strong performance in real-world applications, yet existing open-source instruction datasets often concentrate on narrow domains, such as mathematics or coding, limiting generalization and widening the gap with proprietary models. To bridge this gap, we introduce Infinity-Instruct, a high-quality instruction dataset designed to enhance both foundational and chat capabilities of LLMs through a two-phase pipeline. In Phase 1, we curate 7.4M high-quality foundational instructions (InfInstruct-F-7.4M) from over 100M samples using hybrid data selection techniques. In Phase 2, we synthesize 1.5M high-quality chat instructions (InfInstruct-G-1.5M) through a two-stage process involving instruction selection, evolution, and diagnostic filtering. We empirically evaluate Infinity-Instruct by fine-tuning several open-source models, including Mistral, LLaMA, Qwen, and Yi, and observe substantial performance gains across both foundational and instruction following benchmarks, consistently surpassing official instruction-tuned counterparts. Notably, InfInstruct-LLaMA3.1-70B outperforms GPT-4-0314 by 8.6\% on instruction following tasks while achieving comparable foundational performance. These results underscore the synergy between foundational and chat training and offer new insights into holistic LLM development. Our dataset\footnote{this https URL} and codes\footnote{this https URL} have been publicly released.</li>
</ul>

<h3>Title: ScIRGen: Synthesize Realistic and Large-Scale RAG Dataset for Scientific Research</h3>
<ul>
<li><strong>Authors: </strong>Junyong Lin, Lu Dai, Ruiqian Han, Yijie Sui, Ruilin Wang, Xingliang Sun, Qinglin Wu, Min Feng, Hao Liu, Hui Xiong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11117">https://arxiv.org/abs/2506.11117</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11117">https://arxiv.org/pdf/2506.11117</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11117]] ScIRGen: Synthesize Realistic and Large-Scale RAG Dataset for Scientific Research(https://arxiv.org/abs/2506.11117)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Scientific researchers need intensive information about datasets to effectively evaluate and develop theories and methodologies. The information needs regarding datasets are implicitly embedded in particular research tasks, rather than explicitly expressed in search queries. However, existing scientific retrieval and question-answering (QA) datasets typically address straightforward questions, which do not align with the distribution of real-world research inquiries. To bridge this gap, we developed ScIRGen, a dataset generation framework for scientific QA \& retrieval that more accurately reflects the information needs of professional science researchers, and uses it to create a large-scale scientific retrieval-augmented generation (RAG) dataset with realistic queries, datasets and papers. Technically, we designed a dataset-oriented information extraction method that leverages academic papers to augment the dataset representation. We then proposed a question generation framework by employing cognitive taxonomy to ensure the quality of synthesized questions. We also design a method to automatically filter synthetic answers based on the perplexity shift of LLMs, which is highly aligned with human judgment of answers' validity. Collectively, these methodologies culminated in the creation of the 61k QA dataset, ScIRGen-Geo. We benchmarked representative methods on the ScIRGen-Geo dataset for their question-answering and retrieval capabilities, finding out that current methods still suffer from reasoning from complex questions. This work advances the development of more sophisticated tools to support the intricate information needs of the scientific community.</li>
</ul>

<h3>Title: Benchmarking Foundation Speech and Language Models for Alzheimer's Disease and Related Dementia Detection from Spontaneous Speech</h3>
<ul>
<li><strong>Authors: </strong>Jingyu Li, Lingchao Mao, Hairong Wang, Zhendong Wang, Xi Mao, Xuelei Sherry Ni</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11119">https://arxiv.org/abs/2506.11119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11119">https://arxiv.org/pdf/2506.11119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11119]] Benchmarking Foundation Speech and Language Models for Alzheimer's Disease and Related Dementia Detection from Spontaneous Speech(https://arxiv.org/abs/2506.11119)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Background: Alzheimer's disease and related dementias (ADRD) are progressive neurodegenerative conditions where early detection is vital for timely intervention and care. Spontaneous speech contains rich acoustic and linguistic markers that may serve as non-invasive biomarkers for cognitive decline. Foundation models, pre-trained on large-scale audio or text data, produce high-dimensional embeddings encoding contextual and acoustic features. Methods: We used the PREPARE Challenge dataset, which includes audio recordings from over 1,600 participants with three cognitive statuses: healthy control (HC), mild cognitive impairment (MCI), and Alzheimer's Disease (AD). We excluded non-English, non-spontaneous, or poor-quality recordings. The final dataset included 703 (59.13%) HC, 81 (6.81%) MCI, and 405 (34.06%) AD cases. We benchmarked a range of open-source foundation speech and language models to classify cognitive status into the three categories. Results: The Whisper-medium model achieved the highest performance among speech models (accuracy = 0.731, AUC = 0.802). Among language models, BERT with pause annotation performed best (accuracy = 0.662, AUC = 0.744). ADRD detection using state-of-the-art automatic speech recognition (ASR) model-generated audio embeddings outperformed others. Including non-semantic features like pause patterns consistently improved text-based classification. Conclusion: This study introduces a benchmarking framework using foundation models and a clinically relevant dataset. Acoustic-based approaches -- particularly ASR-derived embeddings -- demonstrate strong potential for scalable, non-invasive, and cost-effective early detection of ADRD.</li>
</ul>

<h3>Title: SDMPrune: Self-Distillation MLP Pruning for Efficient Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hourun Zhu, Chengchao Shen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11120">https://arxiv.org/abs/2506.11120</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11120">https://arxiv.org/pdf/2506.11120</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11120]] SDMPrune: Self-Distillation MLP Pruning for Efficient Large Language Models(https://arxiv.org/abs/2506.11120)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>In spite of strong performance achieved by LLMs, the costs of their deployment are unaffordable. For the compression of LLMs, gradient-based pruning methods present promising effectiveness. However, in these methods, the gradient computation with one-hot labels ignore the potential predictions on other words, thus missing key information for generative capability of the original model. To address this issue, we introduce a self-distillation loss during the pruning phase (rather than post-training) to fully exploit the predictions of the original model, thereby obtaining more accurate gradient information for pruning. Moreover, we find that, compared to attention modules, the predictions of LLM are less sensitive to multilayer perceptron (MLP) modules, which take up more than $5 \times$ parameters (LLaMA3.2-1.2B). To this end, we focus on the pruning of MLP modules, to significantly compress LLM without obvious performance degradation. Experimental results on extensive zero-shot benchmarks demonstrate that our method significantly outperforms existing pruning methods. Furthermore, our method achieves very competitive performance among 1B-scale open source LLMs. The source code and trained weights are available at this https URL.</li>
</ul>

<h3>Title: SUTA-LM: Bridging Test-Time Adaptation and Language Model Rescoring for Robust ASR</h3>
<ul>
<li><strong>Authors: </strong>Wei-Ping Huang, Guan-Ting Lin, Hung-yi Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11121">https://arxiv.org/abs/2506.11121</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11121">https://arxiv.org/pdf/2506.11121</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11121]] SUTA-LM: Bridging Test-Time Adaptation and Language Model Rescoring for Robust ASR(https://arxiv.org/abs/2506.11121)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Despite progress in end-to-end ASR, real-world domain mismatches still cause performance drops, which Test-Time Adaptation (TTA) aims to mitigate by adjusting models during inference. Recent work explores combining TTA with external language models, using techniques like beam search rescoring or generative error correction. In this work, we identify a previously overlooked challenge: TTA can interfere with language model rescoring, revealing the nontrivial nature of effectively combining the two methods. Based on this insight, we propose SUTA-LM, a simple yet effective extension of SUTA, an entropy-minimization-based TTA approach, with language model rescoring. SUTA-LM first applies a controlled adaptation process guided by an auto-step selection mechanism leveraging both acoustic and linguistic information, followed by language model rescoring to refine the outputs. Experiments on 18 diverse ASR datasets show that SUTA-LM achieves robust results across a wide range of domains.</li>
</ul>

<h3>Title: Adaptive Object Detection with ESRGAN-Enhanced Resolution & Faster R-CNN</h3>
<ul>
<li><strong>Authors: </strong>Divya Swetha K, Ziaul Haque Choudhury, Hemanta Kumar Bhuyan, Biswajit Brahma, Nilayam Kumar Kamila</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11122">https://arxiv.org/abs/2506.11122</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11122">https://arxiv.org/pdf/2506.11122</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11122]] Adaptive Object Detection with ESRGAN-Enhanced Resolution & Faster R-CNN(https://arxiv.org/abs/2506.11122)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>In this study, proposes a method for improved object detection from the low-resolution images by integrating Enhanced Super-Resolution Generative Adversarial Networks (ESRGAN) and Faster Region-Convolutional Neural Network (Faster R-CNN). ESRGAN enhances low-quality images, restoring details and improving clarity, while Faster R-CNN performs accurate object detection on the enhanced images. The combination of these techniques ensures better detection performance, even with poor-quality inputs, offering an effective solution for applications where image resolution is in consistent. ESRGAN is employed as a pre-processing step to enhance the low-resolution input image, effectively restoring lost details and improving overall image quality. Subsequently, the enhanced image is fed into the Faster R-CNN model for accurate object detection and localization. Experimental results demonstrate that this integrated approach yields superior performance compared to traditional methods applied directly to low-resolution images. The proposed framework provides a promising solution for applications where image quality is variable or limited, enabling more robust and reliable object detection in challenging scenarios. It achieves a balance between improved image quality and efficient object detection</li>
</ul>

<h3>Title: Technical Report for Argoverse2 Scenario Mining Challenges on Iterative Error Correction and Spatially-Aware Prompting</h3>
<ul>
<li><strong>Authors: </strong>Yifei Chen, Ross Greer</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11124">https://arxiv.org/abs/2506.11124</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11124">https://arxiv.org/pdf/2506.11124</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11124]] Technical Report for Argoverse2 Scenario Mining Challenges on Iterative Error Correction and Spatially-Aware Prompting(https://arxiv.org/abs/2506.11124)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Scenario mining from extensive autonomous driving datasets, such as Argoverse 2, is crucial for the development and validation of self-driving systems. The RefAV framework represents a promising approach by employing Large Language Models (LLMs) to translate natural-language queries into executable code for identifying relevant scenarios. However, this method faces challenges, including runtime errors stemming from LLM-generated code and inaccuracies in interpreting parameters for functions that describe complex multi-object spatial relationships. This technical report introduces two key enhancements to address these limitations: (1) a fault-tolerant iterative code-generation mechanism that refines code by re-prompting the LLM with error feedback, and (2) specialized prompt engineering that improves the LLM's comprehension and correct application of spatial-relationship functions. Experiments on the Argoverse 2 validation set with diverse LLMs-Qwen2.5-VL-7B, Gemini 2.5 Flash, and Gemini 2.5 Pro-show consistent gains across multiple metrics; most notably, the proposed system achieves a HOTA-Temporal score of 52.37 on the official test set using Gemini 2.5 Pro. These results underline the efficacy of the proposed techniques for reliable, high-precision scenario mining.</li>
</ul>

<h3>Title: ASRJam: Human-Friendly AI Speech Jamming to Prevent Automated Phone Scams</h3>
<ul>
<li><strong>Authors: </strong>Freddie Grabovski, Gilad Gressel, Yisroel Mirsky</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11125">https://arxiv.org/abs/2506.11125</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11125">https://arxiv.org/pdf/2506.11125</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11125]] ASRJam: Human-Friendly AI Speech Jamming to Prevent Automated Phone Scams(https://arxiv.org/abs/2506.11125)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs), combined with Text-to-Speech (TTS) and Automatic Speech Recognition (ASR), are increasingly used to automate voice phishing (vishing) scams. These systems are scalable and convincing, posing a significant security threat. We identify the ASR transcription step as the most vulnerable link in the scam pipeline and introduce ASRJam, a proactive defence framework that injects adversarial perturbations into the victim's audio to disrupt the attacker's ASR. This breaks the scam's feedback loop without affecting human callers, who can still understand the conversation. While prior adversarial audio techniques are often unpleasant and impractical for real-time use, we also propose EchoGuard, a novel jammer that leverages natural distortions, such as reverberation and echo, that are disruptive to ASR but tolerable to humans. To evaluate EchoGuard's effectiveness and usability, we conducted a 39-person user study comparing it with three state-of-the-art attacks. Results show that EchoGuard achieved the highest overall utility, offering the best combination of ASR disruption and human listening experience.</li>
</ul>

<h3>Title: Image-Based Method For Measuring And Classification Of Iron Ore Pellets Using Star-Convex Polygons</h3>
<ul>
<li><strong>Authors: </strong>Artem Solomko, Oleg Kartashev, Andrey Golov, Mikhail Deulin, Vadim Valynkin, Vasily Kharin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11126">https://arxiv.org/abs/2506.11126</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11126">https://arxiv.org/pdf/2506.11126</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11126]] Image-Based Method For Measuring And Classification Of Iron Ore Pellets Using Star-Convex Polygons(https://arxiv.org/abs/2506.11126)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>We would like to present a comprehensive study on the classification of iron ore pellets, aimed at identifying quality violations in the final product, alongside the development of an innovative imagebased measurement method utilizing the StarDist algorithm, which is primarily employed in the medical field. This initiative is motivated by the necessity to accurately identify and analyze objects within densely packed and unstable environments. The process involves segmenting these objects, determining their contours, classifying them, and measuring their physical dimensions. This is crucial because the size distribution and classification of pellets such as distinguishing between nice (quality) and joint (caused by the presence of moisture or indicating a process of production failure) types are among the most significant characteristics that define the quality of the final product. Traditional algorithms, including image classification techniques using Vision Transformer (ViT), instance segmentation methods like Mask R-CNN, and various anomaly segmentation algorithms, have not yielded satisfactory results in this context. Consequently, we explored methodologies from related fields to enhance our approach. The outcome of our research is a novel method designed to detect objects with smoothed boundaries. This advancement significantly improves the accuracy of physical dimension measurements and facilitates a more precise analysis of size distribution among the iron ore pellets. By leveraging the strengths of the StarDist algorithm, we aim to provide a robust solution that addresses the challenges posed by the complex nature of pellet classification and measurement.</li>
</ul>

<h3>Title: GUIRoboTron-Speech: Towards Automated GUI Agents Based on Speech Instructions</h3>
<ul>
<li><strong>Authors: </strong>Wenkang Han, Zhixiong Zeng, Jing Huang, Shu Jiang, Liming Zheng, Longrong Yang, Haibo Qiu, Chang Yao, Jingyuan Chen, Lin Ma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11127">https://arxiv.org/abs/2506.11127</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11127">https://arxiv.org/pdf/2506.11127</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11127]] GUIRoboTron-Speech: Towards Automated GUI Agents Based on Speech Instructions(https://arxiv.org/abs/2506.11127)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Autonomous agents for Graphical User Interfaces (GUIs) are revolutionizing human-computer interaction, yet their reliance on text-based instructions imposes limitations on accessibility and convenience, particularly in hands-free scenarios. To address this gap, we propose GUIRoboTron-Speech, the first end-to-end autonomous GUI agent that directly accepts speech instructions and on-device screenshots to predict actions. Confronted with the scarcity of speech-based GUI agent datasets, we initially generated high-quality speech instructions for training by leveraging a random timbre text-to-speech (TTS) model to convert existing text instructions. We then develop GUIRoboTron-Speech's capabilities through progressive grounding and planning training stages. A key contribution is a heuristic mixed-instruction training strategy designed to mitigate the modality imbalance inherent in pre-trained foundation models. Comprehensive experiments on several benchmark datasets validate the robust and superior performance of GUIRoboTron-Speech, demonstrating the significant potential and widespread applicability of speech as an effective instruction modality for driving GUI agents. Our code and datasets are available at this https URL.</li>
</ul>

<h3>Title: Trustworthy AI for Medicine: Continuous Hallucination Detection and Elimination with CHECK</h3>
<ul>
<li><strong>Authors: </strong>Carlos Garcia-Fernandez, Luis Felipe, Monique Shotande, Muntasir Zitu, Aakash Tripathi, Ghulam Rasool, Issam El Naqa, Vivek Rudrapatna, Gilmer Valdes</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11129">https://arxiv.org/abs/2506.11129</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11129">https://arxiv.org/pdf/2506.11129</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11129]] Trustworthy AI for Medicine: Continuous Hallucination Detection and Elimination with CHECK(https://arxiv.org/abs/2506.11129)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) show promise in healthcare, but hallucinations remain a major barrier to clinical use. We present CHECK, a continuous-learning framework that integrates structured clinical databases with a classifier grounded in information theory to detect both factual and reasoning-based hallucinations. Evaluated on 1500 questions from 100 pivotal clinical trials, CHECK reduced LLama3.3-70B-Instruct hallucination rates from 31% to 0.3% - making an open source model state of the art. Its classifier generalized across medical benchmarks, achieving AUCs of 0.95-0.96, including on the MedQA (USMLE) benchmark and HealthBench realistic multi-turn medical questioning. By leveraging hallucination probabilities to guide GPT-4o's refinement and judiciously escalate compute, CHECK boosted its USMLE passing rate by 5 percentage points, achieving a state-of-the-art 92.1%. By suppressing hallucinations below accepted clinical error thresholds, CHECK offers a scalable foundation for safe LLM deployment in medicine and other high-stakes domains.</li>
</ul>

<h3>Title: Segment This Thing: Foveated Tokenization for Efficient Point-Prompted Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Tanner Schmidt, Richard Newcombe</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11131">https://arxiv.org/abs/2506.11131</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11131">https://arxiv.org/pdf/2506.11131</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11131]] Segment This Thing: Foveated Tokenization for Efficient Point-Prompted Segmentation(https://arxiv.org/abs/2506.11131)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This paper presents Segment This Thing (STT), a new efficient image segmentation model designed to produce a single segment given a single point prompt. Instead of following prior work and increasing efficiency by decreasing model size, we gain efficiency by foveating input images. Given an image and a point prompt, we extract a crop centered on the prompt and apply a novel variable-resolution patch tokenization in which patches are downsampled at a rate that increases with increased distance from the prompt. This approach yields far fewer image tokens than uniform patch tokenization. As a result we can drastically reduce the computational cost of segmentation without reducing model size. Furthermore, the foveation focuses the model on the region of interest, a potentially useful inductive bias. We show that our Segment This Thing model is more efficient than prior work while remaining competitive on segmentation benchmarks. It can easily run at interactive frame rates on consumer hardware and is thus a promising tool for augmented reality or robotics applications.</li>
</ul>

<h3>Title: Gender Fairness of Machine Learning Algorithms for Pain Detection</h3>
<ul>
<li><strong>Authors: </strong>Dylan Green, Yuting Shang, Jiaee Cheong, Yang Liu, Hatice Gunes</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11132">https://arxiv.org/abs/2506.11132</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11132">https://arxiv.org/pdf/2506.11132</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11132]] Gender Fairness of Machine Learning Algorithms for Pain Detection(https://arxiv.org/abs/2506.11132)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, transformer</a></li>
<li><strong>Abstract: </strong>Automated pain detection through machine learning (ML) and deep learning (DL) algorithms holds significant potential in healthcare, particularly for patients unable to self-report pain levels. However, the accuracy and fairness of these algorithms across different demographic groups (e.g., gender) remain under-researched. This paper investigates the gender fairness of ML and DL models trained on the UNBC-McMaster Shoulder Pain Expression Archive Database, evaluating the performance of various models in detecting pain based solely on the visual modality of participants' facial expressions. We compare traditional ML algorithms, Linear Support Vector Machine (L SVM) and Radial Basis Function SVM (RBF SVM), with DL methods, Convolutional Neural Network (CNN) and Vision Transformer (ViT), using a range of performance and fairness metrics. While ViT achieved the highest accuracy and a selection of fairness metrics, all models exhibited gender-based biases. These findings highlight the persistent trade-off between accuracy and fairness, emphasising the need for fairness-aware techniques to mitigate biases in automated healthcare systems.</li>
</ul>

<h3>Title: Monocular 3D Hand Pose Estimation with Implicit Camera Alignment</h3>
<ul>
<li><strong>Authors: </strong>Christos Pantazopoulos, Spyridon Thermos, Gerasimos Potamianos</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11133">https://arxiv.org/abs/2506.11133</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11133">https://arxiv.org/pdf/2506.11133</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11133]] Monocular 3D Hand Pose Estimation with Implicit Camera Alignment(https://arxiv.org/abs/2506.11133)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Estimating the 3D hand articulation from a single color image is a continuously investigated problem with applications in Augmented Reality (AR), Virtual Reality (VR), Human-Computer Interaction (HCI), and robotics. Apart from the absence of depth information, occlusions, articulation complexity, and the need for camera parameters knowledge pose additional challenges. In this work, we propose an optimization pipeline for estimating the 3D hand articulation from 2D keypoint input, which includes a keypoint alignment step and a fingertip loss to overcome the need to know or estimate the camera parameters. We evaluate our approach on the EgoDexter and Dexter+Object benchmarks to showcase that our approach performs competitively with the SotA, while also demonstrating its robustness when processing "in-the-wild" images without any prior camera knowledge. Our quantitative analysis highlights the sensitivity of the 2D keypoint estimation accuracy, despite the use of hand priors. Code is available at this https URL</li>
</ul>

<h3>Title: ContextLoss: Context Information for Topology-Preserving Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Benedict Schacht, Imke Greving, Simone Frintrop, Berit Zeller-Plumhoff, Christian Wilms</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11134">https://arxiv.org/abs/2506.11134</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11134">https://arxiv.org/pdf/2506.11134</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11134]] ContextLoss: Context Information for Topology-Preserving Segmentation(https://arxiv.org/abs/2506.11134)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In image segmentation, preserving the topology of segmented structures like vessels, membranes, or roads is crucial. For instance, topological errors on road networks can significantly impact navigation. Recently proposed solutions are loss functions based on critical pixel masks that consider the whole skeleton of the segmented structures in the critical pixel mask. We propose the novel loss function ContextLoss (CLoss) that improves topological correctness by considering topological errors with their whole context in the critical pixel mask. The additional context improves the network focus on the topological errors. Further, we propose two intuitive metrics to verify improved connectivity due to a closing of missed connections. We benchmark our proposed CLoss on three public datasets (2D & 3D) and our own 3D nano-imaging dataset of bone cement lines. Training with our proposed CLoss increases performance on topology-aware metrics and repairs up to 44% more missed connections than other state-of-the-art methods. We make the code publicly available.</li>
</ul>

<h3>Title: Large Language Models and Emergence: A Complex Systems Perspective</h3>
<ul>
<li><strong>Authors: </strong>David C. Krakauer, John W. Krakauer, Melanie Mitchell</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11135">https://arxiv.org/abs/2506.11135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11135">https://arxiv.org/pdf/2506.11135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11135]] Large Language Models and Emergence: A Complex Systems Perspective(https://arxiv.org/abs/2506.11135)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Emergence is a concept in complexity science that describes how many-body systems manifest novel higher-level properties, properties that can be described by replacing high-dimensional mechanisms with lower-dimensional effective variables and theories. This is captured by the idea "more is different". Intelligence is a consummate emergent property manifesting increasingly efficient -- cheaper and faster -- uses of emergent capabilities to solve problems. This is captured by the idea "less is more". In this paper, we first examine claims that Large Language Models exhibit emergent capabilities, reviewing several approaches to quantifying emergence, and secondly ask whether LLMs possess emergent intelligence.</li>
</ul>

<h3>Title: Scalable Medication Extraction and Discontinuation Identification from Electronic Health Records Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chong Shao, Douglas Snyder, Chiran Li, Bowen Gu, Kerry Ngan, Chun-Ting Yang, Jiageng Wu, Richard Wyss, Kueiyu Joshua Lin, Jie Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11137">https://arxiv.org/abs/2506.11137</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11137">https://arxiv.org/pdf/2506.11137</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11137]] Scalable Medication Extraction and Discontinuation Identification from Electronic Health Records Using Large Language Models(https://arxiv.org/abs/2506.11137)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Identifying medication discontinuations in electronic health records (EHRs) is vital for patient safety but is often hindered by information being buried in unstructured notes. This study aims to evaluate the capabilities of advanced open-sourced and proprietary large language models (LLMs) in extracting medications and classifying their medication status from EHR notes, focusing on their scalability on medication information extraction without human annotation. We collected three EHR datasets from diverse sources to build the evaluation benchmark. We evaluated 12 advanced LLMs and explored multiple LLM prompting strategies. Performance on medication extraction, medication status classification, and their joint task (extraction then classification) was systematically compared across all experiments. We found that LLMs showed promising performance on the medication extraction and discontinuation classification from EHR notes. GPT-4o consistently achieved the highest average F1 scores in all tasks under zero-shot setting - 94.0% for medication extraction, 78.1% for discontinuation classification, and 72.7% for the joint task. Open-sourced models followed closely, Llama-3.1-70B-Instruct achieved the highest performance in medication status classification on the MIV-Med dataset (68.7%) and in the joint task on both the Re-CASI (76.2%) and MIV-Med (60.2%) datasets. Medical-specific LLMs demonstrated lower performance compared to advanced general-domain LLMs. Few-shot learning generally improved performance, while CoT reasoning showed inconsistent gains. LLMs demonstrate strong potential for medication extraction and discontinuation identification on EHR notes, with open-sourced models offering scalable alternatives to proprietary systems and few-shot can further improve LLMs' capability.</li>
</ul>

<h3>Title: Autonomous Computer Vision Development with Agentic AI</h3>
<ul>
<li><strong>Authors: </strong>Jin Kim, Muhammad Wahi-Anwa, Sangyun Park, Shawn Shin, John M. Hoffman, Matthew S. Brown</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11140">https://arxiv.org/abs/2506.11140</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11140">https://arxiv.org/pdf/2506.11140</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11140]] Autonomous Computer Vision Development with Agentic AI(https://arxiv.org/abs/2506.11140)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Agentic Artificial Intelligence (AI) systems leveraging Large Language Models (LLMs) exhibit significant potential for complex reasoning, planning, and tool utilization. We demonstrate that a specialized computer vision system can be built autonomously from a natural language prompt using Agentic AI methods. This involved extending SimpleMind (SM), an open-source Cognitive AI environment with configurable tools for medical image analysis, with an LLM-based agent, implemented using OpenManus, to automate the planning (tool configuration) for a particular computer vision task. We provide a proof-of-concept demonstration that an agentic system can interpret a computer vision task prompt, plan a corresponding SimpleMind workflow by decomposing the task and configuring appropriate tools. From the user input prompt, "provide sm (SimpleMind) config for lungs, heart, and ribs segmentation for cxr (chest x-ray)"), the agent LLM was able to generate the plan (tool configuration file in YAML format), and execute SM-Learn (training) and SM-Think (inference) scripts autonomously. The computer vision agent automatically configured, trained, and tested itself on 50 chest x-ray images, achieving mean dice scores of 0.96, 0.82, 0.83, for lungs, heart, and ribs, respectively. This work shows the potential for autonomous planning and tool configuration that has traditionally been performed by a data scientist in the development of computer vision applications.</li>
</ul>

<h3>Title: FARCLUSS: Fuzzy Adaptive Rebalancing and Contrastive Uncertainty Learning for Semi-Supervised Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Ebenezer Tarubinga, Jenifer Kalafatovich</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11142">https://arxiv.org/abs/2506.11142</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11142">https://arxiv.org/pdf/2506.11142</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11142]] FARCLUSS: Fuzzy Adaptive Rebalancing and Contrastive Uncertainty Learning for Semi-Supervised Semantic Segmentation(https://arxiv.org/abs/2506.11142)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Semi-supervised semantic segmentation (SSSS) faces persistent challenges in effectively leveraging unlabeled data, such as ineffective utilization of pseudo-labels, exacerbation of class imbalance biases, and neglect of prediction uncertainty. Current approaches often discard uncertain regions through strict thresholding favouring dominant classes. To address these limitations, we introduce a holistic framework that transforms uncertainty into a learning asset through four principal components: (1) fuzzy pseudo-labeling, which preserves soft class distributions from top-K predictions to enrich supervision; (2) uncertainty-aware dynamic weighting, that modulate pixel-wise contributions via entropy-based reliability scores; (3) adaptive class rebalancing, which dynamically adjust losses to counteract long-tailed class distributions; and (4) lightweight contrastive regularization, that encourage compact and discriminative feature embeddings. Extensive experiments on benchmarks demonstrate that our method outperforms current state-of-the-art approaches, achieving significant improvements in the segmentation of under-represented classes and ambiguous regions.</li>
</ul>

<h3>Title: AlignHuman: Improving Motion and Fidelity via Timestep-Segment Preference Optimization for Audio-Driven Human Animation</h3>
<ul>
<li><strong>Authors: </strong>Chao Liang, Jianwen Jiang, Wang Liao, Jiaqi Yang, Zerong zheng, Weihong Zeng, Han Liang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11144">https://arxiv.org/abs/2506.11144</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11144">https://arxiv.org/pdf/2506.11144</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11144]] AlignHuman: Improving Motion and Fidelity via Timestep-Segment Preference Optimization for Audio-Driven Human Animation(https://arxiv.org/abs/2506.11144)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advancements in human video generation and animation tasks, driven by diffusion models, have achieved significant progress. However, expressive and realistic human animation remains challenging due to the trade-off between motion naturalness and visual fidelity. To address this, we propose \textbf{AlignHuman}, a framework that combines Preference Optimization as a post-training technique with a divide-and-conquer training strategy to jointly optimize these competing objectives. Our key insight stems from an analysis of the denoising process across timesteps: (1) early denoising timesteps primarily control motion dynamics, while (2) fidelity and human structure can be effectively managed by later timesteps, even if early steps are skipped. Building on this observation, we propose timestep-segment preference optimization (TPO) and introduce two specialized LoRAs as expert alignment modules, each targeting a specific dimension in its corresponding timestep interval. The LoRAs are trained using their respective preference data and activated in the corresponding intervals during inference to enhance motion naturalness and fidelity. Extensive experiments demonstrate that AlignHuman improves strong baselines and reduces NFEs during inference, achieving a 3.3$\times$ speedup (from 100 NFEs to 30 NFEs) with minimal impact on generation quality. Homepage: \href{this https URL}{this https URL}</li>
</ul>

<h3>Title: 3D-RAD: A Comprehensive 3D Radiology Med-VQA Dataset with Multi-Temporal Analysis and Diverse Diagnostic Tasks</h3>
<ul>
<li><strong>Authors: </strong>Xiaotang Gai, Jiaxiang Liu, Yichen Li, Zijie Meng, Jian Wu, Zuozhu Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11147">https://arxiv.org/abs/2506.11147</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11147">https://arxiv.org/pdf/2506.11147</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11147]] 3D-RAD: A Comprehensive 3D Radiology Med-VQA Dataset with Multi-Temporal Analysis and Diverse Diagnostic Tasks(https://arxiv.org/abs/2506.11147)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Medical Visual Question Answering (Med-VQA) holds significant potential for clinical decision support, yet existing efforts primarily focus on 2D imaging with limited task diversity. This paper presents 3D-RAD, a large-scale dataset designed to advance 3D Med-VQA using radiology CT scans. The 3D-RAD dataset encompasses six diverse VQA tasks: anomaly detection, image observation, medical computation, existence detection, static temporal diagnosis, and longitudinal temporal diagnosis. It supports both open- and closed-ended questions while introducing complex reasoning challenges, including computational tasks and multi-stage temporal analysis, to enable comprehensive benchmarking. Extensive evaluations demonstrate that existing vision-language models (VLMs), especially medical VLMs exhibit limited generalization, particularly in multi-temporal tasks, underscoring the challenges of real-world 3D diagnostic reasoning. To drive future advancements, we release a high-quality training set 3D-RAD-T of 136,195 expert-aligned samples, showing that fine-tuning on this dataset could significantly enhance model performance. Our dataset and code, aiming to catalyze multimodal medical AI research and establish a robust foundation for 3D medical visual understanding, are publicly available at this https URL.</li>
</ul>

<h3>Title: LLM-to-Phy3D: Physically Conform Online 3D Object Generation with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Melvin Wong, Yueming Lyu, Thiago Rios, Stefan Menzel, Yew-Soon Ong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11148">https://arxiv.org/abs/2506.11148</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11148">https://arxiv.org/pdf/2506.11148</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11148]] LLM-to-Phy3D: Physically Conform Online 3D Object Generation with LLMs(https://arxiv.org/abs/2506.11148)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>The emergence of generative artificial intelligence (GenAI) and large language models (LLMs) has revolutionized the landscape of digital content creation in different modalities. However, its potential use in Physical AI for engineering design, where the production of physically viable artifacts is paramount, remains vastly underexplored. The absence of physical knowledge in existing LLM-to-3D models often results in outputs detached from real-world physical constraints. To address this gap, we introduce LLM-to-Phy3D, a physically conform online 3D object generation that enables existing LLM-to-3D models to produce physically conforming 3D objects on the fly. LLM-to-Phy3D introduces a novel online black-box refinement loop that empowers large language models (LLMs) through synergistic visual and physics-based evaluations. By delivering directional feedback in an iterative refinement process, LLM-to-Phy3D actively drives the discovery of prompts that yield 3D artifacts with enhanced physical performance and greater geometric novelty relative to reference objects, marking a substantial contribution to AI-driven generative design. Systematic evaluations of LLM-to-Phy3D, supported by ablation studies in vehicle design optimization, reveal various LLM improvements gained by 4.5% to 106.7% in producing physically conform target domain 3D designs over conventional LLM-to-3D models. The encouraging results suggest the potential general use of LLM-to-Phy3D in Physical AI for scientific and engineering applications.</li>
</ul>

<h3>Title: Evaluating Multimodal Large Language Models on Video Captioning via Monte Carlo Tree Search</h3>
<ul>
<li><strong>Authors: </strong>Linhao Yu, Xinguang Ji, Yahui Liu, Fanheng Kong, Chenxi Sun, Jingyuan Zhang, Hongzhi Zhang, V. W., Fuzheng Zhang, Deyi Xiong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11155">https://arxiv.org/abs/2506.11155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11155">https://arxiv.org/pdf/2506.11155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11155]] Evaluating Multimodal Large Language Models on Video Captioning via Monte Carlo Tree Search(https://arxiv.org/abs/2506.11155)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Video captioning can be used to assess the video understanding capabilities of Multimodal Large Language Models (MLLMs). However, existing benchmarks and evaluation protocols suffer from crucial issues, such as inadequate or homogeneous creation of key points, exorbitant cost of data creation, and limited evaluation scopes. To address these issues, we propose an automatic framework, named AutoCaption, which leverages Monte Carlo Tree Search (MCTS) to construct numerous and diverse descriptive sentences (\textit{i.e.}, key points) that thoroughly represent video content in an iterative way. This iterative captioning strategy enables the continuous enhancement of video details such as actions, objects' attributes, environment details, etc. We apply AutoCaption to curate MCTS-VCB, a fine-grained video caption benchmark covering video details, thereby enabling a comprehensive evaluation of MLLMs on the video captioning task. We evaluate more than 20 open- and closed-source MLLMs of varying sizes on MCTS-VCB. Results show that MCTS-VCB can effectively and comprehensively evaluate the video captioning capability, with Gemini-1.5-Pro achieving the highest F1 score of 71.2. Interestingly, we fine-tune InternVL2.5-8B with the AutoCaption-generated data, which helps the model achieve an overall improvement of 25.0% on MCTS-VCB and 16.3% on DREAM-1K, further demonstrating the effectiveness of AutoCaption. The code and data are available at this https URL.</li>
</ul>

<h3>Title: Digitization of Document and Information Extraction using OCR</h3>
<ul>
<li><strong>Authors: </strong>Rasha Sinha, Rekha B S</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11156">https://arxiv.org/abs/2506.11156</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11156">https://arxiv.org/pdf/2506.11156</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11156]] Digitization of Document and Information Extraction using OCR(https://arxiv.org/abs/2506.11156)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Retrieving accurate details from documents is a crucial task, especially when handling a combination of scanned images and native digital formats. This document presents a combined framework for text extraction that merges Optical Character Recognition (OCR) techniques with Large Language Models (LLMs) to deliver structured outputs enriched by contextual understanding and confidence indicators. Scanned files are processed using OCR engines, while digital files are interpreted through layout-aware libraries. The extracted raw text is subsequently analyzed by an LLM to identify key-value pairs and resolve ambiguities. A comparative analysis of different OCR tools is presented to evaluate their effectiveness concerning accuracy, layout recognition, and processing speed. The approach demonstrates significant improvements over traditional rule-based and template-based methods, offering enhanced flexibility and semantic precision across different document categories</li>
</ul>

<h3>Title: Synthetic Geology -- Structural Geology Meets Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Simon Ghyselincks, Valeriia Okhmak, Stefano Zampini, George Turkiyyah, David Keyes, Eldad Haber</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11164">https://arxiv.org/abs/2506.11164</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11164">https://arxiv.org/pdf/2506.11164</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11164]] Synthetic Geology -- Structural Geology Meets Deep Learning(https://arxiv.org/abs/2506.11164)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Visualizing the first few kilometers of the Earth's subsurface, a long-standing challenge gating a virtually inexhaustible list of important applications, is coming within reach through deep learning. Building on techniques of generative artificial intelligence applied to voxelated images, we demonstrate a method that extends surface geological data supplemented by boreholes to a three-dimensional subsurface region by training a neural network. The Earth's land area having been extensively mapped for geological features, the bottleneck of this or any related technique is the availability of data below the surface. We close this data gap in the development of subsurface deep learning by designing a synthetic data-generator process that mimics eons of geological activity such as sediment compaction, volcanic intrusion, and tectonic dynamics to produce a virtually limitless number of samples of the near lithosphere. A foundation model trained on such synthetic data is able to generate a 3D image of the subsurface from a previously unseen map of surface topography and geology, showing increasing fidelity with increasing access to borehole data, depicting such structures as layers, faults, folds, dikes, and sills. We illustrate the early promise of the combination of a synthetic lithospheric generator with a trained neural network model using generative flow matching. Ultimately, such models will be fine-tuned on data from applicable campaigns, such as mineral prospecting in a given region. Though useful in itself, a regionally fine-tuned models may be employed not as an end but as a means: as an AI-based regularizer in a more traditional inverse problem application, in which the objective function represents the mismatch of additional data with physical models with applications in resource exploration, hazard assessment, and geotechnical engineering.</li>
</ul>

<h3>Title: Test-Time-Scaling for Zero-Shot Diagnosis with Visual-Language Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Ji Young Byun, Young-Jin Park, Navid Azizan, Rama Chellappa</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11166">https://arxiv.org/abs/2506.11166</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11166">https://arxiv.org/pdf/2506.11166</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11166]] Test-Time-Scaling for Zero-Shot Diagnosis with Visual-Language Reasoning(https://arxiv.org/abs/2506.11166)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As a cornerstone of patient care, clinical decision-making significantly influences patient outcomes and can be enhanced by large language models (LLMs). Although LLMs have demonstrated remarkable performance, their application to visual question answering in medical imaging, particularly for reasoning-based diagnosis, remains largely unexplored. Furthermore, supervised fine-tuning for reasoning tasks is largely impractical due to limited data availability and high annotation costs. In this work, we introduce a zero-shot framework for reliable medical image diagnosis that enhances the reasoning capabilities of LLMs in clinical settings through test-time scaling. Given a medical image and a textual prompt, a vision-language model processes a medical image along with a corresponding textual prompt to generate multiple descriptions or interpretations of visual features. These interpretations are then fed to an LLM, where a test-time scaling strategy consolidates multiple candidate outputs into a reliable final diagnosis. We evaluate our approach across various medical imaging modalities -- including radiology, ophthalmology, and histopathology -- and demonstrate that the proposed test-time scaling strategy enhances diagnostic accuracy for both our and baseline methods. Additionally, we provide an empirical analysis showing that the proposed approach, which allows unbiased prompting in the first stage, improves the reliability of LLM-generated diagnoses and enhances classification accuracy.</li>
</ul>

<h3>Title: WaveFormer: A Lightweight Transformer Model for sEMG-based Gesture Recognition</h3>
<ul>
<li><strong>Authors: </strong>Yanlong Chen, Mattia Orlandi, Pierangelo Maria Rapa, Simone Benatti, Luca Benini, Yawei Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11168">https://arxiv.org/abs/2506.11168</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11168">https://arxiv.org/pdf/2506.11168</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11168]] WaveFormer: A Lightweight Transformer Model for sEMG-based Gesture Recognition(https://arxiv.org/abs/2506.11168)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Human-machine interaction, particularly in prosthetic and robotic control, has seen progress with gesture recognition via surface electromyographic (sEMG) this http URL, classifying similar gestures that produce nearly identical muscle signals remains a challenge, often reducing classification accuracy. Traditional deep learning models for sEMG gesture recognition are large and computationally expensive, limiting their deployment on resource-constrained embedded systems. In this work, we propose WaveFormer, a lightweight transformer-based architecture tailored for sEMG gesture recognition. Our model integrates time-domain and frequency-domain features through a novel learnable wavelet transform, enhancing feature extraction. In particular, the WaveletConv module, a multi-level wavelet decomposition layer with depthwise separable convolution, ensures both efficiency and compactness. With just 3.1 million parameters, WaveFormer achieves 95% classification accuracy on the EPN612 dataset, outperforming larger models. Furthermore, when profiled on a laptop equipped with an Intel CPU, INT8 quantization achieves real-time deployment with a 6.75 ms inference latency.</li>
</ul>

<h3>Title: PromptTSS: A Prompting-Based Approach for Interactive Multi-Granularity Time Series Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Ching Chang, Ming-Chih Lo, Wen-Chih Peng, Tien-Fu Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11170">https://arxiv.org/abs/2506.11170</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11170">https://arxiv.org/pdf/2506.11170</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11170]] PromptTSS: A Prompting-Based Approach for Interactive Multi-Granularity Time Series Segmentation(https://arxiv.org/abs/2506.11170)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Multivariate time series data, collected across various fields such as manufacturing and wearable technology, exhibit states at multiple levels of granularity, from coarse-grained system behaviors to fine-grained, detailed events. Effectively segmenting and integrating states across these different granularities is crucial for tasks like predictive maintenance and performance optimization. However, existing time series segmentation methods face two key challenges: (1) the inability to handle multiple levels of granularity within a unified model, and (2) limited adaptability to new, evolving patterns in dynamic environments. To address these challenges, we propose PromptTSS, a novel framework for time series segmentation with multi-granularity states. PromptTSS uses a unified model with a prompting mechanism that leverages label and boundary information to guide segmentation, capturing both coarse- and fine-grained patterns while adapting dynamically to unseen patterns. Experiments show PromptTSS improves accuracy by 24.49% in multi-granularity segmentation, 17.88% in single-granularity segmentation, and up to 599.24% in transfer learning, demonstrating its adaptability to hierarchical states and evolving time series dynamics.</li>
</ul>

<h3>Title: Collapsing Sequence-Level Data-Policy Coverage via Poisoning Attack in Offline Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Xue Zhou, Dapeng Man, Chen Xu, Fanyi Zeng, Tao Liu, Huan Wang, Shucheng He, Chaoyang Gao, Wu Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11172">https://arxiv.org/abs/2506.11172</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11172">https://arxiv.org/pdf/2506.11172</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11172]] Collapsing Sequence-Level Data-Policy Coverage via Poisoning Attack in Offline Reinforcement Learning(https://arxiv.org/abs/2506.11172)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Offline reinforcement learning (RL) heavily relies on the coverage of pre-collected data over the target policy's distribution. Existing studies aim to improve data-policy coverage to mitigate distributional shifts, but overlook security risks from insufficient coverage, and the single-step analysis is not consistent with the multi-step decision-making nature of offline RL. To address this, we introduce the sequence-level concentrability coefficient to quantify coverage, and reveal its exponential amplification on the upper bound of estimation errors through theoretical analysis. Building on this, we propose the Collapsing Sequence-Level Data-Policy Coverage (CSDPC) poisoning attack. Considering the continuous nature of offline RL data, we convert state-action pairs into decision units, and extract representative decision patterns that capture multi-step behavior. We identify rare patterns likely to cause insufficient coverage, and poison them to reduce coverage and exacerbate distributional shifts. Experiments show that poisoning just 1% of the dataset can degrade agent performance by 90%. This finding provides new perspectives for analyzing and safeguarding the security of offline RL.</li>
</ul>

<h3>Title: BrainMAP: Multimodal Graph Learning For Efficient Brain Disease Localization</h3>
<ul>
<li><strong>Authors: </strong>Nguyen Linh Dan Le, Jing Ren, Ciyuan Peng, Chengyao Xie, Bowen Li, Feng Xia</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11178">https://arxiv.org/abs/2506.11178</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11178">https://arxiv.org/pdf/2506.11178</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11178]] BrainMAP: Multimodal Graph Learning For Efficient Brain Disease Localization(https://arxiv.org/abs/2506.11178)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent years have seen a surge in research focused on leveraging graph learning techniques to detect neurodegenerative diseases. However, existing graph-based approaches typically lack the ability to localize and extract the specific brain regions driving neurodegenerative pathology within the full connectome. Additionally, recent works on multimodal brain graph models often suffer from high computational complexity, limiting their practical use in resource-constrained devices. In this study, we present BrainMAP, a novel multimodal graph learning framework designed for precise and computationally efficient identification of brain regions affected by neurodegenerative diseases. First, BrainMAP utilizes an atlas-driven filtering approach guided by the AAL atlas to pinpoint and extract critical brain subgraphs. Unlike recent state-of-the-art methods, which model the entire brain network, BrainMAP achieves more than 50% reduction in computational overhead by concentrating on disease-relevant subgraphs. Second, we employ an advanced multimodal fusion process comprising cross-node attention to align functional magnetic resonance imaging (fMRI) and diffusion tensor imaging (DTI) data, coupled with an adaptive gating mechanism to blend and integrate these modalities dynamically. Experimental results demonstrate that BrainMAP outperforms state-of-the-art methods in computational efficiency, without compromising predictive accuracy.</li>
</ul>

<h3>Title: User Perceptions and Attitudes Toward Untraceability in Messaging Platforms</h3>
<ul>
<li><strong>Authors: </strong>Carla F. Griggio, Boel Nelson, Zefan Sramek, Aslan Askarov</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11212">https://arxiv.org/abs/2506.11212</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11212">https://arxiv.org/pdf/2506.11212</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11212]] User Perceptions and Attitudes Toward Untraceability in Messaging Platforms(https://arxiv.org/abs/2506.11212)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>Mainstream messaging platforms offer a variety of features designed to enhance user privacy, such as disappearing messages, password-protected chats, and end-to-end encryption (E2EE), which primarily protect message contents. Beyond contents, the transmission of messages generates metadata that can reveal who communicates with whom, when and how often. In this paper, we study user perceptions of "untraceability", i.e., preventing third parties from tracing who communicates with whom, with the goal of informing the design of privacy-enhancing features in messaging platforms and untraceable communication protocols that depend on large anonymity sets and widespread user adoption. We explore this from a broad conceptual standpoint: rather than studying mental models of a particular solution, we analyze how users reason about what features should be incorporated by two fictitious platforms, Texty and Chatty, to prevent third parties from knowing who communicates with whom. Through a vignette-based survey with 189 participants, we found that users associate the concept of untraceability with a wide range of privacy enhancing technologies, implying a diverse set of threat models. Overall, the features suggested by participants show awareness of privacy threats stemming from forms of surveillance and unauthorized access to message contents. Many participants also associated untraceability with the notion of anonymity, but interpreted it as senders and receivers concealing their identity from each other rather than only from third parties. We discuss the gap between users' perceptions of untraceability and the threat models addressed by untraceable communication protocols, as well as how different privacy attitudes point to challenges and opportunities for the adoption of untraceable communication tools in messaging platforms.</li>
</ul>

<h3>Title: Detection of obstructions in oil and gas pipelines: machine learning techniques for hydrate classification</h3>
<ul>
<li><strong>Authors: </strong>Hellockston Gomes de Brito, Carla Wilza Souza de Paula Maitelli, Osvaldo Chiavone-Filho</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11220">https://arxiv.org/abs/2506.11220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11220">https://arxiv.org/pdf/2506.11220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11220]] Detection of obstructions in oil and gas pipelines: machine learning techniques for hydrate classification(https://arxiv.org/abs/2506.11220)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Oil and gas reserves are vital resources for the global economy, serving as key components in transportation, energy production, and industrial processes. However, oil and gas extraction and production operations may encounter several challenges, such as pipeline and production line blockages, caused by factors including sediment accumulation, wax deposition, mineral scaling, and corrosion. This study addresses these challenges by employing supervised machine learning techniques, specifically decision trees, the k-Nearest Neighbors (k-NN) algorithm (k-NN), and the Naive Bayes classifier method, to detect and mitigate flow assurance challenges, ensuring efficient fluid transport. The primary focus is on preventing gas hydrate formation in oil production systems. To achieve this, data preprocessing and cleaning were conducted to ensure the quality and consistency of the dataset, which was sourced from Petrobras publicly available 3W project repository on GitHub. The scikit-learn Python library, a widely recognized open-source tool for supervised machine learning techniques, was utilized for classification tasks due to its robustness and versatility. The results demonstrate that the proposed methodology effectively classifies hydrate formation under operational conditions, with the decision tree algorithm exhibiting the highest predictive accuracy (99.99 percent). Consequently, this approach provides a reliable solution for optimizing production efficiency.</li>
</ul>

<h3>Title: uPVC-Net: A Universal Premature Ventricular Contraction Detection Deep Learning Algorithm</h3>
<ul>
<li><strong>Authors: </strong>Hagai Hamami, Yosef Solewicz, Daniel Zur, Yonatan Kleerekoper, Joachim A. Behar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11238">https://arxiv.org/abs/2506.11238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11238">https://arxiv.org/pdf/2506.11238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11238]] uPVC-Net: A Universal Premature Ventricular Contraction Detection Deep Learning Algorithm(https://arxiv.org/abs/2506.11238)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Introduction: Premature Ventricular Contractions (PVCs) are common cardiac arrhythmias originating from the ventricles. Accurate detection remains challenging due to variability in electrocardiogram (ECG) waveforms caused by differences in lead placement, recording conditions, and population demographics. Methods: We developed uPVC-Net, a universal deep learning model to detect PVCs from any single-lead ECG recordings. The model is developed on four independent ECG datasets comprising a total of 8.3 million beats collected from Holter monitors and a modern wearable ECG patch. uPVC-Net employs a custom architecture and a multi-source, multi-lead training strategy. For each experiment, one dataset is held out to evaluate out-of-distribution (OOD) generalization. Results: uPVC-Net achieved an AUC between 97.8% and 99.1% on the held-out datasets. Notably, performance on wearable single-lead ECG data reached an AUC of 99.1%. Conclusion: uPVC-Net exhibits strong generalization across diverse lead configurations and populations, highlighting its potential for robust, real-world clinical deployment.</li>
</ul>

<h3>Title: A Causal Lens for Learning Long-term Fair Policies</h3>
<ul>
<li><strong>Authors: </strong>Jacob Lear, Lu Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11242">https://arxiv.org/abs/2506.11242</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11242">https://arxiv.org/pdf/2506.11242</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11242]] A Causal Lens for Learning Long-term Fair Policies(https://arxiv.org/abs/2506.11242)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Fairness-aware learning studies the development of algorithms that avoid discriminatory decision outcomes despite biased training data. While most studies have concentrated on immediate bias in static contexts, this paper highlights the importance of investigating long-term fairness in dynamic decision-making systems while simultaneously considering instantaneous fairness requirements. In the context of reinforcement learning, we propose a general framework where long-term fairness is measured by the difference in the average expected qualification gain that individuals from different groups could this http URL, through a causal lens, we decompose this metric into three components that represent the direct impact, the delayed impact, as well as the spurious effect the policy has on the qualification gain. We analyze the intrinsic connection between these components and an emerging fairness notion called benefit fairness that aims to control the equity of outcomes in decision-making. Finally, we develop a simple yet effective approach for balancing various fairness notions.</li>
</ul>

<h3>Title: No Universal Prompt: Unifying Reasoning through Adaptive Prompting for Temporal Table Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Kushagra Dixit, Abhishek Rajgaria, Harshavardhan Kalalbandi, Dan Roth, Vivek Gupta</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11246">https://arxiv.org/abs/2506.11246</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11246">https://arxiv.org/pdf/2506.11246</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11246]] No Universal Prompt: Unifying Reasoning through Adaptive Prompting for Temporal Table Reasoning(https://arxiv.org/abs/2506.11246)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Temporal Table Reasoning is a critical challenge for Large Language Models (LLMs), requiring effective prompting techniques to extract relevant insights. Despite existence of multiple prompting methods, their impact on table reasoning remains largely unexplored. Furthermore, the performance of these models varies drastically across different table and context structures, making it difficult to determine an optimal approach. This work investigates multiple prompting technique across diverse table types to determine optimal approaches for different scenarios. We find that performance varies based on entity type, table structure, requirement of additional context and question complexity, with NO single method consistently outperforming others. To mitigate these challenges, we introduce SEAR, an adaptive prompting framework inspired by human reasoning that dynamically adjusts based on context characteristics and integrates a structured reasoning. Our results demonstrate that SEAR achieves superior performance across all table types compared to other baseline prompting techniques. Additionally, we explore the impact of table structure refactoring, finding that a unified representation enhances model's reasoning.</li>
</ul>

<h3>Title: Can Time-Series Foundation Models Perform Building Energy Management Tasks?</h3>
<ul>
<li><strong>Authors: </strong>Ozan Baris Mulayim, Pengrui Quan, Liying Han, Xiaomin Ouyang, Dezhi Hong, Mario Bergés, Mani Srivastava</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11250">https://arxiv.org/abs/2506.11250</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11250">https://arxiv.org/pdf/2506.11250</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11250]] Can Time-Series Foundation Models Perform Building Energy Management Tasks?(https://arxiv.org/abs/2506.11250)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Building energy management (BEM) tasks require processing and learning from a variety of time-series data. Existing solutions rely on bespoke task- and data-specific models to perform these tasks, limiting their broader applicability. Inspired by the transformative success of Large Language Models (LLMs), Time-Series Foundation Models (TSFMs), trained on diverse datasets, have the potential to change this. Were TSFMs to achieve a level of generalizability across tasks and contexts akin to LLMs, they could fundamentally address the scalability challenges pervasive in BEM. To understand where they stand today, we evaluate TSFMs across four dimensions: (1) generalizability in zero-shot univariate forecasting, (2) forecasting with covariates for thermal behavior modeling, (3) zero-shot representation learning for classification tasks, and (4) robustness to performance metrics and varying operational conditions. Our results reveal that TSFMs exhibit \emph{limited} generalizability, performing only marginally better than statistical models on unseen datasets and modalities for univariate forecasting. Similarly, inclusion of covariates in TSFMs does not yield performance improvements, and their performance remains inferior to conventional models that utilize covariates. While TSFMs generate effective zero-shot representations for downstream classification tasks, they may remain inferior to statistical models in forecasting when statistical models perform test-time fitting. Moreover, TSFMs forecasting performance is sensitive to evaluation metrics, and they struggle in more complex building environments compared to statistical models. These findings underscore the need for targeted advancements in TSFM design, particularly their handling of covariates and incorporating context and temporal dynamics into prediction mechanisms, to develop more adaptable and scalable solutions for BEM.</li>
</ul>

<h3>Title: Domain-Constrained Diffusion Models to Synthesize Tabular Data: A Case Study in Power Systems</h3>
<ul>
<li><strong>Authors: </strong>Milad Hoseinpour, Vladimir Dvorkin</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11281">https://arxiv.org/abs/2506.11281</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11281">https://arxiv.org/pdf/2506.11281</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11281]] Domain-Constrained Diffusion Models to Synthesize Tabular Data: A Case Study in Power Systems(https://arxiv.org/abs/2506.11281)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Growing concerns over privacy, security, and legal barriers are driving the rising demand for synthetic data across domains such as healthcare, finance, and energy. While generative models offer a promising solution to overcome these barriers, their utility depends on the incorporation of domain-specific knowledge. We propose to synthesize data using a guided diffusion model that integrates domain constraints directly into the generative process. We develop the model in the context of power systems, with potential applicability to other domains that involve tabular data. Specifically, we synthesize statistically representative and high-fidelity power flow datasets. To satisfy domain constraints, e.g., Kirchhoff laws, we introduce a gradient-based guidance to steer the sampling trajectory in a feasible direction. Numerical results demonstrate the effectiveness of our approach.</li>
</ul>

<h3>Title: TARDIS STRIDE: A Spatio-Temporal Road Image Dataset for Exploration and Autonomy</h3>
<ul>
<li><strong>Authors: </strong>Héctor Carrión, Yutong Bai, Víctor A. Hernández Castro, Kishan Panaganti, Ayush Zenith, Matthew Trang, Tony Zhang, Pietro Perona, Jitendra Malik</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11302">https://arxiv.org/abs/2506.11302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11302">https://arxiv.org/pdf/2506.11302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11302]] TARDIS STRIDE: A Spatio-Temporal Road Image Dataset for Exploration and Autonomy(https://arxiv.org/abs/2506.11302)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, generative</a></li>
<li><strong>Abstract: </strong>World models aim to simulate environments and enable effective agent behavior. However, modeling real-world environments presents unique challenges as they dynamically change across both space and, crucially, time. To capture these composed dynamics, we introduce a Spatio-Temporal Road Image Dataset for Exploration (STRIDE) permuting 360-degree panoramic imagery into rich interconnected observation, state and action nodes. Leveraging this structure, we can simultaneously model the relationship between egocentric views, positional coordinates, and movement commands across both space and time. We benchmark this dataset via TARDIS, a transformer-based generative world model that integrates spatial and temporal dynamics through a unified autoregressive framework trained on STRIDE. We demonstrate robust performance across a range of agentic tasks such as controllable photorealistic image synthesis, instruction following, autonomous self-control, and state-of-the-art georeferencing. These results suggest a promising direction towards sophisticated generalist agents--capable of understanding and manipulating the spatial and temporal aspects of their material environments--with enhanced embodied reasoning capabilities. Training code, datasets, and model checkpoints are made available at this https URL.</li>
</ul>

<h3>Title: Don't Pay Attention</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Hammoud, Devang Acharya</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11305">https://arxiv.org/abs/2506.11305</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11305">https://arxiv.org/pdf/2506.11305</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11305]] Don't Pay Attention(https://arxiv.org/abs/2506.11305)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>The Transformer has become the de facto standard for large language models and a wide range of downstream tasks across various domains. Despite its numerous advantages like inherent training parallelism, the Transformer still faces key challenges due to its inability to effectively process sequences beyond a fixed context window and the quadratic complexity of its attention mechanism. These challenges have renewed interest in RNN-like architectures, which offer linear scaling with sequence length and improved handling of long-range dependencies, albeit with limited parallelism due to their inherently recurrent nature. In this paper, we propose Avey, a new neural foundational architecture that breaks away from both attention and recurrence. Avey comprises a ranker and an autoregressive neural processor, which collaboratively identify and contextualize only the most relevant tokens for any given token, regardless of their positions in the sequence. Specifically, Avey decouples sequence length from context width, thus enabling effective processing of arbitrarily long sequences. Experimental results show that Avey compares favorably to the Transformer across a variety of standard short-range NLP benchmarks, while notably excelling at capturing long-range dependencies.</li>
</ul>

<h3>Title: HyBiomass: Global Hyperspectral Imagery Benchmark Dataset for Evaluating Geospatial Foundation Models in Forest Aboveground Biomass Estimation</h3>
<ul>
<li><strong>Authors: </strong>Aaron Banze, Timothée Stassin, Nassim Ait Ali Braham, Rıdvan Salih Kuzu, Simon Besnard, Michael Schmitt</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11314">https://arxiv.org/abs/2506.11314</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11314">https://arxiv.org/pdf/2506.11314</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11314]] HyBiomass: Global Hyperspectral Imagery Benchmark Dataset for Evaluating Geospatial Foundation Models in Forest Aboveground Biomass Estimation(https://arxiv.org/abs/2506.11314)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Comprehensive evaluation of geospatial foundation models (Geo-FMs) requires benchmarking across diverse tasks, sensors, and geographic regions. However, most existing benchmark datasets are limited to segmentation or classification tasks, and focus on specific geographic areas. To address this gap, we introduce a globally distributed dataset for forest aboveground biomass (AGB) estimation, a pixel-wise regression task. This benchmark dataset combines co-located hyperspectral imagery (HSI) from the Environmental Mapping and Analysis Program (EnMAP) satellite and predictions of AGB density estimates derived from the Global Ecosystem Dynamics Investigation lidars, covering seven continental regions. Our experimental results on this dataset demonstrate that the evaluated Geo-FMs can match or, in some cases, surpass the performance of a baseline U-Net, especially when fine-tuning the encoder. We also find that the performance difference between the U-Net and Geo-FMs depends on the dataset size for each region and highlight the importance of the token patch size in the Vision Transformer backbone for accurate predictions in pixel-wise regression tasks. By releasing this globally distributed hyperspectral benchmark dataset, we aim to facilitate the development and evaluation of Geo-FMs for HSI applications. Leveraging this dataset additionally enables research into geographic bias and generalization capacity of Geo-FMs. The dataset and source code will be made publicly available.</li>
</ul>

<h3>Title: Uncovering Reliable Indicators: Improving IoC Extraction from Threat Reports</h3>
<ul>
<li><strong>Authors: </strong>Evangelos Froudakis, Athanasios Avgetidis, Sean Tyler Frankum, Roberto Perdisci, Manos Antonakakis, Angelos Keromytis</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11325">https://arxiv.org/abs/2506.11325</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11325">https://arxiv.org/pdf/2506.11325</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11325]] Uncovering Reliable Indicators: Improving IoC Extraction from Threat Reports(https://arxiv.org/abs/2506.11325)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, fair, large language model</a></li>
<li><strong>Abstract: </strong>Indicators of Compromise (IoCs) are critical for threat detection and response, marking malicious activity across networks and systems. Yet, the effectiveness of automated IoC extraction systems is fundamentally limited by one key issue: the lack of high-quality ground truth. Current extraction tools rely either on manually extracted ground truth, which is labor-intensive and costly, or on automated ground truth creation methods that include non-malicious artifacts, leading to inflated false positive (FP) rates and unreliable threat intelligence. In this work, we analyze the shortcomings of existing ground truth creation strategies and address them by introducing the first hybrid human-in-the-loop pipeline for IoC extraction, which combines a large language model-based classifier (LANCE) with expert analyst validation. Our system improves precision through explainable, context-aware labeling and reduces analysts' work factor by 43% compared to manual annotation, as demonstrated in our evaluation with six analysts. Using this approach, we produce PRISM, a high-quality, publicly available benchmark of 1,791 labeled IoCs from 50 real-world threat reports. PRISM supports both fair evaluation and training of IoC extraction methods and enables reproducible research grounded in expert-validated indicators.</li>
</ul>

<h3>Title: An Attention-based Spatio-Temporal Neural Operator for Evolving Physics</h3>
<ul>
<li><strong>Authors: </strong>Vispi Karkaria, Doksoo Lee, Yi-Ping Chen, Yue Yu, Wei Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11328">https://arxiv.org/abs/2506.11328</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11328">https://arxiv.org/pdf/2506.11328</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11328]] An Attention-based Spatio-Temporal Neural Operator for Evolving Physics(https://arxiv.org/abs/2506.11328)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>In scientific machine learning (SciML), a key challenge is learning unknown, evolving physical processes and making predictions across spatio-temporal scales. For example, in real-world manufacturing problems like additive manufacturing, users adjust known machine settings while unknown environmental parameters simultaneously fluctuate. To make reliable predictions, it is desired for a model to not only capture long-range spatio-temporal interactions from data but also adapt to new and unknown environments; traditional machine learning models excel at the first task but often lack physical interpretability and struggle to generalize under varying environmental conditions. To tackle these challenges, we propose the Attention-based Spatio-Temporal Neural Operator (ASNO), a novel architecture that combines separable attention mechanisms for spatial and temporal interactions and adapts to unseen physical parameters. Inspired by the backward differentiation formula (BDF), ASNO learns a transformer for temporal prediction and extrapolation and an attention-based neural operator for handling varying external loads, enhancing interpretability by isolating historical state contributions and external forces, enabling the discovery of underlying physical laws and generalizability to unseen physical environments. Empirical results on SciML benchmarks demonstrate that ASNO outperforms over existing models, establishing its potential for engineering applications, physics discovery, and interpretable machine learning.</li>
</ul>

<h3>Title: Surprisal from Larger Transformer-based Language Models Predicts fMRI Data More Poorly</h3>
<ul>
<li><strong>Authors: </strong>Yi-Chien Lin, William Schuler</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11338">https://arxiv.org/abs/2506.11338</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11338">https://arxiv.org/pdf/2506.11338</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11338]] Surprisal from Larger Transformer-based Language Models Predicts fMRI Data More Poorly(https://arxiv.org/abs/2506.11338)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>As Transformers become more widely incorporated into natural language processing tasks, there has been considerable interest in using surprisal from these models as predictors of human sentence processing difficulty. Recent work has observed a positive relationship between Transformer-based models' perplexity and the predictive power of their surprisal estimates on reading times, showing that language models with more parameters and trained on more data are less predictive of human reading times. However, these studies focus on predicting latency-based measures (i.e., self-paced reading times and eye-gaze durations) with surprisal estimates from Transformer-based language models. This trend has not been tested on brain imaging data. This study therefore evaluates the predictive power of surprisal estimates from 17 pre-trained Transformer-based models across three different language families on two functional magnetic resonance imaging datasets. Results show that the positive relationship between model perplexity and model fit still obtains, suggesting that this trend is not specific to latency-based measures and can be generalized to neural measures.</li>
</ul>

<h3>Title: From Replication to Redesign: Exploring Pairwise Comparisons for LLM-Based Peer Review</h3>
<ul>
<li><strong>Authors: </strong>Yaohui Zhang, Haijing Zhang, Wenlong Ji, Tianyu Hua, Nick Haber, Hancheng Cao, Weixin Liang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11343">https://arxiv.org/abs/2506.11343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11343">https://arxiv.org/pdf/2506.11343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11343]] From Replication to Redesign: Exploring Pairwise Comparisons for LLM-Based Peer Review(https://arxiv.org/abs/2506.11343)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The advent of large language models (LLMs) offers unprecedented opportunities to reimagine peer review beyond the constraints of traditional workflows. Despite these opportunities, prior efforts have largely focused on replicating traditional review workflows with LLMs serving as direct substitutes for human reviewers, while limited attention has been given to exploring new paradigms that fundamentally rethink how LLMs can participate in the academic review process. In this paper, we introduce and explore a novel mechanism that employs LLM agents to perform pairwise comparisons among manuscripts instead of individual scoring. By aggregating outcomes from substantial pairwise evaluations, this approach enables a more accurate and robust measure of relative manuscript quality. Our experiments demonstrate that this comparative approach significantly outperforms traditional rating-based methods in identifying high-impact papers. However, our analysis also reveals emergent biases in the selection process, notably a reduced novelty in research topics and an increased institutional imbalance. These findings highlight both the transformative potential of rethinking peer review with LLMs and critical challenges that future systems must address to ensure equity and diversity.</li>
</ul>

<h3>Title: Improving Group Robustness on Spurious Correlation via Evidential Alignment</h3>
<ul>
<li><strong>Authors: </strong>Wenqian Ye, Guangtao Zheng, Aidong Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11347">https://arxiv.org/abs/2506.11347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11347">https://arxiv.org/pdf/2506.11347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11347]] Improving Group Robustness on Spurious Correlation via Evidential Alignment(https://arxiv.org/abs/2506.11347)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep neural networks often learn and rely on spurious correlations, i.e., superficial associations between non-causal features and the targets. For instance, an image classifier may identify camels based on the desert backgrounds. While it can yield high overall accuracy during training, it degrades generalization on more diverse scenarios where such correlations do not hold. This problem poses significant challenges for out-of-distribution robustness and trustworthiness. Existing methods typically mitigate this issue by using external group annotations or auxiliary deterministic models to learn unbiased representations. However, such information is costly to obtain, and deterministic models may fail to capture the full spectrum of biases learned by the models. To address these limitations, we propose Evidential Alignment, a novel framework that leverages uncertainty quantification to understand the behavior of the biased models without requiring group annotations. By quantifying the evidence of model prediction with second-order risk minimization and calibrating the biased models with the proposed evidential calibration technique, Evidential Alignment identifies and suppresses spurious correlations while preserving core features. We theoretically justify the effectiveness of our method as capable of learning the patterns of biased models and debiasing the model without requiring any spurious correlation annotations. Empirical results demonstrate that our method significantly improves group robustness across diverse architectures and data modalities, providing a scalable and principled solution to spurious correlations.</li>
</ul>

<h3>Title: GynSurg: A Comprehensive Gynecology Laparoscopic Surgery Dataset</h3>
<ul>
<li><strong>Authors: </strong>Sahar Nasirihaghighi, Negin Ghamsarian, Leonie Peschek, Matteo Munari, Heinrich Husslein, Raphael Sznitman, Klaus Schoeffmann</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11356">https://arxiv.org/abs/2506.11356</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11356">https://arxiv.org/pdf/2506.11356</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11356]] GynSurg: A Comprehensive Gynecology Laparoscopic Surgery Dataset(https://arxiv.org/abs/2506.11356)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Recent advances in deep learning have transformed computer-assisted intervention and surgical video analysis, driving improvements not only in surgical training, intraoperative decision support, and patient outcomes, but also in postoperative documentation and surgical discovery. Central to these developments is the availability of large, high-quality annotated datasets. In gynecologic laparoscopy, surgical scene understanding and action recognition are fundamental for building intelligent systems that assist surgeons during operations and provide deeper analysis after surgery. However, existing datasets are often limited by small scale, narrow task focus, or insufficiently detailed annotations, limiting their utility for comprehensive, end-to-end workflow analysis. To address these limitations, we introduce GynSurg, the largest and most diverse multi-task dataset for gynecologic laparoscopic surgery to date. GynSurg provides rich annotations across multiple tasks, supporting applications in action recognition, semantic segmentation, surgical documentation, and discovery of novel procedural insights. We demonstrate the dataset quality and versatility by benchmarking state-of-the-art models under a standardized training protocol. To accelerate progress in the field, we publicly release the GynSurg dataset and its annotations</li>
</ul>

<h3>Title: The Biased Samaritan: LLM biases in Perceived Kindness</h3>
<ul>
<li><strong>Authors: </strong>Jack H Fagan, Ruhaan Juyaal, Amy Yue-Ming Yu, Siya Pun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11361">https://arxiv.org/abs/2506.11361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11361">https://arxiv.org/pdf/2506.11361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11361]] The Biased Samaritan: LLM biases in Perceived Kindness(https://arxiv.org/abs/2506.11361)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>While Large Language Models (LLMs) have become ubiquitous in many fields, understanding and mitigating LLM biases is an ongoing issue. This paper provides a novel method for evaluating the demographic biases of various generative AI models. By prompting models to assess a moral patient's willingness to intervene constructively, we aim to quantitatively evaluate different LLMs' biases towards various genders, races, and ages. Our work differs from existing work by aiming to determine the baseline demographic identities for various commercial models and the relationship between the baseline and other demographics. We strive to understand if these biases are positive, neutral, or negative, and the strength of these biases. This paper can contribute to the objective assessment of bias in Large Language Models and give the user or developer the power to account for these biases in LLM output or in training future LLMs. Our analysis suggested two key findings: that models view the baseline demographic as a white middle-aged or young adult male; however, a general trend across models suggested that non-baseline demographics are more willing to help than the baseline. These methodologies allowed us to distinguish these two biases that are often tangled together.</li>
</ul>

<h3>Title: EDN: A Novel Edge-Dependent Noise Model for Graph Data</h3>
<ul>
<li><strong>Authors: </strong>Pintu Kumar, Nandyala Hemachandra</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11368">https://arxiv.org/abs/2506.11368</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11368">https://arxiv.org/pdf/2506.11368</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11368]] EDN: A Novel Edge-Dependent Noise Model for Graph Data(https://arxiv.org/abs/2506.11368)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>An important structural feature of a graph is its set of edges, as it captures the relationships among the nodes (the graph's topology). Existing node label noise models like Symmetric Label Noise (SLN) and Class Conditional Noise (CCN) disregard this important node relationship in graph data; and the Edge-Dependent Noise (EDN) model addresses this limitation. EDN posits that in real-world scenarios, label noise may be influenced by the connections between nodes. We explore three variants of EDN. A crucial notion that relates nodes and edges in a graph is the degree of a node; we show that in all three variants, the probability of a node's label corruption is dependent on its degree. Additionally, we compare the dependence of these probabilities on node degree across different variants. We performed experiments on popular graph datasets using 5 different GNN architectures and 8 noise robust algorithms for graph data. The results demonstrate that 2 variants of EDN lead to greater performance degradation in both Graph Neural Networks (GNNs) and existing noise-robust algorithms, as compared to traditional node label noise models. We statistically verify this by posing a suitable hypothesis-testing problem. This emphasizes the importance of incorporating EDN when evaluating noise robust algorithms for graphs, to enhance the reliability of graph-based learning in noisy environments.</li>
</ul>

<h3>Title: A Watermark for Auto-Regressive Image Generation Models</h3>
<ul>
<li><strong>Authors: </strong>Yihan Wu, Xuehao Cui, Ruibo Chen, Georgios Milis, Heng Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11371">https://arxiv.org/abs/2506.11371</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11371">https://arxiv.org/pdf/2506.11371</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11371]] A Watermark for Auto-Regressive Image Generation Models(https://arxiv.org/abs/2506.11371)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, attack, robust, watermark</a></li>
<li><strong>Abstract: </strong>The rapid evolution of image generation models has revolutionized visual content creation, enabling the synthesis of highly realistic and contextually accurate images for diverse applications. However, the potential for misuse, such as deepfake generation, image based phishing attacks, and fabrication of misleading visual evidence, underscores the need for robust authenticity verification mechanisms. While traditional statistical watermarking techniques have proven effective for autoregressive language models, their direct adaptation to image generation models encounters significant challenges due to a phenomenon we term retokenization mismatch, a disparity between original and retokenized sequences during the image generation process. To overcome this limitation, we propose C-reweight, a novel, distortion-free watermarking method explicitly designed for image generation models. By leveraging a clustering-based strategy that treats tokens within the same cluster equivalently, C-reweight mitigates retokenization mismatch while preserving image fidelity. Extensive evaluations on leading image generation platforms reveal that C-reweight not only maintains the visual quality of generated images but also improves detectability over existing distortion-free watermarking techniques, setting a new standard for secure and trustworthy image synthesis.</li>
</ul>

<h3>Title: The Effect of Stochasticity in Score-Based Diffusion Sampling: a KL Divergence Analysis</h3>
<ul>
<li><strong>Authors: </strong>Bernardo P. Schaeffer, Ricardo M. S. Rosa, Glauco Valle</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11378">https://arxiv.org/abs/2506.11378</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11378">https://arxiv.org/pdf/2506.11378</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11378]] The Effect of Stochasticity in Score-Based Diffusion Sampling: a KL Divergence Analysis(https://arxiv.org/abs/2506.11378)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Sampling in score-based diffusion models can be performed by solving either a probability flow ODE or a reverse-time stochastic differential equation (SDE) parameterized by an arbitrary stochasticity parameter. In this work, we study the effect of stochasticity on the generation process through bounds on the Kullback-Leibler (KL) divergence and complement the analysis with numerical and analytical examples. Our results apply to general forward SDEs with additive noise and Lipschitz-continuous score functions, and quantify how errors from the prior distribution and score approximation propagate under different choices of the stochasticity parameter. The theoretical bounds are derived using log-Sobolev inequalities for the marginals of the forward process, which enable a more effective control of the KL divergence decay along sampling. For exact score functions, we find that stochasticity acts as an error-correcting mechanism, decreasing KL divergence along the sampling trajectory. For an approximate score function, there is a trade-off between error correction and score error amplification, so that stochasticity can either improve or worsen the performance, depending on the structure of the score error. Numerical experiments on simple datasets and a fully analytical example are included to illustrate and enlighten the theoretical results.</li>
</ul>

<h3>Title: A Variational Approach for Mitigating Entity Bias in Relation Extraction</h3>
<ul>
<li><strong>Authors: </strong>Samuel Mensah, Elena Kochkina, Jabez Magomere, Joy Prakash Sain, Simerjot Kaur, Charese Smiley</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11381">https://arxiv.org/abs/2506.11381</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11381">https://arxiv.org/pdf/2506.11381</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11381]] A Variational Approach for Mitigating Entity Bias in Relation Extraction(https://arxiv.org/abs/2506.11381)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Mitigating entity bias is a critical challenge in Relation Extraction (RE), where models often rely excessively on entities, resulting in poor generalization. This paper presents a novel approach to address this issue by adapting a Variational Information Bottleneck (VIB) framework. Our method compresses entity-specific information while preserving task-relevant features. It achieves state-of-the-art performance on relation extraction datasets across general, financial, and biomedical domains, in both indomain (original test sets) and out-of-domain (modified test sets with type-constrained entity replacements) settings. Our approach offers a robust, interpretable, and theoretically grounded methodology.</li>
</ul>

<h3>Title: Curriculum-Guided Layer Scaling for Language Model Pretraining</h3>
<ul>
<li><strong>Authors: </strong>Karanpartap Singh, Neil Band, Ehsan Adeli</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11389">https://arxiv.org/abs/2506.11389</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11389">https://arxiv.org/pdf/2506.11389</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11389]] Curriculum-Guided Layer Scaling for Language Model Pretraining(https://arxiv.org/abs/2506.11389)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As the cost of pretraining large language models grows, there is continued interest in strategies to improve learning efficiency during this core training stage. Motivated by cognitive development, where humans gradually build knowledge as their brains mature, we propose Curriculum-Guided Layer Scaling (CGLS), a framework for compute-efficient pretraining that synchronizes increasing data difficulty with model growth through progressive layer stacking (i.e. gradually adding layers during training). At the 100M parameter scale, using a curriculum transitioning from synthetic short stories to general web data, CGLS outperforms baseline methods on the question-answering benchmarks PIQA and ARC. Pretraining at the 1.2B scale, we stratify the DataComp-LM corpus with a DistilBERT-based classifier and progress from general text to highly technical or specialized content. Our results show that progressively increasing model depth alongside sample difficulty leads to better generalization and zero-shot performance on various downstream benchmarks. Altogether, our findings demonstrate that CGLS unlocks the potential of progressive stacking, offering a simple yet effective strategy for improving generalization on knowledge-intensive and reasoning tasks.</li>
</ul>

<h3>Title: FIGNN: Feature-Specific Interpretability for Graph Neural Network Surrogate Models</h3>
<ul>
<li><strong>Authors: </strong>Riddhiman Raut, Romit Maulik, Shivam Barwey</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.flu-dyn</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11398">https://arxiv.org/abs/2506.11398</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11398">https://arxiv.org/pdf/2506.11398</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11398]] FIGNN: Feature-Specific Interpretability for Graph Neural Network Surrogate Models(https://arxiv.org/abs/2506.11398)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>This work presents a novel graph neural network (GNN) architecture, the Feature-specific Interpretable Graph Neural Network (FIGNN), designed to enhance the interpretability of deep learning surrogate models defined on unstructured grids in scientific applications. Traditional GNNs often obscure the distinct spatial influences of different features in multivariate prediction tasks. FIGNN addresses this limitation by introducing a feature-specific pooling strategy, which enables independent attribution of spatial importance for each predicted variable. Additionally, a mask-based regularization term is incorporated into the training objective to explicitly encourage alignment between interpretability and predictive error, promoting localized attribution of model performance. The method is evaluated for surrogate modeling of two physically distinct systems: the SPEEDY atmospheric circulation model and the backward-facing step (BFS) fluid dynamics benchmark. Results demonstrate that FIGNN achieves competitive predictive performance while revealing physically meaningful spatial patterns unique to each feature. Analysis of rollout stability, feature-wise error budgets, and spatial mask overlays confirm the utility of FIGNN as a general-purpose framework for interpretable surrogate modeling in complex physical domains.</li>
</ul>

<h3>Title: LoRA Users Beware: A Few Spurious Tokens Can Manipulate Your Finetuned Model</h3>
<ul>
<li><strong>Authors: </strong>Pradyut Sekhsaria, Marcel Mateos Salles, Hai Huang, Randall Balestriero</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11402">https://arxiv.org/abs/2506.11402</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11402">https://arxiv.org/pdf/2506.11402</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11402]] LoRA Users Beware: A Few Spurious Tokens Can Manipulate Your Finetuned Model(https://arxiv.org/abs/2506.11402)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Parameter Efficient FineTuning (PEFT), such as Low-Rank Adaptation (LoRA), aligns pre-trained Large Language Models (LLMs) to particular downstream tasks in a resource-efficient manner. Because efficiency has been the main metric of progress, very little attention has been put in understanding possible catastrophic failures. We uncover one such failure: PEFT encourages a model to search for shortcut solutions to solve its fine-tuning tasks. When very small amount of tokens, e.g., one token per prompt, are correlated with downstream task classes, PEFT makes any pretrained model rely predominantly on that token for decision making. While such spurious tokens may emerge accidentally from incorrect data cleaning, it also opens opportunities for malevolent parties to control a model's behavior from Seamless Spurious Token Injection (SSTI). In SSTI, a small amount of tokens correlated with downstream classes are injected by the dataset creators. At test time, the finetuned LLM's behavior can be controlled solely by injecting those few tokens. We apply SSTI across models from three families (Snowflake Arctic, Apple OpenELM, and Meta LLaMA-3) and four diverse datasets (IMDB, Financial Classification, CommonSense QA, and Bias in Bios). Our findings reveal three astonishing behaviors. First, as few as a single token of SSTI is sufficient to steer a model's decision making. Second, for light SSTI, the reliance on spurious tokens is proportional to the LoRA rank. Lastly, with aggressive SSTI, larger LoRA rank values become preferable to small rank values as it makes the model attend to non-spurious tokens, hence improving robustness.</li>
</ul>

<h3>Title: Predicting Early-Onset Colorectal Cancer with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wilson Lau, Youngwon Kim, Sravanthi Parasa, Md Enamul Haque, Anand Oka, Jay Nanduri</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11410">https://arxiv.org/abs/2506.11410</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11410">https://arxiv.org/pdf/2506.11410</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11410]] Predicting Early-Onset Colorectal Cancer with Large Language Models(https://arxiv.org/abs/2506.11410)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The incidence rate of early-onset colorectal cancer (EoCRC, age < 45) has increased every year, but this population is younger than the recommended age established by national guidelines for cancer screening. In this paper, we applied 10 different machine learning models to predict EoCRC, and compared their performance with advanced large language models (LLM), using patient conditions, lab results, and observations within 6 months of patient journey prior to the CRC diagnoses. We retrospectively identified 1,953 CRC patients from multiple health systems across the United States. The results demonstrated that the fine-tuned LLM achieved an average of 73% sensitivity and 91% specificity.</li>
</ul>

<h3>Title: Byzantine Outside, Curious Inside: Reconstructing Data Through Malicious Updates</h3>
<ul>
<li><strong>Authors: </strong>Kai Yue, Richeng Jin, Chau-Wai Wong, Huaiyu Dai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11413">https://arxiv.org/abs/2506.11413</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11413">https://arxiv.org/pdf/2506.11413</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11413]] Byzantine Outside, Curious Inside: Reconstructing Data Through Malicious Updates(https://arxiv.org/abs/2506.11413)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, defense, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) enables decentralized machine learning without sharing raw data, allowing multiple clients to collaboratively learn a global model. However, studies reveal that privacy leakage is possible under commonly adopted FL protocols. In particular, a server with access to client gradients can synthesize data resembling the clients' training data. In this paper, we introduce a novel threat model in FL, named the maliciously curious client, where a client manipulates its own gradients with the goal of inferring private data from peers. This attacker uniquely exploits the strength of a Byzantine adversary, traditionally aimed at undermining model robustness, and repurposes it to facilitate data reconstruction attack. We begin by formally defining this novel client-side threat model and providing a theoretical analysis that demonstrates its ability to achieve significant reconstruction success during FL training. To demonstrate its practical impact, we further develop a reconstruction algorithm that combines gradient inversion with malicious update strategies. Our analysis and experimental results reveal a critical blind spot in FL defenses: both server-side robust aggregation and client-side privacy mechanisms may fail against our proposed attack. Surprisingly, standard server- and client-side defenses designed to enhance robustness or privacy may unintentionally amplify data leakage. Compared to the baseline approach, a mistakenly used defense may instead improve the reconstructed image quality by 10-15%.</li>
</ul>

<h3>Title: Bias Amplification in RAG: Poisoning Knowledge Retrieval to Steer LLMs</h3>
<ul>
<li><strong>Authors: </strong>Linlin Wang, Tianqing Zhu, Laiqiao Qin, Longxiang Gao, Wanlei Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11415">https://arxiv.org/abs/2506.11415</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11415">https://arxiv.org/pdf/2506.11415</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11415]] Bias Amplification in RAG: Poisoning Knowledge Retrieval to Steer LLMs(https://arxiv.org/abs/2506.11415)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, fair, large language model</a></li>
<li><strong>Abstract: </strong>In Large Language Models, Retrieval-Augmented Generation (RAG) systems can significantly enhance the performance of large language models by integrating external knowledge. However, RAG also introduces new security risks. Existing research focuses mainly on how poisoning attacks in RAG systems affect model output quality, overlooking their potential to amplify model biases. For example, when querying about domestic violence victims, a compromised RAG system might preferentially retrieve documents depicting women as victims, causing the model to generate outputs that perpetuate gender stereotypes even when the original query is gender neutral. To show the impact of the bias, this paper proposes a Bias Retrieval and Reward Attack (BRRA) framework, which systematically investigates attack pathways that amplify language model biases through a RAG system manipulation. We design an adversarial document generation method based on multi-objective reward functions, employ subspace projection techniques to manipulate retrieval results, and construct a cyclic feedback mechanism for continuous bias amplification. Experiments on multiple mainstream large language models demonstrate that BRRA attacks can significantly enhance model biases in dimensions. In addition, we explore a dual stage defense mechanism to effectively mitigate the impacts of the attack. This study reveals that poisoning attacks in RAG systems directly amplify model output biases and clarifies the relationship between RAG system security and model fairness. This novel potential attack indicates that we need to keep an eye on the fairness issues of the RAG system.</li>
</ul>

<h3>Title: Stop learning it all to mitigate visual hallucination, Focus on the hallucination target</h3>
<ul>
<li><strong>Authors: </strong>Dokyoon Yoon, Youngsook Song, Woomyong Park</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11417">https://arxiv.org/abs/2506.11417</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11417">https://arxiv.org/pdf/2506.11417</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11417]] Stop learning it all to mitigate visual hallucination, Focus on the hallucination target(https://arxiv.org/abs/2506.11417)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) frequently suffer from hallucination issues, generating information about objects that are not present in input images during vision-language tasks. These hallucinations particularly undermine model reliability in practical applications requiring accurate object identification. To address this challenge, we propose \mymethod,\ a preference learning approach that mitigates hallucinations by focusing on targeted areas where they occur. To implement this, we build a dataset containing hallucinated responses, correct responses, and target information (i.e., objects present in the images and the corresponding chunk positions in responses affected by hallucinations). By applying a preference learning method restricted to these specific targets, the model can filter out irrelevant signals and focus on correcting hallucinations. This allows the model to produce more factual responses by concentrating solely on relevant information. Experimental results demonstrate that \mymethod\ effectively reduces hallucinations across multiple vision hallucination tasks, improving the reliability and performance of MLLMs without diminishing overall performance.</li>
</ul>

<h3>Title: Efficient Long-Context LLM Inference via KV Cache Clustering</h3>
<ul>
<li><strong>Authors: </strong>Jie Hu, Shengnan Wang, Yutong He, Ping Gong, Jiawei Yi, Juncheng Zhang, Youhui Bai, Renhai Chen, Gong Zhang, Cheng Li, Kun Yuan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11418">https://arxiv.org/abs/2506.11418</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11418">https://arxiv.org/pdf/2506.11418</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11418]] Efficient Long-Context LLM Inference via KV Cache Clustering(https://arxiv.org/abs/2506.11418)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) with extended context windows have become increasingly prevalent for tackling complex tasks. However, the substantial Key-Value (KV) cache required for long-context LLMs poses significant deployment challenges. Existing approaches either discard potentially critical information needed for future generations or offer limited efficiency gains due to high computational overhead. In this paper, we introduce Chelsea, a simple yet effective framework for online KV cache clustering. Our approach is based on the observation that key states exhibit high similarity along the sequence dimension. To enable efficient clustering, we divide the sequence into chunks and propose Chunked Soft Matching, which employs an alternating partition strategy within each chunk and identifies clusters based on similarity. Chelsea then merges the KV cache within each cluster into a single centroid. Additionally, we provide a theoretical analysis of the computational complexity and the optimality of the intra-chunk partitioning strategy. Extensive experiments across various models and long-context benchmarks demonstrate that Chelsea achieves up to 80% reduction in KV cache memory usage while maintaining comparable model performance. Moreover, with minimal computational overhead, Chelsea accelerates the decoding stage of inference by up to 3.19$\times$ and reduces end-to-end latency by up to 2.72$\times$.</li>
</ul>

<h3>Title: PPDiff: Diffusing in Hybrid Sequence-Structure Space for Protein-Protein Complex Design</h3>
<ul>
<li><strong>Authors: </strong>Zhenqiao Song, Tiaoxiao Li, Lei Li, Martin Renqiang Min</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11420">https://arxiv.org/abs/2506.11420</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11420">https://arxiv.org/pdf/2506.11420</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11420]] PPDiff: Diffusing in Hybrid Sequence-Structure Space for Protein-Protein Complex Design(https://arxiv.org/abs/2506.11420)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Designing protein-binding proteins with high affinity is critical in biomedical research and biotechnology. Despite recent advancements targeting specific proteins, the ability to create high-affinity binders for arbitrary protein targets on demand, without extensive rounds of wet-lab testing, remains a significant challenge. Here, we introduce PPDiff, a diffusion model to jointly design the sequence and structure of binders for arbitrary protein targets in a non-autoregressive manner. PPDiffbuilds upon our developed Sequence Structure Interleaving Network with Causal attention layers (SSINC), which integrates interleaved self-attention layers to capture global amino acid correlations, k-nearest neighbor (kNN) equivariant graph layers to model local interactions in three-dimensional (3D) space, and causal attention layers to simplify the intricate interdependencies within the protein sequence. To assess PPDiff, we curate PPBench, a general protein-protein complex dataset comprising 706,360 complexes from the Protein Data Bank (PDB). The model is pretrained on PPBenchand finetuned on two real-world applications: target-protein mini-binder complex design and antigen-antibody complex design. PPDiffconsistently surpasses baseline methods, achieving success rates of 50.00%, 23.16%, and 16.89% for the pretraining task and the two downstream applications, respectively.</li>
</ul>

<h3>Title: Agent-RLVR: Training Software Engineering Agents via Guidance and Environment Rewards</h3>
<ul>
<li><strong>Authors: </strong>Jeff Da, Clinton Wang, Xiang Deng, Yuntao Ma, Nikhil Barhate, Sean Hendryx</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11425">https://arxiv.org/abs/2506.11425</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11425">https://arxiv.org/pdf/2506.11425</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11425]] Agent-RLVR: Training Software Engineering Agents via Guidance and Environment Rewards(https://arxiv.org/abs/2506.11425)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning from Verifiable Rewards (RLVR) has been widely adopted as the de facto method for enhancing the reasoning capabilities of large language models and has demonstrated notable success in verifiable domains like math and competitive programming tasks. However, the efficacy of RLVR diminishes significantly when applied to agentic environments. These settings, characterized by multi-step, complex problem solving, lead to high failure rates even for frontier LLMs, as the reward landscape is too sparse for effective model training via conventional RLVR. In this work, we introduce Agent-RLVR, a framework that makes RLVR effective in challenging agentic settings, with an initial focus on software engineering tasks. Inspired by human pedagogy, Agent-RLVR introduces agent guidance, a mechanism that actively steers the agent towards successful trajectories by leveraging diverse informational cues. These cues, ranging from high-level strategic plans to dynamic feedback on the agent's errors and environmental interactions, emulate a teacher's guidance, enabling the agent to navigate difficult solution spaces and promotes active self-improvement via additional environment exploration. In the Agent-RLVR training loop, agents first attempt to solve tasks to produce initial trajectories, which are then validated by unit tests and supplemented with agent guidance. Agents then reattempt with guidance, and the agent policy is updated with RLVR based on the rewards of these guided trajectories. Agent-RLVR elevates the pass@1 performance of Qwen-2.5-72B-Instruct from 9.4% to 22.4% on SWE-Bench Verified. We find that our guidance-augmented RLVR data is additionally useful for test-time reward model training, shown by further boosting pass@1 to 27.8%. Agent-RLVR lays the groundwork for training agents with RLVR in complex, real-world environments where conventional RL methods struggle.</li>
</ul>

<h3>Title: TruncQuant: Truncation-Ready Quantization for DNNs with Flexible Weight Bit Precision</h3>
<ul>
<li><strong>Authors: </strong>Jinhee Kim, Seoyeon Yoon, Taeho Lee, Joo Chan Lee, Kang Eun Jeon, Jong Hwan Ko</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11431">https://arxiv.org/abs/2506.11431</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11431">https://arxiv.org/pdf/2506.11431</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11431]] TruncQuant: Truncation-Ready Quantization for DNNs with Flexible Weight Bit Precision(https://arxiv.org/abs/2506.11431)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The deployment of deep neural networks on edge devices is a challenging task due to the increasing complexity of state-of-the-art models, requiring efforts to reduce model size and inference latency. Recent studies explore models operating at diverse quantization settings to find the optimal point that balances computational efficiency and accuracy. Truncation, an effective approach for achieving lower bit precision mapping, enables a single model to adapt to various hardware platforms with little to no cost. However, formulating a training scheme for deep neural networks to withstand the associated errors introduced by truncation remains a challenge, as the current quantization-aware training schemes are not designed for the truncation process. We propose TruncQuant, a novel truncation-ready training scheme allowing flexible bit precision through bit-shifting in runtime. We achieve this by aligning TruncQuant with the output of the truncation process, demonstrating strong robustness across bit-width settings, and offering an easily implementable training scheme within existing quantization-aware frameworks. Our code is released at this https URL.</li>
</ul>

<h3>Title: KoGEC : Korean Grammatical Error Correction with Pre-trained Translation Models</h3>
<ul>
<li><strong>Authors: </strong>Taeeun Kim, Semin Jeong, Youngsook Song</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11432">https://arxiv.org/abs/2506.11432</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11432">https://arxiv.org/pdf/2506.11432</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11432]] KoGEC : Korean Grammatical Error Correction with Pre-trained Translation Models(https://arxiv.org/abs/2506.11432)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This research introduces KoGEC, a Korean Grammatical Error Correction system using pre\--trained translation models. We fine-tuned NLLB (No Language Left Behind) models for Korean GEC, comparing their performance against large language models like GPT-4 and HCX-3. The study used two social media conversation datasets for training and testing. The NLLB models were fine-tuned using special language tokens to distinguish between original and corrected Korean sentences. Evaluation was done using BLEU scores and an "LLM as judge" method to classify error types. Results showed that the fine-tuned NLLB (KoGEC) models outperformed GPT-4o and HCX-3 in Korean GEC tasks. KoGEC demonstrated a more balanced error correction profile across various error types, whereas the larger LLMs tended to focus less on punctuation errors. We also developed a Chrome extension to make the KoGEC system accessible to users. Finally, we explored token vocabulary expansion to further improve the model but found it to decrease model performance. This research contributes to the field of NLP by providing an efficient, specialized Korean GEC system and a new evaluation method. It also highlights the potential of compact, task-specific models to compete with larger, general-purpose language models in specialized NLP tasks.</li>
</ul>

<h3>Title: Auditing Data Provenance in Real-world Text-to-Image Diffusion Models for Privacy and Copyright Protection</h3>
<ul>
<li><strong>Authors: </strong>Jie Zhu, Leye Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11434">https://arxiv.org/abs/2506.11434</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11434">https://arxiv.org/pdf/2506.11434</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11434]] Auditing Data Provenance in Real-world Text-to-Image Diffusion Models for Privacy and Copyright Protection(https://arxiv.org/abs/2506.11434)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion model since its propose has significantly influenced the content creation due to its impressive generation capability. However, this capability depends on large-scale text-image datasets gathered from web platforms like social media, posing substantial challenges in copyright compliance and personal privacy leakage. Though there are some efforts devoted to explore approaches for auditing data provenance in text-to-image diffusion models, existing work has unrealistic assumptions that can obtain model internal knowledge, e.g., intermediate results, or the evaluation is not reliable. To fill this gap, we propose a completely black-box auditing framework called Feature Semantic Consistency-based Auditing (FSCA). It utilizes two types of semantic connections within the text-to-image diffusion model for auditing, eliminating the need for access to internal knowledge. To demonstrate the effectiveness of our FSCA framework, we perform extensive experiments on LAION-mi dataset and COCO dataset, and compare with eight state-of-the-art baseline approaches. The results show that FSCA surpasses previous baseline approaches across various metrics and different data distributions, showcasing the superiority of our FSCA. Moreover, we introduce a recall balance strategy and a threshold adjustment strategy, which collectively allows FSCA to reach up a user-level accuracy of 90% in a real-world auditing scenario with only 10 samples/user, highlighting its strong auditing potential in real-world applications. Our code is made available at this https URL.</li>
</ul>

<h3>Title: TAViS: Text-bridged Audio-Visual Segmentation with Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Ziyang Luo, Nian Liu, Xuguang Yang, Salman Khan, Rao Muhammad Anwer, Hisham Cholakkal, Fahad Shahbaz Khan, Junwei Han</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11436">https://arxiv.org/abs/2506.11436</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11436">https://arxiv.org/pdf/2506.11436</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11436]] TAViS: Text-bridged Audio-Visual Segmentation with Foundation Models(https://arxiv.org/abs/2506.11436)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Audio-Visual Segmentation (AVS) faces a fundamental challenge of effectively aligning audio and visual modalities. While recent approaches leverage foundation models to address data scarcity, they often rely on single-modality knowledge or combine foundation models in an off-the-shelf manner, failing to address the cross-modal alignment challenge. In this paper, we present TAViS, a novel framework that \textbf{couples} the knowledge of multimodal foundation models (ImageBind) for cross-modal alignment and a segmentation foundation model (SAM2) for precise segmentation. However, effectively combining these models poses two key challenges: the difficulty in transferring the knowledge between SAM2 and ImageBind due to their different feature spaces, and the insufficiency of using only segmentation loss for supervision. To address these challenges, we introduce a text-bridged design with two key components: (1) a text-bridged hybrid prompting mechanism where pseudo text provides class prototype information while retaining modality-specific details from both audio and visual inputs, and (2) an alignment supervision strategy that leverages text as a bridge to align shared semantic concepts within audio-visual modalities. Our approach achieves superior performance on single-source, multi-source, semantic datasets, and excels in zero-shot settings.</li>
</ul>

<h3>Title: AbsenceBench: Language Models Can't Tell What's Missing</h3>
<ul>
<li><strong>Authors: </strong>Harvey Yiyun Fu, Aryan Shrivastava, Jared Moore, Peter West, Chenhao Tan, Ari Holtzman</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11440">https://arxiv.org/abs/2506.11440</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11440">https://arxiv.org/pdf/2506.11440</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11440]] AbsenceBench: Language Models Can't Tell What's Missing(https://arxiv.org/abs/2506.11440)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly capable of processing long inputs and locating specific information within them, as evidenced by their performance on the Needle in a Haystack (NIAH) test. However, while models excel at recalling surprising information, they still struggle to identify clearly omitted information. We introduce AbsenceBench to assesses LLMs' capacity to detect missing information across three domains: numerical sequences, poetry, and GitHub pull requests. AbsenceBench asks models to identify which pieces of a document were deliberately removed, given access to both the original and edited contexts. Despite the apparent straightforwardness of these tasks, our experiments reveal that even state-of-the-art models like Claude-3.7-Sonnet achieve only 69.6% F1-score with a modest average context length of 5K tokens. Our analysis suggests this poor performance stems from a fundamental limitation: Transformer attention mechanisms cannot easily attend to "gaps" in documents since these absences don't correspond to any specific keys that can be attended to. Overall, our results and analysis provide a case study of the close proximity of tasks where models are already superhuman (NIAH) and tasks where models breakdown unexpectedly (AbsenceBench).</li>
</ul>

<h3>Title: GaussMarker: Robust Dual-Domain Watermark for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Kecen Li, Zhicong Huang, Xinwen Hou, Cheng Hong</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11444">https://arxiv.org/abs/2506.11444</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11444">https://arxiv.org/pdf/2506.11444</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11444]] GaussMarker: Robust Dual-Domain Watermark for Diffusion Models(https://arxiv.org/abs/2506.11444)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, watermark, diffusion</a></li>
<li><strong>Abstract: </strong>As Diffusion Models (DM) generate increasingly realistic images, related issues such as copyright and misuse have become a growing concern. Watermarking is one of the promising solutions. Existing methods inject the watermark into the single-domain of initial Gaussian noise for generation, which suffers from unsatisfactory robustness. This paper presents the first dual-domain DM watermarking approach using a pipelined injector to consistently embed watermarks in both the spatial and frequency domains. To further boost robustness against certain image manipulations and advanced attacks, we introduce a model-independent learnable Gaussian Noise Restorer (GNR) to refine Gaussian noise extracted from manipulated images and enhance detection robustness by integrating the detection scores of both watermarks. GaussMarker efficiently achieves state-of-the-art performance under eight image distortions and four advanced attacks across three versions of Stable Diffusion with better recall and lower false positive rates, as preferred in real applications.</li>
</ul>

<h3>Title: Computational Attestations of Polynomial Integrity Towards Verifiable Machine-Learning</h3>
<ul>
<li><strong>Authors: </strong>Dustin Ray, Caroline El Jazmi</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11458">https://arxiv.org/abs/2506.11458</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11458">https://arxiv.org/pdf/2506.11458</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11458]] Computational Attestations of Polynomial Integrity Towards Verifiable Machine-Learning(https://arxiv.org/abs/2506.11458)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Machine-learning systems continue to advance at a rapid pace, demonstrating remarkable utility in various fields and disciplines. As these systems continue to grow in size and complexity, a nascent industry is emerging which aims to bring machine-learning-as-a-service (MLaaS) to market. Outsourcing the operation and training of these systems to powerful hardware carries numerous advantages, but challenges arise when privacy and the correctness of work carried out must be ensured. Recent advancements in the field of zero-knowledge cryptography have led to a means of generating arguments of integrity for any computation, which in turn can be efficiently verified by any party, in any place, at any time. In this work we prove the correct training of a differentially-private (DP) linear regression over a dataset of 50,000 samples on a single machine in less than 6 minutes, verifying the entire computation in 0.17 seconds. To our knowledge, this result represents the fastest known instance in the literature of provable-DP over a dataset of this size. We believe this result constitutes a key stepping-stone towards end-to-end private MLaaS.</li>
</ul>

<h3>Title: RollingQ: Reviving the Cooperation Dynamics in Multimodal Transformer</h3>
<ul>
<li><strong>Authors: </strong>Haotian Ni, Yake Wei, Hang Liu, Gong Chen, Chong Peng, Hao Lin, Di Hu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11465">https://arxiv.org/abs/2506.11465</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11465">https://arxiv.org/pdf/2506.11465</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11465]] RollingQ: Reviving the Cooperation Dynamics in Multimodal Transformer(https://arxiv.org/abs/2506.11465)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Multimodal learning faces challenges in effectively fusing information from diverse modalities, especially when modality quality varies across samples. Dynamic fusion strategies, such as attention mechanism in Transformers, aim to address such challenge by adaptively emphasizing modalities based on the characteristics of input data. However, through amounts of carefully designed experiments, we surprisingly observed that the dynamic adaptability of widely-used self-attention models diminishes. Model tends to prefer one modality regardless of data characteristics. This bias triggers a self-reinforcing cycle that progressively overemphasizes the favored modality, widening the distribution gap in attention keys across modalities and deactivating attention mechanism's dynamic properties. To revive adaptability, we propose a simple yet effective method Rolling Query (RollingQ), which balances attention allocation by rotating the query to break the self-reinforcing cycle and mitigate the key distribution gap. Extensive experiments on various multimodal scenarios validate the effectiveness of RollingQ and the restoration of cooperation dynamics is pivotal for enhancing the broader capabilities of widely deployed multimodal Transformers. The source code is available at this https URL.</li>
</ul>

<h3>Title: A Gamified Evaluation and Recruitment Platform for Low Resource Language Machine Translation Systems</h3>
<ul>
<li><strong>Authors: </strong>Carlos Rafael Catalan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11467">https://arxiv.org/abs/2506.11467</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11467">https://arxiv.org/pdf/2506.11467</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11467]] A Gamified Evaluation and Recruitment Platform for Low Resource Language Machine Translation Systems(https://arxiv.org/abs/2506.11467)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Human evaluators provide necessary contributions in evaluating large language models. In the context of Machine Translation (MT) systems for low-resource languages (LRLs), this is made even more apparent since popular automated metrics tend to be string-based, and therefore do not provide a full picture of the nuances of the behavior of the system. Human evaluators, when equipped with the necessary expertise of the language, will be able to test for adequacy, fluency, and other important metrics. However, the low resource nature of the language means that both datasets and evaluators are in short supply. This presents the following conundrum: How can developers of MT systems for these LRLs find adequate human evaluators and datasets? This paper first presents a comprehensive review of existing evaluation procedures, with the objective of producing a design proposal for a platform that addresses the resource gap in terms of datasets and evaluators in developing MT systems. The result is a design for a recruitment and gamified evaluation platform for developers of MT systems. Challenges are also discussed in terms of evaluating this platform, as well as its possible applications in the wider scope of Natural Language Processing (NLP) research.</li>
</ul>

<h3>Title: On the Natural Robustness of Vision-Language Models Against Visual Perception Attacks in Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Pedram MohajerAnsari (1), Amir Salarpour (1), Michael Kühr (2), Siyu Huang (1), Mohammad Hamad (2), Sebastian Steinhorst (2), Habeeb Olufowobi (3), Mert D. Pesé (1) ((1) Clemson University, Clemson, SC, USA, (2) Technical University of Munich, Munich, Germany, (3) University of Texas at Arlington, Arlington, TX, USA)</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11472">https://arxiv.org/abs/2506.11472</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11472">https://arxiv.org/pdf/2506.11472</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11472]] On the Natural Robustness of Vision-Language Models Against Visual Perception Attacks in Autonomous Driving(https://arxiv.org/abs/2506.11472)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Autonomous vehicles (AVs) rely on deep neural networks (DNNs) for critical tasks such as traffic sign recognition (TSR), automated lane centering (ALC), and vehicle detection (VD). However, these models are vulnerable to attacks that can cause misclassifications and compromise safety. Traditional defense mechanisms, including adversarial training, often degrade benign accuracy and fail to generalize against unseen attacks. In this work, we introduce Vehicle Vision Language Models (V2LMs), fine-tuned vision-language models specialized for AV perception. Our findings demonstrate that V2LMs inherently exhibit superior robustness against unseen attacks without requiring adversarial training, maintaining significantly higher accuracy than conventional DNNs under adversarial conditions. We evaluate two deployment strategies: Solo Mode, where individual V2LMs handle specific perception tasks, and Tandem Mode, where a single unified V2LM is fine-tuned for multiple tasks simultaneously. Experimental results reveal that DNNs suffer performance drops of 33% to 46% under attacks, whereas V2LMs maintain adversarial accuracy with reductions of less than 8% on average. The Tandem Mode further offers a memory-efficient alternative while achieving comparable robustness to Solo Mode. We also explore integrating V2LMs as parallel components to AV perception to enhance resilience against adversarial threats. Our results suggest that V2LMs offer a promising path toward more secure and resilient AV perception systems.</li>
</ul>

<h3>Title: Med-PRM: Medical Reasoning Models with Stepwise, Guideline-verified Process Rewards</h3>
<ul>
<li><strong>Authors: </strong>Jaehoon Yun, Jiwoong Sohn, Jungwoo Park, Hyunjae Kim, Xiangru Tang, Yanjun Shao, Yonghoe Koo, Minhyeok Ko, Qingyu Chen, Mark Gerstein, Michael Moor, Jaewoo Kang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11474">https://arxiv.org/abs/2506.11474</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11474">https://arxiv.org/pdf/2506.11474</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11474]] Med-PRM: Medical Reasoning Models with Stepwise, Guideline-verified Process Rewards(https://arxiv.org/abs/2506.11474)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models have shown promise in clinical decision making, but current approaches struggle to localize and correct errors at specific steps of the reasoning process. This limitation is critical in medicine, where identifying and addressing reasoning errors is essential for accurate diagnosis and effective patient care. We introduce Med-PRM, a process reward modeling framework that leverages retrieval-augmented generation to verify each reasoning step against established medical knowledge bases. By verifying intermediate reasoning steps with evidence retrieved from clinical guidelines and literature, our model can precisely assess the reasoning quality in a fine-grained manner. Evaluations on five medical QA benchmarks and two open-ended diagnostic tasks demonstrate that Med-PRM achieves state-of-the-art performance, with improving the performance of base models by up to 13.50% using Med-PRM. Moreover, we demonstrate the generality of Med-PRM by integrating it in a plug-and-play fashion with strong policy models such as Meerkat, achieving over 80\% accuracy on MedQA for the first time using small-scale models of 8 billion parameters. Our code and data are available at: this https URL</li>
</ul>

<h3>Title: FAME: A Lightweight Spatio-Temporal Network for Model Attribution of Face-Swap Deepfakes</h3>
<ul>
<li><strong>Authors: </strong>Wasim Ahmad, Yan-Tsung Peng, Yuan-Hao Chang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11477">https://arxiv.org/abs/2506.11477</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11477">https://arxiv.org/pdf/2506.11477</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11477]] FAME: A Lightweight Spatio-Temporal Network for Model Attribution of Face-Swap Deepfakes(https://arxiv.org/abs/2506.11477)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, generative</a></li>
<li><strong>Abstract: </strong>The widespread emergence of face-swap Deepfake videos poses growing risks to digital security, privacy, and media integrity, necessitating effective forensic tools for identifying the source of such manipulations. Although most prior research has focused primarily on binary Deepfake detection, the task of model attribution -- determining which generative model produced a given Deepfake -- remains underexplored. In this paper, we introduce FAME (Fake Attribution via Multilevel Embeddings), a lightweight and efficient spatio-temporal framework designed to capture subtle generative artifacts specific to different face-swap models. FAME integrates spatial and temporal attention mechanisms to improve attribution accuracy while remaining computationally efficient. We evaluate our model on three challenging and diverse datasets: Deepfake Detection and Manipulation (DFDM), FaceForensics++, and FakeAVCeleb. Results show that FAME consistently outperforms existing methods in both accuracy and runtime, highlighting its potential for deployment in real-world forensic and information security applications.</li>
</ul>

<h3>Title: ImmunoFOMO: Are Language Models missing what oncologists see?</h3>
<ul>
<li><strong>Authors: </strong>Aman Sinha, Bogdan-Valentin Popescu, Xavier Coubez, Marianne Clausel, Mathieu Constant</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11478">https://arxiv.org/abs/2506.11478</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11478">https://arxiv.org/pdf/2506.11478</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11478]] ImmunoFOMO: Are Language Models missing what oncologists see?(https://arxiv.org/abs/2506.11478)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Language models (LMs) capabilities have grown with a fast pace over the past decade leading researchers in various disciplines, such as biomedical research, to increasingly explore the utility of LMs in their day-to-day applications. Domain specific language models have already been in use for biomedical natural language processing (NLP) applications. Recently however, the interest has grown towards medical language models and their understanding capabilities. In this paper, we investigate the medical conceptual grounding of various language models against expert clinicians for identification of hallmarks of immunotherapy in breast cancer abstracts. Our results show that pre-trained language models have potential to outperform large language models in identifying very specific (low-level) concepts.</li>
</ul>

<h3>Title: LearnAlign: Reasoning Data Selection for Reinforcement Learning in Large Language Models Based on Improved Gradient Alignment</h3>
<ul>
<li><strong>Authors: </strong>Shikun Li, Shipeng Li, Zhiqin Yang, Xinghua Zhang, Gaode Chen, Xiaobo Xia, Hengyu Liu, Zhe Peng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11480">https://arxiv.org/abs/2506.11480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11480">https://arxiv.org/pdf/2506.11480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11480]] LearnAlign: Reasoning Data Selection for Reinforcement Learning in Large Language Models Based on Improved Gradient Alignment(https://arxiv.org/abs/2506.11480)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) has become a key technique for enhancing LLMs' reasoning abilities, yet its data inefficiency remains a major bottleneck. To address this critical yet challenging issue, we present a novel gradient-alignment-based method, named LearnAlign, which intelligently selects the learnable and representative training reasoning data for RL post-training. To overcome the well-known issue of response-length bias in gradient norms, we introduce the data learnability based on the success rate, which can indicate the learning potential of each data point. Experiments across three mathematical reasoning benchmarks demonstrate that our method significantly reduces training data requirements while achieving minor performance degradation or even improving performance compared to full-data training. For example, it reduces data requirements by up to 1,000 data points with better performance (77.53%) than that on the full dataset on GSM8K benchmark (77.04%). Furthermore, we show its effectiveness in the staged RL setting. This work provides valuable insights into data-efficient RL post-training and establishes a foundation for future research in optimizing reasoning data this http URL facilitate future work, we will release code.</li>
</ul>

<h3>Title: Relational Schemata in BERT Are Inducible, Not Emergent: A Study of Performance vs. Competence in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Cole Gawin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11485">https://arxiv.org/abs/2506.11485</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11485">https://arxiv.org/pdf/2506.11485</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11485]] Relational Schemata in BERT Are Inducible, Not Emergent: A Study of Performance vs. Competence in Language Models(https://arxiv.org/abs/2506.11485)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While large language models like BERT demonstrate strong empirical performance on semantic tasks, whether this reflects true conceptual competence or surface-level statistical association remains unclear. I investigate whether BERT encodes abstract relational schemata by examining internal representations of concept pairs across taxonomic, mereological, and functional relations. I compare BERT's relational classification performance with representational structure in [CLS] token embeddings. Results reveal that pretrained BERT enables high classification accuracy, indicating latent relational signals. However, concept pairs organize by relation type in high-dimensional embedding space only after fine-tuning on supervised relation classification tasks. This indicates relational schemata are not emergent from pretraining alone but can be induced via task scaffolding. These findings demonstrate that behavioral performance does not necessarily imply structured conceptual understanding, though models can acquire inductive biases for grounded relational abstraction through appropriate training.</li>
</ul>

<h3>Title: Composite Data Augmentations for Synthetic Image Detection Against Real-World Perturbations</h3>
<ul>
<li><strong>Authors: </strong>Efthymia Amarantidou, Christos Koutlis, Symeon Papadopoulos, Panagiotis C. Petrantonakis</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11490">https://arxiv.org/abs/2506.11490</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11490">https://arxiv.org/pdf/2506.11490</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11490]] Composite Data Augmentations for Synthetic Image Detection Against Real-World Perturbations(https://arxiv.org/abs/2506.11490)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The advent of accessible Generative AI tools enables anyone to create and spread synthetic images on social media, often with the intention to mislead, thus posing a significant threat to online information integrity. Most existing Synthetic Image Detection (SID) solutions struggle on generated images sourced from the Internet, as these are often altered by compression and other operations. To address this, our research enhances SID by exploring data augmentation combinations, leveraging a genetic algorithm for optimal augmentation selection, and introducing a dual-criteria optimization approach. These methods significantly improve model performance under real-world perturbations. Our findings provide valuable insights for developing detection models capable of identifying synthetic images across varying qualities and transformations, with the best-performing model achieving a mean average precision increase of +22.53% compared to models without augmentations. The implementation is available at this http URL.</li>
</ul>

<h3>Title: Preserving Clusters in Prompt Learning for Unsupervised Domain Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Tung-Long Vuong, Hoang Phan, Vy Vo, Anh Bui, Thanh-Toan Do, Trung Le, Dinh Phung</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11493">https://arxiv.org/abs/2506.11493</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11493">https://arxiv.org/pdf/2506.11493</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11493]] Preserving Clusters in Prompt Learning for Unsupervised Domain Adaptation(https://arxiv.org/abs/2506.11493)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent approaches leveraging multi-modal pre-trained models like CLIP for Unsupervised Domain Adaptation (UDA) have shown significant promise in bridging domain gaps and improving generalization by utilizing rich semantic knowledge and robust visual representations learned through extensive pre-training on diverse image-text datasets. While these methods achieve state-of-the-art performance across benchmarks, much of the improvement stems from base pseudo-labels (CLIP zero-shot predictions) and self-training mechanisms. Thus, the training mechanism exhibits a key limitation wherein the visual embedding distribution in target domains can deviate from the visual embedding distribution in the pre-trained model, leading to misguided signals from class descriptions. This work introduces a fresh solution to reinforce these pseudo-labels and facilitate target-prompt learning, by exploiting the geometry of visual and text embeddings - an aspect that is overlooked by existing methods. We first propose to directly leverage the reference predictions (from source prompts) based on the relationship between source and target visual embeddings. We later show that there is a strong clustering behavior observed between visual and text embeddings in pre-trained multi-modal models. Building on optimal transport theory, we transform this insight into a novel strategy to enforce the clustering property in text embeddings, further enhancing the alignment in the target domain. Our experiments and ablation studies validate the effectiveness of the proposed approach, demonstrating superior performance and improved quality of target prompts in terms of representation.</li>
</ul>

<h3>Title: Lag-Relative Sparse Attention In Long Context Training</h3>
<ul>
<li><strong>Authors: </strong>Manlai Liang, Wanyi Huang, Mandi Liu, Huaijun Li, Jinlong Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11498">https://arxiv.org/abs/2506.11498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11498">https://arxiv.org/pdf/2506.11498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11498]] Lag-Relative Sparse Attention In Long Context Training(https://arxiv.org/abs/2506.11498)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have made significant strides in natural language processing and generation, yet their ability to handle long-context input remains constrained by the quadratic complexity of attention computation and linear-increasing key-value memory footprint. To reduce computational costs and memory, key-value cache compression techniques are commonly applied at inference time, but this often leads to severe performance degradation, as models are not trained to handle compressed context. Although there are more sophisticated compression methods, they are typically unsuitable for post-training because of their incompatibility with gradient-based optimization or high computation overhead. To fill this gap with no additional parameter and little computation overhead, we propose Lag-Relative Sparse Attention(LRSA) anchored by the LagKV compression method for long context post-training. Our method performs chunk-by-chunk prefilling, which selects the top K most relevant key-value pairs in a fixed-size lagging window, allowing the model to focus on salient historical context while maintaining efficiency. Experimental results show that our approach significantly enhances the robustness of the LLM with key-value compression and achieves better fine-tuned results in the question-answer tuning task.</li>
</ul>

<h3>Title: Task-Driven Discrete Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Tung-Long Vuong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11511">https://arxiv.org/abs/2506.11511</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11511">https://arxiv.org/pdf/2506.11511</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11511]] Task-Driven Discrete Representation Learning(https://arxiv.org/abs/2506.11511)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In recent years, deep discrete representation learning (DRL) has achieved significant success across various domains. Most DRL frameworks (e.g., the widely used VQ-VAE and its variants) have primarily focused on generative settings, where the quality of a representation is implicitly gauged by the fidelity of its generation. In fact, the goodness of a discrete representation remain ambiguously defined across the literature. In this work, we adopt a practical approach that examines DRL from a task-driven perspective. We propose a unified framework that explores the usefulness of discrete features in relation to downstream tasks, with generation naturally viewed as one possible application. In this context, the properties of discrete representations as well as the way they benefit certain tasks are also relatively understudied. We therefore provide an additional theoretical analysis of the trade-off between representational capacity and sample complexity, shedding light on how discrete representation utilization impacts task performance. Finally, we demonstrate the flexibility and effectiveness of our framework across diverse applications.</li>
</ul>

<h3>Title: Prioritizing Alignment Paradigms over Task-Specific Model Customization in Time-Series LLMs</h3>
<ul>
<li><strong>Authors: </strong>Wei Li, Yunyao Cheng, Xinli Hao, Chaohong Ma, Yuxuan Liang, Bin Yang, Christian S.Jensen, Xiaofeng Meng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11512">https://arxiv.org/abs/2506.11512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11512">https://arxiv.org/pdf/2506.11512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11512]] Prioritizing Alignment Paradigms over Task-Specific Model Customization in Time-Series LLMs(https://arxiv.org/abs/2506.11512)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in Large Language Models (LLMs) have enabled unprecedented capabilities for time-series reasoning in diverse real-world applications, including medical, financial, and spatio-temporal domains. However, existing approaches typically focus on task-specific model customization, such as forecasting and anomaly detection, while overlooking the data itself, referred to as time-series primitives, which are essential for in-depth reasoning. This position paper advocates a fundamental shift in approaching time-series reasoning with LLMs: prioritizing alignment paradigms grounded in the intrinsic primitives of time series data over task-specific model customization. This realignment addresses the core limitations of current time-series reasoning approaches, which are often costly, inflexible, and inefficient, by systematically accounting for intrinsic structure of data before task engineering. To this end, we propose three alignment paradigms: Injective Alignment, Bridging Alignment, and Internal Alignment, which are emphasized by prioritizing different aspects of time-series primitives: domain, characteristic, and representation, respectively, to activate time-series reasoning capabilities of LLMs to enable economical, flexible, and efficient reasoning. We further recommend that practitioners adopt an alignment-oriented method to avail this instruction to select an appropriate alignment paradigm. Additionally, we categorize relevant literature into these alignment paradigms and outline promising research directions.</li>
</ul>

<h3>Title: Manager: Aggregating Insights from Unimodal Experts in Two-Tower VLMs and MLLMs</h3>
<ul>
<li><strong>Authors: </strong>Xiao Xu, Libo Qin, Wanxiang Che, Min-Yen Kan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11515">https://arxiv.org/abs/2506.11515</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11515">https://arxiv.org/pdf/2506.11515</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11515]] Manager: Aggregating Insights from Unimodal Experts in Two-Tower VLMs and MLLMs(https://arxiv.org/abs/2506.11515)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Two-Tower Vision--Language Models (VLMs) have demonstrated strong performance across various downstream VL tasks. While BridgeTower further enhances performance by building bridges between encoders, it \textit{(i)} suffers from ineffective layer-by-layer utilization of unimodal representations, \textit{(ii)} restricts the flexible exploitation of different levels of unimodal semantic knowledge, and \textit{(iii)} is limited to the evaluation on traditional low-resolution datasets only with the Two-Tower VLM architecture. In this work, we propose Manager, a lightweight, efficient and effective plugin that adaptively aggregates insights from different levels of pre-trained unimodal experts to facilitate more comprehensive VL alignment and fusion. First, under the Two-Tower VLM architecture, we introduce ManagerTower, a novel VLM that introduces the manager in each cross-modal layer. Whether with or without VL pre-training, ManagerTower outperforms previous strong baselines and achieves superior performance on 4 downstream VL tasks. Moreover, we extend our exploration to the latest Multimodal Large Language Model (MLLM) architecture. We demonstrate that LLaVA-OV-Manager significantly boosts the zero-shot performance of LLaVA-OV across different categories of capabilities, images, and resolutions on 20 downstream datasets, whether the multi-grid algorithm is enabled or not. In-depth analysis reveals that both our manager and the multi-grid algorithm can be viewed as a plugin that improves the visual representation by capturing more diverse visual details from two orthogonal perspectives (depth and width). Their synergy can mitigate the semantic ambiguity caused by the multi-grid algorithm and further improve performance. Code and models are available at this https URL.</li>
</ul>

<h3>Title: Brewing Knowledge in Context: Distillation Perspectives on In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Chengye Li, Haiyun Liu, Yuanxi Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11516">https://arxiv.org/abs/2506.11516</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11516">https://arxiv.org/pdf/2506.11516</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11516]] Brewing Knowledge in Context: Distillation Perspectives on In-Context Learning(https://arxiv.org/abs/2506.11516)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) allows large language models (LLMs) to solve novel tasks without weight updates. Despite its empirical success, the mechanism behind ICL remains poorly understood, limiting our ability to interpret, improve, and reliably apply it. In this paper, we propose a new theoretical perspective that interprets ICL as an implicit form of knowledge distillation (KD), where prompt demonstrations guide the model to form a task-specific reference model during inference. Under this view, we derive a Rademacher complexity-based generalization bound and prove that the bias of the distilled weights grows linearly with the Maximum Mean Discrepancy (MMD) between the prompt and target distributions. This theoretical framework explains several empirical phenomena and unifies prior gradient-based and distributional analyses. To the best of our knowledge, this is the first to formalize inference-time attention as a distillation process, which provides theoretical insights for future prompt engineering and automated demonstration selection.</li>
</ul>

<h3>Title: Investigating Vulnerabilities and Defenses Against Audio-Visual Attacks: A Comprehensive Survey Emphasizing Multimodal Models</h3>
<ul>
<li><strong>Authors: </strong>Jinming Wen, Xinyi Wu, Shuai Zhao, Yanhao Jia, Yuwen Li</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11521">https://arxiv.org/abs/2506.11521</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11521">https://arxiv.org/pdf/2506.11521</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11521]] Investigating Vulnerabilities and Defenses Against Audio-Visual Attacks: A Comprehensive Survey Emphasizing Multimodal Models(https://arxiv.org/abs/2506.11521)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs), which bridge the gap between audio-visual and natural language processing, achieve state-of-the-art performance on several audio-visual tasks. Despite the superior performance of MLLMs, the scarcity of high-quality audio-visual training data and computational resources necessitates the utilization of third-party data and open-source MLLMs, a trend that is increasingly observed in contemporary research. This prosperity masks significant security risks. Empirical studies demonstrate that the latest MLLMs can be manipulated to produce malicious or harmful content. This manipulation is facilitated exclusively through instructions or inputs, including adversarial perturbations and malevolent queries, effectively bypassing the internal security mechanisms embedded within the models. To gain a deeper comprehension of the inherent security vulnerabilities associated with audio-visual-based multimodal models, a series of surveys investigates various types of attacks, including adversarial and backdoor attacks. While existing surveys on audio-visual attacks provide a comprehensive overview, they are limited to specific types of attacks, which lack a unified review of various types of attacks. To address this issue and gain insights into the latest trends in the field, this paper presents a comprehensive and systematic review of audio-visual attacks, which include adversarial attacks, backdoor attacks, and jailbreak attacks. Furthermore, this paper also reviews various types of attacks in the latest audio-visual-based MLLMs, a dimension notably absent in existing surveys. Drawing upon comprehensive insights from a substantial review, this paper delineates both challenges and emergent trends for future research on audio-visual attacks and defense.</li>
</ul>

<h3>Title: Delayformer: spatiotemporal transformation for predicting high-dimensional dynamics</h3>
<ul>
<li><strong>Authors: </strong>Zijian Wang, Peng Tao, Luonan Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11528">https://arxiv.org/abs/2506.11528</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11528">https://arxiv.org/pdf/2506.11528</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11528]] Delayformer: spatiotemporal transformation for predicting high-dimensional dynamics(https://arxiv.org/abs/2506.11528)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Predicting time-series is of great importance in various scientific and engineering fields. However, in the context of limited and noisy data, accurately predicting dynamics of all variables in a high-dimensional system is a challenging task due to their nonlinearity and also complex interactions. Current methods including deep learning approaches often perform poorly for real-world systems under such circumstances. This study introduces the Delayformer framework for simultaneously predicting dynamics of all variables, by developing a novel multivariate spatiotemporal information (mvSTI) transformation that makes each observed variable into a delay-embedded state (vector) and further cross-learns those states from different variables. From dynamical systems viewpoint, Delayformer predicts system states rather than individual variables, thus theoretically and computationally overcoming such nonlinearity and cross-interaction problems. Specifically, it first utilizes a single shared Visual Transformer (ViT) encoder to cross-represent dynamical states from observed variables in a delay embedded form and then employs distinct linear decoders for predicting next states, i.e. equivalently predicting all original variables parallelly. By leveraging the theoretical foundations of delay embedding theory and the representational capabilities of Transformers, Delayformer outperforms current state-of-the-art methods in forecasting tasks on both synthetic and real-world datasets. Furthermore, the potential of Delayformer as a foundational time-series model is demonstrated through cross-domain forecasting tasks, highlighting its broad applicability across various scenarios.</li>
</ul>

<h3>Title: Robust Filtering -- Novel Statistical Learning and Inference Algorithms with Applications</h3>
<ul>
<li><strong>Authors: </strong>Aamir Hussain Chughtai</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11530">https://arxiv.org/abs/2506.11530</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11530">https://arxiv.org/pdf/2506.11530</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11530]] Robust Filtering -- Novel Statistical Learning and Inference Algorithms with Applications(https://arxiv.org/abs/2506.11530)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>State estimation or filtering serves as a fundamental task to enable intelligent decision-making in applications such as autonomous vehicles, robotics, healthcare monitoring, smart grids, intelligent transportation, and predictive maintenance. Standard filtering assumes prior knowledge of noise statistics to extract latent system states from noisy sensor data. However, real-world scenarios involve abnormalities like outliers, biases, drifts, and missing observations with unknown or partially known statistics, limiting conventional approaches. This thesis presents novel robust nonlinear filtering methods to mitigate these challenges. Based on insights from our filtering proposals, we extend the formulations to offline estimation/learning setups and propose smoothing extensions. Our methods leverage Bayesian inference frameworks, employing both deterministic and stochastic approximation techniques including Variational Inference (VI) and Particle Filters/Sequential Monte Carlo (SMC). We also study theoretical estimation limits using Bayesian Cramér-Rao bounds (BCRBs) in the context of measurement abnormalities. To validate the performance gains of the proposed methods, we perform simulations and experiments in scenarios including target tracking, indoor localization, 3D point cloud registration, mesh registration, and pose graph optimization. The fundamental nature of the work makes it useful in diverse applications, with possible future extensions toward developing outlier-robust machine learning pipelines, learning system dynamics from anomalous data, and addressing challenges in generative AI where standard diffusion models struggle with outliers, imbalanced datasets, and mode collapse.</li>
</ul>

<h3>Title: GNSS-inertial state initialization by distance residuals</h3>
<ul>
<li><strong>Authors: </strong>Samuel Cerezo, Javier Civera</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11534">https://arxiv.org/abs/2506.11534</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11534">https://arxiv.org/pdf/2506.11534</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11534]] GNSS-inertial state initialization by distance residuals(https://arxiv.org/abs/2506.11534)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Initializing the state of a sensorized platform can be challenging, as a limited set of initial measurements often carry limited information, leading to poor initial estimates that may converge to local minima during non-linear optimization. This paper proposes a novel GNSS-inertial initialization strategy that delays the use of global GNSS measurements until sufficient information is available to accurately estimate the transformation between the GNSS and inertial frames. Instead, the method initially relies on GNSS relative distance residuals. To determine the optimal moment for switching to global measurements, we introduce a criterion based on the evolution of the Hessian matrix singular values. Experiments on the EuRoC and GVINS datasets show that our approach consistently outperforms the naive strategy of using global GNSS data from the start, yielding more accurate and robust initializations.</li>
</ul>

<h3>Title: FIMA-Q: Post-Training Quantization for Vision Transformers by Fisher Information Matrix Approximation</h3>
<ul>
<li><strong>Authors: </strong>Zhuguanyu Wu, Shihe Wang, Jiayi Zhang, Jiaxin Chen, Yunhong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11543">https://arxiv.org/abs/2506.11543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11543">https://arxiv.org/pdf/2506.11543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11543]] FIMA-Q: Post-Training Quantization for Vision Transformers by Fisher Information Matrix Approximation(https://arxiv.org/abs/2506.11543)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Post-training quantization (PTQ) has stood out as a cost-effective and promising model compression paradigm in recent years, as it avoids computationally intensive model retraining. Nevertheless, current PTQ methods for Vision Transformers (ViTs) still suffer from significant accuracy degradation, especially under low-bit quantization. To address these shortcomings, we analyze the prevailing Hessian-guided quantization loss, and uncover certain limitations of conventional Hessian approximations. By following the block-wise reconstruction framework, we propose a novel PTQ method for ViTs, dubbed FIMA-Q. Specifically, we firstly establish the connection between KL divergence and FIM, which enables fast computation of the quantization loss during reconstruction. We further propose an efficient FIM approximation method, namely DPLR-FIM, by employing the diagonal plus low-rank principle, and formulate the ultimate quantization loss. Our extensive experiments, conducted across various vision tasks with representative ViT-based architectures on public datasets, demonstrate that our method substantially promotes the accuracy compared to the state-of-the-art approaches, especially in the case of low-bit quantization. The source code is available at this https URL.</li>
</ul>

<h3>Title: Linearly Solving Robust Rotation Estimation</h3>
<ul>
<li><strong>Authors: </strong>Yinlong Liu, Tianyu Huang, Zhi-Xin Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11547">https://arxiv.org/abs/2506.11547</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11547">https://arxiv.org/pdf/2506.11547</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11547]] Linearly Solving Robust Rotation Estimation(https://arxiv.org/abs/2506.11547)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Rotation estimation plays a fundamental role in computer vision and robot tasks, and extremely robust rotation estimation is significantly useful for safety-critical applications. Typically, estimating a rotation is considered a non-linear and non-convex optimization problem that requires careful design. However, in this paper, we provide some new perspectives that solving a rotation estimation problem can be reformulated as solving a linear model fitting problem without dropping any constraints and without introducing any singularities. In addition, we explore the dual structure of a rotation motion, revealing that it can be represented as a great circle on a quaternion sphere surface. Accordingly, we propose an easily understandable voting-based method to solve rotation estimation. The proposed method exhibits exceptional robustness to noise and outliers and can be computed in parallel with graphics processing units (GPUs) effortlessly. Particularly, leveraging the power of GPUs, the proposed method can obtain a satisfactory rotation solution for large-scale($10^6$) and severely corrupted (99$\%$ outlier ratio) rotation estimation problems under 0.5 seconds. Furthermore, to validate our theoretical framework and demonstrate the superiority of our proposed method, we conduct controlled experiments and real-world dataset experiments. These experiments provide compelling evidence supporting the effectiveness and robustness of our approach in solving rotation estimation problems.</li>
</ul>

<h3>Title: EyeSim-VQA: A Free-Energy-Guided Eye Simulation Framework for Video Quality Assessment</h3>
<ul>
<li><strong>Authors: </strong>Zhaoyang Wang, Wen Lu, Jie Li, Lihuo He, Maoguo Gong, Xinbo Gao</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11549">https://arxiv.org/abs/2506.11549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11549">https://arxiv.org/pdf/2506.11549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11549]] EyeSim-VQA: A Free-Energy-Guided Eye Simulation Framework for Video Quality Assessment(https://arxiv.org/abs/2506.11549)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Free-energy-guided self-repair mechanisms have shown promising results in image quality assessment (IQA), but remain under-explored in video quality assessment (VQA), where temporal dynamics and model constraints pose unique challenges. Unlike static images, video content exhibits richer spatiotemporal complexity, making perceptual restoration more difficult. Moreover, VQA systems often rely on pre-trained backbones, which limits the direct integration of enhancement modules without affecting model stability. To address these issues, we propose EyeSimVQA, a novel VQA framework that incorporates free-energy-based self-repair. It adopts a dual-branch architecture, with an aesthetic branch for global perceptual evaluation and a technical branch for fine-grained structural and semantic analysis. Each branch integrates specialized enhancement modules tailored to distinct visual inputs-resized full-frame images and patch-based fragments-to simulate adaptive repair behaviors. We also explore a principled strategy for incorporating high-level visual features without disrupting the original backbone. In addition, we design a biologically inspired prediction head that models sweeping gaze dynamics to better fuse global and local representations for quality prediction. Experiments on five public VQA benchmarks demonstrate that EyeSimVQA achieves competitive or superior performance compared to state-of-the-art methods, while offering improved interpretability through its biologically grounded design.</li>
</ul>

<h3>Title: From Persona to Person: Enhancing the Naturalness with Multiple Discourse Relations Graph Learning in Personalized Dialogue Generation</h3>
<ul>
<li><strong>Authors: </strong>Chih-Hao Hsu, Ying-Jia Lin, Hung-Yu Kao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11557">https://arxiv.org/abs/2506.11557</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11557">https://arxiv.org/pdf/2506.11557</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11557]] From Persona to Person: Enhancing the Naturalness with Multiple Discourse Relations Graph Learning in Personalized Dialogue Generation(https://arxiv.org/abs/2506.11557)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In dialogue generation, the naturalness of responses is crucial for effective human-machine interaction. Personalized response generation poses even greater challenges, as the responses must remain coherent and consistent with the user's personal traits or persona descriptions. We propose MUDI ($\textbf{Mu}$ltiple $\textbf{Di}$scourse Relations Graph Learning) for personalized dialogue generation. We utilize a Large Language Model to assist in annotating discourse relations and to transform dialogue data into structured dialogue graphs. Our graph encoder, the proposed DialogueGAT model, then captures implicit discourse relations within this structure, along with persona descriptions. During the personalized response generation phase, novel coherence-aware attention strategies are implemented to enhance the decoder's consideration of discourse relations. Our experiments demonstrate significant improvements in the quality of personalized responses, thus resembling human-like dialogue exchanges.</li>
</ul>

<h3>Title: DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning with Video LLMs</h3>
<ul>
<li><strong>Authors: </strong>Bo-Cheng Chiu, Jen-Jee Chen, Yu-Chee Tseng, Feng-Chi Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11558">https://arxiv.org/abs/2506.11558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11558">https://arxiv.org/pdf/2506.11558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11558]] DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning with Video LLMs(https://arxiv.org/abs/2506.11558)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have recently been extended to the video domain, enabling sophisticated video-language understanding. However, existing Video LLMs often exhibit limitations in fine-grained temporal reasoning, restricting their ability to precisely attribute responses to specific video moments, especially under constrained supervision. We introduce DaMO, a data-efficient Video LLM explicitly designed for accurate temporal reasoning and multimodal understanding. At its core, the proposed Temporal-aware Fuseformer employs a hierarchical dual-stream architecture that progressively captures temporal dynamics within each modality and effectively fuses complementary visual and audio information. To further enhance computational efficiency, DaMO integrates a global residual that reduces spatial redundancy while preserving essential semantic details. We train DaMO via a structured four-stage progressive training paradigm, incrementally equipping the model with multimodal alignment, semantic grounding, and temporal reasoning capabilities. This work also contributes multiple datasets augmented from existing ones with GPT-generated temporally grounded QA pairs for tasks requiring temporal supervision. Comprehensive experiments on temporal grounding and video QA benchmarks demonstrate that DaMO consistently surpasses prior methods, particularly in tasks demanding precise temporal alignment and reasoning. Our work establishes a promising direction for data-efficient video-language modeling.</li>
</ul>

<h3>Title: Learn to Preserve Personality: Federated Foundation Models in Recommendations</h3>
<ul>
<li><strong>Authors: </strong>Zhiwei Li, Guodong Long, Chunxu Zhang, Honglei Zhang, Jing Jiang, Chengqi Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11563">https://arxiv.org/abs/2506.11563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11563">https://arxiv.org/pdf/2506.11563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11563]] Learn to Preserve Personality: Federated Foundation Models in Recommendations(https://arxiv.org/abs/2506.11563)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>A core learning challenge for existed Foundation Models (FM) is striking the tradeoff between generalization with personalization, which is a dilemma that has been highlighted by various parameter-efficient adaptation techniques. Federated foundation models (FFM) provide a structural means to decouple shared knowledge from individual specific adaptations via decentralized processes. Recommendation systems offer a perfect testbed for FFMs, given their reliance on rich implicit feedback reflecting unique user characteristics. This position paper discusses a novel learning paradigm where FFMs not only harness their generalization capabilities but are specifically designed to preserve the integrity of user personality, illustrated thoroughly within the recommendation contexts. We envision future personal agents, powered by personalized adaptive FMs, guiding user decisions on content. Such an architecture promises a user centric, decentralized system where individuals maintain control over their personalized agents.</li>
</ul>

<h3>Title: VFaith: Do Large Multimodal Models Really Reason on Seen Images Rather than Previous Memories?</h3>
<ul>
<li><strong>Authors: </strong>Jiachen Yu, Yufei Zhan, Ziheng Wu, Yousong Zhu, Jinqiao Wang, Minghui Qiu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11571">https://arxiv.org/abs/2506.11571</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11571">https://arxiv.org/pdf/2506.11571</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11571]] VFaith: Do Large Multimodal Models Really Reason on Seen Images Rather than Previous Memories?(https://arxiv.org/abs/2506.11571)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Recent extensive works have demonstrated that by introducing long CoT, the capabilities of MLLMs to solve complex problems can be effectively enhanced. However, the reasons for the effectiveness of such paradigms remain unclear. It is challenging to analysis with quantitative results how much the model's specific extraction of visual cues and its subsequent so-called reasoning during inference process contribute to the performance improvements. Therefore, evaluating the faithfulness of MLLMs' reasoning to visual information is crucial. To address this issue, we first present a cue-driven automatic and controllable editing pipeline with the help of GPT-Image-1. It enables the automatic and precise editing of specific visual cues based on the instruction. Furthermore, we introduce VFaith-Bench, the first benchmark to evaluate MLLMs' visual reasoning capabilities and analyze the source of such capabilities with an emphasis on the visual faithfulness. Using the designed pipeline, we constructed comparative question-answer pairs by altering the visual cues in images that are crucial for solving the original reasoning problem, thereby changing the question's answer. By testing similar questions with images that have different details, the average accuracy reflects the model's visual reasoning ability, while the difference in accuracy before and after editing the test set images effectively reveals the relationship between the model's reasoning ability and visual perception. We further designed specific metrics to expose this relationship. VFaith-Bench includes 755 entries divided into five distinct subsets, along with an additional human-labeled perception task. We conducted in-depth testing and analysis of existing mainstream flagship models and prominent open-source model series/reasoning models on VFaith-Bench, further investigating the underlying factors of their reasoning capabilities.</li>
</ul>

<h3>Title: OV-MAP : Open-Vocabulary Zero-Shot 3D Instance Segmentation Map for Robots</h3>
<ul>
<li><strong>Authors: </strong>Juno Kim, Yesol Park, Hye-Jung Yoon, Byoung-Tak Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11585">https://arxiv.org/abs/2506.11585</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11585">https://arxiv.org/pdf/2506.11585</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11585]] OV-MAP : Open-Vocabulary Zero-Shot 3D Instance Segmentation Map for Robots(https://arxiv.org/abs/2506.11585)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>We introduce OV-MAP, a novel approach to open-world 3D mapping for mobile robots by integrating open-features into 3D maps to enhance object recognition capabilities. A significant challenge arises when overlapping features from adjacent voxels reduce instance-level precision, as features spill over voxel boundaries, blending neighboring regions together. Our method overcomes this by employing a class-agnostic segmentation model to project 2D masks into 3D space, combined with a supplemented depth image created by merging raw and synthetic depth from point clouds. This approach, along with a 3D mask voting mechanism, enables accurate zero-shot 3D instance segmentation without relying on 3D supervised segmentation models. We assess the effectiveness of our method through comprehensive experiments on public datasets such as ScanNet200 and Replica, demonstrating superior zero-shot performance, robustness, and adaptability across diverse environments. Additionally, we conducted real-world experiments to demonstrate our method's adaptability and robustness when applied to diverse real-world environments.</li>
</ul>

<h3>Title: SecONNds: Secure Outsourced Neural Network Inference on ImageNet</h3>
<ul>
<li><strong>Authors: </strong>Shashank Balla</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11586">https://arxiv.org/abs/2506.11586</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11586">https://arxiv.org/pdf/2506.11586</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11586]] SecONNds: Secure Outsourced Neural Network Inference on ImageNet(https://arxiv.org/abs/2506.11586)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy</a></li>
<li><strong>Abstract: </strong>The widespread adoption of outsourced neural network inference presents significant privacy challenges, as sensitive user data is processed on untrusted remote servers. Secure inference offers a privacy-preserving solution, but existing frameworks suffer from high computational overhead and communication costs, rendering them impractical for real-world deployment. We introduce SecONNds, a non-intrusive secure inference framework optimized for large ImageNet-scale Convolutional Neural Networks. SecONNds integrates a novel fully Boolean Goldreich-Micali-Wigderson (GMW) protocol for secure comparison -- addressing Yao's millionaires' problem -- using preprocessed Beaver's bit triples generated from Silent Random Oblivious Transfer. Our novel protocol achieves an online speedup of 17$\times$ in nonlinear operations compared to state-of-the-art solutions while reducing communication overhead. To further enhance performance, SecONNds employs Number Theoretic Transform (NTT) preprocessing and leverages GPU acceleration for homomorphic encryption operations, resulting in speedups of 1.6$\times$ on CPU and 2.2$\times$ on GPU for linear operations. We also present SecONNds-P, a bit-exact variant that ensures verifiable full-precision results in secure computation, matching the results of plaintext computations. Evaluated on a 37-bit quantized SqueezeNet model, SecONNds achieves an end-to-end inference time of 2.8 s on GPU and 3.6 s on CPU, with a total communication of just 420 MiB. SecONNds' efficiency and reduced computational load make it well-suited for deploying privacy-sensitive applications in resource-constrained environments. SecONNds is open source and can be accessed from: this https URL.</li>
</ul>

<h3>Title: EasyARC: Evaluating Vision Language Models on True Visual Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Mert Unsal, Aylin Akkus</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11595">https://arxiv.org/abs/2506.11595</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11595">https://arxiv.org/pdf/2506.11595</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11595]] EasyARC: Evaluating Vision Language Models on True Visual Reasoning(https://arxiv.org/abs/2506.11595)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Building on recent advances in language-based reasoning models, we explore multimodal reasoning that integrates vision and text. Existing multimodal benchmarks primarily test visual extraction combined with text-based reasoning, lacking true visual reasoning with more complex interactions between vision and language. Inspired by the ARC challenge, we introduce EasyARC, a vision-language benchmark requiring multi-image, multi-step reasoning, and self-correction. EasyARC is procedurally generated, fully verifiable, and scalable, making it ideal for reinforcement learning (RL) pipelines. The generators incorporate progressive difficulty levels, enabling structured evaluation across task types and complexities. We benchmark state-of-the-art vision-language models and analyze their failure modes. We argue that EasyARC sets a new standard for evaluating true reasoning and test-time scaling capabilities in vision-language models. We open-source our benchmark dataset and evaluation code.</li>
</ul>

<h3>Title: A$^2$LC: Active and Automated Label Correction for Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Youjin Jeon, Kyusik Cho, Suhan Woo, Euntai Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11599">https://arxiv.org/abs/2506.11599</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11599">https://arxiv.org/pdf/2506.11599</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11599]] A$^2$LC: Active and Automated Label Correction for Semantic Segmentation(https://arxiv.org/abs/2506.11599)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Active Label Correction (ALC) has emerged as a promising solution to the high cost and error-prone nature of manual pixel-wise annotation in semantic segmentation, by selectively identifying and correcting mislabeled data. Although recent work has improved correction efficiency by generating pseudo-labels using foundation models, substantial inefficiencies still remain. In this paper, we propose Active and Automated Label Correction for semantic segmentation (A$^2$LC), a novel and efficient ALC framework that integrates an automated correction stage into the conventional pipeline. Specifically, the automated correction stage leverages annotator feedback to perform label correction beyond the queried samples, thereby maximizing cost efficiency. In addition, we further introduce an adaptively balanced acquisition function that emphasizes underrepresented tail classes and complements the automated correction mechanism. Extensive experiments on Cityscapes and PASCAL VOC 2012 demonstrate that A$^2$LC significantly outperforms previous state-of-the-art methods. Notably, A$^2$LC achieves high efficiency by outperforming previous methods using only 20% of their budget, and demonstrates strong effectiveness by yielding a 27.23% performance improvement under an equivalent budget constraint on the Cityscapes dataset. The code will be released upon acceptance.</li>
</ul>

<h3>Title: Are LLMs Good Text Diacritizers? An Arabic and Yorùbá Case Study</h3>
<ul>
<li><strong>Authors: </strong>Hawau Olamide Toyin, Samar M. Magdy, Hanan Aldarmaki</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11602">https://arxiv.org/abs/2506.11602</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11602">https://arxiv.org/pdf/2506.11602</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11602]] Are LLMs Good Text Diacritizers? An Arabic and Yorùbá Case Study(https://arxiv.org/abs/2506.11602)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We investigate the effectiveness of large language models (LLMs) for text diacritization in two typologically distinct languages: Arabic and Yoruba. To enable a rigorous evaluation, we introduce a novel multilingual dataset MultiDiac, with diverse samples that capture a range of diacritic ambiguities. We evaluate 14 LLMs varying in size, accessibility, and language coverage, and benchmark them against 6 specialized diacritization models. Additionally, we fine-tune four small open-source models using LoRA for Yoruba. Our results show that many off-the-shelf LLMs outperform specialized diacritization models for both Arabic and Yoruba, but smaller models suffer from hallucinations. Fine-tuning on a small dataset can help improve diacritization performance and reduce hallucination rates.</li>
</ul>

<h3>Title: KCES: Training-Free Defense for Robust Graph Neural Networks via Kernel Complexity</h3>
<ul>
<li><strong>Authors: </strong>Yaning Jia, Shenyang Deng, Chiyu Ma, Yaoqing Yang, Soroush Vosoughi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11611">https://arxiv.org/abs/2506.11611</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11611">https://arxiv.org/pdf/2506.11611</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11611]] KCES: Training-Free Defense for Robust Graph Neural Networks via Kernel Complexity(https://arxiv.org/abs/2506.11611)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) have achieved impressive success across a wide range of graph-based tasks, yet they remain highly vulnerable to small, imperceptible perturbations and adversarial attacks. Although numerous defense methods have been proposed to address these vulnerabilities, many rely on heuristic metrics, overfit to specific attack patterns, and suffer from high computational complexity. In this paper, we propose Kernel Complexity-Based Edge Sanitization (KCES), a training-free, model-agnostic defense framework. KCES leverages Graph Kernel Complexity (GKC), a novel metric derived from the graph's Gram matrix that characterizes GNN generalization via its test error bound. Building on GKC, we define a KC score for each edge, measuring the change in GKC when the edge is removed. Edges with high KC scores, typically introduced by adversarial perturbations, are pruned to mitigate their harmful effects, thereby enhancing GNNs' robustness. KCES can also be seamlessly integrated with existing defense strategies as a plug-and-play module without requiring training. Theoretical analysis and extensive experiments demonstrate that KCES consistently enhances GNN robustness, outperforms state-of-the-art baselines, and amplifies the effectiveness of existing defenses, offering a principled and efficient solution for securing GNNs.</li>
</ul>

<h3>Title: KEENHash: Hashing Programs into Function-Aware Embeddings for Large-Scale Binary Code Similarity Analysis</h3>
<ul>
<li><strong>Authors: </strong>Zhijie Liu, Qiyi Tang, Sen Nie, Shi Wu, Liang Feng Zhang, Yutian Tang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11612">https://arxiv.org/abs/2506.11612</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11612">https://arxiv.org/pdf/2506.11612</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11612]] KEENHash: Hashing Programs into Function-Aware Embeddings for Large-Scale Binary Code Similarity Analysis(https://arxiv.org/abs/2506.11612)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Binary code similarity analysis (BCSA) is a crucial research area in many fields such as cybersecurity. Specifically, function-level diffing tools are the most widely used in BCSA: they perform function matching one by one for evaluating the similarity between binary programs. However, such methods need a high time complexity, making them unscalable in large-scale scenarios (e.g., 1/n-to-n search). Towards effective and efficient program-level BCSA, we propose KEENHash, a novel hashing approach that hashes binaries into program-level representations through large language model (LLM)-generated function embeddings. KEENHash condenses a binary into one compact and fixed-length program embedding using K-Means and Feature Hashing, allowing us to do effective and efficient large-scale program-level BCSA, surpassing the previous state-of-the-art methods. The experimental results show that KEENHash is at least 215 times faster than the state-of-the-art function matching tools while maintaining effectiveness. Furthermore, in a large-scale scenario with 5.3 billion similarity evaluations, KEENHash takes only 395.83 seconds while these tools will cost at least 56 days. We also evaluate KEENHash on the program clone search of large-scale BCSA across extensive datasets in 202,305 binaries. Compared with 4 state-of-the-art methods, KEENHash outperforms all of them by at least 23.16%, and displays remarkable superiority over them in the large-scale BCSA security scenario of malware detection.</li>
</ul>

<h3>Title: Model Organisms for Emergent Misalignment</h3>
<ul>
<li><strong>Authors: </strong>Edward Turner, Anna Soligo, Mia Taylor, Senthooran Rajamanoharan, Neel Nanda</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11613">https://arxiv.org/abs/2506.11613</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11613">https://arxiv.org/pdf/2506.11613</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11613]] Model Organisms for Emergent Misalignment(https://arxiv.org/abs/2506.11613)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Recent work discovered Emergent Misalignment (EM): fine-tuning large language models on narrowly harmful datasets can lead them to become broadly misaligned. A survey of experts prior to publication revealed this was highly unexpected, demonstrating critical gaps in our understanding of model alignment. In this work, we both advance understanding and provide tools for future research. Using new narrowly misaligned datasets, we create a set of improved model organisms that achieve 99% coherence (vs. 67% prior), work with smaller 0.5B parameter models (vs. 32B), and that induce misalignment using a single rank-1 LoRA adapter. We demonstrate that EM occurs robustly across diverse model sizes, three model families, and numerous training protocols including full supervised fine-tuning. Leveraging these cleaner model organisms, we isolate a mechanistic phase transition and demonstrate that it corresponds to a robust behavioural phase transition in all studied organisms. Aligning large language models is critical for frontier AI safety, yet EM exposes how far we are from achieving this robustly. By distilling clean model organisms that isolate a minimal alignment-compromising change, and where this is learnt, we establish a foundation for future research into understanding and mitigating alignment risks in LLMs.</li>
</ul>

<h3>Title: Machine Unlearning for Robust DNNs: Attribution-Guided Partitioning and Neuron Pruning in Noisy Environments</h3>
<ul>
<li><strong>Authors: </strong>Deliang Jin, Gang Chen, Shuo Feng, Yufeng Ling, Haoran Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11615">https://arxiv.org/abs/2506.11615</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11615">https://arxiv.org/pdf/2506.11615</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11615]] Machine Unlearning for Robust DNNs: Attribution-Guided Partitioning and Neuron Pruning in Noisy Environments(https://arxiv.org/abs/2506.11615)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep neural networks (DNNs) have achieved remarkable success across diverse domains, but their performance can be severely degraded by noisy or corrupted training data. Conventional noise mitigation methods often rely on explicit assumptions about noise distributions or require extensive retraining, which can be impractical for large-scale models. Inspired by the principles of machine unlearning, we propose a novel framework that integrates attribution-guided data partitioning, discriminative neuron pruning, and targeted fine-tuning to mitigate the impact of noisy samples. Our approach employs gradient-based attribution to probabilistically distinguish high-quality examples from potentially corrupted ones without imposing restrictive assumptions on the noise. It then applies regression-based sensitivity analysis to identify and prune neurons that are most vulnerable to noise. Finally, the resulting network is fine-tuned on the high-quality data subset to efficiently recover and enhance its generalization performance. This integrated unlearning-inspired framework provides several advantages over conventional noise-robust learning approaches. Notably, it combines data-level unlearning with model-level adaptation, thereby avoiding the need for full model retraining or explicit noise modeling. We evaluate our method on representative tasks (e.g., CIFAR-10 image classification and speech recognition) under various noise levels and observe substantial gains in both accuracy and efficiency. For example, our framework achieves approximately a 10% absolute accuracy improvement over standard retraining on CIFAR-10 with injected label noise, while reducing retraining time by up to 47% in some settings. These results demonstrate the effectiveness and scalability of the proposed approach for achieving robust generalization in noisy environments.</li>
</ul>

<h3>Title: Convergent Linear Representations of Emergent Misalignment</h3>
<ul>
<li><strong>Authors: </strong>Anna Soligo, Edward Turner, Senthooran Rajamanoharan, Neel Nanda</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11618">https://arxiv.org/abs/2506.11618</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11618">https://arxiv.org/pdf/2506.11618</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11618]] Convergent Linear Representations of Emergent Misalignment(https://arxiv.org/abs/2506.11618)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning large language models on narrow datasets can cause them to develop broadly misaligned behaviours: a phenomena known as emergent misalignment. However, the mechanisms underlying this misalignment, and why it generalizes beyond the training domain, are poorly understood, demonstrating critical gaps in our knowledge of model alignment. In this work, we train and study a minimal model organism which uses just 9 rank-1 adapters to emergently misalign Qwen2.5-14B-Instruct. Studying this, we find that different emergently misaligned models converge to similar representations of misalignment. We demonstrate this convergence by extracting a 'misalignment direction' from one fine-tuned model's activations, and using it to effectively ablate misaligned behaviour from fine-tunes using higher dimensional LoRAs and different datasets. Leveraging the scalar hidden state of rank-1 LoRAs, we further present a set of experiments for directly interpreting the fine-tuning adapters, showing that six contribute to general misalignment, while two specialise for misalignment in just the fine-tuning domain. Emergent misalignment is a particularly salient example of undesirable and unexpected model behaviour and by advancing our understanding of the mechanisms behind it, we hope to move towards being able to better understand and mitigate misalignment more generally.</li>
</ul>

<h3>Title: SignAligner: Harmonizing Complementary Pose Modalities for Coherent Sign Language Generation</h3>
<ul>
<li><strong>Authors: </strong>Xu Wang, Shengeng Tang, Lechao Cheng, Feng Li, Shuo Wang, Richang Hong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11621">https://arxiv.org/abs/2506.11621</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11621">https://arxiv.org/pdf/2506.11621</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11621]] SignAligner: Harmonizing Complementary Pose Modalities for Coherent Sign Language Generation(https://arxiv.org/abs/2506.11621)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Sign language generation aims to produce diverse sign representations based on spoken language. However, achieving realistic and naturalistic generation remains a significant challenge due to the complexity of sign language, which encompasses intricate hand gestures, facial expressions, and body movements. In this work, we introduce PHOENIX14T+, an extended version of the widely-used RWTH-PHOENIX-Weather 2014T dataset, featuring three new sign representations: Pose, Hamer and Smplerx. We also propose a novel method, SignAligner, for realistic sign language generation, consisting of three stages: text-driven pose modalities co-generation, online collaborative correction of multimodality, and realistic sign video synthesis. First, by incorporating text semantics, we design a joint sign language generator to simultaneously produce posture coordinates, gesture actions, and body movements. The text encoder, based on a Transformer architecture, extracts semantic features, while a cross-modal attention mechanism integrates these features to generate diverse sign language representations, ensuring accurate mapping and controlling the diversity of modal features. Next, online collaborative correction is introduced to refine the generated pose modalities using a dynamic loss weighting strategy and cross-modal attention, facilitating the complementarity of information across modalities, eliminating spatiotemporal conflicts, and ensuring semantic coherence and action consistency. Finally, the corrected pose modalities are fed into a pre-trained video generation network to produce high-fidelity sign language videos. Extensive experiments demonstrate that SignAligner significantly improves both the accuracy and expressiveness of the generated sign videos.</li>
</ul>

<h3>Title: Physically-informed change-point kernels for structural dynamics</h3>
<ul>
<li><strong>Authors: </strong>Daniel James Pitchforth, Matthew Rhys Jones, Samuel John Gibson, Elizabeth Jane Cross</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11625">https://arxiv.org/abs/2506.11625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11625">https://arxiv.org/pdf/2506.11625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11625]] Physically-informed change-point kernels for structural dynamics(https://arxiv.org/abs/2506.11625)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>The relative balance between physics and data within any physics-informed machine learner is an important modelling consideration to ensure that the benefits of both physics and data-based approaches are maximised. An over reliance on physical knowledge can be detrimental, particularly when the physics-based component of a model may not accurately represent the true underlying system. An underutilisation of physical knowledge potentially wastes a valuable resource, along with benefits in model interpretability and reduced demand for expensive data collection. Achieving an optimal physics-data balance is a challenging aspect of model design, particularly if the level varies through time; for example, one might have a physical approximation, only valid within particular regimes, or a physical phenomenon may be known to only occur when given conditions are met (e.g. at high temperatures). This paper develops novel, physically-informed, change-point kernels for Gaussian processes, capable of dynamically varying the reliance upon available physical knowledge. A high level of control is granted to a user, allowing for the definition of conditions in which they believe a phenomena should occur and the rate at which the knowledge should be phased in and out of a model. In circumstances where users may be less certain, the switching reliance upon physical knowledge may be automatically learned and recovered from the model in an interpretable and intuitive manner. Variation of the modelled noise based on the physical phenomena occurring is also implemented to provide a more representative capture of uncertainty alongside predictions. The capabilities of the new kernel structures are explored through the use of two engineering case studies: the directional wind loading of a cable-stayed bridge and the prediction of aircraft wing strain during in-flight manoeuvring.</li>
</ul>

<h3>Title: Evaluating Fairness and Mitigating Bias in Machine Learning: A Novel Technique using Tensor Data and Bayesian Regression</h3>
<ul>
<li><strong>Authors: </strong>Kuniko Paxton, Koorosh Aslansefat, Dhavalkumar Thakker, Yiannis Papadopoulos</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11627">https://arxiv.org/abs/2506.11627</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11627">https://arxiv.org/pdf/2506.11627</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11627]] Evaluating Fairness and Mitigating Bias in Machine Learning: A Novel Technique using Tensor Data and Bayesian Regression(https://arxiv.org/abs/2506.11627)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Fairness is a critical component of Trustworthy AI. In this paper, we focus on Machine Learning (ML) and the performance of model predictions when dealing with skin color. Unlike other sensitive attributes, the nature of skin color differs significantly. In computer vision, skin color is represented as tensor data rather than categorical values or single numerical points. However, much of the research on fairness across sensitive groups has focused on categorical features such as gender and race. This paper introduces a new technique for evaluating fairness in ML for image classification tasks, specifically without the use of annotation. To address the limitations of prior work, we handle tensor data, like skin color, without classifying it rigidly. Instead, we convert it into probability distributions and apply statistical distance measures. This novel approach allows us to capture fine-grained nuances in fairness both within and across what would traditionally be considered distinct groups. Additionally, we propose an innovative training method to mitigate the latent biases present in conventional skin tone categorization. This method leverages color distance estimates calculated through Bayesian regression with polynomial functions, ensuring a more nuanced and equitable treatment of skin color in ML models.</li>
</ul>

<h3>Title: FAA Framework: A Large Language Model-Based Approach for Credit Card Fraud Investigations</h3>
<ul>
<li><strong>Authors: </strong>Shaun Shuster, Eyal Zaloof, Asaf Shabtai, Rami Puzis</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11635">https://arxiv.org/abs/2506.11635</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11635">https://arxiv.org/pdf/2506.11635</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11635]] FAA Framework: A Large Language Model-Based Approach for Credit Card Fraud Investigations(https://arxiv.org/abs/2506.11635)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The continuous growth of the e-commerce industry attracts fraudsters who exploit stolen credit card details. Companies often investigate suspicious transactions in order to retain customer trust and address gaps in their fraud detection systems. However, analysts are overwhelmed with an enormous number of alerts from credit card transaction monitoring systems. Each alert investigation requires from the fraud analysts careful attention, specialized knowledge, and precise documentation of the outcomes, leading to alert fatigue. To address this, we propose a fraud analyst assistant (FAA) framework, which employs multi-modal large language models (LLMs) to automate credit card fraud investigations and generate explanatory reports. The FAA framework leverages the reasoning, code execution, and vision capabilities of LLMs to conduct planning, evidence collection, and analysis in each investigation step. A comprehensive empirical evaluation of 500 credit card fraud investigations demonstrates that the FAA framework produces reliable and efficient investigations comprising seven steps on average. Thus we found that the FAA framework can automate large parts of the workload and help reduce the challenges faced by fraud analysts.</li>
</ul>

<h3>Title: LoRA-Gen: Specializing Large Language Model via Online LoRA Generation</h3>
<ul>
<li><strong>Authors: </strong>Yicheng Xiao, Lin Song, Rui Yang, Cheng Cheng, Yixiao Ge, Xiu Li, Ying Shan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11638">https://arxiv.org/abs/2506.11638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11638">https://arxiv.org/pdf/2506.11638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11638]] LoRA-Gen: Specializing Large Language Model via Online LoRA Generation(https://arxiv.org/abs/2506.11638)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances have highlighted the benefits of scaling language models to enhance performance across a wide range of NLP tasks. However, these approaches still face limitations in effectiveness and efficiency when applied to domain-specific tasks, particularly for small edge-side models. We propose the LoRA-Gen framework, which utilizes a large cloud-side model to generate LoRA parameters for edge-side models based on task descriptions. By employing the reparameterization technique, we merge the LoRA parameters into the edge-side model to achieve flexible specialization. Our method facilitates knowledge transfer between models while significantly improving the inference efficiency of the specialized model by reducing the input context length. Without specialized training, LoRA-Gen outperforms conventional LoRA fine-tuning, which achieves competitive accuracy and a 2.1x speedup with TinyLLaMA-1.1B in reasoning tasks. Besides, our method delivers a compression ratio of 10.1x with Gemma-2B on intelligent agent tasks.</li>
</ul>

<h3>Title: Prohibited Items Segmentation via Occlusion-aware Bilayer Modeling</h3>
<ul>
<li><strong>Authors: </strong>Yunhan Ren, Ruihuang Li, Lingbo Liu, Changwen Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11661">https://arxiv.org/abs/2506.11661</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11661">https://arxiv.org/pdf/2506.11661</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11661]] Prohibited Items Segmentation via Occlusion-aware Bilayer Modeling(https://arxiv.org/abs/2506.11661)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, segmentation</a></li>
<li><strong>Abstract: </strong>Instance segmentation of prohibited items in security X-ray images is a critical yet challenging task. This is mainly caused by the significant appearance gap between prohibited items in X-ray images and natural objects, as well as the severe overlapping among objects in X-ray images. To address these issues, we propose an occlusion-aware instance segmentation pipeline designed to identify prohibited items in X-ray images. Specifically, to bridge the representation gap, we integrate the Segment Anything Model (SAM) into our pipeline, taking advantage of its rich priors and zero-shot generalization capabilities. To address the overlap between prohibited items, we design an occlusion-aware bilayer mask decoder module that explicitly models the occlusion relationships. To supervise occlusion estimation, we manually annotated occlusion areas of prohibited items in two large-scale X-ray image segmentation datasets, PIDray and PIXray. We then reorganized these additional annotations together with the original information as two occlusion-annotated datasets, PIDray-A and PIXray-A. Extensive experimental results on these occlusion-annotated datasets demonstrate the effectiveness of our proposed method. The datasets and codes are available at: this https URL</li>
</ul>

<h3>Title: Converting Annotated Clinical Cases into Structured Case Report Forms</h3>
<ul>
<li><strong>Authors: </strong>Pietro Ferrazzi, Alberto Lavelli, Bernardo Magnini</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11666">https://arxiv.org/abs/2506.11666</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11666">https://arxiv.org/pdf/2506.11666</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11666]] Converting Annotated Clinical Cases into Structured Case Report Forms(https://arxiv.org/abs/2506.11666)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Case Report Forms (CRFs) are largely used in medical research as they ensure accuracy, reliability, and validity of results in clinical studies. However, publicly available, wellannotated CRF datasets are scarce, limiting the development of CRF slot filling systems able to fill in a CRF from clinical notes. To mitigate the scarcity of CRF datasets, we propose to take advantage of available datasets annotated for information extraction tasks and to convert them into structured CRFs. We present a semi-automatic conversion methodology, which has been applied to the E3C dataset in two languages (English and Italian), resulting in a new, high-quality dataset for CRF slot filling. Through several experiments on the created dataset, we report that slot filling achieves 59.7% for Italian and 67.3% for English on a closed Large Language Models (zero-shot) and worse performances on three families of open-source models, showing that filling CRFs is challenging even for recent state-of-the-art LLMs. We release the datest at this https URL</li>
</ul>

<h3>Title: DTHA: A Digital Twin-Assisted Handover Authentication Scheme for 5G and Beyond</h3>
<ul>
<li><strong>Authors: </strong>Guanjie Li, Tom H. Luan, Chengzhe Lai, Jinkai Zheng, Rongxing Lu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11669">https://arxiv.org/abs/2506.11669</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11669">https://arxiv.org/pdf/2506.11669</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11669]] DTHA: A Digital Twin-Assisted Handover Authentication Scheme for 5G and Beyond(https://arxiv.org/abs/2506.11669)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>With the rapid development and extensive deployment of the fifth-generation wireless system (5G), it has achieved ubiquitous high-speed connectivity and improved overall communication performance. Additionally, as one of the promising technologies for integration beyond 5G, digital twin in cyberspace can interact with the core network, transmit essential information, and further enhance the wireless communication quality of the corresponding mobile device (MD). However, the utilization of millimeter-wave, terahertz band, and ultra-dense network technologies presents urgent challenges for MD in 5G and beyond, particularly in terms of frequent handover authentication with target base stations during faster mobility, which can cause connection interruption and incur malicious attacks. To address such challenges in 5G and beyond, in this paper, we propose a secure and efficient handover authentication scheme by utilizing digital twin. Acting as an intelligent intermediate, the authorized digital twin can handle computations and assist the corresponding MD in performing secure mutual authentication and key negotiation in advance before attaching the target base stations in both intra-domain and inter-domain scenarios. In addition, we provide the formal verification based on BAN logic, RoR model, and ProVerif, and informal analysis to demonstrate that the proposed scheme can offer diverse security functionality. Performance evaluation shows that the proposed scheme outperforms most related schemes in terms of signaling, computation, and communication overheads.</li>
</ul>

<h3>Title: Dynamic Mixture of Curriculum LoRA Experts for Continual Multimodal Instruction Tuning</h3>
<ul>
<li><strong>Authors: </strong>Chendi Ge, Xin Wang, Zeyang Zhang, Hong Chen, Jiapei Fan, Longtao Huang, Hui Xue, Wenwu Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11672">https://arxiv.org/abs/2506.11672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11672">https://arxiv.org/pdf/2506.11672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11672]] Dynamic Mixture of Curriculum LoRA Experts for Continual Multimodal Instruction Tuning(https://arxiv.org/abs/2506.11672)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Continual multimodal instruction tuning is crucial for adapting Multimodal Large Language Models (MLLMs) to evolving tasks. However, most existing methods adopt a fixed architecture, struggling with adapting to new tasks due to static model capacity. We propose to evolve the architecture under parameter budgets for dynamic task adaptation, which remains unexplored and imposes two challenges: 1) task architecture conflict, where different tasks require varying layer-wise adaptations, and 2) modality imbalance, where different tasks rely unevenly on modalities, leading to unbalanced updates. To address these challenges, we propose a novel Dynamic Mixture of Curriculum LoRA Experts (D-MoLE) method, which automatically evolves MLLM's architecture with controlled parameter budgets to continually adapt to new tasks while retaining previously learned knowledge. Specifically, we propose a dynamic layer-wise expert allocator, which automatically allocates LoRA experts across layers to resolve architecture conflicts, and routes instructions layer-wisely to facilitate knowledge sharing among experts. Then, we propose a gradient-based inter-modal continual curriculum, which adjusts the update ratio of each module in MLLM based on the difficulty of each modality within the task to alleviate the modality imbalance problem. Extensive experiments show that D-MoLE significantly outperforms state-of-the-art baselines, achieving a 15% average improvement over the best baseline. To the best of our knowledge, this is the first study of continual learning for MLLMs from an architectural perspective.</li>
</ul>

<h3>Title: Cross-Modal Clustering-Guided Negative Sampling for Self-Supervised Joint Learning from Medical Images and Reports</h3>
<ul>
<li><strong>Authors: </strong>Libin Lan, Hongxing Li, Zunhui Xia, Juan Zhou, Xiaofei Zhu, Yongmei Li, Yudong Zhang, Xin Luo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11674">https://arxiv.org/abs/2506.11674</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11674">https://arxiv.org/pdf/2506.11674</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11674]] Cross-Modal Clustering-Guided Negative Sampling for Self-Supervised Joint Learning from Medical Images and Reports(https://arxiv.org/abs/2506.11674)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Learning medical visual representations directly from paired images and reports through multimodal self-supervised learning has emerged as a novel and efficient approach to digital diagnosis in recent years. However, existing models suffer from several severe limitations. 1) neglecting the selection of negative samples, resulting in the scarcity of hard negatives and the inclusion of false negatives; 2) focusing on global feature extraction, but overlooking the fine-grained local details that are crucial for medical image recognition tasks; and 3) contrastive learning primarily targets high-level features but ignoring low-level details which are essential for accurate medical analysis. Motivated by these critical issues, this paper presents a Cross-Modal Cluster-Guided Negative Sampling (CM-CGNS) method with two-fold ideas. First, it extends the k-means clustering used for local text features in the single-modal domain to the multimodal domain through cross-modal attention. This improvement increases the number of negative samples and boosts the model representation capability. Second, it introduces a Cross-Modal Masked Image Reconstruction (CM-MIR) module that leverages local text-to-image features obtained via cross-modal attention to reconstruct masked local image regions. This module significantly strengthens the model's cross-modal information interaction capabilities and retains low-level image features essential for downstream tasks. By well handling the aforementioned limitations, the proposed CM-CGNS can learn effective and robust medical visual representations suitable for various recognition tasks. Extensive experimental results on classification, detection, and segmentation tasks across five downstream datasets show that our method outperforms state-of-the-art approaches on multiple metrics, verifying its superior performance.</li>
</ul>

<h3>Title: Predicting Patient Survival with Airway Biomarkers using nn-Unet/Radiomics</h3>
<ul>
<li><strong>Authors: </strong>Zacharia Mesbah, Dhruv Jain, Tsiry Mayet, Romain Modzelewski, Romain Herault, Simon Bernard, Sebastien Thureau, Clement Chatelain</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11677">https://arxiv.org/abs/2506.11677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11677">https://arxiv.org/pdf/2506.11677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11677]] Predicting Patient Survival with Airway Biomarkers using nn-Unet/Radiomics(https://arxiv.org/abs/2506.11677)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The primary objective of the AIIB 2023 competition is to evaluate the predictive significance of airway-related imaging biomarkers in determining the survival outcomes of patients with lung this http URL study introduces a comprehensive three-stage approach. Initially, a segmentation network, namely nn-Unet, is employed to delineate the airway's structural boundaries. Subsequently, key features are extracted from the radiomic images centered around the trachea and an enclosing bounding box around the airway. This step is motivated by the potential presence of critical survival-related insights within the tracheal region as well as pertinent information encoded in the structure and dimensions of the airway. Lastly, radiomic features obtained from the segmented areas are integrated into an SVM classifier. We could obtain an overall-score of 0.8601 for the segmentation in Task 1 while 0.7346 for the classification in Task 2.</li>
</ul>

<h3>Title: Pose Matters: Evaluating Vision Transformers and CNNs for Human Action Recognition on Small COCO Subsets</h3>
<ul>
<li><strong>Authors: </strong>MingZe Tang, Madiha Kazi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11678">https://arxiv.org/abs/2506.11678</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11678">https://arxiv.org/pdf/2506.11678</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11678]] Pose Matters: Evaluating Vision Transformers and CNNs for Human Action Recognition on Small COCO Subsets(https://arxiv.org/abs/2506.11678)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, transformer</a></li>
<li><strong>Abstract: </strong>This study explores human action recognition using a three-class subset of the COCO image corpus, benchmarking models from simple fully connected networks to transformer architectures. The binary Vision Transformer (ViT) achieved 90% mean test accuracy, significantly exceeding multiclass classifiers such as convolutional networks (approximately 35%) and CLIP-based models (approximately 62-64%). A one-way ANOVA (F = 61.37, p < 0.001) confirmed these differences are statistically significant. Qualitative analysis with SHAP explainer and LeGrad heatmaps indicated that the ViT localizes pose-specific regions (e.g., lower limbs for walking or running), while simpler feed-forward models often focus on background textures, explaining their errors. These findings emphasize the data efficiency of transformer representations and the importance of explainability techniques in diagnosing class-specific failures.</li>
</ul>

<h3>Title: LLMs on support of privacy and security of mobile apps: state of the art and research directions</h3>
<ul>
<li><strong>Authors: </strong>Tran Thanh Lam Nguyen, Barbara Carminati, Elena Ferrari</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11679">https://arxiv.org/abs/2506.11679</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11679">https://arxiv.org/pdf/2506.11679</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11679]] LLMs on support of privacy and security of mobile apps: state of the art and research directions(https://arxiv.org/abs/2506.11679)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, large language model</a></li>
<li><strong>Abstract: </strong>Modern life has witnessed the explosion of mobile devices. However, besides the valuable features that bring convenience to end users, security and privacy risks still threaten users of mobile apps. The increasing sophistication of these threats in recent years has underscored the need for more advanced and efficient detection approaches. In this chapter, we explore the application of Large Language Models (LLMs) to identify security risks and privacy violations and mitigate them for the mobile application ecosystem. By introducing state-of-the-art research that applied LLMs to mitigate the top 10 common security risks of smartphone platforms, we highlight the feasibility and potential of LLMs to replace traditional analysis methods, such as dynamic and hybrid analysis of mobile apps. As a representative example of LLM-based solutions, we present an approach to detect sensitive data leakage when users share images online, a common behavior of smartphone users nowadays. Finally, we discuss open research challenges.</li>
</ul>

<h3>Title: LLMs for Sentence Simplification: A Hybrid Multi-Agent prompting Approach</h3>
<ul>
<li><strong>Authors: </strong>Pratibha Zunjare, Michael Hsiao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11681">https://arxiv.org/abs/2506.11681</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11681">https://arxiv.org/pdf/2506.11681</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11681]] LLMs for Sentence Simplification: A Hybrid Multi-Agent prompting Approach(https://arxiv.org/abs/2506.11681)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper addresses the challenge of transforming complex sentences into sequences of logical, simplified sentences while preserving semantic and logical integrity with the help of Large Language Models. We propose a hybrid approach that combines advanced prompting with multi-agent architectures to enhance the sentence simplification process. Experimental results show that our approach was able to successfully simplify 70% of the complex sentences written for video game design application. In comparison, a single-agent approach attained a 48% success rate on the same task.</li>
</ul>

<h3>Title: MTabVQA: Evaluating Multi-Tabular Reasoning of Language Models in Visual Space</h3>
<ul>
<li><strong>Authors: </strong>Anshul Singh, Chris Biemann, Jan Strich</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11684">https://arxiv.org/abs/2506.11684</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11684">https://arxiv.org/pdf/2506.11684</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11684]] MTabVQA: Evaluating Multi-Tabular Reasoning of Language Models in Visual Space(https://arxiv.org/abs/2506.11684)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) have demonstrated remarkable capabilities in interpreting visual layouts and text. However, a significant challenge remains in their ability to interpret robustly and reason over multi-tabular data presented as images, a common occurrence in real-world scenarios like web pages and digital documents. Existing benchmarks typically address single tables or non-visual data (text/structured). This leaves a critical gap: they don't assess the ability to parse diverse table images, correlate information across them, and perform multi-hop reasoning on the combined visual data. We introduce MTabVQA, a novel benchmark specifically designed for multi-tabular visual question answering to bridge that gap. MTabVQA comprises 3,745 complex question-answer pairs that necessitate multi-hop reasoning across several visually rendered table images. We provide extensive benchmark results for state-of-the-art VLMs on MTabVQA, revealing significant performance limitations. We further investigate post-training techniques to enhance these reasoning abilities and release MTabVQA-Instruct, a large-scale instruction-tuning dataset. Our experiments show that fine-tuning VLMs with MTabVQA-Instruct substantially improves their performance on visual multi-tabular reasoning. Code and dataset (this https URL) are available online (this https URL).</li>
</ul>

<h3>Title: Differential Privacy in Machine Learning: From Symbolic AI to LLMs</h3>
<ul>
<li><strong>Authors: </strong>Francisco Aguilera-Martínez, Fernando Berzal</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11687">https://arxiv.org/abs/2506.11687</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11687">https://arxiv.org/pdf/2506.11687</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11687]] Differential Privacy in Machine Learning: From Symbolic AI to LLMs(https://arxiv.org/abs/2506.11687)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, attack</a></li>
<li><strong>Abstract: </strong>Machine learning models should not reveal particular information that is not otherwise accessible. Differential privacy provides a formal framework to mitigate privacy risks by ensuring that the inclusion or exclusion of any single data point does not significantly alter the output of an algorithm, thus limiting the exposure of private information. This survey paper explores the foundational definitions of differential privacy, reviews its original formulations and tracing its evolution through key research contributions. It then provides an in-depth examination of how DP has been integrated into machine learning models, analyzing existing proposals and methods to preserve privacy when training ML models. Finally, it describes how DP-based ML techniques can be evaluated in practice. %Finally, it discusses the broader implications of DP, highlighting its potential for public benefit, its real-world applications, and the challenges it faces, including vulnerabilities to adversarial attacks. By offering a comprehensive overview of differential privacy in machine learning, this work aims to contribute to the ongoing development of secure and responsible AI systems.</li>
</ul>

<h3>Title: DMAF-Net: An Effective Modality Rebalancing Framework for Incomplete Multi-Modal Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Libin Lan, Hongxing Li, Zunhui Xia, Yudong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11691">https://arxiv.org/abs/2506.11691</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11691">https://arxiv.org/pdf/2506.11691</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11691]] DMAF-Net: An Effective Modality Rebalancing Framework for Incomplete Multi-Modal Medical Image Segmentation(https://arxiv.org/abs/2506.11691)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Incomplete multi-modal medical image segmentation faces critical challenges from modality imbalance, including imbalanced modality missing rates and heterogeneous modality contributions. Due to their reliance on idealized assumptions of complete modality availability, existing methods fail to dynamically balance contributions and neglect the structural relationships between modalities, resulting in suboptimal performance in real-world clinical scenarios. To address these limitations, we propose a novel model, named Dynamic Modality-Aware Fusion Network (DMAF-Net). The DMAF-Net adopts three key ideas. First, it introduces a Dynamic Modality-Aware Fusion (DMAF) module to suppress missing-modality interference by combining transformer attention with adaptive masking and weight modality contributions dynamically through attention maps. Second, it designs a synergistic Relation Distillation and Prototype Distillation framework to enforce global-local feature alignment via covariance consistency and masked graph attention, while ensuring semantic consistency through cross-modal class-specific prototype alignment. Third, it presents a Dynamic Training Monitoring (DTM) strategy to stabilize optimization under imbalanced missing rates by tracking distillation gaps in real-time, and to balance convergence speeds across modalities by adaptively reweighting losses and scaling gradients. Extensive experiments on BraTS2020 and MyoPS2020 demonstrate that DMAF-Net outperforms existing methods for incomplete multi-modal medical image segmentation. Extensive experiments on BraTS2020 and MyoPS2020 demonstrate that DMAF-Net outperforms existing methods for incomplete multi-modal medical image segmentation. Our code is available at this https URL.</li>
</ul>

<h3>Title: Geometry-Aware Edge Pooling for Graph Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Katharina Limbeck, Lydia Mezrag, Guy Wolf, Bastian Rieck</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11700">https://arxiv.org/abs/2506.11700</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11700">https://arxiv.org/pdf/2506.11700</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11700]] Geometry-Aware Edge Pooling for Graph Neural Networks(https://arxiv.org/abs/2506.11700)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, diffusion</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) have shown significant success for graph-based tasks. Motivated by the prevalence of large datasets in real-world applications, pooling layers are crucial components of GNNs. By reducing the size of input graphs, pooling enables faster training and potentially better generalisation. However, existing pooling operations often optimise for the learning task at the expense of fundamental graph structures and interpretability. This leads to unreliable performance across varying dataset types, downstream tasks and pooling ratios. Addressing these concerns, we propose novel graph pooling layers for structure aware pooling via edge collapses. Our methods leverage diffusion geometry and iteratively reduce a graph's size while preserving both its metric structure and structural diversity. We guide pooling using magnitude, an isometry-invariant diversity measure, which permits us to control the fidelity of the pooling process. Further, we use the spread of a metric space as a faster and more stable alternative ensuring computational efficiency. Empirical results demonstrate that our methods (i) achieve superior performance compared to alternative pooling layers across a range of diverse graph classification tasks, (ii) preserve key spectral properties of the input graphs, and (iii) retain high accuracy across varying pooling ratios.</li>
</ul>

<h3>Title: DART: Distilling Autoregressive Reasoning to Silent Thought</h3>
<ul>
<li><strong>Authors: </strong>Nan Jiang, Ziming Wu, De-Chuan Zhan, Fuming Lai, Shaobing Lian</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11752">https://arxiv.org/abs/2506.11752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11752">https://arxiv.org/pdf/2506.11752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11752]] DART: Distilling Autoregressive Reasoning to Silent Thought(https://arxiv.org/abs/2506.11752)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Chain-of-Thought (CoT) reasoning has significantly advanced Large Language Models (LLMs) in solving complex tasks. However, its autoregressive paradigm leads to significant computational overhead, hindering its deployment in latency-sensitive applications. To address this, we propose \textbf{DART} (\textbf{D}istilling \textbf{A}utoregressive \textbf{R}easoning to Silent \textbf{T}hought), a self-distillation framework that enables LLMs to replace autoregressive CoT with non-autoregressive Silent Thought (ST). Specifically, DART introduces two training pathways: the CoT pathway for traditional reasoning and the ST pathway for generating answers directly from a few ST tokens. The ST pathway utilizes a lightweight Reasoning Evolvement Module (REM) to align its hidden states with the CoT pathway, enabling the ST tokens to evolve into informative embeddings. During inference, only the ST pathway is activated, leveraging evolving ST tokens to deliver the answer directly. Extensive experimental results demonstrate that DART achieves comparable reasoning performance to existing baselines while offering significant efficiency gains, serving as a feasible alternative for efficient reasoning.</li>
</ul>

<h3>Title: DiffFuSR: Super-Resolution of all Sentinel-2 Multispectral Bands using Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Sarmad, Arnt-Børre Salberg, Michael Kampffmeyer</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11764">https://arxiv.org/abs/2506.11764</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11764">https://arxiv.org/pdf/2506.11764</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11764]] DiffFuSR: Super-Resolution of all Sentinel-2 Multispectral Bands using Diffusion Models(https://arxiv.org/abs/2506.11764)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>This paper presents DiffFuSR, a modular pipeline for super-resolving all 12 spectral bands of Sentinel-2 Level-2A imagery to a unified ground sampling distance (GSD) of 2.5 meters. The pipeline comprises two stages: (i) a diffusion-based super-resolution (SR) model trained on high-resolution RGB imagery from the NAIP and WorldStrat datasets, harmonized to simulate Sentinel-2 characteristics; and (ii) a learned fusion network that upscales the remaining multispectral bands using the super-resolved RGB image as a spatial prior. We introduce a robust degradation model and contrastive degradation encoder to support blind SR. Extensive evaluations of the proposed SR pipeline on the OpenSR benchmark demonstrate that the proposed method outperforms current SOTA baselines in terms of reflectance fidelity, spectral consistency, spatial alignment, and hallucination suppression. Furthermore, the fusion network significantly outperforms classical pansharpening approaches, enabling accurate enhancement of Sentinel-2's 20 m and 60 m bands. This study underscores the power of harmonized learning with generative priors and fusion strategies to create a modular framework for Sentinel-2 SR. Our code and models can be found at this https URL.</li>
</ul>

<h3>Title: MambaVSR: Content-Aware Scanning State Space Model for Video Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Linfeng He, Meiqin Liu, Qi Tang, Chao Yao, Yao Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11768">https://arxiv.org/abs/2506.11768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11768">https://arxiv.org/pdf/2506.11768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11768]] MambaVSR: Content-Aware Scanning State Space Model for Video Super-Resolution(https://arxiv.org/abs/2506.11768)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Video super-resolution (VSR) faces critical challenges in effectively modeling non-local dependencies across misaligned frames while preserving computational efficiency. Existing VSR methods typically rely on optical flow strategies or transformer architectures, which struggle with large motion displacements and long video sequences. To address this, we propose MambaVSR, the first state-space model framework for VSR that incorporates an innovative content-aware scanning mechanism. Unlike rigid 1D sequential processing in conventional vision Mamba methods, our MambaVSR enables dynamic spatiotemporal interactions through the Shared Compass Construction (SCC) and the Content-Aware Sequentialization (CAS). Specifically, the SCC module constructs intra-frame semantic connectivity graphs via efficient sparse attention and generates adaptive spatial scanning sequences through spectral clustering. Building upon SCC, the CAS module effectively aligns and aggregates non-local similar content across multiple frames by interleaving temporal features along the learned spatial order. To bridge global dependencies with local details, the Global-Local State Space Block (GLSSB) synergistically integrates window self-attention operations with SSM-based feature propagation, enabling high-frequency detail recovery under global dependency guidance. Extensive experiments validate MambaVSR's superiority, outperforming the Transformer-based method by 0.58 dB PSNR on the REDS dataset with 55% fewer parameters.</li>
</ul>

<h3>Title: Long-Short Alignment for Effective Long-Context Modeling in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Tianqi Du, Haotian Huang, Yifei Wang, Yisen Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11769">https://arxiv.org/abs/2506.11769</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11769">https://arxiv.org/pdf/2506.11769</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11769]] Long-Short Alignment for Effective Long-Context Modeling in LLMs(https://arxiv.org/abs/2506.11769)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have exhibited impressive performance and surprising emergent properties. However, their effectiveness remains limited by the fixed context window of the transformer architecture, posing challenges for long-context modeling. Among these challenges, length generalization -- the ability to generalize to sequences longer than those seen during training -- is a classical and fundamental problem. In this work, we propose a fresh perspective on length generalization, shifting the focus from the conventional emphasis on input features such as positional encodings or data structures to the output distribution of the model. Specifically, through case studies on synthetic tasks, we highlight the critical role of \textbf{long-short alignment} -- the consistency of output distributions across sequences of varying lengths. Extending this insight to natural language tasks, we propose a metric called Long-Short Misalignment to quantify this phenomenon, uncovering a strong correlation between the metric and length generalization performance. Building on these findings, we develop a regularization term that promotes long-short alignment during training. Extensive experiments validate the effectiveness of our approach, offering new insights for achieving more effective long-context modeling in LLMs. Code is available at this https URL.</li>
</ul>

<h3>Title: CLIP Meets Diffusion: A Synergistic Approach to Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Byeongchan Lee, John Won, Seunghyun Lee, Jinwoo Shin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11772">https://arxiv.org/abs/2506.11772</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11772">https://arxiv.org/pdf/2506.11772</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11772]] CLIP Meets Diffusion: A Synergistic Approach to Anomaly Detection(https://arxiv.org/abs/2506.11772)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, segmentation</a></li>
<li><strong>Abstract: </strong>Anomaly detection is a complex problem due to the ambiguity in defining anomalies, the diversity of anomaly types (e.g., local and global defect), and the scarcity of training data. As such, it necessitates a comprehensive model capable of capturing both low-level and high-level features, even with limited data. To address this, we propose CLIPFUSION, a method that leverages both discriminative and generative foundation models. Specifically, the CLIP-based discriminative model excels at capturing global features, while the diffusion-based generative model effectively captures local details, creating a synergistic and complementary approach. Notably, we introduce a methodology for utilizing cross-attention maps and feature maps extracted from diffusion models specifically for anomaly detection. Experimental results on benchmark datasets (MVTec-AD, VisA) demonstrate that CLIPFUSION consistently outperforms baseline methods, achieving outstanding performance in both anomaly segmentation and classification. We believe that our method underscores the effectiveness of multi-modal and multi-model fusion in tackling the multifaceted challenges of anomaly detection, providing a scalable solution for real-world applications.</li>
</ul>

<h3>Title: AgentSense: Virtual Sensor Data Generation Using LLM Agent in Simulated Home Environments</h3>
<ul>
<li><strong>Authors: </strong>Zikang Leng, Megha Thukral, Yaqi Liu, Hrudhai Rajasekhar, Shruthi K. Hiremath, Thomas Plötz</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11773">https://arxiv.org/abs/2506.11773</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11773">https://arxiv.org/pdf/2506.11773</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11773]] AgentSense: Virtual Sensor Data Generation Using LLM Agent in Simulated Home Environments(https://arxiv.org/abs/2506.11773)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>A major obstacle in developing robust and generalizable smart home-based Human Activity Recognition (HAR) systems is the lack of large-scale, diverse labeled datasets. Variability in home layouts, sensor configurations, and user behavior adds further complexity, as individuals follow varied routines and perform activities in distinct ways. Building HAR systems that generalize well requires training data that captures the diversity across users and environments. To address these challenges, we introduce AgentSense, a virtual data generation pipeline where diverse personas are generated by leveraging Large Language Models. These personas are used to create daily routines, which are then decomposed into low-level action sequences. Subsequently, the actions are executed in a simulated home environment called VirtualHome that we extended with virtual ambient sensors capable of recording the agents activities as they unfold. Overall, AgentSense enables the generation of rich, virtual sensor datasets that represent a wide range of users and home settings. Across five benchmark HAR datasets, we show that leveraging our virtual sensor data substantially improves performance, particularly when real data are limited. Notably, models trained on a combination of virtual data and just a few days of real data achieve performance comparable to those trained on the entire real datasets. These results demonstrate and prove the potential of virtual data to address one of the most pressing challenges in ambient sensing, which is the distinct lack of large-scale, annotated datasets without requiring any manual data collection efforts.</li>
</ul>

<h3>Title: Real-Time Feedback and Benchmark Dataset for Isometric Pose Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Abhishek Jaiswal, Armeet Singh Luthra, Purav Jangir, Bhavya Garg, Nisheeth Srivastava</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11774">https://arxiv.org/abs/2506.11774</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11774">https://arxiv.org/pdf/2506.11774</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11774]] Real-Time Feedback and Benchmark Dataset for Isometric Pose Evaluation(https://arxiv.org/abs/2506.11774)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust</a></li>
<li><strong>Abstract: </strong>Isometric exercises appeal to individuals seeking convenience, privacy, and minimal dependence on equipments. However, such fitness training is often overdependent on unreliable digital media content instead of expert supervision, introducing serious risks, including incorrect posture, injury, and disengagement due to lack of corrective feedback. To address these challenges, we present a real-time feedback system for assessing isometric poses. Our contributions include the release of the largest multiclass isometric exercise video dataset to date, comprising over 3,600 clips across six poses with correct and incorrect variations. To support robust evaluation, we benchmark state-of-the-art models-including graph-based networks-on this dataset and introduce a novel three-part metric that captures classification accuracy, mistake localization, and model confidence. Our results enhance the feasibility of intelligent and personalized exercise training systems for home workouts. This expert-level diagnosis, delivered directly to the users, also expands the potential applications of these systems to rehabilitation, physiotherapy, and various other fitness disciplines that involve physical motion.</li>
</ul>

<h3>Title: Self-supervised Learning of Echocardiographic Video Representations via Online Cluster Distillation</h3>
<ul>
<li><strong>Authors: </strong>Divyanshu Mishra, Mohammadreza Salehi, Pramit Saha, Olga Patey, Aris T. Papageorghiou, Yuki M. Asano, J. Alison Noble</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11777">https://arxiv.org/abs/2506.11777</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11777">https://arxiv.org/pdf/2506.11777</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11777]] Self-supervised Learning of Echocardiographic Video Representations via Online Cluster Distillation(https://arxiv.org/abs/2506.11777)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Self-supervised learning (SSL) has achieved major advances in natural images and video understanding, but challenges remain in domains like echocardiography (heart ultrasound) due to subtle anatomical structures, complex temporal dynamics, and the current lack of domain-specific pre-trained models. Existing SSL approaches such as contrastive, masked modeling, and clustering-based methods struggle with high intersample similarity, sensitivity to low PSNR inputs common in ultrasound, or aggressive augmentations that distort clinically relevant features. We present DISCOVR (Distilled Image Supervision for Cross Modal Video Representation), a self-supervised dual branch framework for cardiac ultrasound video representation learning. DISCOVR combines a clustering-based video encoder that models temporal dynamics with an online image encoder that extracts fine-grained spatial semantics. These branches are connected through a semantic cluster distillation loss that transfers anatomical knowledge from the evolving image encoder to the video encoder, enabling temporally coherent representations enriched with fine-grained semantic understanding. Evaluated on six echocardiography datasets spanning fetal, pediatric, and adult populations, DISCOVR outperforms both specialized video anomaly detection methods and state-of-the-art video-SSL baselines in zero-shot and linear probing setups, and achieves superior segmentation transfer.</li>
</ul>

<h3>Title: GPLQ: A General, Practical, and Lightning QAT Method for Vision Transformers</h3>
<ul>
<li><strong>Authors: </strong>Guang Liang, Xinyao Liu, Jianxin Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11784">https://arxiv.org/abs/2506.11784</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11784">https://arxiv.org/pdf/2506.11784</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11784]] GPLQ: A General, Practical, and Lightning QAT Method for Vision Transformers(https://arxiv.org/abs/2506.11784)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Vision Transformers (ViTs) are essential in computer vision but are computationally intensive, too. Model quantization, particularly to low bit-widths like 4-bit, aims to alleviate this difficulty, yet existing Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) methods exhibit significant limitations. PTQ often incurs substantial accuracy drop, while QAT achieves high accuracy but suffers from prohibitive computational costs, limited generalization to downstream tasks, training instability, and lacking of open-source codebase. To address these challenges, this paper introduces General, Practical, and Lightning Quantization (GPLQ), a novel framework designed for efficient and effective ViT quantization. GPLQ is founded on two key empirical insights: the paramount importance of activation quantization and the necessity of preserving the model's original optimization ``basin'' to maintain generalization. Consequently, GPLQ employs a sequential ``activation-first, weights-later'' strategy. Stage 1 keeps weights in FP32 while quantizing activations with a feature mimicking loss in only 1 epoch to keep it stay in the same ``basin'', thereby preserving generalization. Stage 2 quantizes weights using a PTQ method. As a result, GPLQ is 100x faster than existing QAT methods, lowers memory footprint to levels even below FP32 training, and achieves 4-bit model performance that is highly competitive with FP32 models in terms of both accuracy on ImageNet and generalization to diverse downstream tasks, including fine-grained visual classification and object detection. We will release an easy-to-use open-source toolkit supporting multiple vision tasks.</li>
</ul>

<h3>Title: SSPINNpose: A Self-Supervised PINN for Inertial Pose and Dynamics Estimation</h3>
<ul>
<li><strong>Authors: </strong>Markus Gambietz, Eva Dorschky, Altan Akat, Marcel Schöckel, Jörg Miehling, Anne D. Koelewijn</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11786">https://arxiv.org/abs/2506.11786</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11786">https://arxiv.org/pdf/2506.11786</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11786]] SSPINNpose: A Self-Supervised PINN for Inertial Pose and Dynamics Estimation(https://arxiv.org/abs/2506.11786)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate real-time estimation of human movement dynamics, including internal joint moments and muscle forces, is essential for applications in clinical diagnostics and sports performance monitoring. Inertial measurement units (IMUs) provide a minimally intrusive solution for capturing motion data, particularly when used in sparse sensor configurations. However, current real-time methods rely on supervised learning, where a ground truth dataset needs to be measured with laboratory measurement systems, such as optical motion capture. These systems are known to introduce measurement and processing errors and often fail to generalize to real-world or previously unseen movements, necessitating new data collection efforts that are time-consuming and impractical. To overcome these limitations, we propose SSPINNpose, a self-supervised, physics-informed neural network that estimates joint kinematics and kinetics directly from IMU data, without requiring ground truth labels for training. We run the network output through a physics model of the human body to optimize physical plausibility and generate virtual measurement data. Using this virtual sensor data, the network is trained directly on the measured sensor data instead of a ground truth. When compared to optical motion capture, SSPINNpose is able to accurately estimate joint angles and joint moments at an RMSD of 8.7 deg and 4.9 BWBH%, respectively, for walking and running at speeds up to 4.9 m/s at a latency of 3.5 ms. Furthermore, the framework demonstrates robustness across sparse sensor configurations and can infer the anatomical locations of the sensors. These results underscore the potential of SSPINNpose as a scalable and adaptable solution for real-time biomechanical analysis in both laboratory and field environments.</li>
</ul>

<h3>Title: SEC-bench: Automated Benchmarking of LLM Agents on Real-World Software Security Tasks</h3>
<ul>
<li><strong>Authors: </strong>Hwiwon Lee, Ziqi Zhang, Hanxiao Lu, Lingming Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11791">https://arxiv.org/abs/2506.11791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11791">https://arxiv.org/pdf/2506.11791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11791]] SEC-bench: Automated Benchmarking of LLM Agents on Real-World Software Security Tasks(https://arxiv.org/abs/2506.11791)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Rigorous security-focused evaluation of large language model (LLM) agents is imperative for establishing trust in their safe deployment throughout the software development lifecycle. However, existing benchmarks largely rely on synthetic challenges or simplified vulnerability datasets that fail to capture the complexity and ambiguity encountered by security engineers in practice. We introduce SEC-bench, the first fully automated benchmarking framework for evaluating LLM agents on authentic security engineering tasks. SEC-bench employs a novel multi-agent scaffold that automatically constructs code repositories with harnesses, reproduces vulnerabilities in isolated environments, and generates gold patches for reliable evaluation. Our framework automatically creates high-quality software vulnerability datasets with reproducible artifacts at a cost of only $0.87 per instance. Using SEC-bench, we implement two critical software security tasks to rigorously evaluate LLM agents' capabilities: proof-of-concept (PoC) generation and vulnerability patching. A comprehensive evaluation of state-of-the-art LLM code agents reveals significant performance gaps, achieving at most 18.0% success in PoC generation and 34.0% in vulnerability patching on our complete dataset. These results highlight the crucial steps needed toward developing LLM agents that are more practical, intelligent, and autonomous for security engineering.</li>
</ul>

<h3>Title: Persona-driven Simulation of Voting Behavior in the European Parliament with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Maximilian Kreutner, Marlene Lutz, Markus Strohmaier</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11798">https://arxiv.org/abs/2506.11798</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11798">https://arxiv.org/pdf/2506.11798</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11798]] Persona-driven Simulation of Voting Behavior in the European Parliament with Large Language Models(https://arxiv.org/abs/2506.11798)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) display remarkable capabilities to understand or even produce political discourse, but have been found to consistently display a progressive left-leaning bias. At the same time, so-called persona or identity prompts have been shown to produce LLM behavior that aligns with socioeconomic groups that the base model is not aligned with. In this work, we analyze whether zero-shot persona prompting with limited information can accurately predict individual voting decisions and, by aggregation, accurately predict positions of European groups on a diverse set of policies. We evaluate if predictions are stable towards counterfactual arguments, different persona prompts and generation methods. Finally, we find that we can simulate voting behavior of Members of the European Parliament reasonably well with a weighted F1 score of approximately 0.793. Our persona dataset of politicians in the 2024 European Parliament and our code are available at this https URL.</li>
</ul>

<h3>Title: Are Multimodal Large Language Models Pragmatically Competent Listeners in Simple Reference Resolution Tasks?</h3>
<ul>
<li><strong>Authors: </strong>Simeon Junker, Manar Ali, Larissa Koch, Sina Zarrieß, Hendrik Buschmeier</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11807">https://arxiv.org/abs/2506.11807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11807">https://arxiv.org/pdf/2506.11807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11807]] Are Multimodal Large Language Models Pragmatically Competent Listeners in Simple Reference Resolution Tasks?(https://arxiv.org/abs/2506.11807)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We investigate the linguistic abilities of multimodal large language models in reference resolution tasks featuring simple yet abstract visual stimuli, such as color patches and color grids. Although the task may not seem challenging for today's language models, being straightforward for human dyads, we consider it to be a highly relevant probe of the pragmatic capabilities of MLLMs. Our results and analyses indeed suggest that basic pragmatic capabilities, such as context-dependent interpretation of color descriptions, still constitute major challenges for state-of-the-art MLLMs.</li>
</ul>

<h3>Title: Rethinking Multilingual Vision-Language Translation: Dataset, Evaluation, and Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Xintong Wang, Jingheng Pan, Yixiao Liu, Xiaohu Zhao, Chenyang Lyu, Minghao Wu, Chris Biemann, Longyue Wang, Linlong Xu, Weihua Luo, Kaifu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11820">https://arxiv.org/abs/2506.11820</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11820">https://arxiv.org/pdf/2506.11820</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11820]] Rethinking Multilingual Vision-Language Translation: Dataset, Evaluation, and Adaptation(https://arxiv.org/abs/2506.11820)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Vision-Language Translation (VLT) is a challenging task that requires accurately recognizing multilingual text embedded in images and translating it into the target language with the support of visual context. While recent Large Vision-Language Models (LVLMs) have demonstrated strong multilingual and visual understanding capabilities, there is a lack of systematic evaluation and understanding of their performance on VLT. In this work, we present a comprehensive study of VLT from three key perspectives: data quality, model architecture, and evaluation metrics. (1) We identify critical limitations in existing datasets, particularly in semantic and cultural fidelity, and introduce AibTrans -- a multilingual, parallel, human-verified dataset with OCR-corrected annotations. (2) We benchmark 11 commercial LVLMs/LLMs and 6 state-of-the-art open-source models across end-to-end and cascaded architectures, revealing their OCR dependency and contrasting generation versus reasoning behaviors. (3) We propose Density-Aware Evaluation to address metric reliability issues under varying contextual complexity, introducing the DA Score as a more robust measure of translation quality. Building upon these findings, we establish a new evaluation benchmark for VLT. Notably, we observe that fine-tuning LVLMs on high-resource language pairs degrades cross-lingual performance, and we propose a balanced multilingual fine-tuning strategy that effectively adapts LVLMs to VLT without sacrificing their generalization ability.</li>
</ul>

<h3>Title: TrustGLM: Evaluating the Robustness of GraphLLMs Against Prompt, Text, and Structure Attacks</h3>
<ul>
<li><strong>Authors: </strong>Qihai Zhang, Xinyue Sheng, Yuanfu Sun, Qiaoyu Tan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11844">https://arxiv.org/abs/2506.11844</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11844">https://arxiv.org/pdf/2506.11844</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11844]] TrustGLM: Evaluating the Robustness of GraphLLMs Against Prompt, Text, and Structure Attacks(https://arxiv.org/abs/2506.11844)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Inspired by the success of large language models (LLMs), there is a significant research shift from traditional graph learning methods to LLM-based graph frameworks, formally known as GraphLLMs. GraphLLMs leverage the reasoning power of LLMs by integrating three key components: the textual attributes of input nodes, the structural information of node neighborhoods, and task-specific prompts that guide decision-making. Despite their promise, the robustness of GraphLLMs against adversarial perturbations remains largely unexplored-a critical concern for deploying these models in high-stakes scenarios. To bridge the gap, we introduce TrustGLM, a comprehensive study evaluating the vulnerability of GraphLLMs to adversarial attacks across three dimensions: text, graph structure, and prompt manipulations. We implement state-of-the-art attack algorithms from each perspective to rigorously assess model resilience. Through extensive experiments on six benchmark datasets from diverse domains, our findings reveal that GraphLLMs are highly susceptible to text attacks that merely replace a few semantically similar words in a node's textual attribute. We also find that standard graph structure attack methods can significantly degrade model performance, while random shuffling of the candidate label set in prompt templates leads to substantial performance drops. Beyond characterizing these vulnerabilities, we investigate defense techniques tailored to each attack vector through data-augmented training and adversarial training, which show promising potential to enhance the robustness of GraphLLMs. We hope that our open-sourced library will facilitate rapid, equitable evaluation and inspire further innovative research in this field.</li>
</ul>

<h3>Title: In Defense of Defensive Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Juan Carlos Perdomo, Benjamin Recht</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11848">https://arxiv.org/abs/2506.11848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11848">https://arxiv.org/pdf/2506.11848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11848]] In Defense of Defensive Forecasting(https://arxiv.org/abs/2506.11848)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense</a></li>
<li><strong>Abstract: </strong>This tutorial provides a survey of algorithms for Defensive Forecasting, where predictions are derived not by prognostication but by correcting past mistakes. Pioneered by Vovk, Defensive Forecasting frames the goal of prediction as a sequential game, and derives predictions to minimize metrics no matter what outcomes occur. We present an elementary introduction to this general theory and derive simple, near-optimal algorithms for online learning, calibration, prediction with expert advice, and online conformal prediction.</li>
</ul>

<h3>Title: Post Persona Alignment for Multi-Session Dialogue Generation</h3>
<ul>
<li><strong>Authors: </strong>Yi-Pei Chen, Noriki Nishida, Hideki Nakayama, Yuji Matsumoto</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11857">https://arxiv.org/abs/2506.11857</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11857">https://arxiv.org/pdf/2506.11857</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11857]] Post Persona Alignment for Multi-Session Dialogue Generation(https://arxiv.org/abs/2506.11857)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multi-session persona-based dialogue generation presents challenges in maintaining long-term consistency and generating diverse, personalized responses. While large language models (LLMs) excel in single-session dialogues, they struggle to preserve persona fidelity and conversational coherence across extended interactions. Existing methods typically retrieve persona information before response generation, which can constrain diversity and result in generic outputs. We propose Post Persona Alignment (PPA), a novel two-stage framework that reverses this process. PPA first generates a general response based solely on dialogue context, then retrieves relevant persona memories using the response as a query, and finally refines the response to align with the speaker's persona. This post-hoc alignment strategy promotes naturalness and diversity while preserving consistency and personalization. Experiments on multi-session LLM-generated dialogue data demonstrate that PPA significantly outperforms prior approaches in consistency, diversity, and persona relevance, offering a more flexible and effective paradigm for long-term personalized dialogue generation.</li>
</ul>

<h3>Title: Robust Molecular Property Prediction via Densifying Scarce Labeled Data</h3>
<ul>
<li><strong>Authors: </strong>Jina Kim, Jeffrey Willette, Bruno Andreis, Sung Ju Hwang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11877">https://arxiv.org/abs/2506.11877</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11877">https://arxiv.org/pdf/2506.11877</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11877]] Robust Molecular Property Prediction via Densifying Scarce Labeled Data(https://arxiv.org/abs/2506.11877)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>A widely recognized limitation of molecular prediction models is their reliance on structures observed in the training data, resulting in poor generalization to out-of-distribution compounds. Yet in drug discovery, the compounds most critical for advancing research often lie beyond the training set, making the bias toward the training data particularly problematic. This mismatch introduces substantial covariate shift, under which standard deep learning models produce unstable and inaccurate predictions. Furthermore, the scarcity of labeled data, stemming from the onerous and costly nature of experimental validation, further exacerbates the difficulty of achieving reliable generalization. To address these limitations, we propose a novel meta-learning-based approach that leverages unlabeled data to interpolate between in-distribution (ID) and out-of-distribution (OOD) data, enabling the model to meta-learn how to generalize beyond the training distribution. We demonstrate significant performance gains over state-of-the-art methods on challenging real-world datasets that exhibit substantial covariate shift.</li>
</ul>

<h3>Title: An Explainable AI Framework for Dynamic Resource Management in Vehicular Network Slicing</h3>
<ul>
<li><strong>Authors: </strong>Haochen Sun, Yifan Liu, Ahmed Al-Tahmeesschi, Swarna Chetty, Syed Ali Raza Zaidi, Avishek Nag, Hamed Ahmadi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11882">https://arxiv.org/abs/2506.11882</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11882">https://arxiv.org/pdf/2506.11882</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11882]] An Explainable AI Framework for Dynamic Resource Management in Vehicular Network Slicing(https://arxiv.org/abs/2506.11882)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Effective resource management and network slicing are essential to meet the diverse service demands of vehicular networks, including Enhanced Mobile Broadband (eMBB) and Ultra-Reliable and Low-Latency Communications (URLLC). This paper introduces an Explainable Deep Reinforcement Learning (XRL) framework for dynamic network slicing and resource allocation in vehicular networks, built upon a near-real-time RAN intelligent controller. By integrating a feature-based approach that leverages Shapley values and an attention mechanism, we interpret and refine the decisions of our reinforcementlearning agents, addressing key reliability challenges in vehicular communication systems. Simulation results demonstrate that our approach provides clear, real-time insights into the resource allocation process and achieves higher interpretability precision than a pure attention mechanism. Furthermore, the Quality of Service (QoS) satisfaction for URLLC services increased from 78.0% to 80.13%, while that for eMBB services improved from 71.44% to 73.21%.</li>
</ul>

<h3>Title: Beyond Homogeneous Attention: Memory-Efficient LLMs via Fourier-Approximated KV Cache</h3>
<ul>
<li><strong>Authors: </strong>Xiaoran Liu, Siyang He, Qiqi Wang, Ruixiao Li, Yuerong Song, Zhigeng Liu, Linlin Li, Qun Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, Xipeng Qiu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11886">https://arxiv.org/abs/2506.11886</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11886">https://arxiv.org/pdf/2506.11886</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11886]] Beyond Homogeneous Attention: Memory-Efficient LLMs via Fourier-Approximated KV Cache(https://arxiv.org/abs/2506.11886)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models struggle with memory demands from the growing Key-Value (KV) cache as context lengths increase. Existing compression methods homogenize head dimensions or rely on attention-guided token pruning, often sacrificing accuracy or introducing computational overhead. We propose FourierAttention, a training-free framework that exploits the heterogeneous roles of transformer head dimensions: lower dimensions prioritize local context, while upper ones capture long-range dependencies. By projecting the long-context-insensitive dimensions onto orthogonal Fourier bases, FourierAttention approximates their temporal evolution with fixed-length spectral coefficients. Evaluations on LLaMA models show that FourierAttention achieves the best long-context accuracy on LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel, FlashFourierAttention, is designed to optimize memory via streamlined read-write operations, enabling efficient deployment without performance compromise.</li>
</ul>

<h3>Title: Understanding Input Selectivity in Mamba: Impact on Approximation Power, Memorization, and Associative Recall Capacity</h3>
<ul>
<li><strong>Authors: </strong>Ningyuan Huang, Miguel Sarabia, Abhinav Moudgil, Pau Rodriguez, Luca Zappella, Federico Danieli</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11891">https://arxiv.org/abs/2506.11891</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11891">https://arxiv.org/pdf/2506.11891</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11891]] Understanding Input Selectivity in Mamba: Impact on Approximation Power, Memorization, and Associative Recall Capacity(https://arxiv.org/abs/2506.11891)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>State-Space Models (SSMs), and particularly Mamba, have recently emerged as a promising alternative to Transformers. Mamba introduces input selectivity to its SSM layer (S6) and incorporates convolution and gating into its block definition. While these modifications do improve Mamba's performance over its SSM predecessors, it remains largely unclear how Mamba leverages the additional functionalities provided by input selectivity, and how these interact with the other operations in the Mamba architecture. In this work, we demystify the role of input selectivity in Mamba, investigating its impact on function approximation power, long-term memorization, and associative recall capabilities. In particular: (i) we prove that the S6 layer of Mamba can represent projections onto Haar wavelets, providing an edge over its Diagonal SSM (S4D) predecessor in approximating discontinuous functions commonly arising in practice; (ii) we show how the S6 layer can dynamically counteract memory decay; (iii) we provide analytical solutions to the MQAR associative recall task using the Mamba architecture with different mixers -- Mamba, Mamba-2, and S4D. We demonstrate the tightness of our theoretical constructions with empirical results on concrete tasks. Our findings offer a mechanistic understanding of Mamba and reveal opportunities for improvement.</li>
</ul>

<h3>Title: Attention-based Adversarial Robust Distillation in Radio Signal Classifications for Low-Power IoT Devices</h3>
<ul>
<li><strong>Authors: </strong>Lu Zhang, Sangarapillai Lambotharan, Gan Zheng, Guisheng Liao, Basil AsSadhan, Fabio Roli</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11892">https://arxiv.org/abs/2506.11892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11892">https://arxiv.org/pdf/2506.11892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11892]] Attention-based Adversarial Robust Distillation in Radio Signal Classifications for Low-Power IoT Devices(https://arxiv.org/abs/2506.11892)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, defense, attack, robust, transformer</a></li>
<li><strong>Abstract: </strong>Due to great success of transformers in many applications such as natural language processing and computer vision, transformers have been successfully applied in automatic modulation classification. We have shown that transformer-based radio signal classification is vulnerable to imperceptible and carefully crafted attacks called adversarial examples. Therefore, we propose a defense system against adversarial examples in transformer-based modulation classifications. Considering the need for computationally efficient architecture particularly for Internet of Things (IoT)-based applications or operation of devices in environment where power supply is limited, we propose a compact transformer for modulation classification. The advantages of robust training such as adversarial training in transformers may not be attainable in compact transformers. By demonstrating this, we propose a novel compact transformer that can enhance robustness in the presence of adversarial attacks. The new method is aimed at transferring the adversarial attention map from the robustly trained large transformer to a compact transformer. The proposed method outperforms the state-of-the-art techniques for the considered white-box scenarios including fast gradient method and projected gradient descent attacks. We have provided reasoning of the underlying working mechanisms and investigated the transferability of the adversarial examples between different architectures. The proposed method has the potential to protect the transformer from the transferability of adversarial examples.</li>
</ul>

<h3>Title: Measurement-aligned Flow for Inverse Problem</h3>
<ul>
<li><strong>Authors: </strong>Shaorong Zhang, Rob Brekelmans, Yunshu Wu, Greg Ver Steeg</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11893">https://arxiv.org/abs/2506.11893</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11893">https://arxiv.org/pdf/2506.11893</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11893]] Measurement-aligned Flow for Inverse Problem(https://arxiv.org/abs/2506.11893)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models provide a powerful way to incorporate complex prior information for solving inverse problems. However, existing methods struggle to correctly incorporate guidance from conflicting signals in the prior and measurement, especially in the challenging setting of non-Gaussian or unknown noise. To bridge these gaps, we propose Measurement-Aligned Sampling (MAS), a novel framework for linear inverse problem solving that can more flexibly balance prior and measurement information. MAS unifies and extends existing approaches like DDNM and DAPS, and offers a new optimization perspective. MAS can generalize to handle known Gaussian noise, unknown or non-Gaussian noise types. Extensive experiments show that MAS consistently outperforms state-of-the-art methods across a range of tasks.</li>
</ul>

<h3>Title: A Neural Rejection System Against Universal Adversarial Perturbations in Radio Signal Classification</h3>
<ul>
<li><strong>Authors: </strong>Lu Zhang, Sangarapillai Lambotharan, Gan Zheng, Fabio Roli</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11901">https://arxiv.org/abs/2506.11901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11901">https://arxiv.org/pdf/2506.11901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11901]] A Neural Rejection System Against Universal Adversarial Perturbations in Radio Signal Classification(https://arxiv.org/abs/2506.11901)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense</a></li>
<li><strong>Abstract: </strong>Advantages of deep learning over traditional methods have been demonstrated for radio signal classification in the recent years. However, various researchers have discovered that even a small but intentional feature perturbation known as adversarial examples can significantly deteriorate the performance of the deep learning based radio signal classification. Among various kinds of adversarial examples, universal adversarial perturbation has gained considerable attention due to its feature of being data independent, hence as a practical strategy to fool the radio signal classification with a high success rate. Therefore, in this paper, we investigate a defense system called neural rejection system to propose against universal adversarial perturbations, and evaluate its performance by generating white-box universal adversarial perturbations. We show that the proposed neural rejection system is able to defend universal adversarial perturbations with significantly higher accuracy than the undefended deep neural network.</li>
</ul>

<h3>Title: GeistBERT: Breathing Life into German NLP</h3>
<ul>
<li><strong>Authors: </strong>Raphael Scheible-Schmitt, Johann Frei</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11903">https://arxiv.org/abs/2506.11903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11903">https://arxiv.org/pdf/2506.11903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11903]] GeistBERT: Breathing Life into German NLP(https://arxiv.org/abs/2506.11903)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, transformer</a></li>
<li><strong>Abstract: </strong>Advances in transformer-based language models have highlighted the benefits of language-specific pre-training on high-quality corpora. In this context, German NLP stands to gain from updated architectures and modern datasets tailored to the linguistic characteristics of the German language. GeistBERT seeks to improve German language processing by incrementally training on a diverse corpus and optimizing model performance across various NLP tasks. It was pre-trained using fairseq with standard hyperparameters, initialized from GottBERT weights, and trained on a large-scale German corpus using Whole Word Masking (WWM). Based on the pre-trained model, we derived extended-input variants using Nyströmformer and Longformer architectures with support for sequences up to 8k tokens. While these long-context models were not evaluated on dedicated long-context benchmarks, they are included in our release. We assessed all models on NER (CoNLL 2003, GermEval 2014) and text classification (GermEval 2018 fine/coarse, 10kGNAD) using $F_1$ score and accuracy. The GeistBERT models achieved strong performance, leading all tasks among the base models and setting a new state-of-the-art (SOTA). Notably, the base models outperformed larger models in several tasks. To support the German NLP research community, we are releasing GeistBERT under the MIT license.</li>
</ul>

<h3>Title: O2Former:Direction-Aware and Multi-Scale Query Enhancement for SAR Ship Instance Segmentation</h3>
<ul>
<li><strong>Authors: </strong>F. Gao, Y Li, X He, J Sun, J Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11913">https://arxiv.org/abs/2506.11913</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11913">https://arxiv.org/pdf/2506.11913</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11913]] O2Former:Direction-Aware and Multi-Scale Query Enhancement for SAR Ship Instance Segmentation(https://arxiv.org/abs/2506.11913)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, segmentation</a></li>
<li><strong>Abstract: </strong>Instance segmentation of ships in synthetic aperture radar (SAR) imagery is critical for applications such as maritime monitoring, environmental analysis, and national security. SAR ship images present challenges including scale variation, object density, and fuzzy target boundary, which are often overlooked in existing methods, leading to suboptimal performance. In this work, we propose O2Former, a tailored instance segmentation framework that extends Mask2Former by fully leveraging the structural characteristics of SAR imagery. We introduce two key components. The first is the Optimized Query Generator(OQG). It enables multi-scale feature interaction by jointly encoding shallow positional cues and high-level semantic information. This improves query quality and convergence efficiency. The second component is the Orientation-Aware Embedding Module(OAEM). It enhances directional sensitivity through direction-aware convolution and polar-coordinate encoding. This effectively addresses the challenge of uneven target orientations in SAR scenes. Together, these modules facilitate precise feature alignment from backbone to decoder and strengthen the model's capacity to capture fine-grained structural details. Extensive experiments demonstrate that O2Former outperforms state of the art instance segmentation baselines, validating its effectiveness and generalization on SAR ship datasets.</li>
</ul>

<h3>Title: Effectiveness of Counter-Speech against Abusive Content: A Multidimensional Annotation and Classification Study</h3>
<ul>
<li><strong>Authors: </strong>Greta Damo, Elena Cabrio, Serena Villata</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11919">https://arxiv.org/abs/2506.11919</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11919">https://arxiv.org/pdf/2506.11919</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11919]] Effectiveness of Counter-Speech against Abusive Content: A Multidimensional Annotation and Classification Study(https://arxiv.org/abs/2506.11919)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Counter-speech (CS) is a key strategy for mitigating online Hate Speech (HS), yet defining the criteria to assess its effectiveness remains an open challenge. We propose a novel computational framework for CS effectiveness classification, grounded in social science concepts. Our framework defines six core dimensions - Clarity, Evidence, Emotional Appeal, Rebuttal, Audience Adaptation, and Fairness - which we use to annotate 4,214 CS instances from two benchmark datasets, resulting in a novel linguistic resource released to the community. In addition, we propose two classification strategies, multi-task and dependency-based, achieving strong results (0.94 and 0.96 average F1 respectively on both expert- and user-written CS), outperforming standard baselines, and revealing strong interdependence among dimensions.</li>
</ul>

<h3>Title: Aligned Novel View Image and Geometry Synthesis via Cross-modal Attention Instillation</h3>
<ul>
<li><strong>Authors: </strong>Min-Seop Kwak, Junho Kim, Sangdoo Yun, Dongyoon Han, Taekyoung Kim, Seungryong Kim, Jin-Hwa Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11924">https://arxiv.org/abs/2506.11924</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11924">https://arxiv.org/pdf/2506.11924</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11924]] Aligned Novel View Image and Geometry Synthesis via Cross-modal Attention Instillation(https://arxiv.org/abs/2506.11924)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>We introduce a diffusion-based framework that performs aligned novel view image and geometry generation via a warping-and-inpainting methodology. Unlike prior methods that require dense posed images or pose-embedded generative models limited to in-domain views, our method leverages off-the-shelf geometry predictors to predict partial geometries viewed from reference images, and formulates novel-view synthesis as an inpainting task for both image and geometry. To ensure accurate alignment between generated images and geometry, we propose cross-modal attention distillation, where attention maps from the image diffusion branch are injected into a parallel geometry diffusion branch during both training and inference. This multi-task approach achieves synergistic effects, facilitating geometrically robust image synthesis as well as well-defined geometry prediction. We further introduce proximity-based mesh conditioning to integrate depth and normal cues, interpolating between point cloud and filtering erroneously predicted geometry from influencing the generation process. Empirically, our method achieves high-fidelity extrapolative view synthesis on both image and geometry across a range of unseen scenes, delivers competitive reconstruction quality under interpolation settings, and produces geometrically aligned colored point clouds for comprehensive 3D completion. Project page is available at this https URL.</li>
</ul>

<h3>Title: Improving Large Language Model Safety with Contrastive Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Samuel Simko, Mrinmaya Sachan, Bernhard Schölkopf, Zhijing Jin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11938">https://arxiv.org/abs/2506.11938</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11938">https://arxiv.org/pdf/2506.11938</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11938]] Improving Large Language Model Safety with Contrastive Representation Learning(https://arxiv.org/abs/2506.11938)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are powerful tools with profound societal impacts, yet their ability to generate responses to diverse and uncontrolled inputs leaves them vulnerable to adversarial attacks. While existing defenses often struggle to generalize across varying attack types, recent advancements in representation engineering offer promising alternatives. In this work, we propose a defense framework that formulates model defense as a contrastive representation learning (CRL) problem. Our method finetunes a model using a triplet-based loss combined with adversarial hard negative mining to encourage separation between benign and harmful representations. Our experimental results across multiple models demonstrate that our approach outperforms prior representation engineering-based defenses, improving robustness against both input-level and embedding-space attacks without compromising standard performance. Our code is available at this https URL</li>
</ul>

<h3>Title: Technical Evaluation of a Disruptive Approach in Homomorphic AI</h3>
<ul>
<li><strong>Authors: </strong>Eric Filiol</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11954">https://arxiv.org/abs/2506.11954</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11954">https://arxiv.org/pdf/2506.11954</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11954]] Technical Evaluation of a Disruptive Approach in Homomorphic AI(https://arxiv.org/abs/2506.11954)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect</a></li>
<li><strong>Abstract: </strong>We present a technical evaluation of a new, disruptive cryptographic approach to data security, known as HbHAI (Hash-based Homomorphic Artificial Intelligence). HbHAI is based on a novel class of key-dependent hash functions that naturally preserve most similarity properties, most AI algorithms rely on. As a main claim, HbHAI makes now possible to analyze and process data in its cryptographically secure form while using existing native AI algorithms without modification, with unprecedented performances compared to existing homomorphic encryption schemes. We tested various HbHAI-protected datasets (non public preview) using traditional unsupervised and supervised learning techniques (clustering, classification, deep neural networks) with classical unmodified AI algorithms. This paper presents technical results from an independent analysis conducted with those different, off-the-shelf AI algorithms. The aim was to assess the security, operability and performance claims regarding HbHAI techniques. As a results, our results confirm most these claims, with only a few minor reservations.</li>
</ul>

<h3>Title: CnC-PRAC: Coalesce, not Cache, Per Row Activation Counts for an Efficient in-DRAM Rowhammer Mitigation</h3>
<ul>
<li><strong>Authors: </strong>Chris S. Lin, Jeonghyun Woo, Prashant J. Nair, Gururaj Saileshwar</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11970">https://arxiv.org/abs/2506.11970</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11970">https://arxiv.org/pdf/2506.11970</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11970]] CnC-PRAC: Coalesce, not Cache, Per Row Activation Counts for an Efficient in-DRAM Rowhammer Mitigation(https://arxiv.org/abs/2506.11970)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, attack</a></li>
<li><strong>Abstract: </strong>JEDEC has introduced the Per Row Activation Counting (PRAC) framework for DDR5 and future DRAMs to enable precise counting of DRAM row activations using per-row activation counts. While recent PRAC implementations enable holistic mitigation of Rowhammer attacks, they impose slowdowns of up to 10% due to the increased DRAM timings for performing a read-modify-write of the counter. Alternatively, recent work, Chronus, addresses these slowdowns, but incurs energy overheads due to the additional DRAM activations for counters. In this paper, we propose CnC-PRAC, a PRAC implementation that addresses both performance and energy overheads. Unlike prior works focusing on caching activation counts to reduce their overheads, our key idea is to reorder and coalesce accesses to activation counts located in the same physical row. Our design achieves this by decoupling counter access from the critical path of data accesses. This enables optimizations such as buffering counter read-modify-write requests and coalescing requests to the same row. Together, these enable a reduction in row activations for counter accesses by almost 75%-83% compared to state-of-the-art solutions like Chronus and enable a PRAC implementation with negligible slowdown and a minimal dynamic energy overhead of 0.84%-1% compared to insecure DDR5 DRAM.</li>
</ul>

<h3>Title: Self-Regulating Cars: Automating Traffic Control in Free Flow Road Networks</h3>
<ul>
<li><strong>Authors: </strong>Ankit Bhardwaj, Rohail Asim, Sachin Chauhan, Yasir Zaki, Lakshminarayanan Subramanian</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11973">https://arxiv.org/abs/2506.11973</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11973">https://arxiv.org/pdf/2506.11973</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11973]] Self-Regulating Cars: Automating Traffic Control in Free Flow Road Networks(https://arxiv.org/abs/2506.11973)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Free-flow road networks, such as suburban highways, are increasingly experiencing traffic congestion due to growing commuter inflow and limited infrastructure. Traditional control mechanisms, such as traffic signals or local heuristics, are ineffective or infeasible in these high-speed, signal-free environments. We introduce self-regulating cars, a reinforcement learning-based traffic control protocol that dynamically modulates vehicle speeds to optimize throughput and prevent congestion, without requiring new physical infrastructure. Our approach integrates classical traffic flow theory, gap acceptance models, and microscopic simulation into a physics-informed RL framework. By abstracting roads into super-segments, the agent captures emergent flow dynamics and learns robust speed modulation policies from instantaneous traffic observations. Evaluated in the high-fidelity PTV Vissim simulator on a real-world highway network, our method improves total throughput by 5%, reduces average delay by 13%, and decreases total stops by 3% compared to the no-control setting. It also achieves smoother, congestion-resistant flow while generalizing across varied traffic patterns, demonstrating its potential for scalable, ML-driven traffic management.</li>
</ul>

<h3>Title: How Visual Representations Map to Language Feature Space in Multimodal LLMs</h3>
<ul>
<li><strong>Authors: </strong>Constantin Venhoff, Ashkan Khakzar, Sonia Joseph, Philip Torr, Neel Nanda</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11976">https://arxiv.org/abs/2506.11976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11976">https://arxiv.org/pdf/2506.11976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11976]] How Visual Representations Map to Language Feature Space in Multimodal LLMs(https://arxiv.org/abs/2506.11976)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Effective multimodal reasoning depends on the alignment of visual and linguistic representations, yet the mechanisms by which vision-language models (VLMs) achieve this alignment remain poorly understood. We introduce a methodological framework that deliberately maintains a frozen large language model (LLM) and a frozen vision transformer (ViT), connected solely by training a linear adapter during visual instruction tuning. This design is fundamental to our approach: by keeping the language model frozen, we ensure it maintains its original language representations without adaptation to visual data. Consequently, the linear adapter must map visual features directly into the LLM's existing representational space rather than allowing the language model to develop specialized visual understanding through fine-tuning. Our experimental design uniquely enables the use of pre-trained sparse autoencoders (SAEs) of the LLM as analytical probes. These SAEs remain perfectly aligned with the unchanged language model and serve as a snapshot of the learned language feature-representations. Through systematic analysis of SAE reconstruction error, sparsity patterns, and feature SAE descriptions, we reveal the layer-wise progression through which visual representations gradually align with language feature representations, converging in middle-to-later layers. This suggests a fundamental misalignment between ViT outputs and early LLM layers, raising important questions about whether current adapter-based architectures optimally facilitate cross-modal representation learning.</li>
</ul>

<h3>Title: VGR: Visual Grounded Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Jiacong Wang, Zijiang Kang, Haochen Wang, Haiyong Jiang, Jiawen Li, Bohong Wu, Ya Wang, Jiao Ran, Xiao Liang, Chao Feng, Jun Xiao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11991">https://arxiv.org/abs/2506.11991</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11991">https://arxiv.org/pdf/2506.11991</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11991]] VGR: Visual Grounded Reasoning(https://arxiv.org/abs/2506.11991)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In the field of multimodal chain-of-thought (CoT) reasoning, existing approaches predominantly rely on reasoning on pure language space, which inherently suffers from language bias and is largely confined to math or science domains. This narrow focus limits their ability to handle complex visual reasoning tasks that demand comprehensive understanding of image details. To address these limitations, this paper introduces VGR, a novel reasoning multimodal large language model (MLLM) with enhanced fine-grained visual perception capabilities. Unlike traditional MLLMs that answer the question or reasoning solely on the language space, our VGR first detects relevant regions that may help to solve problems, and then provides precise answers based on replayed image regions. To achieve this, we conduct a large-scale SFT dataset called VGR -SFT that contains reasoning data with mixed vision grounding and language deduction. The inference pipeline of VGR allows the model to choose bounding boxes for visual reference and a replay stage is introduced to integrates the corresponding regions into the reasoning process, enhancing multimodel comprehension. Experiments on the LLaVA-NeXT-7B baseline show that VGR achieves superior performance on multi-modal benchmarks requiring comprehensive image detail understanding. Compared to the baseline, VGR uses only 30\% of the image token count while delivering scores of +4.1 on MMStar, +7.1 on AI2D, and a +12.9 improvement on ChartQA.</li>
</ul>

<h3>Title: Compression Aware Certified Training</h3>
<ul>
<li><strong>Authors: </strong>Changming Xu, Gagandeep Singh</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11992">https://arxiv.org/abs/2506.11992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11992">https://arxiv.org/pdf/2506.11992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11992]] Compression Aware Certified Training(https://arxiv.org/abs/2506.11992)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep neural networks deployed in safety-critical, resource-constrained environments must balance efficiency and robustness. Existing methods treat compression and certified robustness as separate goals, compromising either efficiency or safety. We propose CACTUS (Compression Aware Certified Training Using network Sets), a general framework for unifying these objectives during training. CACTUS models maintain high certified accuracy even when compressed. We apply CACTUS for both pruning and quantization and show that it effectively trains models which can be efficiently compressed while maintaining high accuracy and certifiable robustness. CACTUS achieves state-of-the-art accuracy and certified performance for both pruning and quantization on a variety of datasets and input specifications.</li>
</ul>

<h3>Title: pLSTM: parallelizable Linear Source Transition Mark networks</h3>
<ul>
<li><strong>Authors: </strong>Korbinian Pöppel, Richard Freinschlag, Thomas Schmied, Wei Lin, Sepp Hochreiter</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.11997">https://arxiv.org/abs/2506.11997</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.11997">https://arxiv.org/pdf/2506.11997</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.11997]] pLSTM: parallelizable Linear Source Transition Mark networks(https://arxiv.org/abs/2506.11997)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Modern recurrent architectures, such as xLSTM and Mamba, have recently challenged the Transformer in language modeling. However, their structure constrains their applicability to sequences only or requires processing multi-dimensional data structures, such as images or molecular graphs, in a pre-defined sequential order. In contrast, Multi-Dimensional RNNs (MDRNNs) are well suited for data with a higher level structure, like 2D grids, trees, and directed acyclic graphs (DAGs). In this work, we extend the notion of multi-dimensionality to linear RNNs. We introduce parallelizable Linear Source Transition Mark networks (pLSTMs) using Source, Transition, and Mark gates that act on the line graph of a general DAG. This enables parallelization in analogy to parallel associative scans and the chunkwise-recurrent form of sequential linear RNNs, but for DAGs. For regular grids (1D and 2D), like images, this scheme can be efficiently implemented using einsum operations, concatenations, and padding in logarithmic time. pLSTMs tackle the vanishing/exploding activation/gradient problem for long distances in DAGs via two distinct modes: a directed propagation mode (P-mode) and a diffusive distribution mode (D-mode). To showcase the long-range capabilities of pLSTM, we introduce arrow-pointing extrapolation as a synthetic computer vision task that contains long-distance directional information. We demonstrate that pLSTMs generalize well to larger image sizes, whereas Transformers struggle to extrapolate. On established molecular graph and computer vision benchmarks, pLSTMs also show strong performance. Code and Datasets are available at: this https URL.</li>
</ul>

<h3>Title: SIMSHIFT: A Benchmark for Adapting Neural Surrogates to Distribution Shifts</h3>
<ul>
<li><strong>Authors: </strong>Paul Setinek, Gianluca Galletti, Thomas Gross, Dominik Schnürer, Johannes Brandstetter, Werner Zellinger</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.12007">https://arxiv.org/abs/2506.12007</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.12007">https://arxiv.org/pdf/2506.12007</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.12007]] SIMSHIFT: A Benchmark for Adapting Neural Surrogates to Distribution Shifts(https://arxiv.org/abs/2506.12007)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Neural surrogates for Partial Differential Equations (PDEs) often suffer significant performance degradation when evaluated on unseen problem configurations, such as novel material types or structural dimensions. Meanwhile, Domain Adaptation (DA) techniques have been widely used in vision and language processing to generalize from limited information about unseen configurations. In this work, we address this gap through two focused contributions. First, we introduce SIMSHIFT, a novel benchmark dataset and evaluation suite composed of four industrial simulation tasks: hot rolling, sheet metal forming, electric motor design and heatsink design. Second, we extend established domain adaptation methods to state of the art neural surrogates and systematically evaluate them. These approaches use parametric descriptions and ground truth simulations from multiple source configurations, together with only parametric descriptions from target configurations. The goal is to accurately predict target simulations without access to ground truth simulation data. Extensive experiments on SIMSHIFT highlight the challenges of out of distribution neural surrogate modeling, demonstrate the potential of DA in simulation, and reveal open problems in achieving robust neural surrogates under distribution shifts in industrially relevant scenarios. Our codebase is available at this https URL</li>
</ul>

<h3>Title: code_transformed: The Influence of Large Language Models on Code</h3>
<ul>
<li><strong>Authors: </strong>Yuliang Xu, Siming Huang, Mingmeng Geng, Yao Wan, Xuanhua Shi, Dongping Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2506.12014">https://arxiv.org/abs/2506.12014</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2506.12014">https://arxiv.org/pdf/2506.12014</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2506.12014]] code_transformed: The Influence of Large Language Models on Code(https://arxiv.org/abs/2506.12014)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Coding remains one of the most fundamental modes of interaction between humans and machines. With the rapid advancement of Large Language Models (LLMs), code generation capabilities have begun to significantly reshape programming practices. This development prompts a central question: Have LLMs transformed code style, and how can such transformation be characterized? In this paper, we present a pioneering study that investigates the impact of LLMs on code style, with a focus on naming conventions, complexity, maintainability, and similarity. By analyzing code from over 19,000 GitHub repositories linked to arXiv papers published between 2020 and 2025, we identify measurable trends in the evolution of coding style that align with characteristics of LLM-generated code. For instance, the proportion of snake\_case variable names in Python code increased from 47% in Q1 2023 to 51% in Q1 2025. Furthermore, we investigate how LLMs approach algorithmic problems by examining their reasoning processes. Given the diversity of LLMs and usage scenarios, among other factors, it is difficult or even impossible to precisely estimate the proportion of code generated or assisted by LLMs. Our experimental results provide the first large-scale empirical evidence that LLMs affect real-world programming style.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
