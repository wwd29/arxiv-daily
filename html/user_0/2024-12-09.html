<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-12-09</h1>
<h3>Title: Take Package as Language: Anomaly Detection Using Transformer</h3>
<ul>
<li><strong>Authors: </strong>Jie Huang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04473">https://arxiv.org/abs/2412.04473</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04473">https://arxiv.org/pdf/2412.04473</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04473]] Take Package as Language: Anomaly Detection Using Transformer(https://arxiv.org/abs/2412.04473)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Network data packet anomaly detection faces numerous challenges, including exploring new anomaly supervision signals, researching weakly supervised anomaly detection, and improving model interpretability. This paper proposes NIDS-GPT, a GPT-based causal language model for network intrusion detection. Unlike previous work, NIDS-GPT innovatively treats each number in the packet as an independent "word" rather than packet fields, enabling a more fine-grained data representation. We adopt an improved GPT-2 model and design special tokenizers and embedding layers to better capture the structure and semantics of network data. NIDS-GPT has good scalability, supports unsupervised pre-training, and enhances model interpretability through attention weight visualization. Experiments on the CICIDS2017 and car-hacking datasets show that NIDS-GPT achieves 100\% accuracy under extreme imbalance conditions, far surpassing traditional methods; it also achieves over 90\% accuracy in one-shot learning. These results demonstrate NIDS-GPT's excellent performance and potential in handling complex network anomaly detection tasks, especially in data-imbalanced and resource-constrained scenarios. The code is available at \url{this https URL</li>
</ul>

<h3>Title: Socio-Emotional Response Generation: A Human Evaluation Protocol for LLM-Based Conversational Systems</h3>
<ul>
<li><strong>Authors: </strong>Lorraine Vanel, Ariel R. Ramos Vela, Alya Yacoubi, Chlo√© Clavel (IDS, S2A, LTCI)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04492">https://arxiv.org/abs/2412.04492</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04492">https://arxiv.org/pdf/2412.04492</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04492]] Socio-Emotional Response Generation: A Human Evaluation Protocol for LLM-Based Conversational Systems(https://arxiv.org/abs/2412.04492)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Conversational systems are now capable of producing impressive and generally relevant responses. However, we have no visibility nor control of the socio-emotional strategies behind state-of-the-art Large Language Models (LLMs), which poses a problem in terms of their transparency and thus their trustworthiness for critical applications. Another issue is that current automated metrics are not able to properly evaluate the quality of generated responses beyond the dataset's ground truth. In this paper, we propose a neural architecture that includes an intermediate step in planning socio-emotional strategies before response generation. We compare the performance of open-source baseline LLMs to the outputs of these same models augmented with our planning module. We also contrast the outputs obtained from automated metrics and evaluation results provided by human annotators. We describe a novel evaluation protocol that includes a coarse-grained consistency evaluation, as well as a finer-grained annotation of the responses on various social and emotional criteria. Our study shows that predicting a sequence of expected strategy labels and using this sequence to generate a response yields better results than a direct end-to-end generation scheme. It also highlights the divergences and the limits of current evaluation metrics for generated content. The code for the annotation platform and the annotated data are made publicly available for the evaluation of future models.</li>
</ul>

<h3>Title: MAG-V: A Multi-Agent Framework for Synthetic Data Generation and Verification</h3>
<ul>
<li><strong>Authors: </strong>Saptarshi Sengupta, Kristal Curtis, Akshay Mallipeddi, Abhinav Mathur, Joseph Ross, Liang Gou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04494">https://arxiv.org/abs/2412.04494</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04494">https://arxiv.org/pdf/2412.04494</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04494]] MAG-V: A Multi-Agent Framework for Synthetic Data Generation and Verification(https://arxiv.org/abs/2412.04494)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Extending the capabilities of Large Language Models (LLMs) with functions or tools for environment interaction has led to the emergence of the agent paradigm. In industry, training an LLM is not always feasible because of the scarcity of domain data, legal holds on proprietary customer data, rapidly changing business requirements, and the need to prototype new assistants. Agents provide an elegant solution to the above by relying on the zero-shot reasoning abilities of the underlying LLM and utilizing tools to explore and reason over customer data and respond to user requests. However, there are two concerns here: (I) acquiring large scale customer queries for agent testing is time-consuming, and (II) high reliance on the tool call sequence (or trajectory) followed by the agent to respond to user queries may lead to unexpected or incorrect behavior. To address this, we propose MAG-V, a multi-agent framework to first generate a dataset of questions that mimic customer queries; and second, reverse-engineer alternate questions from the responses for trajectory verification. Initial results indicate that our synthetic data can improve agent performance on actual customer queries. Furthermore, our trajectory verification methodology, inspired by distant supervision and using traditional machine learning (ML) models, outperforms a GPT-4o judge baseline by 11% accuracy and matches the performance of a GPT-4 judge on our constructed dataset. Overall, our approach is a step towards unifying diverse task agents into a cohesive framework for achieving an aligned objective.</li>
</ul>

<h3>Title: Artificial intelligence and cybersecurity in banking sector: opportunities and risks</h3>
<ul>
<li><strong>Authors: </strong>Ana Kovacevic, Sonja D. Radenkovic, Dragana Nikolic</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04495">https://arxiv.org/abs/2412.04495</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04495">https://arxiv.org/pdf/2412.04495</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04495]] Artificial intelligence and cybersecurity in banking sector: opportunities and risks(https://arxiv.org/abs/2412.04495)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect, attack, robust</a></li>
<li><strong>Abstract: </strong>The rapid advancements in artificial intelligence (AI) have presented new opportunities for enhancing efficiency and economic competitiveness across various industries, espcially in banking. Machine learning (ML), as a subset of artificial intelligence, enables systems to adapt and learn from vast datasets, revolutionizing decision-making processes, fraud detection, and customer service automation. However, these innovations also introduce new challenges, particularly in the realm of cybersecurity. Adversarial attacks, such as data poisoning and evasion attacks, represent critical threats to machine learning models, exploiting vulnerabilities to manipulate outcomes or compromise sensitive information. Furthermore, this study highlights the dual-use nature of AI tools, which can be used by malicious users. To address these challenges, the paper emphasizes the importance of developing machine learning models with key characteristics such as security, trust, resilience and robustness. These features are essential to mitigating risks and ensuring the secure deployment of AI technologies in banking sectors, where the protection of financial data is paramount. The findings underscore the urgent need for enhanced cybersecurity frameworks and continuous improvements in defensive mechanisms. By exploring both opportunities and risks, this paper aims to guide the responsible integration of AI in the banking sector, paving the way for innovation while safeguarding against emerging threats.</li>
</ul>

<h3>Title: Credible fusion of evidence in distributed system subject to cyberattacks</h3>
<ul>
<li><strong>Authors: </strong>Chaoxiong Ma, Yan Liang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04496">https://arxiv.org/abs/2412.04496</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04496">https://arxiv.org/pdf/2412.04496</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04496]] Credible fusion of evidence in distributed system subject to cyberattacks(https://arxiv.org/abs/2412.04496)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack</a></li>
<li><strong>Abstract: </strong>Given that distributed systems face adversarial behaviors such as eavesdropping and cyberattacks, how to ensure the evidence fusion result is credible becomes a must-be-addressed topic. Different from traditional research that assumes nodes are cooperative, we focus on three requirements for evidence fusion, i.e., preserving evidence's privacy, identifying attackers and excluding their evidence, and dissipating high-conflicting among evidence caused by random noise and interference. To this end, this paper proposes an algorithm for credible evidence fusion against cyberattacks. Firstly, the fusion strategy is constructed based on conditionalized credibility to avoid counterintuitive fusion results caused by high-conflicting. Under this strategy, distributed evidence fusion is transformed into the average consensus problem for the weighted average value by conditional credibility of multi-source evidence (WAVCCME), which implies a more concise consensus process and lower computational complexity than existing algorithms. Secondly, a state decomposition and reconstruction strategy with weight encryption is designed, and its effectiveness for privacy-preserving under directed graphs is guaranteed: decomposing states into different random sub-states for different neighbors to defend against internal eavesdroppers, and encrypting the sub-states' weight in the reconstruction to guard against out-of-system eavesdroppers. Finally, the identities and types of attackers are identified by inter-neighbor broadcasting and comparison of nodes' states, and the proposed update rule with state corrections is used to achieve the consensus of the WAVCCME. The states of normal nodes are shown to converge to their WAVCCME, while the attacker's evidence is excluded from the fusion, as verified by the simulation on a distributed unmanned reconnaissance swarm.</li>
</ul>

<h3>Title: Opportunities and Challenges of Large Language Models for Low-Resource Languages in Humanities Research</h3>
<ul>
<li><strong>Authors: </strong>Tianyang Zhong, Zhenyuan Yang, Zhengliang Liu, Ruidong Zhang, Yiheng Liu, Haiyang Sun, Yi Pan, Yiwei Li, Yifan Zhou, Hanqi Jiang, Junhao Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04497">https://arxiv.org/abs/2412.04497</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04497">https://arxiv.org/pdf/2412.04497</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04497]] Opportunities and Challenges of Large Language Models for Low-Resource Languages in Humanities Research(https://arxiv.org/abs/2412.04497)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Low-resource languages serve as invaluable repositories of human history, embodying cultural evolution and intellectual diversity. Despite their significance, these languages face critical challenges, including data scarcity and technological limitations, which hinder their comprehensive study and preservation. Recent advancements in large language models (LLMs) offer transformative opportunities for addressing these challenges, enabling innovative methodologies in linguistic, historical, and cultural research. This study systematically evaluates the applications of LLMs in low-resource language research, encompassing linguistic variation, historical documentation, cultural expressions, and literary analysis. By analyzing technical frameworks, current methodologies, and ethical considerations, this paper identifies key challenges such as data accessibility, model adaptability, and cultural sensitivity. Given the cultural, historical, and linguistic richness inherent in low-resource languages, this work emphasizes interdisciplinary collaboration and the development of customized models as promising avenues for advancing research in this domain. By underscoring the potential of integrating artificial intelligence with the humanities to preserve and study humanity's linguistic and cultural heritage, this study fosters global efforts towards safeguarding intellectual diversity.</li>
</ul>

<h3>Title: Large Language Models in Politics and Democracy: A Comprehensive Survey</h3>
<ul>
<li><strong>Authors: </strong>Goshi Aoki</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04498">https://arxiv.org/abs/2412.04498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04498">https://arxiv.org/pdf/2412.04498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04498]] Large Language Models in Politics and Democracy: A Comprehensive Survey(https://arxiv.org/abs/2412.04498)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, generative, large language model</a></li>
<li><strong>Abstract: </strong>The advancement of generative AI, particularly large language models (LLMs), has a significant impact on politics and democracy, offering potential across various domains, including policymaking, political communication, analysis, and governance. This paper surveys the recent and potential applications of LLMs in politics, examining both their promises and the associated challenges. This paper examines the ways in which LLMs are being employed in legislative processes, political communication, and political analysis. Moreover, we investigate the potential of LLMs in diplomatic and national security contexts, economic and social modeling, and legal applications. While LLMs offer opportunities to enhance efficiency, inclusivity, and decision-making in political processes, they also present challenges related to bias, transparency, and accountability. The paper underscores the necessity for responsible development, ethical considerations, and governance frameworks to ensure that the integration of LLMs into politics aligns with democratic values and promotes a more just and equitable society.</li>
</ul>

<h3>Title: A Primer on Large Language Models and their Limitations</h3>
<ul>
<li><strong>Authors: </strong>Sandra Johnson, David Hyland-Wood</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04503">https://arxiv.org/abs/2412.04503</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04503">https://arxiv.org/pdf/2412.04503</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04503]] A Primer on Large Language Models and their Limitations(https://arxiv.org/abs/2412.04503)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper provides a primer on Large Language Models (LLMs) and identifies their strengths, limitations, applications and research directions. It is intended to be useful to those in academia and industry who are interested in gaining an understanding of the key LLM concepts and technologies, and in utilising this knowledge in both day to day tasks and in more complex scenarios where this technology can enhance current practices and processes.</li>
</ul>

<h3>Title: Multi-Bin Batching for Increasing LLM Inference Throughput</h3>
<ul>
<li><strong>Authors: </strong>Ozgur Guldogan, Jackson Kunde, Kangwook Lee, Ramtin Pedarsani</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.DC, cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04504">https://arxiv.org/abs/2412.04504</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04504">https://arxiv.org/pdf/2412.04504</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04504]] Multi-Bin Batching for Increasing LLM Inference Throughput(https://arxiv.org/abs/2412.04504)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) grow in popularity for their diverse capabilities, improving the efficiency of their inference systems has become increasingly critical. Batching LLM requests is a critical step in scheduling the inference jobs on servers (e.g. GPUs), enabling the system to maximize throughput by allowing multiple requests to be processed in parallel. However, requests often have varying generation lengths, causing resource underutilization, as hardware must wait for the longest-running request in the batch to complete before moving to the next batch. We formalize this problem from a queueing-theoretic perspective, and aim to design a control policy which is throughput-optimal. We propose Multi-Bin Batching, a simple yet effective method that can provably improve LLM inference throughput by grouping requests with similar (predicted) execution times into predetermined bins. Through a combination of theoretical analysis and experiments, including real-world LLM inference scenarios, we demonstrate significant throughput gains compared to standard batching approaches.</li>
</ul>

<h3>Title: Achieving Semantic Consistency Using BERT: Application of Pre-training Semantic Representations Model in Social Sciences Research</h3>
<ul>
<li><strong>Authors: </strong>Ruiyu Zhang, Lin Nie, Ce Zhao, Qingyang Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, econ.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04505">https://arxiv.org/abs/2412.04505</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04505">https://arxiv.org/pdf/2412.04505</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04505]] Achieving Semantic Consistency Using BERT: Application of Pre-training Semantic Representations Model in Social Sciences Research(https://arxiv.org/abs/2412.04505)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Achieving consistent word interpretations across different time spans is crucial in social sciences research and text analysis tasks, as stable semantic representations form the foundation for research and task correctness, enhancing understanding of socio-political and cultural analysis. Traditional models like Word2Vec have provided significant insights into long-term semantic changes but often struggle to capture stable meanings in short-term contexts, which may be attributed to fluctuations in embeddings caused by unbalanced training data. Recent advancements, particularly BERT (Bidirectional Encoder Representations from Transformers), its pre-trained nature and transformer encoder architecture offer contextual embeddings that improve semantic consistency, making it a promising tool for short-term analysis. This study empirically compares the performance of Word2Vec and BERT in maintaining stable word meanings over time in text analysis tasks relevant to social sciences research. Using articles from the People's Daily spanning 20 years (2004-2023), we evaluated the semantic stability of each model across different timeframes. The results indicate that BERT consistently outperforms Word2Vec in maintaining semantic stability, offering greater stability in contextual embeddings. However, the study also acknowledges BERT's limitations in capturing gradual semantic shifts over longer periods due to its inherent stability. The findings suggest that while BERT is advantageous for short-term semantic analysis in social sciences, researchers should consider complementary approaches for long-term studies to fully capture semantic drift. This research underscores the importance of selecting appropriate word embedding models based on the specific temporal context of social science analyses.</li>
</ul>

<h3>Title: Pragmatic Metacognitive Prompting Improves LLM Performance on Sarcasm Detection</h3>
<ul>
<li><strong>Authors: </strong>Joshua Lee, Wyatt Fong, Alexander Le, Sur Shah, Kevin Han, Kevin Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04509">https://arxiv.org/abs/2412.04509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04509">https://arxiv.org/pdf/2412.04509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04509]] Pragmatic Metacognitive Prompting Improves LLM Performance on Sarcasm Detection(https://arxiv.org/abs/2412.04509)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Sarcasm detection is a significant challenge in sentiment analysis due to the nuanced and context-dependent nature of verbiage. We introduce Pragmatic Metacognitive Prompting (PMP) to improve the performance of Large Language Models (LLMs) in sarcasm detection, which leverages principles from pragmatics and reflection helping LLMs interpret implied meanings, consider contextual cues, and reflect on discrepancies to identify sarcasm. Using state-of-the-art LLMs such as LLaMA-3-8B, GPT-4o, and Claude 3.5 Sonnet, PMP achieves state-of-the-art performance on GPT-4o on MUStARD and SemEval2018. This study demonstrates that integrating pragmatic reasoning and metacognitive strategies into prompting significantly enhances LLMs' ability to detect sarcasm, offering a promising direction for future research in sentiment analysis.</li>
</ul>

<h3>Title: A Taxonomy of System-Level Attacks on Deep Learning Models in Autonomous Vehicles</h3>
<ul>
<li><strong>Authors: </strong>Masoud Jamshidiyan Tehrani, Jinhan Kim, Rosmael Zidane Lekeufack Foulefack, Alessandro Marchetto, Paolo Tonella</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04510">https://arxiv.org/abs/2412.04510</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04510">https://arxiv.org/pdf/2412.04510</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04510]] A Taxonomy of System-Level Attacks on Deep Learning Models in Autonomous Vehicles(https://arxiv.org/abs/2412.04510)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>The advent of deep learning and its astonishing performance in perception tasks, such as object recognition and classification, has enabled its usage in complex systems, including autonomous vehicles. On the other hand, deep learning models are susceptible to mis-predictions when small, adversarial changes are introduced into their input. Such mis-predictions can be triggered in the real world and can propagate to a failure of the entire system, as opposed to a localized mis-prediction. In recent years, a growing number of research works have investigated ways to mount attacks against autonomous vehicles that exploit deep learning components for perception tasks. Such attacks are directed toward elements of the environment where these systems operate and their effectiveness is assessed in terms of system-level failures triggered by them. There has been however no systematic attempt to analyze and categorize such attacks. In this paper, we present the first taxonomy of system-level attacks against autonomous vehicles. We constructed our taxonomy by first collecting 8,831 papers, then filtering them down to 1,125 candidates and eventually selecting a set of 19 highly relevant papers that satisfy all inclusion criteria. Then, we tagged them with taxonomy categories, involving three assessors per paper. The resulting taxonomy includes 12 top-level categories and several sub-categories. The taxonomy allowed us to investigate the attack features, the most attacked components, the underlying threat models, and the propagation chains from input perturbation to system-level failure. We distilled several lessons for practitioners and identified possible directions for future work for researchers.</li>
</ul>

<h3>Title: Prompting Large Language Models for Clinical Temporal Relation Extraction</h3>
<ul>
<li><strong>Authors: </strong>Jianping He, Laila Rasmy, Haifang Li, Jianfu Li, Zenan Sun, Evan Yu, Degui Zhi, Cui Tao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04512">https://arxiv.org/abs/2412.04512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04512">https://arxiv.org/pdf/2412.04512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04512]] Prompting Large Language Models for Clinical Temporal Relation Extraction(https://arxiv.org/abs/2412.04512)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Objective: This paper aims to prompt large language models (LLMs) for clinical temporal relation extraction (CTRE) in both few-shot and fully supervised settings. Materials and Methods: This study utilizes four LLMs: Encoder-based GatorTron-Base (345M)/Large (8.9B); Decoder-based LLaMA3-8B/MeLLaMA-13B. We developed full (FFT) and parameter-efficient (PEFT) fine-tuning strategies and evaluated these strategies on the 2012 i2b2 CTRE task. We explored four fine-tuning strategies for GatorTron-Base: (1) Standard Fine-Tuning, (2) Hard-Prompting with Unfrozen LLMs, (3) Soft-Prompting with Frozen LLMs, and (4) Low-Rank Adaptation (LoRA) with Frozen LLMs. For GatorTron-Large, we assessed two PEFT strategies-Soft-Prompting and LoRA with Frozen LLMs-leveraging Quantization techniques. Additionally, LLaMA3-8B and MeLLaMA-13B employed two PEFT strategies: LoRA strategy with Quantization (QLoRA) applied to Frozen LLMs using instruction tuning and standard fine-tuning. Results: Under fully supervised settings, Hard-Prompting with Unfrozen GatorTron-Base achieved the highest F1 score (89.54%), surpassing the SOTA model (85.70%) by 3.74%. Additionally, two variants of QLoRA adapted to GatorTron-Large and Standard Fine-Tuning of GatorTron-Base exceeded the SOTA model by 2.36%, 1.88%, and 0.25%, respectively. Decoder-based models with frozen parameters outperformed their Encoder-based counterparts in this setting; however, the trend reversed in few-shot scenarios. Discussions and Conclusions: This study presented new methods that significantly improved CTRE performance, benefiting downstream tasks reliant on CTRE systems. The findings underscore the importance of selecting appropriate models and fine-tuning strategies based on task requirements and data availability. Future work will explore larger models and broader CTRE applications.</li>
</ul>

<h3>Title: Privacy-Preserving Gesture Tracking System Utilizing Frequency-Hopping RFID Signals</h3>
<ul>
<li><strong>Authors: </strong>Bojun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04518">https://arxiv.org/abs/2412.04518</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04518">https://arxiv.org/pdf/2412.04518</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04518]] Privacy-Preserving Gesture Tracking System Utilizing Frequency-Hopping RFID Signals(https://arxiv.org/abs/2412.04518)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect</a></li>
<li><strong>Abstract: </strong>Gesture tracking technology provides users with a hands free interactive experience without the need to hold or touch devices. However, current gesture tracking research has primarily focused on tracking accuracy while neglecting issues of user privacy protection and security. This study aims to develop a gesture tracking system based on frequency hopping RFID signals that effectively protects user privacy without compromising tracking efficiency and accuracy. By introducing frequency hopping technology, we have designed a mechanism that prevents potential eavesdroppers from obtaining raw RFID signals, thereby enhancing the systems privacy protection capabilities. The system architec ture includes the collection of RFID signals, data processing, signal recovery, and gesture tracking. Experimental results show that our method significantly improves privacy protection levels while maintaining real time and accuracy. This research not only provides a new perspective for the field of gesture tracking but also offers valuable insights for the use of RFID technology in privacy-sensitive applications.</li>
</ul>

<h3>Title: FedDW: Distilling Weights through Consistency Optimization in Heterogeneous Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Jiayu Liu, Yong Wang, Nianbin Wang, Jing Yang, Xiaohui Tao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04521">https://arxiv.org/abs/2412.04521</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04521">https://arxiv.org/pdf/2412.04521</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04521]] FedDW: Distilling Weights through Consistency Optimization in Heterogeneous Federated Learning(https://arxiv.org/abs/2412.04521)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) is an innovative distributed machine learning paradigm that enables neural network training across devices without centralizing data. While this addresses issues of information sharing and data privacy, challenges arise from data heterogeneity across clients and increasing network scale, leading to impacts on model performance and training efficiency. Previous research shows that in IID environments, the parameter structure of the model is expected to adhere to certain specific consistency principles. Thus, identifying and regularizing these consistencies can mitigate issues from heterogeneous data. We found that both soft labels derived from knowledge distillation and the classifier head parameter matrix, when multiplied by their own transpose, capture the intrinsic relationships between data classes. These shared relationships suggest inherent consistency. Therefore, the work in this paper identifies the consistency between the two and leverages it to regulate training, underpinning our proposed FedDW framework. Experimental results show FedDW outperforms 10 state-of-the-art FL methods, improving accuracy by an average of 3% in highly heterogeneous settings. Additionally, we provide a theoretical proof that FedDW offers higher efficiency, with the additional computational load from backpropagation being negligible. The code is available at this https URL.</li>
</ul>

<h3>Title: Leveraging Multimodal Protein Representations to Predict Protein Melting Temperatures</h3>
<ul>
<li><strong>Authors: </strong>Daiheng Zhang, Yan Zeng, Xinyu Hong, Jinbo Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04526">https://arxiv.org/abs/2412.04526</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04526">https://arxiv.org/pdf/2412.04526</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04526]] Leveraging Multimodal Protein Representations to Predict Protein Melting Temperatures(https://arxiv.org/abs/2412.04526)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, fair</a></li>
<li><strong>Abstract: </strong>Accurately predicting protein melting temperature changes (Delta Tm) is fundamental for assessing protein stability and guiding protein engineering. Leveraging multi-modal protein representations has shown great promise in capturing the complex relationships among protein sequences, structures, and functions. In this study, we develop models based on powerful protein language models, including ESM-2, ESM-3, SaProt, and AlphaFold, using various feature extraction methods to enhance prediction accuracy. By utilizing the ESM-3 model, we achieve a new state-of-the-art performance on the s571 test dataset, obtaining a Pearson correlation coefficient (PCC) of 0.50. Furthermore, we conduct a fair evaluation to compare the performance of different protein language models in the Delta Tm prediction task. Our results demonstrate that integrating multi-modal protein representations could advance the prediction of protein melting temperatures.</li>
</ul>

<h3>Title: WinTSR: A Windowed Temporal Saliency Rescaling Method for Interpreting Time Series Deep Learning Models</h3>
<ul>
<li><strong>Authors: </strong>Md. Khairul Islam, Judy Fox</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04532">https://arxiv.org/abs/2412.04532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04532">https://arxiv.org/pdf/2412.04532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04532]] WinTSR: A Windowed Temporal Saliency Rescaling Method for Interpreting Time Series Deep Learning Models(https://arxiv.org/abs/2412.04532)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Interpreting complex time series forecasting models is challenging due to the temporal dependencies between time steps and the dynamic relevance of input features over time. Existing interpretation methods are limited by focusing mostly on classification tasks, evaluating using custom baseline models instead of the latest time series models, using simple synthetic datasets, and requiring training another model. We introduce a novel interpretation method called Windowed Temporal Saliency Rescaling (WinTSR) addressing these limitations. WinTSR explicitly captures temporal dependencies among the past time steps and efficiently scales the feature importance with this time importance. We benchmark WinTSR against 10 recent interpretation techniques with 5 state-of-the-art deep-learning models of different architectures, including a time series foundation model. We use 3 real-world datasets for both time-series classification and regression. Our comprehensive analysis shows that WinTSR significantly outranks the other local interpretation methods in overall performance. Finally, we provide a novel and open-source framework to interpret the latest time series transformers and foundation models.</li>
</ul>

<h3>Title: Mask-Adapter: The Devil is in the Masks for Open-Vocabulary Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Yongkang Li, Tianheng Cheng, Wenyu Liu, Xinggang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04533">https://arxiv.org/abs/2412.04533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04533">https://arxiv.org/pdf/2412.04533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04533]] Mask-Adapter: The Devil is in the Masks for Open-Vocabulary Segmentation(https://arxiv.org/abs/2412.04533)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Recent open-vocabulary segmentation methods adopt mask generators to predict segmentation masks and leverage pre-trained vision-language models, e.g., CLIP, to classify these masks via mask pooling. Although these approaches show promising results, it is counterintuitive that accurate masks often fail to yield accurate classification results through pooling CLIP image embeddings within the mask regions. In this paper, we reveal the performance limitations of mask pooling and introduce Mask-Adapter, a simple yet effective method to address these challenges in open-vocabulary segmentation. Compared to directly using proposal masks, our proposed Mask-Adapter extracts semantic activation maps from proposal masks, providing richer contextual information and ensuring alignment between masks and CLIP. Additionally, we propose a mask consistency loss that encourages proposal masks with similar IoUs to obtain similar CLIP embeddings to enhance models' robustness to varying predicted masks. Mask-Adapter integrates seamlessly into open-vocabulary segmentation methods based on mask pooling in a plug-and-play manner, delivering more accurate classification results. Extensive experiments across several zero-shot benchmarks demonstrate significant performance gains for the proposed Mask-Adapter on several well-established methods. Notably, Mask-Adapter also extends effectively to SAM and achieves impressive results on several open-vocabulary segmentation datasets. Code and models are available at \url{this https URL}.</li>
</ul>

<h3>Title: Understanding Hidden Computations in Chain-of-Thought Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Aryasomayajula Ram Bharadwaj</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04537">https://arxiv.org/abs/2412.04537</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04537">https://arxiv.org/pdf/2412.04537</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04537]] Understanding Hidden Computations in Chain-of-Thought Reasoning(https://arxiv.org/abs/2412.04537)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Chain-of-Thought (CoT) prompting has significantly enhanced the reasoning abilities of large language models. However, recent studies have shown that models can still perform complex reasoning tasks even when the CoT is replaced with filler(hidden) characters (e.g., "..."), leaving open questions about how models internally process and represent reasoning steps. In this paper, we investigate methods to decode these hidden characters in transformer models trained with filler CoT sequences. By analyzing layer-wise representations using the logit lens method and examining token rankings, we demonstrate that the hidden characters can be recovered without loss of performance. Our findings provide insights into the internal mechanisms of transformer models and open avenues for improving interpretability and transparency in language model reasoning.</li>
</ul>

<h3>Title: Communication Compression for Distributed Learning without Control Variates</h3>
<ul>
<li><strong>Authors: </strong>Tomas Ortega, Chun-Yin Huang, Xiaoxiao Li, Hamid Jafarkhani</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04538">https://arxiv.org/abs/2412.04538</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04538">https://arxiv.org/pdf/2412.04538</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04538]] Communication Compression for Distributed Learning without Control Variates(https://arxiv.org/abs/2412.04538)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Distributed learning algorithms, such as the ones employed in Federated Learning (FL), require communication compression to reduce the cost of client uploads. The compression methods used in practice are often biased, which require error feedback to achieve convergence when the compression is aggressive. In turn, error feedback requires client-specific control variates, which directly contradicts privacy-preserving principles and requires stateful clients. In this paper, we propose Compressed Aggregate Feedback (CAFe), a novel distributed learning framework that allows highly compressible client updates by exploiting past aggregated updates, and does not require control variates. We consider Distributed Gradient Descent (DGD) as a representative algorithm and provide a theoretical proof of CAFe's superiority to Distributed Compressed Gradient Descent (DCGD) with biased compression in the non-smooth regime with bounded gradient dissimilarity. Experimental results confirm that CAFe consistently outperforms distributed learning with direct compression and highlight the compressibility of the client updates with CAFe.</li>
</ul>

<h3>Title: Solving High-dimensional Inverse Problems Using Amortized Likelihood-free Inference with Noisy and Incomplete Data</h3>
<ul>
<li><strong>Authors: </strong>Jice Zeng, Yuanzhe Wang, Alexandre M. Tartakovsky, David Barajas-Solano</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04565">https://arxiv.org/abs/2412.04565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04565">https://arxiv.org/pdf/2412.04565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04565]] Solving High-dimensional Inverse Problems Using Amortized Likelihood-free Inference with Noisy and Incomplete Data(https://arxiv.org/abs/2412.04565)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present a novel likelihood-free probabilistic inversion method based on normalizing flows for high-dimensional inverse problems. The proposed method is comprised of two complementary networks: a summary network for data compression, and an inference network for parameter estimation. The summary network encodes raw observations into a fixed-size vector of summary statistics, while the inference network generates samples of the approximate posterior distribution of the model parameters based on these summary statistics. The posterior samples are produced in a deep generative fashion by sampling from a latent Gaussian distribution and passing these samples through an invertible transformation. We construct this invertible transformation by sequentially alternating conditional invertible neural network (cINN) and conditional neural spline flow (cNSF) layers. The summary and inference networks are trained simultaneously. We apply the proposed method to an inversion problem in groundwater hydrology to estimate the posterior distribution of the system's log-conductivity field conditioned on spatially sparse time-series observations of the system's hydraulic head responses. The conductivity field is represented with 706 degrees of freedom in the considered problem. The comparison with the likelihood-based iterative ensemble smoother PEST-IES method demonstrates that the proposed method accurately estimates the parameter posterior distribution and the observations' predictive posterior distribution at a fraction of the inference time of PEST-IES.</li>
</ul>

<h3>Title: Give me Some Hard Questions: Synthetic Data Generation for Clinical QA</h3>
<ul>
<li><strong>Authors: </strong>Fan Bai, Keith Harrigian, Joel Stremmel, Hamid Hassanzadeh, Ardavan Saeedi, Mark Dredze</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04573">https://arxiv.org/abs/2412.04573</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04573">https://arxiv.org/pdf/2412.04573</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04573]] Give me Some Hard Questions: Synthetic Data Generation for Clinical QA(https://arxiv.org/abs/2412.04573)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Clinical Question Answering (QA) systems enable doctors to quickly access patient information from electronic health records (EHRs). However, training these systems requires significant annotated data, which is limited due to the expertise needed and the privacy concerns associated with clinical data. This paper explores generating Clinical QA data using large language models (LLMs) in a zero-shot setting. We find that naive prompting often results in easy questions that do not reflect the complexity of clinical scenarios. To address this, we propose two prompting strategies: 1) instructing the model to generate questions that do not overlap with the input context, and 2) summarizing the input record using a predefined schema to scaffold question generation. Experiments on two Clinical QA datasets demonstrate that our method generates more challenging questions, significantly improving fine-tuning performance over baselines. We compare synthetic and gold data and find a gap between their training efficacy resulting from the quality of synthetically generated answers.</li>
</ul>

<h3>Title: Show, Don't Tell: Uncovering Implicit Character Portrayal using LLMs</h3>
<ul>
<li><strong>Authors: </strong>Brandon Jaipersaud, Zining Zhu, Frank Rudzicz, Elliot Creager</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04576">https://arxiv.org/abs/2412.04576</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04576">https://arxiv.org/pdf/2412.04576</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04576]] Show, Don't Tell: Uncovering Implicit Character Portrayal using LLMs(https://arxiv.org/abs/2412.04576)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, large language model</a></li>
<li><strong>Abstract: </strong>Tools for analyzing character portrayal in fiction are valuable for writers and literary scholars in developing and interpreting compelling stories. Existing tools, such as visualization tools for analyzing fictional characters, primarily rely on explicit textual indicators of character attributes. However, portrayal is often implicit, revealed through actions and behaviors rather than explicit statements. We address this gap by leveraging large language models (LLMs) to uncover implicit character portrayals. We start by generating a dataset for this task with greater cross-topic similarity, lexical diversity, and narrative lengths than existing narrative text corpora such as TinyStories and WritingPrompts. We then introduce LIIPA (LLMs for Inferring Implicit Portrayal for Character Analysis), a framework for prompting LLMs to uncover character portrayals. LIIPA can be configured to use various types of intermediate computation (character attribute word lists, chain-of-thought) to infer how fictional characters are portrayed in the source text. We find that LIIPA outperforms existing approaches, and is more robust to increasing character counts (number of unique persons depicted) due to its ability to utilize full narrative context. Lastly, we investigate the sensitivity of portrayal estimates to character demographics, identifying a fairness-accuracy tradeoff among methods in our LIIPA framework -- a phenomenon familiar within the algorithmic fairness literature. Despite this tradeoff, all LIIPA variants consistently outperform non-LLM baselines in both fairness and accuracy. Our work demonstrates the potential benefits of using LLMs to analyze complex characters and to better understand how implicit portrayal biases may manifest in narrative texts.</li>
</ul>

<h3>Title: Loss Terms and Operator Forms of Koopman Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Dustin Enyeart, Guang Lin</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04578">https://arxiv.org/abs/2412.04578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04578">https://arxiv.org/pdf/2412.04578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04578]] Loss Terms and Operator Forms of Koopman Autoencoders(https://arxiv.org/abs/2412.04578)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Koopman autoencoders are a prevalent architecture in operator learning. But, the loss functions and the form of the operator vary significantly in the literature. This paper presents a fair and systemic study of these options. Furthermore, it introduces novel loss terms.</li>
</ul>

<h3>Title: ARTeFACT: Benchmarking Segmentation Models on Diverse Analogue Media Damage</h3>
<ul>
<li><strong>Authors: </strong>Daniela Ivanova, Marco Aversa, Paul Henderson, John Williamson</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04580">https://arxiv.org/abs/2412.04580</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04580">https://arxiv.org/pdf/2412.04580</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04580]] ARTeFACT: Benchmarking Segmentation Models on Diverse Analogue Media Damage(https://arxiv.org/abs/2412.04580)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Accurately detecting and classifying damage in analogue media such as paintings, photographs, textiles, mosaics, and frescoes is essential for cultural heritage preservation. While machine learning models excel in correcting degradation if the damage operator is known a priori, we show that they fail to robustly predict where the damage is even after supervised training; thus, reliable damage detection remains a challenge. Motivated by this, we introduce ARTeFACT, a dataset for damage detection in diverse types analogue media, with over 11,000 annotations covering 15 kinds of damage across various subjects, media, and historical provenance. Furthermore, we contribute human-verified text prompts describing the semantic contents of the images, and derive additional textual descriptions of the annotated damage. We evaluate CNN, Transformer, diffusion-based segmentation models, and foundation vision models in zero-shot, supervised, unsupervised and text-guided settings, revealing their limitations in generalising across media types. Our dataset is available at $\href{this https URL}{this https URL}$ as the first-of-its-kind benchmark for analogue media damage detection and restoration.</li>
</ul>

<h3>Title: Learning Symmetries via Weight-Sharing with Doubly Stochastic Tensors</h3>
<ul>
<li><strong>Authors: </strong>Putri A. van der Linden, Alejandro Garc√≠a-Castellanos, Sharvaree Vadgama, Thijs P. Kuipers, Erik J. Bekkers</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04594">https://arxiv.org/abs/2412.04594</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04594">https://arxiv.org/pdf/2412.04594</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04594]] Learning Symmetries via Weight-Sharing with Doubly Stochastic Tensors(https://arxiv.org/abs/2412.04594)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Group equivariance has emerged as a valuable inductive bias in deep learning, enhancing generalization, data efficiency, and robustness. Classically, group equivariant methods require the groups of interest to be known beforehand, which may not be realistic for real-world data. Additionally, baking in fixed group equivariance may impose overly restrictive constraints on model architecture. This highlights the need for methods that can dynamically discover and apply symmetries as soft constraints. For neural network architectures, equivariance is commonly achieved through group transformations of a canonical weight tensor, resulting in weight sharing over a given group $G$. In this work, we propose to learn such a weight-sharing scheme by defining a collection of learnable doubly stochastic matrices that act as soft permutation matrices on canonical weight tensors, which can take regular group representations as a special case. This yields learnable kernel transformations that are jointly optimized with downstream tasks. We show that when the dataset exhibits strong symmetries, the permutation matrices will converge to regular group representations and our weight-sharing networks effectively become regular group convolutions. Additionally, the flexibility of the method enables it to effectively pick up on partial symmetries.</li>
</ul>

<h3>Title: Evaluating an Effective Ransomware Infection Vector in Low Earth Orbit Satellites</h3>
<ul>
<li><strong>Authors: </strong>Marin Donchev, Dylan Smyth</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04601">https://arxiv.org/abs/2412.04601</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04601">https://arxiv.org/pdf/2412.04601</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04601]] Evaluating an Effective Ransomware Infection Vector in Low Earth Orbit Satellites(https://arxiv.org/abs/2412.04601)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Non-Terrestrial Networks (NTNs) and satellite systems have become an important component of modern data communication systems in recent years. Despite their importance, the security of these systems is often limited, leaving them vulnerable to determined attackers. In this paper, we outline a scenario in which an attacker can infect an in-orbit NASA Core Flight System (cFS) based satellite with ransomware and communicate the infection back to a satellite operator. This paper is the first to demonstrate an end-to-end exploit path that results in a ransomware infection without the need for a supply chain attack or compromised credentials. Novel ransomware is delivered to an emulated satellite system using custom shellcode that exploits a weakness in the considered scenario. The scenario considered by this initial piece of work achieves a success rate of 33.3\% for a complete successful infection.</li>
</ul>

<h3>Title: Assessing and Learning Alignment of Unimodal Vision and Language Models</h3>
<ul>
<li><strong>Authors: </strong>Le Zhang, Qian Yang, Aishwarya Agrawal</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04616">https://arxiv.org/abs/2412.04616</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04616">https://arxiv.org/pdf/2412.04616</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04616]] Assessing and Learning Alignment of Unimodal Vision and Language Models(https://arxiv.org/abs/2412.04616)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>How well are unimodal vision and language models aligned? Although prior work have approached answering this question, their assessment methods do not directly translate to how these models are used in practical vision-language tasks. In this paper, we propose a direct assessment method, inspired by linear probing, to assess vision-language alignment. We identify that the degree of alignment of the SSL vision models depends on their SSL training objective, and we find that the clustering quality of SSL representations has a stronger impact on alignment performance than their linear separability. Next, we introduce Swift Alignment of Image and Language (SAIL), a efficient transfer learning framework that aligns pretrained unimodal vision and language models for downstream vision-language tasks. Since SAIL leverages the strengths of pretrained unimodal models, it requires significantly fewer (6%) paired image-text data for the multimodal alignment compared to models like CLIP which are trained from scratch. SAIL training only requires a single A100 GPU, 5 hours of training and can accommodate a batch size up to 32,768. SAIL achieves 73.4% zero-shot accuracy on ImageNet (vs. CLIP's 72.7%) and excels in zero-shot retrieval, complex reasoning, and semantic segmentation. Additionally, SAIL improves the language-compatibility of vision encoders that in turn enhance the performance of multimodal large language models. The entire codebase and model weights are open-source: this https URL</li>
</ul>

<h3>Title: Using Diffusion Priors for Video Amodal Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Kaihua Chen, Deva Ramanan, Tarasha Khurana</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04623">https://arxiv.org/abs/2412.04623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04623">https://arxiv.org/pdf/2412.04623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04623]] Using Diffusion Priors for Video Amodal Segmentation(https://arxiv.org/abs/2412.04623)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, segmentation</a></li>
<li><strong>Abstract: </strong>Object permanence in humans is a fundamental cue that helps in understanding persistence of objects, even when they are fully occluded in the scene. Present day methods in object segmentation do not account for this amodal nature of the world, and only work for segmentation of visible or modal objects. Few amodal methods exist; single-image segmentation methods cannot handle high-levels of occlusions which are better inferred using temporal information, and multi-frame methods have focused solely on segmenting rigid objects. To this end, we propose to tackle video amodal segmentation by formulating it as a conditional generation task, capitalizing on the foundational knowledge in video generative models. Our method is simple; we repurpose these models to condition on a sequence of modal mask frames of an object along with contextual pseudo-depth maps, to learn which object boundary may be occluded and therefore, extended to hallucinate the complete extent of an object. This is followed by a content completion stage which is able to inpaint the occluded regions of an object. We benchmark our approach alongside a wide array of state-of-the-art methods on four datasets and show a dramatic improvement of upto 13% for amodal segmentation in an object's occluded region.</li>
</ul>

<h3>Title: SWEPO: Simultaneous Weighted Preference Optimization for Group Contrastive Alignment</h3>
<ul>
<li><strong>Authors: </strong>Taneesh Gupta, Rahul Madhavan, Xuchao Zhang, Chetan Bansal, Saravan Rajmohan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04628">https://arxiv.org/abs/2412.04628</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04628">https://arxiv.org/pdf/2412.04628</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04628]] SWEPO: Simultaneous Weighted Preference Optimization for Group Contrastive Alignment(https://arxiv.org/abs/2412.04628)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We introduce Simultaneous Weighted Preference Optimization (SWEPO), a novel extension of Direct Preference Optimization (DPO) designed to accommodate multiple dynamically chosen positive and negative responses for each query. SWEPO employs a weighted group contrastive loss, assigning weights to responses based on their deviation from the mean reward score. This approach effectively prioritizes responses that are significantly better or worse than the average, enhancing optimization. Our theoretical analysis demonstrates that simultaneously considering multiple preferences reduces alignment bias, resulting in more robust alignment. Additionally, we provide insights into the training dynamics of our loss function and a related function, InfoNCA. Empirical validation on the UltraFeedback dataset establishes SWEPO as state-of-the-art, with superior performance in downstream evaluations using the AlpacaEval dataset.</li>
</ul>

<h3>Title: Improving LLM Group Fairness on Tabular Data via In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Valeriia Cherepanova, Chia-Jung Lee, Nil-Jana Akpinar, Riccardo Fogliato, Martin Andres Bertran, Michael Kearns, James Zou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04642">https://arxiv.org/abs/2412.04642</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04642">https://arxiv.org/pdf/2412.04642</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04642]] Improving LLM Group Fairness on Tabular Data via In-Context Learning(https://arxiv.org/abs/2412.04642)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have been shown to be effective on tabular prediction tasks in the low-data regime, leveraging their internal knowledge and ability to learn from instructions and examples. However, LLMs can fail to generate predictions that satisfy group fairness, that is, produce equitable outcomes across groups. Critically, conventional debiasing approaches for natural language tasks do not directly translate to mitigating group unfairness in tabular settings. In this work, we systematically investigate four empirical approaches to improve group fairness of LLM predictions on tabular datasets, including fair prompt optimization, soft prompt tuning, strategic selection of few-shot examples, and self-refining predictions via chain-of-thought reasoning. Through experiments on four tabular datasets using both open-source and proprietary LLMs, we show the effectiveness of these methods in enhancing demographic parity while maintaining high overall performance. Our analysis provides actionable insights for practitioners in selecting the most suitable approach based on their specific requirements and constraints.</li>
</ul>

<h3>Title: One Communication Round is All It Needs for Federated Fine-Tuning Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Ziyao Wang, Bowei Tian, Yexiao He, Zheyu Shen, Luyang Liu, Ang Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04650">https://arxiv.org/abs/2412.04650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04650">https://arxiv.org/pdf/2412.04650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04650]] One Communication Round is All It Needs for Federated Fine-Tuning Foundation Models(https://arxiv.org/abs/2412.04650)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>The recent advancement of large foundation models (FMs) has increased the demand for fine-tuning these models on large-scale and cross-domain datasets. To address this, federated fine-tuning has emerged as a solution, allowing models to be fine-tuned on distributed datasets across multiple devices while ensuring data privacy. However, the substantial parameter size of FMs and the multi-round communication required by traditional federated fine-tuning algorithms result in prohibitively high communication costs, challenging the practicality of federated fine-tuning. In this paper, we are the first to reveal, both theoretically and empirically, that the traditional multi-round aggregation algorithms may not be necessary for federated fine-tuning large FMs. Our experiments reveal that a single round of communication (i.e., one-shot federated fine-tuning) yields a global model performance comparable to that achieved through multiple rounds of communication. Through rigorous mathematical and empirical analyses, we demonstrate that large FMs, due to their extensive parameter sizes and pre-training on general tasks, achieve significantly lower training loss in one-shot federated fine-tuning compared to smaller models. Our extensive experiments show that one-shot federated fine-tuning not only reduces communication costs but also enables asynchronous aggregation, enhances privacy, and maintains performance consistency with multi-round federated fine-tuning for models larger than 1 billion parameters, on text generation and text-to-image generation tasks. Our findings have the potential to revolutionize federated fine-tuning in practice, enhancing efficiency, reducing costs, and expanding accessibility for large-scale models. This breakthrough paves the way for broader adoption and application of federated fine-tuning across various domains.</li>
</ul>

<h3>Title: Cross-Self KV Cache Pruning for Efficient Vision-Language Inference</h3>
<ul>
<li><strong>Authors: </strong>Xiaohuan Pei, Tao Huang, Chang Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04652">https://arxiv.org/abs/2412.04652</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04652">https://arxiv.org/pdf/2412.04652</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04652]] Cross-Self KV Cache Pruning for Efficient Vision-Language Inference(https://arxiv.org/abs/2412.04652)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>KV cache pruning has emerged as a promising technique for reducing memory and computation costs in long-context auto-regressive generation. Existing methods for vision-language models (VLMs) typically rely on self-attention scores from large language models (LLMs) to identify and prune irrelevant tokens. However, these approaches overlook the inherent distributional discrepancies between modalities, often leading to inaccurate token importance estimation and the over-pruning of critical visual tokens. To address this, we propose decomposing attention scores into intra-modality attention (within the same modality) and inter-modality attention (across modalities), enabling more precise KV cache pruning by independently managing these distinct attention types. Additionally, we introduce an n-softmax function to counteract distribution shifts caused by pruning, preserving the original smoothness of attention scores and ensuring stable performance. Our final training-free method, \textbf{C}ross-\textbf{S}elf \textbf{P}runing (CSP), achieves competitive performance compared to models with full KV caches while significantly outperforming previous pruning methods. Extensive evaluations on MileBench, a benchmark encompassing 29 multimodal datasets, demonstrate CSP's effectiveness, achieving up to a 41\% performance improvement on challenging tasks like conversational embodied dialogue while reducing the KV cache budget by 13.6\%. The code is available at this https URL</li>
</ul>

<h3>Title: Hidden in the Noise: Two-Stage Robust Watermarking for Images</h3>
<ul>
<li><strong>Authors: </strong>Kasra Arabi, Benjamin Feuer, R. Teal Witter, Chinmay Hegde, Niv Cohen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04653">https://arxiv.org/abs/2412.04653</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04653">https://arxiv.org/pdf/2412.04653</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04653]] Hidden in the Noise: Two-Stage Robust Watermarking for Images(https://arxiv.org/abs/2412.04653)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, watermark, diffusion</a></li>
<li><strong>Abstract: </strong>As the quality of image generators continues to improve, deepfakes become a topic of considerable societal debate. Image watermarking allows responsible model owners to detect and label their AI-generated content, which can mitigate the harm. Yet, current state-of-the-art methods in image watermarking remain vulnerable to forgery and removal attacks. This vulnerability occurs in part because watermarks distort the distribution of generated images, unintentionally revealing information about the watermarking techniques. In this work, we first demonstrate a distortion-free watermarking method for images, based on a diffusion model's initial noise. However, detecting the watermark requires comparing the initial noise reconstructed for an image to all previously used initial noises. To mitigate these issues, we propose a two-stage watermarking framework for efficient detection. During generation, we augment the initial noise with generated Fourier patterns to embed information about the group of initial noises we used. For detection, we (i) retrieve the relevant group of noises, and (ii) search within the given group for an initial noise that might match our image. This watermarking approach achieves state-of-the-art robustness to forgery and removal against a large battery of attacks.</li>
</ul>

<h3>Title: Multiclass Post-Earthquake Building Assessment Integrating Optical and SAR Satellite Imagery, Ground Motion, and Soil Data with Transformers</h3>
<ul>
<li><strong>Authors: </strong>Deepank Singh, Vedhus Hoskere, Pietro Milillo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04664">https://arxiv.org/abs/2412.04664</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04664">https://arxiv.org/pdf/2412.04664</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04664]] Multiclass Post-Earthquake Building Assessment Integrating Optical and SAR Satellite Imagery, Ground Motion, and Soil Data with Transformers(https://arxiv.org/abs/2412.04664)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Timely and accurate assessments of building damage are crucial for effective response and recovery in the aftermath of earthquakes. Conventional preliminary damage assessments (PDA) often rely on manual door-to-door inspections, which are not only time-consuming but also pose significant safety risks. To safely expedite the PDA process, researchers have studied the applicability of satellite imagery processed with heuristic and machine learning approaches. These approaches output binary or, more recently, multiclass damage states at the scale of a block or a single building. However, the current performance of such approaches limits practical applicability. To address this limitation, we introduce a metadata-enriched, transformer based framework that combines high-resolution post-earthquake satellite imagery with building-specific metadata relevant to the seismic performance of the structure. Our model achieves state-of-the-art performance in multiclass post-earthquake damage identification for buildings from the Turkey-Syria earthquake on February 6, 2023. Specifically, we demonstrate that incorporating metadata, such as seismic intensity indicators, soil properties, and SAR damage proxy maps not only enhances the model's accuracy and ability to distinguish between damage classes, but also improves its generalizability across various regions. Furthermore, we conducted a detailed, class-wise analysis of feature importance to understand the model's decision-making across different levels of building damage. This analysis reveals how individual metadata features uniquely contribute to predictions for each damage class. By leveraging both satellite imagery and metadata, our proposed framework enables faster and more accurate damage assessments for precise, multiclass, building-level evaluations that can improve disaster response and accelerate recovery efforts for affected communities.</li>
</ul>

<h3>Title: LAA-Net: A Physical-prior-knowledge Based Network for Robust Nighttime Depth Estimation</h3>
<ul>
<li><strong>Authors: </strong>Kebin Peng, Haotang Li, Zhenyu Qi, Huashan Chen, Zi Wang, Wei Zhang, Sen He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04666">https://arxiv.org/abs/2412.04666</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04666">https://arxiv.org/pdf/2412.04666</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04666]] LAA-Net: A Physical-prior-knowledge Based Network for Robust Nighttime Depth Estimation(https://arxiv.org/abs/2412.04666)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Existing self-supervised monocular depth estimation (MDE) models attempt to improve nighttime performance by using GANs to transfer nighttime images into their daytime versions. However, this can introduce inconsistencies due to the complexities of real-world daytime lighting variations, which may finally lead to inaccurate estimation results. To address this issue, we leverage physical-prior-knowledge about light wavelength and light attenuation during nighttime. Specifically, our model, Light-Attenuation-Aware Network (LAA-Net), incorporates physical insights from Rayleigh scattering theory for robust nighttime depth estimation: LAA-Net is trained based on red channel values because red light preserves more information under nighttime scenarios due to its longer wavelength. Additionally, based on Beer-Lambert law, we introduce Red Channel Attenuation (RCA) loss to guide LAA-Net's training. Experiments on the RobotCar-Night, nuScenes-Night, RobotCar-Day, and KITTI datasets demonstrate that our model outperforms SOTA models.</li>
</ul>

<h3>Title: Diffusion-Augmented Coreset Expansion for Scalable Dataset Distillation</h3>
<ul>
<li><strong>Authors: </strong>Ali Abbasi, Shima Imani, Chenyang An, Gayathri Mahalingam, Harsh Shrivastava, Maurice Diesendruck, Hamed Pirsiavash, Pramod Sharma, Soheil Kolouri</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04668">https://arxiv.org/abs/2412.04668</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04668">https://arxiv.org/pdf/2412.04668</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04668]] Diffusion-Augmented Coreset Expansion for Scalable Dataset Distillation(https://arxiv.org/abs/2412.04668)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>With the rapid scaling of neural networks, data storage and communication demands have intensified. Dataset distillation has emerged as a promising solution, condensing information from extensive datasets into a compact set of synthetic samples by solving a bilevel optimization problem. However, current methods face challenges in computational efficiency, particularly with high-resolution data and complex architectures. Recently, knowledge-distillation-based dataset condensation approaches have made this process more computationally feasible. Yet, with the recent developments of generative foundation models, there is now an opportunity to achieve even greater compression, enhance the quality of distilled data, and introduce valuable diversity into the data representation. In this work, we propose a two-stage solution. First, we compress the dataset by selecting only the most informative patches to form a coreset. Next, we leverage a generative foundation model to dynamically expand this compressed set in real-time, enhancing the resolution of these patches and introducing controlled variability to the coreset. Our extensive experiments demonstrate the robustness and efficiency of our approach across a range of dataset distillation benchmarks. We demonstrate a significant improvement of over 10% compared to the state-of-the-art on several large-scale dataset distillation benchmarks. The code will be released soon.</li>
</ul>

<h3>Title: Zephyr quantum-assisted hierarchical Calo4pQVAE for particle-calorimeter interactions</h3>
<ul>
<li><strong>Authors: </strong>Ian Lu, Hao Jia, Sebastian Gonzalez, Deniz Sogutlu, J. Quetzalcoatl Toledo-Marin, Sehmimul Hoque, Abhishek Abhishek, Colin Gay, Roger Melko, Eric Paquet, Geoffrey Fox, Maximilian Swiatlowski, Wojciech Fedorko</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, hep-ph, physics.comp-ph, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04677">https://arxiv.org/abs/2412.04677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04677">https://arxiv.org/pdf/2412.04677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04677]] Zephyr quantum-assisted hierarchical Calo4pQVAE for particle-calorimeter interactions(https://arxiv.org/abs/2412.04677)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>With the approach of the High Luminosity Large Hadron Collider (HL-LHC) era set to begin particle collisions by the end of this decade, it is evident that the computational demands of traditional collision simulation methods are becoming increasingly unsustainable. Existing approaches, which rely heavily on first-principles Monte Carlo simulations for modeling event showers in calorimeters, are projected to require millions of CPU-years annually -- far exceeding current computational capacities. This bottleneck presents an exciting opportunity for advancements in computational physics by integrating deep generative models with quantum simulations. We propose a quantum-assisted hierarchical deep generative surrogate founded on a variational autoencoder (VAE) in combination with an energy conditioned restricted Boltzmann machine (RBM) embedded in the model's latent space as a prior. By mapping the topology of D-Wave's Zephyr quantum annealer (QA) into the nodes and couplings of a 4-partite RBM, we leverage quantum simulation to accelerate our shower generation times significantly. To evaluate our framework, we use Dataset 2 of the CaloChallenge 2022. Through the integration of classical computation and quantum simulation, this hybrid framework paves way for utilizing large-scale quantum simulations as priors in deep generative models.</li>
</ul>

<h3>Title: Unsupervised Segmentation by Diffusing, Walking and Cutting</h3>
<ul>
<li><strong>Authors: </strong>Daniela Ivanova, Marco Aversa, Paul Henderson, John Williamson</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04678">https://arxiv.org/abs/2412.04678</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04678">https://arxiv.org/pdf/2412.04678</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04678]] Unsupervised Segmentation by Diffusing, Walking and Cutting(https://arxiv.org/abs/2412.04678)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>We propose an unsupervised image segmentation method using features from pre-trained text-to-image diffusion models. Inspired by classic spectral clustering approaches, we construct adjacency matrices from self-attention layers between image patches and recursively partition using Normalised Cuts. A key insight is that self-attention probability distributions, which capture semantic relations between patches, can be interpreted as a transition matrix for random walks across the image. We leverage this by first using Random Walk Normalized Cuts directly on these self-attention activations to partition the image, minimizing transition probabilities between clusters while maximizing coherence within clusters. Applied recursively, this yields a hierarchical segmentation that reflects the rich semantics in the pre-trained attention layers, without any additional training. Next, we explore other ways to build the NCuts adjacency matrix from features, and how we can use the random walk interpretation of self-attention to capture long-range relationships. Finally, we propose an approach to automatically determine the NCut cost criterion, avoiding the need to tune this manually. We quantitatively analyse the effect incorporating different features, a constant versus dynamic NCut threshold, and incorporating multi-node paths when constructing the NCuts adjacency matrix. We show that our approach surpasses all existing methods for zero-shot unsupervised segmentation, achieving state-of-the-art results on COCO-Stuff-27 and Cityscapes.</li>
</ul>

<h3>Title: Superpixel Tokenization for Vision Transformers: Preserving Semantic Integrity in Visual Tokens</h3>
<ul>
<li><strong>Authors: </strong>Jaihyun Lew, Soohyuk Jang, Jaehoon Lee, Seungryong Yoo, Eunji Kim, Saehyung Lee, Jisoo Mok, Siwon Kim, Sungroh Yoon</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04680">https://arxiv.org/abs/2412.04680</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04680">https://arxiv.org/pdf/2412.04680</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04680]] Superpixel Tokenization for Vision Transformers: Preserving Semantic Integrity in Visual Tokens(https://arxiv.org/abs/2412.04680)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, transformer</a></li>
<li><strong>Abstract: </strong>Transformers, a groundbreaking architecture proposed for Natural Language Processing (NLP), have also achieved remarkable success in Computer Vision. A cornerstone of their success lies in the attention mechanism, which models relationships among tokens. While the tokenization process in NLP inherently ensures that a single token does not contain multiple semantics, the tokenization of Vision Transformer (ViT) utilizes tokens from uniformly partitioned square image patches, which may result in an arbitrary mixing of visual concepts in a token. In this work, we propose to substitute the grid-based tokenization in ViT with superpixel tokenization, which employs superpixels to generate a token that encapsulates a sole visual concept. Unfortunately, the diverse shapes, sizes, and locations of superpixels make integrating superpixels into ViT tokenization rather challenging. Our tokenization pipeline, comprised of pre-aggregate extraction and superpixel-aware aggregation, overcomes the challenges that arise in superpixel tokenization. Extensive experiments demonstrate that our approach, which exhibits strong compatibility with existing frameworks, enhances the accuracy and robustness of ViT on various downstream tasks.</li>
</ul>

<h3>Title: LLM-Align: Utilizing Large Language Models for Entity Alignment in Knowledge Graphs</h3>
<ul>
<li><strong>Authors: </strong>Xuan Chen, Tong Lu, Zhichun Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04690">https://arxiv.org/abs/2412.04690</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04690">https://arxiv.org/pdf/2412.04690</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04690]] LLM-Align: Utilizing Large Language Models for Entity Alignment in Knowledge Graphs(https://arxiv.org/abs/2412.04690)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Entity Alignment (EA) seeks to identify and match corresponding entities across different Knowledge Graphs (KGs), playing a crucial role in knowledge fusion and integration. Embedding-based entity alignment (EA) has recently gained considerable attention, resulting in the emergence of many innovative approaches. Initially, these approaches concentrated on learning entity embeddings based on the structural features of knowledge graphs (KGs) as defined by relation triples. Subsequent methods have integrated entities' names and attributes as supplementary information to improve the embeddings used for EA. However, existing methods lack a deep semantic understanding of entity attributes and relations. In this paper, we propose a Large Language Model (LLM) based Entity Alignment method, LLM-Align, which explores the instruction-following and zero-shot capabilities of Large Language Models to infer alignments of entities. LLM-Align uses heuristic methods to select important attributes and relations of entities, and then feeds the selected triples of entities to an LLM to infer the alignment results. To guarantee the quality of alignment results, we design a multi-round voting mechanism to mitigate the hallucination and positional bias issues that occur with LLMs. Experiments on three EA datasets, demonstrating that our approach achieves state-of-the-art performance compared to existing EA methods.</li>
</ul>

<h3>Title: Privacy-Preserving Retrieval Augmented Generation with Differential Privacy</h3>
<ul>
<li><strong>Authors: </strong>Tatsuki Koga, Ruihan Wu, Kamalika Chaudhuri</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04697">https://arxiv.org/abs/2412.04697</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04697">https://arxiv.org/pdf/2412.04697</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04697]] Privacy-Preserving Retrieval Augmented Generation with Differential Privacy(https://arxiv.org/abs/2412.04697)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>With the recent remarkable advancement of large language models (LLMs), there has been a growing interest in utilizing them in the domains with highly sensitive data that lies outside their training data. For this purpose, retrieval augmented generation (RAG) is particularly effective -- it assists LLMs by directly providing relevant information from the external knowledge sources. However, without extra privacy safeguards, RAG outputs risk leaking sensitive information from the external data source. In this work, we explore RAG under differential privacy (DP), a formal guarantee of data privacy. The main challenge with differentially private RAG is how to generate long accurate answers within a moderate privacy budget. We address this by proposing an algorithm that smartly spends privacy budget only for the tokens that require the sensitive information and uses the non-private LLM for other tokens. Our extensive empirical evaluations reveal that our algorithm outperforms the non-RAG baseline under a reasonable privacy budget of $\epsilon\approx 10$ across different models and datasets.</li>
</ul>

<h3>Title: Transformers Struggle to Learn to Search</h3>
<ul>
<li><strong>Authors: </strong>Abulhair Saparov, Srushti Pawar, Shreyas Pimpalgaonkar, Nitish Joshi, Richard Yuanzhe Pang, Vishakh Padmakumar, Seyed Mehran Kazemi, Najoung Kim, He He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04703">https://arxiv.org/abs/2412.04703</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04703">https://arxiv.org/pdf/2412.04703</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04703]] Transformers Struggle to Learn to Search(https://arxiv.org/abs/2412.04703)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Search is an ability foundational in many important tasks, and recent studies have shown that large language models (LLMs) struggle to perform search robustly. It is unknown whether this inability is due to a lack of data, insufficient model parameters, or fundamental limitations of the transformer architecture. In this work, we use the foundational graph connectivity problem as a testbed to generate effectively limitless high-coverage data to train small transformers and test whether they can learn to perform search. We find that, when given the right training distribution, the transformer is able to learn to search. We analyze the algorithm that the transformer has learned through a novel mechanistic interpretability technique that enables us to extract the computation graph from the trained model. We find that for each vertex in the input graph, transformers compute the set of vertices reachable from that vertex. Each layer then progressively expands these sets, allowing the model to search over a number of vertices exponential in the number of layers. However, we find that as the input graph size increases, the transformer has greater difficulty in learning the task. This difficulty is not resolved even as the number of parameters is increased, suggesting that increasing model scale will not lead to robust search abilities. We also find that performing search in-context (i.e., chain-of-thought) does not resolve this inability to learn to search on larger graphs.</li>
</ul>

<h3>Title: PCTreeS: 3D Point Cloud Tree Species Classification Using Airborne LiDAR Images</h3>
<ul>
<li><strong>Authors: </strong>Hongjin Lin, Matthew Nazari, Derek Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04714">https://arxiv.org/abs/2412.04714</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04714">https://arxiv.org/pdf/2412.04714</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04714]] PCTreeS: 3D Point Cloud Tree Species Classification Using Airborne LiDAR Images(https://arxiv.org/abs/2412.04714)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Reliable large-scale data on the state of forests is crucial for monitoring ecosystem health, carbon stock, and the impact of climate change. Current knowledge of tree species distribution relies heavily on manual data collection in the field, which often takes years to complete, resulting in limited datasets that cover only a small subset of the world's forests. Recent works show that state-of-the-art deep learning models using Light Detection and Ranging (LiDAR) images enable accurate and scalable classification of tree species in various ecosystems. While LiDAR images contain rich 3D information, most previous works flatten the 3D images into 2D projections to use Convolutional Neural Networks (CNNs). This paper offers three significant contributions: (1) we apply the deep learning framework for tree classification in tropical savannas; (2) we use Airborne LiDAR images, which have a lower resolution but greater scalability than Terrestrial LiDAR images used in most previous works; (3) we introduce the approach of directly feeding 3D point cloud images into a vision transformer model (PCTreeS). Our results show that the PCTreeS approach outperforms current CNN baselines with 2D projections in AUC (0.81), overall accuracy (0.72), and training time (~45 mins). This paper also motivates further LiDAR image collection and validation for accurate large-scale automatic classification of tree species.</li>
</ul>

<h3>Title: Addressing Attribute Leakages in Diffusion-based Image Editing without Training</h3>
<ul>
<li><strong>Authors: </strong>Sunung Mun, Jinhwan Nam, Sunghyun Cho, Jungseul Ok</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04715">https://arxiv.org/abs/2412.04715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04715">https://arxiv.org/pdf/2412.04715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04715]] Addressing Attribute Leakages in Diffusion-based Image Editing without Training(https://arxiv.org/abs/2412.04715)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have become a cornerstone in image editing, offering flexibility with language prompts and source images. However, a key challenge is attribute leakage, where unintended modifications occur in non-target regions or within target regions due to attribute interference. Existing methods often suffer from leakage due to naive text embeddings and inadequate handling of End-of-Sequence (EOS) token embeddings. We propose a novel framework to address attribute leakage with three components: (1) Object-Restricted Embeddings (ORE) to localize object-specific attributes in text embeddings, (2) Region-Guided Blending for Cross-Attention Masking (RGB-CAM) to align attention with target regions, and (3) Background Blending (BB) to preserve non-edited regions. Additionally, we introduce ALE-Bench, a benchmark for evaluating attribute leakage with new metrics for target-external and target-internal leakage. Experiments demonstrate that our framework significantly reduces attribute leakage while maintaining high editing quality, providing an efficient and tuning-free solution for multi-object image editing.</li>
</ul>

<h3>Title: BESSTIE: A Benchmark for Sentiment and Sarcasm Classification for Varieties of English</h3>
<ul>
<li><strong>Authors: </strong>Dipankar Srirag, Aditya Joshi, Jordan Painter, Diptesh Kanojia</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04726">https://arxiv.org/abs/2412.04726</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04726">https://arxiv.org/pdf/2412.04726</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04726]] BESSTIE: A Benchmark for Sentiment and Sarcasm Classification for Varieties of English(https://arxiv.org/abs/2412.04726)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite large language models (LLMs) being known to exhibit bias against non-mainstream varieties, there are no known labeled datasets for sentiment analysis of English. To address this gap, we introduce BESSTIE, a benchmark for sentiment and sarcasm classification for three varieties of English: Australian (en-AU), Indian (en-IN), and British (en-UK). Using web-based content from two domains, namely, Google Place reviews and Reddit comments, we collect datasets for these language varieties using two methods: location-based and topic-based filtering. Native speakers of the language varieties manually annotate the datasets with sentiment and sarcasm labels. Subsequently, we fine-tune nine large language models (LLMs) (representing a range of encoder/decoder and mono/multilingual models) on these datasets, and evaluate their performance on the two tasks. Our results reveal that the models consistently perform better on inner-circle varieties (i.e., en-AU and en-UK), with significant performance drops for en-IN, particularly in sarcasm detection. We also report challenges in cross-variety generalisation, highlighting the need for language variety-specific datasets such as ours. BESSTIE promises to be a useful evaluative benchmark for future research in equitable LLMs, specifically in terms of language varieties. The BESSTIE datasets, code, and models are currently available on request, while the paper is under review. Please email this http URL@unsw.this http URL.</li>
</ul>

<h3>Title: Espresso: High Compression For Rich Extraction From Videos for Your Vision-Language Model</h3>
<ul>
<li><strong>Authors: </strong>Keunwoo Peter Yu, Achal Dave, Rares Ambrus, Jean Mercat</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04729">https://arxiv.org/abs/2412.04729</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04729">https://arxiv.org/pdf/2412.04729</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04729]] Espresso: High Compression For Rich Extraction From Videos for Your Vision-Language Model(https://arxiv.org/abs/2412.04729)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Most of the current vision-language models (VLMs) for videos struggle to understand videos longer than a few seconds. This is primarily due to the fact that they do not scale to utilizing a large number of frames. In order to address this limitation, we propose Espresso, a novel method that extracts and compresses spatial and temporal information separately. Through extensive evaluations, we show that spatial and temporal compression in Espresso each have a positive impact on the long-form video understanding capabilities; when combined, their positive impact increases. Furthermore, we show that Espresso's performance scales well with more training data, and that Espresso is far more effective than the existing projectors for VLMs in long-form video understanding. Moreover, we devise a more difficult evaluation setting for EgoSchema called "needle-in-a-haystack" that multiplies the lengths of the input videos. Espresso achieves SOTA performance on this task, outperforming the SOTA VLMs that have been trained on much more training data.</li>
</ul>

<h3>Title: Generative Humanization for Therapeutic Antibodies</h3>
<ul>
<li><strong>Authors: </strong>Cade Gordon, Aniruddh Raghu, Hunter Elliott, Peyton Greenside</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04737">https://arxiv.org/abs/2412.04737</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04737">https://arxiv.org/pdf/2412.04737</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04737]] Generative Humanization for Therapeutic Antibodies(https://arxiv.org/abs/2412.04737)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Antibody therapies have been employed to address some of today's most challenging diseases, but must meet many criteria during drug development before reaching a patient. Humanization is a sequence optimization strategy that addresses one critical risk called immunogenicity - a patient's immune response to the drug - by making an antibody more "human-like" in the absence of a predictive lab-based test for immunogenicity. However, existing humanization strategies generally yield very few humanized candidates, which may have degraded biophysical properties or decreased drug efficacy. Here, we re-frame humanization as a conditional generative modeling task, where humanizing mutations are sampled from a language model trained on human antibody data. We describe a sampling process that incorporates models of therapeutic attributes, such as antigen binding affinity, to obtain candidate sequences that have both reduced immunogenicity risk and maintained or improved therapeutic properties, allowing this algorithm to be readily embedded into an iterative antibody optimization campaign. We demonstrate in silico and in lab validation that in real therapeutic programs our generative humanization method produces diverse sets of antibodies that are both (1) highly-human and (2) have favorable therapeutic properties, such as improved binding to target antigens.</li>
</ul>

<h3>Title: DHIL-GT: Scalable Graph Transformer with Decoupled Hierarchy Labeling</h3>
<ul>
<li><strong>Authors: </strong>Ningyi Liao, Zihao Yu, Siqiang Luo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04738">https://arxiv.org/abs/2412.04738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04738">https://arxiv.org/pdf/2412.04738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04738]] DHIL-GT: Scalable Graph Transformer with Decoupled Hierarchy Labeling(https://arxiv.org/abs/2412.04738)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Graph Transformer (GT) has recently emerged as a promising neural network architecture for learning graph-structured data. However, its global attention mechanism with quadratic complexity concerning the graph scale prevents wider application to large graphs. While current methods attempt to enhance GT scalability by altering model architecture or encoding hierarchical graph data, our analysis reveals that these models still suffer from the computational bottleneck related to graph-scale operations. In this work, we target the GT scalability issue and propose DHIL-GT, a scalable Graph Transformer that simplifies network learning by fully decoupling the graph computation to a separate stage in advance. DHIL-GT effectively retrieves hierarchical information by exploiting the graph labeling technique, as we show that the graph label hierarchy is more informative than plain adjacency by offering global connections while promoting locality, and is particularly suitable for handling complex graph patterns such as heterophily. We further design subgraph sampling and positional encoding schemes for precomputing model input on top of graph labels in an end-to-end manner. The training stage thus favorably removes graph-related computations, leading to ideal mini-batch capability and GPU utilization. Notably, the precomputation and training processes of DHIL-GT achieve complexities linear to the number of graph edges and nodes, respectively. Extensive experiments demonstrate that DHIL-GT is efficient in terms of computational boost and mini-batch capability over existing scalable Graph Transformer designs on large-scale benchmarks, while achieving top-tier effectiveness on both homophilous and heterophilous graphs.</li>
</ul>

<h3>Title: Fair Diagnosis: Leveraging Causal Modeling to Mitigate Medical Bias</h3>
<ul>
<li><strong>Authors: </strong>Bowei Tian, Yexiao He, Meng Liu, Yucong Dai, Ziyao Wang, Shwai He, Guoheng Sun, Zheyu Shen, Wanghao Ye, Yongkai Wu, Ang Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04739">https://arxiv.org/abs/2412.04739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04739">https://arxiv.org/pdf/2412.04739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04739]] Fair Diagnosis: Leveraging Causal Modeling to Mitigate Medical Bias(https://arxiv.org/abs/2412.04739)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, interpretability</a></li>
<li><strong>Abstract: </strong>In medical image analysis, model predictions can be affected by sensitive attributes, such as race and gender, leading to fairness concerns and potential biases in diagnostic outcomes. To mitigate this, we present a causal modeling framework, which aims to reduce the impact of sensitive attributes on diagnostic predictions. Our approach introduces a novel fairness criterion, \textbf{Diagnosis Fairness}, and a unique fairness metric, leveraging path-specific fairness to control the influence of demographic attributes, ensuring that predictions are primarily informed by clinically relevant features rather than sensitive attributes. By incorporating adversarial perturbation masks, our framework directs the model to focus on critical image regions, suppressing bias-inducing information. Experimental results across multiple datasets demonstrate that our framework effectively reduces bias directly associated with sensitive attributes while preserving diagnostic accuracy. Our findings suggest that causal modeling can enhance both fairness and interpretability in AI-powered clinical decision support systems.</li>
</ul>

<h3>Title: Latent Space Characterization of Autoencoder Variants</h3>
<ul>
<li><strong>Authors: </strong>Anika Shrivastava, Renu Rameshan, Samar Agnihotri</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04755">https://arxiv.org/abs/2412.04755</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04755">https://arxiv.org/pdf/2412.04755</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04755]] Latent Space Characterization of Autoencoder Variants(https://arxiv.org/abs/2412.04755)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Understanding the latent spaces learned by deep learning models is crucial in exploring how they represent and generate complex data. Autoencoders (AEs) have played a key role in the area of representation learning, with numerous regularization techniques and training principles developed not only to enhance their ability to learn compact and robust representations, but also to reveal how different architectures influence the structure and smoothness of the lower-dimensional non-linear manifold. We strive to characterize the structure of the latent spaces learned by different autoencoders including convolutional autoencoders (CAEs), denoising autoencoders (DAEs), and variational autoencoders (VAEs) and how they change with the perturbations in the input. By characterizing the matrix manifolds corresponding to the latent spaces, we provide an explanation for the well-known observation that the latent spaces of CAE and DAE form non-smooth manifolds, while that of VAE forms a smooth manifold. We also map the points of the matrix manifold to a Hilbert space using distance preserving transforms and provide an alternate view in terms of the subspaces generated in the Hilbert space as a function of the distortion in the input. The results show that the latent manifolds of CAE and DAE are stratified with each stratum being a smooth product manifold, while the manifold of VAE is a smooth product manifold of two symmetric positive definite matrices and a symmetric positive semi-definite matrix.</li>
</ul>

<h3>Title: ChatNVD: Advancing Cybersecurity Vulnerability Assessment with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shivansh Chopra, Hussain Ahmad, Diksha Goel, Claudia Szabo</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04756">https://arxiv.org/abs/2412.04756</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04756">https://arxiv.org/pdf/2412.04756</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04756]] ChatNVD: Advancing Cybersecurity Vulnerability Assessment with Large Language Models(https://arxiv.org/abs/2412.04756)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>The increasing frequency and sophistication of cybersecurity vulnerabilities in software systems underscore the urgent need for robust and effective methods of vulnerability assessment. However, existing approaches often rely on highly technical and abstract frameworks, which hinders understanding and increases the likelihood of exploitation, resulting in severe cyberattacks. Given the growing adoption of Large Language Models (LLMs) across diverse domains, this paper explores their potential application in cybersecurity, specifically for enhancing the assessment of software vulnerabilities. We propose ChatNVD, an LLM-based cybersecurity vulnerability assessment tool leveraging the National Vulnerability Database (NVD) to provide context-rich insights and streamline vulnerability analysis for cybersecurity professionals, developers, and non-technical users. We develop three variants of ChatNVD, utilizing three prominent LLMs: GPT-4o mini by OpenAI, Llama 3 by Meta, and Gemini 1.5 Pro by Google. To evaluate their efficacy, we conduct a comparative analysis of these models using a comprehensive questionnaire comprising common security vulnerability questions, assessing their accuracy in identifying and analyzing software vulnerabilities. This study provides valuable insights into the potential of LLMs to address critical challenges in understanding and mitigation of software vulnerabilities.</li>
</ul>

<h3>Title: Ltri-LLM: Streaming Long Context Inference for LLMs with Training-Free Dynamic Triangular Attention Pattern</h3>
<ul>
<li><strong>Authors: </strong>Hongyin Tang, Di Xiu, Lanrui Wang, Xiurui Geng, Jingang Wang, Xunliang Cai</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04757">https://arxiv.org/abs/2412.04757</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04757">https://arxiv.org/pdf/2412.04757</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04757]] Ltri-LLM: Streaming Long Context Inference for LLMs with Training-Free Dynamic Triangular Attention Pattern(https://arxiv.org/abs/2412.04757)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The quadratic computational complexity of the attention mechanism in current Large Language Models (LLMs) renders inference with long contexts prohibitively expensive. To address this challenge, various approaches aim to retain critical portions of the context to optimally approximate Full Attention (FA) through Key-Value (KV) compression or Sparse Attention (SA), enabling the processing of virtually unlimited text lengths in a streaming manner. However, these methods struggle to achieve performance levels comparable to FA, particularly in retrieval tasks. In this paper, our analysis of attention head patterns reveals that LLMs' attention distributions show strong local correlations, naturally reflecting a chunking mechanism for input context. We propose Ltri-LLM framework, which divides KVs into spans, stores them in an offline index, and retrieves the relevant KVs into memory for various queries. Experimental results on popular long text benchmarks show that Ltri-LLM can achieve performance close to FA while maintaining efficient, streaming-based inference.</li>
</ul>

<h3>Title: Towards counterfactual fairness thorough auxiliary variables</h3>
<ul>
<li><strong>Authors: </strong>Bowei Tian, Ziyao Wang, Shwai He, Wanghao Ye, Guoheng Sun, Yucong Dai, Yongkai Wu, Ang Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DS, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04767">https://arxiv.org/abs/2412.04767</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04767">https://arxiv.org/pdf/2412.04767</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04767]] Towards counterfactual fairness thorough auxiliary variables(https://arxiv.org/abs/2412.04767)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>The challenge of balancing fairness and predictive accuracy in machine learning models, especially when sensitive attributes such as race, gender, or age are considered, has motivated substantial research in recent years. Counterfactual fairness ensures that predictions remain consistent across counterfactual variations of sensitive attributes, which is a crucial concept in addressing societal biases. However, existing counterfactual fairness approaches usually overlook intrinsic information about sensitive features, limiting their ability to achieve fairness while simultaneously maintaining performance. To tackle this challenge, we introduce EXOgenous Causal reasoning (EXOC), a novel causal reasoning framework motivated by exogenous variables. It leverages auxiliary variables to uncover intrinsic properties that give rise to sensitive attributes. Our framework explicitly defines an auxiliary node and a control node that contribute to counterfactual fairness and control the information flow within the model. Our evaluation, conducted on synthetic and real-world datasets, validates EXOC's superiority, showing that it outperforms state-of-the-art approaches in achieving counterfactual fairness.</li>
</ul>

<h3>Title: Foundation Models for Low-Resource Language Education (Vision Paper)</h3>
<ul>
<li><strong>Authors: </strong>Zhaojun Ding, Zhengliang Liu, Hanqi Jiang, Yizhu Gao, Xiaoming Zhai, Tianming Liu, Ninghao Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04774">https://arxiv.org/abs/2412.04774</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04774">https://arxiv.org/pdf/2412.04774</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04774]] Foundation Models for Low-Resource Language Education (Vision Paper)(https://arxiv.org/abs/2412.04774)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent studies show that large language models (LLMs) are powerful tools for working with natural language, bringing advances in many areas of computational linguistics. However, these models face challenges when applied to low-resource languages due to limited training data and difficulty in understanding cultural nuances. Research is now focusing on multilingual models to improve LLM performance for these languages. Education in these languages also struggles with a lack of resources and qualified teachers, particularly in underdeveloped regions. Here, LLMs can be transformative, supporting innovative methods like community-driven learning and digital platforms. This paper discusses how LLMs could enhance education for low-resource languages, emphasizing practical applications and benefits.</li>
</ul>

<h3>Title: A Temporally Correlated Latent Exploration for Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>SuMin Oh, WanSoo Kim, HyunJin Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04775">https://arxiv.org/abs/2412.04775</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04775">https://arxiv.org/pdf/2412.04775</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04775]] A Temporally Correlated Latent Exploration for Reinforcement Learning(https://arxiv.org/abs/2412.04775)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Efficient exploration remains one of the longstanding problems of deep reinforcement learning. Instead of depending solely on extrinsic rewards from the environments, existing methods use intrinsic rewards to enhance exploration. However, we demonstrate that these methods are vulnerable to Noisy TV and stochasticity. To tackle this problem, we propose Temporally Correlated Latent Exploration (TeCLE), which is a novel intrinsic reward formulation that employs an action-conditioned latent space and temporal correlation. The action-conditioned latent space estimates the probability distribution of states, thereby avoiding the assignment of excessive intrinsic rewards to unpredictable states and effectively addressing both problems. Whereas previous works inject temporal correlation for action selection, the proposed method injects it for intrinsic reward computation. We find that the injected temporal correlation determines the exploratory behaviors of agents. Various experiments show that the environment where the agent performs well depends on the amount of temporal correlation. To the best of our knowledge, the proposed TeCLE is the first approach to consider the action conditioned latent space and temporal correlation for curiosity-driven exploration. We prove that the proposed TeCLE can be robust to the Noisy TV and stochasticity in benchmark environments, including Minigrid and Stochastic Atari.</li>
</ul>

<h3>Title: Megatron: Evasive Clean-Label Backdoor Attacks against Vision Transformer</h3>
<ul>
<li><strong>Authors: </strong>Xueluan Gong, Bowei Tian, Meng Xue, Shuike Li, Yanjiao Chen, Qian Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04776">https://arxiv.org/abs/2412.04776</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04776">https://arxiv.org/pdf/2412.04776</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04776]] Megatron: Evasive Clean-Label Backdoor Attacks against Vision Transformer(https://arxiv.org/abs/2412.04776)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Vision transformers have achieved impressive performance in various vision-related tasks, but their vulnerability to backdoor attacks is under-explored. A handful of existing works focus on dirty-label attacks with wrongly-labeled poisoned training samples, which may fail if a benign model trainer corrects the labels. In this paper, we propose Megatron, an evasive clean-label backdoor attack against vision transformers, where the attacker injects the backdoor without manipulating the data-labeling process. To generate an effective trigger, we customize two loss terms based on the attention mechanism used in transformer networks, i.e., latent loss and attention diffusion loss. The latent loss aligns the last attention layer between triggered samples and clean samples of the target label. The attention diffusion loss emphasizes the attention diffusion area that encompasses the trigger. A theoretical analysis is provided to underpin the rationale behind the attention diffusion loss. Extensive experiments on CIFAR-10, GTSRB, CIFAR-100, and Tiny ImageNet demonstrate the effectiveness of Megatron. Megatron can achieve attack success rates of over 90% even when the position of the trigger is slightly shifted during testing. Furthermore, Megatron achieves better evasiveness than baselines regarding both human visual inspection and defense strategies (i.e., DBAVT, BAVT, Beatrix, TeCo, and SAGE).</li>
</ul>

<h3>Title: IterNorm: Fast Iterative Normalization</h3>
<ul>
<li><strong>Authors: </strong>ChangMin Ye, Yonguk Sim, Youngchae Kim, SeongMin Jin, Doo Seok Jeong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04778">https://arxiv.org/abs/2412.04778</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04778">https://arxiv.org/pdf/2412.04778</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04778]] IterNorm: Fast Iterative Normalization(https://arxiv.org/abs/2412.04778)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Transformer-based large language models are a memory-bound model whose operation is based on a large amount of data that are marginally reused. Thus, the data movement between a host and accelerator likely dictates the total wall-clock time. Layer normalization is one of the key workloads in the transformer model, following each of multi-head attention and feed-forward network blocks. To reduce data movement, layer normalization needs to be performed on the same chip as the matrix-matrix multiplication engine. To this end, we introduce an iterative L2-normalization method for 1D input (IterNorm), ensuring fast convergence to the steady-state solution within five iteration steps and high precision, outperforming the fast inverse square root algorithm in six out of nine cases for FP32 and five out of nine for BFloat16 across the embedding lengths used in the OPT models. Implemented in 32/28nm CMOS, the IterNorm macro normalizes $d$-dimensional vectors, where $64 \leq d \leq 1024$, with a latency of 112-227 cycles at 100MHz/1.05V.</li>
</ul>

<h3>Title: DPGIIL: Dirichlet Process-Deep Generative Model-Integrated Incremental Learning for Clustering in Transmissibility-based Online Structural Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Lin-Feng Mei, Wang-Ji Yan</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.data-an, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04781">https://arxiv.org/abs/2412.04781</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04781">https://arxiv.org/pdf/2412.04781</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04781]] DPGIIL: Dirichlet Process-Deep Generative Model-Integrated Incremental Learning for Clustering in Transmissibility-based Online Structural Anomaly Detection(https://arxiv.org/abs/2412.04781)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, generative</a></li>
<li><strong>Abstract: </strong>Clustering based on vibration responses, such as transmissibility functions (TFs), is promising in structural anomaly detection, but most existing approaches struggle with determining the optimal cluster number and handling high-dimensional streaming data, while their shallow structures also make them sensitive to manually-engineered feature quality. To bridge this gap, this work proposes the Dirichlet process-deep generative model-integrated incremental learning (DPGIIL) for clustering by combining the advantages of deep generative models (DGMs) in representation learning and the Dirichlet process mixture model (DPMM) in identifying distinct patterns in observed data. By introducing a DPMM prior into the latent space of DGMs, DPGIIL automatically captures dissimilarities in extracted latent representations, enabling both generative modeling and clustering. Within the context of variational Bayesian inference, a lower bound on the log marginal likelihood of DPGIIL, tighter than the evidence lower bound given sufficient training data, is derived analytically, which enables the joint optimization of DGM and DPMM parameters, thereby allowing the DPMM to regularize the DGM's feature extraction process. Additionally, a greedy split-merge scheme-based coordinate ascent variational inference method is devised to accelerate the optimization. The summary statistics of the DPMM, along with the network parameters, are used to retain information about previous data for incremental learning. Notably, this study uses variational autoencoder (VAE) within DPGIIL as an illustrative example, while this framework is adaptable to other DGMs. Two case studies show that the proposed method outperforms some state-of-the-art approaches in structural anomaly detection and clustering, while also dynamically generating new clusters to indicate the emergence of new structural conditions for online monitoring.</li>
</ul>

<h3>Title: NLP-ADBench: NLP Anomaly Detection Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Yuangang Li, Jiaqi Li, Zhuo Xiao, Tiankai Yang, Yi Nian, Xiyang Hu, Yue Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04784">https://arxiv.org/abs/2412.04784</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04784">https://arxiv.org/pdf/2412.04784</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04784]] NLP-ADBench: NLP Anomaly Detection Benchmark(https://arxiv.org/abs/2412.04784)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Anomaly detection (AD) is a critical machine learning task with diverse applications in web systems, including fraud detection, content moderation, and user behavior analysis. Despite its significance, AD in natural language processing (NLP) remains underexplored, limiting advancements in detecting anomalies in text data such as harmful content, phishing attempts, or spam reviews. In this paper, we introduce NLP-ADBench, the most comprehensive benchmark for NLP anomaly detection (NLP-AD), comprising eight curated datasets and evaluations of nineteen state-of-the-art algorithms. These include three end-to-end methods and sixteen two-step algorithms that apply traditional anomaly detection techniques to language embeddings generated by bert-base-uncased and OpenAI's text-embedding-3-large models. Our results reveal critical insights and future directions for NLP-AD. Notably, no single model excels across all datasets, highlighting the need for automated model selection. Moreover, two-step methods leveraging transformer-based embeddings consistently outperform specialized end-to-end approaches, with OpenAI embeddings demonstrating superior performance over BERT embeddings. By releasing NLP-ADBench at this https URL, we provide a standardized framework for evaluating NLP-AD methods, fostering the development of innovative approaches. This work fills a crucial gap in the field and establishes a foundation for advancing NLP anomaly detection, particularly in the context of improving the safety and reliability of web-based systems.</li>
</ul>

<h3>Title: Differentially Private Random Feature Model</h3>
<ul>
<li><strong>Authors: </strong>Chunyang Liao, Deanna Needell, Alexander Xue</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04785">https://arxiv.org/abs/2412.04785</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04785">https://arxiv.org/pdf/2412.04785</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04785]] Differentially Private Random Feature Model(https://arxiv.org/abs/2412.04785)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, fair</a></li>
<li><strong>Abstract: </strong>Designing privacy-preserving machine learning algorithms has received great attention in recent years, especially in the setting when the data contains sensitive information. Differential privacy (DP) is a widely used mechanism for data analysis with privacy guarantees. In this paper, we produce a differentially private random feature model. Random features, which were proposed to approximate large-scale kernel machines, have been used to study privacy-preserving kernel machines as well. We consider the over-parametrized regime (more features than samples) where the non-private random feature model is learned via solving the min-norm interpolation problem, and then we apply output perturbation techniques to produce a private model. We show that our method preserves privacy and derive a generalization error bound for the method. To the best of our knowledge, we are the first to consider privacy-preserving random feature models in the over-parametrized regime and provide theoretical guarantees. We empirically compare our method with other privacy-preserving learning methods in the literature as well. Our results show that our approach is superior to the other methods in terms of generalization performance on synthetic data and benchmark data sets. Additionally, it was recently observed that DP mechanisms may exhibit and exacerbate disparate impact, which means that the outcomes of DP learning algorithms vary significantly among different groups. We show that both theoretically and empirically, random features have the potential to reduce disparate impact, and hence achieve better fairness.</li>
</ul>

<h3>Title: Slicing Vision Transformer for Flexible Inference</h3>
<ul>
<li><strong>Authors: </strong>Yitian Zhang, Huseyin Coskun, Xu Ma, Huan Wang, Ke Ma, Xi (Stephen)Chen, Derek Hao Hu, Yun Fu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04786">https://arxiv.org/abs/2412.04786</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04786">https://arxiv.org/pdf/2412.04786</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04786]] Slicing Vision Transformer for Flexible Inference(https://arxiv.org/abs/2412.04786)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Vision Transformers (ViT) is known for its scalability. In this work, we target to scale down a ViT to fit in an environment with dynamic-changing resource constraints. We observe that smaller ViTs are intrinsically the sub-networks of a larger ViT with different widths. Thus, we propose a general framework, named Scala, to enable a single network to represent multiple smaller ViTs with flexible inference capability, which aligns with the inherent design of ViT to vary from widths. Concretely, Scala activates several subnets during training, introduces Isolated Activation to disentangle the smallest sub-network from other subnets, and leverages Scale Coordination to ensure each sub-network receives simplified, steady, and accurate learning objectives. Comprehensive empirical validations on different tasks demonstrate that with only one-shot training, Scala learns slimmable representation without modifying the original ViT structure and matches the performance of Separate Training. Compared with the prior art, Scala achieves an average improvement of 1.6% on ImageNet-1K with fewer parameters.</li>
</ul>

<h3>Title: Direct Quantized Training of Language Models with Stochastic Rounding</h3>
<ul>
<li><strong>Authors: </strong>Kaiyan Zhao, Tsuguchika Tabaru, Kenichi Kobayashi, Takumi Honda, Masafumi Yamazaki, Yoshimasa Tsuruoka</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04787">https://arxiv.org/abs/2412.04787</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04787">https://arxiv.org/pdf/2412.04787</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04787]] Direct Quantized Training of Language Models with Stochastic Rounding(https://arxiv.org/abs/2412.04787)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Although recent quantized Large Language Models (LLMs), such as BitNet, have paved the way for significant reduction in memory usage during deployment with binary or ternary weights, training these models still demands substantial memory footprints. This is partly because high-precision (i.e., unquantized) weight matrices required for straight-through estimation must be maintained throughout the whole training process. To address this, we explore the potential of directly updating the quantized low-precision weight matrices without relying on the straight-through estimator during backpropagation, thereby saving memory usage during training. Specifically, we employ a stochastic rounding technique to minimize information loss caused by the use of low-bit weights throughout training. Experimental results on our LLaMA-structured models indicate that (1) training with only low-precision weights is feasible even when they are constrained to ternary values, (2) extending the bit width to 8 bits results in only a 5% loss degradation compared to BitNet b1.58 while offering the potential for reduced memory usage during training, and (3) our models can also perform inference using ternary weights, showcasing their flexibility in deployment.</li>
</ul>

<h3>Title: DrIFT: Autonomous Drone Dataset with Integrated Real and Synthetic Data, Flexible Views, and Transformed Domains</h3>
<ul>
<li><strong>Authors: </strong>Fardad Dadboud, Hamid Azad, Varun Mehta, Miodrag Bolic, Iraj Mntegh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04789">https://arxiv.org/abs/2412.04789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04789">https://arxiv.org/pdf/2412.04789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04789]] DrIFT: Autonomous Drone Dataset with Integrated Real and Synthetic Data, Flexible Views, and Transformed Domains(https://arxiv.org/abs/2412.04789)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, segmentation</a></li>
<li><strong>Abstract: </strong>Dependable visual drone detection is crucial for the secure integration of drones into the airspace. However, drone detection accuracy is significantly affected by domain shifts due to environmental changes, varied points of view, and background shifts. To address these challenges, we present the DrIFT dataset, specifically developed for visual drone detection under domain shifts. DrIFT includes fourteen distinct domains, each characterized by shifts in point of view, synthetic-to-real data, season, and adverse weather. DrIFT uniquely emphasizes background shift by providing background segmentation maps to enable background-wise metrics and evaluation. Our new uncertainty estimation metric, MCDO-map, features lower postprocessing complexity, surpassing traditional methods. We use the MCDO-map in our uncertainty-aware unsupervised domain adaptation method, demonstrating superior performance to SOTA unsupervised domain adaptation techniques. The dataset is available at: this https URL.</li>
</ul>

<h3>Title: Rethinking Time Series Forecasting with LLMs via Nearest Neighbor Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Jayanie Bogahawatte, Sachith Seneviratne, Maneesha Perera, Saman Halgamuge</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04806">https://arxiv.org/abs/2412.04806</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04806">https://arxiv.org/pdf/2412.04806</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04806]] Rethinking Time Series Forecasting with LLMs via Nearest Neighbor Contrastive Learning(https://arxiv.org/abs/2412.04806)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Adapting Large Language Models (LLMs) that are extensively trained on abundant text data, and customizing the input prompt to enable time series forecasting has received considerable attention. While recent work has shown great potential for adapting the learned prior of LLMs, the formulation of the prompt to finetune LLMs remains challenging as prompt should be aligned with time series data. Additionally, current approaches do not effectively leverage word token embeddings which embody the rich representation space learned by LLMs. This emphasizes the need for a robust approach to formulate the prompt which utilizes the word token embeddings while effectively representing the characteristics of the time series. To address these challenges, we propose NNCL-TLLM: Nearest Neighbor Contrastive Learning for Time series forecasting via LLMs. First, we generate time series compatible text prototypes such that each text prototype represents both word token embeddings in its neighborhood and time series characteristics via end-to-end finetuning. Next, we draw inspiration from Nearest Neighbor Contrastive Learning to formulate the prompt while obtaining the top-$k$ nearest neighbor time series compatible text prototypes. We then fine-tune the layer normalization and positional embeddings of the LLM, keeping the other layers intact, reducing the trainable parameters and decreasing the computational cost. Our comprehensive experiments demonstrate that NNCL-TLLM outperforms in few-shot forecasting while achieving competitive or superior performance over the state-of-the-art methods in long-term and short-term forecasting tasks.</li>
</ul>

<h3>Title: LiFT: Leveraging Human Feedback for Text-to-Video Model Alignment</h3>
<ul>
<li><strong>Authors: </strong>Yibin Wang, Zhiyu Tan, Junyan Wang, Xiaomeng Yang, Cheng Jin, Hao Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04814">https://arxiv.org/abs/2412.04814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04814">https://arxiv.org/pdf/2412.04814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04814]] LiFT: Leveraging Human Feedback for Text-to-Video Model Alignment(https://arxiv.org/abs/2412.04814)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in text-to-video (T2V) generative models have shown impressive capabilities. However, these models are still inadequate in aligning synthesized videos with human preferences (e.g., accurately reflecting text descriptions), which is particularly difficult to address, as human preferences are inherently subjective and challenging to formalize as objective functions. Therefore, this paper proposes LiFT, a novel fine-tuning method leveraging human feedback for T2V model alignment. Specifically, we first construct a Human Rating Annotation dataset, LiFT-HRA, consisting of approximately 10k human annotations, each including a score and its corresponding rationale. Based on this, we train a reward model LiFT-Critic to learn reward function effectively, which serves as a proxy for human judgment, measuring the alignment between given videos and human expectations. Lastly, we leverage the learned reward function to align the T2V model by maximizing the reward-weighted likelihood. As a case study, we apply our pipeline to CogVideoX-2B, showing that the fine-tuned model outperforms the CogVideoX-5B across all 16 metrics, highlighting the potential of human feedback in improving the alignment and quality of synthesized videos.</li>
</ul>

<h3>Title: DAug: Diffusion-based Channel Augmentation for Radiology Image Retrieval and Classification</h3>
<ul>
<li><strong>Authors: </strong>Ying Jin, Zhuoran Zhou, Haoquan Fang, Jenq-Neng Hwang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04828">https://arxiv.org/abs/2412.04828</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04828">https://arxiv.org/pdf/2412.04828</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04828]] DAug: Diffusion-based Channel Augmentation for Radiology Image Retrieval and Classification(https://arxiv.org/abs/2412.04828)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Medical image understanding requires meticulous examination of fine visual details, with particular regions requiring additional attention. While radiologists build such expertise over years of experience, it is challenging for AI models to learn where to look with limited amounts of training data. This limitation results in unsatisfying robustness in medical image understanding. To address this issue, we propose Diffusion-based Feature Augmentation (DAug), a portable method that improves a perception model's performance with a generative model's output. Specifically, we extend a radiology image to multiple channels, with the additional channels being the heatmaps of regions where diseases tend to develop. A diffusion-based image-to-image translation model was used to generate such heatmaps conditioned on selected disease classes. Our method is motivated by the fact that generative models learn the distribution of normal and abnormal images, and such knowledge is complementary to image understanding tasks. In addition, we propose the Image-Text-Class Hybrid Contrastive learning to utilize both text and class labels. With two novel approaches combined, our method surpasses baseline models without changing the model architecture, and achieves state-of-the-art performance on both medical image retrieval and classification tasks.</li>
</ul>

<h3>Title: Wavelet Diffusion Neural Operator</h3>
<ul>
<li><strong>Authors: </strong>Peiyan Hu, Rui Wang, Xiang Zheng, Tao Zhang, Haodong Feng, Ruiqi Feng, Long Wei, Yue Wang, Zhi-Ming Ma, Tailin Wu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04833">https://arxiv.org/abs/2412.04833</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04833">https://arxiv.org/pdf/2412.04833</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04833]] Wavelet Diffusion Neural Operator(https://arxiv.org/abs/2412.04833)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Simulating and controlling physical systems described by partial differential equations (PDEs) are crucial tasks across science and engineering. Recently, diffusion generative models have emerged as a competitive class of methods for these tasks due to their ability to capture long-term dependencies and model high-dimensional states. However, diffusion models typically struggle with handling system states with abrupt changes and generalizing to higher resolutions. In this work, we propose Wavelet Diffusion Neural Operator (WDNO), a novel PDE simulation and control framework that enhances the handling of these complexities. WDNO comprises two key innovations. Firstly, WDNO performs diffusion-based generative modeling in the wavelet domain for the entire trajectory to handle abrupt changes and long-term dependencies effectively. Secondly, to address the issue of poor generalization across different resolutions, which is one of the fundamental tasks in modeling physical systems, we introduce multi-resolution training. We validate WDNO on five physical systems, including 1D advection equation, three challenging physical systems with abrupt changes (1D Burgers' equation, 1D compressible Navier-Stokes equation and 2D incompressible fluid), and a real-world dataset ERA5, which demonstrates superior performance on both simulation and control tasks over state-of-the-art methods, with significant improvements in long-term and detail prediction accuracy. Remarkably, in the challenging context of the 2D high-dimensional and indirect control task aimed at reducing smoke leakage, WDNO reduces the leakage by 33.2% compared to the second-best baseline.</li>
</ul>

<h3>Title: Using Machine Learning to Discover Parsimonious and Physically-Interpretable Representations of Catchment-Scale Rainfall-Runoff Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Yuan-Heng Wang, Hoshin V. Gupta</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04845">https://arxiv.org/abs/2412.04845</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04845">https://arxiv.org/pdf/2412.04845</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04845]] Using Machine Learning to Discover Parsimonious and Physically-Interpretable Representations of Catchment-Scale Rainfall-Runoff Dynamics(https://arxiv.org/abs/2412.04845)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Despite the excellent real-world predictive performance of modern machine learning (ML) methods, many scientists remain hesitant to discard traditional physical-conceptual (PC) approaches due mainly to their relative interpretability, which contributes to credibility during decision-making. In this context, a currently underexplored aspect of ML is how to develop minimally-optimal representations that can facilitate better insight regarding system functioning. Regardless of how this is achieved, it is arguably true that parsimonious representations better support the advancement of scientific understanding. Our own view is that ML-based modeling of geoscientific systems should be based in the use of computational units that are fundamentally interpretable by design. This paper continues our exploration of how the strengths of ML can be exploited in the service of better understanding via scientific investigation. Here, we use the Mass Conserving Perceptron (MCP) as the fundamental computational unit in a generic network architecture consisting of nodes arranged in series and parallel to explore several generic and important issues related to the use of observational data for constructing input-state-output models of dynamical systems. In the context of lumped catchment modeling, we show that physical interpretability and excellent predictive performance can both be achieved using a relatively parsimonious distributed-state multiple-flow-path network with context-dependent gating and information sharing across the nodes, suggesting that MCP-based modeling can play a significant role in application of ML to geoscientific investigation.</li>
</ul>

<h3>Title: SleeperMark: Towards Robust Watermark against Fine-Tuning Text-to-image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Zilan Wang, Junfeng Guo, Jiacheng Zhu, Yiming Li, Heng Huang, Muhao Chen, Zhengzhong Tu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04852">https://arxiv.org/abs/2412.04852</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04852">https://arxiv.org/pdf/2412.04852</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04852]] SleeperMark: Towards Robust Watermark against Fine-Tuning Text-to-image Diffusion Models(https://arxiv.org/abs/2412.04852)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack, robust, watermark, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in large-scale text-to-image (T2I) diffusion models have enabled a variety of downstream applications, including style customization, subject-driven personalization, and conditional generation. As T2I models require extensive data and computational resources for training, they constitute highly valued intellectual property (IP) for their legitimate owners, yet making them incentive targets for unauthorized fine-tuning by adversaries seeking to leverage these models for customized, usually profitable applications. Existing IP protection methods for diffusion models generally involve embedding watermark patterns and then verifying ownership through generated outputs examination, or inspecting the model's feature space. However, these techniques are inherently ineffective in practical scenarios when the watermarked model undergoes fine-tuning, and the feature space is inaccessible during verification ((i.e., black-box setting). The model is prone to forgetting the previously learned watermark knowledge when it adapts to a new task. To address this challenge, we propose SleeperMark, a novel framework designed to embed resilient watermarks into T2I diffusion models. SleeperMark explicitly guides the model to disentangle the watermark information from the semantic concepts it learns, allowing the model to retain the embedded watermark while continuing to be fine-tuned to new downstream tasks. Our extensive experiments demonstrate the effectiveness of SleeperMark across various types of diffusion models, including latent diffusion models (e.g., Stable Diffusion) and pixel diffusion models (e.g., DeepFloyd-IF), showing robustness against downstream fine-tuning and various attacks at both the image and model levels, with minimal impact on the model's generative capability. The code is available at this https URL.</li>
</ul>

<h3>Title: Breaking Event Rumor Detection via Stance-Separated Multi-Agent Debate</h3>
<ul>
<li><strong>Authors: </strong>Mingqing Zhang, Haisong Gong, Qiang Liu, Shu Wu, Liang Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04859">https://arxiv.org/abs/2412.04859</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04859">https://arxiv.org/pdf/2412.04859</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04859]] Breaking Event Rumor Detection via Stance-Separated Multi-Agent Debate(https://arxiv.org/abs/2412.04859)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid spread of rumors on social media platforms during breaking events severely hinders the dissemination of the truth. Previous studies reveal that the lack of annotated resources hinders the direct detection of unforeseen breaking events not covered in yesterday's news. Leveraging large language models (LLMs) for rumor detection holds significant promise. However, it is challenging for LLMs to provide comprehensive responses to complex or controversial issues due to limited diversity. In this work, we propose the Stance Separated Multi-Agent Debate (S2MAD) to address this issue. Specifically, we firstly introduce Stance Separation, categorizing comments as either supporting or opposing the original claim. Subsequently, claims are classified as subjective or objective, enabling agents to generate reasonable initial viewpoints with different prompt strategies for each type of claim. Debaters then follow specific instructions through multiple rounds of debate to reach a consensus. If a consensus is not reached, a judge agent evaluates the opinions and delivers a final verdict on the claim's veracity. Extensive experiments conducted on two real-world datasets demonstrate that our proposed model outperforms state-of-the-art methods in terms of performance and effectively improves the performance of LLMs in breaking event rumor detection.</li>
</ul>

<h3>Title: MSECG: Incorporating Mamba for Robust and Efficient ECG Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Jie Lin, I Chiu, Kuan-Chen Wang, Kai-Chun Liu, Hsin-Min Wang, Ping-Cheng Yeh, Yu Tsao</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04861">https://arxiv.org/abs/2412.04861</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04861">https://arxiv.org/pdf/2412.04861</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04861]] MSECG: Incorporating Mamba for Robust and Efficient ECG Super-Resolution(https://arxiv.org/abs/2412.04861)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Electrocardiogram (ECG) signals play a crucial role in diagnosing cardiovascular diseases. To reduce power consumption in wearable or portable devices used for long-term ECG monitoring, super-resolution (SR) techniques have been developed, enabling these devices to collect and transmit signals at a lower sampling rate. In this study, we propose MSECG, a compact neural network model designed for ECG SR. MSECG combines the strength of the recurrent Mamba model with convolutional layers to capture both local and global dependencies in ECG waveforms, allowing for the effective reconstruction of high-resolution signals. We also assess the model's performance in real-world noisy conditions by utilizing ECG data from the PTB-XL database and noise data from the MIT-BIH Noise Stress Test Database. Experimental results show that MSECG outperforms two contemporary ECG SR models under both clean and noisy conditions while using fewer parameters, offering a more powerful and robust solution for long-term ECG monitoring applications.</li>
</ul>

<h3>Title: EXAONE 3.5: Series of Large Language Models for Real-world Use Cases</h3>
<ul>
<li><strong>Authors: </strong>LG AI Research, Soyoung An, Kyunghoon Bae, Eunbi Choi, Kibong Choi, Stanley Jungkyu Choi, Seokhee Hong, Junwon Hwang, Hyojin Jeon, Gerrard Jeongwon Jo, Hyunjik Jo, Jiyeon Jung, Yountae Jung, Hyosang Kim, Joonkee Kim, Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Yongil Kim, Youchul Kim, Edward Hwayoung Lee, Haeju Lee, Honglak Lee, Jinsik Lee, Kyungmin Lee, Woohyung Lim, Sangha Park, Sooyoun Park, Yongmin Park, Sihoon Yang, Heuiyeen Yeen, Hyeongu Yun</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04862">https://arxiv.org/abs/2412.04862</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04862">https://arxiv.org/pdf/2412.04862</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04862]] EXAONE 3.5: Series of Large Language Models for Real-world Use Cases(https://arxiv.org/abs/2412.04862)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This technical report introduces the EXAONE 3.5 instruction-tuned language models, developed and released by LG AI Research. The EXAONE 3.5 language models are offered in three configurations: 32B, 7.8B, and 2.4B. These models feature several standout capabilities: 1) exceptional instruction following capabilities in real-world scenarios, achieving the highest scores across seven benchmarks, 2) outstanding long-context comprehension, attaining the top performance in four benchmarks, and 3) competitive results compared to state-of-the-art open models of similar sizes across nine general benchmarks. The EXAONE 3.5 language models are open to anyone for research purposes and can be downloaded from this https URL. For commercial use, please reach out to the official contact point of LG AI Research: contact_us@lgresearch.ai.</li>
</ul>

<h3>Title: MozzaVID: Mozzarella Volumetric Image Dataset</h3>
<ul>
<li><strong>Authors: </strong>Pawel Tomasz Pieta, Peter Winkel Rasmussen, Anders Bjorholm Dahl, Jeppe Revall Frisvad, Siavash Arjomand Bigdeli, Carsten Gundlach, Anders Nymark Christensen</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04880">https://arxiv.org/abs/2412.04880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04880">https://arxiv.org/pdf/2412.04880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04880]] MozzaVID: Mozzarella Volumetric Image Dataset(https://arxiv.org/abs/2412.04880)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Influenced by the complexity of volumetric imaging, there is a shortage of established datasets useful for benchmarking volumetric deep-learning models. As a consequence, new and existing models are not easily comparable, limiting the development of architectures optimized specifically for volumetric data. To counteract this trend, we introduce MozzaVID - a large, clean, and versatile volumetric classification dataset. Our dataset contains X-ray computed tomography (CT) images of mozzarella microstructure and enables the classification of 25 cheese types and 149 cheese samples. We provide data in three different resolutions, resulting in three dataset instances containing from 591 to 37,824 images. While being general-purpose, the dataset also facilitates investigating mozzarella structure properties. The structure of food directly affects its functional properties and thus its consumption experience. Understanding food structure helps tune the production and mimicking it enables sustainable alternatives to animal-derived food products. The complex and disordered nature of food structures brings a unique challenge, where a choice of appropriate imaging method, scale, and sample size is not trivial. With this dataset we aim to address these complexities, contributing to more robust structural analysis models. The dataset can be downloaded from: this https URL.</li>
</ul>

<h3>Title: AI-Driven Non-Invasive Detection and Staging of Steatosis in Fatty Liver Disease Using a Novel Cascade Model and Information Fusion Techniques</h3>
<ul>
<li><strong>Authors: </strong>Niloufar Delfan, Pardis Ketabi Moghadam, Mohammad Khoshnevisan, Mehdi Hosseini Chagahi, Behzad Hatami, Melika Asgharzadeh, Mohammadreza Zali, Behzad Moshiri, Amin Momeni Moghaddam, Mohammad Amin Khalafi, Khosrow Dehnad</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04884">https://arxiv.org/abs/2412.04884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04884">https://arxiv.org/pdf/2412.04884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04884]] AI-Driven Non-Invasive Detection and Staging of Steatosis in Fatty Liver Disease Using a Novel Cascade Model and Information Fusion Techniques(https://arxiv.org/abs/2412.04884)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Non-alcoholic fatty liver disease (NAFLD) is one of the most widespread liver disorders on a global scale, posing a significant threat of progressing to more severe conditions like nonalcoholic steatohepatitis (NASH), liver fibrosis, cirrhosis, and hepatocellular carcinoma. Diagnosing and staging NAFLD presents challenges due to its non-specific symptoms and the invasive nature of liver biopsies. Our research introduces a novel artificial intelligence cascade model employing ensemble learning and feature fusion techniques. We developed a non-invasive, robust, and reliable diagnostic artificial intelligence tool that utilizes anthropometric and laboratory parameters, facilitating early detection and intervention in NAFLD progression. Our novel artificial intelligence achieved an 86% accuracy rate for the NASH steatosis staging task (non-NASH, steatosis grade 1, steatosis grade 2, and steatosis grade 3) and an impressive 96% AUC-ROC for distinguishing between NASH (steatosis grade 1, grade 2, and grade3) and non-NASH cases, outperforming current state-of-the-art models. This notable improvement in diagnostic performance underscores the potential application of artificial intelligence in the early diagnosis and treatment of NAFLD, leading to better patient outcomes and a reduced healthcare burden associated with advanced liver disease.</li>
</ul>

<h3>Title: Mitigating Instance-Dependent Label Noise: Integrating Self-Supervised Pretraining with Pseudo-Label Refinement</h3>
<ul>
<li><strong>Authors: </strong>Gouranga Bala, Anuj Gupta, Subrat Kumar Behera, Amit Sethi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04898">https://arxiv.org/abs/2412.04898</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04898">https://arxiv.org/pdf/2412.04898</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04898]] Mitigating Instance-Dependent Label Noise: Integrating Self-Supervised Pretraining with Pseudo-Label Refinement(https://arxiv.org/abs/2412.04898)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep learning models rely heavily on large volumes of labeled data to achieve high performance. However, real-world datasets often contain noisy labels due to human error, ambiguity, or resource constraints during the annotation process. Instance-dependent label noise (IDN), where the probability of a label being corrupted depends on the input features, poses a significant challenge because it is more prevalent and harder to address than instance-independent noise. In this paper, we propose a novel hybrid framework that combines self-supervised learning using SimCLR with iterative pseudo-label refinement to mitigate the effects of IDN. The self-supervised pre-training phase enables the model to learn robust feature representations without relying on potentially noisy labels, establishing a noise-agnostic foundation. Subsequently, we employ an iterative training process with pseudo-label refinement, where confidently predicted samples are identified through a multistage approach and their labels are updated to improve label quality progressively. We evaluate our method on the CIFAR-10 and CIFAR-100 datasets augmented with synthetic instance-dependent noise at varying noise levels. Experimental results demonstrate that our approach significantly outperforms several state-of-the-art methods, particularly under high noise conditions, achieving notable improvements in classification accuracy and robustness. Our findings suggest that integrating self-supervised learning with iterative pseudo-label refinement offers an effective strategy for training deep neural networks on noisy datasets afflicted by instance-dependent label noise.</li>
</ul>

<h3>Title: A cyber-physical digital twin approach to replicating realistic multi-stage cyberattacks on smart grids</h3>
<ul>
<li><strong>Authors: </strong>Omer Sen, Nathalie Bleser, Martin Henze, Andreas Ulbig</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04900">https://arxiv.org/abs/2412.04900</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04900">https://arxiv.org/pdf/2412.04900</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04900]] A cyber-physical digital twin approach to replicating realistic multi-stage cyberattacks on smart grids(https://arxiv.org/abs/2412.04900)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>The integration of information and communication technology in distribution grids presents opportunities for active grid operation management, but also increases the need for security against power outages and cyberattacks. This paper examines the impact of cyberattacks on smart grids by replicating the power grid in a secure laboratory environment as a cyber-physical digital twin. A simulation is used to study communication infrastructures for secure operation of smart grids. The cyber-physical digital twin approach combines communication network emulation and power grid simulation in a common modular environment, and is demonstrated through laboratory tests and attack replications.</li>
</ul>

<h3>Title: Encryption-Aware Anomaly Detection in Power Grid Communication Networks</h3>
<ul>
<li><strong>Authors: </strong>Omer Sen, Mehdi Akbari Gurabi, Milan Deruelle, Andreas Ulbig, Stefan Decker</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04901">https://arxiv.org/abs/2412.04901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04901">https://arxiv.org/pdf/2412.04901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04901]] Encryption-Aware Anomaly Detection in Power Grid Communication Networks(https://arxiv.org/abs/2412.04901)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect</a></li>
<li><strong>Abstract: </strong>The shift to smart grids has made electrical power systems more vulnerable to sophisticated cyber threats. To protect these systems, holistic security measures that encompass preventive, detective, and reactive components are required, even with encrypted data. However, traditional intrusion detection methods struggle with encrypted traffic, our research focuses on the low-level communication layers of encrypted power grid systems to identify irregular patterns using statistics and machine learning. Our results indicate that a harmonic security concept based on encrypted traffic and anomaly detection is promising for smart grid security; however, further research is necessary to improve detection accuracy.</li>
</ul>

<h3>Title: On Process Awareness in Detecting Multi-stage Cyberattacks in Smart Grids</h3>
<ul>
<li><strong>Authors: </strong>Omer Sen, Yanico Aust, Simon Glomb, Andreas Ulbig</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04902">https://arxiv.org/abs/2412.04902</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04902">https://arxiv.org/pdf/2412.04902</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04902]] On Process Awareness in Detecting Multi-stage Cyberattacks in Smart Grids(https://arxiv.org/abs/2412.04902)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>This study delves into the role of process awareness in enhancing intrusion detection within Smart Grids, considering the increasing fusion of ICT in power systems and the associated emerging threats. The research harnesses a co-simulation environment, encapsulating IT, OT, and ET layers, to model multi-stage cyberattacks and evaluate machine learning-based IDS strategies. The key observation is that process-aware IDS demonstrate superior detection capabilities, especially in scenarios closely tied to operational processes, as opposed to IT-only IDS. This improvement is notable in distinguishing complex cyber threats from regular IT activities. The findings underscore the significance of further developing sophisticated IDS benchmarks and digital twin datasets in Smart Grid environments, paving the way for more resilient cybersecurity infrastructures.</li>
</ul>

<h3>Title: EACO: Enhancing Alignment in Multimodal LLMs via Critical Observation</h3>
<ul>
<li><strong>Authors: </strong>Yongxin Wang, Meng Cao, Haokun Lin, Mingfei Han, Liang Ma, Jin Jiang, Yuhao Cheng, Xiaodan Liang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04903">https://arxiv.org/abs/2412.04903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04903">https://arxiv.org/pdf/2412.04903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04903]] EACO: Enhancing Alignment in Multimodal LLMs via Critical Observation(https://arxiv.org/abs/2412.04903)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) have achieved remarkable progress on various visual question answering and reasoning tasks leveraging instruction fine-tuning specific datasets. They can also learn from preference data annotated by human to enhance their reasoning ability and mitigate hallucinations. Most of preference data is generated from the model itself. However, existing methods require high-quality critical labels, which are costly and rely on human or proprietary models like GPT-4V. In this work, we propose Enhancing Alignment in MLLMs via Critical Observation (EACO), which aligns MLLMs by self-generated preference data using only 5k images economically. Our approach begins with collecting and refining a Scoring Evaluation Instruction-tuning dataset to train a critical evaluation model, termed the Critic. This Critic observes model responses across multiple dimensions, selecting preferred and non-preferred outputs for refined Direct Preference Optimization (DPO) tuning. To further enhance model performance, we employ an additional supervised fine-tuning stage after preference tuning. EACO reduces the overall hallucinations by 65.6% on HallusionBench and improves the reasoning ability by 21.8% on MME-Cognition. EACO achieves an 8.5% improvement over LLaVA-v1.6-Mistral-7B across multiple benchmarks. Remarkably, EACO also shows the potential critical ability in open-source MLLMs, demonstrating that EACO is a viable path to boost the competence of MLLMs.</li>
</ul>

<h3>Title: DEMO: Reframing Dialogue Interaction with Fine-grained Element Modeling</h3>
<ul>
<li><strong>Authors: </strong>Minzheng Wang, Xinghua Zhang, Kun Chen, Nan Xu, Haiyang Yu, Fei Huang, Wenji Mao, Yongbin Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04905">https://arxiv.org/abs/2412.04905</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04905">https://arxiv.org/pdf/2412.04905</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04905]] DEMO: Reframing Dialogue Interaction with Fine-grained Element Modeling(https://arxiv.org/abs/2412.04905)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have made dialogue one of the central modes of human-machine interaction, leading to the accumulation of vast amounts of conversation logs and increasing demand for dialogue generation. A conversational life-cycle spans from the Prelude through the Interlocution to the Epilogue, encompassing various elements. Despite the existence of numerous dialogue-related studies, there is a lack of benchmarks that encompass comprehensive dialogue elements, hindering precise modeling and systematic evaluation. To bridge this gap, we introduce an innovative research task $\textbf{D}$ialogue $\textbf{E}$lement $\textbf{MO}$deling, including $\textit{Element Awareness}$ and $\textit{Dialogue Agent Interaction}$, and propose a novel benchmark, $\textbf{DEMO}$, designed for a comprehensive dialogue modeling and assessment. Inspired by imitation learning, we further build the agent which possesses the adept ability to model dialogue elements based on the DEMO benchmark. Extensive experiments indicate that existing LLMs still exhibit considerable potential for enhancement, and our DEMO agent has superior performance in both in-domain and out-of-domain tasks.</li>
</ul>

<h3>Title: Achieving Group Fairness through Independence in Predictive Process Monitoring</h3>
<ul>
<li><strong>Authors: </strong>Jari Peeperkorn, Simon De Vos</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04914">https://arxiv.org/abs/2412.04914</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04914">https://arxiv.org/pdf/2412.04914</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04914]] Achieving Group Fairness through Independence in Predictive Process Monitoring(https://arxiv.org/abs/2412.04914)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Predictive process monitoring focuses on forecasting future states of ongoing process executions, such as predicting the outcome of a particular case. In recent years, the application of machine learning models in this domain has garnered significant scientific attention. When using historical execution data, which may contain biases or exhibit unfair behavior, these biases may be encoded into the trained models. Consequently, when such models are deployed to make decisions or guide interventions for new cases, they risk perpetuating this unwanted behavior. This work addresses group fairness in predictive process monitoring by investigating independence, i.e. ensuring predictions are unaffected by sensitive group membership. We explore independence through metrics for demographic parity such as $\Delta$DP, as well as recently introduced, threshold-independent distribution-based alternatives. Additionally, we propose a composite loss functions existing of binary cross-entropy and a distribution-based loss (Wasserstein) to train models that balance predictive performance and fairness, and allow for customizable trade-offs. The effectiveness of both the fairness metrics and the composite loss functions is validated through a controlled experimental setup.</li>
</ul>

<h3>Title: Beyond Boxes: Mask-Guided Spatio-Temporal Feature Aggregation for Video Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Khurram Azeem Hashmi, Talha Uddin Sheikh, Didier Stricker, Muhammad Zeshan Afzal</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04915">https://arxiv.org/abs/2412.04915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04915">https://arxiv.org/pdf/2412.04915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04915]] Beyond Boxes: Mask-Guided Spatio-Temporal Feature Aggregation for Video Object Detection(https://arxiv.org/abs/2412.04915)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>The primary challenge in Video Object Detection (VOD) is effectively exploiting temporal information to enhance object representations. Traditional strategies, such as aggregating region proposals, often suffer from feature variance due to the inclusion of background information. We introduce a novel instance mask-based feature aggregation approach, significantly refining this process and deepening the understanding of object dynamics across video frames. We present FAIM, a new VOD method that enhances temporal Feature Aggregation by leveraging Instance Mask features. In particular, we propose the lightweight Instance Feature Extraction Module (IFEM) to learn instance mask features and the Temporal Instance Classification Aggregation Module (TICAM) to aggregate instance mask and classification features across video frames. Using YOLOX as a base detector, FAIM achieves 87.9% mAP on the ImageNet VID dataset at 33 FPS on a single 2080Ti GPU, setting a new benchmark for the speed-accuracy trade-off. Additional experiments on multiple datasets validate that our approach is robust, method-agnostic, and effective in multi-object tracking, demonstrating its broader applicability to video understanding tasks.</li>
</ul>

<h3>Title: Large Language Models for Ingredient Substitution in Food Recipes using Supervised Fine-tuning and Direct Preference Optimization</h3>
<ul>
<li><strong>Authors: </strong>Thevin Senath, Kumuthu Athukorala, Ransika Costa, Surangika Ranathunga, Rishemjit Kaur</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04922">https://arxiv.org/abs/2412.04922</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04922">https://arxiv.org/pdf/2412.04922</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04922]] Large Language Models for Ingredient Substitution in Food Recipes using Supervised Fine-tuning and Direct Preference Optimization(https://arxiv.org/abs/2412.04922)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we address the challenge of recipe personalization through ingredient substitution. We make use of Large Language Models (LLMs) to build an ingredient substitution system designed to predict plausible substitute ingredients within a given recipe context. Given that the use of LLMs for this task has been barely done, we carry out an extensive set of experiments to determine the best LLM, prompt, and the fine-tuning setups. We further experiment with methods such as multi-task learning, two-stage fine-tuning, and Direct Preference Optimization (DPO). The experiments are conducted using the publicly available Recipe1MSub corpus. The best results are produced by the Mistral7-Base LLM after fine-tuning and DPO. This result outperforms the strong baseline available for the same corpus with a Hit@1 score of 22.04. Thus we believe that this research represents a significant step towards enabling personalized and creative culinary experiences by utilizing LLM-based ingredient substitution.</li>
</ul>

<h3>Title: $S^3$: Synonymous Semantic Space for Improving Zero-Shot Generalization of Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xiaojie Yin, Qilong Wang, Bing Cao, Qinghua Hu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04925">https://arxiv.org/abs/2412.04925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04925">https://arxiv.org/pdf/2412.04925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04925]] $S^3$: Synonymous Semantic Space for Improving Zero-Shot Generalization of Vision-Language Models(https://arxiv.org/abs/2412.04925)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Recently, many studies have been conducted to enhance the zero-shot generalization ability of vision-language models (e.g., CLIP) by addressing the semantic misalignment between image and text embeddings in downstream tasks. Although many efforts have been made, existing methods barely consider the fact that a class of images can be described by notably different textual concepts due to well-known lexical variation in natural language processing, which heavily affects the zero-shot generalization of CLIP. Therefore, this paper proposes a \textbf{S}ynonymous \textbf{S}emantic \textbf{S}pace ($S^3$) for each image class, rather than relying on a single textual concept, achieving more stable semantic alignment and improving the zero-shot generalization of CLIP. Specifically, our $S^3$ method first generates several synonymous concepts based on the label of each class by using large language models, and constructs a continuous yet compact synonymous semantic space based on the Vietoris-Rips complex of the generated synonymous concepts. Furthermore, we explore the effect of several point-to-space metrics on our $S^3$, while presenting a point-to-local-center metric to compute similarity between image embeddings and the synonymous semantic space of each class, accomplishing effective zero-shot predictions. Extensive experiments are conducted across 17 benchmarks, including fine-grained zero-shot classification, natural distribution zero-shot classification, and open-vocabulary segmentation, and the results show that our $S^3$ outperforms state-of-the-art methods.</li>
</ul>

<h3>Title: Continuous Video Process: Modeling Videos as Continuous Multi-Dimensional Processes for Video Prediction</h3>
<ul>
<li><strong>Authors: </strong>Gaurav Shrivastava, Abhinav Shrivastava</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04929">https://arxiv.org/abs/2412.04929</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04929">https://arxiv.org/pdf/2412.04929</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04929]] Continuous Video Process: Modeling Videos as Continuous Multi-Dimensional Processes for Video Prediction(https://arxiv.org/abs/2412.04929)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have made significant strides in image generation, mastering tasks such as unconditional image synthesis, text-image translation, and image-to-image conversions. However, their capability falls short in the realm of video prediction, mainly because they treat videos as a collection of independent images, relying on external constraints such as temporal attention mechanisms to enforce temporal coherence. In our paper, we introduce a novel model class, that treats video as a continuous multi-dimensional process rather than a series of discrete frames. We also report a reduction of 75\% sampling steps required to sample a new frame thus making our framework more efficient during the inference time. Through extensive experimentation, we establish state-of-the-art performance in video prediction, validated on benchmark datasets including KTH, BAIR, Human3.6M, and UCF101. Navigate to the project page this https URL for video results.}</li>
</ul>

<h3>Title: Video Decomposition Prior: A Methodology to Decompose Videos into Layers</h3>
<ul>
<li><strong>Authors: </strong>Gaurav Shrivastava, Ser-Nam Lim, Abhinav Shrivastava</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04930">https://arxiv.org/abs/2412.04930</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04930">https://arxiv.org/pdf/2412.04930</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04930]] Video Decomposition Prior: A Methodology to Decompose Videos into Layers(https://arxiv.org/abs/2412.04930)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In the evolving landscape of video enhancement and editing methodologies, a majority of deep learning techniques often rely on extensive datasets of observed input and ground truth sequence pairs for optimal performance. Such reliance often falters when acquiring data becomes challenging, especially in tasks like video dehazing and relighting, where replicating identical motions and camera angles in both corrupted and ground truth sequences is complicated. Moreover, these conventional methodologies perform best when the test distribution closely mirrors the training distribution. Recognizing these challenges, this paper introduces a novel video decomposition prior `\texttt{VDP}' framework which derives inspiration from professional video editing practices. Our methodology does not mandate task-specific external data corpus collection, instead pivots to utilizing the motion and appearance of the input video. \texttt{VDP} framework decomposes a video sequence into a set of multiple RGB layers and associated opacity levels. These set of layers are then manipulated individually to obtain the desired results. We addresses tasks such as video object segmentation, dehazing, and relighting. Moreover, we introduce a novel logarithmic video decomposition formulation for video relighting tasks, setting a new benchmark over the existing methodologies. We observe the property of relighting emerge as we optimize for our novel relighting decomposition formulation. We evaluate our approach on standard video datasets like DAVIS, REVIDE, \& SDSD and show qualitative results on a diverse array of internet videos. Project Page - this https URL for video results.</li>
</ul>

<h3>Title: Probing the contents of semantic representations from text, behavior, and brain data using the psychNorms metabase</h3>
<ul>
<li><strong>Authors: </strong>Zak Hussain, Rui Mata, Ben R. Newell, Dirk U. Wulff</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04936">https://arxiv.org/abs/2412.04936</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04936">https://arxiv.org/pdf/2412.04936</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04936]] Probing the contents of semantic representations from text, behavior, and brain data using the psychNorms metabase(https://arxiv.org/abs/2412.04936)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Semantic representations are integral to natural language processing, psycholinguistics, and artificial intelligence. Although often derived from internet text, recent years have seen a rise in the popularity of behavior-based (e.g., free associations) and brain-based (e.g., fMRI) representations, which promise improvements in our ability to measure and model human representations. We carry out the first systematic evaluation of the similarities and differences between semantic representations derived from text, behavior, and brain data. Using representational similarity analysis, we show that word vectors derived from behavior and brain data encode information that differs from their text-derived cousins. Furthermore, drawing on our psychNorms metabase, alongside an interpretability method that we call representational content analysis, we find that, in particular, behavior representations capture unique variance on certain affective, agentic, and socio-moral dimensions. We thus establish behavior as an important complement to text for capturing human representations and behavior. These results are broadly relevant to research aimed at learning human-aligned semantic representations, including work on evaluating and aligning large language models.</li>
</ul>

<h3>Title: Who Speaks Next? Multi-party AI Discussion Leveraging the Systematics of Turn-taking in Murder Mystery Games</h3>
<ul>
<li><strong>Authors: </strong>Ryota Nonomura, Hiroki Mori</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04937">https://arxiv.org/abs/2412.04937</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04937">https://arxiv.org/pdf/2412.04937</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04937]] Who Speaks Next? Multi-party AI Discussion Leveraging the Systematics of Turn-taking in Murder Mystery Games(https://arxiv.org/abs/2412.04937)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multi-agent systems utilizing large language models (LLMs) have shown great promise in achieving natural dialogue. However, smooth dialogue control and autonomous decision making among agents still remain challenges. In this study, we focus on conversational norms such as adjacency pairs and turn-taking found in conversation analysis and propose a new framework called "Murder Mystery Agents" that applies these norms to AI agents' dialogue control. As an evaluation target, we employed the "Murder Mystery" game, a reasoning-type table-top role-playing game that requires complex social reasoning and information manipulation. In this game, players need to unravel the truth of the case based on fragmentary information through cooperation and bargaining. The proposed framework integrates next speaker selection based on adjacency pairs and a self-selection mechanism that takes agents' internal states into account to achieve more natural and strategic dialogue. To verify the effectiveness of this new approach, we analyzed utterances that led to dialogue breakdowns and conducted automatic evaluation using LLMs, as well as human evaluation using evaluation criteria developed for the Murder Mystery game. Experimental results showed that the implementation of the next speaker selection mechanism significantly reduced dialogue breakdowns and improved the ability of agents to share information and perform logical reasoning. The results of this study demonstrate that the systematics of turn-taking in human conversation are also effective in controlling dialogue among AI agents, and provide design guidelines for more advanced multi-agent dialogue systems.</li>
</ul>

<h3>Title: Verb Mirage: Unveiling and Assessing Verb Concept Hallucinations in Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zehao Wang, Xinpeng Liu, Xiaoqian Wu, Yudonglin Zhang, Zhou Fang, Yifan Fang, Junfu Pu, Cewu Lu, Yong-Lu Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04939">https://arxiv.org/abs/2412.04939</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04939">https://arxiv.org/pdf/2412.04939</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04939]] Verb Mirage: Unveiling and Assessing Verb Concept Hallucinations in Multimodal Large Language Models(https://arxiv.org/abs/2412.04939)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have garnered significant attention recently and demonstrate outstanding capabilities in various tasks such as OCR, VQA, captioning, $\textit{etc}$. However, hallucination remains a persistent issue. While numerous methods have been proposed to mitigate hallucinations, achieving notable improvements, these methods primarily focus on mitigating hallucinations about $\textbf{object/noun-related}$ concepts. Verb concepts, crucial for understanding human actions, have been largely overlooked. In this paper, to the best of our knowledge, we are the $\textbf{first}$ to investigate the $\textbf{verb hallucination}$ phenomenon of MLLMs from various perspectives. Our findings reveal that most state-of-the-art MLLMs suffer from severe verb hallucination. To assess the effectiveness of existing mitigation methods for object concept hallucination on verb hallucination, we evaluated these methods and found that they do not effectively address verb hallucination. To address this issue, we propose a novel rich verb knowledge-based tuning method to mitigate verb hallucination. The experiment results demonstrate that our method significantly reduces hallucinations related to verbs. $\textit{Our code and data will be made publicly available}$.</li>
</ul>

<h3>Title: A Federated Approach to Few-Shot Hate Speech Detection for Marginalized Communities</h3>
<ul>
<li><strong>Authors: </strong>Haotian Ye, Axel Wisiorek, Antonis Maronikolakis, √ñzge Ala√ßam, Hinrich Sch√ºtze</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04942">https://arxiv.org/abs/2412.04942</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04942">https://arxiv.org/pdf/2412.04942</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04942]] A Federated Approach to Few-Shot Hate Speech Detection for Marginalized Communities(https://arxiv.org/abs/2412.04942)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, robust, federate</a></li>
<li><strong>Abstract: </strong>Hate speech online remains an understudied issue for marginalized communities, and has seen rising relevance, especially in the Global South, which includes developing societies with increasing internet penetration. In this paper, we aim to provide marginalized communities living in societies where the dominant language is low-resource with a privacy-preserving tool to protect themselves from hate speech on the internet by filtering offensive content in their native languages. Our contribution in this paper is twofold: 1) we release REACT (REsponsive hate speech datasets Across ConTexts), a collection of high-quality, culture-specific hate speech detection datasets comprising seven distinct target groups in eight low-resource languages, curated by experienced data collectors; 2) we propose a solution to few-shot hate speech detection utilizing federated learning (FL), a privacy-preserving and collaborative learning approach, to continuously improve a central model that exhibits robustness when tackling different target groups and languages. By keeping the training local to the users' devices, we ensure the privacy of the users' data while benefitting from the efficiency of federated learning. Furthermore, we personalize client models to target-specific training data and evaluate their performance. Our results indicate the effectiveness of FL across different target groups, whereas the benefits of personalization on few-shot learning are not clear.</li>
</ul>

<h3>Title: HOLa: HoloLens Object Labeling</h3>
<ul>
<li><strong>Authors: </strong>Michael Schwimmbeck, Serouj Khajarian, Konstantin Holzapfel, Johannes Schmidt, Stefanie Remmele</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04945">https://arxiv.org/abs/2412.04945</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04945">https://arxiv.org/pdf/2412.04945</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04945]] HOLa: HoloLens Object Labeling(https://arxiv.org/abs/2412.04945)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In the context of medical Augmented Reality (AR) applications, object tracking is a key challenge and requires a significant amount of annotation masks. As segmentation foundation models like the Segment Anything Model (SAM) begin to emerge, zero-shot segmentation requires only minimal human participation obtaining high-quality object masks. We introduce a HoloLens-Object-Labeling (HOLa) Unity and Python application based on the SAM-Track algorithm that offers fully automatic single object annotation for HoloLens 2 while requiring minimal human participation. HOLa does not have to be adjusted to a specific image appearance and could thus alleviate AR research in any application field. We evaluate HOLa for different degrees of image complexity in open liver surgery and in medical phantom experiments. Using HOLa for image annotation can increase the labeling speed by more than 500 times while providing Dice scores between 0.875 and 0.982, which are comparable to human annotators. Our code is publicly available at: this https URL</li>
</ul>

<h3>Title: C$^2$LEVA: Toward Comprehensive and Contamination-Free Language Model Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Yanyang Li, Tin Long Wong, Cheung To Hung, Jianqiao Zhao, Duo Zheng, Ka Wai Liu, Michael R. Lyu, Liwei Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04947">https://arxiv.org/abs/2412.04947</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04947">https://arxiv.org/pdf/2412.04947</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04947]] C$^2$LEVA: Toward Comprehensive and Contamination-Free Language Model Evaluation(https://arxiv.org/abs/2412.04947)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) have shown significant promise, yet their evaluation raises concerns, particularly regarding data contamination due to the lack of access to proprietary training data. To address this issue, we present C$^2$LEVA, a comprehensive bilingual benchmark featuring systematic contamination prevention. C$^2$LEVA firstly offers a holistic evaluation encompassing 22 tasks, each targeting a specific application or ability of LLMs, and secondly a trustworthy assessment due to our contamination-free tasks, ensured by a systematic contamination prevention strategy that fully automates test data renewal and enforces data protection during benchmark data release. Our large-scale evaluation of 15 open-source and proprietary models demonstrates the effectiveness of C$^2$LEVA.</li>
</ul>

<h3>Title: KaLM: Knowledge-aligned Autoregressive Language Modeling via Dual-view Knowledge Graph Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Peng Yu, Cheng Deng, Beiya Dai, Xinbing Wang, Ying Wen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04948">https://arxiv.org/abs/2412.04948</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04948">https://arxiv.org/pdf/2412.04948</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04948]] KaLM: Knowledge-aligned Autoregressive Language Modeling via Dual-view Knowledge Graph Contrastive Learning(https://arxiv.org/abs/2412.04948)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Autoregressive large language models (LLMs) pre-trained by next token prediction are inherently proficient in generative tasks. However, their performance on knowledge-driven tasks such as factual knowledge querying remains unsatisfactory. Knowledge graphs (KGs), as high-quality structured knowledge bases, can provide reliable knowledge for LLMs, potentially compensating for their knowledge deficiencies. Aligning LLMs with explicit, structured knowledge from KGs has been a challenge; previous attempts either failed to effectively align knowledge representations or compromised the generative capabilities of LLMs, leading to less-than-optimal outcomes. This paper proposes \textbf{KaLM}, a \textit{Knowledge-aligned Language Modeling} approach, which fine-tunes autoregressive LLMs to align with KG knowledge via the joint objective of explicit knowledge alignment and implicit knowledge alignment. The explicit knowledge alignment objective aims to directly optimize the knowledge representation of LLMs through dual-view knowledge graph contrastive learning. The implicit knowledge alignment objective focuses on incorporating textual patterns of knowledge into LLMs through triple completion language modeling. Notably, our method achieves a significant performance boost in evaluations of knowledge-driven tasks, specifically embedding-based knowledge graph completion and generation-based knowledge graph question answering.</li>
</ul>

<h3>Title: Bed-Attached Vibration Sensor System: A Machine Learning Approach for Fall Detection in Nursing Homes</h3>
<ul>
<li><strong>Authors: </strong>Thomas Bartz-Beielstein, Axel Wellendorf, Noah P√ºtz, Jens Brandt, Alexander Hinterleitner, Richard Schulz, Richard Scholz, Olaf Mersmann, Robin Knabe</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04950">https://arxiv.org/abs/2412.04950</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04950">https://arxiv.org/pdf/2412.04950</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04950]] Bed-Attached Vibration Sensor System: A Machine Learning Approach for Fall Detection in Nursing Homes(https://arxiv.org/abs/2412.04950)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust</a></li>
<li><strong>Abstract: </strong>The increasing shortage of nursing staff and the acute risk of falls in nursing homes pose significant challenges for the healthcare system. This study presents the development of an automated fall detection system integrated into care beds, aimed at enhancing patient safety without compromising privacy through wearables or video monitoring. Mechanical vibrations transmitted through the bed frame are processed using a short-time Fourier transform, enabling robust classification of distinct human fall patterns with a convolutional neural network. Challenges pertaining to the quantity and diversity of the data are addressed, proposing the generation of additional data with a specific emphasis on enhancing variation. While the model shows promising results in distinguishing fall events from noise using lab data, further testing in real-world environments is recommended for validation and improvement. Despite limited available data, the proposed system shows the potential for an accurate and rapid response to falls, mitigating health implications, and addressing the needs of an aging population. This case study was performed as part of the ZIM Project. Further research on sensors enhanced by artificial intelligence will be continued in the ShapeFuture Project.</li>
</ul>

<h3>Title: Gla-AI4BioMed at RRG24: Visual Instruction-tuned Adaptation for Radiology Report Generation</h3>
<ul>
<li><strong>Authors: </strong>Xi Zhang, Zaiqiao Meng, Jake Lever, Edmond S. L. Ho</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04954">https://arxiv.org/abs/2412.04954</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04954">https://arxiv.org/pdf/2412.04954</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04954]] Gla-AI4BioMed at RRG24: Visual Instruction-tuned Adaptation for Radiology Report Generation(https://arxiv.org/abs/2412.04954)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce a radiology-focused visual language model designed to generate radiology reports from chest X-rays. Building on previous findings that large language models (LLMs) can acquire multimodal capabilities when aligned with pretrained vision encoders, we demonstrate similar potential with chest X-ray images. This integration enhances the ability of model to understand and describe chest X-ray images. Our model combines an image encoder with a fine-tuned LLM based on the Vicuna-7B architecture, enabling it to generate different sections of a radiology report with notable accuracy. The training process involves a two-stage approach: (i) initial alignment of chest X-ray features with the LLM (ii) followed by fine-tuning for radiology report generation.</li>
</ul>

<h3>Title: A Key Encapsulation Mechanism from Low Density Lattice Codes</h3>
<ul>
<li><strong>Authors: </strong>Reza Hooshmand</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04979">https://arxiv.org/abs/2412.04979</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04979">https://arxiv.org/pdf/2412.04979</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04979]] A Key Encapsulation Mechanism from Low Density Lattice Codes(https://arxiv.org/abs/2412.04979)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Key Encapsulation Mechanisms (KEMs) are a set of cryptographic techniques that are designed to provide symmetric encryption key using asymmetric mechanism (public key). In the current study, we concentrate on design and analysis of key encapsulation mechanism from low density lattice codes (KEM-LDLC) to go down the key size by keeping an acceptable level of security. The security of the proposed KEM-LDLC relies on the difficulty of solving the closest vector problem (CVP) and the shortest basis problem (SBP) of the lattices. Furthermore, this paper discusses other performance analyses results such as key size, error performance, and computational complexity, as well as conventional security analysis against applied attacks. Reducing the key size is performed by two approaches: (i) saving the generation sequence of the latin square LDLCs parity-check matrix of as a part of the secret key set; (ii) using the hermite normal form (HNF) of the latin square LDLCs generator matrix as part of the public key set. These enhancements enable us to attain greater efficiency and security compared to earlier code-based KEMs.</li>
</ul>

<h3>Title: Power Plant Detection for Energy Estimation using GIS with Remote Sensing, CNN & Vision Transformers</h3>
<ul>
<li><strong>Authors: </strong>Blessing Austin-Gabriel, Cristian Noriega Monsalve, Aparna S. Varde</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04986">https://arxiv.org/abs/2412.04986</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04986">https://arxiv.org/pdf/2412.04986</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04986]] Power Plant Detection for Energy Estimation using GIS with Remote Sensing, CNN & Vision Transformers(https://arxiv.org/abs/2412.04986)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>In this research, we propose a hybrid model for power plant detection to assist energy estimation applications, by pipelining GIS (Geographical Information Systems) having Remote Sensing capabilities with CNN (Convolutional Neural Networks) and ViT (Vision Transformers). Our proposed approach enables real-time analysis with multiple data types on a common map via the GIS, entails feature-extraction abilities due to the CNN, and captures long-range dependencies through the ViT. This hybrid approach is found to enhance classification, thus helping in the monitoring and operational management of power plants; hence assisting energy estimation and sustainable energy planning in the future. It exemplifies adequate deployment of machine learning methods in conjunction with domain-specific approaches to enhance performance.</li>
</ul>

<h3>Title: ETLNet: An Efficient TCN-BiLSTM Network for Road Anomaly Detection Using Smartphone Sensors</h3>
<ul>
<li><strong>Authors: </strong>Mohd Faiz Ansari, Rakshit Sandilya, Mohammed Javed, David Doermann</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04990">https://arxiv.org/abs/2412.04990</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04990">https://arxiv.org/pdf/2412.04990</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04990]] ETLNet: An Efficient TCN-BiLSTM Network for Road Anomaly Detection Using Smartphone Sensors(https://arxiv.org/abs/2412.04990)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Road anomalies can be defined as irregularities on the road surface or in the surface itself. Some may be intentional (such as speedbumps), accidental (such as materials falling off a truck), or the result of roads' excessive use or low or no maintenance, such as potholes. Despite their varying origins, these irregularities often harm vehicles substantially. Speed bumps are intentionally placed for safety but are dangerous due to their non-standard shape, size, and lack of proper markings. Potholes are unintentional and can also cause severe damage. To address the detection of these anomalies, we need an automated road monitoring system. Today, various systems exist that use visual information to track these anomalies. Still, due to poor lighting conditions and improper or missing markings, they may go undetected and have severe consequences for public transport, automated vehicles, etc. In this paper, the Enhanced Temporal-BiLSTM Network (ETLNet) is introduced as a novel approach that integrates two Temporal Convolutional Network (TCN) layers with a Bidirectional Long Short-Term Memory (BiLSTM) layer. This combination is tailored to detect anomalies effectively irrespective of lighting conditions, as it depends not on visuals but smartphone inertial sensor data. Our methodology employs accelerometer and gyroscope sensors, typically in smartphones, to gather data on road conditions. Empirical evaluations demonstrate that the ETLNet model maintains an F1-score for detecting speed bumps of 99.3%. The ETLNet model's robustness and efficiency significantly advance automated road surface monitoring technologies.</li>
</ul>

<h3>Title: Noise Matters: Diffusion Model-based Urban Mobility Generation with Collaborative Noise Priors</h3>
<ul>
<li><strong>Authors: </strong>Yuheng Zhang, Yuan Yuan, Jingtao Ding, Jian Yuan, Yong Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05000">https://arxiv.org/abs/2412.05000</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05000">https://arxiv.org/pdf/2412.05000</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05000]] Noise Matters: Diffusion Model-based Urban Mobility Generation with Collaborative Noise Priors(https://arxiv.org/abs/2412.05000)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, protect, diffusion</a></li>
<li><strong>Abstract: </strong>With global urbanization, the focus on sustainable cities has largely grown, driving research into equity, resilience, and urban planning, which often relies on mobility data. The rise of web-based apps and mobile devices has provided valuable user data for mobility-related research. However, real-world mobility data is costly and raises privacy concerns. To protect privacy while retaining key features of real-world movement, the demand for synthetic data has steadily increased. Recent advances in diffusion models have shown great potential for mobility trajectory generation due to their ability to model randomness and uncertainty. However, existing approaches often directly apply identically distributed (i.i.d.) noise sampling from image generation techniques, which fail to account for the spatiotemporal correlations and social interactions that shape urban mobility patterns. In this paper, we propose CoDiffMob, a diffusion method for urban mobility generation with collaborative noise priors, we emphasize the critical role of noise in diffusion models for generating mobility data. By leveraging both individual movement characteristics and population-wide dynamics, we construct novel collaborative noise priors that provide richer and more informative guidance throughout the generation process. Extensive experiments demonstrate the superiority of our method, with generated data accurately capturing both individual preferences and collective patterns, achieving an improvement of over 32\%. Furthermore, it can effectively replace web-derived mobility data to better support downstream applications, while safeguarding user privacy and fostering a more secure and ethical web. This highlights its tremendous potential for applications in sustainable city-related research.</li>
</ul>

<h3>Title: SLayR: Scene Layout Generation with Rectified Flow</h3>
<ul>
<li><strong>Authors: </strong>Cameron Braunstein, Hevra Petekkaya, Jan Eric Lenssen, Mariya Toneva, Eddy Ilg</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05003">https://arxiv.org/abs/2412.05003</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05003">https://arxiv.org/pdf/2412.05003</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05003]] SLayR: Scene Layout Generation with Rectified Flow(https://arxiv.org/abs/2412.05003)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We introduce SLayR, Scene Layout Generation with Rectified flow. State-of-the-art text-to-image models achieve impressive results. However, they generate images end-to-end, exposing no fine-grained control over the process. SLayR presents a novel transformer-based rectified flow model for layout generation over a token space that can be decoded into bounding boxes and corresponding labels, which can then be transformed into images using existing models. We show that established metrics for generated images are inconclusive for evaluating their underlying scene layout, and introduce a new benchmark suite, including a carefully designed repeatable human-evaluation procedure that assesses the plausibility and variety of generated layouts. In contrast to previous works, which perform well in either high variety or plausibility, we show that our approach performs well on both of these axes at the same time. It is also at least 5x times smaller in the number of parameters and 37% faster than the baselines. Our complete text-to-image pipeline demonstrates the added benefits of an interpretable and editable intermediate representation.</li>
</ul>

<h3>Title: Prompt Transfer for Dual-Aspect Cross Domain Cognitive Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Fei Liu, Yizhong Zhang, Shuochen Liu, Shengwei Ji, Kui Yu, Le Wu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05004">https://arxiv.org/abs/2412.05004</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05004">https://arxiv.org/pdf/2412.05004</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05004]] Prompt Transfer for Dual-Aspect Cross Domain Cognitive Diagnosis(https://arxiv.org/abs/2412.05004)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Cognitive Diagnosis (CD) aims to evaluate students' cognitive states based on their interaction data, enabling downstream applications such as exercise recommendation and personalized learning guidance. However, existing methods often struggle with accuracy drops in cross-domain cognitive diagnosis (CDCD), a practical yet challenging task. While some efforts have explored exercise-aspect CDCD, such as crosssubject scenarios, they fail to address the broader dual-aspect nature of CDCD, encompassing both student- and exerciseaspect variations. This diversity creates significant challenges in developing a scenario-agnostic framework. To address these gaps, we propose PromptCD, a simple yet effective framework that leverages soft prompt transfer for cognitive diagnosis. PromptCD is designed to adapt seamlessly across diverse CDCD scenarios, introducing PromptCD-S for student-aspect CDCD and PromptCD-E for exercise-aspect CDCD. Extensive experiments on real-world datasets demonstrate the robustness and effectiveness of PromptCD, consistently achieving superior performance across various CDCD scenarios. Our work offers a unified and generalizable approach to CDCD, advancing both theoretical and practical understanding in this critical domain. The implementation of our framework is publicly available at this https URL.</li>
</ul>

<h3>Title: Backdooring Outlier Detection Methods: A Novel Attack Approach</h3>
<ul>
<li><strong>Authors: </strong>ZeinabSadat Taghavi, Hossein Mirzaei</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05010">https://arxiv.org/abs/2412.05010</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05010">https://arxiv.org/pdf/2412.05010</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05010]] Backdooring Outlier Detection Methods: A Novel Attack Approach(https://arxiv.org/abs/2412.05010)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>There have been several efforts in backdoor attacks, but these have primarily focused on the closed-set performance of classifiers (i.e., classification). This has left a gap in addressing the threat to classifiers' open-set performance, referred to as outlier detection in the literature. Reliable outlier detection is crucial for deploying classifiers in critical real-world applications such as autonomous driving and medical image analysis. First, we show that existing backdoor attacks fall short in affecting the open-set performance of classifiers, as they have been specifically designed to confuse intra-closed-set decision boundaries. In contrast, an effective backdoor attack for outlier detection needs to confuse the decision boundary between the closed and open sets. Motivated by this, in this study, we propose BATOD, a novel Backdoor Attack targeting the Outlier Detection task. Specifically, we design two categories of triggers to shift inlier samples to outliers and vice versa. We evaluate BATOD using various real-world datasets and demonstrate its superior ability to degrade the open-set performance of classifiers compared to previous attacks, both before and after applying defenses.</li>
</ul>

<h3>Title: SAMCL: Empowering SAM to Continually Learn from Dynamic Domains</h3>
<ul>
<li><strong>Authors: </strong>Zeqing Wang, Kangye Ji, Di Wang, Fei Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05012">https://arxiv.org/abs/2412.05012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05012">https://arxiv.org/pdf/2412.05012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05012]] SAMCL: Empowering SAM to Continually Learn from Dynamic Domains(https://arxiv.org/abs/2412.05012)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Segment Anything Model (SAM) struggles with segmenting objects in the open world, especially across diverse and dynamic domains. Continual segmentation (CS) is a potential technique to solve this issue, but a significant obstacle is the intractable balance between previous domains (stability) and new domains (plasticity) during CS. Furthermore, how to utilize two kinds of features of SAM, images and prompts, in an efficient and effective CS manner remains a significant hurdle. In this work, we propose a novel CS method, termed SAMCL, to address these challenges. It is the first study to empower SAM with the CS ability across dynamic domains. SAMCL decouples stability and plasticity during CS by two components: $\textit{AugModule}$ and $\textit{Module Selector}$. Specifically, SAMCL leverages individual $\textit{AugModule}$ to effectively and efficiently learn new relationships between images and prompts in each domain. $\textit{Module Selector}$ selects the appropriate module during testing, based on the inherent ability of SAM to distinguish between different domains. These two components enable SAMCL to realize a task-agnostic method without any interference across different domains. Experimental results demonstrate that SAMCL outperforms state-of-the-art methods, achieving an exceptionally low average forgetting of just $0.5$%, along with at least a $2.5$% improvement in transferring to unseen domains. Moreover, the tunable parameter consumption in AugModule is about $0.236$MB, marking at least a $23.3$% reduction compared to other fine-tuning methods.</li>
</ul>

<h3>Title: ReF-LDM: A Latent Diffusion Model for Reference-based Face Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Chi-Wei Hsiao, Yu-Lun Liu, Cheng-Kun Yang, Sheng-Po Kuo, Kevin Jou, Chia-Ping Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05043">https://arxiv.org/abs/2412.05043</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05043">https://arxiv.org/pdf/2412.05043</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05043]] ReF-LDM: A Latent Diffusion Model for Reference-based Face Image Restoration(https://arxiv.org/abs/2412.05043)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>While recent works on blind face image restoration have successfully produced impressive high-quality (HQ) images with abundant details from low-quality (LQ) input images, the generated content may not accurately reflect the real appearance of a person. To address this problem, incorporating well-shot personal images as additional reference inputs could be a promising strategy. Inspired by the recent success of the Latent Diffusion Model (LDM), we propose ReF-LDM, an adaptation of LDM designed to generate HQ face images conditioned on one LQ image and multiple HQ reference images. Our model integrates an effective and efficient mechanism, CacheKV, to leverage the reference images during the generation process. Additionally, we design a timestep-scaled identity loss, enabling our LDM-based model to focus on learning the discriminating features of human faces. Lastly, we construct FFHQ-Ref, a dataset consisting of 20,405 high-quality (HQ) face images with corresponding reference images, which can serve as both training and evaluation data for reference-based face restoration models.</li>
</ul>

<h3>Title: BimArt: A Unified Approach for the Synthesis of 3D Bimanual Interaction with Articulated Objects</h3>
<ul>
<li><strong>Authors: </strong>Wanyue Zhang, Rishabh Dabral, Vladislav Golyanik, Vasileios Choutas, Eduardo Alvarado, Thabo Beeler, Marc Habermann, Christian Theobalt</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05066">https://arxiv.org/abs/2412.05066</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05066">https://arxiv.org/pdf/2412.05066</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05066]] BimArt: A Unified Approach for the Synthesis of 3D Bimanual Interaction with Articulated Objects(https://arxiv.org/abs/2412.05066)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present BimArt, a novel generative approach for synthesizing 3D bimanual hand interactions with articulated objects. Unlike prior works, we do not rely on a reference grasp, a coarse hand trajectory, or separate modes for grasping and articulating. To achieve this, we first generate distance-based contact maps conditioned on the object trajectory with an articulation-aware feature representation, revealing rich bimanual patterns for manipulation. The learned contact prior is then used to guide our hand motion generator, producing diverse and realistic bimanual motions for object movement and articulation. Our work offers key insights into feature representation and contact prior for articulated objects, demonstrating their effectiveness in taming the complex, high-dimensional space of bimanual hand-object interactions. Through comprehensive quantitative experiments, we demonstrate a clear step towards simplified and high-quality hand-object animations that excel over the state-of-the-art in motion quality and diversity.</li>
</ul>

<h3>Title: LoFi: Vision-Aided Label Generator for Wi-Fi Localization and Tracking</h3>
<ul>
<li><strong>Authors: </strong>Zijian Zhao, Tingwei Chen, Fanyi Meng, Zhijie Cai, Hang Li, Xiaoyang Li, Guangxu Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05074">https://arxiv.org/abs/2412.05074</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05074">https://arxiv.org/pdf/2412.05074</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05074]] LoFi: Vision-Aided Label Generator for Wi-Fi Localization and Tracking(https://arxiv.org/abs/2412.05074)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Wi-Fi localization and tracking has shown immense potential due to its privacy-friendliness, wide coverage, permeability, independence from lighting conditions, and low cost. Current methods can be broadly categorized as model-based and data-driven approaches, where data-driven methods show better performance and have less requirement for specialized devices, but struggle with limited datasets for training. Due to limitations in current data collection methods, most datasets only provide coarse-grained ground truth (GT) or limited amount of label points, which greatly hinders the development of data-driven methods. Even though lidar can provide accurate GT, their high cost makes them inaccessible to many users. To address these challenges, we propose LoFi, a vision-aided label generator for Wi-Fi localization and tracking, which can generate ground truth position coordinates solely based on 2D images. The easy and quick data collection method also helps data-driven based methods deploy in practice, since Wi-Fi is a low-generalization modality and when using relevant methods, it always requires fine-tuning the model using newly collected data. Based on our method, we also collect a Wi-Fi tracking and localization dataset using ESP32-S3 and a webcam. To facilitate future research, we will make our code and dataset publicly available upon publication.</li>
</ul>

<h3>Title: Improving analytical color and texture similarity estimation methods for dataset-agnostic person reidentification</h3>
<ul>
<li><strong>Authors: </strong>Nikita Gabdullin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05076">https://arxiv.org/abs/2412.05076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05076">https://arxiv.org/pdf/2412.05076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05076]] Improving analytical color and texture similarity estimation methods for dataset-agnostic person reidentification(https://arxiv.org/abs/2412.05076)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>This paper studies a combined person reidentification (re-id) method that uses human parsing, analytical feature extraction and similarity estimation schemes. One of its prominent features is its low computational requirements so it can be implemented on edge devices. The method allows direct comparison of specific image regions using interpretable features which consist of color and texture channels. It is proposed to analyze and compare colors in CIE-Lab color space using histogram smoothing for noise reduction. A novel pre-configured latent space (LS) supervised autoencoder (SAE) is proposed for texture analysis which encodes input textures as LS points. This allows to obtain more accurate similarity measures compared to simplistic label comparison. The proposed method also does not rely upon photos or other re-id data for training, which makes it completely re-id dataset-agnostic. The viability of the proposed method is verified by computing rank-1, rank-10, and mAP re-id metrics on Market1501 dataset. The results are comparable to those of conventional deep learning methods and the potential ways to further improve the method are discussed.</li>
</ul>

<h3>Title: The Silent Prompt: Initial Noise as Implicit Guidance for Goal-Driven Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Ruoyu Wang, Huayang Huang, Ye Zhu, Olga Russakovsky, Yu Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05101">https://arxiv.org/abs/2412.05101</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05101">https://arxiv.org/pdf/2412.05101</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05101]] The Silent Prompt: Initial Noise as Implicit Guidance for Goal-Driven Image Generation(https://arxiv.org/abs/2412.05101)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Text-to-image synthesis (T2I) has advanced remarkably with the emergence of large-scale diffusion models. In the conventional setup, the text prompt provides explicit, user-defined guidance, directing the generation process by denoising a randomly sampled Gaussian noise. In this work, we reveal that the often-overlooked noise itself encodes inherent generative tendencies, acting as a "silent prompt" that implicitly guides the output. This implicit guidance, embedded in the noise scheduler design of diffusion model formulations and their training stages, generalizes across a wide range of T2I models and backbones. Building on this insight, we introduce NoiseQuery, a novel strategy that selects optimal initial noise from a pre-built noise library to meet diverse user needs. Our approach not only enhances high-level semantic alignment with text prompts, but also allows for nuanced adjustments of low-level visual attributes, such as texture, sharpness, shape, and color, which are typically challenging to control through text alone. Extensive experiments across various models and target attributes demonstrate the strong performance and zero-shot transferability of our approach, requiring no additional optimization.</li>
</ul>

<h3>Title: Transformers Can Navigate Mazes With Multi-Step Prediction</h3>
<ul>
<li><strong>Authors: </strong>Niklas Nolte, Ouail Kitouni, Adina Williams, Mike Rabbat, Mark Ibrahim</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05117">https://arxiv.org/abs/2412.05117</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05117">https://arxiv.org/pdf/2412.05117</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05117]] Transformers Can Navigate Mazes With Multi-Step Prediction(https://arxiv.org/abs/2412.05117)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Despite their remarkable success in language modeling, transformers trained to predict the next token in a sequence struggle with long-term planning. This limitation is particularly evident in tasks requiring foresight to plan multiple steps ahead such as maze navigation. The standard next single token prediction objective, however, offers no explicit mechanism to predict multiple steps ahead - or revisit the path taken so far. Consequently, in this work we study whether explicitly predicting multiple steps ahead (and backwards) can improve transformers' maze navigation. We train parameter-matched transformers from scratch, under identical settings, to navigate mazes of varying types and sizes with standard next token prediction and MLM-U, an objective explicitly predicting multiple steps ahead and backwards. We find that MLM-U considerably improves transformers' ability to navigate mazes compared to standard next token prediction across maze types and complexities. We also find MLM-U training is 4x more sample efficient and converges 2x faster in terms of GPU training hours relative to next token training. Finally, for more complex mazes we find MLM-U benefits from scaling to larger transformers. Remarkably, we find transformers trained with MLM-U outperform larger transformers trained with next token prediction using additional supervision from A* search traces. We hope these findings underscore the promise of learning objectives to advance transformers' capacity for long-term planning.</li>
</ul>

<h3>Title: Robust Computation with Intrinsic Heterogeneity</h3>
<ul>
<li><strong>Authors: </strong>Arash Golmohammadi, Christian Tetzlaff</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05126">https://arxiv.org/abs/2412.05126</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05126">https://arxiv.org/pdf/2412.05126</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05126]] Robust Computation with Intrinsic Heterogeneity(https://arxiv.org/abs/2412.05126)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Intrinsic within-type neuronal heterogeneity is a ubiquitous feature of biological systems, with well-documented computational advantages. Recent works in machine learning have incorporated such diversities by optimizing neuronal parameters alongside synaptic connections and demonstrated state-of-the-art performance across common benchmarks. However, this performance gain comes at the cost of significantly higher computational costs, imposed by a larger parameter space. Furthermore, it is unclear how the neuronal parameters, constrained by the biophysics of their surroundings, are globally orchestrated to minimize top-down errors. To address these challenges, we postulate that neurons are intrinsically diverse, and investigate the computational capabilities of such heterogeneous neuronal parameters. Our results show that intrinsic heterogeneity, viewed as a fixed quenched disorder, often substantially improves performance across hundreds of temporal tasks. Notably, smaller but heterogeneous networks outperform larger homogeneous networks, despite consuming less data. We elucidate the underlying mechanisms driving this performance boost and illustrate its applicability to both rate and spiking dynamics. Moreover, our findings demonstrate that heterogeneous networks are highly resilient to severe alterations in their recurrent synaptic hyperparameters, and even recurrent connections removal does not compromise performance. The remarkable effectiveness of heterogeneous networks with small sizes and relaxed connectivity is particularly relevant for the neuromorphic community, which faces challenges due to device-to-device variability. Furthermore, understanding the mechanism of robust computation with heterogeneity also benefits neuroscientists and machine learners.</li>
</ul>

<h3>Title: Learning Hidden Physics and System Parameters with Deep Operator Networks</h3>
<ul>
<li><strong>Authors: </strong>Vijay Kag, Dibakar Roy Sarkar, Birupaksha Pal, Somdatta Goswami</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05133">https://arxiv.org/abs/2412.05133</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05133">https://arxiv.org/pdf/2412.05133</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05133]] Learning Hidden Physics and System Parameters with Deep Operator Networks(https://arxiv.org/abs/2412.05133)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Big data is transforming scientific progress by enabling the discovery of novel models, enhancing existing frameworks, and facilitating precise uncertainty quantification, while advancements in scientific machine learning complement this by providing powerful tools to solve inverse problems to identify the complex systems where traditional methods falter due to sparse or noisy data. We introduce two innovative neural operator frameworks tailored for discovering hidden physics and identifying unknown system parameters from sparse measurements. The first framework integrates a popular neural operator, DeepONet, and a physics-informed neural network to capture the relationship between sparse data and the underlying physics, enabling the accurate discovery of a family of governing equations. The second framework focuses on system parameter identification, leveraging a DeepONet pre-trained on sparse sensor measurements to initialize a physics-constrained inverse model. Both frameworks excel in handling limited data and preserving physical consistency. Benchmarking on the Burgers' equation and reaction-diffusion system demonstrates state-of-the-art performance, achieving average $L_2$ errors of $\mathcal{O}(10^{-2})$ for hidden physics discovery and absolute errors of $\mathcal{O}(10^{-3})$ for parameter identification. These results underscore the frameworks' robustness, efficiency, and potential for solving complex scientific problems with minimal observational data.</li>
</ul>

<h3>Title: How to Squeeze An Explanation Out of Your Model</h3>
<ul>
<li><strong>Authors: </strong>Tiago Roxo, Joana C. Costa, Pedro R. M. In√°cio, Hugo Proen√ßa</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05134">https://arxiv.org/abs/2412.05134</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05134">https://arxiv.org/pdf/2412.05134</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05134]] How to Squeeze An Explanation Out of Your Model(https://arxiv.org/abs/2412.05134)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, biometric, interpretability</a></li>
<li><strong>Abstract: </strong>Deep learning models are widely used nowadays for their reliability in performing various tasks. However, they do not typically provide the reasoning behind their decision, which is a significant drawback, particularly for more sensitive areas such as biometrics, security and healthcare. The most commonly used approaches to provide interpretability create visual attention heatmaps of regions of interest on an image based on models gradient backpropagation. Although this is a viable approach, current methods are targeted toward image settings and default/standard deep learning models, meaning that they require significant adaptations to work on video/multi-modal settings and custom architectures. This paper proposes an approach for interpretability that is model-agnostic, based on a novel use of the Squeeze and Excitation (SE) block that creates visual attention heatmaps. By including an SE block prior to the classification layer of any model, we are able to retrieve the most influential features via SE vector manipulation, one of the key components of the SE block. Our results show that this new SE-based interpretability can be applied to various models in image and video/multi-modal settings, namely biometrics of facial features with CelebA and behavioral biometrics using Active Speaker Detection datasets. Furthermore, our proposal does not compromise model performance toward the original task, and has competitive results with current interpretability approaches in state-of-the-art object datasets, highlighting its robustness to perform in varying data aside from the biometric context.</li>
</ul>

<h3>Title: Supply Chain Insecurity: The Lack of Integrity Protection in SBOM Solutions</h3>
<ul>
<li><strong>Authors: </strong>Can Ozkan, Xinhai Zhou, Dave Singelee</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05138">https://arxiv.org/abs/2412.05138</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05138">https://arxiv.org/pdf/2412.05138</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05138]] Supply Chain Insecurity: The Lack of Integrity Protection in SBOM Solutions(https://arxiv.org/abs/2412.05138)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack</a></li>
<li><strong>Abstract: </strong>The SolarWinds attack that exploited weaknesses in the software update mechanism highlights the critical need for organizations to have better visibility into their software dependencies and potential vulnerabilities associated with them, and the Software Bill of Materials (SBOM) is paramount in ensuring software supply chain security. Under the Executive Order issued by President Biden, the adoption of the SBOM has become obligatory within the United States. The executive order mandates that an SBOM should be provided for all software purchased by federal agencies. The main applications of SBOMs are vulnerability management and license management. This work presents an in-depth and systematic investigation into the integrity of SBOMs. We explore different attack vectors that can be exploited to manipulate SBOM data, including flaws in the SBOM generation and consumption phases in the SBOM life cycle. We thoroughly investigated four SBOM consumption tools and the generation process of SBOMs for seven prominent programming languages. Our systematic investigation reveals that the tools used for consumption lack integrity control mechanisms for dependencies. Similarly, the generation process is susceptible to integrity attacks as well, by manipulating dependency version numbers in package managers and additional files, resulting in incorrect SBOM data. This could lead to incorrect views on software dependencies and vulnerabilities being overlooked during SBOM consumption. To mitigate these issues, we propose a solution incorporating the decentralized storage of hash values of software libraries.</li>
</ul>

<h3>Title: A Practical Examination of AI-Generated Text Detectors for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Brian Tufts, Xuandong Zhao, Lei Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05139">https://arxiv.org/abs/2412.05139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05139">https://arxiv.org/pdf/2412.05139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05139]] A Practical Examination of AI-Generated Text Detectors for Large Language Models(https://arxiv.org/abs/2412.05139)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>The proliferation of large language models has raised growing concerns about their misuse, particularly in cases where AI-generated text is falsely attributed to human authors. Machine-generated content detectors claim to effectively identify such text under various conditions and from any language model. This paper critically evaluates these claims by assessing several popular detectors (RADAR, Wild, T5Sentinel, Fast-DetectGPT, GPTID, LogRank, Binoculars) on a range of domains, datasets, and models that these detectors have not previously encountered. We employ various prompting strategies to simulate adversarial attacks, demonstrating that even moderate efforts can significantly evade detection. We emphasize the importance of the true positive rate at a specific false positive rate (TPR@FPR) metric and demonstrate that these detectors perform poorly in certain settings, with TPR@.01 as low as 0\%. Our findings suggest that both trained and zero-shot detectors struggle to maintain high sensitivity while achieving a reasonable true positive rate.</li>
</ul>

<h3>Title: Explingo: Explaining AI Predictions using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Alexandra Zytek, Sara Pido, Sarah Alnegheimish, Laure Berti-Equille, Kalyan Veeramachaneni</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05145">https://arxiv.org/abs/2412.05145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05145">https://arxiv.org/pdf/2412.05145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05145]] Explingo: Explaining AI Predictions using Large Language Models(https://arxiv.org/abs/2412.05145)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Explanations of machine learning (ML) model predictions generated by Explainable AI (XAI) techniques such as SHAP are essential for people using ML outputs for decision-making. We explore the potential of Large Language Models (LLMs) to transform these explanations into human-readable, narrative formats that align with natural communication. We address two key research questions: (1) Can LLMs reliably transform traditional explanations into high-quality narratives? and (2) How can we effectively evaluate the quality of narrative explanations? To answer these questions, we introduce Explingo, which consists of two LLM-based subsystems, a Narrator and Grader. The Narrator takes in ML explanations and transforms them into natural-language descriptions. The Grader scores these narratives on a set of metrics including accuracy, completeness, fluency, and conciseness. Our experiments demonstrate that LLMs can generate high-quality narratives that achieve high scores across all metrics, particularly when guided by a small number of human-labeled and bootstrapped examples. We also identified areas that remain challenging, in particular for effectively scoring narratives in complex domains. The findings from this work have been integrated into an open-source tool that makes narrative explanations available for further applications.</li>
</ul>

<h3>Title: LoRA.rar: Learning to Merge LoRAs via Hypernetworks for Subject-Style Conditioned Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Donald Shenaj, Ondrej Bohdal, Mete Ozay, Pietro Zanuttigh, Umberto Michieli</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05148">https://arxiv.org/abs/2412.05148</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05148">https://arxiv.org/pdf/2412.05148</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05148]] LoRA.rar: Learning to Merge LoRAs via Hypernetworks for Subject-Style Conditioned Image Generation(https://arxiv.org/abs/2412.05148)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in image generation models have enabled personalized image creation with both user-defined subjects (content) and styles. Prior works achieved personalization by merging corresponding low-rank adaptation parameters (LoRAs) through optimization-based methods, which are computationally demanding and unsuitable for real-time use on resource-constrained devices like smartphones. To address this, we introduce this http URL, a method that not only improves image quality but also achieves a remarkable speedup of over $4000\times$ in the merging process. this http URL pre-trains a hypernetwork on a diverse set of content-style LoRA pairs, learning an efficient merging strategy that generalizes to new, unseen content-style pairs, enabling fast, high-quality personalization. Moreover, we identify limitations in existing evaluation metrics for content-style quality and propose a new protocol using multimodal large language models (MLLM) for more accurate assessment. Our method significantly outperforms the current state of the art in both content and style fidelity, as validated by MLLM assessments and human evaluations.</li>
</ul>

<h3>Title: BIAS: A Body-based Interpretable Active Speaker Approach</h3>
<ul>
<li><strong>Authors: </strong>Tiago Roxo, Joana C. Costa, Pedro R. M. In√°cio, Hugo Proen√ßa</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05150">https://arxiv.org/abs/2412.05150</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05150">https://arxiv.org/pdf/2412.05150</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05150]] BIAS: A Body-based Interpretable Active Speaker Approach(https://arxiv.org/abs/2412.05150)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>State-of-the-art Active Speaker Detection (ASD) approaches heavily rely on audio and facial features to perform, which is not a sustainable approach in wild scenarios. Although these methods achieve good results in the standard AVA-ActiveSpeaker set, a recent wilder ASD dataset (WASD) showed the limitations of such models and raised the need for new approaches. As such, we propose BIAS, a model that, for the first time, combines audio, face, and body information, to accurately predict active speakers in varying/challenging conditions. Additionally, we design BIAS to provide interpretability by proposing a novel use for Squeeze-and-Excitation blocks, namely in attention heatmaps creation and feature importance assessment. For a full interpretability setup, we annotate an ASD-related actions dataset (ASD-Text) to finetune a ViT-GPT2 for text scene description to complement BIAS interpretability. The results show that BIAS is state-of-the-art in challenging conditions where body-based features are of utmost importance (Columbia, open-settings, and WASD), and yields competitive results in AVA-ActiveSpeaker, where face is more influential than body for ASD. BIAS interpretability also shows the features/aspects more relevant towards ASD prediction in varying settings, making it a strong baseline for further developments in interpretable ASD models, and is available at this https URL.</li>
</ul>

<h3>Title: Navigating Shortcuts, Spurious Correlations, and Confounders: From Origins via Detection to Mitigation</h3>
<ul>
<li><strong>Authors: </strong>David Steinmann, Felix Divo, Maurice Kraus, Antonia W√ºst, Lukas Struppek, Felix Friedrich, Kristian Kersting</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05152">https://arxiv.org/abs/2412.05152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05152">https://arxiv.org/pdf/2412.05152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05152]] Navigating Shortcuts, Spurious Correlations, and Confounders: From Origins via Detection to Mitigation(https://arxiv.org/abs/2412.05152)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust</a></li>
<li><strong>Abstract: </strong>Shortcuts, also described as Clever Hans behavior, spurious correlations, or confounders, present a significant challenge in machine learning and AI, critically affecting model generalization and robustness. Research in this area, however, remains fragmented across various terminologies, hindering the progress of the field as a whole. Consequently, we introduce a unifying taxonomy of shortcut learning by providing a formal definition of shortcuts and bridging the diverse terms used in the literature. In doing so, we further establish important connections between shortcuts and related fields, including bias, causality, and security, where parallels exist but are rarely discussed. Our taxonomy organizes existing approaches for shortcut detection and mitigation, providing a comprehensive overview of the current state of the field and revealing underexplored areas and open challenges. Moreover, we compile and classify datasets tailored to study shortcut learning. Altogether, this work provides a holistic perspective to deepen understanding and drive the development of more effective strategies for addressing shortcuts in machine learning.</li>
</ul>

<h3>Title: A text-to-tabular approach to generate synthetic patient data using LLMs</h3>
<ul>
<li><strong>Authors: </strong>Margaux Tornqvist, Jean-Daniel Zucker, Tristan Fauvel, Nicolas Lambert, Mathilde Berthelot, Antoine Movschin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05153">https://arxiv.org/abs/2412.05153</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05153">https://arxiv.org/pdf/2412.05153</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05153]] A text-to-tabular approach to generate synthetic patient data using LLMs(https://arxiv.org/abs/2412.05153)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Access to large-scale high-quality healthcare databases is key to accelerate medical research and make insightful discoveries about diseases. However, access to such data is often limited by patient privacy concerns, data sharing restrictions and high costs. To overcome these limitations, synthetic patient data has emerged as an alternative. However, synthetic data generation (SDG) methods typically rely on machine learning (ML) models trained on original data, leading back to the data scarcity problem. We propose an approach to generate synthetic tabular patient data that does not require access to the original data, but only a description of the desired database. We leverage prior medical knowledge and in-context learning capabilities of large language models (LLMs) to generate realistic patient data, even in a low-resource setting. We quantitatively evaluate our approach against state-of-the-art SDG models, using fidelity, privacy, and utility metrics. Our results show that while LLMs may not match the performance of state-of-the-art models trained on the original data, they effectively generate realistic patient data with well-preserved clinical correlations. An ablation study highlights key elements of our prompt contributing to high-quality synthetic patient data generation. This approach, which is easy to use and does not require original data or advanced ML skills, is particularly valuable for quickly generating custom-designed patient data, supporting project implementation and providing educational resources.</li>
</ul>

<h3>Title: Towards Flexible 3D Perception: Object-Centric Occupancy Completion Augments 3D Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Chaoda Zheng, Feng Wang, Naiyan Wang, Shuguang Cui, Zhen Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05154">https://arxiv.org/abs/2412.05154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05154">https://arxiv.org/pdf/2412.05154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05154]] Towards Flexible 3D Perception: Object-Centric Occupancy Completion Augments 3D Object Detection(https://arxiv.org/abs/2412.05154)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>While 3D object bounding box (bbox) representation has been widely used in autonomous driving perception, it lacks the ability to capture the precise details of an object's intrinsic geometry. Recently, occupancy has emerged as a promising alternative for 3D scene perception. However, constructing a high-resolution occupancy map remains infeasible for large scenes due to computational constraints. Recognizing that foreground objects only occupy a small portion of the scene, we introduce object-centric occupancy as a supplement to object bboxes. This representation not only provides intricate details for detected objects but also enables higher voxel resolution in practical applications. We advance the development of object-centric occupancy perception from both data and algorithm perspectives. On the data side, we construct the first object-centric occupancy dataset from scratch using an automated pipeline. From the algorithmic standpoint, we introduce a novel object-centric occupancy completion network equipped with an implicit shape decoder that manages dynamic-size occupancy generation. This network accurately predicts the complete object-centric occupancy volume for inaccurate object proposals by leveraging temporal information from long sequences. Our method demonstrates robust performance in completing object shapes under noisy detection and tracking conditions. Additionally, we show that our occupancy features significantly enhance the detection results of state-of-the-art 3D object detectors, especially for incomplete or distant objects in the Waymo Open Dataset.</li>
</ul>

<h3>Title: Gaining Explainability from a CNN for Stereotype Detection Based on Mice Stopping Behavior</h3>
<ul>
<li><strong>Authors: </strong>Raul Alfredo de Sousa Silva, Yasmine Belaidouni, Rabah Iguernaissi, Djamal Merad, S√©verine Dubuisson</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05158">https://arxiv.org/abs/2412.05158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05158">https://arxiv.org/pdf/2412.05158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05158]] Gaining Explainability from a CNN for Stereotype Detection Based on Mice Stopping Behavior(https://arxiv.org/abs/2412.05158)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Understanding the behavior of laboratory animals is a key to find answers about diseases and neurodevelopmental disorders that also affects humans. One behavior of interest is the stopping, as it correlates with exploration, feeding and sleeping habits of individuals. To improve comprehension of animal's behavior, we focus on identifying trait revealing age/sex of mice through the series of stopping spots of each individual. We track 4 mice using LiveMouseTracker (LMT) system during 3 days. Then, we build a stack of 2D histograms of the stop positions. This stack of histograms passes through a shallow CNN architecture to classify mice in terms of age and sex. We observe that female mice show more recognizable behavioral patterns, reaching a classification accuracy of more than 90%, while males, which do not present as many distinguishable patterns, reach an accuracy of 62.5%. To gain explainability from the model, we look at the activation function of the convolutional layers and found that some regions of the cage are preferentially explored by females. Males, especially juveniles, present behavior patterns that oscillate between juvenile female and adult male.</li>
</ul>

<h3>Title: DNF: Unconditional 4D Generation with Dictionary-based Neural Fields</h3>
<ul>
<li><strong>Authors: </strong>Xinyi Zhang, Naiqi Li, Angela Dai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05161">https://arxiv.org/abs/2412.05161</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05161">https://arxiv.org/pdf/2412.05161</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05161]] DNF: Unconditional 4D Generation with Dictionary-based Neural Fields(https://arxiv.org/abs/2412.05161)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>While remarkable success has been achieved through diffusion-based 3D generative models for shapes, 4D generative modeling remains challenging due to the complexity of object deformations over time. We propose DNF, a new 4D representation for unconditional generative modeling that efficiently models deformable shapes with disentangled shape and motion while capturing high-fidelity details in the deforming objects. To achieve this, we propose a dictionary learning approach to disentangle 4D motion from shape as neural fields. Both shape and motion are represented as learned latent spaces, where each deformable shape is represented by its shape and motion global latent codes, shape-specific coefficient vectors, and shared dictionary information. This captures both shape-specific detail and global shared information in the learned dictionary. Our dictionary-based representation well balances fidelity, contiguity and compression -- combined with a transformer-based diffusion model, our method is able to generate effective, high-fidelity 4D animations.</li>
</ul>

<h3>Title: A Differentially Private Kaplan-Meier Estimator for Privacy-Preserving Survival Analysis</h3>
<ul>
<li><strong>Authors: </strong>Narasimha Raghavan Veeraragavan, Sai Praneeth Karimireddy, Jan Franz Nyg√•rd</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05164">https://arxiv.org/abs/2412.05164</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05164">https://arxiv.org/pdf/2412.05164</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05164]] A Differentially Private Kaplan-Meier Estimator for Privacy-Preserving Survival Analysis(https://arxiv.org/abs/2412.05164)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, membership infer</a></li>
<li><strong>Abstract: </strong>This paper presents a differentially private approach to Kaplan-Meier estimation that achieves accurate survival probability estimates while safeguarding individual privacy. The Kaplan-Meier estimator is widely used in survival analysis to estimate survival functions over time, yet applying it to sensitive datasets, such as clinical records, risks revealing private information. To address this, we introduce a novel algorithm that applies time-indexed Laplace noise, dynamic clipping, and smoothing to produce a privacy-preserving survival curve while maintaining the cumulative structure of the Kaplan-Meier estimator. By scaling noise over time, the algorithm accounts for decreasing sensitivity as fewer individuals remain at risk, while dynamic clipping and smoothing prevent extreme values and reduce fluctuations, preserving the natural shape of the survival curve. Our results, evaluated on the NCCTG lung cancer dataset, show that the proposed method effectively lowers root mean squared error (RMSE) and enhances accuracy across privacy budgets ($\epsilon$). At $\epsilon = 10$, the algorithm achieves an RMSE as low as 0.04, closely approximating non-private estimates. Additionally, membership inference attacks reveal that higher $\epsilon$ values (e.g., $\epsilon \geq 6$) significantly reduce influential points, particularly at higher thresholds, lowering susceptibility to inference attacks. These findings confirm that our approach balances privacy and utility, advancing privacy-preserving survival analysis.</li>
</ul>

<h3>Title: Variational Encoder-Decoders for Learning Latent Representations of Physical Systems</h3>
<ul>
<li><strong>Authors: </strong>Subashree Venkatasubramanian, David A. Barajas-Solano</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05175">https://arxiv.org/abs/2412.05175</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05175">https://arxiv.org/pdf/2412.05175</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05175]] Variational Encoder-Decoders for Learning Latent Representations of Physical Systems(https://arxiv.org/abs/2412.05175)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present a deep-learning Variational Encoder-Decoder (VED) framework for learning data-driven low-dimensional representations of the relationship between high-dimensional parameters of a physical system and the system's high-dimensional observable response. The framework consists of two deep learning-based probabilistic transformations: An encoder mapping parameters to latent codes and a decoder mapping latent codes to the observable response. The hyperparameters of these transformations are identified by maximizing a variational lower bound on the log-conditional distribution of the observable response given parameters. To promote the disentanglement of latent codes, we equip this variational loss with a penalty on the off-diagonal entries of the aggregate distribution covariance of codes. This regularization penalty encourages the pushforward of a standard Gaussian distribution of latent codes to approximate the marginal distribution of the observable response. Using the proposed framework we successfully model the hydraulic pressure response at observation wells of a groundwater flow model as a function of its discrete log-hydraulic transmissivity field. Compared to the canonical correlation analysis encoding, the VED model achieves a lower-dimensional latent representation, with as low as $r = 50$ latent dimensions without a significant loss of reconstruction accuracy. We explore the impact of regularization on model performance, finding that KL-divergence and covariance regularization improve feature disentanglement in latent space while maintaining reconstruction accuracy. Furthermore, we evaluate the generative capabilities of the regularized model by decoding random Gaussian noise, revealing that tuning both $\beta$ and $\lambda$ parameters enhances the quality of the generated observable response data.</li>
</ul>

<h3>Title: DreamColour: Controllable Video Colour Editing without Training</h3>
<ul>
<li><strong>Authors: </strong>Chaitat Utintu, Pinaki Nath Chowdhury, Aneeshan Sain, Subhadeep Koley, Ayan Kumar Bhunia, Yi-Zhe Song</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05180">https://arxiv.org/abs/2412.05180</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05180">https://arxiv.org/pdf/2412.05180</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05180]] DreamColour: Controllable Video Colour Editing without Training(https://arxiv.org/abs/2412.05180)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Video colour editing is a crucial task for content creation, yet existing solutions either require painstaking frame-by-frame manipulation or produce unrealistic results with temporal artefacts. We present a practical, training-free framework that makes precise video colour editing accessible through an intuitive interface while maintaining professional-quality output. Our key insight is that by decoupling spatial and temporal aspects of colour editing, we can better align with users' natural workflow -- allowing them to focus on precise colour selection in key frames before automatically propagating changes across time. We achieve this through a novel technical framework that combines: (i) a simple point-and-click interface merging grid-based colour selection with automatic instance segmentation for precise spatial control, (ii) bidirectional colour propagation that leverages inherent video motion patterns, and (iii) motion-aware blending that ensures smooth transitions even with complex object movements. Through extensive evaluation on diverse scenarios, we demonstrate that our approach matches or exceeds state-of-the-art methods while eliminating the need for training or specialized hardware, making professional-quality video colour editing accessible to everyone.</li>
</ul>

<h3>Title: Privacy Drift: Evolving Privacy Concerns in Incremental Learning</h3>
<ul>
<li><strong>Authors: </strong>Sayyed Farid Ahamed, Soumya Banerjee, Sandip Roy, Aayush Kapoor, Marc Vucovich, Kevin Choi, Abdul Rahman, Edward Bowen, Sachin Shetty</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05183">https://arxiv.org/abs/2412.05183</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05183">https://arxiv.org/pdf/2412.05183</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05183]] Privacy Drift: Evolving Privacy Concerns in Incremental Learning(https://arxiv.org/abs/2412.05183)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, membership infer, federate</a></li>
<li><strong>Abstract: </strong>In the evolving landscape of machine learning (ML), Federated Learning (FL) presents a paradigm shift towards decentralized model training while preserving user data privacy. This paper introduces the concept of ``privacy drift", an innovative framework that parallels the well-known phenomenon of concept drift. While concept drift addresses the variability in model accuracy over time due to changes in the data, privacy drift encapsulates the variation in the leakage of private information as models undergo incremental training. By defining and examining privacy drift, this study aims to unveil the nuanced relationship between the evolution of model performance and the integrity of data privacy. Through rigorous experimentation, we investigate the dynamics of privacy drift in FL systems, focusing on how model updates and data distribution shifts influence the susceptibility of models to privacy attacks, such as membership inference attacks (MIA). Our results highlight a complex interplay between model accuracy and privacy safeguards, revealing that enhancements in model performance can lead to increased privacy risks. We provide empirical evidence from experiments on customized datasets derived from CIFAR-100 (Canadian Institute for Advanced Research, 100 classes), showcasing the impact of data and concept drift on privacy. This work lays the groundwork for future research on privacy-aware machine learning, aiming to achieve a delicate balance between model accuracy and data privacy in decentralized environments.</li>
</ul>

<h3>Title: QueEn: A Large Language Model for Quechua-English Translation</h3>
<ul>
<li><strong>Authors: </strong>Junhao Chen, Peng Shu, Yiwei Li, Huaqin Zhao, Hanqi Jiang, Yi Pan, Yifan Zhou, Zhengliang Liu, Lewis C Howe, Tianming Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05184">https://arxiv.org/abs/2412.05184</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05184">https://arxiv.org/pdf/2412.05184</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05184]] QueEn: A Large Language Model for Quechua-English Translation(https://arxiv.org/abs/2412.05184)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent studies show that large language models (LLMs) are powerful tools for working with natural language, bringing advances in many areas of computational linguistics. However, these models face challenges when applied to low-resource languages due to limited training data and difficulty in understanding cultural nuances. In this paper, we propose QueEn, a novel approach for Quechua-English translation that combines Retrieval-Augmented Generation (RAG) with parameter-efficient fine-tuning techniques. Our method leverages external linguistic resources through RAG and uses Low-Rank Adaptation (LoRA) for efficient model adaptation. Experimental results show that our approach substantially exceeds baseline models, with a BLEU score of 17.6 compared to 1.5 for standard GPT models. The integration of RAG with fine-tuning allows our system to address the challenges of low-resource language translation while maintaining computational efficiency. This work contributes to the broader goal of preserving endangered languages through advanced language technologies.</li>
</ul>

<h3>Title: LinVT: Empower Your Image-level Large Language Model to Understand Videos</h3>
<ul>
<li><strong>Authors: </strong>Lishuai Gao, Yujie Zhong, Yingsen Zeng, Haoxian Tan, Dengjie Li, Zheng Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05185">https://arxiv.org/abs/2412.05185</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05185">https://arxiv.org/pdf/2412.05185</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05185]] LinVT: Empower Your Image-level Large Language Model to Understand Videos(https://arxiv.org/abs/2412.05185)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have been widely used in various tasks, motivating us to develop an LLM-based assistant for videos. Instead of training from scratch, we propose a module to transform arbitrary well-trained image-based LLMs into video-LLMs (after being trained on video data). To better adapt image-LLMs for processing videos, we introduce two design principles: linear transformation to preserve the original visual-language alignment and representative information condensation from redundant video content. Guided by these principles, we propose a plug-and-play Linear Video Tokenizer(LinVT), which enables existing image-LLMs to understand videos. We benchmark LinVT with six recent visual LLMs: Aquila, Blip-3, InternVL2, Mipha, Molmo and Qwen2-VL, showcasing the high compatibility of LinVT. LinVT-based LLMs achieve state-of-the-art performance across various video benchmarks, illustrating the effectiveness of LinVT in multi-modal video understanding.</li>
</ul>

<h3>Title: One-shot Federated Learning via Synthetic Distiller-Distillate Communication</h3>
<ul>
<li><strong>Authors: </strong>Junyuan Zhang, Songhua Liu, Xinchao Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05186">https://arxiv.org/abs/2412.05186</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05186">https://arxiv.org/pdf/2412.05186</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05186]] One-shot Federated Learning via Synthetic Distiller-Distillate Communication(https://arxiv.org/abs/2412.05186)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate, data-free</a></li>
<li><strong>Abstract: </strong>One-shot Federated learning (FL) is a powerful technology facilitating collaborative training of machine learning models in a single round of communication. While its superiority lies in communication efficiency and privacy preservation compared to iterative FL, one-shot FL often compromises model performance. Prior research has primarily focused on employing data-free knowledge distillation to optimize data generators and ensemble models for better aggregating local knowledge into the server model. However, these methods typically struggle with data heterogeneity, where inconsistent local data distributions can cause teachers to provide misleading knowledge. Additionally, they may encounter scalability issues with complex datasets due to inherent two-step information loss: first, during local training (from data to model), and second, when transferring knowledge to the server model (from model to inversed data). In this paper, we propose FedSD2C, a novel and practical one-shot FL framework designed to address these challenges. FedSD2C introduces a distiller to synthesize informative distillates directly from local data to reduce information loss and proposes sharing synthetic distillates instead of inconsistent local models to tackle data heterogeneity. Our empirical results demonstrate that FedSD2C consistently outperforms other one-shot FL methods with more complex and real datasets, achieving up to 2.6 the performance of the best baseline. Code: this https URL</li>
</ul>

<h3>Title: Archaeoscape: Bringing Aerial Laser Scanning Archaeology to the Deep Learning Era</h3>
<ul>
<li><strong>Authors: </strong>Yohann Perron, Vladyslav Sydorov, Adam P. Wijker, Damian Evans, Christophe Pottier, Loic Landrieu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05203">https://arxiv.org/abs/2412.05203</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05203">https://arxiv.org/pdf/2412.05203</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05203]] Archaeoscape: Bringing Aerial Laser Scanning Archaeology to the Deep Learning Era(https://arxiv.org/abs/2412.05203)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Airborne Laser Scanning (ALS) technology has transformed modern archaeology by unveiling hidden landscapes beneath dense vegetation. However, the lack of expert-annotated, open-access resources has hindered the analysis of ALS data using advanced deep learning techniques. We address this limitation with Archaeoscape (available at this https URL), a novel large-scale archaeological ALS dataset spanning 888 km$^2$ in Cambodia with 31,141 annotated archaeological features from the Angkorian period. Archaeoscape is over four times larger than comparable datasets, and the first ALS archaeology resource with open-access data, annotations, and models. We benchmark several recent segmentation models to demonstrate the benefits of modern vision techniques for this problem and highlight the unique challenges of discovering subtle human-made structures under dense jungle canopies. By making Archaeoscape available in open access, we hope to bridge the gap between traditional archaeology and modern computer vision methods.</li>
</ul>

<h3>Title: Evaluating and Aligning CodeLLMs on Human Preference</h3>
<ul>
<li><strong>Authors: </strong>Jian Yang, Jiaxi Yang, Ke Jin, Yibo Miao, Lei Zhang, Liqun Yang, Zeyu Cui, Yichang Zhang, Binyuan Hui, Junyang Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05210">https://arxiv.org/abs/2412.05210</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05210">https://arxiv.org/pdf/2412.05210</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05210]] Evaluating and Aligning CodeLLMs on Human Preference(https://arxiv.org/abs/2412.05210)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Code large language models (codeLLMs) have made significant strides in code generation. Most previous code-related benchmarks, which consist of various programming exercises along with the corresponding test cases, are used as a common measure to evaluate the performance and capabilities of code LLMs. However, the current code LLMs focus on synthesizing the correct code snippet, ignoring the alignment with human preferences, where the query should be sampled from the practical application scenarios and the model-generated responses should satisfy the human preference. To bridge the gap between the model-generated response and human preference, we present a rigorous human-curated benchmark CodeArena to emulate the complexity and diversity of real-world coding tasks, where 397 high-quality samples spanning 40 categories and 44 programming languages, carefully curated from user queries. Further, we propose a diverse synthetic instruction corpus SynCode-Instruct (nearly 20B tokens) by scaling instructions from the website to verify the effectiveness of the large-scale synthetic instruction fine-tuning, where Qwen2.5-SynCoder totally trained on synthetic instruction data can achieve top-tier performance of open-source code LLMs. The results find performance differences between execution-based benchmarks and CodeArena. Our systematic experiments of CodeArena on 40+ LLMs reveal a notable performance gap between open SOTA code LLMs (e.g. Qwen2.5-Coder) and proprietary LLMs (e.g., OpenAI o1), underscoring the importance of the human preference alignment.\footnote{\url{this https URL }}</li>
</ul>

<h3>Title: Transformers Meet Relational Databases</h3>
<ul>
<li><strong>Authors: </strong>Jakub Pele≈°ka, Gustav ≈†√≠r</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05218">https://arxiv.org/abs/2412.05218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05218">https://arxiv.org/pdf/2412.05218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05218]] Transformers Meet Relational Databases(https://arxiv.org/abs/2412.05218)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformer models have continuously expanded into all machine learning domains convertible to the underlying sequence-to-sequence representation, including tabular data. However, while ubiquitous, this representation restricts their extension to the more general case of relational databases. In this paper, we introduce a modular neural message-passing scheme that closely adheres to the formal relational model, enabling direct end-to-end learning of tabular Transformers from database storage systems. We address the challenges of appropriate learning data representation and loading, which are critical in the database setting, and compare our approach against a number of representative models from various related fields across a significantly wide range of datasets. Our results demonstrate a superior performance of this newly proposed class of neural architectures.</li>
</ul>

<h3>Title: 100% Hallucination Elimination Using Acurai</h3>
<ul>
<li><strong>Authors: </strong>Michael C. Wood, Adam A. Forbes</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05223">https://arxiv.org/abs/2412.05223</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05223">https://arxiv.org/pdf/2412.05223</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05223]] 100% Hallucination Elimination Using Acurai(https://arxiv.org/abs/2412.05223)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The issue of hallucinations in large language models (LLMs) remains a critical barrier to the adoption of AI in enterprise and other high-stakes applications. Despite advancements in retrieval-augmented generation (RAG) systems, current state-of-the-art methods fail to achieve more than 80% accuracy in generating faithful and factually correct outputs, even when provided with relevant and accurate context. In this work, we introduce Acurai, a novel systematic approach that achieves 100% hallucination-free responses in LLMs by reformatting queries and context data prior to input. Leveraging a deep understanding of LLM internal representations, the importance of noun-phrase dominance, and the role of discrete functional units (DFUs), Acurai ensures alignment between input context and generated output. We validate this method using the RAGTruth corpus, demonstrating its ability to eliminate 100% hallucinations for both GPT-4 and GPT-3.5 Turbo. Acurai sets a new standard for achieving consistent, accurate, and faithful AI responses, marking a significant step forward in the development of trustworthy AI systems.</li>
</ul>

<h3>Title: BEExformer: A Fast Inferencing Transformer Architecture via Binarization with Multiple Early Exits</h3>
<ul>
<li><strong>Authors: </strong>Wazib Ansar, Saptarsi Goswami, Amlan Chakrabarti</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05225">https://arxiv.org/abs/2412.05225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05225">https://arxiv.org/pdf/2412.05225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05225]] BEExformer: A Fast Inferencing Transformer Architecture via Binarization with Multiple Early Exits(https://arxiv.org/abs/2412.05225)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) based on transformers achieve cutting-edge results on a variety of applications. However, their enormous size and processing requirements make deployment on devices with constrained resources extremely difficult. Among various efficiency considerations, model binarization and Early Exit (EE) are common effective solutions. However, binarization may lead to performance loss due to reduced precision affecting gradient estimation and parameter updates. Besides, the present early-exit mechanisms are still in the nascent stages of research. To ameliorate these issues, we propose Binarized Early Exit Transformer (BEExformer), the first-ever selective learning transformer architecture to combine early exit with binarization for textual inference. It improves the binarization process through a differentiable second-order approximation to the impulse function. This enables gradient computation concerning both the sign as well as the magnitude of the weights. In contrast to absolute threshold-based EE, the proposed EE mechanism hinges on fractional reduction in entropy among intermediate transformer blocks with soft-routing loss estimation. While binarization results in 18.44 times reduction in model size, early exit reduces the FLOPs during inference by 54.85% and even improves accuracy by 5.98% through resolving the "overthinking" problem inherent in deep networks. Moreover, the proposed BEExformer simplifies training by not requiring knowledge distillation from a full-precision LLM. Extensive evaluation on the GLUE dataset and comparison with the SOTA works showcase its pareto-optimal performance-efficiency trade-off.</li>
</ul>

<h3>Title: MC3: Memory Contention based Covert Channel Communication on Shared DRAM System-on-Chips</h3>
<ul>
<li><strong>Authors: </strong>Ismet Dagli, James Crea, Soner Seckiner, Yuanchao Xu, Sel√ßuk K√∂se, Mehmet E. Belviranli</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05228">https://arxiv.org/abs/2412.05228</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05228">https://arxiv.org/pdf/2412.05228</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05228]] MC3: Memory Contention based Covert Channel Communication on Shared DRAM System-on-Chips(https://arxiv.org/abs/2412.05228)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a wide-range of mobile computing platforms, including edge/IoT devices, autonomous systems and smartphones. In SM-SoCs, system-wide shared physical memory enables a convenient and financially-feasible way to make data accessible by dozens of processing units (PUs), such as CPU cores and domain specific accelerators. In this study, we investigate vulnerabilities that stem from the shared use of physical memory in such systems. Due to the diverse computational characteristics of the PUs they embed, SM-SoCs often do not employ a shared last level cache (LLC). While the literature proposes covert channel attacks for shared memory systems, high-throughput communication is currently possible by either relying on an LLC or privileged/physical access to the shared memory subsystem. In this study, we introduce a new memory-contention based covert communication attack, MC3, which specifically targets the shared system memory in mobile SoCs. Different from existing attacks, our approach achieves high throughput communication between applications running on CPU and GPU without the need for an LLC or elevated access to the system. We extensively explore the effectiveness of our methodology by demonstrating the trade-off between the channel transmission rate and the robustness of the communication. We demonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to a transmit rate of 6.4 kbps with less than 1% error rate.</li>
</ul>

<h3>Title: LIAR: Leveraging Alignment (Best-of-N) to Jailbreak LLMs in Seconds</h3>
<ul>
<li><strong>Authors: </strong>James Beetham, Souradip Chakraborty, Mengdi Wang, Furong Huang, Amrit Singh Bedi, Mubarak Shah</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05232">https://arxiv.org/abs/2412.05232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05232">https://arxiv.org/pdf/2412.05232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05232]] LIAR: Leveraging Alignment (Best-of-N) to Jailbreak LLMs in Seconds(https://arxiv.org/abs/2412.05232)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Many existing jailbreak techniques rely on solving discrete combinatorial optimization, while more recent approaches involve training LLMs to generate multiple adversarial prompts. However, both approaches require significant computational resources to produce even a single adversarial prompt. We hypothesize that the inefficiency of current approaches stems from an inadequate characterization of the jailbreak problem. To address this gap, we formulate the jailbreak problem in terms of alignment. By starting from an available safety-aligned model, we leverage an unsafe reward to guide the safe model towards generating unsafe outputs using alignment techniques (e.g., reinforcement learning from human feedback), effectively performing jailbreaking via alignment. We propose a novel jailbreak method called LIAR (LeveragIng Alignment to jailbReak). To demonstrate the simplicity and effectiveness of our approach, we employ a best-of-N method to solve the alignment problem. LIAR offers significant advantages: lower computational requirements without additional training, fully black-box operation, competitive attack success rates, and more human-readable prompts. We provide theoretical insights into the possibility of jailbreaking a safety-aligned model, revealing inherent vulnerabilities in current alignment strategies for LLMs. We also provide sub-optimality guarantees for the proposed \algo. Experimentally, we achieve ASR comparable to the SoTA with a 10x improvement to perplexity and a Time-to-Attack measured in seconds rather than tens of hours.</li>
</ul>

<h3>Title: MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at Scale</h3>
<ul>
<li><strong>Authors: </strong>Jarvis Guo, Tuney Zheng, Yuelin Bai, Bo Li, Yubo Wang, King Zhu, Yizhi Li, Graham Neubig, Wenhu Chen, Xiang Yue</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05237">https://arxiv.org/abs/2412.05237</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05237">https://arxiv.org/pdf/2412.05237</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05237]] MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at Scale(https://arxiv.org/abs/2412.05237)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Open-source multimodal large language models (MLLMs) have shown significant potential in a broad range of multimodal tasks. However, their reasoning capabilities remain constrained by existing instruction-tuning datasets, which were predominately repurposed from academic datasets such as VQA, AI2D, and ChartQA. These datasets target simplistic tasks, and only provide phrase-level answers without any intermediate rationales. To address these challenges, we introduce a scalable and cost-effective method to construct a large-scale multimodal instruction-tuning dataset with rich intermediate rationales designed to elicit CoT reasoning. Using only open models, we create a dataset containing 12M instruction-response pairs to cover diverse, reasoning-intensive tasks with detailed and faithful rationales. Experiments demonstrate that training MLLMs on this dataset significantly improves reasoning capabilities, achieving state-of-the-art performance on benchmarks such as MathVerse (+8.1%), MMMU-Pro (+7%), and MuirBench (+13.3%). Additionally, the model demonstrates notable improvements of up to 4% on non-reasoning-based benchmarks. Ablation studies further highlight the importance of key components, such as rewriting and self-filtering, in the dataset construction process.</li>
</ul>

<h3>Title: CompCap: Improving Multimodal Large Language Models with Composite Captions</h3>
<ul>
<li><strong>Authors: </strong>Xiaohui Chen, Satya Narayan Shukla, Mahmoud Azab, Aashu Singh, Qifan Wang, David Yang, ShengYun Peng, Hanchao Yu, Shen Yan, Xuewen Zhang, Baosheng He</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05243">https://arxiv.org/abs/2412.05243</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05243">https://arxiv.org/pdf/2412.05243</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05243]] CompCap: Improving Multimodal Large Language Models with Composite Captions(https://arxiv.org/abs/2412.05243)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>How well can Multimodal Large Language Models (MLLMs) understand composite images? Composite images (CIs) are synthetic visuals created by merging multiple visual elements, such as charts, posters, or screenshots, rather than being captured directly by a camera. While CIs are prevalent in real-world applications, recent MLLM developments have primarily focused on interpreting natural images (NIs). Our research reveals that current MLLMs face significant challenges in accurately understanding CIs, often struggling to extract information or perform complex reasoning based on these images. We find that existing training data for CIs are mostly formatted for question-answer tasks (e.g., in datasets like ChartQA and ScienceQA), while high-quality image-caption datasets, critical for robust vision-language alignment, are only available for NIs. To bridge this gap, we introduce Composite Captions (CompCap), a flexible framework that leverages Large Language Models (LLMs) and automation tools to synthesize CIs with accurate and detailed captions. Using CompCap, we curate CompCap-118K, a dataset containing 118K image-caption pairs across six CI types. We validate the effectiveness of CompCap-118K by supervised fine-tuning MLLMs of three sizes: xGen-MM-inst.-4B and LLaVA-NeXT-Vicuna-7B/13B. Empirical results show that CompCap-118K significantly enhances MLLMs' understanding of CIs, yielding average gains of 1.7%, 2.0%, and 2.9% across eleven benchmarks, respectively.</li>
</ul>

<h3>Title: Uncertainty Quantification for Transformer Models for Dark-Pattern Detection</h3>
<ul>
<li><strong>Authors: </strong>Javier Mu√±oz, √Ålvaro Huertas-Garc√≠a, Carlos Mart√≠-Gonz√°lez, Enrique De Miguel Ambite</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, math.PR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05251">https://arxiv.org/abs/2412.05251</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05251">https://arxiv.org/pdf/2412.05251</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05251]] Uncertainty Quantification for Transformer Models for Dark-Pattern Detection(https://arxiv.org/abs/2412.05251)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability, transformer</a></li>
<li><strong>Abstract: </strong>The opaque nature of transformer-based models, particularly in applications susceptible to unethical practices such as dark-patterns in user interfaces, requires models that integrate uncertainty quantification to enhance trust in predictions. This study focuses on dark-pattern detection, deceptive design choices that manipulate user decisions, undermining autonomy and consent. We propose a differential fine-tuning approach implemented at the final classification head via uncertainty quantification with transformer-based pre-trained models. Employing a dense neural network (DNN) head architecture as a baseline, we examine two methods capable of quantifying uncertainty: Spectral-normalized Neural Gaussian Processes (SNGPs) and Bayesian Neural Networks (BNNs). These methods are evaluated on a set of open-source foundational models across multiple dimensions: model performance, variance in certainty of predictions and environmental impact during training and inference phases. Results demonstrate that integrating uncertainty quantification maintains performance while providing insights into challenging instances within the models. Moreover, the study reveals that the environmental impact does not uniformly increase with the incorporation of uncertainty quantification techniques. The study's findings demonstrate that uncertainty quantification enhances transparency and provides measurable confidence in predictions, improving the explainability and clarity of black-box models. This facilitates informed decision-making and mitigates the influence of dark-patterns on user interfaces. These results highlight the importance of incorporating uncertainty quantification techniques in developing machine learning models, particularly in domains where interpretability and trustworthiness are critical.</li>
</ul>

<h3>Title: Extrapolated Urban View Synthesis Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Xiangyu Han, Zhen Jia, Boyi Li, Yan Wang, Boris Ivanovic, Yurong You, Lingjie Liu, Yue Wang, Marco Pavone, Chen Feng, Yiming Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05256">https://arxiv.org/abs/2412.05256</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05256">https://arxiv.org/pdf/2412.05256</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05256]] Extrapolated Urban View Synthesis Benchmark(https://arxiv.org/abs/2412.05256)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Photorealistic simulators are essential for the training and evaluation of vision-centric autonomous vehicles (AVs). At their core is Novel View Synthesis (NVS), a crucial capability that generates diverse unseen viewpoints to accommodate the broad and continuous pose distribution of AVs. Recent advances in radiance fields, such as 3D Gaussian Splatting, achieve photorealistic rendering at real-time speeds and have been widely used in modeling large-scale driving scenes. However, their performance is commonly evaluated using an interpolated setup with highly correlated training and test views. In contrast, extrapolation, where test views largely deviate from training views, remains underexplored, limiting progress in generalizable simulation technology. To address this gap, we leverage publicly available AV datasets with multiple traversals, multiple vehicles, and multiple cameras to build the first Extrapolated Urban View Synthesis (EUVS) benchmark. Meanwhile, we conduct quantitative and qualitative evaluations of state-of-the-art Gaussian Splatting methods across different difficulty levels. Our results show that Gaussian Splatting is prone to overfitting to training views. Besides, incorporating diffusion priors and improving geometry cannot fundamentally improve NVS under large view changes, highlighting the need for more robust approaches and large-scale training. We have released our data to help advance self-driving and urban robotics simulation technology.</li>
</ul>

<h3>Title: Mind the Time: Temporally-Controlled Multi-Event Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Ziyi Wu, Aliaksandr Siarohin, Willi Menapace, Ivan Skorokhodov, Yuwei Fang, Varnith Chordia, Igor Gilitschenski, Sergey Tulyakov</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05263">https://arxiv.org/abs/2412.05263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05263">https://arxiv.org/pdf/2412.05263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05263]] Mind the Time: Temporally-Controlled Multi-Event Video Generation(https://arxiv.org/abs/2412.05263)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Real-world videos consist of sequences of events. Generating such sequences with precise temporal control is infeasible with existing video generators that rely on a single paragraph of text as input. When tasked with generating multiple events described using a single prompt, such methods often ignore some of the events or fail to arrange them in the correct order. To address this limitation, we present MinT, a multi-event video generator with temporal control. Our key insight is to bind each event to a specific period in the generated video, which allows the model to focus on one event at a time. To enable time-aware interactions between event captions and video tokens, we design a time-based positional encoding method, dubbed ReRoPE. This encoding helps to guide the cross-attention operation. By fine-tuning a pre-trained video diffusion transformer on temporally grounded data, our approach produces coherent videos with smoothly connected events. For the first time in the literature, our model offers control over the timing of events in generated videos. Extensive experiments demonstrate that MinT outperforms existing open-source models by a large margin.</li>
</ul>

<h3>Title: Chimera: Accurate retrosynthesis prediction by ensembling models with diverse inductive biases</h3>
<ul>
<li><strong>Authors: </strong>Krzysztof Maziarz, Guoqing Liu, Hubert Misztela, Aleksei Kornev, Piotr Gai≈Ñski, Holger Hoefling, Mike Fortunato, Rishi Gupta, Marwin Segler</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05269">https://arxiv.org/abs/2412.05269</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05269">https://arxiv.org/pdf/2412.05269</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05269]] Chimera: Accurate retrosynthesis prediction by ensembling models with diverse inductive biases(https://arxiv.org/abs/2412.05269)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Planning and conducting chemical syntheses remains a major bottleneck in the discovery of functional small molecules, and prevents fully leveraging generative AI for molecular inverse design. While early work has shown that ML-based retrosynthesis models can predict reasonable routes, their low accuracy for less frequent, yet important reactions has been pointed out. As multi-step search algorithms are limited to reactions suggested by the underlying model, the applicability of those tools is inherently constrained by the accuracy of retrosynthesis prediction. Inspired by how chemists use different strategies to ideate reactions, we propose Chimera: a framework for building highly accurate reaction models that combine predictions from diverse sources with complementary inductive biases using a learning-based ensembling strategy. We instantiate the framework with two newly developed models, which already by themselves achieve state of the art in their categories. Through experiments across several orders of magnitude in data scale and time-splits, we show Chimera outperforms all major models by a large margin, owing both to the good individual performance of its constituents, but also to the scalability of our ensembling strategy. Moreover, we find that PhD-level organic chemists prefer predictions from Chimera over baselines in terms of quality. Finally, we transfer the largest-scale checkpoint to an internal dataset from a major pharmaceutical company, showing robust generalization under distribution shift. With the new dimension that our framework unlocks, we anticipate further acceleration in the development of even more accurate models.</li>
</ul>

<h3>Title: APOLLO: SGD-like Memory, AdamW-level Performance</h3>
<ul>
<li><strong>Authors: </strong>Hanqing Zhu, Zhenyu Zhang, Wenyan Cong, Xi Liu, Sem Park, Vikas Chandra, Bo Long, David Z. Pan, Zhangyang Wang, Jinwon Lee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05270">https://arxiv.org/abs/2412.05270</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05270">https://arxiv.org/pdf/2412.05270</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05270]] APOLLO: SGD-like Memory, AdamW-level Performance(https://arxiv.org/abs/2412.05270)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are notoriously memory-intensive during training, particularly with the popular AdamW optimizer. This memory burden necessitates using more or higher-end GPUs or reducing batch sizes, limiting training scalability and throughput. To address this, various memory-efficient optimizers have been proposed to reduce optimizer memory usage. However, they face critical challenges: (i) reliance on costly SVD operations; (ii) significant performance trade-offs compared to AdamW; and (iii) still substantial optimizer memory overhead to maintain competitive performance. In this work, we identify that AdamW's learning rate adaptation rule can be effectively coarsened as a structured learning rate update. Based on this insight, we propose Approximated Gradient Scaling for Memory-Efficient LLM Optimization (APOLLO), which approximates learning rate scaling using an auxiliary low-rank optimizer state based on pure random projection. This structured learning rate update rule makes APOLLO highly tolerant to further memory reductions while delivering comparable pre-training performance. Even its rank-1 variant, APOLLO-Mini, achieves superior pre-training performance compared to AdamW with SGD-level memory costs. Extensive experiments demonstrate that the APOLLO series performs on-par with or better than AdamW, while achieving greater memory savings by nearly eliminating the optimization states of AdamW. These savings provide significant system-level benefits: (1) Enhanced Throughput: 3x throughput on an 8xA100-80GB setup compared to AdamW by supporting 4x larger batch sizes. (2) Improved Model Scalability: Pre-training LLaMA-13B with naive DDP on A100-80GB GPUs without system-level optimizations. (3) Low-End GPU Friendly Pre-training: Pre-training LLaMA-7B on a single GPU using less than 12 GB of memory with weight quantization.</li>
</ul>

<h3>Title: Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling</h3>
<ul>
<li><strong>Authors: </strong>Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yimin Ren, Zixuan Chen, Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo Wang, Conghui He, Botian Shi, Xingcheng Zhang, Han Lv, Yi Wang, Wenqi Shao, Pei Chu, Zhongying Tu, Tong He, Zhiyong Wu, Huipeng Deng, Jiaye Ge, Kai Chen, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, Wenhai Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05271">https://arxiv.org/abs/2412.05271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05271">https://arxiv.org/pdf/2412.05271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05271]] Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling(https://arxiv.org/abs/2412.05271)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce InternVL 2.5, an advanced multimodal large language model (MLLM) series that builds upon InternVL 2.0, maintaining its core model architecture while introducing significant enhancements in training and testing strategies as well as data quality. In this work, we delve into the relationship between model scaling and performance, systematically exploring the performance trends in vision encoders, language models, dataset sizes, and test-time configurations. Through extensive evaluations on a wide range of benchmarks, including multi-discipline reasoning, document understanding, multi-image / video understanding, real-world comprehension, multimodal hallucination detection, visual grounding, multilingual capabilities, and pure language processing, InternVL 2.5 exhibits competitive performance, rivaling leading commercial models such as GPT-4o and Claude-3.5-Sonnet. Notably, our model is the first open-source MLLMs to surpass 70% on the MMMU benchmark, achieving a 3.7-point improvement through Chain-of-Thought (CoT) reasoning and showcasing strong potential for test-time scaling. We hope this model contributes to the open-source community by setting new standards for developing and applying multimodal AI systems. HuggingFace demo see this https URL</li>
</ul>

<h3>Title: MotionFlow: Attention-Driven Motion Transfer in Video Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Tuna Han Salih Meral, Hidir Yesiltepe, Connor Dunlop, Pinar Yanardag</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05275">https://arxiv.org/abs/2412.05275</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05275">https://arxiv.org/pdf/2412.05275</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05275]] MotionFlow: Attention-Driven Motion Transfer in Video Diffusion Models(https://arxiv.org/abs/2412.05275)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Text-to-video models have demonstrated impressive capabilities in producing diverse and captivating video content, showcasing a notable advancement in generative AI. However, these models generally lack fine-grained control over motion patterns, limiting their practical applicability. We introduce MotionFlow, a novel framework designed for motion transfer in video diffusion models. Our method utilizes cross-attention maps to accurately capture and manipulate spatial and temporal dynamics, enabling seamless motion transfers across various contexts. Our approach does not require training and works on test-time by leveraging the inherent capabilities of pre-trained video diffusion models. In contrast to traditional approaches, which struggle with comprehensive scene changes while maintaining consistent motion, MotionFlow successfully handles such complex transformations through its attention-based mechanism. Our qualitative and quantitative experiments demonstrate that MotionFlow significantly outperforms existing models in both fidelity and versatility even during drastic scene alterations.</li>
</ul>

<h3>Title: Sparse autoencoders reveal selective remapping of visual concepts during adaptation</h3>
<ul>
<li><strong>Authors: </strong>Hyesu Lim, Jinho Choi, Jaegul Choo, Steffen Schneider</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05276">https://arxiv.org/abs/2412.05276</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05276">https://arxiv.org/pdf/2412.05276</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05276]] Sparse autoencoders reveal selective remapping of visual concepts during adaptation(https://arxiv.org/abs/2412.05276)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Adapting foundation models for specific purposes has become a standard approach to build machine learning systems for downstream applications. Yet, it is an open question which mechanisms take place during adaptation. Here we develop a new Sparse Autoencoder (SAE) for the CLIP vision transformer, named PatchSAE, to extract interpretable concepts at granular levels (e.g. shape, color, or semantics of an object) and their patch-wise spatial attributions. We explore how these concepts influence the model output in downstream image classification tasks and investigate how recent state-of-the-art prompt-based adaptation techniques change the association of model inputs to these concepts. While activations of concepts slightly change between adapted and non-adapted models, we find that the majority of gains on common adaptation tasks can be explained with the existing concepts already present in the non-adapted foundation model. This work provides a concrete framework to train and use SAEs for Vision Transformers and provides insights into explaining adaptation mechanisms.</li>
</ul>

<h3>Title: Birth and Death of a Rose</h3>
<ul>
<li><strong>Authors: </strong>Chen Geng, Yunzhi Zhang, Shangzhe Wu, Jiajun Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05278">https://arxiv.org/abs/2412.05278</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05278">https://arxiv.org/pdf/2412.05278</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05278]] Birth and Death of a Rose(https://arxiv.org/abs/2412.05278)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We study the problem of generating temporal object intrinsics -- temporally evolving sequences of object geometry, reflectance, and texture, such as a blooming rose -- from pre-trained 2D foundation models. Unlike conventional 3D modeling and animation techniques that require extensive manual effort and expertise, we introduce a method that generates such assets with signals distilled from pre-trained 2D diffusion models. To ensure the temporal consistency of object intrinsics, we propose Neural Templates for temporal-state-guided distillation, derived automatically from image features from self-supervised learning. Our method can generate high-quality temporal object intrinsics for several natural phenomena and enable the sampling and controllable rendering of these dynamic objects from any viewpoint, under any environmental lighting conditions, at any time of their lifespan. Project website: this https URL</li>
</ul>

<h3>Title: Perturb-and-Revise: Flexible 3D Editing with Generative Trajectories</h3>
<ul>
<li><strong>Authors: </strong>Susung Hong, Johanna Karras, Ricardo Martin-Brualla, Ira Kemelmacher-Shlizerman</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05279">https://arxiv.org/abs/2412.05279</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05279">https://arxiv.org/pdf/2412.05279</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05279]] Perturb-and-Revise: Flexible 3D Editing with Generative Trajectories(https://arxiv.org/abs/2412.05279)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The fields of 3D reconstruction and text-based 3D editing have advanced significantly with the evolution of text-based diffusion models. While existing 3D editing methods excel at modifying color, texture, and style, they struggle with extensive geometric or appearance changes, thus limiting their applications. We propose Perturb-and-Revise, which makes possible a variety of NeRF editing. First, we perturb the NeRF parameters with random initializations to create a versatile initialization. We automatically determine the perturbation magnitude through analysis of the local loss landscape. Then, we revise the edited NeRF via generative trajectories. Combined with the generative process, we impose identity-preserving gradients to refine the edited NeRF. Extensive experiments demonstrate that Perturb-and-Revise facilitates flexible, effective, and consistent editing of color, appearance, and geometry in 3D. For 360¬∞ results, please visit our project page: this https URL.</li>
</ul>

<h3>Title: Stag-1: Towards Realistic 4D Driving Simulation with Video Generation Model</h3>
<ul>
<li><strong>Authors: </strong>Lening Wang, Wenzhao Zheng, Dalong Du, Yunpeng Zhang, Yilong Ren, Han Jiang, Zhiyong Cui, Haiyang Yu, Jie Zhou, Jiwen Lu, Shanghang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05280">https://arxiv.org/abs/2412.05280</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05280">https://arxiv.org/pdf/2412.05280</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05280]] Stag-1: Towards Realistic 4D Driving Simulation with Video Generation Model(https://arxiv.org/abs/2412.05280)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>4D driving simulation is essential for developing realistic autonomous driving simulators. Despite advancements in existing methods for generating driving scenes, significant challenges remain in view transformation and spatial-temporal dynamic modeling. To address these limitations, we propose a Spatial-Temporal simulAtion for drivinG (Stag-1) model to reconstruct real-world scenes and design a controllable generative network to achieve 4D simulation. Stag-1 constructs continuous 4D point cloud scenes using surround-view data from autonomous vehicles. It decouples spatial-temporal relationships and produces coherent keyframe videos. Additionally, Stag-1 leverages video generation models to obtain photo-realistic and controllable 4D driving simulation videos from any perspective. To expand the range of view generation, we train vehicle motion videos based on decomposed camera poses, enhancing modeling capabilities for distant scenes. Furthermore, we reconstruct vehicle camera trajectories to integrate 3D points across consecutive views, enabling comprehensive scene understanding along the temporal dimension. Following extensive multi-level scene training, Stag-1 can simulate from any desired viewpoint and achieve a deep understanding of scene evolution under static spatial-temporal conditions. Compared to existing methods, our approach shows promising performance in multi-view scene consistency, background coherence, and accuracy, and contributes to the ongoing advancements in realistic autonomous driving simulation. Code: this https URL.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
