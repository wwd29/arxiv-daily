<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-05-08</h1>
<h3>Title: Pseudo Random Number Generator using Internet-of-Things Techniques on Portable Field-Programmable-Gate-Array Platform</h3>
<ul>
<li><strong>Authors: </strong>Tee Hui Teo</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03741">https://arxiv.org/abs/2505.03741</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03741">https://arxiv.org/pdf/2505.03741</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03741]] Pseudo Random Number Generator using Internet-of-Things Techniques on Portable Field-Programmable-Gate-Array Platform(https://arxiv.org/abs/2505.03741)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>This paper conducts a comparative study of three IoT-based PRNG models, including Logistic Map, Double Pendulum, and Multi-LFSR, implemented on an FPGA platform. Comparisons are made across key performance metrics like randomness, latency, power consumption, hardware resource usage, energy efficiency, scalability, and application suitability. Compared to Multi-LFSR, Logistic Map, and Double Pendulum Models provide perfect quality randomness, which is quite apt for high-security grade applications; however, the requirements of these models concerning power and hardware resources are also considerably high. By contrast, the Multi-LFSR comes into its own due to its lower latency, power consumption, and resource-efficient design. It is, therefore, suited for embedded or real-time applications. Furthermore, environmental sensors will also be introduced as entropy sources for the PRNGs to enhance the randomness of the systems, particularly in IoT-enabled battery-powered FPGA platforms. The experimental results confirm that the Multi-LFSR model has the highest energy efficiency, while the Logistic Map and Double Pendulum outperform in generating numbers with very high security. The study thus provides a deeper insight into decision- making for selecting PRNG models.</li>
</ul>

<h3>Title: Hardware-Enabled Mechanisms for Verifying Responsible AI Development</h3>
<ul>
<li><strong>Authors: </strong>Aidan O'Gara, Gabriel Kulp, Will Hodgkins, James Petrie, Vincent Immler, Aydin Aysu, Kanad Basu, Shivam Bhasin, Stjepan Picek, Ankur Srivastava</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03742">https://arxiv.org/abs/2505.03742</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03742">https://arxiv.org/pdf/2505.03742</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03742]] Hardware-Enabled Mechanisms for Verifying Responsible AI Development(https://arxiv.org/abs/2505.03742)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, robust</a></li>
<li><strong>Abstract: </strong>Advancements in AI capabilities, driven in large part by scaling up computing resources used for AI training, have created opportunities to address major global challenges but also pose risks of misuse. Hardware-enabled mechanisms (HEMs) can support responsible AI development by enabling verifiable reporting of key properties of AI training activities such as quantity of compute used, training cluster configuration or location, as well as policy enforcement. Such tools can promote transparency and improve security, while addressing privacy and intellectual property concerns. Based on insights from an interdisciplinary workshop, we identify open questions regarding potential implementation approaches, emphasizing the need for further research to ensure robust, scalable solutions.</li>
</ul>

<h3>Title: From Concept to Measurement: A Survey of How the Blockchain Trilemma Can Be Analyzed</h3>
<ul>
<li><strong>Authors: </strong>Mansur Aliyu, Niclas Kannengie√üer, Sunyaev Ali</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03768">https://arxiv.org/abs/2505.03768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03768">https://arxiv.org/pdf/2505.03768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03768]] From Concept to Measurement: A Survey of How the Blockchain Trilemma Can Be Analyzed(https://arxiv.org/abs/2505.03768)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>To meet non-functional requirements, practitioners must identify Pareto-optimal configurations of the degree of decentralization, scalability, and security of blockchain systems. Maximizing all of these subconcepts is, however, impossible due to the trade-offs highlighted by the blockchain trilemma. We reviewed analysis approaches to identify constructs and their operationalization through metrics for analyzing the blockchain trilemma subconcepts and to assess the applicability of the operationalized constructs to various blockchain systems. By clarifying these constructs and metrics, this work offers a theoretical foundation for more sophisticated investigations into how the blockchain trilemma manifests in blockchain systems, helping practitioners identify Pareto-optimal configurations.</li>
</ul>

<h3>Title: Hierarchical Multi-Label Generation with Probabilistic Level-Constraint</h3>
<ul>
<li><strong>Authors: </strong>Linqing Chen, Weilei Wang, Wentao Wu, Hanmeng Zhong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03775">https://arxiv.org/abs/2505.03775</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03775">https://arxiv.org/pdf/2505.03775</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03775]] Hierarchical Multi-Label Generation with Probabilistic Level-Constraint(https://arxiv.org/abs/2505.03775)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Hierarchical Extreme Multi-Label Classification poses greater difficulties compared to traditional multi-label classification because of the intricate hierarchical connections of labels within a domain-specific taxonomy and the substantial number of labels. Some of the prior research endeavors centered on classifying text through several ancillary stages such as the cluster algorithm and multiphase classification. Others made attempts to leverage the assistance of generative methods yet were unable to properly control the output of the generative model. We redefine the task from hierarchical multi-Label classification to Hierarchical Multi-Label Generation (HMG) and employ a generative framework with Probabilistic Level Constraints (PLC) to generate hierarchical labels within a specific taxonomy that have complex hierarchical relationships. The approach we proposed in this paper enables the framework to generate all relevant labels across levels for each document without relying on preliminary operations like clustering. Meanwhile, it can control the model output precisely in terms of count, length, and level aspects. Experiments demonstrate that our approach not only achieves a new SOTA performance in the HMG task, but also has a much better performance in constrained the output of model than previous research work.</li>
</ul>

<h3>Title: PAPN: Proximity Attention Encoder and Pointer Network Decoder for Parcel Pickup Route Prediction</h3>
<ul>
<li><strong>Authors: </strong>Hansi Denis, Siegfried Mercelis, Ngoc-Quang Luong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03776">https://arxiv.org/abs/2505.03776</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03776">https://arxiv.org/pdf/2505.03776</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03776]] PAPN: Proximity Attention Encoder and Pointer Network Decoder for Parcel Pickup Route Prediction(https://arxiv.org/abs/2505.03776)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Optimization of the last-mile delivery and first-mile pickup of parcels is an integral part of the broader logistics optimization pipeline as it entails both cost and resource efficiency as well as a heightened service quality. Such optimization requires accurate route and time prediction systems to adapt to different scenarios in advance. This work tackles the first building block, namely route prediction. This is done by introducing a novel Proximity Attention mechanism in an encoder-decoder architecture utilizing a Pointer Network in the decoding process (Proximity Attention Encoder and Pointer Network decoder: PAPN) to leverage the underlying connections between the different visitable pickup positions at each timestep. To this local attention process is coupled global context computing via a multi-head attention transformer encoder. The obtained global context is then mixed to an aggregated version of the local embedding thus achieving a mix of global and local attention for complete modeling of the problems. Proximity attention is also used in the decoding process to skew predictions towards the locations with the highest attention scores and thus using inter-connectivity of locations as a base for next-location prediction. This method is trained, validated and tested on a large industry-level dataset of real-world, large-scale last-mile delivery and first-mile pickup named LaDE[1]. This approach shows noticeable promise, outperforming all state-of-the-art supervised systems in terms of most metrics used for benchmarking methods on this dataset while still being competitive with the best-performing reinforcement learning method named DRL4Route[2].</li>
</ul>

<h3>Title: MolMole: Molecule Mining from Scientific Literature</h3>
<ul>
<li><strong>Authors: </strong>LG AI Research, Sehyun Chun, Jiye Kim, Ahra Jo, Yeonsik Jo, Seungyul Oh, Seungjun Lee, Kwangrok Ryoo, Jongmin Lee, Seunghwan Kim, Byung Jun Kang, Soonyoung Lee, Jun Ha Park, Chanwoo Moon, Jiwon Ham, Haein Lee, Heejae Han, Jaeseung Byun, Soojong Do, Minju Ha, Dongyun Kim, Kyunghoon Bae, Woohyung Lim, Edward Hwayoung Lee, Yongmin Park, Jeongsang Yu, Gerrard Jeongwon Jo, Yeonjung Hong, Kyungjae Yoo, Sehui Han, Jaewan Lee, Changyoung Park, Kijeong Jeon, Sihyuk Yi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03777">https://arxiv.org/abs/2505.03777</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03777">https://arxiv.org/pdf/2505.03777</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03777]] MolMole: Molecule Mining from Scientific Literature(https://arxiv.org/abs/2505.03777)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>The extraction of molecular structures and reaction data from scientific documents is challenging due to their varied, unstructured chemical formats and complex document layouts. To address this, we introduce MolMole, a vision-based deep learning framework that unifies molecule detection, reaction diagram parsing, and optical chemical structure recognition (OCSR) into a single pipeline for automating the extraction of chemical data directly from page-level documents. Recognizing the lack of a standard page-level benchmark and evaluation metric, we also present a testset of 550 pages annotated with molecule bounding boxes, reaction labels, and MOLfiles, along with a novel evaluation metric. Experimental results demonstrate that MolMole outperforms existing toolkits on both our benchmark and public datasets. The benchmark testset will be publicly available, and the MolMole toolkit will be accessible soon through an interactive demo on the LG AI Research website. For commercial inquiries, please contact us at \href{mailto:contact_ddu@lgresearch.ai}{contact\_ddu@lgresearch.ai}.</li>
</ul>

<h3>Title: ALFRED: Ask a Large-language model For Reliable ECG Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Jin Yu, JaeHo Park, TaeJun Park, Gyurin Kim, JiHyun Lee, Min Sung Lee, Joon-myoung Kwon, Jeong Min Son, Yong-Yeon Jo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03781">https://arxiv.org/abs/2505.03781</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03781">https://arxiv.org/pdf/2505.03781</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03781]] ALFRED: Ask a Large-language model For Reliable ECG Diagnosis(https://arxiv.org/abs/2505.03781)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, large language model</a></li>
<li><strong>Abstract: </strong>Leveraging Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) for analyzing medical data, particularly Electrocardiogram (ECG), offers high accuracy and convenience. However, generating reliable, evidence-based results in specialized fields like healthcare remains a challenge, as RAG alone may not suffice. We propose a Zero-shot ECG diagnosis framework based on RAG for ECG analysis that incorporates expert-curated knowledge to enhance diagnostic accuracy and explainability. Evaluation on the PTB-XL dataset demonstrates the framework's effectiveness, highlighting the value of structured domain expertise in automated ECG interpretation. Our framework is designed to support comprehensive ECG analysis, addressing diverse diagnostic needs with potential applications beyond the tested dataset.</li>
</ul>

<h3>Title: A general physics-constrained method for the modelling of equation's closure terms with sparse data</h3>
<ul>
<li><strong>Authors: </strong>Tian Chen, Shengping Liu, Li Liu, Heng Yong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03783">https://arxiv.org/abs/2505.03783</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03783">https://arxiv.org/pdf/2505.03783</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03783]] A general physics-constrained method for the modelling of equation's closure terms with sparse data(https://arxiv.org/abs/2505.03783)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate modeling of closure terms is a critical challenge in engineering and scientific research, particularly when data is sparse (scarse or incomplete), making widely applicable models difficult to develop. This study proposes a novel approach for constructing closure models in such challenging scenarios. We introduce a Series-Parallel Multi-Network Architecture that integrates Physics-Informed Neural Networks (PINNs) to incorporate physical constraints and heterogeneous data from multiple initial and boundary conditions, while employing dedicated subnetworks to independently model unknown closure terms, enhancing generalizability across diverse problems. These closure models are integrated into an accurate Partial Differential Equation (PDE) solver, enabling robust solutions to complex predictive simulations in engineering applications.</li>
</ul>

<h3>Title: Insulin Resistance Prediction From Wearables and Routine Blood Biomarkers</h3>
<ul>
<li><strong>Authors: </strong>Ahmed A. Metwally, A. Ali Heydari, Daniel McDuff, Alexandru Solot, Zeinab Esmaeilpour, Anthony Z Faranesh, Menglian Zhou, David B. Savage, Conor Heneghan, Shwetak Patel, Cathy Speed, Javier L. Prieto</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03784">https://arxiv.org/abs/2505.03784</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03784">https://arxiv.org/pdf/2505.03784</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03784]] Insulin Resistance Prediction From Wearables and Routine Blood Biomarkers(https://arxiv.org/abs/2505.03784)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Insulin resistance, a precursor to type 2 diabetes, is characterized by impaired insulin action in tissues. Current methods for measuring insulin resistance, while effective, are expensive, inaccessible, not widely available and hinder opportunities for early intervention. In this study, we remotely recruited the largest dataset to date across the US to study insulin resistance (N=1,165 participants, with median BMI=28 kg/m2, age=45 years, HbA1c=5.4%), incorporating wearable device time series data and blood biomarkers, including the ground-truth measure of insulin resistance, homeostatic model assessment for insulin resistance (HOMA-IR). We developed deep neural network models to predict insulin resistance based on readily available digital and blood biomarkers. Our results show that our models can predict insulin resistance by combining both wearable data and readily available blood biomarkers better than either of the two data sources separately (R2=0.5, auROC=0.80, Sensitivity=76%, and specificity 84%). The model showed 93% sensitivity and 95% adjusted specificity in obese and sedentary participants, a subpopulation most vulnerable to developing type 2 diabetes and who could benefit most from early intervention. Rigorous evaluation of model performance, including interpretability, and robustness, facilitates generalizability across larger cohorts, which is demonstrated by reproducing the prediction performance on an independent validation cohort (N=72 participants). Additionally, we demonstrated how the predicted insulin resistance can be integrated into a large language model agent to help understand and contextualize HOMA-IR values, facilitating interpretation and safe personalized recommendations. This work offers the potential for early detection of people at risk of type 2 diabetes and thereby facilitate earlier implementation of preventative strategies.</li>
</ul>

<h3>Title: mAIstro: an open-source multi-agentic system for automated end-to-end development of radiomics and deep learning models for medical imaging</h3>
<ul>
<li><strong>Authors: </strong>Eleftherios Tzanis, Michail E. Klontzas</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03785">https://arxiv.org/abs/2505.03785</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03785">https://arxiv.org/pdf/2505.03785</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03785]] mAIstro: an open-source multi-agentic system for automated end-to-end development of radiomics and deep learning models for medical imaging(https://arxiv.org/abs/2505.03785)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Agentic systems built on large language models (LLMs) offer promising capabilities for automating complex workflows in healthcare AI. We introduce mAIstro, an open-source, autonomous multi-agentic framework for end-to-end development and deployment of medical AI models. The system orchestrates exploratory data analysis, radiomic feature extraction, image segmentation, classification, and regression through a natural language interface, requiring no coding from the user. Built on a modular architecture, mAIstro supports both open- and closed-source LLMs, and was evaluated using a large and diverse set of prompts across 16 open-source datasets, covering a wide range of imaging modalities, anatomical regions, and data types. The agents successfully executed all tasks, producing interpretable outputs and validated models. This work presents the first agentic framework capable of unifying data analysis, AI model development, and inference across varied healthcare applications, offering a reproducible and extensible foundation for clinical and research AI integration. The code is available at: this https URL</li>
</ul>

<h3>Title: When Reasoning Beats Scale: A 1.5B Reasoning Model Outranks 13B LLMs as Discriminator</h3>
<ul>
<li><strong>Authors: </strong>Md Fahim Anjum</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03786">https://arxiv.org/abs/2505.03786</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03786">https://arxiv.org/pdf/2505.03786</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03786]] When Reasoning Beats Scale: A 1.5B Reasoning Model Outranks 13B LLMs as Discriminator(https://arxiv.org/abs/2505.03786)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLM) with reasoning capabilities offer a promising path for improving candidate evaluation in planning frameworks, but their relative performance against traditional non-reasoning models remains largely underexplored. In this study, we benchmark a distilled 1.5B parameter reasoning model (DeepSeek-R1) against several state-of-the-art non-reasoning LLMs within a generator-discriminator LLM planning framework for the text-to-SQL task. For this, we introduce a novel method for extracting soft scores from the chain-of-thought (CoT) outputs from reasoning that enables fine-grained ranking of candidates. Our central hypothesis is that reasoning models are more effective discriminators than non-reasoning LLMs. Our results show that distilled DeepSeek-R1-1.5B achieves up to $87\%$ higher F1 and $3.7\%$ better discrimination accuracy than CodeLlama-7B, as well as $3.7\%$ higher execution accuracy than CodeLlama-13B, despite having significantly fewer parameters. Furthermore, we find that there is a limit to the logical capabilities of reasoning models, and only providing more context or allowing more compute budget for reasoning is not enough to improve their discrimination performance. Finally, we demonstrate that, unlike non-reasoning LLMs, reasoning models find generation more challenging than discrimination and may underperform as generators compared to smaller non-reasoning LLMs. Our work highlights the potential of reasoning models as discriminators in agentic frameworks, far outweighing their capabilities as generators, offering insights into their optimal role within LLM planning infrastructures.</li>
</ul>

<h3>Title: ArrhythmiaVision: Resource-Conscious Deep Learning Models with Visual Explanations for ECG Arrhythmia Classification</h3>
<ul>
<li><strong>Authors: </strong>Zuraiz Baig, Sidra Nasir, Rizwan Ahmed Khan, Muhammad Zeeshan Ul Haque</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03787">https://arxiv.org/abs/2505.03787</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03787">https://arxiv.org/pdf/2505.03787</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03787]] ArrhythmiaVision: Resource-Conscious Deep Learning Models with Visual Explanations for ECG Arrhythmia Classification(https://arxiv.org/abs/2505.03787)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Cardiac arrhythmias are a leading cause of life-threatening cardiac events, highlighting the urgent need for accurate and timely detection. Electrocardiography (ECG) remains the clinical gold standard for arrhythmia diagnosis; however, manual interpretation is time-consuming, dependent on clinical expertise, and prone to human error. Although deep learning has advanced automated ECG analysis, many existing models abstract away the signal's intrinsic temporal and morphological features, lack interpretability, and are computationally intensive-hindering their deployment on resource-constrained platforms. In this work, we propose two novel lightweight 1D convolutional neural networks, ArrhythmiNet V1 and V2, optimized for efficient, real-time arrhythmia classification on edge devices. Inspired by MobileNet's depthwise separable convolutional design, these models maintain memory footprints of just 302.18 KB and 157.76 KB, respectively, while achieving classification accuracies of 0.99 (V1) and 0.98 (V2) on the MIT-BIH Arrhythmia Dataset across five classes: Normal Sinus Rhythm, Left Bundle Branch Block, Right Bundle Branch Block, Atrial Premature Contraction, and Premature Ventricular Contraction. In order to ensure clinical transparency and relevance, we integrate Shapley Additive Explanations and Gradient-weighted Class Activation Mapping, enabling both local and global interpretability. These techniques highlight physiologically meaningful patterns such as the QRS complex and T-wave that contribute to the model's predictions. We also discuss performance-efficiency trade-offs and address current limitations related to dataset diversity and generalizability. Overall, our findings demonstrate the feasibility of combining interpretability, predictive accuracy, and computational efficiency in practical, wearable, and embedded ECG monitoring systems.</li>
</ul>

<h3>Title: Calibrating Uncertainty Quantification of Multi-Modal LLMs using Grounding</h3>
<ul>
<li><strong>Authors: </strong>Trilok Padhi, Ramneet Kaur, Adam D. Cobb, Manoj Acharya, Anirban Roy, Colin Samplawski, Brian Matejek, Alexander M. Berenbeim, Nathaniel D. Bastian, Susmit Jha</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03788">https://arxiv.org/abs/2505.03788</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03788">https://arxiv.org/pdf/2505.03788</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03788]] Calibrating Uncertainty Quantification of Multi-Modal LLMs using Grounding(https://arxiv.org/abs/2505.03788)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce a novel approach for calibrating uncertainty quantification (UQ) tailored for multi-modal large language models (LLMs). Existing state-of-the-art UQ methods rely on consistency among multiple responses generated by the LLM on an input query under diverse settings. However, these approaches often report higher confidence in scenarios where the LLM is consistently incorrect. This leads to a poorly calibrated confidence with respect to accuracy. To address this, we leverage cross-modal consistency in addition to self-consistency to improve the calibration of the multi-modal models. Specifically, we ground the textual responses to the visual inputs. The confidence from the grounding model is used to calibrate the overall confidence. Given that using a grounding model adds its own uncertainty in the pipeline, we apply temperature scaling - a widely accepted parametric calibration technique - to calibrate the grounding model's confidence in the accuracy of generated responses. We evaluate the proposed approach across multiple multi-modal tasks, such as medical question answering (Slake) and visual question answering (VQAv2), considering multi-modal models such as LLaVA-Med and LLaVA. The experiments demonstrate that the proposed framework achieves significantly improved calibration on both tasks.</li>
</ul>

<h3>Title: A Time-Series Data Augmentation Model through Diffusion and Transformer Integration</h3>
<ul>
<li><strong>Authors: </strong>Yuren Zhang, Zhongnan Pu, Lei Jing</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03790">https://arxiv.org/abs/2505.03790</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03790">https://arxiv.org/pdf/2505.03790</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03790]] A Time-Series Data Augmentation Model through Diffusion and Transformer Integration(https://arxiv.org/abs/2505.03790)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>With the development of Artificial Intelligence, numerous real-world tasks have been accomplished using technology integrated with deep learning. To achieve optimal performance, deep neural networks typically require large volumes of data for training. Although advances in data augmentation have facilitated the acquisition of vast datasets, most of this data is concentrated in domains like images and speech. However, there has been relatively less focus on augmenting time-series data. To address this gap and generate a substantial amount of time-series data, we propose a simple and effective method that combines the Diffusion and Transformer models. By utilizing an adjusted diffusion denoising model to generate a large volume of initial time-step action data, followed by employing a Transformer model to predict subsequent actions, and incorporating a weighted loss function to achieve convergence, the method demonstrates its effectiveness. Using the performance improvement of the model after applying augmented data as a benchmark, and comparing the results with those obtained without data augmentation or using traditional data augmentation methods, this approach shows its capability to produce high-quality augmented data.</li>
</ul>

<h3>Title: LENSLLM: Unveiling Fine-Tuning Dynamics for LLM Selection</h3>
<ul>
<li><strong>Authors: </strong>Xinyue Zeng, Haohui Wang, Junhong Lin, Jun Wu, Tyler Cody, Dawei Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03793">https://arxiv.org/abs/2505.03793</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03793">https://arxiv.org/pdf/2505.03793</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03793]] LENSLLM: Unveiling Fine-Tuning Dynamics for LLM Selection(https://arxiv.org/abs/2505.03793)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The proliferation of open-sourced Large Language Models (LLMs) and diverse downstream tasks necessitates efficient model selection, given the impracticality of fine-tuning all candidates due to computational constraints. Despite the recent advances in LLM selection, a fundamental research question largely remains nascent: how can we model the dynamic behaviors of LLMs during fine-tuning, thereby enhancing our understanding of their generalization performance across diverse downstream tasks? In this work, we propose a novel theoretical framework that provides a proper lens to assess the generalization capabilities of LLMs, thereby enabling accurate and efficient LLM selection for downstream applications. In particular, we first derive a Hessian-based PAC-Bayes generalization bound that unveils fine-tuning dynamics of LLMs and then introduce LENSLLM, a Neural Tangent Kernel(NTK)-based Rectified Scaling Model that enables accurate performance predictions across diverse tasks while maintaining computational efficiency. Extensive empirical results on 3 large-scale benchmarks demonstrate that our model achieves up to 91.1% accuracy and reduces up to 88.5% computational cost in LLM selection, outperforming 5 state-of-the-art methods. We open-source our proposed LENSLLM model and corresponding results at the Github link: this https URL.</li>
</ul>

<h3>Title: AI-Driven IRM: Transforming insider risk management with adaptive scoring and LLM-based threat detection</h3>
<ul>
<li><strong>Authors: </strong>Lokesh Koli, Shubham Kalra, Rohan Thakur, Anas Saifi, Karanpreet Singh</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03796">https://arxiv.org/abs/2505.03796</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03796">https://arxiv.org/pdf/2505.03796</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03796]] AI-Driven IRM: Transforming insider risk management with adaptive scoring and LLM-based threat detection(https://arxiv.org/abs/2505.03796)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, federate</a></li>
<li><strong>Abstract: </strong>Insider threats pose a significant challenge to organizational security, often evading traditional rule-based detection systems due to their subtlety and contextual nature. This paper presents an AI-powered Insider Risk Management (IRM) system that integrates behavioral analytics, dynamic risk scoring, and real-time policy enforcement to detect and mitigate insider threats with high accuracy and adaptability. We introduce a hybrid scoring mechanism - transitioning from the static PRISM model to an adaptive AI-based model utilizing an autoencoder neural network trained on expert-annotated user activity data. Through iterative feedback loops and continuous learning, the system reduces false positives by 59% and improves true positive detection rates by 30%, demonstrating substantial gains in detection precision. Additionally, the platform scales efficiently, processing up to 10 million log events daily with sub-300ms query latency, and supports automated enforcement actions for policy violations, reducing manual intervention. The IRM system's deployment resulted in a 47% reduction in incident response times, highlighting its operational impact. Future enhancements include integrating explainable AI, federated learning, graph-based anomaly detection, and alignment with Zero Trust principles to further elevate its adaptability, transparency, and compliance-readiness. This work establishes a scalable and proactive framework for mitigating emerging insider risks in both on-premises and hybrid environments.</li>
</ul>

<h3>Title: Scalability Matters: Overcoming Challenges in InstructGLM with Similarity-Degree-Based Sampling</h3>
<ul>
<li><strong>Authors: </strong>Hyun Lee, Chris Yi, Maminur Islam, B.D.S. Aritra</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03799">https://arxiv.org/abs/2505.03799</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03799">https://arxiv.org/pdf/2505.03799</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03799]] Scalability Matters: Overcoming Challenges in InstructGLM with Similarity-Degree-Based Sampling(https://arxiv.org/abs/2505.03799)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated strong capabilities in various natural language processing tasks; however, their application to graph-related problems remains limited, primarily due to scalability constraints and the absence of dedicated mechanisms for processing graph structures. Existing approaches predominantly integrate LLMs with Graph Neural Networks (GNNs), using GNNs as feature encoders or auxiliary components. However, directly encoding graph structures within LLMs has been underexplored, particularly in the context of large-scale graphs where token limitations hinder effective representation. To address these challenges, we propose SDM-InstructGLM, a novel instruction-tuned Graph Language Model (InstructGLM) framework that enhances scalability and efficiency without relying on GNNs. Our method introduces a similarity-degree-based biased random walk mechanism, which selectively samples and encodes graph information based on node-feature similarity and degree centrality, ensuring an adaptive and structured representation within the LLM. This approach significantly improves token efficiency, mitigates information loss due to random sampling, and enhances performance on graph-based tasks such as node classification and link prediction. Furthermore, our results demonstrate the feasibility of LLM-only graph processing, enabling scalable and interpretable Graph Language Models (GLMs) optimized through instruction-based fine-tuning. This work paves the way for GNN-free approaches to graph learning, leveraging LLMs as standalone graph reasoning models. Our source code is available on GitHub.</li>
</ul>

<h3>Title: Large Language Model Compression with Global Rank and Sparsity Optimization</h3>
<ul>
<li><strong>Authors: </strong>Changhai Zhou, Qian Qiao, Weizhong Zhang, Cheng Jin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03801">https://arxiv.org/abs/2505.03801</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03801">https://arxiv.org/pdf/2505.03801</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03801]] Large Language Model Compression with Global Rank and Sparsity Optimization(https://arxiv.org/abs/2505.03801)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Low-rank and sparse composite approximation is a natural idea to compress Large Language Models (LLMs). However, such an idea faces two primary challenges that adversely affect the performance of existing methods. The first challenge relates to the interaction and cooperation between low-rank and sparse matrices, while the second involves determining weight allocation across different layers, as redundancy varies considerably among them. To address these challenges, we propose a novel two-stage LLM compression method with the capability of global rank and sparsity optimization. It is noteworthy that the overall optimization space is vast, making comprehensive optimization computationally prohibitive. Therefore, to reduce the optimization space, our first stage utilizes robust principal component analysis to decompose the weight matrices of LLMs into low-rank and sparse components, which span the low dimensional and sparse spaces containing the resultant low-rank and sparse matrices, respectively. In the second stage, we propose a probabilistic global optimization technique to jointly identify the low-rank and sparse structures within the above two spaces. The appealing feature of our approach is its ability to automatically detect the redundancy across different layers and to manage the interaction between the sparse and low-rank components. Extensive experimental results indicate that our method significantly surpasses state-of-the-art techniques for sparsification and composite approximation.</li>
</ul>

<h3>Title: Efficient Fine-Tuning of Quantized Models via Adaptive Rank and Bitwidth</h3>
<ul>
<li><strong>Authors: </strong>Changhai Zhou, Yuhua Zhou, Qian Qiao, Weizhong Zhang, Cheng Jin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03802">https://arxiv.org/abs/2505.03802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03802">https://arxiv.org/pdf/2505.03802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03802]] Efficient Fine-Tuning of Quantized Models via Adaptive Rank and Bitwidth(https://arxiv.org/abs/2505.03802)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>QLoRA effectively combines low-bit quantization and LoRA to achieve memory-friendly fine-tuning for large language models (LLM). Recently, methods based on SVD for continuous update iterations to initialize LoRA matrices to accommodate quantization errors have generally failed to consistently improve performance. Dynamic mixed precision is a natural idea for continuously improving the fine-tuning performance of quantized models, but previous methods often optimize low-rank subspaces or quantization components separately, without considering their synergy. To address this, we propose \textbf{QR-Adaptor}, a unified, gradient-free strategy that uses partial calibration data to jointly search the quantization components and the rank of low-rank spaces for each layer, thereby continuously improving model performance. QR-Adaptor does not minimize quantization error but treats precision and rank allocation as a discrete optimization problem guided by actual downstream performance and memory usage. Compared to state-of-the-art (SOTA) quantized LoRA fine-tuning methods, our approach achieves a 4.89\% accuracy improvement on GSM8K, and in some cases even outperforms the 16-bit fine-tuned model while maintaining the memory footprint of the 4-bit setting.</li>
</ul>

<h3>Title: RWKVQuant: Quantizing the RWKV Family with Proxy Guided Hybrid of Scalar and Vector Quantization</h3>
<ul>
<li><strong>Authors: </strong>Chen Xu, Yuxuan Yue, Zukang Xu, Xing Hu, Jiangyong Yu, Zhixuan Chen, Sifan Zhou, Zhihang Yuan, Dawei Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03803">https://arxiv.org/abs/2505.03803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03803">https://arxiv.org/pdf/2505.03803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03803]] RWKVQuant: Quantizing the RWKV Family with Proxy Guided Hybrid of Scalar and Vector Quantization(https://arxiv.org/abs/2505.03803)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>RWKV is a modern RNN architecture with comparable performance to Transformer, but still faces challenges when deployed to resource-constrained devices. Post Training Quantization (PTQ), which is a an essential technique to reduce model size and inference latency, has been widely used in Transformer models. However, it suffers significant degradation of performance when applied to RWKV. This paper investigates and identifies two key constraints inherent in the properties of RWKV: (1) Non-linear operators hinder the parameter-fusion of both smooth- and rotation-based quantization, introducing extra computation overhead. (2) The larger amount of uniformly distributed weights poses challenges for cluster-based quantization, leading to reduced accuracy. To this end, we propose RWKVQuant, a PTQ framework tailored for RWKV models, consisting of two novel techniques: (1) a coarse-to-fine proxy capable of adaptively selecting different quantization approaches by assessing the uniformity and identifying outliers in the weights, and (2) a codebook optimization algorithm that enhances the performance of cluster-based quantization methods for element-wise multiplication in RWKV. Experiments show that RWKVQuant can quantize RWKV-6-14B into about 3-bit with less than 1% accuracy loss and 2.14x speed up.</li>
</ul>

<h3>Title: MoEQuant: Enhancing Quantization for Mixture-of-Experts Large Language Models via Expert-Balanced Sampling and Affinity Guidance</h3>
<ul>
<li><strong>Authors: </strong>Xing Hu, Zhixuan Chen, Dawei Yang, Zukang Xu, Chen Xu, Zhihang Yuan, Sifan Zhou, Jiangyong Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03804">https://arxiv.org/abs/2505.03804</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03804">https://arxiv.org/pdf/2505.03804</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03804]] MoEQuant: Enhancing Quantization for Mixture-of-Experts Large Language Models via Expert-Balanced Sampling and Affinity Guidance(https://arxiv.org/abs/2505.03804)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Mixture-of-Experts (MoE) large language models (LLMs), which leverage dynamic routing and sparse activation to enhance efficiency and scalability, have achieved higher performance while reducing computational costs. However, these models face significant memory overheads, limiting their practical deployment and broader adoption. Post-training quantization (PTQ), a widely used method for compressing LLMs, encounters severe accuracy degradation and diminished generalization performance when applied to MoE models. This paper investigates the impact of MoE's sparse and dynamic characteristics on quantization and identifies two primary challenges: (1) Inter-expert imbalance, referring to the uneven distribution of samples across experts, which leads to insufficient and biased calibration for less frequently utilized experts; (2) Intra-expert imbalance, arising from MoE's unique aggregation mechanism, which leads to varying degrees of correlation between different samples and their assigned experts. To address these challenges, we propose MoEQuant, a novel quantization framework tailored for MoE LLMs. MoE-Quant includes two novel techniques: 1) Expert-Balanced Self-Sampling (EBSS) is an efficient sampling method that efficiently constructs a calibration set with balanced expert distributions by leveraging the cumulative probabilities of tokens and expert balance metrics as guiding factors. 2) Affinity-Guided Quantization (AGQ), which incorporates affinities between experts and samples into the quantization process, thereby accurately assessing the impact of individual samples on different experts within the MoE layer. Experiments demonstrate that MoEQuant achieves substantial performance gains (more than 10 points accuracy gain in the HumanEval for DeepSeekMoE-16B under 4-bit quantization) and boosts efficiency.</li>
</ul>

<h3>Title: Feature Optimization for Time Series Forecasting via Novel Randomized Uphill Climbing</h3>
<ul>
<li><strong>Authors: </strong>Nguyen Van Thanh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.PF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03805">https://arxiv.org/abs/2505.03805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03805">https://arxiv.org/pdf/2505.03805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03805]] Feature Optimization for Time Series Forecasting via Novel Randomized Uphill Climbing(https://arxiv.org/abs/2505.03805)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Randomized Uphill Climbing is a lightweight, stochastic search heuristic that has delivered state of the art equity alpha factors for quantitative hedge funds. I propose to generalize RUC into a model agnostic feature optimization framework for multivariate time series forecasting. The core idea is to synthesize candidate feature programs by randomly composing operators from a domain specific grammar, score candidates rapidly with inexpensive surrogate models on rolling windows, and filter instability via nested cross validation and information theoretic shrinkage. By decoupling feature discovery from GPU heavy deep learning, the method promises faster iteration cycles, lower energy consumption, and greater interpretability. Societal relevance: accurate, transparent forecasting tools empower resource constrained institutions, energy regulators, climate risk NGOs to make data driven decisions without proprietary black box models.</li>
</ul>

<h3>Title: AI-driven multi-source data fusion for algal bloom severity classification in small inland water bodies: Leveraging Sentinel-2, DEM, and NOAA climate data</h3>
<ul>
<li><strong>Authors: </strong>Ioannis Nasios</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03808">https://arxiv.org/abs/2505.03808</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03808">https://arxiv.org/pdf/2505.03808</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03808]] AI-driven multi-source data fusion for algal bloom severity classification in small inland water bodies: Leveraging Sentinel-2, DEM, and NOAA climate data(https://arxiv.org/abs/2505.03808)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Harmful algal blooms are a growing threat to inland water quality and public health worldwide, creating an urgent need for efficient, accurate, and cost-effective detection methods. This research introduces a high-performing methodology that integrates multiple open-source remote sensing data with advanced artificial intelligence models. Key data sources include Copernicus Sentinel-2 optical imagery, the Copernicus Digital Elevation Model (DEM), and NOAA's High-Resolution Rapid Refresh (HRRR) climate data, all efficiently retrieved using platforms like Google Earth Engine (GEE) and Microsoft Planetary Computer (MPC). The NIR and two SWIR bands from Sentinel-2, the altitude from the elevation model, the temperature and wind from NOAA as well as the longitude and latitude were the most important features. The approach combines two types of machine learning models, tree-based models and a neural network, into an ensemble for classifying algal bloom severity. While the tree models performed strongly on their own, incorporating a neural network added robustness and demonstrated how deep learning models can effectively use diverse remote sensing inputs. The method leverages high-resolution satellite imagery and AI-driven analysis to monitor algal blooms dynamically, and although initially developed for a NASA competition in the U.S., it shows potential for global application. The complete code is available for further adaptation and practical implementation, illustrating the convergence of remote sensing data and AI to address critical environmental challenges (this https URL).</li>
</ul>

<h3>Title: When Dynamic Data Selection Meets Data Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Suorong Yang, Peng Ye, Furao Shen, Dongzhan Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03809">https://arxiv.org/abs/2505.03809</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03809">https://arxiv.org/pdf/2505.03809</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03809]] When Dynamic Data Selection Meets Data Augmentation(https://arxiv.org/abs/2505.03809)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Dynamic data selection aims to accelerate training with lossless performance. However, reducing training data inherently limits data diversity, potentially hindering generalization. While data augmentation is widely used to enhance diversity, it is typically not optimized in conjunction with selection. As a result, directly combining these techniques fails to fully exploit their synergies. To tackle the challenge, we propose a novel online data training framework that, for the first time, unifies dynamic data selection and augmentation, achieving both training efficiency and enhanced performance. Our method estimates each sample's joint distribution of local density and multimodal semantic consistency, allowing for the targeted selection of augmentation-suitable samples while suppressing the inclusion of noisy or ambiguous data. This enables a more significant reduction in dataset size without sacrificing model generalization. Experimental results demonstrate that our method outperforms existing state-of-the-art approaches on various benchmark datasets and architectures, e.g., reducing 50\% training costs on ImageNet-1k with lossless performance. Furthermore, our approach enhances noise resistance and improves model robustness, reinforcing its practical utility in real-world scenarios.</li>
</ul>

<h3>Title: Grouped Sequency-arranged Rotation: Optimizing Rotation Transformation for Quantization for Free</h3>
<ul>
<li><strong>Authors: </strong>Euntae Choi, Sumin Song, Woosang Lim, Sungjoo Yoo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03810">https://arxiv.org/abs/2505.03810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03810">https://arxiv.org/pdf/2505.03810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03810]] Grouped Sequency-arranged Rotation: Optimizing Rotation Transformation for Quantization for Free(https://arxiv.org/abs/2505.03810)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) face deployment challenges due to high computational costs, and while Post-Training Quantization (PTQ) offers a solution, existing rotation-based methods struggle at very low bit-widths like 2-bit. We introduce a novel, training-free approach to construct an improved rotation matrix, addressing the limitations of current methods. The key contributions include leveraging the Walsh-Hadamard transform with sequency ordering, which clusters similar frequency components to reduce quantization error compared to standard Hadamard matrices, significantly improving performance. Furthermore, we propose a Grouped Sequency-arranged Rotation (GSR) using block-diagonal matrices with smaller Walsh blocks, effectively isolating outlier impacts and achieving performance comparable to optimization-based methods without requiring any training. Our method demonstrates robust performance on reasoning tasks and Perplexity (PPL) score on WikiText-2. Our method also enhances results even when applied over existing learned rotation techniques.</li>
</ul>

<h3>Title: ScarceGAN: Discriminative Classification Framework for Rare Class Identification for Longitudinal Data with Weak Prior</h3>
<ul>
<li><strong>Authors: </strong>Surajit Chakrabarty, Rukma Talwadker, Tridib Mukherjee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03811">https://arxiv.org/abs/2505.03811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03811">https://arxiv.org/pdf/2505.03811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03811]] ScarceGAN: Discriminative Classification Framework for Rare Class Identification for Longitudinal Data with Weak Prior(https://arxiv.org/abs/2505.03811)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>This paper introduces ScarceGAN which focuses on identification of extremely rare or scarce samples from multi-dimensional longitudinal telemetry data with small and weak label prior. We specifically address: (i) severe scarcity in positive class, stemming from both underlying organic skew in the data, as well as extremely limited labels; (ii) multi-class nature of the negative samples, with uneven density distributions and partially overlapping feature distributions; and (iii) massively unlabelled data leading to tiny and weak prior on both positive and negative classes, and possibility of unseen or unknown behavior in the unlabelled set, especially in the negative class. Although related to PU learning problems, we contend that knowledge (or lack of it) on the negative class can be leveraged to learn the compliment of it (i.e., the positive class) better in a semi-supervised manner. To this effect, ScarceGAN re-formulates semi-supervised GAN by accommodating weakly labelled multi-class negative samples and the available positive samples. It relaxes the supervised discriminator's constraint on exact differentiation between negative samples by introducing a 'leeway' term for samples with noisy prior. We propose modifications to the cost objectives of discriminator, in supervised and unsupervised path as well as that of the generator. For identifying risky players in skill gaming, this formulation in whole gives us a recall of over 85% (~60% jump over vanilla semi-supervised GAN) on our scarce class with very minimal verbosity in the unknown space. Further ScarceGAN outperforms the recall benchmarks established by recent GAN based specialized models for the positive imbalanced class identification and establishes a new benchmark in identifying one of rare attack classes (0.09%) in the intrusion dataset from the KDDCUP99 challenge.</li>
</ul>

<h3>Title: Information Filtering Networks: Theoretical Foundations, Generative Methodologies, and Real-World Applications</h3>
<ul>
<li><strong>Authors: </strong>Tomaso Aste</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03812">https://arxiv.org/abs/2505.03812</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03812">https://arxiv.org/pdf/2505.03812</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03812]] Information Filtering Networks: Theoretical Foundations, Generative Methodologies, and Real-World Applications(https://arxiv.org/abs/2505.03812)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, generative</a></li>
<li><strong>Abstract: </strong>Information Filtering Networks (IFNs) provide a powerful framework for modeling complex systems through globally sparse yet locally dense and interpretable structures that capture multivariate dependencies. This review offers a comprehensive account of IFNs, covering their theoretical foundations, construction methodologies, and diverse applications. Tracing their origins from early network-based models to advanced formulations such as the Triangulated Maximally Filtered Graph (TMFG) and the Maximally Filtered Clique Forest (MFCF), the paper highlights how IFNs address key challenges in high-dimensional data-driven modeling. IFNs and their construction methodologies are intrinsically higher-order networks that generate simplicial complexes-structures that are only now becoming popular in the broader literature. Applications span fields including finance, biology, psychology, and artificial intelligence, where IFNs improve interpretability, computational efficiency, and predictive performance. Special attention is given to their role in graphical modeling, where IFNs enable the estimation of sparse inverse covariance matrices with greater accuracy and scalability than traditional approaches like Graphical LASSO. Finally, the review discusses recent developments that integrate IFNs with machine learning and deep learning, underscoring their potential not only to bridge classical network theory with contemporary data-driven paradigms, but also to shape the architectures of deep learning models themselves.</li>
</ul>

<h3>Title: Modeling Behavioral Preferences of Cyber Adversaries Using Inverse Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Aditya Shinde, Prashant Doshi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03817">https://arxiv.org/abs/2505.03817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03817">https://arxiv.org/pdf/2505.03817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03817]] Modeling Behavioral Preferences of Cyber Adversaries Using Inverse Reinforcement Learning(https://arxiv.org/abs/2505.03817)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>This paper presents a holistic approach to attacker preference modeling from system-level audit logs using inverse reinforcement learning (IRL). Adversary modeling is an important capability in cybersecurity that lets defenders characterize behaviors of potential attackers, which enables attribution to known cyber adversary groups. Existing approaches rely on documenting an ever-evolving set of attacker tools and techniques to track known threat actors. Although attacks evolve constantly, attacker behavioral preferences are intrinsic and less volatile. Our approach learns the behavioral preferences of cyber adversaries from forensics data on their tools and techniques. We model the attacker as an expert decision-making agent with unknown behavioral preferences situated in a computer host. We leverage attack provenance graphs of audit logs to derive a state-action trajectory of the attack. We test our approach on open datasets of audit logs containing real attack data. Our results demonstrate for the first time that low-level forensics data can automatically reveal an adversary's subjective preferences, which serves as an additional dimension to modeling and documenting cyber adversaries. Attackers' preferences tend to be invariant despite their different tools and indicate predispositions that are inherent to the attacker. As such, these inferred preferences can potentially serve as unique behavioral signatures of attackers and improve threat attribution.</li>
</ul>

<h3>Title: Program Semantic Inequivalence Game with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Antonio Valerio Miceli-Barone, Vaishak Belle, Ali Payani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03818">https://arxiv.org/abs/2505.03818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03818">https://arxiv.org/pdf/2505.03818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03818]] Program Semantic Inequivalence Game with Large Language Models(https://arxiv.org/abs/2505.03818)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) can achieve strong performance on everyday coding tasks, but they can fail on complex tasks that require non-trivial reasoning about program semantics. Finding training examples to teach LLMs to solve these tasks can be challenging. In this work, we explore a method to synthetically generate code reasoning training data based on a semantic inequivalence game SInQ: a generator agent creates program variants that are semantically distinct, derived from a dataset of real-world programming tasks, while an evaluator agent has to identify input examples that cause the original programs and the generated variants to diverge in their behaviour, with the agents training each other semi-adversarially. We prove that this setup enables theoretically unlimited improvement through self-play in the limit of infinite computational resources. We evaluated our approach on multiple code generation and understanding benchmarks, including cross-language vulnerability detection (Lu et al., 2021), where our method improves vulnerability detection in C/C++ code despite being trained exclusively on Python code, and the challenging Python builtin identifier swap benchmark (Miceli-Barone et al., 2023), showing that whereas modern LLMs still struggle with this benchmark, our approach yields substantial improvements. We release the code needed to replicate the experiments, as well as the generated synthetic data, which can be used to fine-tune LLMs.</li>
</ul>

<h3>Title: VideoLLM Benchmarks and Evaluation: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Yogesh Kumar</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03829">https://arxiv.org/abs/2505.03829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03829">https://arxiv.org/pdf/2505.03829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03829]] VideoLLM Benchmarks and Evaluation: A Survey(https://arxiv.org/abs/2505.03829)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>The rapid development of Large Language Models (LLMs) has catalyzed significant advancements in video understanding technologies. This survey provides a comprehensive analysis of benchmarks and evaluation methodologies specifically designed or used for Video Large Language Models (VideoLLMs). We examine the current landscape of video understanding benchmarks, discussing their characteristics, evaluation protocols, and limitations. The paper analyzes various evaluation methodologies, including closed-set, open-set, and specialized evaluations for temporal and spatiotemporal understanding tasks. We highlight the performance trends of state-of-the-art VideoLLMs across these benchmarks and identify key challenges in current evaluation frameworks. Additionally, we propose future research directions to enhance benchmark design, evaluation metrics, and protocols, including the need for more diverse, multimodal, and interpretability-focused benchmarks. This survey aims to equip researchers with a structured understanding of how to effectively evaluate VideoLLMs and identify promising avenues for advancing the field of video understanding with large language models.</li>
</ul>

<h3>Title: A Comprehensive Analysis of Adversarial Attacks against Spam Filters</h3>
<ul>
<li><strong>Authors: </strong>Esra Hotoƒülu, Sevil Sen, Burcu Can</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03831">https://arxiv.org/abs/2505.03831</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03831">https://arxiv.org/pdf/2505.03831</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03831]] A Comprehensive Analysis of Adversarial Attacks against Spam Filters(https://arxiv.org/abs/2505.03831)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack</a></li>
<li><strong>Abstract: </strong>Deep learning has revolutionized email filtering, which is critical to protect users from cyber threats such as spam, malware, and phishing. However, the increasing sophistication of adversarial attacks poses a significant challenge to the effectiveness of these filters. This study investigates the impact of adversarial attacks on deep learning-based spam detection systems using real-world datasets. Six prominent deep learning models are evaluated on these datasets, analyzing attacks at the word, character sentence, and AI-generated paragraph-levels. Novel scoring functions, including spam weights and attention weights, are introduced to improve attack effectiveness. This comprehensive analysis sheds light on the vulnerabilities of spam filters and contributes to efforts to improve their security against evolving adversarial threats.</li>
</ul>

<h3>Title: Video Forgery Detection for Surveillance Cameras: A Review</h3>
<ul>
<li><strong>Authors: </strong>Noor B. Tayfor, Tarik A. Rashid, Shko M. Qader, Bryar A. Hassan, Mohammed H. Abdalla, Jafar Majidpour, Aram M. Ahmed, Hussein M. Ali, Aso M. Aladdin, Abdulhady A. Abdullah, Ahmed S. Shamsaldin, Haval M. Sidqi, Abdulrahman Salih, Zaher M. Yaseen, Azad A. Ameen, Janmenjoy Nayak, Mahmood Yashar Hamza</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03832">https://arxiv.org/abs/2505.03832</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03832">https://arxiv.org/pdf/2505.03832</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03832]] Video Forgery Detection for Surveillance Cameras: A Review(https://arxiv.org/abs/2505.03832)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust</a></li>
<li><strong>Abstract: </strong>The widespread availability of video recording through smartphones and digital devices has made video-based evidence more accessible than ever. Surveillance footage plays a crucial role in security, law enforcement, and judicial processes. However, with the rise of advanced video editing tools, tampering with digital recordings has become increasingly easy, raising concerns about their authenticity. Ensuring the integrity of surveillance videos is essential, as manipulated footage can lead to misinformation and undermine judicial decisions. This paper provides a comprehensive review of existing forensic techniques used to detect video forgery, focusing on their effectiveness in verifying the authenticity of surveillance recordings. Various methods, including compression-based analysis, frame duplication detection, and machine learning-based approaches, are explored. The findings highlight the growing necessity for more robust forensic techniques to counteract evolving forgery methods. Strengthening video forensic capabilities will ensure that surveillance recordings remain credible and admissible as legal evidence.</li>
</ul>

<h3>Title: PointExplainer: Towards Transparent Parkinson's Disease Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Xuechao Wang, Sven Nomm, Junqing Huang, Kadri Medijainen, Aaro Toomela, Michael Ruzhansky</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03833">https://arxiv.org/abs/2505.03833</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03833">https://arxiv.org/pdf/2505.03833</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03833]] PointExplainer: Towards Transparent Parkinson's Disease Diagnosis(https://arxiv.org/abs/2505.03833)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Deep neural networks have shown potential in analyzing digitized hand-drawn signals for early diagnosis of Parkinson's disease. However, the lack of clear interpretability in existing diagnostic methods presents a challenge to clinical trust. In this paper, we propose PointExplainer, an explainable diagnostic strategy to identify hand-drawn regions that drive model diagnosis. Specifically, PointExplainer assigns discrete attribution values to hand-drawn segments, explicitly quantifying their relative contributions to the model's decision. Its key components include: (i) a diagnosis module, which encodes hand-drawn signals into 3D point clouds to represent hand-drawn trajectories, and (ii) an explanation module, which trains an interpretable surrogate model to approximate the local behavior of the black-box diagnostic model. We also introduce consistency measures to further address the issue of faithfulness in explanations. Extensive experiments on two benchmark datasets and a newly constructed dataset show that PointExplainer can provide intuitive explanations with no diagnostic performance degradation. The source code is available at this https URL.</li>
</ul>

<h3>Title: Explainable Face Recognition via Improved Localization</h3>
<ul>
<li><strong>Authors: </strong>Rashik Shadman, Daqing Hou, Faraz Hussain, M G Sarwar Murshed</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03837">https://arxiv.org/abs/2505.03837</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03837">https://arxiv.org/pdf/2505.03837</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03837]] Explainable Face Recognition via Improved Localization(https://arxiv.org/abs/2505.03837)</code><input type="text"></li>
<li><strong>Keywords: </strong>biometric</a></li>
<li><strong>Abstract: </strong>Biometric authentication has become one of the most widely used tools in the current technological era to authenticate users and to distinguish between genuine users and imposters. Face is the most common form of biometric modality that has proven effective. Deep learning-based face recognition systems are now commonly used across different domains. However, these systems usually operate like black-box models that do not provide necessary explanations or justifications for their decisions. This is a major disadvantage because users cannot trust such artificial intelligence-based biometric systems and may not feel comfortable using them when clear explanations or justifications are not provided. This paper addresses this problem by applying an efficient method for explainable face recognition systems. We use a Class Activation Mapping (CAM)-based discriminative localization (very narrow/specific localization) technique called Scaled Directed Divergence (SDD) to visually explain the results of deep learning-based face recognition systems. We perform fine localization of the face features relevant to the deep learning model for its prediction/decision. Our experiments show that the SDD Class Activation Map (CAM) highlights the relevant face features very specifically compared to the traditional CAM and very accurately. The provided visual explanations with narrow localization of relevant features can ensure much-needed transparency and trust for deep learning-based face recognition systems.</li>
</ul>

<h3>Title: Economic Security of Multiple Shared Security Protocols</h3>
<ul>
<li><strong>Authors: </strong>Abhimanyu Nag, Dhruv Bodani, Abhishek Kumar</a></li>
<li><strong>Subjects: </strong>cs.CR, q-fin.RM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03843">https://arxiv.org/abs/2505.03843</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03843">https://arxiv.org/pdf/2505.03843</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03843]] Economic Security of Multiple Shared Security Protocols(https://arxiv.org/abs/2505.03843)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>As restaking protocols gain adoption across blockchain ecosystems, there is a need for Actively Validated Services (AVSs) to span multiple Shared Security Providers (SSPs). This leads to stake fragmentation which introduces new complications where an adversary may compromise an AVS by targeting its weakest SSP. In this paper, we formalize the Multiple SSP Problem and analyze two architectures : an isolated fragmented model called Model $\mathbb{M}$ and a shared unified model called Model $\mathbb{S}$, through a convex optimization and game-theoretic lens. We derive utility bounds, attack cost conditions, and market equilibrium that describes protocol security for both models. Our results show that while Model $\mathbb{M}$ offers deployment flexibility, it inherits lowest-cost attack vulnerabilities, whereas Model $\mathbb{S}$ achieves tighter security guarantees through single validator sets and aggregated slashing logic. We conclude with future directions of work including an incentive-compatible stake rebalancing allocation in restaking ecosystems.</li>
</ul>

<h3>Title: GAME: Learning Multimodal Interactions via Graph Structures for Personality Trait Estimation</h3>
<ul>
<li><strong>Authors: </strong>Kangsheng Wang, Yuhang Li, Chengwei Ye, Yufei Lin, Huanzhen Zhang, Bohan Hu, Linuo Xu, Shuyan Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03846">https://arxiv.org/abs/2505.03846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03846">https://arxiv.org/pdf/2505.03846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03846]] GAME: Learning Multimodal Interactions via Graph Structures for Personality Trait Estimation(https://arxiv.org/abs/2505.03846)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Apparent personality analysis from short videos poses significant chal-lenges due to the complex interplay of visual, auditory, and textual cues. In this paper, we propose GAME, a Graph-Augmented Multimodal Encoder designed to robustly model and fuse multi-source features for automatic personality prediction. For the visual stream, we construct a facial graph and introduce a dual-branch Geo Two-Stream Network, which combines Graph Convolutional Networks (GCNs) and Convolutional Neural Net-works (CNNs) with attention mechanisms to capture both structural and appearance-based facial cues. Complementing this, global context and iden-tity features are extracted using pretrained ResNet18 and VGGFace back-bones. To capture temporal dynamics, frame-level features are processed by a BiGRU enhanced with temporal attention modules. Meanwhile, audio representations are derived from the VGGish network, and linguistic se-mantics are captured via the XLM-Roberta transformer. To achieve effective multimodal integration, we propose a Channel Attention-based Fusion module, followed by a Multi-Layer Perceptron (MLP) regression head for predicting personality traits. Extensive experiments show that GAME con-sistently outperforms existing methods across multiple benchmarks, vali-dating its effectiveness and generalizability.</li>
</ul>

<h3>Title: Impact Analysis of Inference Time Attack of Perception Sensors on Autonomous Vehicles</h3>
<ul>
<li><strong>Authors: </strong>Hanlin Chen, Simin Chen, Wenyu Li, Wei Yang, Yiheng Feng</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03850">https://arxiv.org/abs/2505.03850</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03850">https://arxiv.org/pdf/2505.03850</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03850]] Impact Analysis of Inference Time Attack of Perception Sensors on Autonomous Vehicles(https://arxiv.org/abs/2505.03850)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>As a safety-critical cyber-physical system, cybersecurity and related safety issues for Autonomous Vehicles (AVs) have been important research topics for a while. Among all the modules on AVs, perception is one of the most accessible attack surfaces, as drivers and AVs have no control over the outside environment. Most current work targeting perception security for AVs focuses on perception correctness. In this work, we propose an impact analysis based on inference time attacks for autonomous vehicles. We demonstrate in a simulation system that such inference time attacks can also threaten the safety of both the ego vehicle and other traffic participants.</li>
</ul>

<h3>Title: Machine Learning: a Lecture Note</h3>
<ul>
<li><strong>Authors: </strong>Kyunghyun Cho</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03861">https://arxiv.org/abs/2505.03861</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03861">https://arxiv.org/pdf/2505.03861</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03861]] Machine Learning: a Lecture Note(https://arxiv.org/abs/2505.03861)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This lecture note is intended to prepare early-year master's and PhD students in data science or a related discipline with foundational ideas in machine learning. It starts with basic ideas in modern machine learning with classification as a main target task. These basic ideas include loss formulation, backpropagation, stochastic gradient descent, generalization, model selection as well as fundamental blocks of artificial neural networks. Based on these basic ideas, the lecture note explores in depth the probablistic approach to unsupervised learning, covering directed latent variable models, product of experts, generative adversarial networks and autoregressive models. Finally, the note ends by covering a diverse set of further topics, such as reinforcement learning, ensemble methods and meta-learning. After reading this lecture note, a student should be ready to embark on studying and researching more advanced topics in machine learning and more broadly artificial intelligence.</li>
</ul>

<h3>Title: Data-Driven Falsification of Cyber-Physical Systems</h3>
<ul>
<li><strong>Authors: </strong>Atanu Kundu, Sauvik Gon, Rajarshi Ray</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03863">https://arxiv.org/abs/2505.03863</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03863">https://arxiv.org/pdf/2505.03863</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03863]] Data-Driven Falsification of Cyber-Physical Systems(https://arxiv.org/abs/2505.03863)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, interpretability</a></li>
<li><strong>Abstract: </strong>Cyber-Physical Systems (CPS) are abundant in safety-critical domains such as healthcare, avionics, and autonomous vehicles. Formal verification of their operational safety is, therefore, of utmost importance. In this paper, we address the falsification problem, where the focus is on searching for an unsafe execution in the system instead of proving their absence. The contribution of this paper is a framework that (a) connects the falsification of CPS with the falsification of deep neural networks (DNNs) and (b) leverages the inherent interpretability of Decision Trees for faster falsification of CPS. This is achieved by: (1) building a surrogate model of the CPS under test, either as a DNN model or a Decision Tree, (2) application of various DNN falsification tools to falsify CPS, and (3) a novel falsification algorithm guided by the explanations of safety violations of the CPS model extracted from its Decision Tree surrogate. The proposed framework has the potential to exploit a repertoire of \emph{adversarial attack} algorithms designed to falsify robustness properties of DNNs, as well as state-of-the-art falsification algorithms for DNNs. Although the presented methodology is applicable to systems that can be executed/simulated in general, we demonstrate its effectiveness, particularly in CPS. We show that our framework, implemented as a tool \textsc{FlexiFal}, can detect hard-to-find counterexamples in CPS that have linear and non-linear dynamics. Decision tree-guided falsification shows promising results in efficiently finding multiple counterexamples in the ARCH-COMP 2024 falsification benchmarks~\cite{khandait2024arch}.</li>
</ul>

<h3>Title: Novel Extraction of Discriminative Fine-Grained Feature to Improve Retinal Vessel Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Shuang Zeng, Chee Hong Lee, Micky C Nnamdi, Wenqi Shi, J Ben Tamo, Lei Zhu, Hangzhou He, Xinliang Zhang, Qian Chen, May D. Wang, Yanye Lu, Qiushi Ren</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03896">https://arxiv.org/abs/2505.03896</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03896">https://arxiv.org/pdf/2505.03896</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03896]] Novel Extraction of Discriminative Fine-Grained Feature to Improve Retinal Vessel Segmentation(https://arxiv.org/abs/2505.03896)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, interpretability, segmentation</a></li>
<li><strong>Abstract: </strong>Retinal vessel segmentation is a vital early detection method for several severe ocular diseases. Despite significant progress in retinal vessel segmentation with the advancement of Neural Networks, there are still challenges to overcome. Specifically, retinal vessel segmentation aims to predict the class label for every pixel within a fundus image, with a primary focus on intra-image discrimination, making it vital for models to extract more discriminative features. Nevertheless, existing methods primarily focus on minimizing the difference between the output from the decoder and the label, but ignore fully using feature-level fine-grained representations from the encoder. To address these issues, we propose a novel Attention U-shaped Kolmogorov-Arnold Network named AttUKAN along with a novel Label-guided Pixel-wise Contrastive Loss for retinal vessel segmentation. Specifically, we implement Attention Gates into Kolmogorov-Arnold Networks to enhance model sensitivity by suppressing irrelevant feature activations and model interpretability by non-linear modeling of KAN blocks. Additionally, we also design a novel Label-guided Pixel-wise Contrastive Loss to supervise our proposed AttUKAN to extract more discriminative features by distinguishing between foreground vessel-pixel pairs and background pairs. Experiments are conducted across four public datasets including DRIVE, STARE, CHASE_DB1, HRF and our private dataset. AttUKAN achieves F1 scores of 82.50%, 81.14%, 81.34%, 80.21% and 80.09%, along with MIoU scores of 70.24%, 68.64%, 68.59%, 67.21% and 66.94% in the above datasets, which are the highest compared to 11 networks for retinal vessel segmentation. Quantitative and qualitative results show that our AttUKAN achieves state-of-the-art performance and outperforms existing retinal vessel segmentation methods. Our code will be available at this https URL.</li>
</ul>

<h3>Title: Explaining Anomalies with Tensor Networks</h3>
<ul>
<li><strong>Authors: </strong>Hans Hohenfeld, Marius Beuerle, Elie Mounzer</a></li>
<li><strong>Subjects: </strong>cs.LG, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03911">https://arxiv.org/abs/2505.03911</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03911">https://arxiv.org/pdf/2505.03911</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03911]] Explaining Anomalies with Tensor Networks(https://arxiv.org/abs/2505.03911)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Tensor networks, a class of variational quantum many-body wave functions have attracted considerable research interest across many disciplines, including classical machine learning. Recently, Aizpurua et al. demonstrated explainable anomaly detection with matrix product states on a discrete-valued cyber-security task, using quantum-inspired methods to gain insight into the learned model and detected anomalies. Here, we extend this framework to real-valued data domains. We furthermore introduce tree tensor networks for the task of explainable anomaly detection. We demonstrate these methods with three benchmark problems, show adequate predictive performance compared to several baseline models and both tensor network architectures' ability to explain anomalous samples. We thereby extend the application of tensor networks to a broader class of potential problems and open a pathway for future extensions to more complex tensor network architectures.</li>
</ul>

<h3>Title: SAND: One-Shot Feature Selection with Additive Noise Distortion</h3>
<ul>
<li><strong>Authors: </strong>Pedram Pad, Hadi Hammoud, Mohamad Dia, Nadim Maamari, L. Andrea Dunbar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03923">https://arxiv.org/abs/2505.03923</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03923">https://arxiv.org/pdf/2505.03923</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03923]] SAND: One-Shot Feature Selection with Additive Noise Distortion(https://arxiv.org/abs/2505.03923)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Feature selection is a critical step in data-driven applications, reducing input dimensionality to enhance learning accuracy, computational efficiency, and interpretability. Existing state-of-the-art methods often require post-selection retraining and extensive hyperparameter tuning, complicating their adoption. We introduce a novel, non-intrusive feature selection layer that, given a target feature count $k$, automatically identifies and selects the $k$ most informative features during neural network training. Our method is uniquely simple, requiring no alterations to the loss function, network architecture, or post-selection retraining. The layer is mathematically elegant and can be fully described by: \begin{align} \nonumber \tilde{x}_i = a_i x_i + (1-a_i)z_i \end{align} where $x_i$ is the input feature, $\tilde{x}_i$ the output, $z_i$ a Gaussian noise, and $a_i$ trainable gain such that $\sum_i{a_i^2}=k$. This formulation induces an automatic clustering effect, driving $k$ of the $a_i$ gains to $1$ (selecting informative features) and the rest to $0$ (discarding redundant ones) via weighted noise distortion and gain normalization. Despite its extreme simplicity, our method delivers state-of-the-art performance on standard benchmark datasets and a novel real-world dataset, outperforming or matching existing approaches without requiring hyperparameter search for $k$ or retraining. Theoretical analysis in the context of linear regression further validates its efficacy. Our work demonstrates that simplicity and performance are not mutually exclusive, offering a powerful yet straightforward tool for feature selection in machine learning.</li>
</ul>

<h3>Title: AI-Driven Security in Cloud Computing: Enhancing Threat Detection, Automated Response, and Cyber Resilience</h3>
<ul>
<li><strong>Authors: </strong>Shamnad Mohamed Shaffi, Sunish Vengathattil, Jezeena Nikarthil Sidhick, Resmi Vijayan</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03945">https://arxiv.org/abs/2505.03945</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03945">https://arxiv.org/pdf/2505.03945</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03945]] AI-Driven Security in Cloud Computing: Enhancing Threat Detection, Automated Response, and Cyber Resilience(https://arxiv.org/abs/2505.03945)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect</a></li>
<li><strong>Abstract: </strong>Cloud security concerns have been greatly realized in recent years due to the increase of complicated threats in the computing world. Many traditional solutions do not work well in real-time to detect or prevent more complex threats. Artificial intelligence is today regarded as a revolution in determining a protection plan for cloud data architecture through machine learning, statistical visualization of computing infrastructure, and detection of security breaches followed by counteraction. These AI-enabled systems make work easier as more network activities are scrutinized, and any anomalous behavior that might be a precursor to a more serious breach is prevented. This paper examines ways AI can enhance cloud security by applying predictive analytics, behavior-based security threat detection, and AI-stirring encryption. It also outlines the problems of the previous security models and how AI overcomes them. For a similar reason, issues like data privacy, biases in the AI model, and regulatory compliance are also covered. So, AI improves the protection of cloud computing contexts; however, more efforts are needed in the subsequent phases to extend the technology's reliability, modularity, and ethical aspects. This means that AI can be blended with other new computing technologies, including blockchain, to improve security frameworks further. The paper discusses the current trends in securing cloud data architecture using AI and presents further research and application directions.</li>
</ul>

<h3>Title: A Reasoning-Focused Legal Retrieval Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Lucia Zheng, Neel Guha, Javokhir Arifov, Sarah Zhang, Michal Skreta, Christopher D. Manning, Peter Henderson, Daniel E. Ho</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03970">https://arxiv.org/abs/2505.03970</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03970">https://arxiv.org/pdf/2505.03970</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03970]] A Reasoning-Focused Legal Retrieval Benchmark(https://arxiv.org/abs/2505.03970)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>As the legal community increasingly examines the use of large language models (LLMs) for various legal applications, legal AI developers have turned to retrieval-augmented LLMs ("RAG" systems) to improve system performance and robustness. An obstacle to the development of specialized RAG systems is the lack of realistic legal RAG benchmarks which capture the complexity of both legal retrieval and downstream legal question-answering. To address this, we introduce two novel legal RAG benchmarks: Bar Exam QA and Housing Statute QA. Our tasks correspond to real-world legal research tasks, and were produced through annotation processes which resemble legal research. We describe the construction of these benchmarks and the performance of existing retriever pipelines. Our results suggest that legal RAG remains a challenging application, thus motivating future research.</li>
</ul>

<h3>Title: Diffusion Models are Secretly Exchangeable: Parallelizing DDPMs via Autospeculation</h3>
<ul>
<li><strong>Authors: </strong>Hengyuan Hu, Aniket Das, Dorsa Sadigh, Nima Anari</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03983">https://arxiv.org/abs/2505.03983</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03983">https://arxiv.org/pdf/2505.03983</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03983]] Diffusion Models are Secretly Exchangeable: Parallelizing DDPMs via Autospeculation(https://arxiv.org/abs/2505.03983)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Denoising Diffusion Probabilistic Models (DDPMs) have emerged as powerful tools for generative modeling. However, their sequential computation requirements lead to significant inference-time bottlenecks. In this work, we utilize the connection between DDPMs and Stochastic Localization to prove that, under an appropriate reparametrization, the increments of DDPM satisfy an exchangeability property. This general insight enables near-black-box adaptation of various performance optimization techniques from autoregressive models to the diffusion setting. To demonstrate this, we introduce \emph{Autospeculative Decoding} (ASD), an extension of the widely used speculative decoding algorithm to DDPMs that does not require any auxiliary draft models. Our theoretical analysis shows that ASD achieves a $\tilde{O} (K^{\frac{1}{3}})$ parallel runtime speedup over the $K$ step sequential DDPM. We also demonstrate that a practical implementation of autospeculative decoding accelerates DDPM inference significantly in various domains.</li>
</ul>

<h3>Title: Action Spotting and Precise Event Detection in Sports: Datasets, Methods, and Challenges</h3>
<ul>
<li><strong>Authors: </strong>Hao Xu, Arbind Agrahari Baniya, Sam Well, Mohamed Reda Bouadjenek, Richard Dazeley, Sunil Aryal</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03991">https://arxiv.org/abs/2505.03991</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03991">https://arxiv.org/pdf/2505.03991</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03991]] Action Spotting and Precise Event Detection in Sports: Datasets, Methods, and Challenges(https://arxiv.org/abs/2505.03991)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Video event detection has become an essential component of sports analytics, enabling automated identification of key moments and enhancing performance analysis, viewer engagement, and broadcast efficiency. Recent advancements in deep learning, particularly Convolutional Neural Networks (CNNs) and Transformers, have significantly improved accuracy and efficiency in Temporal Action Localization (TAL), Action Spotting (AS), and Precise Event Spotting (PES). This survey provides a comprehensive overview of these three key tasks, emphasizing their differences, applications, and the evolution of methodological approaches. We thoroughly review and categorize existing datasets and evaluation metrics specifically tailored for sports contexts, highlighting the strengths and limitations of each. Furthermore, we analyze state-of-the-art techniques, including multi-modal approaches that integrate audio and visual information, methods utilizing self-supervised learning and knowledge distillation, and approaches aimed at generalizing across multiple sports. Finally, we discuss critical open challenges and outline promising research directions toward developing more generalized, efficient, and robust event detection frameworks applicable to diverse sports. This survey serves as a foundation for future research on efficient, generalizable, and multi-modal sports event detection.</li>
</ul>

<h3>Title: Algorithmic Accountability in Small Data: Sample-Size-Induced Bias Within Classification Metrics</h3>
<ul>
<li><strong>Authors: </strong>Jarren Briscoe, Garrett Kepler, Daryl Deford, Assefaw Gebremedhin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03992">https://arxiv.org/abs/2505.03992</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03992">https://arxiv.org/pdf/2505.03992</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03992]] Algorithmic Accountability in Small Data: Sample-Size-Induced Bias Within Classification Metrics(https://arxiv.org/abs/2505.03992)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Evaluating machine learning models is crucial not only for determining their technical accuracy but also for assessing their potential societal implications. While the potential for low-sample-size bias in algorithms is well known, we demonstrate the significance of sample-size bias induced by combinatorics in classification metrics. This revelation challenges the efficacy of these metrics in assessing bias with high resolution, especially when comparing groups of disparate sizes, which frequently arise in social applications. We provide analyses of the bias that appears in several commonly applied metrics and propose a model-agnostic assessment and correction technique. Additionally, we analyze counts of undefined cases in metric calculations, which can lead to misleading evaluations if improperly handled. This work illuminates the previously unrecognized challenge of combinatorics and probability in standard evaluation practices and thereby advances approaches for performing fair and trustworthy classification methods.</li>
</ul>

<h3>Title: Quiet Feature Learning in Algorithmic Tasks</h3>
<ul>
<li><strong>Authors: </strong>Prudhviraj Naidu, Zixian Wang, Leon Bergen, Ramamohan Paturi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.03997">https://arxiv.org/abs/2505.03997</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.03997">https://arxiv.org/pdf/2505.03997</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.03997]] Quiet Feature Learning in Algorithmic Tasks(https://arxiv.org/abs/2505.03997)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We train Transformer-based language models on ten foundational algorithmic tasks and observe pronounced phase transitions in their loss curves that deviate from established power-law scaling trends. Over large ranges of compute, the validation loss barely improves, then abruptly decreases. Probing the models' internal representations reveals the learning of quiet features during the stagnant phase, followed by sudden acquisition of loud features that coincide with the sharp drop in loss. Our ablation experiments show that disrupting a single learned feature can dramatically degrade performance, providing evidence of their causal role in task performance. These findings challenge the prevailing assumption that next-token predictive loss reliably tracks incremental progress; instead, key internal features may be developing below the surface until they coalesce, triggering a rapid performance gain.</li>
</ul>

<h3>Title: Rollbaccine : Herd Immunity against Storage Rollback Attacks in TEEs [Technical Report]</h3>
<ul>
<li><strong>Authors: </strong>David Chu, Aditya Balasubramanian, Dee Bao, Natacha Crooks, Heidi Howard, Lucky E. Katahanas, Soujanya Ponnapalli</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04014">https://arxiv.org/abs/2505.04014</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04014">https://arxiv.org/pdf/2505.04014</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04014]] Rollbaccine : Herd Immunity against Storage Rollback Attacks in TEEs [Technical Report](https://arxiv.org/abs/2505.04014)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack</a></li>
<li><strong>Abstract: </strong>Today, users can "lift-and-shift" unmodified applications into modern, VM-based Trusted Execution Environments (TEEs) in order to gain hardware-based security guarantees. However, TEEs do not protect applications against disk rollback attacks, where persistent storage can be reverted to an earlier state after a crash; existing rollback resistance solutions either only support a subset of applications or require code modification. Our key insight is that restoring disk consistency after a rollback attack guarantees rollback resistance for any application. We present Rollbaccine, a device mapper that provides automatic rollback resistance for all applications by provably preserving disk consistency. Rollbaccine intercepts and replicates writes to disk, restores lost state from backups during recovery, and minimizes overheads by taking advantage of the weak, multi-threaded semantics of disk operations. Across benchmarks over two real applications (PostgreSQL and HDFS) and two file systems (ext4 and xfs), Rollbaccine adds only 19% overhead, except for the fsync-heavy Filebench Varmail. In addition, Rollbaccine outperforms the state-of-the-art, non-automatic rollback resistant solution by $208\times$.</li>
</ul>

<h3>Title: MergeGuard: Efficient Thwarting of Trojan Attacks in Machine Learning Models</h3>
<ul>
<li><strong>Authors: </strong>Soheil Zibakhsh Shabgahi, Yaman Jandali, Farinaz Koushanfar</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04015">https://arxiv.org/abs/2505.04015</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04015">https://arxiv.org/pdf/2505.04015</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04015]] MergeGuard: Efficient Thwarting of Trojan Attacks in Machine Learning Models(https://arxiv.org/abs/2505.04015)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, transformer</a></li>
<li><strong>Abstract: </strong>This paper proposes MergeGuard, a novel methodology for mitigation of AI Trojan attacks. Trojan attacks on AI models cause inputs embedded with triggers to be misclassified to an adversary's target class, posing a significant threat to model usability trained by an untrusted third party. The core of MergeGuard is a new post-training methodology for linearizing and merging fully connected layers which we show simultaneously improves model generalizability and performance. Our Proof of Concept evaluation on Transformer models demonstrates that MergeGuard maintains model accuracy while decreasing trojan attack success rate, outperforming commonly used (post-training) Trojan mitigation by fine-tuning methodologies.</li>
</ul>

<h3>Title: SLOT: Structuring the Output of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Darren Yow-Bang Wang, Zhengyuan Shen, Soumya Smruti Mishra, Zhichao Xu, Yifei Teng, Haibo Ding</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04016">https://arxiv.org/abs/2505.04016</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04016">https://arxiv.org/pdf/2505.04016</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04016]] SLOT: Structuring the Output of Large Language Models(https://arxiv.org/abs/2505.04016)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Structured outputs are essential for large language models (LLMs) in critical applications like agents and information extraction. Despite their capabilities, LLMs often generate outputs that deviate from predefined schemas, significantly hampering reliable application development. We present SLOT (Structured LLM Output Transformer), a model-agnostic approach that transforms unstructured LLM outputs into precise structured formats. While existing solutions predominantly rely on constrained decoding techniques or are tightly coupled with specific models, SLOT employs a fine-tuned lightweight language model as a post-processing layer, achieving flexibility across various LLMs and schema specifications. We introduce a systematic pipeline for data curation and synthesis alongside a formal evaluation methodology that quantifies both schema accuracy and content fidelity. Our results demonstrate that fine-tuned Mistral-7B model with constrained decoding achieves near perfect schema accuracy (99.5%) and content similarity (94.0%), outperforming Claude-3.5-Sonnet by substantial margins (+25 and +20 percentage points, respectively). Notably, even compact models like Llama-3.2-1B can match or exceed the structured output capabilities of much larger proprietary models when equipped with SLOT, enabling reliable structured generation in resource-constrained environments.</li>
</ul>

<h3>Title: Reliable Disentanglement Multi-view Learning Against View Adversarial Attacks</h3>
<ul>
<li><strong>Authors: </strong>Xuyang Wang, Siyuan Duan, Qizhi Li, Guiduo Duan, Yuan Sun, Dezhong Peng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04046">https://arxiv.org/abs/2505.04046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04046">https://arxiv.org/pdf/2505.04046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04046]] Reliable Disentanglement Multi-view Learning Against View Adversarial Attacks(https://arxiv.org/abs/2505.04046)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>Recently, trustworthy multi-view learning has attracted extensive attention because evidence learning can provide reliable uncertainty estimation to enhance the credibility of multi-view predictions. Existing trusted multi-view learning methods implicitly assume that multi-view data is secure. In practice, however, in safety-sensitive applications such as autonomous driving and security monitoring, multi-view data often faces threats from adversarial perturbations, thereby deceiving or disrupting multi-view learning models. This inevitably leads to the adversarial unreliability problem (AUP) in trusted multi-view learning. To overcome this tricky problem, we propose a novel multi-view learning framework, namely Reliable Disentanglement Multi-view Learning (RDML). Specifically, we first propose evidential disentanglement learning to decompose each view into clean and adversarial parts under the guidance of corresponding evidences, which is extracted by a pretrained evidence extractor. Then, we employ the feature recalibration module to mitigate the negative impact of adversarial perturbations and extract potential informative features from them. Finally, to further ignore the irreparable adversarial interferences, a view-level evidential attention mechanism is designed. Extensive experiments on multi-view classification tasks with adversarial attacks show that our RDML outperforms the state-of-the-art multi-view learning methods by a relatively large margin.</li>
</ul>

<h3>Title: FoodTrack: Estimating Handheld Food Portions with Egocentric Video</h3>
<ul>
<li><strong>Authors: </strong>Ervin Wang, Yuhao Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04055">https://arxiv.org/abs/2505.04055</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04055">https://arxiv.org/pdf/2505.04055</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04055]] FoodTrack: Estimating Handheld Food Portions with Egocentric Video(https://arxiv.org/abs/2505.04055)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurately tracking food consumption is crucial for nutrition and health monitoring. Traditional approaches typically require specific camera angles, non-occluded images, or rely on gesture recognition to estimate intake, making assumptions about bite size rather than directly measuring food volume. We propose the FoodTrack framework for tracking and measuring the volume of hand-held food items using egocentric video which is robust to hand occlusions and flexible with varying camera and object poses. FoodTrack estimates food volume directly, without relying on intake gestures or fixed assumptions about bite size, offering a more accurate and adaptable solution for tracking food consumption. We achieve absolute percentage loss of approximately 7.01% on a handheld food object, improving upon a previous approach that achieved a 16.40% mean absolute percentage error in its best case, under less flexible conditions.</li>
</ul>

<h3>Title: Advancing and Benchmarking Personalized Tool Invocation for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Xu Huang, Yuefeng Huang, Weiwen Liu, Xingshan Zeng, Yasheng Wang, Ruiming Tang, Hong Xie, Defu Lian</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04072">https://arxiv.org/abs/2505.04072</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04072">https://arxiv.org/pdf/2505.04072</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04072]] Advancing and Benchmarking Personalized Tool Invocation for LLMs(https://arxiv.org/abs/2505.04072)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Tool invocation is a crucial mechanism for extending the capabilities of Large Language Models (LLMs) and has recently garnered significant attention. It enables LLMs to solve complex problems through tool calls while accessing up-to-date world knowledge. However, existing work primarily focuses on the fundamental ability of LLMs to invoke tools for problem-solving, without considering personalized constraints in tool invocation. In this work, we introduce the concept of Personalized Tool Invocation and define two key tasks: Tool Preference and Profile-dependent Query. Tool Preference addresses user preferences when selecting among functionally similar tools, while Profile-dependent Query considers cases where a user query lacks certain tool parameters, requiring the model to infer them from the user profile. To tackle these challenges, we propose PTool, a data synthesis framework designed for personalized tool invocation. Additionally, we construct \textbf{PTBench}, the first benchmark for evaluating personalized tool invocation. We then fine-tune various open-source models, demonstrating the effectiveness of our framework and providing valuable insights. Our benchmark is public at this https URL.</li>
</ul>

<h3>Title: Natural Language Generation in Healthcare: A Review of Methods and Applications</h3>
<ul>
<li><strong>Authors: </strong>Mengxian Lyu, Xiaohan Li, Ziyi Chen, Jinqian Pan, Cheng Peng, Sankalp Talankar, Yonghui Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04073">https://arxiv.org/abs/2505.04073</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04073">https://arxiv.org/pdf/2505.04073</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04073]] Natural Language Generation in Healthcare: A Review of Methods and Applications(https://arxiv.org/abs/2505.04073)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Natural language generation (NLG) is the key technology to achieve generative artificial intelligence (AI). With the breakthroughs in large language models (LLMs), NLG has been widely used in various medical applications, demonstrating the potential to enhance clinical workflows, support clinical decision-making, and improve clinical documentation. Heterogeneous and diverse medical data modalities, such as medical text, images, and knowledge bases, are utilized in NLG. Researchers have proposed many generative models and applied them in a number of healthcare applications. There is a need for a comprehensive review of NLG methods and applications in the medical domain. In this study, we systematically reviewed 113 scientific publications from a total of 3,988 NLG-related articles identified using a literature search, focusing on data modality, model architecture, clinical applications, and evaluation methods. Following PRISMA (Preferred Reporting Items for Systematic reviews and Meta-Analyses) guidelines, we categorize key methods, identify clinical applications, and assess their capabilities, limitations, and emerging challenges. This timely review covers the key NLG technologies and medical applications and provides valuable insights for future studies to leverage NLG to transform medical discovery and healthcare.</li>
</ul>

<h3>Title: LLM-e Guess: Can LLMs Capabilities Advance Without Hardware Progress?</h3>
<ul>
<li><strong>Authors: </strong>Teddy Foley, Spencer Guo, Henry Josephson, Anqi Qu, Jack Sanderson</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04075">https://arxiv.org/abs/2505.04075</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04075">https://arxiv.org/pdf/2505.04075</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04075]] LLM-e Guess: Can LLMs Capabilities Advance Without Hardware Progress?(https://arxiv.org/abs/2505.04075)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>This paper examines whether large language model (LLM) capabilities can continue to advance without additional compute by analyzing the development and role of algorithms used in state-of-the-art LLMs. Motivated by regulatory efforts that have largely focused on restricting access to high-performance hardware, we ask: Can LLMs progress in a compute-constrained environment, and how do algorithmic innovations perform under such conditions? To address these questions, we introduce a novel classification framework that distinguishes between compute-dependent innovations -- which yield disproportionate benefits at high compute levels (e.g., the Transformer architecture and mixture-of-experts models) and compute-independent innovations, which improve efficiency across all compute scales (e.g., rotary positional encoding, FlashAttention, or layer normalization). We quantify these contributions using a metric called compute-equivalent gain (CEG), which estimates the additional compute that would be required to achieve similar improvements without these algorithmic advancements. To validate this framework, we conduct small-scale training experiments with a scaled-down GPT-2 model. Our results confirm that compute-independent advancements yield meaningful performance gains even in resource-constrained settings, with a CEG of up to $3.5\times$ over a baseline model. By contrast, compute-dependent advancements provided little benefit or even degraded performance at the small scale, reinforcing the importance of compute availability for certain algorithmic gains.</li>
</ul>

<h3>Title: SEVA: Leveraging Single-Step Ensemble of Vicinal Augmentations for Test-Time Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Zixuan Hu, Yichun Hu, Ling-Yu Duan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04087">https://arxiv.org/abs/2505.04087</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04087">https://arxiv.org/pdf/2505.04087</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04087]] SEVA: Leveraging Single-Step Ensemble of Vicinal Augmentations for Test-Time Adaptation(https://arxiv.org/abs/2505.04087)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Test-Time adaptation (TTA) aims to enhance model robustness against distribution shifts through rapid model adaptation during inference. While existing TTA methods often rely on entropy-based unsupervised training and achieve promising results, the common practice of a single round of entropy training is typically unable to adequately utilize reliable samples, hindering adaptation efficiency. In this paper, we discover augmentation strategies can effectively unleash the potential of reliable samples, but the rapidly growing computational cost impedes their real-time application. To address this limitation, we propose a novel TTA approach named Single-step Ensemble of Vicinal Augmentations (SEVA), which can take advantage of data augmentations without increasing the computational burden. Specifically, instead of explicitly utilizing the augmentation strategy to generate new data, SEVA develops a theoretical framework to explore the impacts of multiple augmentations on model adaptation and proposes to optimize an upper bound of the entropy loss to integrate the effects of multiple rounds of augmentation training into a single step. Furthermore, we discover and verify that using the upper bound as the loss is more conducive to the selection mechanism, as it can effectively filter out harmful samples that confuse the model. Combining these two key advantages, the proposed efficient loss and a complementary selection strategy can simultaneously boost the potential of reliable samples and meet the stringent time requirements of TTA. The comprehensive experiments on various network architectures across challenging testing scenarios demonstrate impressive performances and the broad adaptability of SEVA. The code will be publicly available.</li>
</ul>

<h3>Title: LLMs' Suitability for Network Security: A Case Study of STRIDE Threat Modeling</h3>
<ul>
<li><strong>Authors: </strong>AbdulAziz AbdulGhaffar, Ashraf Matrawy</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04101">https://arxiv.org/abs/2505.04101</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04101">https://arxiv.org/pdf/2505.04101</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04101]] LLMs' Suitability for Network Security: A Case Study of STRIDE Threat Modeling(https://arxiv.org/abs/2505.04101)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Artificial Intelligence (AI) is expected to be an integral part of next-generation AI-native 6G networks. With the prevalence of AI, researchers have identified numerous use cases of AI in network security. However, there are almost nonexistent studies that analyze the suitability of Large Language Models (LLMs) in network security. To fill this gap, we examine the suitability of LLMs in network security, particularly with the case study of STRIDE threat modeling. We utilize four prompting techniques with five LLMs to perform STRIDE classification of 5G threats. From our evaluation results, we point out key findings and detailed insights along with the explanation of the possible underlying factors influencing the behavior of LLMs in the modeling of certain threats. The numerical results and the insights support the necessity for adjusting and fine-tuning LLMs for network security use cases.</li>
</ul>

<h3>Title: MAISY: Motion-Aware Image SYnthesis for MedicalImage Motion Correction</h3>
<ul>
<li><strong>Authors: </strong>Andrew Zhang, Hao Wang, Shuchang Ye, Michael Fulham, Jinman Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04105">https://arxiv.org/abs/2505.04105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04105">https://arxiv.org/pdf/2505.04105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04105]] MAISY: Motion-Aware Image SYnthesis for MedicalImage Motion Correction(https://arxiv.org/abs/2505.04105)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Patient motion during medical image acquisition causes blurring, ghosting, and distorts organs, which makes image interpretation this http URL state-of-the-art algorithms using Generative Adversarial Network (GAN)-based methods with their ability to learn the mappings between corrupted images and their ground truth via Structural Similarity Index Measure (SSIM) loss effectively generate motion-free images. However, we identified the following limitations: (i) they mainly focus on global structural characteristics and therefore overlook localized features that often carry critical pathological information, and (ii) the SSIM loss function struggles to handle images with varying pixel intensities, luminance factors, and variance. In this study, we propose Motion-Aware Image SYnthesis (MAISY) which initially characterize motion and then uses it for correction by: (a) leveraging the foundation model Segment Anything Model (SAM), to dynamically learn spatial patterns along anatomical boundaries where motion artifacts are most pronounced and, (b) introducing the Variance-Selective SSIM (VS-SSIM) loss which adaptively emphasizes spatial regions with high pixel variance to preserve essential anatomical details during artifact correction. Experiments on chest and head CT datasets demonstrate that our model outperformed the state-of-the-art counterparts, with Peak Signal-to-Noise Ratio (PSNR) increasing by 40%, SSIM by 10%, and Dice by 16%.</li>
</ul>

<h3>Title: One2Any: One-Reference 6D Pose Estimation for Any Object</h3>
<ul>
<li><strong>Authors: </strong>Mengya Liu, Siyuan Li, Ajad Chhatkuli, Prune Truong, Luc Van Gool, Federico Tombari</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04109">https://arxiv.org/abs/2505.04109</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04109">https://arxiv.org/pdf/2505.04109</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04109]] One2Any: One-Reference 6D Pose Estimation for Any Object(https://arxiv.org/abs/2505.04109)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>6D object pose estimation remains challenging for many applications due to dependencies on complete 3D models, multi-view images, or training limited to specific object categories. These requirements make generalization to novel objects difficult for which neither 3D models nor multi-view images may be available. To address this, we propose a novel method One2Any that estimates the relative 6-degrees of freedom (DOF) object pose using only a single reference-single query RGB-D image, without prior knowledge of its 3D model, multi-view data, or category constraints. We treat object pose estimation as an encoding-decoding process, first, we obtain a comprehensive Reference Object Pose Embedding (ROPE) that encodes an object shape, orientation, and texture from a single reference view. Using this embedding, a U-Net-based pose decoding module produces Reference Object Coordinate (ROC) for new views, enabling fast and accurate pose estimation. This simple encoding-decoding framework allows our model to be trained on any pair-wise pose data, enabling large-scale training and demonstrating great scalability. Experiments on multiple benchmark datasets demonstrate that our model generalizes well to novel objects, achieving state-of-the-art accuracy and robustness even rivaling methods that require multi-view or CAD inputs, at a fraction of compute.</li>
</ul>

<h3>Title: Alpha Excel Benchmark</h3>
<ul>
<li><strong>Authors: </strong>David Noever, Forrest McKee</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04110">https://arxiv.org/abs/2505.04110</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04110">https://arxiv.org/pdf/2505.04110</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04110]] Alpha Excel Benchmark(https://arxiv.org/abs/2505.04110)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This study presents a novel benchmark for evaluating Large Language Models (LLMs) using challenges derived from the Financial Modeling World Cup (FMWC) Excel competitions. We introduce a methodology for converting 113 existing FMWC challenges into programmatically evaluable JSON formats and use this dataset to compare the performance of several leading LLMs. Our findings demonstrate significant variations in performance across different challenge categories, with models showing specific strengths in pattern recognition tasks but struggling with complex numerical reasoning. The benchmark provides a standardized framework for assessing LLM capabilities in realistic business-oriented tasks rather than abstract academic problems. This research contributes to the growing field of AI benchmarking by establishing proficiency among the 1.5 billion people who daily use Microsoft Excel as a meaningful evaluation metric that bridges the gap between academic AI benchmarks and practical business applications.</li>
</ul>

<h3>Title: GAPrompt: Geometry-Aware Point Cloud Prompt for 3D Vision Model</h3>
<ul>
<li><strong>Authors: </strong>Zixiang Ai, Zichen Liu, Yuanhang Lei, Zhenyu Cui, Xu Zou, Jiahuan Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04119">https://arxiv.org/abs/2505.04119</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04119">https://arxiv.org/pdf/2505.04119</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04119]] GAPrompt: Geometry-Aware Point Cloud Prompt for 3D Vision Model(https://arxiv.org/abs/2505.04119)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Pre-trained 3D vision models have gained significant attention for their promising performance on point cloud data. However, fully fine-tuning these models for downstream tasks is computationally expensive and storage-intensive. Existing parameter-efficient fine-tuning (PEFT) approaches, which focus primarily on input token prompting, struggle to achieve competitive performance due to their limited ability to capture the geometric information inherent in point clouds. To address this challenge, we propose a novel Geometry-Aware Point Cloud Prompt (GAPrompt) that leverages geometric cues to enhance the adaptability of 3D vision models. First, we introduce a Point Prompt that serves as an auxiliary input alongside the original point cloud, explicitly guiding the model to capture fine-grained geometric details. Additionally, we present a Point Shift Prompter designed to extract global shape information from the point cloud, enabling instance-specific geometric adjustments at the input level. Moreover, our proposed Prompt Propagation mechanism incorporates the shape information into the model's feature extraction process, further strengthening its ability to capture essential geometric characteristics. Extensive experiments demonstrate that GAPrompt significantly outperforms state-of-the-art PEFT methods and achieves competitive results compared to full fine-tuning on various benchmarks, while utilizing only 2.19% of trainable parameters. Our code is available at this https URL.</li>
</ul>

<h3>Title: Vision Graph Prompting via Semantic Low-Rank Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Zixiang Ai, Zichen Liu, Jiahuan Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04121">https://arxiv.org/abs/2505.04121</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04121">https://arxiv.org/pdf/2505.04121</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04121]] Vision Graph Prompting via Semantic Low-Rank Decomposition(https://arxiv.org/abs/2505.04121)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Vision GNN (ViG) demonstrates superior performance by representing images as graph structures, providing a more natural way to capture irregular semantic patterns beyond traditional grid or sequence-based representations. To efficiently adapt ViG to downstream tasks, parameter-efficient fine-tuning techniques like visual prompting become increasingly essential. However, existing prompting methods are primarily designed for Transformer-based models, neglecting the rich topological relationships among nodes and edges in graph-based representations, limiting their capacity to model complex semantics. In this paper, we propose Vision Graph Prompting (VGP), a novel framework tailored for vision graph structures. Our core insight reveals that semantically connected components in the graph exhibit low-rank properties. Building on this observation, we introduce a semantic low-rank prompting method that decomposes low-rank semantic features and integrates them with prompts on vision graph topologies, capturing both global structural patterns and fine-grained semantic dependencies. Extensive experiments demonstrate our method significantly improves ViG's transfer performance on diverse downstream tasks, achieving results comparable to full fine-tuning while maintaining parameter efficiency. Our code is available at this https URL.</li>
</ul>

<h3>Title: A Framework to Prevent Biometric Data Leakage in the Immersive Technologies Domain</h3>
<ul>
<li><strong>Authors: </strong>Keshav Sood, Iynkaran Natgunanathan, Uthayasanker Thayasivam, Vithurabiman Senthuran, Xiaoning Zhang, Shui Yu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04123">https://arxiv.org/abs/2505.04123</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04123">https://arxiv.org/pdf/2505.04123</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04123]] A Framework to Prevent Biometric Data Leakage in the Immersive Technologies Domain(https://arxiv.org/abs/2505.04123)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, robust, biometric</a></li>
<li><strong>Abstract: </strong>Doubtlessly, the immersive technologies have potential to ease people's life and uplift economy, however the obvious data privacy risks cannot be ignored. For example, a participant wears a 3D headset device which detects participant's head motion to track the pose of participant's head to match the orientation of camera with participant's eyes positions in the real-world. In a preliminary study, researchers have proved that the voice command features on such headsets could lead to major privacy leakages. By analyzing the facial dynamics captured with the motion sensors, the headsets suffer security vulnerabilities revealing a user's sensitive speech without user's consent. The psychography data (such as voice command features, facial dynamics, etc.) is sensitive data and it should not be leaked out of the device without users consent else it is a privacy breach. To the best of our literature review, the work done in this particular research problem is very limited. Motivated from this, we develop a simple technical framework to mitigate sensitive data (or biometric data) privacy leaks in immersive technology domain. The performance evaluation is conducted in a robust way using six data sets, to show that the proposed solution is effective and feasible to prevent this issue.</li>
</ul>

<h3>Title: Enhancing Granular Sentiment Classification with Chain-of-Thought Prompting in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Vihaan Miriyala, Smrithi Bukkapatnam, Lavanya Prahallad</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04135">https://arxiv.org/abs/2505.04135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04135">https://arxiv.org/pdf/2505.04135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04135]] Enhancing Granular Sentiment Classification with Chain-of-Thought Prompting in Large Language Models(https://arxiv.org/abs/2505.04135)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We explore the use of Chain-of-Thought (CoT) prompting with large language models (LLMs) to improve the accuracy of granular sentiment categorization in app store reviews. Traditional numeric and polarity-based ratings often fail to capture the nuanced sentiment embedded in user feedback. We evaluated the effectiveness of CoT prompting versus simple prompting on 2000 Amazon app reviews by comparing each method's predictions to human judgements. CoT prompting improved classification accuracy from 84% to 93% highlighting the benefit of explicit reasoning in enhancing sentiment analysis performance.</li>
</ul>

<h3>Title: LHT: Statistically-Driven Oblique Decision Trees for Interpretable Classification</h3>
<ul>
<li><strong>Authors: </strong>Hongyi Li, Jun Xu, William Ward Armstrong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04139">https://arxiv.org/abs/2505.04139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04139">https://arxiv.org/pdf/2505.04139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04139]] LHT: Statistically-Driven Oblique Decision Trees for Interpretable Classification(https://arxiv.org/abs/2505.04139)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>We introduce the Learning Hyperplane Tree (LHT), a novel oblique decision tree model designed for expressive and interpretable classification. LHT fundamentally distinguishes itself through a non-iterative, statistically-driven approach to constructing splitting hyperplanes. Unlike methods that rely on iterative optimization or heuristics, LHT directly computes the hyperplane parameters, which are derived from feature weights based on the differences in feature expectations between classes within each node. This deterministic mechanism enables a direct and well-defined hyperplane construction process. Predictions leverage a unique piecewise linear membership function within leaf nodes, obtained via local least-squares fitting. We formally analyze the convergence of the LHT splitting process, ensuring that each split yields meaningful, non-empty partitions. Furthermore, we establish that the time complexity for building an LHT up to depth $d$ is $O(mnd)$, demonstrating the practical feasibility of constructing trees with powerful oblique splits using this methodology. The explicit feature weighting at each split provides inherent interpretability. Experimental results on benchmark datasets demonstrate LHT's competitive accuracy, positioning it as a practical, theoretically grounded, and interpretable alternative in the landscape of tree-based models. The implementation of the proposed method is available at this https URL.</li>
</ul>

<h3>Title: Unmasking the Canvas: A Dynamic Benchmark for Image Generation Jailbreaking and LLM Content Safety</h3>
<ul>
<li><strong>Authors: </strong>Variath Madhupal Gautham Nair, Vishal Varma Dantuluri</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04146">https://arxiv.org/abs/2505.04146</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04146">https://arxiv.org/pdf/2505.04146</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04146]] Unmasking the Canvas: A Dynamic Benchmark for Image Generation Jailbreaking and LLM Content Safety(https://arxiv.org/abs/2505.04146)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Existing large language models (LLMs) are advancing rapidly and produce outstanding results in image generation tasks, yet their content safety checks remain vulnerable to prompt-based jailbreaks. Through preliminary testing on platforms such as ChatGPT, MetaAI, and Grok, we observed that even short, natural prompts could lead to the generation of compromising images ranging from realistic depictions of forged documents to manipulated images of public figures. We introduce Unmasking the Canvas (UTC Benchmark; UTCB), a dynamic and scalable benchmark dataset to evaluate LLM vulnerability in image generation. Our methodology combines structured prompt engineering, multilingual obfuscation (e.g., Zulu, Gaelic, Base64), and evaluation using Groq-hosted LLaMA-3. The pipeline supports both zero-shot and fallback prompting strategies, risk scoring, and automated tagging. All generations are stored with rich metadata and curated into Bronze (non-verified), Silver (LLM-aided verification), and Gold (manually verified) tiers. UTCB is designed to evolve over time with new data sources, prompt templates, and model behaviors. Warning: This paper includes visual examples of adversarial inputs designed to test model safety. All outputs have been redacted to ensure responsible disclosure.</li>
</ul>

<h3>Title: Can Language Models Understand Social Behavior in Clinical Conversations?</h3>
<ul>
<li><strong>Authors: </strong>Manas Satish Bedmutha, Feng Chen, Andrea Hartzler, Trevor Cohen, Nadir Weibel</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04152">https://arxiv.org/abs/2505.04152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04152">https://arxiv.org/pdf/2505.04152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04152]] Can Language Models Understand Social Behavior in Clinical Conversations?(https://arxiv.org/abs/2505.04152)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Effective communication between providers and their patients influences health and care outcomes. The effectiveness of such conversations has been linked not only to the exchange of clinical information, but also to a range of interpersonal behaviors; commonly referred to as social signals, which are often conveyed through non-verbal cues and shape the quality of the patient-provider relationship. Recent advances in large language models (LLMs) have demonstrated an increasing ability to infer emotional and social behaviors even when analyzing only textual information. As automation increases also in clinical settings, such as for transcription of patient-provider conversations, there is growing potential for LLMs to automatically analyze and extract social behaviors from these interactions. To explore the foundational capabilities of LLMs in tracking social signals in clinical dialogue, we designed task-specific prompts and evaluated model performance across multiple architectures and prompting styles using a highly imbalanced, annotated dataset spanning 20 distinct social signals such as provider dominance, patient warmth, etc. We present the first system capable of tracking all these 20 coded signals, and uncover patterns in LLM behavior. Further analysis of model configurations and clinical context provides insights for enhancing LLM performance on social signal processing tasks in healthcare settings.</li>
</ul>

<h3>Title: FilterTS: Comprehensive Frequency Filtering for Multivariate Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Yulong Wang, Yushuo Liu, Xiaoyi Duan, Kai Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04158">https://arxiv.org/abs/2505.04158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04158">https://arxiv.org/pdf/2505.04158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04158]] FilterTS: Comprehensive Frequency Filtering for Multivariate Time Series Forecasting(https://arxiv.org/abs/2505.04158)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Multivariate time series forecasting is crucial across various industries, where accurate extraction of complex periodic and trend components can significantly enhance prediction performance. However, existing models often struggle to capture these intricate patterns. To address these challenges, we propose FilterTS, a novel forecasting model that utilizes specialized filtering techniques based on the frequency domain. FilterTS introduces a Dynamic Cross-Variable Filtering Module, a key innovation that dynamically leverages other variables as filters to extract and reinforce shared variable frequency components across variables in multivariate time series. Additionally, a Static Global Filtering Module captures stable frequency components, identified throughout the entire training set. Moreover, the model is built in the frequency domain, converting time-domain convolutions into frequency-domain multiplicative operations to enhance computational efficiency. Extensive experimental results on eight real-world datasets have demonstrated that FilterTS significantly outperforms existing methods in terms of prediction accuracy and computational efficiency.</li>
</ul>

<h3>Title: Optimization of Infectious Disease Intervention Measures Based on Reinforcement Learning - Empirical analysis based on UK COVID-19 epidemic data</h3>
<ul>
<li><strong>Authors: </strong>Baida Zhang, Yakai Chen, Huichun Li, Zhenghu Zu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY, cs.MA, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04161">https://arxiv.org/abs/2505.04161</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04161">https://arxiv.org/pdf/2505.04161</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04161]] Optimization of Infectious Disease Intervention Measures Based on Reinforcement Learning - Empirical analysis based on UK COVID-19 epidemic data(https://arxiv.org/abs/2505.04161)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust</a></li>
<li><strong>Abstract: </strong>Globally, the outbreaks of infectious diseases have exerted an extremely profound and severe influence on health security and the economy. During the critical phases of epidemics, devising effective intervention measures poses a significant challenge to both the academic and practical arenas. There is numerous research based on reinforcement learning to optimize intervention measures of infectious diseases. Nevertheless, most of these efforts have been confined within the differential equation based on infectious disease models. Although a limited number of studies have incorporated reinforcement learning methodologies into individual-based infectious disease models, the models employed therein have entailed simplifications and limitations, rendering it incapable of modeling the complexity and dynamics inherent in infectious disease transmission. We establish a decision-making framework based on an individual agent-based transmission model, utilizing reinforcement learning to continuously explore and develop a strategy function. The framework's validity is verified through both experimental and theoretical approaches. Covasim, a detailed and widely used agent-based disease transmission model, was modified to support reinforcement learning research. We conduct an exhaustive exploration of the application efficacy of multiple algorithms across diverse action spaces. Furthermore, we conduct an innovative preliminary theoretical analysis concerning the issue of "time coverage". The results of the experiment robustly validate the effectiveness and feasibility of the methodological framework of this study. The coping strategies gleaned therefrom prove highly efficacious in suppressing the expansion of the epidemic scale and safeguarding the stability of the economic system, thereby providing crucial reference perspectives for the formulation of global public health security strategies.</li>
</ul>

<h3>Title: DiffPattern-Flex: Efficient Layout Pattern Generation via Discrete Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Zixiao Wang, Wenqian Zhao, Yunheng Shen, Yang Bai, Guojin Chen, Farzan Farnia, Bei Yu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04173">https://arxiv.org/abs/2505.04173</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04173">https://arxiv.org/pdf/2505.04173</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04173]] DiffPattern-Flex: Efficient Layout Pattern Generation via Discrete Diffusion(https://arxiv.org/abs/2505.04173)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in layout pattern generation have been dominated by deep generative models. However, relying solely on neural networks for legality guarantees raises concerns in many practical applications. In this paper, we present \tool{DiffPattern}-Flex, a novel approach designed to generate reliable layout patterns efficiently. \tool{DiffPattern}-Flex incorporates a new method for generating diverse topologies using a discrete diffusion model while maintaining a lossless and compute-efficient layout representation. To ensure legal pattern generation, we employ {an} optimization-based, white-box pattern assessment process based on specific design rules. Furthermore, fast sampling and efficient legalization technologies are employed to accelerate the generation process. Experimental results across various benchmarks demonstrate that \tool{DiffPattern}-Flex significantly outperforms existing methods and excels at producing reliable layout patterns.</li>
</ul>

<h3>Title: On-Device LLM for Context-Aware Wi-Fi Roaming</h3>
<ul>
<li><strong>Authors: </strong>Ju-Hyung Lee, Yanqing Lu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04174">https://arxiv.org/abs/2505.04174</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04174">https://arxiv.org/pdf/2505.04174</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04174]] On-Device LLM for Context-Aware Wi-Fi Roaming(https://arxiv.org/abs/2505.04174)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Wireless roaming is a critical yet challenging task for maintaining seamless connectivity in dynamic mobile environments. Conventional threshold-based or heuristic schemes often fail, leading to either sticky or excessive handovers. We introduce the first cross-layer use of an on-device large language model (LLM): high-level reasoning in the application layer that issues real-time actions executed in the PHY/MAC stack. The LLM addresses two tasks: (i) context-aware AP selection, where structured prompts fuse environmental cues (e.g., location, time) to choose the best BSSID; and (ii) dynamic threshold adjustment, where the model adaptively decides when to roam. To satisfy the tight latency and resource budgets of edge hardware, we apply a suite of optimizations-chain-of-thought prompting, parameter-efficient fine-tuning, and quantization. Experiments on indoor and outdoor datasets show that our approach surpasses legacy heuristics and DRL baselines, achieving a strong balance between roaming stability and signal quality. These findings underscore the promise of application-layer LLM reasoning for lower-layer wireless control in future edge systems.</li>
</ul>

<h3>Title: DOTA: Deformable Optimized Transformer Architecture for End-to-End Text Recognition with Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Naphat Nithisopa, Teerapong Panboonyuen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04175">https://arxiv.org/abs/2505.04175</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04175">https://arxiv.org/pdf/2505.04175</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04175]] DOTA: Deformable Optimized Transformer Architecture for End-to-End Text Recognition with Retrieval-Augmented Generation(https://arxiv.org/abs/2505.04175)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Text recognition in natural images remains a challenging yet essential task, with broad applications spanning computer vision and natural language processing. This paper introduces a novel end-to-end framework that combines ResNet and Vision Transformer backbones with advanced methodologies, including Deformable Convolutions, Retrieval-Augmented Generation, and Conditional Random Fields (CRF). These innovations collectively enhance feature representation and improve Optical Character Recognition (OCR) performance. Specifically, the framework substitutes standard convolution layers in the third and fourth blocks with Deformable Convolutions, leverages adaptive dropout for regularization, and incorporates CRF for more refined sequence modeling. Extensive experiments conducted on six benchmark datasets IC13, IC15, SVT, IIIT5K, SVTP, and CUTE80 validate the proposed method's efficacy, achieving notable accuracies: 97.32% on IC13, 58.26% on IC15, 88.10% on SVT, 74.13% on IIIT5K, 82.17% on SVTP, and 66.67% on CUTE80, resulting in an average accuracy of 77.77%. These results establish a new state-of-the-art for text recognition, demonstrating the robustness of the approach across diverse and challenging datasets.</li>
</ul>

<h3>Title: Privacy Challenges In Image Processing Applications</h3>
<ul>
<li><strong>Authors: </strong>Maneesha, Bharat Gupta, Rishabh Sethi, Charvi Adita Das</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04181">https://arxiv.org/abs/2505.04181</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04181">https://arxiv.org/pdf/2505.04181</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04181]] Privacy Challenges In Image Processing Applications(https://arxiv.org/abs/2505.04181)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, protect, federate</a></li>
<li><strong>Abstract: </strong>As image processing systems proliferate, privacy concerns intensify given the sensitive personal information contained in images. This paper examines privacy challenges in image processing and surveys emerging privacy-preserving techniques including differential privacy, secure multiparty computation, homomorphic encryption, and anonymization. Key applications with heightened privacy risks include healthcare, where medical images contain patient health data, and surveillance systems that can enable unwarranted tracking. Differential privacy offers rigorous privacy guarantees by injecting controlled noise, while MPC facilitates collaborative analytics without exposing raw data inputs. Homomorphic encryption enables computations on encrypted data and anonymization directly removes identifying elements. However, balancing privacy protections and utility remains an open challenge. Promising future directions identified include quantum-resilient cryptography, federated learning, dedicated hardware, and conceptual innovations like privacy by design. Ultimately, a holistic effort combining technological innovations, ethical considerations, and policy frameworks is necessary to uphold the fundamental right to privacy as image processing capabilities continue advancing rapidly.</li>
</ul>

<h3>Title: S3D: Sketch-Driven 3D Model Generation</h3>
<ul>
<li><strong>Authors: </strong>Hail Song, Wonsik Shin, Naeun Lee, Soomin Chung, Nojun Kwak, Woontack Woo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04185">https://arxiv.org/abs/2505.04185</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04185">https://arxiv.org/pdf/2505.04185</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04185]] S3D: Sketch-Driven 3D Model Generation(https://arxiv.org/abs/2505.04185)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Generating high-quality 3D models from 2D sketches is a challenging task due to the inherent ambiguity and sparsity of sketch data. In this paper, we present S3D, a novel framework that converts simple hand-drawn sketches into detailed 3D models. Our method utilizes a U-Net-based encoder-decoder architecture to convert sketches into face segmentation masks, which are then used to generate a 3D representation that can be rendered from novel views. To ensure robust consistency between the sketch domain and the 3D output, we introduce a novel style-alignment loss that aligns the U-Net bottleneck features with the initial encoder outputs of the 3D generation module, significantly enhancing reconstruction fidelity. To further enhance the network's robustness, we apply augmentation techniques to the sketch dataset. This streamlined framework demonstrates the effectiveness of S3D in generating high-quality 3D models from sketch inputs. The source code for this project is publicly available at this https URL.</li>
</ul>

<h3>Title: Trajectory Entropy Reinforcement Learning for Predictable and Robust Control</h3>
<ul>
<li><strong>Authors: </strong>Bang You, Chenxu Wang, Huaping Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04193">https://arxiv.org/abs/2505.04193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04193">https://arxiv.org/pdf/2505.04193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04193]] Trajectory Entropy Reinforcement Learning for Predictable and Robust Control(https://arxiv.org/abs/2505.04193)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Simplicity is a critical inductive bias for designing data-driven controllers, especially when robustness is important. Despite the impressive results of deep reinforcement learning in complex control tasks, it is prone to capturing intricate and spurious correlations between observations and actions, leading to failure under slight perturbations to the environment. To tackle this problem, in this work we introduce a novel inductive bias towards simple policies in reinforcement learning. The simplicity inductive bias is introduced by minimizing the entropy of entire action trajectories, corresponding to the number of bits required to describe information in action trajectories after the agent observes state trajectories. Our reinforcement learning agent, Trajectory Entropy Reinforcement Learning, is optimized to minimize the trajectory entropy while maximizing rewards. We show that the trajectory entropy can be effectively estimated by learning a variational parameterized action prediction model, and use the prediction model to construct an information-regularized reward function. Furthermore, we construct a practical algorithm that enables the joint optimization of models, including the policy and the prediction model. Experimental evaluations on several high-dimensional locomotion tasks show that our learned policies produce more cyclical and consistent action trajectories, and achieve superior performance, and robustness to noise and dynamic changes than the state-of-the-art.</li>
</ul>

<h3>Title: AutoPatch: Multi-Agent Framework for Patching Real-World CVE Vulnerabilities</h3>
<ul>
<li><strong>Authors: </strong>Minjae Seo, Wonwoo Choi, Myoungsung You, Seungwon Shin</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04195">https://arxiv.org/abs/2505.04195</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04195">https://arxiv.org/pdf/2505.04195</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04195]] AutoPatch: Multi-Agent Framework for Patching Real-World CVE Vulnerabilities(https://arxiv.org/abs/2505.04195)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have emerged as promising tools in software development, enabling automated code generation and analysis. However, their knowledge is limited to a fixed cutoff date, making them prone to generating code vulnerable to newly disclosed CVEs. Frequent fine-tuning with new CVE sets is costly, and existing LLM-based approaches focus on oversimplified CWE examples and require providing explicit bug locations to LLMs, limiting their ability to patch complex real-world vulnerabilities. To address these limitations, we propose AutoPatch, a multi-agent framework designed to patch vulnerable LLM-generated code, particularly those introduced after the LLMs' knowledge cutoff. AutoPatch integrates Retrieval-Augmented Generation (RAG) with a structured database of recently disclosed vulnerabilities, comprising 525 code snippets derived from 75 high-severity CVEs across real-world systems such as the Linux kernel and Chrome. AutoPatch combines semantic and taint analysis to identify the most relevant CVE and leverages enhanced Chain-of-Thought (CoT) reasoning to construct enriched prompts for verification and patching. Our unified similarity model, which selects the most relevant vulnerabilities, achieves 90.4 percent accuracy in CVE matching. AutoPatch attains 89.5 percent F1-score for vulnerability verification and 95.0 percent accuracy in patching, while being over 50x more cost-efficient than traditional fine-tuning approaches.</li>
</ul>

<h3>Title: A Large Language Model for Feasible and Diverse Population Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Sung Yoo Lim, Hyunsoo Yun, Prateek Bansal, Dong-Kyu Kim, Eui-Jin Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04196">https://arxiv.org/abs/2505.04196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04196">https://arxiv.org/pdf/2505.04196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04196]] A Large Language Model for Feasible and Diverse Population Synthesis(https://arxiv.org/abs/2505.04196)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Generating a synthetic population that is both feasible and diverse is crucial for ensuring the validity of downstream activity schedule simulation in activity-based models (ABMs). While deep generative models (DGMs), such as variational autoencoders and generative adversarial networks, have been applied to this task, they often struggle to balance the inclusion of rare but plausible combinations (i.e., sampling zeros) with the exclusion of implausible ones (i.e., structural zeros). To improve feasibility while maintaining diversity, we propose a fine-tuning method for large language models (LLMs) that explicitly controls the autoregressive generation process through topological orderings derived from a Bayesian Network (BN). Experimental results show that our hybrid LLM-BN approach outperforms both traditional DGMs and proprietary LLMs (e.g., ChatGPT-4o) with few-shot learning. Specifically, our approach achieves approximately 95% feasibility, significantly higher than the ~80% observed in DGMs, while maintaining comparable diversity, making it well-suited for practical applications. Importantly, the method is based on a lightweight open-source LLM, enabling fine-tuning and inference on standard personal computing environments. This makes the approach cost-effective and scalable for large-scale applications, such as synthesizing populations in megacities, without relying on expensive infrastructure. By initiating the ABM pipeline with high-quality synthetic populations, our method improves overall simulation reliability and reduces downstream error propagation. The source code for these methods is available for research and practical application.</li>
</ul>

<h3>Title: Cyber Security Data Science: Machine Learning Methods and their Performance on Imbalanced Datasets</h3>
<ul>
<li><strong>Authors: </strong>Mateo Lopez-Ledezma, Gissel Velarde</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04204">https://arxiv.org/abs/2505.04204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04204">https://arxiv.org/pdf/2505.04204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04204]] Cyber Security Data Science: Machine Learning Methods and their Performance on Imbalanced Datasets(https://arxiv.org/abs/2505.04204)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Cybersecurity has become essential worldwide and at all levels, concerning individuals, institutions, and governments. A basic principle in cybersecurity is to be always alert. Therefore, automation is imperative in processes where the volume of daily operations is large. Several cybersecurity applications can be addressed as binary classification problems, including anomaly detection, fraud detection, intrusion detection, spam detection, or malware detection. We present three experiments. In the first experiment, we evaluate single classifiers including Random Forests, Light Gradient Boosting Machine, eXtreme Gradient Boosting, Logistic Regression, Decision Tree, and Gradient Boosting Decision Tree. In the second experiment, we test different sampling techniques including over-sampling, under-sampling, Synthetic Minority Over-sampling Technique, and Self-Paced Ensembling. In the last experiment, we evaluate Self-Paced Ensembling and its number of base classifiers. We found that imbalance learning techniques had positive and negative effects, as reported in related studies. Thus, these techniques should be applied with caution. Besides, we found different best performers for each dataset. Therefore, we recommend testing single classifiers and imbalance learning techniques for each new dataset and application involving imbalanced datasets as is the case in several cyber security applications.</li>
</ul>

<h3>Title: An Enhanced YOLOv8 Model for Real-Time and Accurate Pothole Detection and Measurement</h3>
<ul>
<li><strong>Authors: </strong>Mustafa Yurdakul, ≈ûakir Tasdemir</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04207">https://arxiv.org/abs/2505.04207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04207">https://arxiv.org/pdf/2505.04207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04207]] An Enhanced YOLOv8 Model for Real-Time and Accurate Pothole Detection and Measurement(https://arxiv.org/abs/2505.04207)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Potholes cause vehicle damage and traffic accidents, creating serious safety and economic problems. Therefore, early and accurate detection of potholes is crucial. Existing detection methods are usually only based on 2D RGB images and cannot accurately analyze the physical characteristics of potholes. In this paper, a publicly available dataset of RGB-D images (PothRGBD) is created and an improved YOLOv8-based model is proposed for both pothole detection and pothole physical features analysis. The Intel RealSense D415 depth camera was used to collect RGB and depth data from the road surfaces, resulting in a PothRGBD dataset of 1000 images. The data was labeled in YOLO format suitable for segmentation. A novel YOLO model is proposed based on the YOLOv8n-seg architecture, which is structurally improved with Dynamic Snake Convolution (DSConv), Simple Attention Module (SimAM) and Gaussian Error Linear Unit (GELU). The proposed model segmented potholes with irregular edge structure more accurately, and performed perimeter and depth measurements on depth maps with high accuracy. The standard YOLOv8n-seg model achieved 91.9% precision, 85.2% recall and 91.9% mAP@50. With the proposed model, the values increased to 93.7%, 90.4% and 93.8% respectively. Thus, an improvement of 1.96% in precision, 6.13% in recall and 2.07% in mAP was achieved. The proposed model performs pothole detection as well as perimeter and depth measurement with high accuracy and is suitable for real-time applications due to its low model complexity. In this way, a lightweight and effective model that can be used in deep learning-based intelligent transportation solutions has been acquired.</li>
</ul>

<h3>Title: CM1 - A Dataset for Evaluating Few-Shot Information Extraction with Large Vision Language Models</h3>
<ul>
<li><strong>Authors: </strong>Fabian Wolf, Oliver T√ºselmann, Arthur Matei, Lukas Hennies, Christoph Rass, Gernot A. Fink</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04214">https://arxiv.org/abs/2505.04214</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04214">https://arxiv.org/pdf/2505.04214</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04214]] CM1 - A Dataset for Evaluating Few-Shot Information Extraction with Large Vision Language Models(https://arxiv.org/abs/2505.04214)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>The automatic extraction of key-value information from handwritten documents is a key challenge in document analysis. A reliable extraction is a prerequisite for the mass digitization efforts of many archives. Large Vision Language Models (LVLM) are a promising technology to tackle this problem especially in scenarios where little annotated training data is available. In this work, we present a novel dataset specifically designed to evaluate the few-shot capabilities of LVLMs. The CM1 documents are a historic collection of forms with handwritten entries created in Europe to administer the Care and Maintenance program after World War Two. The dataset establishes three benchmarks on extracting name and birthdate information and, furthermore, considers different training set sizes. We provide baseline results for two different LVLMs and compare performances to an established full-page extraction model. While the traditional full-page model achieves highly competitive performances, our experiments show that when only a few training samples are available the considered LVLMs benefit from their size and heavy pretraining and outperform the classical approach.</li>
</ul>

<h3>Title: FRAIN to Train: A Fast-and-Reliable Solution for Decentralized Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Sanghyeon Park, Soo-Mook Moon</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04223">https://arxiv.org/abs/2505.04223</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04223">https://arxiv.org/pdf/2505.04223</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04223]] FRAIN to Train: A Fast-and-Reliable Solution for Decentralized Federated Learning(https://arxiv.org/abs/2505.04223)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, federate, transformer</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) enables collaborative model training across distributed clients while preserving data locality. Although FedAvg pioneered synchronous rounds for global model averaging, slower devices can delay collective progress. Asynchronous FL (e.g., FedAsync) addresses stragglers by continuously integrating client updates, yet naive implementations risk client drift due to non-IID data and stale contributions. Some Blockchain-based FL approaches (e.g., BRAIN) employ robust weighting or scoring of updates to resist malicious or misaligned proposals. However, performance drops can still persist under severe data heterogeneity or high staleness, and synchronization overhead has emerged as a new concern due to its aggregator-free architectures. We introduce Fast-and-Reliable AI Network, FRAIN, a new asynchronous FL method that mitigates these limitations by incorporating two key ideas. First, our FastSync strategy eliminates the need to replay past model versions, enabling newcomers and infrequent participants to efficiently approximate the global model. Second, we adopt spherical linear interpolation (SLERP) when merging parameters, preserving models' directions and alleviating destructive interference from divergent local training. Experiments with a CNN image-classification model and a Transformer-based language model demonstrate that FRAIN achieves more stable and robust convergence than FedAvg, FedAsync, and BRAIN, especially under harsh environments: non-IID data distributions, networks that experience delays and require frequent re-synchronization, and the presence of malicious nodes.</li>
</ul>

<h3>Title: Technology prediction of a 3D model using Neural Network</h3>
<ul>
<li><strong>Authors: </strong>Grzegorz Miebs, Rafa≈Ç A. Bachorz</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04241">https://arxiv.org/abs/2505.04241</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04241">https://arxiv.org/pdf/2505.04241</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04241]] Technology prediction of a 3D model using Neural Network(https://arxiv.org/abs/2505.04241)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Accurate estimation of production times is critical for effective manufacturing scheduling, yet traditional methods relying on expert analysis or historical data often fall short in dynamic or customized production environments. This paper introduces a data-driven approach that predicts manufacturing steps and their durations directly from a product's 3D model. By rendering the model into multiple 2D images and leveraging a neural network inspired by the Generative Query Network, the method learns to map geometric features into time estimates for predefined production steps enabling scalable, adaptive, and precise process planning across varied product types.</li>
</ul>

<h3>Title: On the Vulnerability of Underwater Magnetic Induction Communication</h3>
<ul>
<li><strong>Authors: </strong>Muhammad Muzzammil, Waqas Aman, Irfan Ullah, Shang Zhigang, Saif Al-Kuwari, Zhou Tian, Marwa Qaraqe</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04249">https://arxiv.org/abs/2505.04249</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04249">https://arxiv.org/pdf/2505.04249</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04249]] On the Vulnerability of Underwater Magnetic Induction Communication(https://arxiv.org/abs/2505.04249)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>Typical magnetic induction (MI) communication is commonly considered a secure underwater wireless communication (UWC) technology due to its non-audible and non-visible nature compared to acoustic and optical UWC technologies. However, vulnerabilities in communication systems inevitably exist and may lead to different types of attacks. In this paper, we investigate the eavesdropping attack in underwater MI communication to quantitatively measure the system's vulnerability under this attack. We consider different potential eavesdropping configuration setups based on the positions and orientations of the eavesdropper node to investigate how they impact the received voltage and secrecy at the legitimate receiver node. To this end, we develop finite-element-method-based simulation models for each configuration in an underwater environment and evaluate the received voltage and the secrecy capacity against different system parameters such as magnetic flux, magnetic flux density, distance, and orientation sensitivity. Furthermore, we construct an experimental setup within a laboratory environment to replicate the simulation experiments. Both simulation and lab experimental confirm the susceptibility of underwater MI communication to eavesdropping attacks. However, this vulnerability is highly dependent on the position and orientation of the coil between the eavesdropper and the legitimate transmitter. On the positive side, we also observe a unique behavior in the received coil reception that might be used to detect malicious node activities in the vicinity, which might lead to a potential security mechanism against eavesdropping attacks.</li>
</ul>

<h3>Title: LLM-Independent Adaptive RAG: Let the Question Speak for Itself</h3>
<ul>
<li><strong>Authors: </strong>Maria Marina, Nikolay Ivanov, Sergey Pletenev, Mikhail Salnikov, Daria Galimzianova, Nikita Krayko, Vasily Konovalov, Alexander Panchenko, Viktor Moskvoretskii</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04253">https://arxiv.org/abs/2505.04253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04253">https://arxiv.org/pdf/2505.04253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04253]] LLM-Independent Adaptive RAG: Let the Question Speak for Itself(https://arxiv.org/abs/2505.04253)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models~(LLMs) are prone to hallucinations, and Retrieval-Augmented Generation (RAG) helps mitigate this, but at a high computational cost while risking misinformation. Adaptive retrieval aims to retrieve only when necessary, but existing approaches rely on LLM-based uncertainty estimation, which remain inefficient and impractical. In this study, we introduce lightweight LLM-independent adaptive retrieval methods based on external information. We investigated 27 features, organized into 7 groups, and their hybrid combinations. We evaluated these methods on 6 QA datasets, assessing the QA performance and efficiency. The results show that our approach matches the performance of complex LLM-based methods while achieving significant efficiency gains, demonstrating the potential of external information for adaptive retrieval.</li>
</ul>

<h3>Title: Bridging Geometry-Coherent Text-to-3D Generation with Multi-View Diffusion Priors and Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Feng Yang, Wenliang Qian, Wangmeng Zuo, Hui Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04262">https://arxiv.org/abs/2505.04262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04262">https://arxiv.org/pdf/2505.04262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04262]] Bridging Geometry-Coherent Text-to-3D Generation with Multi-View Diffusion Priors and Gaussian Splatting(https://arxiv.org/abs/2505.04262)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Score Distillation Sampling (SDS) leverages pretrained 2D diffusion models to advance text-to-3D generation but neglects multi-view correlations, being prone to geometric inconsistencies and multi-face artifacts in the generated 3D content. In this work, we propose Coupled Score Distillation (CSD), a framework that couples multi-view joint distribution priors to ensure geometrically consistent 3D generation while enabling the stable and direct optimization of 3D Gaussian Splatting. Specifically, by reformulating the optimization as a multi-view joint optimization problem, we derive an effective optimization rule that effectively couples multi-view priors to guide optimization across different viewpoints while preserving the diversity of generated 3D assets. Additionally, we propose a framework that directly optimizes 3D Gaussian Splatting (3D-GS) with random initialization to generate geometrically consistent 3D content. We further employ a deformable tetrahedral grid, initialized from 3D-GS and refined through CSD, to produce high-quality, refined meshes. Quantitative and qualitative experimental results demonstrate the efficiency and competitive quality of our approach.</li>
</ul>

<h3>Title: Physics-Informed DeepONets for drift-diffusion on metric graphs: simulation and parameter identification</h3>
<ul>
<li><strong>Authors: </strong>Jan Blechschmidt, Tom-Christian Riemer, Max Winkler, Martin Stoll, Jan-F. Pietschmann</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04263">https://arxiv.org/abs/2505.04263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04263">https://arxiv.org/pdf/2505.04263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04263]] Physics-Informed DeepONets for drift-diffusion on metric graphs: simulation and parameter identification(https://arxiv.org/abs/2505.04263)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We develop a novel physics informed deep learning approach for solving nonlinear drift-diffusion equations on metric graphs. These models represent an important model class with a large number of applications in areas ranging from transport in biological cells to the motion of human crowds. While traditional numerical schemes require a large amount of tailoring, especially in the case of model design or parameter identification problems, physics informed deep operator networks (DeepONet) have emerged as a versatile tool for the solution of partial differential equations with the particular advantage that they easily incorporate parameter identification questions. We here present an approach where we first learn three DeepONet models for representative inflow, inner and outflow edges, resp., and then subsequently couple these models for the solution of the drift-diffusion metric graph problem by relying on an edge-based domain decomposition approach. We illustrate that our framework is applicable for the accurate evaluation of graph-coupled physics models and is well suited for solving optimization or inverse problems on these coupled networks.</li>
</ul>

<h3>Title: Weaponizing Language Models for Cybersecurity Offensive Operations: Automating Vulnerability Assessment Report Validation; A Review Paper</h3>
<ul>
<li><strong>Authors: </strong>Abdulrahman S Almuhaidib, Azlan Mohd Zain, Zalmiyah Zakaria, Izyan Izzati Kamsani, Abdulaziz S Almuhaidib</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04265">https://arxiv.org/abs/2505.04265</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04265">https://arxiv.org/pdf/2505.04265</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04265]] Weaponizing Language Models for Cybersecurity Offensive Operations: Automating Vulnerability Assessment Report Validation; A Review Paper(https://arxiv.org/abs/2505.04265)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>This, with the ever-increasing sophistication of cyberwar, calls for novel solutions. In this regard, Large Language Models (LLMs) have emerged as a highly promising tool for defensive and offensive cybersecurity-related strategies. While existing literature has focused much on the defensive use of LLMs, when it comes to their offensive utilization, very little has been reported-namely, concerning Vulnerability Assessment (VA) report validation. Consequentially, this paper tries to fill that gap by investigating the capabilities of LLMs in automating and improving the validation process of the report of the VA. From the critical review of the related literature, this paper hereby proposes a new approach to using the LLMs in the automation of the analysis and within the validation process of the report of the VA that could potentially reduce the number of false positives and generally enhance efficiency. These results are promising for LLM automatization for improving validation on reports coming from VA in order to improve accuracy while reducing human effort and security postures. The contribution of this paper provides further evidence about the offensive and defensive LLM capabilities and therefor helps in devising more appropriate cybersecurity strategies and tools accordingly.</li>
</ul>

<h3>Title: HDiffTG: A Lightweight Hybrid Diffusion-Transformer-GCN Architecture for 3D Human Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Yajie Fu, Chaorui Huang, Junwei Li, Hui Kong, Yibin Tian, Huakang Li, Zhiyuan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04276">https://arxiv.org/abs/2505.04276</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04276">https://arxiv.org/pdf/2505.04276</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04276]] HDiffTG: A Lightweight Hybrid Diffusion-Transformer-GCN Architecture for 3D Human Pose Estimation(https://arxiv.org/abs/2505.04276)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>We propose HDiffTG, a novel 3D Human Pose Estimation (3DHPE) method that integrates Transformer, Graph Convolutional Network (GCN), and diffusion model into a unified framework. HDiffTG leverages the strengths of these techniques to significantly improve pose estimation accuracy and robustness while maintaining a lightweight design. The Transformer captures global spatiotemporal dependencies, the GCN models local skeletal structures, and the diffusion model provides step-by-step optimization for fine-tuning, achieving a complementary balance between global and local features. This integration enhances the model's ability to handle pose estimation under occlusions and in complex scenarios. Furthermore, we introduce lightweight optimizations to the integrated model and refine the objective function design to reduce computational overhead without compromising performance. Evaluation results on the Human3.6M and MPI-INF-3DHP datasets demonstrate that HDiffTG achieves state-of-the-art (SOTA) performance on the MPI-INF-3DHP dataset while excelling in both accuracy and computational efficiency. Additionally, the model exhibits exceptional robustness in noisy and occluded environments. Source codes and models are available at this https URL</li>
</ul>

<h3>Title: Non-stationary Diffusion For Probabilistic Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Weiwei Ye, Zhuopeng Xu, Ning Gui</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04278">https://arxiv.org/abs/2505.04278</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04278">https://arxiv.org/pdf/2505.04278</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04278]] Non-stationary Diffusion For Probabilistic Time Series Forecasting(https://arxiv.org/abs/2505.04278)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Due to the dynamics of underlying physics and external influences, the uncertainty of time series often varies over time. However, existing Denoising Diffusion Probabilistic Models (DDPMs) often fail to capture this non-stationary nature, constrained by their constant variance assumption from the additive noise model (ANM). In this paper, we innovatively utilize the Location-Scale Noise Model (LSNM) to relax the fixed uncertainty assumption of ANM. A diffusion-based probabilistic forecasting framework, termed Non-stationary Diffusion (NsDiff), is designed based on LSNM that is capable of modeling the changing pattern of uncertainty. Specifically, NsDiff combines a denoising diffusion-based conditional generative model with a pre-trained conditional mean and variance estimator, enabling adaptive endpoint distribution modeling. Furthermore, we propose an uncertainty-aware noise schedule, which dynamically adjusts the noise levels to accurately reflect the data uncertainty at each step and integrates the time-varying variances into the diffusion process. Extensive experiments conducted on nine real-world and synthetic datasets demonstrate the superior performance of NsDiff compared to existing approaches. Code is available at this https URL.</li>
</ul>

<h3>Title: TS-Diff: Two-Stage Diffusion Model for Low-Light RAW Image Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Yi Li, Zhiyuan Zhang, Jiangnan Xia, Jianghan Cheng, Qilong Wu, Junwei Li, Yibin Tian, Hui Kong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04281">https://arxiv.org/abs/2505.04281</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04281">https://arxiv.org/pdf/2505.04281</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04281]] TS-Diff: Two-Stage Diffusion Model for Low-Light RAW Image Enhancement(https://arxiv.org/abs/2505.04281)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>This paper presents a novel Two-Stage Diffusion Model (TS-Diff) for enhancing extremely low-light RAW images. In the pre-training stage, TS-Diff synthesizes noisy images by constructing multiple virtual cameras based on a noise space. Camera Feature Integration (CFI) modules are then designed to enable the model to learn generalizable features across diverse virtual cameras. During the aligning stage, CFIs are averaged to create a target-specific CFI$^T$, which is fine-tuned using a small amount of real RAW data to adapt to the noise characteristics of specific cameras. A structural reparameterization technique further simplifies CFI$^T$ for efficient deployment. To address color shifts during the diffusion process, a color corrector is introduced to ensure color consistency by dynamically adjusting global color distributions. Additionally, a novel dataset, QID, is constructed, featuring quantifiable illumination levels and a wide dynamic range, providing a comprehensive benchmark for training and evaluation under extreme low-light conditions. Experimental results demonstrate that TS-Diff achieves state-of-the-art performance on multiple datasets, including QID, SID, and ELD, excelling in denoising, generalization, and color consistency across various cameras and illumination levels. These findings highlight the robustness and versatility of TS-Diff, making it a practical solution for low-light imaging applications. Source codes and models are available at this https URL</li>
</ul>

<h3>Title: GASCADE: Grouped Summarization of Adverse Drug Event for Enhanced Cancer Pharmacovigilance</h3>
<ul>
<li><strong>Authors: </strong>Sofia Jamil, Aryan Dabad, Bollampalli Areen Reddy, Sriparna Saha, Rajiv Misra, Adil A. Shakur</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04284">https://arxiv.org/abs/2505.04284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04284">https://arxiv.org/pdf/2505.04284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04284]] GASCADE: Grouped Summarization of Adverse Drug Event for Enhanced Cancer Pharmacovigilance(https://arxiv.org/abs/2505.04284)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>In the realm of cancer treatment, summarizing adverse drug events (ADEs) reported by patients using prescribed drugs is crucial for enhancing pharmacovigilance practices and improving drug-related decision-making. While the volume and complexity of pharmacovigilance data have increased, existing research in this field has predominantly focused on general diseases rather than specifically addressing cancer. This work introduces the task of grouped summarization of adverse drug events reported by multiple patients using the same drug for cancer treatment. To address the challenge of limited resources in cancer pharmacovigilance, we present the MultiLabeled Cancer Adverse Drug Reaction and Summarization (MCADRS) dataset. This dataset includes pharmacovigilance posts detailing patient concerns regarding drug efficacy and adverse effects, along with extracted labels for drug names, adverse drug events, severity, and adversity of reactions, as well as summaries of ADEs for each drug. Additionally, we propose the Grouping and Abstractive Summarization of Cancer Adverse Drug events (GASCADE) framework, a novel pipeline that combines the information extraction capabilities of Large Language Models (LLMs) with the summarization power of the encoder-decoder T5 model. Our work is the first to apply alignment techniques, including advanced algorithms like Direct Preference Optimization, to encoder-decoder models using synthetic datasets for summarization tasks. Through extensive experiments, we demonstrate the superior performance of GASCADE across various metrics, validated through both automated assessments and human evaluations. This multitasking approach enhances drug-related decision-making and fosters a deeper understanding of patient concerns, paving the way for advancements in personalized and responsive cancer care. The code and dataset used in this work are publicly available.</li>
</ul>

<h3>Title: MoDE: Mixture of Diffusion Experts for Any Occluded Face Recognition</h3>
<ul>
<li><strong>Authors: </strong>Qiannan Fan, Zhuoyang Li, Jitong Li, Chenyang Cao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04306">https://arxiv.org/abs/2505.04306</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04306">https://arxiv.org/pdf/2505.04306</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04306]] MoDE: Mixture of Diffusion Experts for Any Occluded Face Recognition(https://arxiv.org/abs/2505.04306)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>With the continuous impact of epidemics, people have become accustomed to wearing masks. However, most current occluded face recognition (OFR) algorithms lack prior knowledge of occlusions, resulting in poor performance when dealing with occluded faces of varying types and severity in reality. Recognizing occluded faces is still a significant challenge, which greatly affects the convenience of people's daily lives. In this paper, we propose an identity-gated mixture of diffusion experts (MoDE) for OFR. Each diffusion-based generative expert estimates one possible complete image for occluded faces. Considering the random sampling process of the diffusion model, which introduces inevitable differences and variations between the inpainted faces and the real ones. To ensemble effective information from multi-reconstructed faces, we introduce an identity-gating network to evaluate the contribution of each reconstructed face to the identity and adaptively integrate the predictions in the decision space. Moreover, our MoDE is a plug-and-play module for most existing face recognition models. Extensive experiments on three public face datasets and two datasets in the wild validate our advanced performance for various occlusions in comparison with the competing methods.</li>
</ul>

<h3>Title: Guardians of the Web: The Evolution and Future of Website Information Security</h3>
<ul>
<li><strong>Authors: </strong>Md Saiful Islam, Li Xiangdong</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04308">https://arxiv.org/abs/2505.04308</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04308">https://arxiv.org/pdf/2505.04308</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04308]] Guardians of the Web: The Evolution and Future of Website Information Security(https://arxiv.org/abs/2505.04308)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect</a></li>
<li><strong>Abstract: </strong>Website information security has become a critical concern in the digital age. This article explores the evolution of website information security, examining its historical development, current practices, and future directions. The early beginnings from the 1960s to the 1980s laid the groundwork for modern cybersecurity, with the development of ARPANET, TCP/IP, public-key cryptography, and the first antivirus programs. The 1990s marked a transformative era, driven by the commercialization of the Internet and the emergence of web-based services. As the Internet grew, so did the range and sophistication of cyber threats, leading to advancements in security technologies such as the Secure Sockets Layer (SSL) protocol, password protection, and firewalls. Current practices in website information security involve a multi-layered approach, including encryption, secure coding practices, regular security audits, and user education. The future of website information security is expected to be shaped by emerging technologies such as artificial intelligence, blockchain, and quantum computing, as well as the increasing importance of international cooperation and standardization efforts. As cyber threats continue to evolve, ongoing research and innovation in website information security will be essential to protect sensitive information and maintain trust in the digital world.</li>
</ul>

<h3>Title: Detecting Concept Drift in Neural Networks Using Chi-squared Goodness of Fit Testing</h3>
<ul>
<li><strong>Authors: </strong>Jacob Glenn Ayers, Buvaneswari A. Ramanan, Manzoor A. Khan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04318">https://arxiv.org/abs/2505.04318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04318">https://arxiv.org/pdf/2505.04318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04318]] Detecting Concept Drift in Neural Networks Using Chi-squared Goodness of Fit Testing(https://arxiv.org/abs/2505.04318)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>As the adoption of deep learning models has grown beyond human capacity for verification, meta-algorithms are needed to ensure reliable model inference. Concept drift detection is a field dedicated to identifying statistical shifts that is underutilized in monitoring neural networks that may encounter inference data with distributional characteristics diverging from their training data. Given the wide variety of model architectures, applications, and datasets, it is important that concept drift detection algorithms are adaptable to different inference scenarios. In this paper, we introduce an application of the $\chi^2$ Goodness of Fit Hypothesis Test as a drift detection meta-algorithm applied to a multilayer perceptron, a convolutional neural network, and a transformer trained for machine vision as they are exposed to simulated drift during inference. To that end, we demonstrate how unexpected drops in accuracy due to concept drift can be detected without directly examining the inference outputs. Our approach enhances safety by ensuring models are continually evaluated for reliability across varying conditions.</li>
</ul>

<h3>Title: Multi-turn Consistent Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Zijun Zhou, Yingying Deng, Xiangyu He, Weiming Dong, Fan Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04320">https://arxiv.org/abs/2505.04320</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04320">https://arxiv.org/pdf/2505.04320</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04320]] Multi-turn Consistent Image Editing(https://arxiv.org/abs/2505.04320)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Many real-world applications, such as interactive photo retouching, artistic content creation, and product design, require flexible and iterative image editing. However, existing image editing methods primarily focus on achieving the desired modifications in a single step, which often struggles with ambiguous user intent, complex transformations, or the need for progressive refinements. As a result, these methods frequently produce inconsistent outcomes or fail to meet user expectations. To address these challenges, we propose a multi-turn image editing framework that enables users to iteratively refine their edits, progressively achieving more satisfactory results. Our approach leverages flow matching for accurate image inversion and a dual-objective Linear Quadratic Regulators (LQR) for stable sampling, effectively mitigating error accumulation. Additionally, by analyzing the layer-wise roles of transformers, we introduce a adaptive attention highlighting method that enhances editability while preserving multi-turn coherence. Extensive experiments demonstrate that our framework significantly improves edit success rates and visual fidelity compared to existing methods.</li>
</ul>

<h3>Title: Applied Post Quantum Cryptography: A Practical Approach for Generating Certificates in Industrial Environments</h3>
<ul>
<li><strong>Authors: </strong>Nino Ricchizzi, Christian Schwinne, Jan Pelzl</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04333">https://arxiv.org/abs/2505.04333</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04333">https://arxiv.org/pdf/2505.04333</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04333]] Applied Post Quantum Cryptography: A Practical Approach for Generating Certificates in Industrial Environments(https://arxiv.org/abs/2505.04333)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>The transition to post-quantum cryptography (PQC) presents significant challenges for certificate-based identity management in industrial environments, where secure onboarding of devices relies on long-lived and interoperable credentials. This work analyzes the integration of PQC into X.509 certificate structures and compares existing tool support for classical, hybrid, composite, and chameleon certificates. A gap is identified in available open-source solutions, particularly for the generation and validation of hybrid and composite certificates via command-line interfaces. To address this, a proof-of-concept implementation based on the Bouncy Castle library is developed. The tool supports the creation of classical, hybrid (Catalyst), composite, and partially chameleon certificates using PQC algorithms such as ML-DSA and SLH-DSA. It demonstrates compatibility with standard X.509 workflows and aims to support headless operation and constrained platforms typical of industrial systems. The implementation is modular, publicly available, and intended to facilitate further research and testing of PQC migration strategies in practice. A comparison with OpenSSL-based solutions highlights current limitations in standardization, toolchain support, and algorithm coverage.</li>
</ul>

<h3>Title: Hyperbolic Fuzzy $C$-Means with Adaptive Weight-based Filtering for Clustering in Non-Euclidean Spaces</h3>
<ul>
<li><strong>Authors: </strong>Swagato Das, Arghya Pratihar, Swagatam Das</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04335">https://arxiv.org/abs/2505.04335</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04335">https://arxiv.org/pdf/2505.04335</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04335]] Hyperbolic Fuzzy $C$-Means with Adaptive Weight-based Filtering for Clustering in Non-Euclidean Spaces(https://arxiv.org/abs/2505.04335)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Clustering algorithms play a pivotal role in unsupervised learning by identifying and grouping similar objects based on shared characteristics. While traditional clustering techniques, such as hard and fuzzy center-based clustering, have been widely used, they struggle with complex, high-dimensional, and non-Euclidean datasets. In particular, the Fuzzy $C$-Means (FCM) algorithm, despite its efficiency and popularity, exhibits notable limitations in non-Euclidean spaces. Euclidean spaces assume linear separability and uniform distance scaling, limiting their effectiveness in capturing complex, hierarchical, or non-Euclidean structures in fuzzy clustering. To overcome these challenges, we introduce Filtration-based Hyperbolic Fuzzy $C$-Means (HypeFCM), a novel clustering algorithm tailored for better representation of data relationships in non-Euclidean spaces. HypeFCM integrates the principles of fuzzy clustering with hyperbolic geometry and employs a weight-based filtering mechanism to improve performance. The algorithm initializes weights using a Dirichlet distribution and iteratively refines cluster centroids and membership assignments based on a hyperbolic metric in the Poincar√© Disc model. Extensive experimental evaluations demonstrate that HypeFCM significantly outperforms conventional fuzzy clustering methods in non-Euclidean settings, underscoring its robustness and effectiveness.</li>
</ul>

<h3>Title: Riemannian Denoising Diffusion Probabilistic Models</h3>
<ul>
<li><strong>Authors: </strong>Zichen Liu, Wei Zhang, Christof Sch√ºtte, Tiejun Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04338">https://arxiv.org/abs/2505.04338</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04338">https://arxiv.org/pdf/2505.04338</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04338]] Riemannian Denoising Diffusion Probabilistic Models(https://arxiv.org/abs/2505.04338)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We propose Riemannian Denoising Diffusion Probabilistic Models (RDDPMs) for learning distributions on submanifolds of Euclidean space that are level sets of functions, including most of the manifolds relevant to applications. Existing methods for generative modeling on manifolds rely on substantial geometric information such as geodesic curves or eigenfunctions of the Laplace-Beltrami operator and, as a result, they are limited to manifolds where such information is available. In contrast, our method, built on a projection scheme, can be applied to more general manifolds, as it only requires being able to evaluate the value and the first order derivatives of the function that defines the submanifold. We provide a theoretical analysis of our method in the continuous-time limit, which elucidates the connection between our RDDPMs and score-based generative models on manifolds. The capability of our method is demonstrated on datasets from previous studies and on new datasets sampled from two high-dimensional manifolds, i.e. $\mathrm{SO}(10)$ and the configuration space of molecular system alanine dipeptide with fixed dihedral angle.</li>
</ul>

<h3>Title: Adaptive and Robust DBSCAN with Multi-agent Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Hao Peng, Xiang Huang, Shuo Sun, Ruitong Zhang, Philip S. Yu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04339">https://arxiv.org/abs/2505.04339</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04339">https://arxiv.org/pdf/2505.04339</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04339]] Adaptive and Robust DBSCAN with Multi-agent Reinforcement Learning(https://arxiv.org/abs/2505.04339)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>DBSCAN, a well-known density-based clustering algorithm, has gained widespread popularity and usage due to its effectiveness in identifying clusters of arbitrary shapes and handling noisy data. However, it encounters challenges in producing satisfactory cluster results when confronted with datasets of varying density scales, a common scenario in real-world applications. In this paper, we propose a novel Adaptive and Robust DBSCAN with Multi-agent Reinforcement Learning cluster framework, namely AR-DBSCAN. First, we model the initial dataset as a two-level encoding tree and categorize the data vertices into distinct density partitions according to the information uncertainty determined in the encoding tree. Each partition is then assigned to an agent to find the best clustering parameters without manual assistance. The allocation is density-adaptive, enabling AR-DBSCAN to effectively handle diverse density distributions within the dataset by utilizing distinct agents for different partitions. Second, a multi-agent deep reinforcement learning guided automatic parameter searching process is designed. The process of adjusting the parameter search direction by perceiving the clustering environment is modeled as a Markov decision process. Using a weakly-supervised reward training policy network, each agent adaptively learns the optimal clustering parameters by interacting with the clusters. Third, a recursive search mechanism adaptable to the data's scale is presented, enabling efficient and controlled exploration of large parameter spaces. Extensive experiments are conducted on nine artificial datasets and a real-world dataset. The results of offline and online tasks show that AR-DBSCAN not only improves clustering accuracy by up to 144.1% and 175.3% in the NMI and ARI metrics, respectively, but also is capable of robustly finding dominant parameters.</li>
</ul>

<h3>Title: CountDiffusion: Text-to-Image Synthesis with Training-Free Counting-Guidance Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Yanyu Li, Pencheng Wan, Liang Han, Yaowei Wang, Liqiang Nie, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04347">https://arxiv.org/abs/2505.04347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04347">https://arxiv.org/pdf/2505.04347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04347]] CountDiffusion: Text-to-Image Synthesis with Training-Free Counting-Guidance Diffusion(https://arxiv.org/abs/2505.04347)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Stable Diffusion has advanced text-to-image synthesis, but training models to generate images with accurate object quantity is still difficult due to the high computational cost and the challenge of teaching models the abstract concept of quantity. In this paper, we propose CountDiffusion, a training-free framework aiming at generating images with correct object quantity from textual descriptions. CountDiffusion consists of two stages. In the first stage, an intermediate denoising result is generated by the diffusion model to predict the final synthesized image with one-step denoising, and a counting model is used to count the number of objects in this image. In the second stage, a correction module is used to correct the object quantity by changing the attention map of the object with universal guidance. The proposed CountDiffusion can be plugged into any diffusion-based text-to-image (T2I) generation models without further training. Experiment results demonstrate the superiority of our proposed CountDiffusion, which improves the accurate object quantity generation ability of T2I models by a large margin.</li>
</ul>

<h3>Title: Balancing Accuracy, Calibration, and Efficiency in Active Learning with Vision Transformers Under Label Noise</h3>
<ul>
<li><strong>Authors: </strong>Moseli Mots'oehli, Hope Mogale, Kyungim Baek</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04375">https://arxiv.org/abs/2505.04375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04375">https://arxiv.org/pdf/2505.04375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04375]] Balancing Accuracy, Calibration, and Efficiency in Active Learning with Vision Transformers Under Label Noise(https://arxiv.org/abs/2505.04375)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Fine-tuning pre-trained convolutional neural networks on ImageNet for downstream tasks is well-established. Still, the impact of model size on the performance of vision transformers in similar scenarios, particularly under label noise, remains largely unexplored. Given the utility and versatility of transformer architectures, this study investigates their practicality under low-budget constraints and noisy labels. We explore how classification accuracy and calibration are affected by symmetric label noise in active learning settings, evaluating four vision transformer configurations (Base and Large with 16x16 and 32x32 patch sizes) and three Swin Transformer configurations (Tiny, Small, and Base) on CIFAR10 and CIFAR100 datasets, under varying label noise rates. Our findings show that larger ViT models (ViTl32 in particular) consistently outperform their smaller counterparts in both accuracy and calibration, even under moderate to high label noise, while Swin Transformers exhibit weaker robustness across all noise levels. We find that smaller patch sizes do not always lead to better performance, as ViTl16 performs consistently worse than ViTl32 while incurring a higher computational cost. We also find that information-based Active Learning strategies only provide meaningful accuracy improvements at moderate label noise rates, but they result in poorer calibration compared to models trained on randomly acquired labels, especially at high label noise rates. We hope these insights provide actionable guidance for practitioners looking to deploy vision transformers in resource-constrained environments, where balancing model complexity, label noise, and compute efficiency is critical in model fine-tuning or distillation.</li>
</ul>

<h3>Title: Tetrahedron-Net for Medical Image Registration</h3>
<ul>
<li><strong>Authors: </strong>Jinhai Xiang, Shuai Guo, Qianru Han, Dantong Shi, Xinwei He, Xiang Bai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04380">https://arxiv.org/abs/2505.04380</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04380">https://arxiv.org/pdf/2505.04380</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04380]] Tetrahedron-Net for Medical Image Registration(https://arxiv.org/abs/2505.04380)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Medical image registration plays a vital role in medical image processing. Extracting expressive representations for medical images is crucial for improving the registration quality. One common practice for this end is constructing a convolutional backbone to enable interactions with skip connections among feature extraction layers. The de facto structure, U-Net-like networks, has attempted to design skip connections such as nested or full-scale ones to connect one single encoder and one single decoder to improve its representation capacity. Despite being effective, it still does not fully explore interactions with a single encoder and decoder architectures. In this paper, we embrace this observation and introduce a simple yet effective alternative strategy to enhance the representations for registrations by appending one additional decoder. The new decoder is designed to interact with both the original encoder and decoder. In this way, it not only reuses feature presentation from corresponding layers in the encoder but also interacts with the original decoder to corporately give more accurate registration results. The new architecture is concise yet generalized, with only one encoder and two decoders forming a ``Tetrahedron'' structure, thereby dubbed Tetrahedron-Net. Three instantiations of Tetrahedron-Net are further constructed regarding the different structures of the appended decoder. Our extensive experiments prove that superior performance can be obtained on several representative benchmarks of medical image registration. Finally, such a ``Tetrahedron'' design can also be easily integrated into popular U-Net-like architectures including VoxelMorph, ViT-V-Net, and TransMorph, leading to consistent performance gains.</li>
</ul>

<h3>Title: The Aloe Family Recipe for Open and Specialized Healthcare LLMs</h3>
<ul>
<li><strong>Authors: </strong>Dario Garcia-Gasulla, Jordi Bayarri-Planas, Ashwin Kumar Gururajan, Enrique Lopez-Cuena, Adrian Tormos, Daniel Hinjos, Pablo Bernabeu-Perez, Anna Arias-Duart, Pablo Agustin Martin-Torres, Marta Gonzalez-Mallo, Sergio Alvarez-Napagao, Eduard Ayguad√©-Parra, Ulises Cort√©s</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04388">https://arxiv.org/abs/2505.04388</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04388">https://arxiv.org/pdf/2505.04388</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04388]] The Aloe Family Recipe for Open and Specialized Healthcare LLMs(https://arxiv.org/abs/2505.04388)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack, large language model</a></li>
<li><strong>Abstract: </strong>Purpose: With advancements in Large Language Models (LLMs) for healthcare, the need arises for competitive open-source models to protect the public interest. This work contributes to the field of open medical LLMs by optimizing key stages of data preprocessing and training, while showing how to improve model safety (through DPO) and efficacy (through RAG). The evaluation methodology used, which includes four different types of tests, defines a new standard for the field. The resultant models, shown to be competitive with the best private alternatives, are released with a permisive license. Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5, Aloe Beta uses a custom dataset to enhance public data with synthetic Chain of Thought examples. The models undergo alignment with Direct Preference Optimization, emphasizing ethical and policy-aligned performance in the presence of jailbreaking attacks. Evaluation includes close-ended, open-ended, safety and human assessments, to maximize the reliability of results. Results: Recommendations are made across the entire pipeline, backed by the solid performance of the Aloe Family. These models deliver competitive performance across healthcare benchmarks and medical fields, and are often preferred by healthcare professionals. On bias and toxicity, the Aloe Beta models significantly improve safety, showing resilience to unseen jailbreaking attacks. For a responsible release, a detailed risk assessment specific to healthcare is attached to the Aloe Family models. Conclusion: The Aloe Beta models, and the recipe that leads to them, are a significant contribution to the open-source medical LLM field, offering top-of-the-line performance while maintaining high ethical requirements. This work sets a new standard for developing and reporting aligned LLMs in healthcare.</li>
</ul>

<h3>Title: Predicting Road Surface Anomalies by Visual Tracking of a Preceding Vehicle</h3>
<ul>
<li><strong>Authors: </strong>Petr Jahoda, Jan Cech</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04392">https://arxiv.org/abs/2505.04392</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04392">https://arxiv.org/pdf/2505.04392</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04392]] Predicting Road Surface Anomalies by Visual Tracking of a Preceding Vehicle(https://arxiv.org/abs/2505.04392)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>A novel approach to detect road surface anomalies by visual tracking of a preceding vehicle is proposed. The method is versatile, predicting any kind of road anomalies, such as potholes, bumps, debris, etc., unlike direct observation methods that rely on training visual detectors of those cases. The method operates in low visibility conditions or in dense traffic where the anomaly is occluded by a preceding vehicle. Anomalies are detected predictively, i.e., before a vehicle encounters them, which allows to pre-configure low-level vehicle systems (such as chassis) or to plan an avoidance maneuver in case of autonomous driving. A challenge is that the signal coming from camera-based tracking of a preceding vehicle may be weak and disturbed by camera ego motion due to vibrations affecting the ego vehicle. Therefore, we propose an efficient method to compensate camera pitch rotation by an iterative robust estimator. Our experiments on both controlled setup and normal traffic conditions show that road anomalies can be detected reliably at a distance even in challenging cases where the ego vehicle traverses imperfect road surfaces. The method is effective and performs in real time on standard consumer hardware.</li>
</ul>

<h3>Title: Large Means Left: Political Bias in Large Language Models Increases with Their Number of Parameters</h3>
<ul>
<li><strong>Authors: </strong>David Exler, Mark Schutera, Markus Reischl, Luca Rettenberger</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04393">https://arxiv.org/abs/2505.04393</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04393">https://arxiv.org/pdf/2505.04393</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04393]] Large Means Left: Political Bias in Large Language Models Increases with Their Number of Parameters(https://arxiv.org/abs/2505.04393)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the increasing prevalence of artificial intelligence, careful evaluation of inherent biases needs to be conducted to form the basis for alleviating the effects these predispositions can have on users. Large language models (LLMs) are predominantly used by many as a primary source of information for various topics. LLMs frequently make factual errors, fabricate data (hallucinations), or present biases, exposing users to misinformation and influencing opinions. Educating users on their risks is key to responsible use, as bias, unlike hallucinations, cannot be caught through data verification. We quantify the political bias of popular LLMs in the context of the recent vote of the German Bundestag using the score produced by the Wahl-O-Mat. This metric measures the alignment between an individual's political views and the positions of German political parties. We compare the models' alignment scores to identify factors influencing their political preferences. Doing so, we discover a bias toward left-leaning parties, most dominant in larger LLMs. Also, we find that the language we use to communicate with the models affects their political views. Additionally, we analyze the influence of a model's origin and release date and compare the results to the outcome of the recent vote of the Bundestag. Our results imply that LLMs are prone to exhibiting political bias. Large corporations with the necessary means to develop LLMs, thus, knowingly or unknowingly, have a responsibility to contain these biases, as they can influence each voter's decision-making process and inform public opinion in general and at scale.</li>
</ul>

<h3>Title: SwinLip: An Efficient Visual Speech Encoder for Lip Reading Using Swin Transformer</h3>
<ul>
<li><strong>Authors: </strong>Young-Hu Park, Rae-Hong Park, Hyung-Min Park</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04394">https://arxiv.org/abs/2505.04394</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04394">https://arxiv.org/pdf/2505.04394</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04394]] SwinLip: An Efficient Visual Speech Encoder for Lip Reading Using Swin Transformer(https://arxiv.org/abs/2505.04394)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>This paper presents an efficient visual speech encoder for lip reading. While most recent lip reading studies have been based on the ResNet architecture and have achieved significant success, they are not sufficiently suitable for efficiently capturing lip reading features due to high computational complexity in modeling spatio-temporal information. Additionally, using a complex visual model not only increases the complexity of lip reading models but also induces delays in the overall network for multi-modal studies (e.g., audio-visual speech recognition, speech enhancement, and speech separation). To overcome the limitations of Convolutional Neural Network (CNN)-based models, we apply the hierarchical structure and window self-attention of the Swin Transformer to lip reading. We configure a new lightweight scale of the Swin Transformer suitable for processing lip reading data and present the SwinLip visual speech encoder, which efficiently reduces computational load by integrating modified Convolution-augmented Transformer (Conformer) temporal embeddings with conventional spatial embeddings in the hierarchical structure. Through extensive experiments, we have validated that our SwinLip successfully improves the performance and inference speed of the lip reading network when applied to various backbones for word and sentence recognition, reducing computational load. In particular, our SwinLip demonstrated robust performance in both English LRW and Mandarin LRW-1000 datasets and achieved state-of-the-art performance on the Mandarin LRW-1000 dataset with less computation compared to the existing state-of-the-art model.</li>
</ul>

<h3>Title: Deep residual learning with product units</h3>
<ul>
<li><strong>Authors: </strong>Ziyuan Li, Uwe Jaekel, Babette Dellen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04397">https://arxiv.org/abs/2505.04397</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04397">https://arxiv.org/pdf/2505.04397</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04397]] Deep residual learning with product units(https://arxiv.org/abs/2505.04397)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We propose a deep product-unit residual neural network (PURe) that integrates product units into residual blocks to improve the expressiveness and parameter efficiency of deep convolutional networks. Unlike standard summation neurons, product units enable multiplicative feature interactions, potentially offering a more powerful representation of complex patterns. PURe replaces conventional convolutional layers with 2D product units in the second layer of each residual block, eliminating nonlinear activation functions to preserve structural information. We validate PURe on three benchmark datasets. On Galaxy10 DECaLS, PURe34 achieves the highest test accuracy of 84.89%, surpassing the much deeper ResNet152, while converging nearly five times faster and demonstrating strong robustness to Poisson noise. On ImageNet, PURe architectures outperform standard ResNet models at similar depths, with PURe34 achieving a top-1 accuracy of 80.27% and top-5 accuracy of 95.78%, surpassing deeper ResNet variants (ResNet50, ResNet101) while utilizing significantly fewer parameters and computational resources. On CIFAR-10, PURe consistently outperforms ResNet variants across varying depths, with PURe272 reaching 95.01% test accuracy, comparable to ResNet1001 but at less than half the model size. These results demonstrate that PURe achieves a favorable balance between accuracy, efficiency, and robustness. Compared to traditional residual networks, PURe not only achieves competitive classification performance with faster convergence and fewer parameters, but also demonstrates greater robustness to noise. Its effectiveness across diverse datasets highlights the potential of product-unit-based architectures for scalable and reliable deep learning in computer vision.</li>
</ul>

<h3>Title: YABLoCo: Yet Another Benchmark for Long Context Code Generation</h3>
<ul>
<li><strong>Authors: </strong>Aidar Valeev (1), Roman Garaev (1), Vadim Lomshakov (2), Irina Piontkovskaya (3), Vladimir Ivanov (1), Israel Adewuyi (1) ((1) Research Center of the Artificial Intelligence Institute, Innopolis University, Russia, (2) St. Petersburg Department of the Steklov Institute of Mathematics, Russia, (3) Huawei Noah's Ark Lab)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04406">https://arxiv.org/abs/2505.04406</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04406">https://arxiv.org/pdf/2505.04406</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04406]] YABLoCo: Yet Another Benchmark for Long Context Code Generation(https://arxiv.org/abs/2505.04406)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models demonstrate the ability to solve various programming tasks, including code generation. Typically, the performance of LLMs is measured on benchmarks with small or medium-sized context windows of thousands of lines of code. At the same time, in real-world software projects, repositories can span up to millions of LoC. This paper closes this gap by contributing to the long context code generation benchmark (YABLoCo). The benchmark featured a test set of 215 functions selected from four large repositories with thousands of functions. The dataset contained metadata of functions, contexts of the functions with different levels of dependencies, docstrings, functions bodies, and call graphs for each repository. This paper presents three key aspects of the contribution. First, the benchmark aims at function body generation in large repositories in C and C++, two languages not covered by previous benchmarks. Second, the benchmark contains large repositories from 200K to 2,000K LoC. Third, we contribute a scalable evaluation pipeline for efficient computing of the target metrics and a tool for visual analysis of generated code. Overall, these three aspects allow for evaluating code generation in large repositories in C and C++.</li>
</ul>

<h3>Title: MFSeg: Efficient Multi-frame 3D Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Chengjie Huang, Krzysztof Czarnecki</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04408">https://arxiv.org/abs/2505.04408</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04408">https://arxiv.org/pdf/2505.04408</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04408]] MFSeg: Efficient Multi-frame 3D Semantic Segmentation(https://arxiv.org/abs/2505.04408)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>We propose MFSeg, an efficient multi-frame 3D semantic segmentation framework. By aggregating point cloud sequences at the feature level and regularizing the feature extraction and aggregation process, MFSeg reduces computational overhead while maintaining high accuracy. Moreover, by employing a lightweight MLP-based point decoder, our method eliminates the need to upsample redundant points from past frames. Experiments on the nuScenes and Waymo datasets show that MFSeg outperforms existing methods, demonstrating its effectiveness and efficiency.</li>
</ul>

<h3>Title: DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception</h3>
<ul>
<li><strong>Authors: </strong>Junjie Wang, Bin Chen, Yulin Li, Bin Kang, Yichi Chen, Zhuotao Tian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04410">https://arxiv.org/abs/2505.04410</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04410">https://arxiv.org/pdf/2505.04410</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04410]] DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception(https://arxiv.org/abs/2505.04410)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Dense visual prediction tasks have been constrained by their reliance on predefined categories, limiting their applicability in real-world scenarios where visual concepts are unbounded. While Vision-Language Models (VLMs) like CLIP have shown promise in open-vocabulary tasks, their direct application to dense prediction often leads to suboptimal performance due to limitations in local feature representation. In this work, we present our observation that CLIP's image tokens struggle to effectively aggregate information from spatially or semantically related regions, resulting in features that lack local discriminability and spatial consistency. To address this issue, we propose DeCLIP, a novel framework that enhances CLIP by decoupling the self-attention module to obtain ``content'' and ``context'' features respectively. The ``content'' features are aligned with image crop representations to improve local discriminability, while ``context'' features learn to retain the spatial correlations under the guidance of vision foundation models, such as DINO. Extensive experiments demonstrate that DeCLIP significantly outperforms existing methods across multiple open-vocabulary dense prediction tasks, including object detection and semantic segmentation. Code is available at \textcolor{magenta}{this https URL}.</li>
</ul>

<h3>Title: OBLIVIATE: Robust and Practical Machine Unlearning for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyu Xu, Minxin Du, Qingqing Ye, Haibo Hu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04416">https://arxiv.org/abs/2505.04416</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04416">https://arxiv.org/pdf/2505.04416</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04416]] OBLIVIATE: Robust and Practical Machine Unlearning for Large Language Models(https://arxiv.org/abs/2505.04416)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, membership infer, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) trained over extensive corpora risk memorizing sensitive, copyrighted, or toxic content. To address this, we propose OBLIVIATE, a robust unlearning framework that removes targeted data while preserving model utility. The framework follows a structured process: extracting target tokens, building retain sets, and fine-tuning with a tailored loss function comprising three components -- masking, distillation, and world fact. Using low-rank adapters (LoRA), it ensures efficiency without compromising unlearning quality. We conduct experiments on multiple datasets, including the Harry Potter series, WMDP, and TOFU, using a comprehensive suite of metrics: forget quality (new document-level memorization score), model utility, and fluency. Results demonstrate its effectiveness in resisting membership inference attacks, minimizing the impact on retained data, and maintaining robustness across diverse scenarios.</li>
</ul>

<h3>Title: Localized Diffusion Models for High Dimensional Distributions Generation</h3>
<ul>
<li><strong>Authors: </strong>Georg A. Gottwald, Shuigen Liu, Youssef Marzouk, Sebastian Reich, Xin T. Tong</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04417">https://arxiv.org/abs/2505.04417</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04417">https://arxiv.org/pdf/2505.04417</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04417]] Localized Diffusion Models for High Dimensional Distributions Generation(https://arxiv.org/abs/2505.04417)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models are the state-of-the-art tools for various generative tasks. However, estimating high-dimensional score functions makes them potentially suffer from the curse of dimensionality (CoD). This underscores the importance of better understanding and exploiting low-dimensional structure in the target distribution. In this work, we consider locality structure, which describes sparse dependencies between model components. Under locality structure, the score function is effectively low-dimensional, so that it can be estimated by a localized neural network with significantly reduced sample complexity. This motivates the localized diffusion model, where a localized score matching loss is used to train the score function within a localized hypothesis space. We prove that such localization enables diffusion models to circumvent CoD, at the price of additional localization error. Under realistic sample size scaling, we show both theoretically and numerically that a moderate localization radius can balance the statistical and localization error, leading to a better overall performance. The localized structure also facilitates parallel training of diffusion models, making it potentially more efficient for large-scale applications.</li>
</ul>

<h3>Title: FedBWO: Enhancing Communication Efficiency in Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Vahideh Hayyolalam, √ñznur √ñzkasap</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04435">https://arxiv.org/abs/2505.04435</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04435">https://arxiv.org/pdf/2505.04435</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04435]] FedBWO: Enhancing Communication Efficiency in Federated Learning(https://arxiv.org/abs/2505.04435)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) is a distributed Machine Learning (ML) setup, where a shared model is collaboratively trained by various clients using their local datasets while keeping the data private. Considering resource-constrained devices, FL clients often suffer from restricted transmission capacity. Aiming to enhance the system performance, the communication between clients and server needs to be diminished. Current FL strategies transmit a tremendous amount of data (model weights) within the FL process, which needs a high communication bandwidth. Considering resource constraints, increasing the number of clients and, consequently, the amount of data (model weights) can lead to a bottleneck. In this paper, we introduce the Federated Black Widow Optimization (FedBWO) technique to decrease the amount of transmitted data by transmitting only a performance score rather than the local model weights from clients. FedBWO employs the BWO algorithm to improve local model updates. The conducted experiments prove that FedBWO remarkably improves the performance of the global model and the communication efficiency of the overall system. According to the experimental outcomes, FedBWO enhances the global model accuracy by an average of 21% over FedAvg, and 12% over FedGWO. Furthermore, FedBWO dramatically decreases the communication cost compared to other methods.</li>
</ul>

<h3>Title: Towards Initialization-Agnostic Clustering with Iterative Adaptive Resonance Theory</h3>
<ul>
<li><strong>Authors: </strong>Xiaozheng Qu, Zhaochuan Li, Zhuang Qi, Xiang Li, Haibei Huang, Lei Meng, Xiangxu Meng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04440">https://arxiv.org/abs/2505.04440</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04440">https://arxiv.org/pdf/2505.04440</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04440]] Towards Initialization-Agnostic Clustering with Iterative Adaptive Resonance Theory(https://arxiv.org/abs/2505.04440)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The clustering performance of Fuzzy Adaptive Resonance Theory (Fuzzy ART) is highly dependent on the preset vigilance parameter, where deviations in its value can lead to significant fluctuations in clustering results, severely limiting its practicality for non-expert users. Existing approaches generally enhance vigilance parameter robustness through adaptive mechanisms such as particle swarm optimization and fuzzy logic rules. However, they often introduce additional hyperparameters or complex frameworks that contradict the original simplicity of the algorithm. To address this, we propose Iterative Refinement Adaptive Resonance Theory (IR-ART), which integrates three key phases into a unified iterative framework: (1) Cluster Stability Detection: A dynamic stability detection module that identifies unstable clusters by analyzing the change of sample size (number of samples in the cluster) in iteration. (2) Unstable Cluster Deletion: An evolutionary pruning module that eliminates low-quality clusters. (3) Vigilance Region Expansion: A vigilance region expansion mechanism that adaptively adjusts similarity thresholds. Independent of the specific execution of clustering, these three phases sequentially focus on analyzing the implicit knowledge within the iterative process, adjusting weights and vigilance parameters, thereby laying a foundation for the next iteration. Experimental evaluation on 15 datasets demonstrates that IR-ART improves tolerance to suboptimal vigilance parameter values while preserving the parameter simplicity of Fuzzy ART. Case studies visually confirm the algorithm's self-optimization capability through iterative refinement, making it particularly suitable for non-expert users in resource-constrained scenarios.</li>
</ul>

<h3>Title: Towards Effectively Leveraging Execution Traces for Program Repair with Code LLMs</h3>
<ul>
<li><strong>Authors: </strong>Mirazul Haque, Petr Babkin, Farima Farmahinifarahani, Manuela Veloso</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04441">https://arxiv.org/abs/2505.04441</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04441">https://arxiv.org/pdf/2505.04441</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04441]] Towards Effectively Leveraging Execution Traces for Program Repair with Code LLMs(https://arxiv.org/abs/2505.04441)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) show promising performance on various programming tasks, including Automatic Program Repair (APR). However, most approaches to LLM-based APR are limited to the static analysis of the programs, while disregarding their runtime behavior. Inspired by knowledge-augmented NLP, in this work, we aim to remedy this potential blind spot by augmenting standard APR prompts with program execution traces. We evaluate our approach using the GPT family of models on three popular APR datasets. Our findings suggest that simply incorporating execution traces into the prompt provides a limited performance improvement over trace-free baselines, in only 2 out of 6 tested dataset / model configurations. We further find that the effectiveness of execution traces for APR diminishes as their complexity increases. We explore several strategies for leveraging traces in prompts and demonstrate that LLM-optimized prompts help outperform trace-free prompts more consistently. Additionally, we show trace-based prompting to be superior to finetuning a smaller LLM on a small-scale dataset; and conduct probing studies reinforcing the notion that execution traces can complement the reasoning abilities of the LLMs.</li>
</ul>

<h3>Title: Spectral and Temporal Denoising for Differentially Private Optimization</h3>
<ul>
<li><strong>Authors: </strong>Hyeju Shin, Kyudan Jung, Seongwon Yun, Juyoung Yun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IT, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04468">https://arxiv.org/abs/2505.04468</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04468">https://arxiv.org/pdf/2505.04468</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04468]] Spectral and Temporal Denoising for Differentially Private Optimization(https://arxiv.org/abs/2505.04468)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, transformer</a></li>
<li><strong>Abstract: </strong>This paper introduces the FFT-Enhanced Kalman Filter (FFTKF), a differentially private optimization method that addresses the challenge of preserving performance in DP-SGD, where added noise typically degrades model utility. FFTKF integrates frequency-domain noise shaping with Kalman filtering to enhance gradient quality while preserving $(\varepsilon, \delta)$-DP guarantees. It employs a high-frequency shaping mask in the Fourier domain to concentrate differential privacy noise in less informative spectral components, preserving low-frequency gradient signals. A scalar-gain Kalman filter with finite-difference Hessian approximation further refines the denoised gradients. With a per-iteration complexity of $\mathcal{O}(d \log d)$, FFTKF demonstrates improved test accuracy over DP-SGD and DiSK across MNIST, CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets using CNNs, Wide ResNets, and Vision Transformers. Theoretical analysis confirms that FFTKF maintains equivalent privacy guarantees while achieving a tighter privacy-utility trade-off through reduced noise and controlled bias.</li>
</ul>

<h3>Title: CAD-Llama: Leveraging Large Language Models for Computer-Aided Design Parametric 3D Model Generation</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Li, Weijian Ma, Xueyang Li, Yunzhong Lou, Guichun Zhou, Xiangdong Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04481">https://arxiv.org/abs/2505.04481</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04481">https://arxiv.org/pdf/2505.04481</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04481]] CAD-Llama: Leveraging Large Language Models for Computer-Aided Design Parametric 3D Model Generation(https://arxiv.org/abs/2505.04481)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Recently, Large Language Models (LLMs) have achieved significant success, prompting increased interest in expanding their generative capabilities beyond general text into domain-specific areas. This study investigates the generation of parametric sequences for computer-aided design (CAD) models using LLMs. This endeavor represents an initial step towards creating parametric 3D shapes with LLMs, as CAD model parameters directly correlate with shapes in three-dimensional space. Despite the formidable generative capacities of LLMs, this task remains challenging, as these models neither encounter parametric sequences during their pretraining phase nor possess direct awareness of 3D structures. To address this, we present CAD-Llama, a framework designed to enhance pretrained LLMs for generating parametric 3D CAD models. Specifically, we develop a hierarchical annotation pipeline and a code-like format to translate parametric 3D CAD command sequences into Structured Parametric CAD Code (SPCC), incorporating hierarchical semantic descriptions. Furthermore, we propose an adaptive pretraining approach utilizing SPCC, followed by an instruction tuning process aligned with CAD-specific guidelines. This methodology aims to equip LLMs with the spatial knowledge inherent in parametric sequences. Experimental results demonstrate that our framework significantly outperforms prior autoregressive methods and existing LLM baselines.</li>
</ul>

<h3>Title: Efficient Flow Matching using Latent Variables</h3>
<ul>
<li><strong>Authors: </strong>Anirban Samaddar, Yixuan Sun, Viktor Nilsson, Sandeep Madireddy</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04486">https://arxiv.org/abs/2505.04486</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04486">https://arxiv.org/pdf/2505.04486</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04486]] Efficient Flow Matching using Latent Variables(https://arxiv.org/abs/2505.04486)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Flow matching models have shown great potential in image generation tasks among probabilistic generative models. Building upon the ideas of continuous normalizing flows, flow matching models generalize the transport path of the diffusion models from a simple prior distribution to the data. Most flow matching models in the literature do not explicitly model the underlying structure/manifold in the target data when learning the flow from a simple source distribution like the standard Gaussian. This leads to inefficient learning, especially for many high-dimensional real-world datasets, which often reside in a low-dimensional manifold. Existing strategies of incorporating manifolds, including data with underlying multi-modal distribution, often require expensive training and hence frequently lead to suboptimal performance. To this end, we present \texttt{Latent-CFM}, which provides simplified training/inference strategies to incorporate multi-modal data structures using pretrained deep latent variable models. Through experiments on multi-modal synthetic data and widely used image benchmark datasets, we show that \texttt{Latent-CFM} exhibits improved generation quality with significantly less training ($\sim 50\%$ less in some cases) and computation than state-of-the-art flow matching models. Using a 2d Darcy flow dataset, we demonstrate that our approach generates more physically accurate samples than competitive approaches. In addition, through latent space analysis, we demonstrate that our approach can be used for conditional image generation conditioned on latent features.</li>
</ul>

<h3>Title: Defining and Quantifying Creative Behavior in Popular Image Generators</h3>
<ul>
<li><strong>Authors: </strong>Aditi Ramaswamy</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04497">https://arxiv.org/abs/2505.04497</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04497">https://arxiv.org/pdf/2505.04497</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04497]] Defining and Quantifying Creative Behavior in Popular Image Generators(https://arxiv.org/abs/2505.04497)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Creativity of generative AI models has been a subject of scientific debate in the last years, without a conclusive answer. In this paper, we study creativity from a practical perspective and introduce quantitative measures that help the user to choose a suitable AI model for a given task. We evaluated our measures on a number of popular image-to-image generation models, and the results of this suggest that our measures conform to human intuition.</li>
</ul>

<h3>Title: Leveraging Simultaneous Usage of Edge GPU Hardware Engines for Video Face Detection and Recognition</h3>
<ul>
<li><strong>Authors: </strong>Asma Baobaid, Mahmoud Meribout</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AR, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04502">https://arxiv.org/abs/2505.04502</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04502">https://arxiv.org/pdf/2505.04502</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04502]] Leveraging Simultaneous Usage of Edge GPU Hardware Engines for Video Face Detection and Recognition(https://arxiv.org/abs/2505.04502)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Video face detection and recognition in public places at the edge is required in several applications, such as security reinforcement and contactless access to authorized venues. This paper aims to maximize the simultaneous usage of hardware engines available in edge GPUs nowadays by leveraging the concurrency and pipelining of tasks required for face detection and recognition. This also includes the video decoding task, which is required in most face monitoring applications as the video streams are usually carried via Gbps Ethernet network. This constitutes an improvement over previous works where the tasks are usually allocated to a single engine due to the lack of a unified and automated framework that simultaneously explores all hardware engines. In addition, previously, the input faces were usually embedded in still images or within raw video streams that overlook the burst delay caused by the decoding stage. The results on real-life video streams suggest that simultaneously using all the hardware engines available in the recent NVIDIA edge Orin GPU, higher throughput, and a slight saving of power consumption of around 300 mW, accounting for around 5%, have been achieved while satisfying the real-time performance constraint. The performance gets even higher by considering several video streams simultaneously. Further performance improvement could have been obtained if the number of shuffle layers that were created by the tensor RT framework for the face recognition task was lower. Thus, the paper suggests some hardware improvements to the existing edge GPU processors to enhance their performance even higher.</li>
</ul>

<h3>Title: Detecting Spelling and Grammatical Anomalies in Russian Poetry Texts</h3>
<ul>
<li><strong>Authors: </strong>Ilya Koziev</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04507">https://arxiv.org/abs/2505.04507</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04507">https://arxiv.org/pdf/2505.04507</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04507]] Detecting Spelling and Grammatical Anomalies in Russian Poetry Texts(https://arxiv.org/abs/2505.04507)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The quality of natural language texts in fine-tuning datasets plays a critical role in the performance of generative models, particularly in computational creativity tasks such as poem or song lyric generation. Fluency defects in generated poems significantly reduce their value. However, training texts are often sourced from internet-based platforms without stringent quality control, posing a challenge for data engineers to manage defect levels effectively. To address this issue, we propose the use of automated linguistic anomaly detection to identify and filter out low-quality texts from training datasets for creative models. In this paper, we present a comprehensive comparison of unsupervised and supervised text anomaly detection approaches, utilizing both synthetic and human-labeled datasets. We also introduce the RUPOR dataset, a collection of Russian-language human-labeled poems designed for cross-sentence grammatical error detection, and provide the full evaluation code. Our work aims to empower the community with tools and insights to improve the quality of training datasets for generative models in creative domains.</li>
</ul>

<h3>Title: HunyuanCustom: A Multimodal-Driven Architecture for Customized Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Teng Hu, Zhentao Yu, Zhengguang Zhou, Sen Liang, Yuan Zhou, Qin Lin, Qinglin Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04512">https://arxiv.org/abs/2505.04512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04512">https://arxiv.org/pdf/2505.04512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04512]] HunyuanCustom: A Multimodal-Driven Architecture for Customized Video Generation(https://arxiv.org/abs/2505.04512)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Customized video generation aims to produce videos featuring specific subjects under flexible user-defined conditions, yet existing methods often struggle with identity consistency and limited input modalities. In this paper, we propose HunyuanCustom, a multi-modal customized video generation framework that emphasizes subject consistency while supporting image, audio, video, and text conditions. Built upon HunyuanVideo, our model first addresses the image-text conditioned generation task by introducing a text-image fusion module based on LLaVA for enhanced multi-modal understanding, along with an image ID enhancement module that leverages temporal concatenation to reinforce identity features across frames. To enable audio- and video-conditioned generation, we further propose modality-specific condition injection mechanisms: an AudioNet module that achieves hierarchical alignment via spatial cross-attention, and a video-driven injection module that integrates latent-compressed conditional video through a patchify-based feature-alignment network. Extensive experiments on single- and multi-subject scenarios demonstrate that HunyuanCustom significantly outperforms state-of-the-art open- and closed-source methods in terms of ID consistency, realism, and text-video alignment. Moreover, we validate its robustness across downstream tasks, including audio and video-driven customized video generation. Our results highlight the effectiveness of multi-modal conditioning and identity-preserving strategies in advancing controllable video generation. All the code and models are available at this https URL.</li>
</ul>

<h3>Title: Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs</h3>
<ul>
<li><strong>Authors: </strong>Yehui Tang, Yichun Yin, Yaoyuan Wang, Hang Zhou, Yu Pan, Wei Guo, Ziyang Zhang, Miao Rang, Fangcheng Liu, Naifu Zhang, Binghan Li, Yonghan Dong, Xiaojun Meng, Yasheng Wang, Dong Li, Yin Li, Dandan Tu, Can Chen, Youliang Yan, Fisher Yu, Ruiming Tang, Yunhe Wang, Botian Huang, Bo Wang, Boxiao Liu, Changzheng Zhang, Da Kuang, Fei Liu, Gang Huang, Jiansheng Wei, Jiarui Qin, Jie Ran, Jinpeng Li, Jun Zhao, Liang Dai, Lin Li, Liqun Deng, Peifeng Qin, Pengyuan Zeng, Qiang Gu, Shaohua Tang, Shengjun Cheng, Tao Gao, Tao Yu, Tianshu Li, Tianyu Bi, Wei He, Weikai Mao, Wenyong Huang, Wulong Liu, Xiabing Li, Xianzhi Yu, Xueyu Wu, Xu He, Yangkai Du, Yan Xu, Ye Tian, Yimeng Wu, Yongbing Huang, Yong Tian, Yong Zhu, Yue Li, Yufei Wang, Yuhang Gai, Yujun Li, Yu Luo, Yunsheng Ni, Yusen Sun, Zelin Chen, Zhe Liu, Zhicheng Liu, Zhipeng Tu, Zilin Ding, Zongyuan Zhan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04519">https://arxiv.org/abs/2505.04519</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04519">https://arxiv.org/pdf/2505.04519</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04519]] Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs(https://arxiv.org/abs/2505.04519)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Sparse large language models (LLMs) with Mixture of Experts (MoE) and close to a trillion parameters are dominating the realm of most capable language models. However, the massive model scale poses significant challenges for the underlying software and hardware systems. In this paper, we aim to uncover a recipe to harness such scale on Ascend NPUs. The key goals are better usage of the computing resources under the dynamic sparse model structures and materializing the expected performance gain on the actual hardware. To select model configurations suitable for Ascend NPUs without repeatedly running the expensive experiments, we leverage simulation to compare the trade-off of various model hyperparameters. This study led to Pangu Ultra MoE, a sparse LLM with 718 billion parameters, and we conducted experiments on the model to verify the simulation results. On the system side, we dig into Expert Parallelism to optimize the communication between NPU devices to reduce the synchronization overhead. We also optimize the memory efficiency within the devices to further reduce the parameter and activation management overhead. In the end, we achieve an MFU of 30.0% when training Pangu Ultra MoE, with performance comparable to that of DeepSeek R1, on 6K Ascend NPUs, and demonstrate that the Ascend system is capable of harnessing all the training stages of the state-of-the-art language models. Extensive experiments indicate that our recipe can lead to efficient training of large-scale sparse language models with MoE. We also study the behaviors of such models for future reference.</li>
</ul>

<h3>Title: Text2CT: Towards 3D CT Volume Generation from Free-text Descriptions Using Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Pengfei Guo, Can Zhao, Dong Yang, Yufan He, Vishwesh Nath, Ziyue Xu, Pedro R. A. S. Bassi, Zongwei Zhou, Benjamin D. Simon, Stephanie Anne Harmon, Baris Turkbey, Daguang Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04522">https://arxiv.org/abs/2505.04522</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04522">https://arxiv.org/pdf/2505.04522</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04522]] Text2CT: Towards 3D CT Volume Generation from Free-text Descriptions Using Diffusion Model(https://arxiv.org/abs/2505.04522)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Generating 3D CT volumes from descriptive free-text inputs presents a transformative opportunity in diagnostics and research. In this paper, we introduce Text2CT, a novel approach for synthesizing 3D CT volumes from textual descriptions using the diffusion model. Unlike previous methods that rely on fixed-format text input, Text2CT employs a novel prompt formulation that enables generation from diverse, free-text descriptions. The proposed framework encodes medical text into latent representations and decodes them into high-resolution 3D CT scans, effectively bridging the gap between semantic text inputs and detailed volumetric representations in a unified 3D framework. Our method demonstrates superior performance in preserving anatomical fidelity and capturing intricate structures as described in the input text. Extensive evaluations show that our approach achieves state-of-the-art results, offering promising potential applications in diagnostics, and data augmentation.</li>
</ul>

<h3>Title: DFVO: Learning Darkness-free Visible and Infrared Image Disentanglement and Fusion All at Once</h3>
<ul>
<li><strong>Authors: </strong>Qi Zhou, Yukai Shi, Xiaojun Yang, Xiaoyu Xian, Lunjia Liao, Ruimao Zhang, Liang Lin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04526">https://arxiv.org/abs/2505.04526</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04526">https://arxiv.org/pdf/2505.04526</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04526]] DFVO: Learning Darkness-free Visible and Infrared Image Disentanglement and Fusion All at Once(https://arxiv.org/abs/2505.04526)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Visible and infrared image fusion is one of the most crucial tasks in the field of image fusion, aiming to generate fused images with clear structural information and high-quality texture features for high-level vision tasks. However, when faced with severe illumination degradation in visible images, the fusion results of existing image fusion methods often exhibit blurry and dim visual effects, posing major challenges for autonomous driving. To this end, a Darkness-Free network is proposed to handle Visible and infrared image disentanglement and fusion all at Once (DFVO), which employs a cascaded multi-task approach to replace the traditional two-stage cascaded training (enhancement and fusion), addressing the issue of information entropy loss caused by hierarchical data transmission. Specifically, we construct a latent-common feature extractor (LCFE) to obtain latent features for the cascaded tasks strategy. Firstly, a details-extraction module (DEM) is devised to acquire high-frequency semantic information. Secondly, we design a hyper cross-attention module (HCAM) to extract low-frequency information and preserve texture features from source images. Finally, a relevant loss function is designed to guide the holistic network learning, thereby achieving better image fusion. Extensive experiments demonstrate that our proposed approach outperforms state-of-the-art alternatives in terms of qualitative and quantitative evaluations. Particularly, DFVO can generate clearer, more informative, and more evenly illuminated fusion results in the dark environments, achieving best performance on the LLVIP dataset with 63.258 dB PSNR and 0.724 CC, providing more effective information for high-level vision tasks. Our code is publicly accessible at this https URL.</li>
</ul>

<h3>Title: RAFT: Robust Augmentation of FeaTures for Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Edward Humes, Xiaomin Lin, Uttej Kallakuri, Tinoosh Mohsenin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04529">https://arxiv.org/abs/2505.04529</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04529">https://arxiv.org/pdf/2505.04529</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04529]] RAFT: Robust Augmentation of FeaTures for Image Segmentation(https://arxiv.org/abs/2505.04529)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Image segmentation is a powerful computer vision technique for scene understanding. However, real-world deployment is stymied by the need for high-quality, meticulously labeled datasets. Synthetic data provides high-quality labels while reducing the need for manual data collection and annotation. However, deep neural networks trained on synthetic data often face the Syn2Real problem, leading to poor performance in real-world deployments. To mitigate the aforementioned gap in image segmentation, we propose RAFT, a novel framework for adapting image segmentation models using minimal labeled real-world data through data and feature augmentations, as well as active learning. To validate RAFT, we perform experiments on the synthetic-to-real "SYNTHIA->Cityscapes" and "GTAV->Cityscapes" benchmarks. We managed to surpass the previous state of the art, HALO. SYNTHIA->Cityscapes experiences an improvement in mIoU* upon domain adaptation of 2.1%/79.9%, and GTAV->Cityscapes experiences a 0.4%/78.2% improvement in mIoU. Furthermore, we test our approach on the real-to-real benchmark of "Cityscapes->ACDC", and again surpass HALO, with a gain in mIoU upon adaptation of 1.3%/73.2%. Finally, we examine the effect of the allocated annotation budget and various components of RAFT upon the final transfer mIoU.</li>
</ul>

<h3>Title: Overcoming Data Scarcity in Generative Language Modelling for Low-Resource Languages: A Systematic Review</h3>
<ul>
<li><strong>Authors: </strong>Josh McGiff, Nikola S. Nikolov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04531">https://arxiv.org/abs/2505.04531</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04531">https://arxiv.org/pdf/2505.04531</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04531]] Overcoming Data Scarcity in Generative Language Modelling for Low-Resource Languages: A Systematic Review(https://arxiv.org/abs/2505.04531)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>Generative language modelling has surged in popularity with the emergence of services such as ChatGPT and Google Gemini. While these models have demonstrated transformative potential in productivity and communication, they overwhelmingly cater to high-resource languages like English. This has amplified concerns over linguistic inequality in natural language processing (NLP). This paper presents the first systematic review focused specifically on strategies to address data scarcity in generative language modelling for low-resource languages (LRL). Drawing from 54 studies, we identify, categorise and evaluate technical approaches, including monolingual data augmentation, back-translation, multilingual training, and prompt engineering, across generative tasks. We also analyse trends in architecture choices, language family representation, and evaluation methods. Our findings highlight a strong reliance on transformer-based models, a concentration on a small subset of LRLs, and a lack of consistent evaluation across studies. We conclude with recommendations for extending these methods to a wider range of LRLs and outline open challenges in building equitable generative language systems. Ultimately, this review aims to support researchers and developers in building inclusive AI tools for underrepresented languages, a necessary step toward empowering LRL speakers and the preservation of linguistic diversity in a world increasingly shaped by large-scale language technologies.</li>
</ul>

<h3>Title: Communication-Efficient Federated Fine-Tuning of Language Models via Dynamic Update Schedules</h3>
<ul>
<li><strong>Authors: </strong>Michail Theologitis, Vasilis Samoladas, Antonios Deligiannakis</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04535">https://arxiv.org/abs/2505.04535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04535">https://arxiv.org/pdf/2505.04535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04535]] Communication-Efficient Federated Fine-Tuning of Language Models via Dynamic Update Schedules(https://arxiv.org/abs/2505.04535)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) makes it possible to train models on data that would otherwise remain untapped and inaccessible. Simultaneously, pre-trained language models (LMs) have emerged as indispensable tools in modern workflows. These models exhibit extraordinary capabilities and are easily adapted to downstream tasks. This opens one of the most exciting frontiers in FL: fine-tuning LMs. However, a persistent challenge in FL is the frequent, rigid communication of parameters, a problem which is magnified by the sheer size of these modern models. Currently, the FedOpt family of algorithms is the prevailing approach in FL, though it relies on fixed, heuristic intervals for model synchronization. Recently, the FDA algorithm introduced a dynamic alternative by monitoring training progress, but it came with its own drawbacks; namely, a hard-to-tune threshold parameter and a rigid synchronization scheme. In this work, we introduce the FDA-Opt family of algorithms -- a unified generalization that extends the principles behind both FDA and FedOpt, while resolving their core limitations. We evaluate our approach on fine-tuning LMs across a range of downstream NLP tasks, and demonstrate that it consistently outperforms FedOpt -- even when FDA-Opt operates under hyper-parameter settings originally optimized for its competitors. In other words, we show that FDA-Opt is a practical, drop-in replacement for FedOpt in modern FL libraries and systems: it requires no additional configuration and delivers superior performance out of the box.</li>
</ul>

<h3>Title: Registration of 3D Point Sets Using Exponential-based Similarity Matrix</h3>
<ul>
<li><strong>Authors: </strong>Ashutosh Singandhupe, Sanket Lokhande, Hung Manh La</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04540">https://arxiv.org/abs/2505.04540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04540">https://arxiv.org/pdf/2505.04540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04540]] Registration of 3D Point Sets Using Exponential-based Similarity Matrix(https://arxiv.org/abs/2505.04540)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Point cloud registration is a fundamental problem in computer vision and robotics, involving the alignment of 3D point sets captured from varying viewpoints using depth sensors such as LiDAR or structured light. In modern robotic systems, especially those focused on mapping, it is essential to merge multiple views of the same environment accurately. However, state-of-the-art registration techniques often struggle when large rotational differences exist between point sets or when the data is significantly corrupted by sensor noise. These challenges can lead to misalignments and, consequently, to inaccurate or distorted 3D reconstructions. In this work, we address both these limitations by proposing a robust modification to the classic Iterative Closest Point (ICP) algorithm. Our method, termed Exponential Similarity Matrix ICP (ESM-ICP), integrates a Gaussian-inspired exponential weighting scheme to construct a similarity matrix that dynamically adapts across iterations. This matrix facilitates improved estimation of both rotational and translational components during alignment. We demonstrate the robustness of ESM-ICP in two challenging scenarios: (i) large rotational discrepancies between the source and target point clouds, and (ii) data corrupted by non-Gaussian noise. Our results show that ESM-ICP outperforms traditional geometric registration techniques as well as several recent learning-based methods. To encourage reproducibility and community engagement, our full implementation is made publicly available on GitHub. this https URL</li>
</ul>

<h3>Title: Purity Law for Generalizable Neural TSP Solvers</h3>
<ul>
<li><strong>Authors: </strong>Wenzhao Liu, Haoran Li, Congying Han, Zicheng Zhang, Anqi Li, Tiande Guo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04558">https://arxiv.org/abs/2505.04558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04558">https://arxiv.org/pdf/2505.04558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04558]] Purity Law for Generalizable Neural TSP Solvers(https://arxiv.org/abs/2505.04558)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Achieving generalization in neural approaches across different scales and distributions remains a significant challenge for the Traveling Salesman Problem~(TSP). A key obstacle is that neural networks often fail to learn robust principles for identifying universal patterns and deriving optimal solutions from diverse instances. In this paper, we first uncover Purity Law (PuLa), a fundamental structural principle for optimal TSP solutions, defining that edge prevalence grows exponentially with the sparsity of surrounding vertices. Statistically validated across diverse instances, PuLa reveals a consistent bias toward local sparsity in global optima. Building on this insight, we propose Purity Policy Optimization~(PUPO), a novel training paradigm that explicitly aligns characteristics of neural solutions with PuLa during the solution construction process to enhance generalization. Extensive experiments demonstrate that PUPO can be seamlessly integrated with popular neural solvers, significantly enhancing their generalization performance without incurring additional computational overhead during inference.</li>
</ul>

<h3>Title: Multitask LSTM for Arboviral Outbreak Prediction Using Public Health Data</h3>
<ul>
<li><strong>Authors: </strong>Lucas R. C. Farias, Talita P. Silva, Pedro H. M. Araujo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04566">https://arxiv.org/abs/2505.04566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04566">https://arxiv.org/pdf/2505.04566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04566]] Multitask LSTM for Arboviral Outbreak Prediction Using Public Health Data(https://arxiv.org/abs/2505.04566)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper presents a multitask learning approach based on long-short-term memory (LSTM) networks for the joint prediction of arboviral outbreaks and case counts of dengue, chikungunya, and Zika in Recife, Brazil. Leveraging historical public health data from DataSUS (2017-2023), the proposed model concurrently performs binary classification (outbreak detection) and regression (case forecasting) tasks. A sliding window strategy was adopted to construct temporal features using varying input lengths (60, 90, and 120 days), with hyperparameter optimization carried out using Keras Tuner. Model evaluation used time series cross-validation for robustness and a held-out test from 2023 for generalization assessment. The results show that longer windows improve dengue regression accuracy, while classification performance peaked at intermediate windows, suggesting an optimal trade-off between sequence length and generalization. The multitask architecture delivers competitive performance across diseases and tasks, demonstrating the feasibility and advantages of unified modeling strategies for scalable epidemic forecasting in data-limited public health scenarios.</li>
</ul>

<h3>Title: Fight Fire with Fire: Defending Against Malicious RL Fine-Tuning via Reward Neutralization</h3>
<ul>
<li><strong>Authors: </strong>Wenjun Cao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04578">https://arxiv.org/abs/2505.04578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04578">https://arxiv.org/pdf/2505.04578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04578]] Fight Fire with Fire: Defending Against Malicious RL Fine-Tuning via Reward Neutralization(https://arxiv.org/abs/2505.04578)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) fine-tuning transforms large language models while creating a vulnerability we experimentally verify: Our experiment shows that malicious RL fine-tuning dismantles safety guardrails with remarkable efficiency, requiring only 50 steps and minimal adversarial prompts, with harmful escalating from 0-2 to 7-9. This attack vector particularly threatens open-source models with parameter-level access. Existing defenses targeting supervised fine-tuning prove ineffective against RL's dynamic feedback mechanisms. We introduce Reward Neutralization, the first defense framework specifically designed against RL fine-tuning attacks, establishing concise rejection patterns that render malicious reward signals ineffective. Our approach trains models to produce minimal-information rejections that attackers cannot exploit, systematically neutralizing attempts to optimize toward harmful outputs. Experiments validate that our approach maintains low harmful scores (no greater than 2) after 200 attack steps, while standard models rapidly deteriorate. This work provides the first constructive proof that robust defense against increasingly accessible RL attacks is achievable, addressing a critical security gap for open-weight models.</li>
</ul>

<h3>Title: ZeroSearch: Incentivize the Search Capability of LLMs without Searching</h3>
<ul>
<li><strong>Authors: </strong>Hao Sun, Zile Qiao, Jiayan Guo, Xuanbo Fan, Yingyan Hou, Yong Jiang, Pengjun Xie, Fei Huang, Yan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04588">https://arxiv.org/abs/2505.04588</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04588">https://arxiv.org/pdf/2505.04588</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04588]] ZeroSearch: Incentivize the Search Capability of LLMs without Searching(https://arxiv.org/abs/2505.04588)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Effective information searching is essential for enhancing the reasoning and generation capabilities of large language models (LLMs). Recent research has explored using reinforcement learning (RL) to improve LLMs' search capabilities by interacting with live search engines in real-world environments. While these approaches show promising results, they face two major challenges: (1) Uncontrolled Document Quality: The quality of documents returned by search engines is often unpredictable, introducing noise and instability into the training process. (2) Prohibitively High API Costs: RL training requires frequent rollouts, potentially involving hundreds of thousands of search requests, which incur substantial API expenses and severely constrain scalability. To address these challenges, we introduce ZeroSearch, a reinforcement learning framework that incentivizes the search capabilities of LLMs without interacting with real search engines. Our approach begins with lightweight supervised fine-tuning to transform the LLM into a retrieval module capable of generating both relevant and noisy documents in response to a query. During RL training, we employ a curriculum-based rollout strategy that incrementally degrades the quality of generated documents, progressively eliciting the model's reasoning ability by exposing it to increasingly challenging retrieval scenarios. Extensive experiments demonstrate that ZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B LLM as the retrieval module. Remarkably, a 7B retrieval module achieves comparable performance to the real search engine, while a 14B retrieval module even surpasses it. Furthermore, it generalizes well across both base and instruction-tuned models of various parameter sizes and is compatible with a wide range of RL algorithms.</li>
</ul>

<h3>Title: MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Zhihao Zhang, Abhinav Kumar, Girish Chandar Ganesan, Xiaoming Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04594">https://arxiv.org/abs/2505.04594</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04594">https://arxiv.org/pdf/2505.04594</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04594]] MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection(https://arxiv.org/abs/2505.04594)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Accurately predicting 3D attributes is crucial for monocular 3D object detection (Mono3D), with depth estimation posing the greatest challenge due to the inherent ambiguity in mapping 2D images to 3D space. While existing methods leverage multiple depth cues (e.g., estimating depth uncertainty, modeling depth error) to improve depth accuracy, they overlook that accurate depth prediction requires conditioning on other 3D attributes, as these attributes are intrinsically inter-correlated through the 3D to 2D projection, which ultimately limits overall accuracy and stability. Inspired by Chain-of-Thought (CoT) in large language models (LLMs), this paper proposes MonoCoP, which leverages a Chain-of-Prediction (CoP) to predict attributes sequentially and conditionally via three key designs. First, it employs a lightweight AttributeNet (AN) for each 3D attribute to learn attribute-specific features. Next, MonoCoP constructs an explicit chain to propagate these learned features from one attribute to the next. Finally, MonoCoP uses a residual connection to aggregate features for each attribute along the chain, ensuring that later attribute predictions are conditioned on all previously processed attributes without forgetting the features of earlier ones. Experimental results show that our MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI leaderboard without requiring additional data and further surpasses existing methods on the Waymo and nuScenes frontal datasets.</li>
</ul>

<h3>Title: Person Recognition at Altitude and Range: Fusion of Face, Body Shape and Gait</h3>
<ul>
<li><strong>Authors: </strong>Feng Liu, Nicholas Chimitt, Lanqing Guo, Jitesh Jain, Aditya Kane, Minchul Kim, Wes Robbins, Yiyang Su, Dingqiang Ye, Xingguang Zhang, Jie Zhu, Siddharth Satyakam, Christopher Perry, Stanley H. Chan, Arun Ross, Humphrey Shi, Zhangyang Wang, Anil Jain, Xiaoming Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04616">https://arxiv.org/abs/2505.04616</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04616">https://arxiv.org/pdf/2505.04616</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04616]] Person Recognition at Altitude and Range: Fusion of Face, Body Shape and Gait(https://arxiv.org/abs/2505.04616)</code><input type="text"></li>
<li><strong>Keywords: </strong>biometric</a></li>
<li><strong>Abstract: </strong>We address the problem of whole-body person recognition in unconstrained environments. This problem arises in surveillance scenarios such as those in the IARPA Biometric Recognition and Identification at Altitude and Range (BRIAR) program, where biometric data is captured at long standoff distances, elevated viewing angles, and under adverse atmospheric conditions (e.g., turbulence and high wind velocity). To this end, we propose FarSight, a unified end-to-end system for person recognition that integrates complementary biometric cues across face, gait, and body shape modalities. FarSight incorporates novel algorithms across four core modules: multi-subject detection and tracking, recognition-aware video restoration, modality-specific biometric feature encoding, and quality-guided multi-modal fusion. These components are designed to work cohesively under degraded image conditions, large pose and scale variations, and cross-domain gaps. Extensive experiments on the BRIAR dataset, one of the most comprehensive benchmarks for long-range, multi-modal biometric recognition, demonstrate the effectiveness of FarSight. Compared to our preliminary system, this system achieves a 34.1% absolute gain in 1:1 verification accuracy (TAR@0.1% FAR), a 17.8% increase in closed-set identification (Rank-20), and a 34.3% reduction in open-set identification errors (FNIR@1% FPIR). Furthermore, FarSight was evaluated in the 2025 NIST RTE Face in Video Evaluation (FIVE), which conducts standardized face recognition testing on the BRIAR dataset. These results establish FarSight as a state-of-the-art solution for operational biometric recognition in challenging real-world conditions.</li>
</ul>

<h3>Title: Merging and Disentangling Views in Visual Reinforcement Learning for Robotic Manipulation</h3>
<ul>
<li><strong>Authors: </strong>Abdulaziz Almuzairee, Rohan Patil, Dwait Bhatt, Henrik I. Christensen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04619">https://arxiv.org/abs/2505.04619</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04619">https://arxiv.org/pdf/2505.04619</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04619]] Merging and Disentangling Views in Visual Reinforcement Learning for Robotic Manipulation(https://arxiv.org/abs/2505.04619)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Vision is well-known for its use in manipulation, especially using visual servoing. To make it robust, multiple cameras are needed to expand the field of view. That is computationally challenging. Merging multiple views and using Q-learning allows the design of more effective representations and optimization of sample efficiency. Such a solution might be expensive to deploy. To mitigate this, we introduce a Merge And Disentanglement (MAD) algorithm that efficiently merges views to increase sample efficiency while augmenting with single-view features to allow lightweight deployment and ensure robust policies. We demonstrate the efficiency and robustness of our approach using Meta-World and ManiSkill3. For project website and code, see this https URL</li>
</ul>

<h3>Title: On Path to Multimodal Generalist: General-Level and General-Bench</h3>
<ul>
<li><strong>Authors: </strong>Hao Fei, Yuan Zhou, Juncheng Li, Xiangtai Li, Qingshan Xu, Bobo Li, Shengqiong Wu, Yaoting Wang, Junbao Zhou, Jiahao Meng, Qingyu Shi, Zhiyuan Zhou, Liangtao Shi, Minghe Gao, Daoan Zhang, Zhiqi Ge, Weiming Wu, Siliang Tang, Kaihang Pan, Yaobo Ye, Haobo Yuan, Tao Zhang, Tianjie Ju, Zixiang Meng, Shilin Xu, Liyu Jia, Wentao Hu, Meng Luo, Jiebo Luo, Tat-Seng Chua, Shuicheng Yan, Hanwang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.04620">https://arxiv.org/abs/2505.04620</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.04620">https://arxiv.org/pdf/2505.04620</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.04620]] On Path to Multimodal Generalist: General-Level and General-Bench(https://arxiv.org/abs/2505.04620)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The Multimodal Large Language Model (MLLM) is currently experiencing rapid growth, driven by the advanced capabilities of LLMs. Unlike earlier specialists, existing MLLMs are evolving towards a Multimodal Generalist paradigm. Initially limited to understanding multiple modalities, these models have advanced to not only comprehend but also generate across modalities. Their capabilities have expanded from coarse-grained to fine-grained multimodal understanding and from supporting limited modalities to arbitrary ones. While many benchmarks exist to assess MLLMs, a critical question arises: Can we simply assume that higher performance across tasks indicates a stronger MLLM capability, bringing us closer to human-level AI? We argue that the answer is not as straightforward as it seems. This project introduces General-Level, an evaluation framework that defines 5-scale levels of MLLM performance and generality, offering a methodology to compare MLLMs and gauge the progress of existing systems towards more robust multimodal generalists and, ultimately, towards AGI. At the core of the framework is the concept of Synergy, which measures whether models maintain consistent capabilities across comprehension and generation, and across multiple modalities. To support this evaluation, we present General-Bench, which encompasses a broader spectrum of skills, modalities, formats, and capabilities, including over 700 tasks and 325,800 instances. The evaluation results that involve over 100 existing state-of-the-art MLLMs uncover the capability rankings of generalists, highlighting the challenges in reaching genuine AI. We expect this project to pave the way for future research on next-generation multimodal foundation models, providing a robust infrastructure to accelerate the realization of AGI. Project page: this https URL</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
