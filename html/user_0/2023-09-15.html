<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Toward Lossless Homomorphic Encryption for Scientific Computation. (arXiv:2309.07284v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07284">http://arxiv.org/abs/2309.07284</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07284]] Toward Lossless Homomorphic Encryption for Scientific Computation(http://arxiv.org/abs/2309.07284)</code></li>
<li>Summary: <p>This paper presents a comprehensive investigation into encrypted computations
using the CKKS (Cheon-Kim-Kim-Song) scheme, with a focus on multi-dimensional
vector operations and real-world applications. Through two meticulously
designed experiments, the study explores the potential of the CKKS scheme in
Super Computing and its implications for data privacy and computational
efficiency. The first experiment reveals the promising applicability of CKKS to
matrix multiplication, indicating marginal differences in Euclidean distance
and near-to-zero mean square error across various matrix sizes. The second
experiment, applied to a wildfire dataset, illustrates the feasibility of using
encrypted machine learning models without significant loss in accuracy. The
insights gleaned from the research set a robust foundation for future
innovations, including the potential for GPU acceleration in CKKS computations
within TenSEAL. Challenges such as noise budget computation, accuracy loss in
multiplication, and the distinct characteristics of arithmetic operations in
the context of CKKS are also discussed. The paper serves as a vital step
towards understanding the complexities and potentials of encrypted
computations, with broad implications for secure data processing and privacy
preservation in various scientific domains.
</p></li>
</ul>

<h3>Title: Secure and Scalable Circuit-based Protocol for Multi-Party Private Set Intersection. (arXiv:2309.07406v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07406">http://arxiv.org/abs/2309.07406</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07406]] Secure and Scalable Circuit-based Protocol for Multi-Party Private Set Intersection(http://arxiv.org/abs/2309.07406)</code></li>
<li>Summary: <p>We propose a novel protocol for computing a circuit which implements the
multi-party private set intersection functionality (PSI). Circuit-based
approach has advantages over using custom protocols to achieve this task, since
many applications of PSI do not require the computation of the intersection
itself, but rather specific functional computations over the items in the
intersection.
</p>
<p>Our protocol represents the pioneering circuit-based multi-party PSI
protocol, which builds upon and optimizes the two-party SCS
\cite{huang2012private} protocol. By using secure computation between two
parties, our protocol sidesteps the complexities associated with multi-party
interactions and demonstrates good scalability.
</p>
<p>In order to mitigate the high overhead associated with circuit-based
constructions, we have further enhanced our protocol by utilizing simple
hashing scheme and permutation-based hash functions. These tricks have enabled
us to minimize circuit size by employing bucketing techniques while
simultaneously attaining noteworthy reductions in both computation and
communication expenses.
</p></li>
</ul>

<h3>Title: TGh: A TEE/GC Hybrid Enabling Confidential FaaS Platforms. (arXiv:2309.07764v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07764">http://arxiv.org/abs/2309.07764</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07764]] TGh: A TEE/GC Hybrid Enabling Confidential FaaS Platforms(http://arxiv.org/abs/2309.07764)</code></li>
<li>Summary: <p>Trusted Execution Environments (TEEs) suffer from performance issues when
executing certain management instructions, such as creating an enclave, context
switching in and out of protected mode, and swapping cached pages. This is
especially problematic for short-running, interactive functions in
Function-as-a-Service (FaaS) platforms, where existing techniques to address
enclave overheads are insufficient. We find FaaS functions can spend more time
managing the enclave than executing application instructions. In this work, we
propose a TEE/GC hybrid (TGh) protocol to enable confidential FaaS platforms.
TGh moves computation out of the enclave onto the untrusted host using garbled
circuits (GC), a cryptographic construction for secure function evaluation. Our
approach retains the security guarantees of enclaves while avoiding the
performance issues associated with enclave management instructions.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: What Matters to Enhance Traffic Rule Compliance of Imitation Learning for Automated Driving. (arXiv:2309.07808v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07808">http://arxiv.org/abs/2309.07808</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07808]] What Matters to Enhance Traffic Rule Compliance of Imitation Learning for Automated Driving(http://arxiv.org/abs/2309.07808)</code></li>
<li>Summary: <p>More research attention has recently been given to end-to-end autonomous
driving technologies where the entire driving pipeline is replaced with a
single neural network because of its simpler structure and faster inference
time. Despite this appealing approach largely reducing the components in
driving pipeline, its simplicity also leads to interpretability problems and
safety issues <a href="http://export.arxiv.org/abs/2003.06404">arXiv:2003.06404</a>. The trained policy is not always compliant with
the traffic rules and it is also hard to discover the reason for the
misbehavior because of the lack of intermediate outputs. Meanwhile, Sensors are
also critical to autonomous driving's security and feasibility to perceive the
surrounding environment under complex driving scenarios. In this paper, we
proposed P-CSG, a novel penalty-based imitation learning approach with cross
semantics generation sensor fusion technologies to increase the overall
performance of End-to-End Autonomous Driving. We conducted an assessment of our
model's performance using the Town 05 Long benchmark, achieving an impressive
driving score improvement of over 15%. Furthermore, we conducted robustness
evaluations against adversarial attacks like FGSM and Dot attacks, revealing a
substantial increase in robustness compared to baseline models.More detailed
information, such as code-based resources, ablation studies and videos can be
found at https://hk-zh.github.io/p-csg-plus.
</p></li>
</ul>

<h3>Title: Commercial Anti-Smishing Tools and Their Comparative Effectiveness Against Modern Threats. (arXiv:2309.07447v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07447">http://arxiv.org/abs/2309.07447</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07447]] Commercial Anti-Smishing Tools and Their Comparative Effectiveness Against Modern Threats(http://arxiv.org/abs/2309.07447)</code></li>
<li>Summary: <p>Smishing, also known as SMS phishing, is a type of fraudulent communication
in which an attacker disguises SMS communications to deceive a target into
providing their sensitive data. Smishing attacks use a variety of tactics;
however, they have a similar goal of stealing money or personally identifying
information (PII) from a victim. In response to these attacks, a wide variety
of anti-smishing tools have been developed to block or filter these
communications. Despite this, the number of phishing attacks continue to rise.
In this paper, we developed a test bed for measuring the effectiveness of
popular anti-smishing tools against fresh smishing attacks. To collect fresh
smishing data, we introduce Smishtank.com, a collaborative online resource for
reporting and collecting smishing data sets. The SMS messages were validated by
a security expert and an in-depth qualitative analysis was performed on the
collected messages to provide further insights. To compare tool effectiveness,
we experimented with 20 smishing and benign messages across 3 key segments of
the SMS messaging delivery ecosystem. Our results revealed significant room for
improvement in all 3 areas against our smishing set. Most anti-phishing apps
and bulk messaging services didn't filter smishing messages beyond the carrier
blocking. The 2 apps that blocked the most smish also blocked 85-100\% of
benign messages. Finally, while carriers did not block any benign messages,
they were only able to reach a 25-35\% blocking rate for smishing messages. Our
work provides insights into the performance of anti-smishing tools and the
roles they play in the message blocking process. This paper would enable the
research community and industry to be better informed on the current state of
anti-smishing technology on the SMS platform.
</p></li>
</ul>

<h3>Title: From Compliance to Impact: Tracing the Transformation of an Organizational Security Awareness Program. (arXiv:2309.07724v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07724">http://arxiv.org/abs/2309.07724</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07724]] From Compliance to Impact: Tracing the Transformation of an Organizational Security Awareness Program(http://arxiv.org/abs/2309.07724)</code></li>
<li>Summary: <p>There is a growing recognition of the need for a transformation from
organizational security awareness programs focused on compliance -- measured by
training completion rates -- to those resulting in behavior change. However,
few prior studies have begun to unpack the organizational practices of the
security awareness teams tasked with executing program transformation. We
conducted a year-long case study of a security awareness program in a United
States (U.S.) government agency, collecting data via field observations,
interviews, and documents. Our findings reveal the challenges and practices
involved in the progression of a security awareness program from being
compliance-focused to emphasizing impact on workforce attitudes and behaviors.
We uniquely capture transformational organizational security awareness
practices in action via a longitudinal study involving multiple workforce
perspectives. Our study insights can serve as a resource for other security
awareness programs and workforce development initiatives aimed at better
defining the security awareness work role.
</p></li>
</ul>

<h3>Title: AIDPS:Adaptive Intrusion Detection and Prevention System for Underwater Acoustic Sensor Networks. (arXiv:2309.07730v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07730">http://arxiv.org/abs/2309.07730</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07730]] AIDPS:Adaptive Intrusion Detection and Prevention System for Underwater Acoustic Sensor Networks(http://arxiv.org/abs/2309.07730)</code></li>
<li>Summary: <p>Underwater Acoustic Sensor Networks (UW-ASNs) are predominantly used for
underwater environments and find applications in many areas. However, a lack of
security considerations, the unstable and challenging nature of the underwater
environment, and the resource-constrained nature of the sensor nodes used for
UW-ASNs (which makes them incapable of adopting security primitives) make the
UW-ASN prone to vulnerabilities. This paper proposes an Adaptive decentralised
Intrusion Detection and Prevention System called AIDPS for UW-ASNs. The
proposed AIDPS can improve the security of the UW-ASNs so that they can
efficiently detect underwater-related attacks (e.g., blackhole, grayhole and
flooding attacks). To determine the most effective configuration of the
proposed construction, we conduct a number of experiments using several
state-of-the-art machine learning algorithms (e.g., Adaptive Random Forest
(ARF), light gradient-boosting machine, and K-nearest neighbours) and concept
drift detection algorithms (e.g., ADWIN, kdqTree, and Page-Hinkley). Our
experimental results show that incremental ARF using ADWIN provides optimal
performance when implemented with One-class support vector machine (SVM)
anomaly-based detectors. Furthermore, our extensive evaluation results also
show that the proposed scheme outperforms state-of-the-art bench-marking
methods while providing a wider range of desirable features such as scalability
and complexity.
</p></li>
</ul>

<h3>Title: RIS-Assisted Physical Layer Authentication for 6G Endogenous Security. (arXiv:2309.07736v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07736">http://arxiv.org/abs/2309.07736</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07736]] RIS-Assisted Physical Layer Authentication for 6G Endogenous Security(http://arxiv.org/abs/2309.07736)</code></li>
<li>Summary: <p>The physical layer authentication (PLA) is a promising technology which can
enhance the access security of a massive number of devices in the near future.
In this paper, we propose a reconfigurable intelligent surface (RIS)-assisted
PLA system, in which the legitimate transmitter can customize the channel
fingerprints during PLA by controlling the ON-OFF state of the RIS. Without
loss of generality, we use the received signal strength (RSS) based spoofing
detection approach to analyze the feasibility of the proposed architecture.
Specifically, based on the RSS, we derive the statistical properties of PLA and
give some interesting insights, which showcase that the RIS-assisted PLA is
theoretically feasible. Then, we derive the optimal detection threshold to
maximize the performance in the context of the presented performance metrics.
Next, the actual feasibility of the proposed system is verified via
proof-of-concept experiments on a RIS-assisted PLA prototype platform. The
experiment results show that there are 3.5% and 76% performance improvements
when the transmission sources are at different locations and at the same
location, respectively.
</p></li>
</ul>

<h3>Title: The Nonce-nce of Web Security: an Investigation of CSP Nonces Reuse. (arXiv:2309.07782v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07782">http://arxiv.org/abs/2309.07782</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07782]] The Nonce-nce of Web Security: an Investigation of CSP Nonces Reuse(http://arxiv.org/abs/2309.07782)</code></li>
<li>Summary: <p>Content Security Policy (CSP) is an effective security mechanism that
prevents the exploitation of Cross-Site Scripting (XSS) vulnerabilities on
websites by specifying the sources from which their web pages can load
resources, such as scripts and styles. CSP nonces enable websites to allow the
execution of specific inline scripts and styles without relying on a whitelist.
In this study, we measure and analyze the use of CSP nonces in the wild,
specifically looking for nonce reuse, short nonces, and invalid nonces. We find
that, of the 2271 sites that deploy a nonce-based policy, 598 of them reuse the
same nonce value in more than one response, potentially enabling attackers to
bypass protection offered by the CSP against XSS attacks. We analyze the causes
of the nonce reuses to identify whether they are introduced by the server-side
code or if the nonces are being cached by web caches. Moreover, we investigate
whether nonces are only reused within the same session or for different
sessions, as this impacts the effectiveness of CSP in preventing XSS attacks.
Finally, we discuss the possibilities for attackers to bypass the CSP and
achieve XSS in different nonce reuse scenarios.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Keep your Identity Small: Privacy-preserving Client-side Fingerprinting. (arXiv:2309.07563v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07563">http://arxiv.org/abs/2309.07563</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07563]] Keep your Identity Small: Privacy-preserving Client-side Fingerprinting(http://arxiv.org/abs/2309.07563)</code></li>
<li>Summary: <p>Device fingerprinting is a widely used technique that allows a third party to
identify a particular device. Applications of device fingerprinting include
authentication, attacker identification, or software license binding.
</p>
<p>Device fingerprinting is also used on the web as a method for identifying
users. Unfortunately, one of its most widespread uses is to identify users
visiting different websites and thus build their browsing history. This
constitutes a specific type of web tracking that poses a threat to users'
privacy. While many anti-tracking solutions have been proposed, all of them
block or tamper with device fingerprinting techniques rather than just blocking
their web tracking application. Therefore, users may be limited in their
experience while using a website.
</p>
<p>In this paper, we propose \textit{Privacy-preserving Client-side
Fingerprinting} (PCF), a new method that allows device fingerprinting on the
web, while blocks the possibility of performing web tracking. To this end, PCF
is built upon fingerprinting transparency: any website ought to declare its
fingerprinting scripts while users will compute them in a privacy-preserving
manner, limiting the resultant fingerprints for each different domain and,
therefore, making web tracking not feasible.
</p></li>
</ul>

<h3>Title: Do Not Give Away My Secrets: Uncovering the Privacy Issue of Neural Code Completion Tools. (arXiv:2309.07639v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07639">http://arxiv.org/abs/2309.07639</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07639]] Do Not Give Away My Secrets: Uncovering the Privacy Issue of Neural Code Completion Tools(http://arxiv.org/abs/2309.07639)</code></li>
<li>Summary: <p>Neural Code Completion Tools (NCCTs) have reshaped the field of software
development, which accurately suggest contextually-relevant code snippets
benefiting from language modeling techniques. However, language models may emit
the training data verbatim during inference with appropriate prompts. This
memorization property raises privacy concerns of commercial NCCTs about the
hard-coded credential leakage, leading to unauthorized access to systems.
Therefore, to answer whether NCCTs will inadvertently emit the hard-coded
credential, we propose an evaluation tool called Hard-coded Credential Revealer
(HCR). HCR effectively constructs test prompts from GitHub code files with
credentials to trigger memorization phenomenon of commercial NCCTs. Then, HCR
extracts credentials with pre-defined format from the responses by four
designed filters. We apply HCR to evaluate two representative commercial NCCTs:
GitHub Copilot and Amazon CodeWhisperer and successfully extracted 2,702
hard-coded credentials from Copilot and 129 secrets from CodeWhisper under the
black-box setting, among which at least 3.6% and 5.4% secrets are real strings
from GitHub repositories. Moreover, two operational credentials were
identified. The experimental results raise the severe privacy concern of the
potential leakage of hard-coded credentials in the training data of commercial
NCCTs.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: Multi-step prediction of chlorophyll concentration based on Adaptive Graph-Temporal Convolutional Network with Series Decomposition. (arXiv:2309.07187v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07187">http://arxiv.org/abs/2309.07187</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07187]] Multi-step prediction of chlorophyll concentration based on Adaptive Graph-Temporal Convolutional Network with Series Decomposition(http://arxiv.org/abs/2309.07187)</code></li>
<li>Summary: <p>Chlorophyll concentration can well reflect the nutritional status and algal
blooms of water bodies, and is an important indicator for evaluating water
quality. The prediction of chlorophyll concentration change trend is of great
significance to environmental protection and aquaculture. However, there is a
complex and indistinguishable nonlinear relationship between many factors
affecting chlorophyll concentration. In order to effectively mine the nonlinear
features contained in the data. This paper proposes a time-series decomposition
adaptive graph-time convolutional network ( AGTCNSD ) prediction model.
Firstly, the original sequence is decomposed into trend component and periodic
component by moving average method. Secondly, based on the graph convolutional
neural network, the water quality parameter data is modeled, and a parameter
embedding matrix is defined. The idea of matrix decomposition is used to assign
weight parameters to each node. The adaptive graph convolution learns the
relationship between different water quality parameters, updates the state
information of each parameter, and improves the learning ability of the update
relationship between nodes. Finally, time dependence is captured by time
convolution to achieve multi-step prediction of chlorophyll concentration. The
validity of the model is verified by the water quality data of the coastal city
Beihai. The results show that the prediction effect of this method is better
than other methods. It can be used as a scientific resource for environmental
management decision-making.
</p></li>
</ul>

<h2>defense</h2>
<h3>Title: Physical Invisible Backdoor Based on Camera Imaging. (arXiv:2309.07428v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07428">http://arxiv.org/abs/2309.07428</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07428]] Physical Invisible Backdoor Based on Camera Imaging(http://arxiv.org/abs/2309.07428)</code></li>
<li>Summary: <p>Backdoor attack aims to compromise a model, which returns an adversary-wanted
output when a specific trigger pattern appears yet behaves normally for clean
inputs. Current backdoor attacks require changing pixels of clean images, which
results in poor stealthiness of attacks and increases the difficulty of the
physical implementation. This paper proposes a novel physical invisible
backdoor based on camera imaging without changing nature image pixels.
Specifically, a compromised model returns a target label for images taken by a
particular camera, while it returns correct results for other images. To
implement and evaluate the proposed backdoor, we take shots of different
objects from multi-angles using multiple smartphones to build a new dataset of
21,500 images. Conventional backdoor attacks work ineffectively with some
classical models, such as ResNet18, over the above-mentioned dataset.
Therefore, we propose a three-step training strategy to mount the backdoor
attack. First, we design and train a camera identification model with the phone
IDs to extract the camera fingerprint feature. Subsequently, we elaborate a
special network architecture, which is easily compromised by our backdoor
attack, by leveraging the attributes of the CFA interpolation algorithm and
combining it with the feature extraction block in the camera identification
model. Finally, we transfer the backdoor from the elaborated special network
architecture to the classical architecture model via teacher-student
distillation learning. Since the trigger of our method is related to the
specific phone, our attack works effectively in the physical world. Experiment
results demonstrate the feasibility of our proposed approach and robustness
against various backdoor defenses.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: Semantic Adversarial Attacks via Diffusion Models. (arXiv:2309.07398v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07398">http://arxiv.org/abs/2309.07398</a></li>
<li>Code URL: https://github.com/steven202/semantic_adv_via_dm</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07398]] Semantic Adversarial Attacks via Diffusion Models(http://arxiv.org/abs/2309.07398)</code></li>
<li>Summary: <p>Traditional adversarial attacks concentrate on manipulating clean examples in
the pixel space by adding adversarial perturbations. By contrast, semantic
adversarial attacks focus on changing semantic attributes of clean examples,
such as color, context, and features, which are more feasible in the real
world. In this paper, we propose a framework to quickly generate a semantic
adversarial attack by leveraging recent diffusion models since semantic
information is included in the latent space of well-trained diffusion models.
Then there are two variants of this framework: 1) the Semantic Transformation
(ST) approach fine-tunes the latent space of the generated image and/or the
diffusion model itself; 2) the Latent Masking (LM) approach masks the latent
space with another target image and local backpropagation-based interpretation
methods. Additionally, the ST approach can be applied in either white-box or
black-box settings. Extensive experiments are conducted on CelebA-HQ and AFHQ
datasets, and our framework demonstrates great fidelity, generalizability, and
transferability compared to other baselines. Our approaches achieve
approximately 100% attack success rate in multiple settings with the best FID
as 36.61. Code is available at
https://github.com/steven202/semantic_adv_via_dm.
</p></li>
</ul>

<h3>Title: Detecting Unknown Attacks in IoT Environments: An Open Set Classifier for Enhanced Network Intrusion Detection. (arXiv:2309.07461v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07461">http://arxiv.org/abs/2309.07461</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07461]] Detecting Unknown Attacks in IoT Environments: An Open Set Classifier for Enhanced Network Intrusion Detection(http://arxiv.org/abs/2309.07461)</code></li>
<li>Summary: <p>The widespread integration of Internet of Things (IoT) devices across all
facets of life has ushered in an era of interconnectedness, creating new
avenues for cybersecurity challenges and underscoring the need for robust
intrusion detection systems. However, traditional security systems are designed
with a closed-world perspective and often face challenges in dealing with the
ever-evolving threat landscape, where new and unfamiliar attacks are constantly
emerging. In this paper, we introduce a framework aimed at mitigating the open
set recognition (OSR) problem in the realm of Network Intrusion Detection
Systems (NIDS) tailored for IoT environments. Our framework capitalizes on
image-based representations of packet-level data, extracting spatial and
temporal patterns from network traffic. Additionally, we integrate stacking and
sub-clustering techniques, enabling the identification of unknown attacks by
effectively modeling the complex and diverse nature of benign behavior. The
empirical results prominently underscore the framework's efficacy, boasting an
impressive 88\% detection rate for previously unseen attacks when compared
against existing approaches and recent advancements. Future work will perform
extensive experimentation across various openness levels and attack scenarios,
further strengthening the adaptability and performance of our proposed solution
in safeguarding IoT environments.
</p></li>
</ul>

<h3>Title: mEBAL2 Database and Benchmark: Image-based Multispectral Eyeblink Detection. (arXiv:2309.07880v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07880">http://arxiv.org/abs/2309.07880</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07880]] mEBAL2 Database and Benchmark: Image-based Multispectral Eyeblink Detection(http://arxiv.org/abs/2309.07880)</code></li>
<li>Summary: <p>This work introduces a new multispectral database and novel approaches for
eyeblink detection in RGB and Near-Infrared (NIR) individual images. Our
contributed dataset (mEBAL2, multimodal Eye Blink and Attention Level
estimation, Version 2) is the largest existing eyeblink database, representing
a great opportunity to improve data-driven multispectral approaches for blink
detection and related applications (e.g., attention level estimation and
presentation attack detection in face biometrics). mEBAL2 includes 21,100 image
sequences from 180 different students (more than 2 million labeled images in
total) while conducting a number of e-learning tasks of varying difficulty or
taking a real course on HTML initiation through the edX MOOC platform. mEBAL2
uses multiple sensors, including two Near-Infrared (NIR) and one RGB camera to
capture facial gestures during the execution of the tasks, as well as an
Electroencephalogram (EEG) band to get the cognitive activity of the user and
blinking events. Furthermore, this work proposes a Convolutional Neural Network
architecture as benchmark for blink detection on mEBAL2 with performances up to
97%. Different training methodologies are implemented using the RGB spectrum,
NIR spectrum, and the combination of both to enhance the performance on
existing eyeblink detectors. We demonstrate that combining NIR and RGB images
during training improves the performance of RGB eyeblink detectors (i.e.,
detection based only on a RGB image). Finally, the generalization capacity of
the proposed eyeblink detectors is validated in wilder and more challenging
environments like the HUST-LEBW dataset to show the usefulness of mEBAL2 to
train a new generation of data-driven approaches for eyeblink detection.
</p></li>
</ul>

<h3>Title: Mitigating Adversarial Attacks in Federated Learning with Trusted Execution Environments. (arXiv:2309.07197v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07197">http://arxiv.org/abs/2309.07197</a></li>
<li>Code URL: https://github.com/queyrusi/pelta</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07197]] Mitigating Adversarial Attacks in Federated Learning with Trusted Execution Environments(http://arxiv.org/abs/2309.07197)</code></li>
<li>Summary: <p>The main premise of federated learning (FL) is that machine learning model
updates are computed locally to preserve user data privacy. This approach
avoids by design user data to ever leave the perimeter of their device. Once
the updates aggregated, the model is broadcast to all nodes in the federation.
However, without proper defenses, compromised nodes can probe the model inside
their local memory in search for adversarial examples, which can lead to
dangerous real-world scenarios. For instance, in image-based applications,
adversarial examples consist of images slightly perturbed to the human eye
getting misclassified by the local model. These adversarial images are then
later presented to a victim node's counterpart model to replay the attack.
Typical examples harness dissemination strategies such as altered traffic signs
(patch attacks) no longer recognized by autonomous vehicles or seemingly
unaltered samples that poison the local dataset of the FL scheme to undermine
its robustness. Pelta is a novel shielding mechanism leveraging Trusted
Execution Environments (TEEs) that reduce the ability of attackers to craft
adversarial samples. Pelta masks inside the TEE the first part of the
back-propagation chain rule, typically exploited by attackers to craft the
malicious samples. We evaluate Pelta on state-of-the-art accurate models using
three well-established datasets: CIFAR-10, CIFAR-100 and ImageNet. We show the
effectiveness of Pelta in mitigating six white-box state-of-the-art adversarial
attacks, such as Projected Gradient Descent, Momentum Iterative Method, Auto
Projected Gradient Descent, the Carlini &amp; Wagner attack. In particular, Pelta
constitutes the first attempt at defending an ensemble model against the
Self-Attention Gradient attack to the best of our knowledge. Our code is
available to the research community at https://github.com/queyrusi/Pelta.
</p></li>
</ul>

<h3>Title: On Autonomous Agents in a Cyber Defence Environment. (arXiv:2309.07388v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07388">http://arxiv.org/abs/2309.07388</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07388]] On Autonomous Agents in a Cyber Defence Environment(http://arxiv.org/abs/2309.07388)</code></li>
<li>Summary: <p>Autonomous Cyber Defence is required to respond to high-tempo cyber-attacks.
To facilitate the research in this challenging area, we explore the utility of
the autonomous cyber operation environments presented as part of the Cyber
Autonomy Gym for Experimentation (CAGE) Challenges, with a specific focus on
CAGE Challenge 2. CAGE Challenge 2 required a defensive Blue agent to defend a
network from an attacking Red agent. We provide a detailed description of the
this challenge and describe the approaches taken by challenge participants.
From the submitted agents, we identify four classes of algorithms, namely,
Single- Agent Deep Reinforcement Learning (DRL), Hierarchical DRL, Ensembles,
and Non-DRL approaches. Of these classes, we found that the hierarchical DRL
approach was the most capable of learning an effective cyber defensive
strategy. Our analysis of the agent policies identified that different
algorithms within the same class produced diverse strategies and that the
strategy used by the defensive Blue agent varied depending on the strategy used
by the offensive Red agent. We conclude that DRL algorithms are a suitable
candidate for autonomous cyber defence applications.
</p></li>
</ul>

<h3>Title: Sync+Sync: A Covert Channel Built on fsync with Storage. (arXiv:2309.07657v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07657">http://arxiv.org/abs/2309.07657</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07657]] Sync+Sync: A Covert Channel Built on fsync with Storage(http://arxiv.org/abs/2309.07657)</code></li>
<li>Summary: <p>Scientists have built a variety of covert channels for secretive information
transmission with CPU cache and main memory. In this paper, we turn to a lower
level in the memory hierarchy, i.e., persistent storage. Most programs store
intermediate or eventual results in the form of files and some of them call
fsync to synchronously persist a file with storage device for orderly
persistence. Our quantitative study shows that one program would undergo
significantly longer response time for fsync call if the other program is
concurrently calling fsync, although they do not share any data. We further
find that, concurrent fsync calls contend at multiple levels of storage stack
due to sharing software structures (e.g., Ext4's journal) and hardware
resources (e.g., disk's I/O dispatch queue).
</p>
<p>We accordingly build a covert channel named Sync+Sync. Sync+Sync delivers a
transmission bandwidth of 20,000 bits per second at an error rate of about
0.40% with an ordinary solid-state drive. Sync+Sync can be conducted in
cross-disk partition, cross-file system, cross-container, cross-virtual
machine, and even cross-disk drive fashions, without sharing data between
programs. Next, we launch side-channel attacks with Sync+Sync and manage to
precisely detect operations of a victim database (e.g., insert/update and
B-Tree node split). We also leverage Sync+Sync to distinguish applications and
websites with high accuracy by detecting and analyzing their fsync frequencies
and flushed data volumes. These attacks are useful to support further
fine-grained information leakage.
</p></li>
</ul>

<h3>Title: Multi-Source Domain Adaptation meets Dataset Distillation through Dataset Dictionary Learning. (arXiv:2309.07666v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07666">http://arxiv.org/abs/2309.07666</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07666]] Multi-Source Domain Adaptation meets Dataset Distillation through Dataset Dictionary Learning(http://arxiv.org/abs/2309.07666)</code></li>
<li>Summary: <p>In this paper, we consider the intersection of two problems in machine
learning: Multi-Source Domain Adaptation (MSDA) and Dataset Distillation (DD).
On the one hand, the first considers adapting multiple heterogeneous labeled
source domains to an unlabeled target domain. On the other hand, the second
attacks the problem of synthesizing a small summary containing all the
information about the datasets. We thus consider a new problem called MSDA-DD.
To solve it, we adapt previous works in the MSDA literature, such as
Wasserstein Barycenter Transport and Dataset Dictionary Learning, as well as DD
method Distribution Matching. We thoroughly experiment with this novel problem
on four benchmarks (Caltech-Office 10, Tennessee-Eastman Process, Continuous
Stirred Tank Reactor, and Case Western Reserve University), where we show that,
even with as little as 1 sample per class, one achieves state-of-the-art
adaptation performance.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Efficiently Robustify Pre-trained Models. (arXiv:2309.07499v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07499">http://arxiv.org/abs/2309.07499</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07499]] Efficiently Robustify Pre-trained Models(http://arxiv.org/abs/2309.07499)</code></li>
<li>Summary: <p>A recent trend in deep learning algorithms has been towards training large
scale models, having high parameter count and trained on big dataset. However,
robustness of such large scale models towards real-world settings is still a
less-explored topic. In this work, we first benchmark the performance of these
models under different perturbations and datasets thereby representing
real-world shifts, and highlight their degrading performance under these
shifts. We then discuss on how complete model fine-tuning based existing
robustification schemes might not be a scalable option given very large scale
networks and can also lead them to forget some of the desired characterstics.
Finally, we propose a simple and cost-effective method to solve this problem,
inspired by knowledge transfer literature. It involves robustifying smaller
models, at a lower computation cost, and then use them as teachers to tune a
fraction of these large scale networks, reducing the overall computational
overhead. We evaluate our proposed method under various vision perturbations
including ImageNet-C,R,S,A datasets and also for transfer learning, zero-shot
evaluation setups on different datasets. Benchmark results show that our method
is able to induce robustness to these large scale models efficiently, requiring
significantly lower time and also preserves the transfer learning, zero-shot
properties of the original model which none of the existing methods are able to
achieve.
</p></li>
</ul>

<h3>Title: Towards Robust and Unconstrained Full Range of Rotation Head Pose Estimation. (arXiv:2309.07654v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07654">http://arxiv.org/abs/2309.07654</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07654]] Towards Robust and Unconstrained Full Range of Rotation Head Pose Estimation(http://arxiv.org/abs/2309.07654)</code></li>
<li>Summary: <p>Estimating the head pose of a person is a crucial problem for numerous
applications that is yet mainly addressed as a subtask of frontal pose
prediction. We present a novel method for unconstrained end-to-end head pose
estimation to tackle the challenging task of full range of orientation head
pose prediction. We address the issue of ambiguous rotation labels by
introducing the rotation matrix formalism for our ground truth data and propose
a continuous 6D rotation matrix representation for efficient and robust direct
regression. This allows to efficiently learn full rotation appearance and to
overcome the limitations of the current state-of-the-art. Together with new
accumulated training data that provides full head pose rotation data and a
geodesic loss approach for stable learning, we design an advanced model that is
able to predict an extended range of head orientations. An extensive evaluation
on public datasets demonstrates that our method significantly outperforms other
state-of-the-art methods in an efficient and robust manner, while its advanced
prediction range allows the expansion of the application area. We open-source
our training and testing code along with our trained models:
https://github.com/thohemp/6DRepNet360.
</p></li>
</ul>

<h3>Title: OmnimatteRF: Robust Omnimatte with 3D Background Modeling. (arXiv:2309.07749v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07749">http://arxiv.org/abs/2309.07749</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07749]] OmnimatteRF: Robust Omnimatte with 3D Background Modeling(http://arxiv.org/abs/2309.07749)</code></li>
<li>Summary: <p>Video matting has broad applications, from adding interesting effects to
casually captured movies to assisting video production professionals. Matting
with associated effects such as shadows and reflections has also attracted
increasing research activity, and methods like Omnimatte have been proposed to
separate dynamic foreground objects of interest into their own layers. However,
prior works represent video backgrounds as 2D image layers, limiting their
capacity to express more complicated scenes, thus hindering application to
real-world videos. In this paper, we propose a novel video matting method,
OmnimatteRF, that combines dynamic 2D foreground layers and a 3D background
model. The 2D layers preserve the details of the subjects, while the 3D
background robustly reconstructs scenes in real-world videos. Extensive
experiments demonstrate that our method reconstructs scenes with better quality
on various videos.
</p></li>
</ul>

<h3>Title: MC-NeRF: Muti-Camera Neural Radiance Fields for Muti-Camera Image Acquisition Systems. (arXiv:2309.07846v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07846">http://arxiv.org/abs/2309.07846</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07846]] MC-NeRF: Muti-Camera Neural Radiance Fields for Muti-Camera Image Acquisition Systems(http://arxiv.org/abs/2309.07846)</code></li>
<li>Summary: <p>Neural Radiance Fields (NeRF) employ multi-view images for 3D scene
representation and have shown remarkable performance. As one of the primary
sources of multi-view images, multi-camera systems encounter challenges such as
varying intrinsic parameters and frequent pose changes. Most previous
NeRF-based methods often assume a global unique camera and seldom consider
scenarios with multiple cameras. Besides, some pose-robust methods still remain
susceptible to suboptimal solutions when poses are poor initialized. In this
paper, we propose MC-NeRF, a method can jointly optimize both intrinsic and
extrinsic parameters for bundle-adjusting Neural Radiance Fields. Firstly, we
conduct a theoretical analysis to tackle the degenerate case and coupling issue
that arise from the joint optimization between intrinsic and extrinsic
parameters. Secondly, based on the proposed solutions, we introduce an
efficient calibration image acquisition scheme for multi-camera systems,
including the design of calibration object. Lastly, we present a global
end-to-end network with training sequence that enables the regression of
intrinsic and extrinsic parameters, along with the rendering network. Moreover,
most existing datasets are designed for unique camera, we create a new dataset
that includes four different styles of multi-camera acquisition systems,
allowing readers to generate custom datasets. Experiments confirm the
effectiveness of our method when each image corresponds to different camera
parameters. Specifically, we adopt up to 110 images with 110 different
intrinsic and extrinsic parameters, to achieve 3D scene representation without
providing initial poses. The Code and supplementary materials are available at
https://in2-viaun.github.io/MC-NeRF.
</p></li>
</ul>

<h3>Title: TEMPO: Efficient Multi-View Pose Estimation, Tracking, and Forecasting. (arXiv:2309.07910v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07910">http://arxiv.org/abs/2309.07910</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07910]] TEMPO: Efficient Multi-View Pose Estimation, Tracking, and Forecasting(http://arxiv.org/abs/2309.07910)</code></li>
<li>Summary: <p>Existing volumetric methods for predicting 3D human pose estimation are
accurate, but computationally expensive and optimized for single time-step
prediction. We present TEMPO, an efficient multi-view pose estimation model
that learns a robust spatiotemporal representation, improving pose accuracy
while also tracking and forecasting human pose. We significantly reduce
computation compared to the state-of-the-art by recurrently computing
per-person 2D pose features, fusing both spatial and temporal information into
a single representation. In doing so, our model is able to use spatiotemporal
context to predict more accurate human poses without sacrificing efficiency. We
further use this representation to track human poses over time as well as
predict future poses. Finally, we demonstrate that our model is able to
generalize across datasets without scene-specific fine-tuning. TEMPO achieves
10$\%$ better MPJPE with a 33$\times$ improvement in FPS compared to TesseTrack
on the challenging CMU Panoptic Studio dataset.
</p></li>
</ul>

<h3>Title: L1-aware Multilingual Mispronunciation Detection Framework. (arXiv:2309.07719v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07719">http://arxiv.org/abs/2309.07719</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07719]] L1-aware Multilingual Mispronunciation Detection Framework(http://arxiv.org/abs/2309.07719)</code></li>
<li>Summary: <p>The phonological discrepancies between a speaker's native (L1) and the
non-native language (L2) serves as a major factor for mispronunciation. This
paper introduces a novel multilingual MDD architecture, L1-MultiMDD, enriched
with L1-aware speech representation. An end-to-end speech encoder is trained on
the input signal and its corresponding reference phoneme sequence. First, an
attention mechanism is deployed to align the input audio with the reference
phoneme sequence. Afterwards, the L1-L2-speech embedding are extracted from an
auxiliary model, pretrained in a multi-task setup identifying L1 and L2
language, and are infused with the primary network. Finally, the L1-MultiMDD is
then optimized for a unified multilingual phoneme recognition task using
connectionist temporal classification (CTC) loss for the target languages:
English, Arabic, and Mandarin. Our experiments demonstrate the effectiveness of
the proposed L1-MultiMDD framework on both seen -- L2-ARTIC, LATIC, and
AraVoiceL2v2; and unseen -- EpaDB and Speechocean762 datasets. The consistent
gains in PER, and false rejection rate (FRR) across all target languages
confirm our approach's robustness, efficacy, and generalizability.
</p></li>
</ul>

<h3>Title: The complementary roles of non-verbal cues for Robust Pronunciation Assessment. (arXiv:2309.07739v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07739">http://arxiv.org/abs/2309.07739</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07739]] The complementary roles of non-verbal cues for Robust Pronunciation Assessment(http://arxiv.org/abs/2309.07739)</code></li>
<li>Summary: <p>Research on pronunciation assessment systems focuses on utilizing phonetic
and phonological aspects of non-native (L2) speech, often neglecting the rich
layer of information hidden within the non-verbal cues. In this study, we
proposed a novel pronunciation assessment framework, IntraVerbalPA. % The
framework innovatively incorporates both fine-grained frame- and abstract
utterance-level non-verbal cues, alongside the conventional speech and phoneme
representations. Additionally, we introduce ''Goodness of phonemic-duration''
metric to effectively model duration distribution within the framework. Our
results validate the effectiveness of the proposed IntraVerbalPA framework and
its individual components, yielding performance that either matches or
outperforms existing research works.
</p></li>
</ul>

<h3>Title: Distribution Grid Line Outage Identification with Unknown Pattern and Performance Guarantee. (arXiv:2309.07157v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07157">http://arxiv.org/abs/2309.07157</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07157]] Distribution Grid Line Outage Identification with Unknown Pattern and Performance Guarantee(http://arxiv.org/abs/2309.07157)</code></li>
<li>Summary: <p>Line outage identification in distribution grids is essential for sustainable
grid operation. In this work, we propose a practical yet robust detection
approach that utilizes only readily available voltage magnitudes, eliminating
the need for costly phase angles or power flow data. Given the sensor data,
many existing detection methods based on change-point detection require prior
knowledge of outage patterns, which are unknown for real-world outage
scenarios. To remove this impractical requirement, we propose a data-driven
method to learn the parameters of the post-outage distribution through gradient
descent. However, directly using gradient descent presents feasibility issues.
To address this, we modify our approach by adding a Bregman divergence
constraint to control the trajectory of the parameter updates, which eliminates
the feasibility problems. As timely operation is the key nowadays, we prove
that the optimal parameters can be learned with convergence guarantees via
leveraging the statistical and physical properties of voltage data. We evaluate
our approach using many representative distribution grids and real load
profiles with 17 outage configurations. The results show that we can detect and
localize the outage in a timely manner with only voltage magnitudes and without
assuming a prior knowledge of outage patterns.
</p></li>
</ul>

<h3>Title: Beta quantile regression for robust estimation of uncertainty in the presence of outliers. (arXiv:2309.07374v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07374">http://arxiv.org/abs/2309.07374</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07374]] Beta quantile regression for robust estimation of uncertainty in the presence of outliers(http://arxiv.org/abs/2309.07374)</code></li>
<li>Summary: <p>Quantile Regression (QR) can be used to estimate aleatoric uncertainty in
deep neural networks and can generate prediction intervals. Quantifying
uncertainty is particularly important in critical applications such as clinical
diagnosis, where a realistic assessment of uncertainty is essential in
determining disease status and planning the appropriate treatment. The most
common application of quantile regression models is in cases where the
parametric likelihood cannot be specified. Although quantile regression is
quite robust to outlier response observations, it can be sensitive to outlier
covariate observations (features). Outlier features can compromise the
performance of deep learning regression problems such as style translation,
image reconstruction, and deep anomaly detection, potentially leading to
misleading conclusions. To address this problem, we propose a robust solution
for quantile regression that incorporates concepts from robust divergence. We
compare the performance of our proposed method with (i) least trimmed quantile
regression and (ii) robust regression based on the regularization of
case-specific parameters in a simple real dataset in the presence of outlier.
These methods have not been applied in a deep learning framework. We also
demonstrate the applicability of the proposed method by applying it to a
medical imaging translation task using diffusion models.
</p></li>
</ul>

<h3>Title: Physics-constrained robust learning of open-form PDEs from limited and noisy data. (arXiv:2309.07672v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07672">http://arxiv.org/abs/2309.07672</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07672]] Physics-constrained robust learning of open-form PDEs from limited and noisy data(http://arxiv.org/abs/2309.07672)</code></li>
<li>Summary: <p>Unveiling the underlying governing equations of nonlinear dynamic systems
remains a significant challenge, especially when encountering noisy
observations and no prior knowledge available. This study proposes R-DISCOVER,
a framework designed to robustly uncover open-form partial differential
equations (PDEs) from limited and noisy data. The framework operates through
two alternating update processes: discovering and embedding. The discovering
phase employs symbolic representation and a reinforcement learning (RL)-guided
hybrid PDE generator to efficiently produce diverse open-form PDEs with tree
structures. A neural network-based predictive model fits the system response
and serves as the reward evaluator for the generated PDEs. PDEs with superior
fits are utilized to iteratively optimize the generator via the RL method and
the best-performing PDE is selected by a parameter-free stability metric. The
embedding phase integrates the initially identified PDE from the discovering
process as a physical constraint into the predictive model for robust training.
The traversal of PDE trees automates the construction of the computational
graph and the embedding process without human intervention. Numerical
experiments demonstrate our framework's capability to uncover governing
equations from nonlinear dynamic systems with limited and highly noisy data and
outperform other physics-informed neural network-based discovery methods. This
work opens new potential for exploring real-world systems with limited
understanding.
</p></li>
</ul>

<h3>Title: Understanding Vector-Valued Neural Networks and Their Relationship with Real and Hypercomplex-Valued Neural Networks. (arXiv:2309.07716v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07716">http://arxiv.org/abs/2309.07716</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07716]] Understanding Vector-Valued Neural Networks and Their Relationship with Real and Hypercomplex-Valued Neural Networks(http://arxiv.org/abs/2309.07716)</code></li>
<li>Summary: <p>Despite the many successful applications of deep learning models for
multidimensional signal and image processing, most traditional neural networks
process data represented by (multidimensional) arrays of real numbers. The
intercorrelation between feature channels is usually expected to be learned
from the training data, requiring numerous parameters and careful training. In
contrast, vector-valued neural networks are conceived to process arrays of
vectors and naturally consider the intercorrelation between feature channels.
Consequently, they usually have fewer parameters and often undergo more robust
training than traditional neural networks. This paper aims to present a broad
framework for vector-valued neural networks, referred to as V-nets. In this
context, hypercomplex-valued neural networks are regarded as vector-valued
models with additional algebraic properties. Furthermore, this paper explains
the relationship between vector-valued and traditional neural networks.
Precisely, a vector-valued neural network can be obtained by placing
restrictions on a real-valued model to consider the intercorrelation between
feature channels. Finally, we show how V-nets, including hypercomplex-valued
neural networks, can be implemented in current deep-learning libraries as
real-valued networks.
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Nucleus-aware Self-supervised Pretraining Using Unpaired Image-to-image Translation for Histopathology Images. (arXiv:2309.07394v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07394">http://arxiv.org/abs/2309.07394</a></li>
<li>Code URL: https://github.com/zhiyuns/UNITPathSSL</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07394]] Nucleus-aware Self-supervised Pretraining Using Unpaired Image-to-image Translation for Histopathology Images(http://arxiv.org/abs/2309.07394)</code></li>
<li>Summary: <p>Self-supervised pretraining attempts to enhance model performance by
obtaining effective features from unlabeled data, and has demonstrated its
effectiveness in the field of histopathology images. Despite its success, few
works concentrate on the extraction of nucleus-level information, which is
essential for pathologic analysis. In this work, we propose a novel
nucleus-aware self-supervised pretraining framework for histopathology images.
The framework aims to capture the nuclear morphology and distribution
information through unpaired image-to-image translation between histopathology
images and pseudo mask images. The generation process is modulated by both
conditional and stochastic style representations, ensuring the reality and
diversity of the generated histopathology images for pretraining. Further, an
instance segmentation guided strategy is employed to capture instance-level
information. The experiments on 7 datasets show that the proposed pretraining
method outperforms supervised ones on Kather classification, multiple instance
learning, and 5 dense-prediction tasks with the transfer learning protocol, and
yields superior results than other self-supervised approaches on 8
semi-supervised tasks. Our project is publicly available at
https://github.com/zhiyuns/UNITPathSSL.
</p></li>
</ul>

<h3>Title: A Multi-scale Generalized Shrinkage Threshold Network for Image Blind Deblurring in Remote Sensing. (arXiv:2309.07524v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07524">http://arxiv.org/abs/2309.07524</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07524]] A Multi-scale Generalized Shrinkage Threshold Network for Image Blind Deblurring in Remote Sensing(http://arxiv.org/abs/2309.07524)</code></li>
<li>Summary: <p>Remote sensing images are essential for many earth science applications, but
their quality can be degraded due to limitations in sensor technology and
complex imaging environments. To address this, various remote sensing image
deblurring methods have been developed to restore sharp, high-quality images
from degraded observational data. However, most traditional model-based
deblurring methods usually require predefined hand-craft prior assumptions,
which are difficult to handle in complex applications, and most deep
learning-based deblurring methods are designed as a black box, lacking
transparency and interpretability. In this work, we propose a novel blind
deblurring learning framework based on alternating iterations of shrinkage
thresholds, alternately updating blurring kernels and images, with the
theoretical foundation of network design. Additionally, we propose a learnable
blur kernel proximal mapping module to improve the blur kernel evaluation in
the kernel domain. Then, we proposed a deep proximal mapping module in the
image domain, which combines a generalized shrinkage threshold operator and a
multi-scale prior feature extraction block. This module also introduces an
attention mechanism to adaptively adjust the prior importance, thus avoiding
the drawbacks of hand-crafted image prior terms. Thus, a novel multi-scale
generalized shrinkage threshold network (MGSTNet) is designed to specifically
focus on learning deep geometric prior features to enhance image restoration.
Experiments demonstrate the superiority of our MGSTNet framework on remote
sensing image datasets compared to existing deblurring methods.
</p></li>
</ul>

<h3>Title: Co-Salient Object Detection with Semantic-Level Consensus Extraction and Dispersion. (arXiv:2309.07753v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07753">http://arxiv.org/abs/2309.07753</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07753]] Co-Salient Object Detection with Semantic-Level Consensus Extraction and Dispersion(http://arxiv.org/abs/2309.07753)</code></li>
<li>Summary: <p>Given a group of images, co-salient object detection (CoSOD) aims to
highlight the common salient object in each image. There are two factors
closely related to the success of this task, namely consensus extraction, and
the dispersion of consensus to each image. Most previous works represent the
group consensus using local features, while we instead utilize a hierarchical
Transformer module for extracting semantic-level consensus. Therefore, it can
obtain a more comprehensive representation of the common object category, and
exclude interference from other objects that share local similarities with the
target object. In addition, we propose a Transformer-based dispersion module
that takes into account the variation of the co-salient object in different
scenes. It distributes the consensus to the image feature maps in an
image-specific way while making full use of interactions within the group.
These two modules are integrated with a ViT encoder and an FPN-like decoder to
form an end-to-end trainable network, without additional branch and auxiliary
loss. The proposed method is evaluated on three commonly used CoSOD datasets
and achieves state-of-the-art performance.
</p></li>
</ul>

<h3>Title: Large-scale Weakly Supervised Learning for Road Extraction from Satellite Imagery. (arXiv:2309.07823v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07823">http://arxiv.org/abs/2309.07823</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07823]] Large-scale Weakly Supervised Learning for Road Extraction from Satellite Imagery(http://arxiv.org/abs/2309.07823)</code></li>
<li>Summary: <p>Automatic road extraction from satellite imagery using deep learning is a
viable alternative to traditional manual mapping. Therefore it has received
considerable attention recently. However, most of the existing methods are
supervised and require pixel-level labeling, which is tedious and error-prone.
To make matters worse, the earth has a diverse range of terrain, vegetation,
and man-made objects. It is well known that models trained in one area
generalize poorly to other areas. Various shooting conditions such as light and
angel, as well as different image processing techniques further complicate the
issue. It is impractical to develop training data to cover all image styles.
This paper proposes to leverage OpenStreetMap road data as weak labels and
large scale satellite imagery to pre-train semantic segmentation models. Our
extensive experimental results show that the prediction accuracy increases with
the amount of the weakly labeled data, as well as the road density in the areas
chosen for training. Using as much as 100 times more data than the widely used
DeepGlobe road dataset, our model with the D-LinkNet architecture and the
ResNet-50 backbone exceeds the top performer of the current DeepGlobe
leaderboard. Furthermore, due to large-scale pre-training, our model
generalizes much better than those trained with only the curated datasets,
implying great application potential.
</p></li>
</ul>

<h3>Title: A Novel Local-Global Feature Fusion Framework for Body-weight Exercise Recognition with Pressure Mapping Sensors. (arXiv:2309.07888v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07888">http://arxiv.org/abs/2309.07888</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07888]] A Novel Local-Global Feature Fusion Framework for Body-weight Exercise Recognition with Pressure Mapping Sensors(http://arxiv.org/abs/2309.07888)</code></li>
<li>Summary: <p>We present a novel local-global feature fusion framework for body-weight
exercise recognition with floor-based dynamic pressure maps. One step further
from the existing studies using deep neural networks mainly focusing on global
feature extraction, the proposed framework aims to combine local and global
features using image processing techniques and the YOLO object detection to
localize pressure profiles from different body parts and consider physical
constraints. The proposed local feature extraction method generates two sets of
high-level local features consisting of cropped pressure mapping and numerical
features such as angular orientation, location on the mat, and pressure area.
In addition, we adopt a knowledge distillation for regularization to preserve
the knowledge of the global feature extraction and improve the performance of
the exercise recognition. Our experimental results demonstrate a notable 11
percent improvement in F1 score for exercise recognition while preserving
label-specific features.
</p></li>
</ul>

<h3>Title: Less is More for Long Document Summary Evaluation by LLMs. (arXiv:2309.07382v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07382">http://arxiv.org/abs/2309.07382</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07382]] Less is More for Long Document Summary Evaluation by LLMs(http://arxiv.org/abs/2309.07382)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have shown promising performance in summary
evaluation tasks, yet they face challenges such as high computational costs and
the Lost-in-the-Middle problem where important information in the middle of
long documents is often overlooked. To address these issues, this paper
introduces a novel approach, Extract-then-Evaluate, which involves extracting
key sentences from a long source document and then evaluating the summary by
prompting LLMs. The results reveal that the proposed method not only
significantly reduces evaluation costs but also exhibits a higher correlation
with human evaluations. Furthermore, we provide practical recommendations for
optimal document length and sentence extraction methods, contributing to the
development of cost-effective yet more accurate methods for LLM-based text
generation evaluation.
</p></li>
</ul>

<h3>Title: Detecting Misinformation with LLM-Predicted Credibility Signals and Weak Supervision. (arXiv:2309.07601v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07601">http://arxiv.org/abs/2309.07601</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07601]] Detecting Misinformation with LLM-Predicted Credibility Signals and Weak Supervision(http://arxiv.org/abs/2309.07601)</code></li>
<li>Summary: <p>Credibility signals represent a wide range of heuristics that are typically
used by journalists and fact-checkers to assess the veracity of online content.
Automating the task of credibility signal extraction, however, is very
challenging as it requires high-accuracy signal-specific extractors to be
trained, while there are currently no sufficiently large datasets annotated
with all credibility signals. This paper investigates whether large language
models (LLMs) can be prompted effectively with a set of 18 credibility signals
to produce weak labels for each signal. We then aggregate these potentially
noisy labels using weak supervision in order to predict content veracity. We
demonstrate that our approach, which combines zero-shot LLM credibility signal
labeling and weak supervision, outperforms state-of-the-art classifiers on two
misinformation datasets without using any ground-truth labels for training. We
also analyse the contribution of the individual credibility signals towards
predicting content veracity, which provides new valuable insights into their
role in misinformation detection.
</p></li>
</ul>

<h3>Title: Attention-based Dynamic Graph Convolutional Recurrent Neural Network for Traffic Flow Prediction in Highway Transportation. (arXiv:2309.07196v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07196">http://arxiv.org/abs/2309.07196</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07196]] Attention-based Dynamic Graph Convolutional Recurrent Neural Network for Traffic Flow Prediction in Highway Transportation(http://arxiv.org/abs/2309.07196)</code></li>
<li>Summary: <p>As one of the important tools for spatial feature extraction, graph
convolution has been applied in a wide range of fields such as traffic flow
prediction. However, current popular works of graph convolution cannot
guarantee spatio-temporal consistency in a long period. The ignorance of
correlational dynamics, convolutional locality and temporal comprehensiveness
would limit predictive accuracy. In this paper, a novel Attention-based Dynamic
Graph Convolutional Recurrent Neural Network (ADGCRNN) is proposed to improve
traffic flow prediction in highway transportation. Three temporal resolutions
of data sequence are effectively integrated by self-attention to extract
characteristics; multi-dynamic graphs and their weights are dynamically created
to compliantly combine the varying characteristics; a dedicated gated kernel
emphasizing highly relative nodes is introduced on these complete graphs to
reduce overfitting for graph convolution operations. Experiments on two public
datasets show our work better than state-of-the-art baselines, and case studies
of a real Web system prove practical benefit in highway transportation.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Client-side Gradient Inversion Against Federated Learning from Poisoning. (arXiv:2309.07415v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07415">http://arxiv.org/abs/2309.07415</a></li>
<li>Code URL: https://github.com/clientsidegia/cgi</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07415]] Client-side Gradient Inversion Against Federated Learning from Poisoning(http://arxiv.org/abs/2309.07415)</code></li>
<li>Summary: <p>Federated Learning (FL) enables distributed participants (e.g., mobile
devices) to train a global model without sharing data directly to a central
server. Recent studies have revealed that FL is vulnerable to gradient
inversion attack (GIA), which aims to reconstruct the original training samples
and poses high risk against the privacy of clients in FL. However, most
existing GIAs necessitate control over the server and rely on strong prior
knowledge including batch normalization and data distribution information. In
this work, we propose Client-side poisoning Gradient Inversion (CGI), which is
a novel attack method that can be launched from clients. For the first time, we
show the feasibility of a client-side adversary with limited knowledge being
able to recover the training samples from the aggregated global model. We take
a distinct approach in which the adversary utilizes a malicious model that
amplifies the loss of a specific targeted class of interest. When honest
clients employ the poisoned global model, the gradients of samples belonging to
the targeted class are magnified, making them the dominant factor in the
aggregated update. This enables the adversary to effectively reconstruct the
private input belonging to other clients using the aggregated update. In
addition, our CGI also features its ability to remain stealthy against
Byzantine-robust aggregation rules (AGRs). By optimizing malicious updates and
blending benign updates with a malicious replacement vector, our method remains
undetected by these defense mechanisms. To evaluate the performance of CGI, we
conduct experiments on various benchmark datasets, considering representative
Byzantine-robust AGRs, and exploring diverse FL settings with different levels
of adversary knowledge about the data. Our results demonstrate that CGI
consistently and successfully extracts training input in all tested scenarios.
</p></li>
</ul>

<h3>Title: Communication Efficient Private Federated Learning Using Dithering. (arXiv:2309.07809v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07809">http://arxiv.org/abs/2309.07809</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07809]] Communication Efficient Private Federated Learning Using Dithering(http://arxiv.org/abs/2309.07809)</code></li>
<li>Summary: <p>The task of preserving privacy while ensuring efficient communication is a
fundamental challenge in federated learning. In this work, we tackle this
challenge in the trusted aggregator model, and propose a solution that achieves
both objectives simultaneously. We show that employing a quantization scheme
based on subtractive dithering at the clients can effectively replicate the
normal noise addition process at the aggregator. This implies that we can
guarantee the same level of differential privacy against other clients while
substantially reducing the amount of communication required, as opposed to
transmitting full precision gradients and using central noise addition. We also
experimentally demonstrate that the accuracy of our proposed approach matches
that of the full precision gradient method.
</p></li>
</ul>

<h3>Title: Learning From Drift: Federated Learning on Non-IID Data via Drift Regularization. (arXiv:2309.07189v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07189">http://arxiv.org/abs/2309.07189</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07189]] Learning From Drift: Federated Learning on Non-IID Data via Drift Regularization(http://arxiv.org/abs/2309.07189)</code></li>
<li>Summary: <p>Federated learning algorithms perform reasonably well on independent and
identically distributed (IID) data. They, on the other hand, suffer greatly
from heterogeneous environments, i.e., Non-IID data. Despite the fact that many
research projects have been done to address this issue, recent findings
indicate that they are still sub-optimal when compared to training on IID data.
In this work, we carefully analyze the existing methods in heterogeneous
environments. Interestingly, we find that regularizing the classifier's outputs
is quite effective in preventing performance degradation on Non-IID data.
Motivated by this, we propose Learning from Drift (LfD), a novel method for
effectively training the model in heterogeneous settings. Our scheme
encapsulates two key components: drift estimation and drift regularization.
Specifically, LfD first estimates how different the local model is from the
global model (i.e., drift). The local model is then regularized such that it
does not fall in the direction of the estimated drift. In the experiment, we
evaluate each method through the lens of the five aspects of federated
learning, i.e., Generalization, Heterogeneity, Scalability, Forgetting, and
Efficiency. Comprehensive evaluation results clearly support the superiority of
LfD in federated learning with Non-IID data.
</p></li>
</ul>

<h3>Title: Federated Dataset Dictionary Learning for Multi-Source Domain Adaptation. (arXiv:2309.07670v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07670">http://arxiv.org/abs/2309.07670</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07670]] Federated Dataset Dictionary Learning for Multi-Source Domain Adaptation(http://arxiv.org/abs/2309.07670)</code></li>
<li>Summary: <p>In this article, we propose an approach for federated domain adaptation, a
setting where distributional shift exists among clients and some have unlabeled
data. The proposed framework, FedDaDiL, tackles the resulting challenge through
dictionary learning of empirical distributions. In our setting, clients'
distributions represent particular domains, and FedDaDiL collectively trains a
federated dictionary of empirical distributions. In particular, we build upon
the Dataset Dictionary Learning framework by designing collaborative
communication protocols and aggregation operations. The chosen protocols keep
clients' data private, thus enhancing overall privacy compared to its
centralized counterpart. We empirically demonstrate that our approach
successfully generates labeled data on the target domain with extensive
experiments on (i) Caltech-Office, (ii) TEP, and (iii) CWRU benchmarks.
Furthermore, we compare our method to its centralized counterpart and other
benchmarks in federated domain adaptation.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: For A More Comprehensive Evaluation of 6DoF Object Pose Tracking. (arXiv:2309.07796v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07796">http://arxiv.org/abs/2309.07796</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07796]] For A More Comprehensive Evaluation of 6DoF Object Pose Tracking(http://arxiv.org/abs/2309.07796)</code></li>
<li>Summary: <p>Previous evaluations on 6DoF object pose tracking have presented obvious
limitations along with the development of this area. In particular, the
evaluation protocols are not unified for different methods, the widely-used
YCBV dataset contains significant annotation error, and the error metrics also
may be biased. As a result, it is hard to fairly compare the methods, which has
became a big obstacle for developing new algorithms. In this paper we
contribute a unified benchmark to address the above problems. For more accurate
annotation of YCBV, we propose a multi-view multi-object global pose refinement
method, which can jointly refine the poses of all objects and view cameras,
resulting in sub-pixel sub-millimeter alignment errors. The limitations of
previous scoring methods and error metrics are analyzed, based on which we
introduce our improved evaluation methods. The unified benchmark takes both
YCBV and BCOT as base datasets, which are shown to be complementary in scene
categories. In experiments, we validate the precision and reliability of the
proposed global pose refinement method with a realistic semi-synthesized
dataset particularly for YCBV, and then present the benchmark results unifying
learning&amp;non-learning and RGB&amp;RGBD methods, with some finds not discovered in
previous studies.
</p></li>
</ul>

<h3>Title: Optimal and Fair Encouragement Policy Evaluation and Learning. (arXiv:2309.07176v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07176">http://arxiv.org/abs/2309.07176</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07176]] Optimal and Fair Encouragement Policy Evaluation and Learning(http://arxiv.org/abs/2309.07176)</code></li>
<li>Summary: <p>In consequential domains, it is often impossible to compel individuals to
take treatment, so that optimal policy rules are merely suggestions in the
presence of human non-adherence to treatment recommendations. In these same
domains, there may be heterogeneity both in who responds in taking-up
treatment, and heterogeneity in treatment efficacy. While optimal treatment
rules can maximize causal outcomes across the population, access parity
constraints or other fairness considerations can be relevant in the case of
encouragement. For example, in social services, a persistent puzzle is the gap
in take-up of beneficial services among those who may benefit from them the
most. When in addition the decision-maker has distributional preferences over
both access and average outcomes, the optimal decision rule changes. We study
causal identification, statistical variance-reduced estimation, and robust
estimation of optimal treatment rules, including under potential violations of
positivity. We consider fairness constraints such as demographic parity in
treatment take-up, and other constraints, via constrained optimization. Our
framework can be extended to handle algorithmic recommendations under an
often-reasonable covariate-conditional exclusion restriction, using our
robustness checks for lack of positivity in the recommendation. We develop a
two-stage algorithm for solving over parametrized policy classes under general
constraints to obtain variance-sensitive regret bounds. We illustrate the
methods in two case studies based on data from randomized encouragement to
enroll in insurance and from pretrial supervised release with electronic
monitoring.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs. (arXiv:2309.07311v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07311">http://arxiv.org/abs/2309.07311</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07311]] Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs(http://arxiv.org/abs/2309.07311)</code></li>
<li>Summary: <p>Most interpretability research in NLP focuses on understanding the behavior
and features of a fully trained model. However, certain insights into model
behavior may only be accessible by observing the trajectory of the training
process. In this paper, we present a case study of syntax acquisition in masked
language models (MLMs). Our findings demonstrate how analyzing the evolution of
interpretable artifacts throughout training deepens our understanding of
emergent behavior. In particular, we study Syntactic Attention Structure (SAS),
a naturally emerging property of MLMs wherein specific Transformer heads tend
to focus on specific syntactic relations. We identify a brief window in
training when models abruptly acquire SAS and find that this window is
concurrent with a steep drop in loss. Moreover, SAS precipitates the subsequent
acquisition of linguistic capabilities. We then examine the causal role of SAS
by introducing a regularizer to manipulate SAS during training, and demonstrate
that SAS is necessary for the development of grammatical capabilities. We
further find that SAS competes with other beneficial traits and capabilities
during training, and that briefly suppressing SAS can improve model quality.
These findings reveal a real-world example of the relationship between
disadvantageous simplicity bias and interpretable breakthrough training
dynamics.
</p></li>
</ul>

<h3>Title: Causal Entropy and Information Gain for Measuring Causal Control. (arXiv:2309.07703v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07703">http://arxiv.org/abs/2309.07703</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07703]] Causal Entropy and Information Gain for Measuring Causal Control(http://arxiv.org/abs/2309.07703)</code></li>
<li>Summary: <p>Artificial intelligence models and methods commonly lack causal
interpretability. Despite the advancements in interpretable machine learning
(IML) methods, they frequently assign importance to features which lack causal
influence on the outcome variable. Selecting causally relevant features among
those identified as relevant by these methods, or even before model training,
would offer a solution. Feature selection methods utilizing information
theoretical quantities have been successful in identifying statistically
relevant features. However, the information theoretical quantities they are
based on do not incorporate causality, rendering them unsuitable for such
scenarios. To address this challenge, this article proposes information
theoretical quantities that incorporate the causal structure of the system,
which can be used to evaluate causal importance of features for some given
outcome variable. Specifically, we introduce causal versions of entropy and
mutual information, termed causal entropy and causal information gain, which
are designed to assess how much control a feature provides over the outcome
variable. These newly defined quantities capture changes in the entropy of a
variable resulting from interventions on other variables. Fundamental results
connecting these quantities to the existence of causal effects are derived. The
use of causal information gain in feature selection is demonstrated,
highlighting its superiority over standard mutual information in revealing
which features provide control over a chosen outcome variable. Our
investigation paves the way for the development of methods with improved
interpretability in domains involving causation.
</p></li>
</ul>

<h3>Title: Interpretability is in the Mind of the Beholder: A Causal Framework for Human-interpretable Representation Learning. (arXiv:2309.07742v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07742">http://arxiv.org/abs/2309.07742</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07742]] Interpretability is in the Mind of the Beholder: A Causal Framework for Human-interpretable Representation Learning(http://arxiv.org/abs/2309.07742)</code></li>
<li>Summary: <p>Focus in Explainable AI is shifting from explanations defined in terms of
low-level elements, such as input features, to explanations encoded in terms of
interpretable concepts learned from data. How to reliably acquire such concepts
is, however, still fundamentally unclear. An agreed-upon notion of concept
interpretability is missing, with the result that concepts used by both
post-hoc explainers and concept-based neural networks are acquired through a
variety of mutually incompatible strategies. Critically, most of these neglect
the human side of the problem: a representation is understandable only insofar
as it can be understood by the human at the receiving end. The key challenge in
Human-interpretable Representation Learning (HRL) is how to model and
operationalize this human element. In this work, we propose a mathematical
framework for acquiring interpretable representations suitable for both
post-hoc explainers and concept-based neural networks. Our formalization of HRL
builds on recent advances in causal representation learning and explicitly
models a human stakeholder as an external observer. This allows us to derive a
principled notion of alignment between the machine representation and the
vocabulary of concepts understood by the human. In doing so, we link alignment
and interpretability through a simple and intuitive name transfer game, and
clarify the relationship between alignment and a well-known property of
representations, namely disentanglment. We also show that alignment is linked
to the issue of undesirable correlations among concepts, also known as concept
leakage, and to content-style separation, all through a general
information-theoretic reformulation of these properties. Our conceptualization
aims to bridge the gap between the human and algorithmic sides of
interpretability and establish a stepping stone for new research on
human-interpretable representations.
</p></li>
</ul>

<h2>explainability</h2>
<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Mitigate Replication and Copying in Diffusion Models with Generalized Caption and Dual Fusion Enhancement. (arXiv:2309.07254v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07254">http://arxiv.org/abs/2309.07254</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07254]] Mitigate Replication and Copying in Diffusion Models with Generalized Caption and Dual Fusion Enhancement(http://arxiv.org/abs/2309.07254)</code></li>
<li>Summary: <p>While diffusion models demonstrate a remarkable capability for generating
high-quality images, their tendency to `replicate' training data raises privacy
concerns. Although recent research suggests that this replication may stem from
the insufficient generalization of training data captions and duplication of
training images, effective mitigation strategies remain elusive. To address
this gap, our paper first introduces a generality score that measures the
caption generality and employ large language model (LLM) to generalize training
captions. Subsequently, we leverage generalized captions and propose a novel
dual fusion enhancement approach to mitigate the replication of diffusion
models. Our empirical results demonstrate that our proposed methods can
significantly reduce replication by 43.5% compared to the original diffusion
model while maintaining the diversity and quality of generations.
</p></li>
</ul>

<h3>Title: Unbiased Face Synthesis With Diffusion Models: Are We There Yet?. (arXiv:2309.07277v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07277">http://arxiv.org/abs/2309.07277</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07277]] Unbiased Face Synthesis With Diffusion Models: Are We There Yet?(http://arxiv.org/abs/2309.07277)</code></li>
<li>Summary: <p>Text-to-image diffusion models have achieved widespread popularity due to
their unprecedented image generation capability. In particular, their ability
to synthesize and modify human faces has spurred research into using generated
face images in both training data augmentation and model performance
assessments. In this paper, we study the efficacy and shortcomings of
generative models in the context of face generation. Utilizing a combination of
qualitative and quantitative measures, including embedding-based metrics and
user studies, we present a framework to audit the characteristics of generated
faces conditioned on a set of social attributes. We applied our framework on
faces generated through state-of-the-art text-to-image diffusion models. We
identify several limitations of face image generation that include faithfulness
to the text prompt, demographic disparities, and distributional shifts.
Furthermore, we present an analytical model that provides insights into how
training data selection contributes to the performance of generative models.
</p></li>
</ul>

<h3>Title: Masked Diffusion with Task-awareness for Procedure Planning in Instructional Videos. (arXiv:2309.07409v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07409">http://arxiv.org/abs/2309.07409</a></li>
<li>Code URL: https://github.com/ffzzy840304/masked-pdpp</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07409]] Masked Diffusion with Task-awareness for Procedure Planning in Instructional Videos(http://arxiv.org/abs/2309.07409)</code></li>
<li>Summary: <p>A key challenge with procedure planning in instructional videos lies in how
to handle a large decision space consisting of a multitude of action types that
belong to various tasks. To understand real-world video content, an AI agent
must proficiently discern these action types (e.g., pour milk, pour water, open
lid, close lid, etc.) based on brief visual observation. Moreover, it must
adeptly capture the intricate semantic relation of the action types and task
goals, along with the variable action sequences. Recently, notable progress has
been made via the integration of diffusion models and visual representation
learning to address the challenge. However, existing models employ rudimentary
mechanisms to utilize task information to manage the decision space. To
overcome this limitation, we introduce a simple yet effective enhancement - a
masked diffusion model. The introduced mask acts akin to a task-oriented
attention filter, enabling the diffusion/denoising process to concentrate on a
subset of action types. Furthermore, to bolster the accuracy of task
classification, we harness more potent visual representation learning
techniques. In particular, we learn a joint visual-text embedding, where a text
embedding is generated by prompting a pre-trained vision-language model to
focus on human actions. We evaluate the method on three public datasets and
achieve state-of-the-art performance on multiple metrics. Code is available at
https://github.com/ffzzy840304/Masked-PDPP.
</p></li>
</ul>

<h3>Title: DiffTalker: Co-driven audio-image diffusion for talking faces via intermediate landmarks. (arXiv:2309.07509v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07509">http://arxiv.org/abs/2309.07509</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07509]] DiffTalker: Co-driven audio-image diffusion for talking faces via intermediate landmarks(http://arxiv.org/abs/2309.07509)</code></li>
<li>Summary: <p>Generating realistic talking faces is a complex and widely discussed task
with numerous applications. In this paper, we present DiffTalker, a novel model
designed to generate lifelike talking faces through audio and landmark
co-driving. DiffTalker addresses the challenges associated with directly
applying diffusion models to audio control, which are traditionally trained on
text-image pairs. DiffTalker consists of two agent networks: a
transformer-based landmarks completion network for geometric accuracy and a
diffusion-based face generation network for texture details. Landmarks play a
pivotal role in establishing a seamless connection between the audio and image
domains, facilitating the incorporation of knowledge from pre-trained diffusion
models. This innovative approach efficiently produces articulate-speaking
faces. Experimental results showcase DiffTalker's superior performance in
producing clear and geometrically accurate talking faces, all without the need
for additional alignment between audio and image features.
</p></li>
</ul>

<h3>Title: Boosting Unsupervised Contrastive Learning Using Diffusion-Based Data Augmentation From Scratch. (arXiv:2309.07909v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07909">http://arxiv.org/abs/2309.07909</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07909]] Boosting Unsupervised Contrastive Learning Using Diffusion-Based Data Augmentation From Scratch(http://arxiv.org/abs/2309.07909)</code></li>
<li>Summary: <p>Unsupervised contrastive learning methods have recently seen significant
improvements, particularly through data augmentation strategies that aim to
produce robust and generalizable representations. However, prevailing data
augmentation methods, whether hand designed or based on foundation models, tend
to rely heavily on prior knowledge or external data. This dependence often
compromises their effectiveness and efficiency. Furthermore, the applicability
of most existing data augmentation strategies is limited when transitioning to
other research domains, especially science-related data. This limitation stems
from the paucity of prior knowledge and labeled data available in these
domains. To address these challenges, we introduce DiffAug-a novel and
efficient Diffusion-based data Augmentation technique. DiffAug aims to ensure
that the augmented and original data share a smoothed latent space, which is
achieved through diffusion steps. Uniquely, unlike traditional methods, DiffAug
first mines sufficient prior semantic knowledge about the neighborhood. This
provides a constraint to guide the diffusion steps, eliminating the need for
labels, external data/models, or prior knowledge. Designed as an
architecture-agnostic framework, DiffAug provides consistent improvements.
Specifically, it improves image classification and clustering accuracy by
1.6%~4.5%. When applied to biological data, DiffAug improves performance by up
to 10.1%, with an average improvement of 5.8%. DiffAug shows good performance
in both vision and biological domains.
</p></li>
</ul>

<h3>Title: Large-Vocabulary 3D Diffusion Model with Transformer. (arXiv:2309.07920v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07920">http://arxiv.org/abs/2309.07920</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07920]] Large-Vocabulary 3D Diffusion Model with Transformer(http://arxiv.org/abs/2309.07920)</code></li>
<li>Summary: <p>Creating diverse and high-quality 3D assets with an automatic generative
model is highly desirable. Despite extensive efforts on 3D generation, most
existing works focus on the generation of a single category or a few
categories. In this paper, we introduce a diffusion-based feed-forward
framework for synthesizing massive categories of real-world 3D objects with a
single generative model. Notably, there are three major challenges for this
large-vocabulary 3D generation: a) the need for expressive yet efficient 3D
representation; b) large diversity in geometry and texture across categories;
c) complexity in the appearances of real-world objects. To this end, we propose
a novel triplane-based 3D-aware Diffusion model with TransFormer, DiffTF, for
handling challenges via three aspects. 1) Considering efficiency and
robustness, we adopt a revised triplane representation and improve the fitting
speed and accuracy. 2) To handle the drastic variations in geometry and
texture, we regard the features of all 3D objects as a combination of
generalized 3D knowledge and specialized 3D features. To extract generalized 3D
knowledge from diverse categories, we propose a novel 3D-aware transformer with
shared cross-plane attention. It learns the cross-plane relations across
different planes and aggregates the generalized 3D knowledge with specialized
3D features. 3) In addition, we devise the 3D-aware encoder/decoder to enhance
the generalized 3D knowledge in the encoded triplanes for handling categories
with complex appearances. Extensive experiments on ShapeNet and OmniObject3D
(over 200 diverse real-world categories) convincingly demonstrate that a single
DiffTF model achieves state-of-the-art large-vocabulary 3D object generation
performance with large diversity, rich semantics, and high quality.
</p></li>
</ul>

<h3>Title: Beta Diffusion. (arXiv:2309.07867v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07867">http://arxiv.org/abs/2309.07867</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07867]] Beta Diffusion(http://arxiv.org/abs/2309.07867)</code></li>
<li>Summary: <p>We introduce beta diffusion, a novel generative modeling method that
integrates demasking and denoising to generate data within bounded ranges.
Using scaled and shifted beta distributions, beta diffusion utilizes
multiplicative transitions over time to create both forward and reverse
diffusion processes, maintaining beta distributions in both the forward
marginals and the reverse conditionals, given the data at any point in time.
Unlike traditional diffusion-based generative models relying on additive
Gaussian noise and reweighted evidence lower bounds (ELBOs), beta diffusion is
multiplicative and optimized with KL-divergence upper bounds (KLUBs) derived
from the convexity of the KL divergence. We demonstrate that the proposed KLUBs
are more effective for optimizing beta diffusion compared to negative ELBOs,
which can also be derived as the KLUBs of the same KL divergence with its two
arguments swapped. The loss function of beta diffusion, expressed in terms of
Bregman divergence, further supports the efficacy of KLUBs for optimization.
Experimental results on both synthetic data and natural images demonstrate the
unique capabilities of beta diffusion in generative modeling of range-bounded
data and validate the effectiveness of KLUBs in optimizing diffusion models,
thereby making them valuable additions to the family of diffusion-based
generative models and the optimization techniques used to train them.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Automated Assessment of Critical View of Safety in Laparoscopic Cholecystectomy. (arXiv:2309.07330v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07330">http://arxiv.org/abs/2309.07330</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07330]] Automated Assessment of Critical View of Safety in Laparoscopic Cholecystectomy(http://arxiv.org/abs/2309.07330)</code></li>
<li>Summary: <p>Cholecystectomy (gallbladder removal) is one of the most common procedures in
the US, with more than 1.2M procedures annually. Compared with classical open
cholecystectomy, laparoscopic cholecystectomy (LC) is associated with
significantly shorter recovery period, and hence is the preferred method.
However, LC is also associated with an increase in bile duct injuries (BDIs),
resulting in significant morbidity and mortality. The primary cause of BDIs
from LCs is misidentification of the cystic duct with the bile duct. Critical
view of safety (CVS) is the most effective of safety protocols, which is said
to be achieved during the surgery if certain criteria are met. However, due to
suboptimal understanding and implementation of CVS, the BDI rates have remained
stable over the last three decades. In this paper, we develop deep-learning
techniques to automate the assessment of CVS in LCs. An innovative aspect of
our research is on developing specialized learning techniques by incorporating
domain knowledge to compensate for the limited training data available in
practice. In particular, our CVS assessment process involves a fusion of two
segmentation maps followed by an estimation of a certain region of interest
based on anatomical structures close to the gallbladder, and then finally
determination of each of the three CVS criteria via rule-based assessment of
structural information. We achieved a gain of over 11.8% in mIoU on relevant
classes with our two-stream semantic segmentation approach when compared to a
single-model baseline, and 1.84% in mIoU with our proposed Sobel loss function
when compared to a Transformer-based baseline model. For CVS criteria, we
achieved up to 16% improvement and, for the overall CVS assessment, we achieved
5% improvement in balanced accuracy compared to DeepCVS under the same
experiment settings.
</p></li>
</ul>

<h3>Title: HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis. (arXiv:2309.07400v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07400">http://arxiv.org/abs/2309.07400</a></li>
<li>Code URL: https://github.com/hku-medai/higt</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07400]] HIGT: Hierarchical Interaction Graph-Transformer for Whole Slide Image Analysis(http://arxiv.org/abs/2309.07400)</code></li>
<li>Summary: <p>In computation pathology, the pyramid structure of gigapixel Whole Slide
Images (WSIs) has recently been studied for capturing various information from
individual cell interactions to tissue microenvironments. This hierarchical
structure is believed to be beneficial for cancer diagnosis and prognosis
tasks. However, most previous hierarchical WSI analysis works (1) only
characterize local or global correlations within the WSI pyramids and (2) use
only unidirectional interaction between different resolutions, leading to an
incomplete picture of WSI pyramids. To this end, this paper presents a novel
Hierarchical Interaction Graph-Transformer (i.e., HIGT) for WSI analysis. With
Graph Neural Network and Transformer as the building commons, HIGT can learn
both short-range local information and long-range global representation of the
WSI pyramids. Considering that the information from different resolutions is
complementary and can benefit each other during the learning process, we
further design a novel Bidirectional Interaction block to establish
communication between different levels within the WSI pyramids. Finally, we
aggregate both coarse-grained and fine-grained features learned from different
levels together for slide-level prediction. We evaluate our methods on two
public WSI datasets from TCGA projects, i.e., kidney carcinoma (KICA) and
esophageal carcinoma (ESCA). Experimental results show that our HIGT
outperforms both hierarchical and non-hierarchical state-of-the-art methods on
both tumor subtyping and staging tasks.
</p></li>
</ul>

<h3>Title: Research on self-cross transformer model of point cloud change detecter. (arXiv:2309.07444v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07444">http://arxiv.org/abs/2309.07444</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07444]] Research on self-cross transformer model of point cloud change detecter(http://arxiv.org/abs/2309.07444)</code></li>
<li>Summary: <p>With the vigorous development of the urban construction industry, engineering
deformation or changes often occur during the construction process. To combat
this phenomenon, it is necessary to detect changes in order to detect
construction loopholes in time, ensure the integrity of the project and reduce
labor costs. Or the inconvenience and injuriousness of the road. In the study
of change detection in 3D point clouds, researchers have published various
research methods on 3D point clouds. Directly based on but mostly based
ontraditional threshold distance methods (C2C, M3C2, M3C2-EP), and some are to
convert 3D point clouds into DSM, which loses a lot of original information.
Although deep learning is used in remote sensing methods, in terms of change
detection of 3D point clouds, it is more converted into two-dimensional
patches, and neural networks are rarely applied directly. We prefer that the
network is given at the level of pixels or points. Variety. Therefore, in this
article, our network builds a network for 3D point cloud change detection, and
proposes a new module Cross transformer suitable for change detection.
Simultaneously simulate tunneling data for change detection, and do test
experiments with our network.
</p></li>
</ul>

<h3>Title: DT-NeRF: Decomposed Triplane-Hash Neural Radiance Fields for High-Fidelity Talking Portrait Synthesis. (arXiv:2309.07752v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07752">http://arxiv.org/abs/2309.07752</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07752]] DT-NeRF: Decomposed Triplane-Hash Neural Radiance Fields for High-Fidelity Talking Portrait Synthesis(http://arxiv.org/abs/2309.07752)</code></li>
<li>Summary: <p>In this paper, we present the decomposed triplane-hash neural radiance fields
(DT-NeRF), a framework that significantly improves the photorealistic rendering
of talking faces and achieves state-of-the-art results on key evaluation
datasets. Our architecture decomposes the facial region into two specialized
triplanes: one specialized for representing the mouth, and the other for the
broader facial features. We introduce audio features as residual terms and
integrate them as query vectors into our model through an audio-mouth-face
transformer. Additionally, our method leverages the capabilities of Neural
Radiance Fields (NeRF) to enrich the volumetric representation of the entire
face through additive volumetric rendering techniques. Comprehensive
experimental evaluations corroborate the effectiveness and superiority of our
proposed approach.
</p></li>
</ul>

<h3>Title: Traveling Words: A Geometric Interpretation of Transformers. (arXiv:2309.07315v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07315">http://arxiv.org/abs/2309.07315</a></li>
<li>Code URL: https://github.com/santiag0m/traveling-words</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07315]] Traveling Words: A Geometric Interpretation of Transformers(http://arxiv.org/abs/2309.07315)</code></li>
<li>Summary: <p>Transformers have significantly advanced the field of natural language
processing, but comprehending their internal mechanisms remains a challenge. In
this paper, we introduce a novel geometric perspective that elucidates the
inner mechanisms of transformer operations. Our primary contribution is
illustrating how layer normalization confines the latent features to a
hyper-sphere, subsequently enabling attention to mold the semantic
representation of words on this surface. This geometric viewpoint seamlessly
connects established properties such as iterative refinement and contextual
embeddings. We validate our insights by probing a pre-trained 124M parameter
GPT-2 model. Our findings reveal clear query-key attention patterns in early
layers and build upon prior observations regarding the subject-specific nature
of attention heads at deeper layers. Harnessing these geometric insights, we
present an intuitive understanding of transformers, depicting them as processes
that model the trajectory of word particles along the hyper-sphere.
</p></li>
</ul>

<h3>Title: Advancing Regular Language Reasoning in Linear Recurrent Neural Networks. (arXiv:2309.07412v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07412">http://arxiv.org/abs/2309.07412</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07412]] Advancing Regular Language Reasoning in Linear Recurrent Neural Networks(http://arxiv.org/abs/2309.07412)</code></li>
<li>Summary: <p>In recent studies, linear recurrent neural networks (LRNNs) have achieved
Transformer-level performance in natural language modeling and long-range
modeling while offering rapid parallel training and constant inference costs.
With the resurged interest in LRNNs, we study whether they can learn the hidden
rules in training sequences, such as the grammatical structures of regular
language. We theoretically analyze some existing LRNNs and discover their
limitations on regular language. Motivated by the analysis, we propose a new
LRNN equipped with a block-diagonal and input-dependent transition matrix.
Experiments suggest that the proposed model is the only LRNN that can perform
length extrapolation on regular language tasks such as Sum, Even Pair, and
Modular Arithmetic.
</p></li>
</ul>

<h3>Title: Text Classification of Cancer Clinical Trial Eligibility Criteria. (arXiv:2309.07812v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07812">http://arxiv.org/abs/2309.07812</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07812]] Text Classification of Cancer Clinical Trial Eligibility Criteria(http://arxiv.org/abs/2309.07812)</code></li>
<li>Summary: <p>Automatic identification of clinical trials for which a patient is eligible
is complicated by the fact that trial eligibility is stated in natural
language. A potential solution to this problem is to employ text classification
methods for common types of eligibility criteria. In this study, we focus on
seven common exclusion criteria in cancer trials: prior malignancy, human
immunodeficiency virus, hepatitis B, hepatitis C, psychiatric illness,
drug/substance abuse, and autoimmune illness. Our dataset consists of 764 phase
III cancer trials with these exclusions annotated at the trial level. We
experiment with common transformer models as well as a new pre-trained clinical
trial BERT model. Our results demonstrate the feasibility of automatically
classifying common exclusion criteria. Additionally, we demonstrate the value
of a pre-trained language model specifically for clinical trials, which yields
the highest average performance across all criteria.
</p></li>
</ul>

<h3>Title: EarthPT: a foundation model for Earth Observation. (arXiv:2309.07207v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07207">http://arxiv.org/abs/2309.07207</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07207]] EarthPT: a foundation model for Earth Observation(http://arxiv.org/abs/2309.07207)</code></li>
<li>Summary: <p>We introduce EarthPT -- an Earth Observation (EO) pretrained transformer.
EarthPT is a 700 million parameter decoding transformer foundation model
trained in an autoregressive self-supervised manner and developed specifically
with EO use-cases in mind. We demonstrate that EarthPT is an effective
forecaster that can accurately predict future pixel-level surface reflectances
across the 400-2300 nm range well into the future. For example, forecasts of
the evolution of the Normalised Difference Vegetation Index (NDVI) have a
typical error of approximately 0.05 (over a natural range of -1 -&gt; 1) at the
pixel level over a five month test set horizon, out-performing simple
phase-folded models based on historical averaging. We also demonstrate that
embeddings learnt by EarthPT hold semantically meaningful information and could
be exploited for downstream tasks such as highly granular, dynamic land use
classification. Excitingly, we note that the abundance of EO data provides us
with -- in theory -- quadrillions of training tokens. Therefore, if we assume
that EarthPT follows neural scaling laws akin to those derived for Large
Language Models (LLMs), there is currently no data-imposed limit to scaling
EarthPT and other similar `Large Observation Models.'
</p></li>
</ul>

<h3>Title: Structure-Preserving Transformers for Sequences of SPD Matrices. (arXiv:2309.07579v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07579">http://arxiv.org/abs/2309.07579</a></li>
<li>Code URL: https://github.com/mathieuseraphim/spdtransnet</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07579]] Structure-Preserving Transformers for Sequences of SPD Matrices(http://arxiv.org/abs/2309.07579)</code></li>
<li>Summary: <p>In recent years, Transformer-based auto-attention mechanisms have been
successfully applied to the analysis of a variety of context-reliant data
types, from texts to images and beyond, including data from non-Euclidean
geometries. In this paper, we present such a mechanism, designed to classify
sequences of Symmetric Positive Definite matrices while preserving their
Riemannian geometry throughout the analysis. We apply our method to automatic
sleep staging on timeseries of EEG-derived covariance matrices from a standard
dataset, obtaining high levels of stage-wise performance.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: GAN-based Algorithm for Efficient Image Inpainting. (arXiv:2309.07293v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07293">http://arxiv.org/abs/2309.07293</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07293]] GAN-based Algorithm for Efficient Image Inpainting(http://arxiv.org/abs/2309.07293)</code></li>
<li>Summary: <p>Global pandemic due to the spread of COVID-19 has post challenges in a new
dimension on facial recognition, where people start to wear masks. Under such
condition, the authors consider utilizing machine learning in image inpainting
to tackle the problem, by complete the possible face that is originally covered
in mask. In particular, autoencoder has great potential on retaining important,
general features of the image as well as the generative power of the generative
adversarial network (GAN). The authors implement a combination of the two
models, context encoders and explain how it combines the power of the two
models and train the model with 50,000 images of influencers faces and yields a
solid result that still contains space for improvements. Furthermore, the
authors discuss some shortcomings with the model, their possible improvements,
as well as some area of study for future investigation for applicative
perspective, as well as directions to further enhance and refine the model.
</p></li>
</ul>

<h3>Title: Dataset Condensation via Generative Model. (arXiv:2309.07698v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07698">http://arxiv.org/abs/2309.07698</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07698]] Dataset Condensation via Generative Model(http://arxiv.org/abs/2309.07698)</code></li>
<li>Summary: <p>Dataset condensation aims to condense a large dataset with a lot of training
samples into a small set. Previous methods usually condense the dataset into
the pixels format. However, it suffers from slow optimization speed and large
number of parameters to be optimized. When increasing image resolutions and
classes, the number of learnable parameters grows accordingly, prohibiting
condensation methods from scaling up to large datasets with diverse classes.
Moreover, the relations among condensed samples have been neglected and hence
the feature distribution of condensed samples is often not diverse. To solve
these problems, we propose to condense the dataset into another format, a
generative model. Such a novel format allows for the condensation of large
datasets because the size of the generative model remains relatively stable as
the number of classes or image resolution increases. Furthermore, an
intra-class and an inter-class loss are proposed to model the relation of
condensed samples. Intra-class loss aims to create more diverse samples for
each class by pushing each sample away from the others of the same class.
Meanwhile, inter-class loss increases the discriminability of samples by
widening the gap between the centers of different classes. Extensive
comparisons with state-of-the-art methods and our ablation studies confirm the
effectiveness of our method and its individual component. To our best
knowledge, we are the first to successfully conduct condensation on
ImageNet-1k.
</p></li>
</ul>

<h3>Title: Generative Image Dynamics. (arXiv:2309.07906v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07906">http://arxiv.org/abs/2309.07906</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07906]] Generative Image Dynamics(http://arxiv.org/abs/2309.07906)</code></li>
<li>Summary: <p>We present an approach to modeling an image-space prior on scene dynamics.
Our prior is learned from a collection of motion trajectories extracted from
real video sequences containing natural, oscillating motion such as trees,
flowers, candles, and clothes blowing in the wind. Given a single image, our
trained model uses a frequency-coordinated diffusion sampling process to
predict a per-pixel long-term motion representation in the Fourier domain,
which we call a neural stochastic motion texture. This representation can be
converted into dense motion trajectories that span an entire video. Along with
an image-based rendering module, these trajectories can be used for a number of
downstream applications, such as turning still images into seamlessly looping
dynamic videos, or allowing users to realistically interact with objects in
real pictures.
</p></li>
</ul>

<h3>Title: Looking at words and points with attention: a benchmark for text-to-shape coherence. (arXiv:2309.07917v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07917">http://arxiv.org/abs/2309.07917</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07917]] Looking at words and points with attention: a benchmark for text-to-shape coherence(http://arxiv.org/abs/2309.07917)</code></li>
<li>Summary: <p>While text-conditional 3D object generation and manipulation have seen rapid
progress, the evaluation of coherence between generated 3D shapes and input
textual descriptions lacks a clear benchmark. The reason is twofold: a) the low
quality of the textual descriptions in the only publicly available dataset of
text-shape pairs; b) the limited effectiveness of the metrics used to
quantitatively assess such coherence. In this paper, we propose a comprehensive
solution that addresses both weaknesses. Firstly, we employ large language
models to automatically refine textual descriptions associated with shapes.
Secondly, we propose a quantitative metric to assess text-to-shape coherence,
through cross-attention mechanisms. To validate our approach, we conduct a user
study and compare quantitatively our metric with existing ones. The refined
dataset, the new metric and a set of text-shape pairs validated by the user
study comprise a novel, fine-grained benchmark that we publicly release to
foster research on text-to-shape coherence of text-conditioned 3D generative
models. Benchmark available at
https://cvlab-unibo.github.io/CrossCoherence-Web/.
</p></li>
</ul>

<h3>Title: Detecting ChatGPT: A Survey of the State of Detecting ChatGPT-Generated Text. (arXiv:2309.07689v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07689">http://arxiv.org/abs/2309.07689</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07689]] Detecting ChatGPT: A Survey of the State of Detecting ChatGPT-Generated Text(http://arxiv.org/abs/2309.07689)</code></li>
<li>Summary: <p>While recent advancements in the capabilities and widespread accessibility of
generative language models, such as ChatGPT (OpenAI, 2022), have brought about
various benefits by generating fluent human-like text, the task of
distinguishing between human- and large language model (LLM) generated text has
emerged as a crucial problem. These models can potentially deceive by
generating artificial text that appears to be human-generated. This issue is
particularly significant in domains such as law, education, and science, where
ensuring the integrity of text is of the utmost importance. This survey
provides an overview of the current approaches employed to differentiate
between texts generated by humans and ChatGPT. We present an account of the
different datasets constructed for detecting ChatGPT-generated text, the
various methods utilized, what qualitative analyses into the characteristics of
human versus ChatGPT-generated text have been performed, and finally, summarize
our findings into general insights
</p></li>
</ul>

<h3>Title: Generative AI Text Classification using Ensemble LLM Approaches. (arXiv:2309.07755v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07755">http://arxiv.org/abs/2309.07755</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07755]] Generative AI Text Classification using Ensemble LLM Approaches(http://arxiv.org/abs/2309.07755)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have shown impressive performance across a
variety of Artificial Intelligence (AI) and natural language processing tasks,
such as content creation, report generation, etc. However, unregulated malign
application of these models can create undesirable consequences such as
generation of fake news, plagiarism, etc. As a result, accurate detection of
AI-generated language can be crucial in responsible usage of LLMs. In this
work, we explore 1) whether a certain body of text is AI generated or written
by human, and 2) attribution of a specific language model in generating a body
of text. Texts in both English and Spanish are considered. The datasets used in
this study are provided as part of the Automated Text Identification
(AuTexTification) shared task. For each of the research objectives stated
above, we propose an ensemble neural model that generates probabilities from
different pre-trained LLMs which are used as features to a Traditional Machine
Learning (TML) classifier following it. For the first task of distinguishing
between AI and human generated text, our model ranked in fifth and thirteenth
place (with macro $F1$ scores of 0.733 and 0.649) for English and Spanish
texts, respectively. For the second task on model attribution, our model ranked
in first place with macro $F1$ scores of 0.625 and 0.653 for English and
Spanish texts, respectively.
</p></li>
</ul>

<h3>Title: Market-GAN: Adding Control to Financial Market Data Generation with Semantic Context. (arXiv:2309.07708v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07708">http://arxiv.org/abs/2309.07708</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07708]] Market-GAN: Adding Control to Financial Market Data Generation with Semantic Context(http://arxiv.org/abs/2309.07708)</code></li>
<li>Summary: <p>Financial simulators play an important role in enhancing forecasting
accuracy, managing risks, and fostering strategic financial decision-making.
Despite the development of financial market simulation methodologies, existing
frameworks often struggle with adapting to specialized simulation context. We
pinpoint the challenges as i) current financial datasets do not contain context
labels; ii) current techniques are not designed to generate financial data with
context as control, which demands greater precision compared to other
modalities; iii) the inherent difficulties in generating context-aligned,
high-fidelity data given the non-stationary, noisy nature of financial data. To
address these challenges, our contributions are: i) we proposed the Contextual
Market Dataset with market dynamics, stock ticker, and history state as
context, leveraging a market dynamics modeling method that combines linear
regression and Dynamic Time Warping clustering to extract market dynamics; ii)
we present Market-GAN, a novel architecture incorporating a Generative
Adversarial Networks (GAN) for the controllable generation with context, an
autoencoder for learning low-dimension features, and supervisors for knowledge
transfer; iii) we introduce a two-stage training scheme to ensure that
Market-GAN captures the intrinsic market distribution with multiple objectives.
In the pertaining stage, with the use of the autoencoder and supervisors, we
prepare the generator with a better initialization for the adversarial training
stage. We propose a set of holistic evaluation metrics that consider alignment,
fidelity, data usability on downstream tasks, and market facts. We evaluate
Market-GAN with the Dow Jones Industrial Average data from 2000 to 2023 and
showcase superior performance in comparison to 4 state-of-the-art time-series
generative models.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: SwitchGPT: Adapting Large Language Models for Non-Text Outputs. (arXiv:2309.07623v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07623">http://arxiv.org/abs/2309.07623</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07623]] SwitchGPT: Adapting Large Language Models for Non-Text Outputs(http://arxiv.org/abs/2309.07623)</code></li>
<li>Summary: <p>Large Language Models (LLMs), primarily trained on text-based datasets,
exhibit exceptional proficiencies in understanding and executing complex
linguistic instructions via text outputs. However, they falter when requests to
generate non-text ones. Concurrently, modality conversion models, such as
text-to-image, despite generating high-quality images, suffer from a lack of
extensive textual pretraining. As a result, these models are only capable of
accommodating specific image descriptions rather than comprehending more
complex instructions. To bridge this gap, we propose a novel approach,
\methodname, from a modality conversion perspective that evolves a text-based
LLM into a multi-modal one. We specifically employ a minimal dataset to
instruct LLMs to recognize the intended output modality as directed by the
instructions. Consequently, the adapted LLM can effectively summon various
off-the-shelf modality conversion models from the model zoos to generate
non-text responses. This circumvents the necessity for complicated pretraining
that typically requires immense quantities of paired multi-modal data, while
simultaneously inheriting the extensive knowledge of LLMs and the ability of
high-quality generative models. To evaluate and compare the adapted multi-modal
LLM with its traditional counterparts, we have constructed a multi-modal
instruction benchmark that solicits diverse modality outputs. The experiment
results reveal that, with minimal training, LLMs can be conveniently adapted to
comprehend requests for non-text responses, thus achieving higher flexibility
in multi-modal scenarios. Code and data will be made available at
https://github.com/xinke-wang/SwitchGPT.
</p></li>
</ul>

<h3>Title: MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning. (arXiv:2309.07915v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07915">http://arxiv.org/abs/2309.07915</a></li>
<li>Code URL: https://github.com/haozhezhao/mic</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07915]] MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning(http://arxiv.org/abs/2309.07915)</code></li>
<li>Summary: <p>Starting from the resurgence of deep learning, vision-language models (VLMs)
benefiting from large language models (LLMs) have never been so popular.
However, while LLMs can utilize extensive background knowledge and task
information with in-context learning, most VLMs still struggle with
understanding complex multi-modal prompts with multiple images. The issue can
traced back to the architectural design of VLMs or pre-training data.
Specifically, the current VLMs primarily emphasize utilizing multi-modal data
with a single image some, rather than multi-modal prompts with interleaved
multiple images and text. Even though some newly proposed VLMs could handle
user prompts with multiple images, pre-training data does not provide more
sophisticated multi-modal prompts than interleaved image and text crawled from
the web. We propose MMICL to address the issue by considering both the model
and data perspectives. We introduce a well-designed architecture capable of
seamlessly integrating visual and textual context in an interleaved manner and
MIC dataset to reduce the gap between the training data and the complex user
prompts in real-world applications, including: 1) multi-modal context with
interleaved images and text, 2) textual references for each image, and 3)
multi-image data with spatial, logical, or temporal relationships. Our
experiments confirm that MMICL achieves new stat-of-the-art zero-shot and
few-shot performance on a wide range of general vision-language tasks,
especially for complex reasoning benchmarks including MME and MMBench. Our
analysis demonstrates that MMICL effectively deals with the challenge of
complex multi-modal prompt understanding. The experiments on ScienceQA-IMG also
show that MMICL successfully alleviates the issue of language bias in VLMs,
which we believe is the reason behind the advanced performance of MMICL.
</p></li>
</ul>

<h3>Title: Unified Human-Scene Interaction via Prompted Chain-of-Contacts. (arXiv:2309.07918v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07918">http://arxiv.org/abs/2309.07918</a></li>
<li>Code URL: https://github.com/openrobotlab/unihsi</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07918]] Unified Human-Scene Interaction via Prompted Chain-of-Contacts(http://arxiv.org/abs/2309.07918)</code></li>
<li>Summary: <p>Human-Scene Interaction (HSI) is a vital component of fields like embodied AI
and virtual reality. Despite advancements in motion quality and physical
plausibility, two pivotal factors, versatile interaction control and the
development of a user-friendly interface, require further exploration before
the practical application of HSI. This paper presents a unified HSI framework,
UniHSI, which supports unified control of diverse interactions through language
commands. This framework is built upon the definition of interaction as Chain
of Contacts (CoC): steps of human joint-object part pairs, which is inspired by
the strong correlation between interaction types and human-object contact
regions. Based on the definition, UniHSI constitutes a Large Language Model
(LLM) Planner to translate language prompts into task plans in the form of CoC,
and a Unified Controller that turns CoC into uniform task execution. To
facilitate training and evaluation, we collect a new dataset named ScenePlan
that encompasses thousands of task plans generated by LLMs based on diverse
scenarios. Comprehensive experiments demonstrate the effectiveness of our
framework in versatile task execution and generalizability to real scanned
scenes. The project page is at https://github.com/OpenRobotLab/UniHSI .
</p></li>
</ul>

<h3>Title: In-Contextual Bias Suppression for Large Language Models. (arXiv:2309.07251v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07251">http://arxiv.org/abs/2309.07251</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07251]] In-Contextual Bias Suppression for Large Language Models(http://arxiv.org/abs/2309.07251)</code></li>
<li>Summary: <p>Despite their impressive performance in a wide range of NLP tasks, Large
Language Models (LLMs) have been reported to encode worrying-levels of gender
bias. Prior work has proposed debiasing methods that require human labelled
examples, data augmentation and fine-tuning of the LLMs, which are
computationally costly. Moreover, one might not even have access to the
internal parameters for performing debiasing such as in the case of
commercially available LLMs such as GPT-4. To address this challenge we propose
bias suppression, a novel alternative to debiasing that does not require access
to model parameters. We show that text-based preambles, generated from manually
designed templates covering counterfactual statements, can accurately suppress
gender biases in LLMs. Moreover, we find that descriptive sentences for
occupations can further suppress gender biases. Interestingly, we find that
bias suppression has a minimal adverse effect on downstream task performance,
while effectively mitigating the gender biases.
</p></li>
</ul>

<h3>Title: An Interactive Framework for Profiling News Media Sources. (arXiv:2309.07384v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07384">http://arxiv.org/abs/2309.07384</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07384]] An Interactive Framework for Profiling News Media Sources(http://arxiv.org/abs/2309.07384)</code></li>
<li>Summary: <p>The recent rise of social media has led to the spread of large amounts of
fake and biased news, content published with the intent to sway beliefs. While
detecting and profiling the sources that spread this news is important to
maintain a healthy society, it is challenging for automated systems.
</p>
<p>In this paper, we propose an interactive framework for news media profiling.
It combines the strengths of graph based news media profiling models,
Pre-trained Large Language Models, and human insight to characterize the social
context on social media. Experimental results show that with as little as 5
human interactions, our framework can rapidly detect fake and biased news
media, even in the most challenging settings of emerging news events, where
test data is unseen.
</p></li>
</ul>

<h3>Title: ChatGPT MT: Competitive for High- (but not Low-) Resource Languages. (arXiv:2309.07423v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07423">http://arxiv.org/abs/2309.07423</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07423]] ChatGPT MT: Competitive for High- (but not Low-) Resource Languages(http://arxiv.org/abs/2309.07423)</code></li>
<li>Summary: <p>Large language models (LLMs) implicitly learn to perform a range of language
tasks, including machine translation (MT). Previous studies explore aspects of
LLMs' MT capabilities. However, there exist a wide variety of languages for
which recent LLM MT performance has never before been evaluated. Without
published experimental evidence on the matter, it is difficult for speakers of
the world's diverse languages to know how and whether they can use LLMs for
their languages. We present the first experimental evidence for an expansive
set of 204 languages, along with MT cost analysis, using the FLORES-200
benchmark. Trends reveal that GPT models approach or exceed traditional MT
model performance for some high-resource languages (HRLs) but consistently lag
for low-resource languages (LRLs), under-performing traditional MT for 84.1% of
languages we covered. Our analysis reveals that a language's resource level is
the most important feature in determining ChatGPT's relative ability to
translate it, and suggests that ChatGPT is especially disadvantaged for LRLs
and African languages.
</p></li>
</ul>

<h3>Title: Clinical Text Summarization: Adapting Large Language Models Can Outperform Human Experts. (arXiv:2309.07430v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07430">http://arxiv.org/abs/2309.07430</a></li>
<li>Code URL: https://github.com/stanfordmimi/clin-summ</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07430]] Clinical Text Summarization: Adapting Large Language Models Can Outperform Human Experts(http://arxiv.org/abs/2309.07430)</code></li>
<li>Summary: <p>Sifting through vast textual data and summarizing key information imposes a
substantial burden on how clinicians allocate their time. Although large
language models (LLMs) have shown immense promise in natural language
processing (NLP) tasks, their efficacy across diverse clinical summarization
tasks has not yet been rigorously examined. In this work, we employ domain
adaptation methods on eight LLMs, spanning six datasets and four distinct
summarization tasks: radiology reports, patient questions, progress notes, and
doctor-patient dialogue. Our thorough quantitative assessment reveals
trade-offs between models and adaptation methods in addition to instances where
recent advances in LLMs may not lead to improved results. Further, in a
clinical reader study with six physicians, we depict that summaries from the
best adapted LLM are preferable to human summaries in terms of completeness and
correctness. Our ensuing qualitative analysis delineates mutual challenges
faced by both LLMs and human experts. Lastly, we correlate traditional
quantitative NLP metrics with reader study scores to enhance our understanding
of how these metrics align with physician preferences. Our research marks the
first evidence of LLMs outperforming human experts in clinical text
summarization across multiple tasks. This implies that integrating LLMs into
clinical workflows could alleviate documentation burden, empowering clinicians
to focus more on personalized patient care and other irreplaceable human
aspects of medicine.
</p></li>
</ul>

<h3>Title: Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?. (arXiv:2309.07462v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07462">http://arxiv.org/abs/2309.07462</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07462]] Are Large Language Model-based Evaluators the Solution to Scaling Up Multilingual Evaluation?(http://arxiv.org/abs/2309.07462)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have demonstrated impressive performance on
Natural Language Processing (NLP) tasks, such as Question Answering,
Summarization, and Classification. The use of LLMs as evaluators, that can rank
or score the output of other models (usually LLMs) has become increasingly
popular, due to the limitations of current evaluation techniques including the
lack of appropriate benchmarks, metrics, cost, and access to human annotators.
While LLMs are capable of handling approximately 100 languages, the majority of
languages beyond the top 20 lack systematic evaluation across various tasks,
metrics, and benchmarks. This creates an urgent need to scale up multilingual
evaluation to ensure a precise understanding of LLM performance across diverse
languages. LLM-based evaluators seem like the perfect solution to this problem,
as they do not require human annotators, human-created references, or
benchmarks and can theoretically be used to evaluate any language covered by
the LLM. In this paper, we investigate whether LLM-based evaluators can help
scale up multilingual evaluation. Specifically, we calibrate LLM-based
evaluation against 20k human judgments of five metrics across three
text-generation tasks in eight languages. Our findings indicate that LLM-based
evaluators may exhibit bias towards higher scores and should be used with
caution and should always be calibrated with a dataset of native speaker
judgments, particularly in low-resource and non-Latin script languages.
</p></li>
</ul>

<h3>Title: Zero-shot Audio Topic Reranking using Large Language Models. (arXiv:2309.07606v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07606">http://arxiv.org/abs/2309.07606</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07606]] Zero-shot Audio Topic Reranking using Large Language Models(http://arxiv.org/abs/2309.07606)</code></li>
<li>Summary: <p>The Multimodal Video Search by Examples (MVSE) project investigates using
video clips as the query term for information retrieval, rather than the more
traditional text query. This enables far richer search modalities such as
images, speaker, content, topic, and emotion. A key element for this process is
highly rapid, flexible, search to support large archives, which in MVSE is
facilitated by representing video attributes by embeddings. This work aims to
mitigate any performance loss from this rapid archive search by examining
reranking approaches. In particular, zero-shot reranking methods using large
language models are investigated as these are applicable to any video archive
audio content. Performance is evaluated for topic-based retrieval on a publicly
available video archive, the BBC Rewind corpus. Results demonstrate that
reranking can achieve improved retrieval ranking without the need for any
task-specific training data.
</p></li>
</ul>

<h3>Title: Tree of Uncertain Thoughts Reasoning for Large Language Models. (arXiv:2309.07694v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07694">http://arxiv.org/abs/2309.07694</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07694]] Tree of Uncertain Thoughts Reasoning for Large Language Models(http://arxiv.org/abs/2309.07694)</code></li>
<li>Summary: <p>While the recently introduced Tree of Thoughts (ToT) has heralded
advancements in allowing Large Language Models (LLMs) to reason through
foresight and backtracking for global decision-making, it has overlooked the
inherent local uncertainties in intermediate decision points or "thoughts".
These local uncertainties, intrinsic to LLMs given their potential for diverse
responses, remain a significant concern in the reasoning process. Addressing
this pivotal gap, we introduce the Tree of Uncertain Thoughts (TouT) - a
reasoning framework tailored for LLMs. Our TouT effectively leverages Monte
Carlo Dropout to quantify uncertainty scores associated with LLMs' diverse
local responses at these intermediate steps. By marrying this local uncertainty
quantification with global search algorithms, TouT enhances the model's
precision in response generation. We substantiate our approach with rigorous
experiments on two demanding planning tasks: Game of 24 and Mini Crosswords.
The empirical evidence underscores TouT's superiority over both ToT and
chain-of-thought prompting methods.
</p></li>
</ul>

<h3>Title: CATfOOD: Counterfactual Augmented Training for Improving Out-of-Domain Performance and Calibration. (arXiv:2309.07822v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07822">http://arxiv.org/abs/2309.07822</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07822]] CATfOOD: Counterfactual Augmented Training for Improving Out-of-Domain Performance and Calibration(http://arxiv.org/abs/2309.07822)</code></li>
<li>Summary: <p>In recent years, large language models (LLMs) have shown remarkable
capabilities at scale, particularly at generating text conditioned on a prompt.
In our work, we investigate the use of LLMs to augment training data of small
language models~(SLMs) with automatically generated counterfactual~(CF)
instances -- i.e. minimally altered inputs -- in order to improve
out-of-domain~(OOD) performance of SLMs in the extractive question
answering~(QA) setup. We show that, across various LLM generators, such data
augmentation consistently enhances OOD performance and improves model
calibration for both confidence-based and rationale-augmented calibrator
models. Furthermore, these performance improvements correlate with higher
diversity of CF instances in terms of their surface form and semantic content.
Finally, we show that CF augmented models which are easier to calibrate also
exhibit much lower entropy when assigning importance, indicating that
rationale-augmented calibrators prefer concise explanations.
</p></li>
</ul>

<h3>Title: Agents: An Open-source Framework for Autonomous Language Agents. (arXiv:2309.07870v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07870">http://arxiv.org/abs/2309.07870</a></li>
<li>Code URL: https://github.com/aiwaves-cn/agents</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07870]] Agents: An Open-source Framework for Autonomous Language Agents(http://arxiv.org/abs/2309.07870)</code></li>
<li>Summary: <p>Recent advances on large language models (LLMs) enable researchers and
developers to build autonomous language agents that can automatically solve
various tasks and interact with environments, humans, and other agents using
natural language interfaces. We consider language agents as a promising
direction towards artificial general intelligence and release Agents, an
open-source library with the goal of opening up these advances to a wider
non-specialist audience. Agents is carefully engineered to support important
features including planning, memory, tool usage, multi-agent communication, and
fine-grained symbolic control. Agents is user-friendly as it enables
non-specialists to build, customize, test, tune, and deploy state-of-the-art
autonomous language agents without much coding. The library is also
research-friendly as its modularized design makes it easily extensible for
researchers. Agents is available at https://github.com/aiwaves-cn/agents.
</p></li>
</ul>

<h3>Title: Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions. (arXiv:2309.07875v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07875">http://arxiv.org/abs/2309.07875</a></li>
<li>Code URL: https://github.com/vinid/instruction-llms-safety-eval</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07875]] Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions(http://arxiv.org/abs/2309.07875)</code></li>
<li>Summary: <p>Training large language models to follow instructions makes them perform
better on a wide range of tasks, generally becoming more helpful. However, a
perfectly helpful model will follow even the most malicious instructions and
readily generate harmful content. In this paper, we raise concerns over the
safety of models that only emphasize helpfulness, not safety, in their
instruction-tuning. We show that several popular instruction-tuned models are
highly unsafe. Moreover, we show that adding just 3% safety examples (a few
hundred demonstrations) in the training set when fine-tuning a model like LLaMA
can substantially improve their safety. Our safety-tuning does not make models
significantly less capable or helpful as measured by standard benchmarks.
However, we do find a behavior of exaggerated safety, where too much
safety-tuning makes models refuse to respond to reasonable prompts that
superficially resemble unsafe ones. Our study sheds light on trade-offs in
training LLMs to follow instructions and exhibit safe behavior.
</p></li>
</ul>

<h3>Title: Ambiguity-Aware In-Context Learning with Large Language Models. (arXiv:2309.07900v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07900">http://arxiv.org/abs/2309.07900</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07900]] Ambiguity-Aware In-Context Learning with Large Language Models(http://arxiv.org/abs/2309.07900)</code></li>
<li>Summary: <p>In-context learning (ICL) i.e. showing LLMs only a few task-specific
demonstrations has led to downstream gains with no task-specific fine-tuning
required. However, LLMs are sensitive to the choice of prompts, and therefore a
crucial research question is how to select good demonstrations for ICL. One
effective strategy is leveraging semantic similarity between the ICL
demonstrations and test inputs by using a text retriever, which however is
sub-optimal as that does not consider the LLM's existing knowledge about that
task. From prior work (Min et al., 2022), we already know that labels paired
with the demonstrations bias the model predictions. This leads us to our
hypothesis whether considering LLM's existing knowledge about the task,
especially with respect to the output label space can help in a better
demonstration selection strategy. Through extensive experimentation on three
text classification tasks, we find that it is beneficial to not only choose
semantically similar ICL demonstrations but also to choose those demonstrations
that help resolve the inherent label ambiguity surrounding the test example.
Interestingly, we find that including demonstrations that the LLM previously
mis-classified and also fall on the test example's decision boundary, brings
the most performance gain.
</p></li>
</ul>

<h3>Title: Two Timin': Repairing Smart Contracts With A Two-Layered Approach. (arXiv:2309.07841v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07841">http://arxiv.org/abs/2309.07841</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07841]] Two Timin': Repairing Smart Contracts With A Two-Layered Approach(http://arxiv.org/abs/2309.07841)</code></li>
<li>Summary: <p>Due to the modern relevance of blockchain technology, smart contracts present
both substantial risks and benefits. Vulnerabilities within them can trigger a
cascade of consequences, resulting in significant losses. Many current papers
primarily focus on classifying smart contracts for malicious intent, often
relying on limited contract characteristics, such as bytecode or opcode. This
paper proposes a novel, two-layered framework: 1) classifying and 2) directly
repairing malicious contracts. Slither's vulnerability report is combined with
source code and passed through a pre-trained RandomForestClassifier (RFC) and
Large Language Models (LLMs), classifying and repairing each suggested
vulnerability. Experiments demonstrate the effectiveness of fine-tuned and
prompt-engineered LLMs. The smart contract repair models, built from
pre-trained GPT-3.5-Turbo and fine-tuned Llama-2-7B models, reduced the overall
vulnerability count by 97.5% and 96.7% respectively. A manual inspection of
repaired contracts shows that all retain functionality, indicating that the
proposed method is appropriate for automatic batch classification and repair of
vulnerabilities in smart contracts.
</p></li>
</ul>

<h3>Title: VerilogEval: Evaluating Large Language Models for Verilog Code Generation. (arXiv:2309.07544v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07544">http://arxiv.org/abs/2309.07544</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07544]] VerilogEval: Evaluating Large Language Models for Verilog Code Generation(http://arxiv.org/abs/2309.07544)</code></li>
<li>Summary: <p>The increasing popularity of large language models (LLMs) has paved the way
for their application in diverse domains. This paper proposes a benchmarking
framework tailored specifically for evaluating LLM performance in the context
of Verilog code generation for hardware design and verification. We present a
comprehensive evaluation dataset consisting of 156 problems from the Verilog
instructional website HDLBits. The evaluation set consists of a diverse set of
Verilog code generation tasks, ranging from simple combinational circuits to
complex finite state machines. The Verilog code completions can be
automatically tested for functional correctness by comparing the transient
simulation outputs of the generated design with a golden solution. We also
demonstrate that the Verilog code generation capability of pretrained language
models could be improved with supervised fine-tuning by bootstrapping with LLM
generated synthetic problem-code pairs.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: JSMNet Improving Indoor Point Cloud Semantic and Instance Segmentation through Self-Attention and Multiscale. (arXiv:2309.07425v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07425">http://arxiv.org/abs/2309.07425</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07425]] JSMNet Improving Indoor Point Cloud Semantic and Instance Segmentation through Self-Attention and Multiscale(http://arxiv.org/abs/2309.07425)</code></li>
<li>Summary: <p>The semantic understanding of indoor 3D point cloud data is crucial for a
range of subsequent applications, including indoor service robots, navigation
systems, and digital twin engineering. Global features are crucial for
achieving high-quality semantic and instance segmentation of indoor point
clouds, as they provide essential long-range context information. To this end,
we propose JSMNet, which combines a multi-layer network with a global feature
self-attention module to jointly segment three-dimensional point cloud
semantics and instances. To better express the characteristics of indoor
targets, we have designed a multi-resolution feature adaptive fusion module
that takes into account the differences in point cloud density caused by
varying scanner distances from the target. Additionally, we propose a framework
for joint semantic and instance segmentation by integrating semantic and
instance features to achieve superior results. We conduct experiments on S3DIS,
which is a large three-dimensional indoor point cloud dataset. Our proposed
method is compared against other methods, and the results show that it
outperforms existing methods in semantic and instance segmentation and provides
better results in target local area segmentation. Specifically, our proposed
method outperforms PointNet (Qi et al., 2017a) by 16.0% and 26.3% in terms of
semantic segmentation mIoU in S3DIS (Area 5) and instance segmentation mPre,
respectively. Additionally, it surpasses ASIS (Wang et al., 2019) by 6.0% and
4.6%, respectively, as well as JSPNet (Chen et al., 2022) by a margin of 3.3%
for semantic segmentation mIoU and a slight improvement of 0.3% for instance
segmentation mPre.
</p></li>
</ul>

<h3>Title: RecycleNet: Latent Feature Recycling Leads to Iterative Decision Refinement. (arXiv:2309.07513v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07513">http://arxiv.org/abs/2309.07513</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07513]] RecycleNet: Latent Feature Recycling Leads to Iterative Decision Refinement(http://arxiv.org/abs/2309.07513)</code></li>
<li>Summary: <p>Despite the remarkable success of deep learning systems over the last decade,
a key difference still remains between neural network and human
decision-making: As humans, we cannot only form a decision on the spot, but
also ponder, revisiting an initial guess from different angles, distilling
relevant information, arriving at a better decision. Here, we propose
RecycleNet, a latent feature recycling method, instilling the pondering
capability for neural networks to refine initial decisions over a number of
recycling steps, where outputs are fed back into earlier network layers in an
iterative fashion. This approach makes minimal assumptions about the neural
network architecture and thus can be implemented in a wide variety of contexts.
Using medical image segmentation as the evaluation environment, we show that
latent feature recycling enables the network to iteratively refine initial
predictions even beyond the iterations seen during training, converging towards
an improved decision. We evaluate this across a variety of segmentation
benchmarks and show consistent improvements even compared with top-performing
segmentation methods. This allows trading increased computation time for
improved performance, which can be beneficial, especially for safety-critical
applications.
</p></li>
</ul>

<h3>Title: NutritionVerse: Empirical Study of Various Dietary Intake Estimation Approaches. (arXiv:2309.07704v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07704">http://arxiv.org/abs/2309.07704</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07704]] NutritionVerse: Empirical Study of Various Dietary Intake Estimation Approaches(http://arxiv.org/abs/2309.07704)</code></li>
<li>Summary: <p>Accurate dietary intake estimation is critical for informing policies and
programs to support healthy eating, as malnutrition has been directly linked to
decreased quality of life. However self-reporting methods such as food diaries
suffer from substantial bias. Other conventional dietary assessment techniques
and emerging alternative approaches such as mobile applications incur high time
costs and may necessitate trained personnel. Recent work has focused on using
computer vision and machine learning to automatically estimate dietary intake
from food images, but the lack of comprehensive datasets with diverse
viewpoints, modalities and food annotations hinders the accuracy and realism of
such methods. To address this limitation, we introduce NutritionVerse-Synth,
the first large-scale dataset of 84,984 photorealistic synthetic 2D food images
with associated dietary information and multimodal annotations (including depth
images, instance masks, and semantic masks). Additionally, we collect a real
image dataset, NutritionVerse-Real, containing 889 images of 251 dishes to
evaluate realism. Leveraging these novel datasets, we develop and benchmark
NutritionVerse, an empirical study of various dietary intake estimation
approaches, including indirect segmentation-based and direct prediction
networks. We further fine-tune models pretrained on synthetic data with real
images to provide insights into the fusion of synthetic and real data. Finally,
we release both datasets (NutritionVerse-Synth, NutritionVerse-Real) on
https://www.kaggle.com/nutritionverse/datasets as part of an open initiative to
accelerate machine learning for dietary sensing.
</p></li>
</ul>

<h3>Title: TFNet: Exploiting Temporal Cues for Fast and Accurate LiDAR Semantic Segmentation. (arXiv:2309.07849v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07849">http://arxiv.org/abs/2309.07849</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07849]] TFNet: Exploiting Temporal Cues for Fast and Accurate LiDAR Semantic Segmentation(http://arxiv.org/abs/2309.07849)</code></li>
<li>Summary: <p>LiDAR semantic segmentation plays a crucial role in enabling autonomous
driving and robots to understand their surroundings accurately and robustly.
There are different types of methods, such as point-based, range image-based,
and polar-based. Among these, range image-based methods are widely used due to
their balance between accuracy and speed. However, they face a significant
challenge known as the ``many-to-one'' problem caused by the range image's
limited horizontal and vertical angular resolution, where around 20% of the 3D
points are occluded during model inference based on our observation. In this
paper, we present TFNet, a range image-based LiDAR semantic segmentation method
that utilizes temporal information to address this issue. Specifically, we
incorporate a temporal fusion layer to extract useful information from previous
scans and integrate it with the current scan. We then design a max-voting-based
post-processing technique to correct false predictions, particularly those
caused by the ``many-to-one'' issue. Experiments on two benchmarks and seven
backbones of three modalities demonstrate the effectiveness and scalability of
our proposed method.
</p></li>
</ul>

<h3>Title: OpenIllumination: A Multi-Illumination Dataset for Inverse Rendering Evaluation on Real Objects. (arXiv:2309.07921v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07921">http://arxiv.org/abs/2309.07921</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07921]] OpenIllumination: A Multi-Illumination Dataset for Inverse Rendering Evaluation on Real Objects(http://arxiv.org/abs/2309.07921)</code></li>
<li>Summary: <p>We introduce OpenIllumination, a real-world dataset containing over 108K
images of 64 objects with diverse materials, captured under 72 camera views and
a large number of different illuminations. For each image in the dataset, we
provide accurate camera parameters, illumination ground truth, and foreground
segmentation masks. Our dataset enables the quantitative evaluation of most
inverse rendering and material decomposition methods for real objects. We
examine several state-of-the-art inverse rendering methods on our dataset and
compare their performances. The dataset and code can be found on the project
page: https://oppo-us-research.github.io/OpenIllumination.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
