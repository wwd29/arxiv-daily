<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-09-16</h1>
<h3>Title: The 1st International Workshop on Disentangled Representation Learning for Controllable Generation (DRL4Real): Methods and Results</h3>
<ul>
<li><strong>Authors: </strong>Qiuyu Chen, Xin Jin, Yue Song, Xihui Liu, Shuai Yang, Tao Yang, Ziqiang Li, Jianguo Huang, Yuntao Wei, Ba'ao Xie, Nicu Sebe, Wenjun (Kevin)Zeng, Jooyeol Yun, Davide Abati, Mohamed Omran, Jaegul Choo, Amir Habibian, Auke Wiggers, Masato Kobayashi, Ning Ding, Toru Tamaki, Marzieh Gheisari, Auguste Genovesio, Yuheng Chen, Dingkun Liu, Xinyao Yang, Xinping Xu, Baicheng Chen, Dongrui Wu, Junhao Geng, Lexiang Lv, Jianxin Lin, Hanzhe Liang, Jie Zhou, Xuanxin Chen, Jinbao Wang, Can Gao, Zhangyi Wang, Zongze Li, Bihan Wen, Yixin Gao, Xiaohan Pan, Xin Li, Zhibo Chen, Baorui Peng, Zhongming Chen, Haoran Jin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10463">https://arxiv.org/abs/2509.10463</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10463">https://arxiv.org/pdf/2509.10463</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10463]] The 1st International Workshop on Disentangled Representation Learning for Controllable Generation (DRL4Real): Methods and Results(https://arxiv.org/abs/2509.10463)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, diffusion</a></li>
<li><strong>Abstract: </strong>This paper reviews the 1st International Workshop on Disentangled Representation Learning for Controllable Generation (DRL4Real), held in conjunction with ICCV 2025. The workshop aimed to bridge the gap between the theoretical promise of Disentangled Representation Learning (DRL) and its application in realistic scenarios, moving beyond synthetic benchmarks. DRL4Real focused on evaluating DRL methods in practical applications such as controllable generation, exploring advancements in model robustness, interpretability, and generalization. The workshop accepted 9 papers covering a broad range of topics, including the integration of novel inductive biases (e.g., language), the application of diffusion models to DRL, 3D-aware disentanglement, and the expansion of DRL into specialized domains like autonomous driving and EEG analysis. This summary details the workshop's objectives, the themes of the accepted papers, and provides an overview of the methodologies proposed by the authors.</li>
</ul>

<h3>Title: A Real-Time Diminished Reality Approach to Privacy in MR Collaboration</h3>
<ul>
<li><strong>Authors: </strong>Christian Fane</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10466">https://arxiv.org/abs/2509.10466</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10466">https://arxiv.org/pdf/2509.10466</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10466]] A Real-Time Diminished Reality Approach to Privacy in MR Collaboration(https://arxiv.org/abs/2509.10466)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Diminished reality (DR) refers to the digital removal of real-world objects by compositing background content in their place. This thesis presents a real-time, inpainting-based DR system designed to enable privacy control in shared-space mixed reality (MR) meetings. The system allows a primary headset user to selectively remove personal or sensitive items from their environment, ensuring that those objects are no longer visible to other participants. Removal is achieved through semantic segmentation and precise object selection, followed by real-time inpainting from the viewpoint of a secondary observer, implemented using a mobile ZED 2i depth camera. The solution is designed to be portable and robust, requiring neither a fixed secondary viewpoint nor prior 3D scanning of the environment. The system utilises YOLOv11 for object detection and a modified Decoupled Spatial-Temporal Transformer (DSTT) model for high-quality video inpainting. At 720p resolution, the pipeline sustains frame rates exceeding 20 fps, demonstrating the feasibility of real-time diminished reality for practical privacy-preserving MR applications.</li>
</ul>

<h3>Title: AegisShield: Democratizing Cyber Threat Modeling with Generative AI</h3>
<ul>
<li><strong>Authors: </strong>Matthew Grofsky</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10482">https://arxiv.org/abs/2509.10482</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10482">https://arxiv.org/pdf/2509.10482</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10482]] AegisShield: Democratizing Cyber Threat Modeling with Generative AI(https://arxiv.org/abs/2509.10482)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, generative</a></li>
<li><strong>Abstract: </strong>The increasing sophistication of technology systems makes traditional threat modeling hard to scale, especially for small organizations with limited resources. This paper develops and evaluates AegisShield, a generative AI enhanced threat modeling tool that implements STRIDE and MITRE ATT&CK to automate threat generation and provide systematic assessments. By integrating real time threat intelligence from the National Vulnerability Database and AlienVault Open Threat Exchange, AegisShield produces streamlined and accessible threat descriptions. Our assessment of 243 threats from 15 case studies and over 8000 AI generated threats shows that AegisShield reduces complexity (p less than 0.001), yields outputs semantically aligned with expert developed threats (p less than 0.05), and achieves an 85.4 percent success rate in mapping threats to MITRE ATT&CK techniques (p less than 0.001). Automating and standardizing threat modeling helps under resourced organizations address risk earlier and supports wider adoption of secure by design practices.</li>
</ul>

<h3>Title: Turning CVEs into Educational Labs:Insights and Challenges</h3>
<ul>
<li><strong>Authors: </strong>Trueye Tafese</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10488">https://arxiv.org/abs/2509.10488</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10488">https://arxiv.org/pdf/2509.10488</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10488]] Turning CVEs into Educational Labs:Insights and Challenges(https://arxiv.org/abs/2509.10488)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>This research focuses on transforming CVEs to hands-on educational lab for cybersecurity training. The study shows the practical application of CVEs by developing containerized lab environments- Docker to simulate real-world vulnerabilities like SQL Injection, arbitrary code execution, and improper SSL certificate validation. These labs has structured tutorials, pre- and post-surveys to evaluate learning outcomes, and remediation this http URL challenges included interpreting limited CVE data, resolving technical complexities in lab design, and ensuring accessibility for diverse learners. Despite these difficulties, the findings highlight the use of educational benefits of vulnerability analysis, bridging theoretical concepts with hands-on experience. The results indicate that students improved comprehension of cybersecurity principles, threat mitigation techniques, and secure coding practices. This innovative approach provides a scalable and reproducible model for integrating CVEs into cybersecurity education, fostering a deeper understanding of real-world security challenges in a controlled and safe environment.</li>
</ul>

<h3>Title: Investigation Of The Distinguishability Of Giraud-Verneuil Atomic Blocks</h3>
<ul>
<li><strong>Authors: </strong>Philip Laryea Doku</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10492">https://arxiv.org/abs/2509.10492</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10492">https://arxiv.org/pdf/2509.10492</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10492]] Investigation Of The Distinguishability Of Giraud-Verneuil Atomic Blocks(https://arxiv.org/abs/2509.10492)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack</a></li>
<li><strong>Abstract: </strong>In this work, we investigate the security of Elliptic Curve Cryptosystem (ECC) implementations against Side-Channel Analysis (SCA). ECC is well known for its efficiency and strong security, yet vulnerable to SCA which exploits physical information leaked during scalar multiplication (kP). Countermeasures such as regularity and atomicity exist; this thesis focuses on atomicity. In this work, we study the Giraud and Verneuil atomic pattern for kP, implementing it using the right-to-left kP algorithm on the NIST EC P-256 curve. We use the FLECC library with constant-time operations and execute on the Texas Instruments LAUNCHXLF28379D MCU. We measure Electromagnetic (EM) emissions during kP using a Lecroy WavePro 604HD Oscilloscope, a Langer ICS 105 Integrated Circuit Scanner, and a Langer MFA-R 0.2-75 Near Field Probe. We investigate whether the Giraud and Verneuil atomic blocks are distinguishable in EM traces. Our findings show that, when additional clock cycle processes are present, the atomic blocks can be visually distinguished; after removing these processes, they become more synchronised and harder to distinguish, reducing the risk of a successful SCA attack. These results show that, although the atomic pattern is correctly implemented with dummy operations, resistance to SCA can still be affected by additional processes inserted at hardware or software this http URL means atomicity alone may not fully protect ECC from SCA. More research is needed to investigate the causes of the additional clock cycle processes and how intermediate operations are addressed in memory registers. This will help to understand the processes that lead to the insertion of these additional clock cycles. This thesis is the first to experimentally implement and investigate Giraud and Verneuil's atomic pattern on hardware, and it offers useful results to improve countermeasures against SCA.</li>
</ul>

<h3>Title: Moment Estimates and DeepRitz Methods on Learning Diffusion Systems with Non-gradient Drifts</h3>
<ul>
<li><strong>Authors: </strong>Fanze Kong, Chen-Chih Lai, Yubin Lu</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10495">https://arxiv.org/abs/2509.10495</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10495">https://arxiv.org/pdf/2509.10495</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10495]] Moment Estimates and DeepRitz Methods on Learning Diffusion Systems with Non-gradient Drifts(https://arxiv.org/abs/2509.10495)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Conservative-dissipative dynamics are ubiquitous across a variety of complex open systems. We propose a data-driven two-phase method, the Moment-DeepRitz Method, for learning drift decompositions in generalized diffusion systems involving conservative-dissipative dynamics. The method is robust to noisy data, adaptable to rough potentials and oscillatory rotations. We demonstrate its effectiveness through several numerical experiments.</li>
</ul>

<h3>Title: From Noise to Precision: A Diffusion-Driven Approach to Zero-Inflated Precipitation Prediction</h3>
<ul>
<li><strong>Authors: </strong>Wentao Gao, Jiuyong Li, Lin Liu, Thuc Duy Le, Xiongren Chen, Xiaojing Du, Jixue Liu, Yanchang Zhao, Yun Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10501">https://arxiv.org/abs/2509.10501</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10501">https://arxiv.org/pdf/2509.10501</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10501]] From Noise to Precision: A Diffusion-Driven Approach to Zero-Inflated Precipitation Prediction(https://arxiv.org/abs/2509.10501)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Zero-inflated data pose significant challenges in precipitation forecasting due to the predominance of zeros with sparse non-zero events. To address this, we propose the Zero Inflation Diffusion Framework (ZIDF), which integrates Gaussian perturbation for smoothing zero-inflated distributions, Transformer-based prediction for capturing temporal patterns, and diffusion-based denoising to restore the original data structure. In our experiments, we use observational precipitation data collected from South Australia along with synthetically generated zero-inflated data. Results show that ZIDF demonstrates significant performance improvements over multiple state-of-the-art precipitation forecasting models, achieving up to 56.7\% reduction in MSE and 21.1\% reduction in MAE relative to the baseline Non-stationary Transformer. These findings highlight ZIDF's ability to robustly handle sparse time series data and suggest its potential generalizability to other domains where zero inflation is a key challenge.</li>
</ul>

<h3>Title: FEDEXCHANGE: Bridging the Domain Gap in Federated Object Detection for Free</h3>
<ul>
<li><strong>Authors: </strong>Haolin Yuan, Jingtao Li, Weiming Zhuang, Chen Chen, Lingjuan Lyu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10503">https://arxiv.org/abs/2509.10503</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10503">https://arxiv.org/pdf/2509.10503</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10503]] FEDEXCHANGE: Bridging the Domain Gap in Federated Object Detection for Free(https://arxiv.org/abs/2509.10503)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Federated Object Detection (FOD) enables clients to collaboratively train a global object detection model without accessing their local data from diverse domains. However, significant variations in environment, weather, and other domain specific factors hinder performance, making cross domain generalization a key challenge. Existing FOD methods often overlook the hardware constraints of edge devices and introduce local training regularizations that incur high computational costs, limiting real-world applicability. In this paper, we propose FEDEXCHANGE, a novel FOD framework that bridges domain gaps without introducing additional local computational overhead. FEDEXCHANGE employs a server side dynamic model exchange strategy that enables each client to gain insights from other clients' domain data without direct data sharing. Specifically, FEDEXCHANGE allows the server to alternate between model aggregation and model exchange. During aggregation rounds, the server aggregates all local models as usual. In exchange rounds, FEDEXCHANGE clusters and exchanges local models based on distance measures, allowing local models to learn from a variety of domains. As all operations are performed on the server side, clients can achieve improved cross domain utility without any additional computational overhead. Extensive evaluations demonstrate that FEDEXCHANGE enhances FOD performance, achieving 1.6X better mean average precision in challenging domains, such as rainy conditions, while requiring only 0.8X the computational resources compared to baseline methods.</li>
</ul>

<h3>Title: AttnBoost: Retail Supply Chain Sales Insights via Gradient Boosting Perspective</h3>
<ul>
<li><strong>Authors: </strong>Muxin Ge, Hanyu Ma, Yiyang Wu, Xiaoli Ma, Yadi Liu, Ye Aung Moe, Weizheng Xie</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10506">https://arxiv.org/abs/2509.10506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10506">https://arxiv.org/pdf/2509.10506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10506]] AttnBoost: Retail Supply Chain Sales Insights via Gradient Boosting Perspective(https://arxiv.org/abs/2509.10506)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability</a></li>
<li><strong>Abstract: </strong>Forecasting product demand in retail supply chains presents a complex challenge due to noisy, heterogeneous features and rapidly shifting consumer behavior. While traditional gradient boosting decision trees (GBDT) offer strong predictive performance on structured data, they often lack adaptive mechanisms to identify and emphasize the most relevant features under changing conditions. In this work, we propose AttnBoost, an interpretable learning framework that integrates feature-level attention into the boosting process to enhance both predictive accuracy and explainability. Specifically, the model dynamically adjusts feature importance during each boosting round via a lightweight attention mechanism, allowing it to focus on high-impact variables such as promotions, pricing, and seasonal trends. We evaluate AttnBoost on a large-scale retail sales dataset and demonstrate that it outperforms standard machine learning and deep tabular models, while also providing actionable insights for supply chain managers. An ablation study confirms the utility of the attention module in mitigating overfitting and improving interpretability. Our results suggest that attention-guided boosting represents a promising direction for interpretable and scalable AI in real-world forecasting applications.</li>
</ul>

<h3>Title: The Anti-Ouroboros Effect: Emergent Resilience in Large Language Models from Recursive Selective Feedback</h3>
<ul>
<li><strong>Authors: </strong>Sai Teja Reddy Adapala</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10509">https://arxiv.org/abs/2509.10509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10509">https://arxiv.org/pdf/2509.10509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10509]] The Anti-Ouroboros Effect: Emergent Resilience in Large Language Models from Recursive Selective Feedback(https://arxiv.org/abs/2509.10509)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>The stability of recursively trained large language models (LLMs) is a foundational problem for AI safety. Prevailing theory predicts model collapse, a progressive degradation when models are trained on their own output. We challenge this narrative by introducing a selective feedback mechanism. Contrary to expectation, instead of merely slowing decay, our experiments provide strong evidence that this pressure reverses it, inducing a statistically significant performance improvement in a Gemma 2B model on a complex summarization task. We name this phenomenon the Anti-Ouroboros Effect. We contrast this with a foundational experiment using a simple classifier, where the theoretical degenerative loop was validated, highlighting the unique dynamics of high-dimensional models. Our findings establish that systemic resilience can be an emergent property of LLMs under simple selection pressure, suggesting a powerful and scalable principle for developing safer and more robust AI systems. Across five generations, a quality-filtered condition improved by 6.6% in ROUGE-L F1 score, whereas an unfiltered control degraded by 3.5% and a random-filter control degraded by 4.2%</li>
</ul>

<h3>Title: LogGuardQ: A Cognitive-Enhanced Reinforcement Learning Framework for Cybersecurity Anomaly Detection in Security Logs</h3>
<ul>
<li><strong>Authors: </strong>Umberto Gonçalves de Sousa</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10511">https://arxiv.org/abs/2509.10511</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10511">https://arxiv.org/pdf/2509.10511</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10511]] LogGuardQ: A Cognitive-Enhanced Reinforcement Learning Framework for Cybersecurity Anomaly Detection in Security Logs(https://arxiv.org/abs/2509.10511)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) has transformed sequential decision-making, but traditional algorithms like Deep Q-Networks (DQNs) and Proximal Policy Optimization (PPO) often struggle with efficient exploration, stability, and adaptability in dynamic environments. This study presents LogGuardQ (Adaptive Log Guard with Cognitive enhancement), a novel framework that integrates a dual-memory system inspired by human cognition and adaptive exploration strategies driven by temperature decay and curiosity. Evaluated on a dataset of 1,000,000 simulated access logs with 47.9% anomalies over 20,000 episodes, LogGuardQ achieves a 96.0% detection rate (versus 93.0% for DQN and 47.1% for PPO), with precision of 0.4776, recall of 0.9996, and an F1-score of 0.6450. The mean reward is 20.34 \pm 44.63 across all episodes (versus 18.80 \pm 43.98 for DQN and -0.17 \pm 23.79 for PPO), with an average of 5.0 steps per episode (constant across models). Graphical analyses, including learning curves smoothed with a Savgol filter (window=501, polynomial=2), variance trends, action distributions, and cumulative detections, demonstrate LogGuardQ's superior stability and efficiency. Statistical tests (Mann-Whitney U) confirm significant performance advantages (e.g., p = 0.0002 vs. DQN with negligible effect size, p < 0.0001 vs. PPO with medium effect size, and p < 0.0001 for DQN vs. PPO with small effect size). By bridging cognitive science and RL, LogGuardQ offers a scalable approach to adaptive learning in uncertain environments, with potential applications in cybersecurity, intrusion detection, and decision-making under uncertainty.</li>
</ul>

<h3>Title: A Service-Oriented Adaptive Hierarchical Incentive Mechanism for Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Jiaxing Cao, Yuzhou Gao, Jiwei Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.GT, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10512">https://arxiv.org/abs/2509.10512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10512">https://arxiv.org/pdf/2509.10512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10512]] A Service-Oriented Adaptive Hierarchical Incentive Mechanism for Federated Learning(https://arxiv.org/abs/2509.10512)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Recently, federated learning (FL) has emerged as a novel framework for distributed model training. In FL, the task publisher (TP) releases tasks, and local model owners (LMOs) use their local data to train models. Sometimes, FL suffers from the lack of training data, and thus workers are recruited for gathering data. To this end, this paper proposes an adaptive incentive mechanism from a service-oriented perspective, with the objective of maximizing the utilities of TP, LMOs and workers. Specifically, a Stackelberg game is theoretically established between the LMOs and TP, positioning TP as the leader and the LMOs as followers. An analytical Nash equilibrium solution is derived to maximize their utilities. The interaction between LMOs and workers is formulated by a multi-agent Markov decision process (MAMDP), with the optimal strategy identified via deep reinforcement learning (DRL). Additionally, an Adaptively Searching the Optimal Strategy Algorithm (ASOSA) is designed to stabilize the strategies of each participant and solve the coupling problems. Extensive numerical experiments are conducted to validate the efficacy of the proposed method.</li>
</ul>

<h3>Title: Mixture-of-Clustered-Experts: Advancing Expert Specialization and Generalization in Instruction Tuning</h3>
<ul>
<li><strong>Authors: </strong>Sugyeong Eo, Jungjun Lee, Chanjun Park, Heuiseok Lim</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10513">https://arxiv.org/abs/2509.10513</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10513">https://arxiv.org/pdf/2509.10513</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10513]] Mixture-of-Clustered-Experts: Advancing Expert Specialization and Generalization in Instruction Tuning(https://arxiv.org/abs/2509.10513)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>A sparse Mixture-of-Experts (MoE) architecture has emerged as a highly scalable solution by conditionally activating sub-modules without a proportional increase in computational costs. However, improving expert specialization to enhance performance and generalization remains a challenge for MoE, especially in instruction tuning scenarios characterized by significant input heterogeneity. In this work, we propose the Mixture-of-Clustered-Experts (MoCE) to address this limitation through a dual-stage routing mechanism. The first stage in the mechanism performs expert group routing based on sequence-level features, while the second stage activates the top-$k$ experts within the group at the token level. This approach enables the effective partitioning of heterogeneous inputs based on their knowledge requirements, encouraging expert group specialization while maintaining the advantages of token-level routing. We evaluate MoCE across a comprehensive set of benchmarks, demonstrating its consistent superiority over strong baselines and its enhanced generalization capabilities. Detailed analysis further highlights the robustness and effectiveness of MoCE.</li>
</ul>

<h3>Title: Adaptive Preference Optimization with Uncertainty-aware Utility Anchor</h3>
<ul>
<li><strong>Authors: </strong>Xiaobo Wang, Zixia Jia, Jiaqi Li, Qi Liu, Zilong Zheng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10515">https://arxiv.org/abs/2509.10515</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10515">https://arxiv.org/pdf/2509.10515</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10515]] Adaptive Preference Optimization with Uncertainty-aware Utility Anchor(https://arxiv.org/abs/2509.10515)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Offline preference optimization methods are efficient for large language models (LLMs) alignment. Direct Preference optimization (DPO)-like learning, one of the most popular approaches, stands out for its efficiency in reward modeling. However, these methods typically follow the convention to use Bradley-Terry (BT) reward modeling that faces several critical assumptions, including the requirement for pairwise training data, model distribution shifting, human rationality assumption, etc. To address these limitations, we propose a general framework for offline preference optimization methods, Adaptive Preference Optimization with Utility Anchor (UAPO), which introduces an anchoring function to estimate the uncertainties brought from preference data annotation. Our method enables training even in scenarios where the data is unpaired, significantly enhancing data utilization efficiency. Moreover, the anchor design makes UAPO more robust in the training process. Experimental results demonstrate that UAPO achieves competitive outcomes without the strict dependency on data pairing, paving the way for more flexible and effective preference optimization methods.</li>
</ul>

<h3>Title: Privacy-Preserving Personalization in Education: A Federated Recommender System for Student Performance Prediction</h3>
<ul>
<li><strong>Authors: </strong>Rodrigo Tertulino</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10516">https://arxiv.org/abs/2509.10516</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10516">https://arxiv.org/pdf/2509.10516</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10516]] Privacy-Preserving Personalization in Education: A Federated Recommender System for Student Performance Prediction(https://arxiv.org/abs/2509.10516)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, robust, federate</a></li>
<li><strong>Abstract: </strong>The increasing digitalization of education presents unprecedented opportunities for data-driven personalization, yet it introduces significant student data privacy challenges. Conventional recommender systems rely on centralized data, a paradigm often incompatible with modern data protection regulations. A novel privacy-preserving recommender system is proposed and evaluated to address this critical issue using Federated Learning (FL). The approach utilizes a Deep Neural Network (DNN) with rich, engineered features from the large-scale ASSISTments educational dataset. A rigorous comparative analysis of federated aggregation strategies was conducted, identifying FedProx as a significantly more stable and effective method for handling heterogeneous student data than the standard FedAvg baseline. The optimized federated model achieves a high-performance F1-Score of 76.28\%, corresponding to 82.85\% of the performance of a powerful, centralized XGBoost model. These findings validate that a federated approach can provide highly effective content recommendations without centralizing sensitive student data. Consequently, our work presents a viable and robust solution to the personalization-privacy dilemma in modern educational platforms.</li>
</ul>

<h3>Title: A Comparative Benchmark of Federated Learning Strategies for Mortality Prediction on Heterogeneous and Imbalanced Clinical Data</h3>
<ul>
<li><strong>Authors: </strong>Rodrigo Tertulino</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10517">https://arxiv.org/abs/2509.10517</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10517">https://arxiv.org/pdf/2509.10517</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10517]] A Comparative Benchmark of Federated Learning Strategies for Mortality Prediction on Heterogeneous and Imbalanced Clinical Data(https://arxiv.org/abs/2509.10517)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>Machine learning models hold significant potential for predicting in-hospital mortality, yet data privacy constraints and the statistical heterogeneity of real-world clinical data often hamper their development. Federated Learning (FL) offers a privacy-preserving solution, but its performance under non-Independent and Identically Distributed (non-IID) and imbalanced conditions requires rigorous investigation. The study presents a comparative benchmark of five federated learning strategies: FedAvg, FedProx, FedAdagrad, FedAdam, and FedCluster for mortality prediction. Using the large-scale MIMIC-IV dataset, we simulate a realistic non-IID environment by partitioning data by clinical care unit. To address the inherent class imbalance of the task, the SMOTE-Tomek technique is applied to each client's local training data. Our experiments, conducted over 50 communication rounds, reveal that the regularization-based strategy, FedProx, consistently outperformed other methods, achieving the highest F1-Score of 0.8831 while maintaining stable convergence. While the baseline FedAvg was the most computationally efficient, its predictive performance was substantially lower. Our findings indicate that regularization-based FL algorithms like FedProx offer a more robust and effective solution for heterogeneous and imbalanced clinical prediction tasks than standard or server-side adaptive aggregation methods. The work provides a crucial empirical benchmark for selecting appropriate FL strategies for real-world healthcare applications.</li>
</ul>

<h3>Title: Holographic Knowledge Manifolds: A Novel Pipeline for Continual Learning Without Catastrophic Forgetting in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Justin Arndt</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10518">https://arxiv.org/abs/2509.10518</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10518">https://arxiv.org/pdf/2509.10518</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10518]] Holographic Knowledge Manifolds: A Novel Pipeline for Continual Learning Without Catastrophic Forgetting in Large Language Models(https://arxiv.org/abs/2509.10518)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce the Holographic Knowledge Manifold (HKM), a four-phase pipeline that achieves zero catastrophic forgetting in AI knowledge representation while maintaining minimal memory growth and high efficiency. Leveraging fractal quantization, probabilistic entanglement, and dynamic diffraction chipping, HKM compresses knowledge substrates by 3x with 67% storage savings, integrates holographically at 100%, and supports over 1,020 updates with 1% growth per increment. In experiments on combined WikiText and FB15k datasets (scaled to 2,997 nodes), we demonstrate industry-leading performance: 0% forgetting (infinite improvement over GEM baselines), 3x compression, and 53% training time reduction on consumer GPU hardware. Hypothetical cost analyses project $92.4M savings over 5 years at petabyte scale, with 21.2% energy reduction and 33% lower carbon footprint. This work hypothesizes a paradigm shift for public large language models (LLMs), enabling "eternal" adaptation without retraining. Future extensions to multimodal fusion and quantum hardware could further democratize scalable AI, potentially reducing fine-tuning costs by 60-80% for models like Llama-3 or Grok-4. Code, datasets, and full results are publicly available for reproducibility.</li>
</ul>

<h3>Title: Gradient Estimation Methods of Approximate Multipliers for High-Accuracy Retraining of Deep Learning Models</h3>
<ul>
<li><strong>Authors: </strong>Chang Meng, Wayne Burleson, Giovanni De Micheli</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10519">https://arxiv.org/abs/2509.10519</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10519">https://arxiv.org/pdf/2509.10519</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10519]] Gradient Estimation Methods of Approximate Multipliers for High-Accuracy Retraining of Deep Learning Models(https://arxiv.org/abs/2509.10519)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Approximate multipliers (AppMults) are widely used in deep learning accelerators to reduce their area, delay, and power consumption. However, AppMults introduce arithmetic errors into deep learning models, necessitating a retraining process to recover accuracy. A key step in retraining is computing the gradient of the AppMult, i.e., the partial derivative of the approximate product with respect to each input operand. Existing approaches typically estimate this gradient using that of the accurate multiplier (AccMult), which can lead to suboptimal retraining results. To address this, we propose two methods to obtain more precise gradients of AppMults. The first, called LUT-2D, characterizes the AppMult gradient with 2-dimensional lookup tables (LUTs), providing fine-grained estimation and achieving the highest retraining accuracy. The second, called LUT-1D, is a compact and more efficient variant that stores gradient values in 1-dimensional LUTs, achieving comparable retraining accuracy with shorter runtime. Experimental results show that on CIFAR-10 with convolutional neural networks, our LUT-2D and LUT-1D methods improve retraining accuracy by 3.83% and 3.72% on average, respectively. On ImageNet with vision transformer models, our LUT-1D method improves retraining accuracy by 23.69% on average, compared to a state-of-the-art retraining framework.</li>
</ul>

<h3>Title: Variational Gaussian Mixture Manifold Models for Client-Specific Federated Personalization</h3>
<ul>
<li><strong>Authors: </strong>Sai Puppala, Ismail Hossain, Md Jahangir Alam, Sajedul Talukder</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10521">https://arxiv.org/abs/2509.10521</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10521">https://arxiv.org/pdf/2509.10521</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10521]] Variational Gaussian Mixture Manifold Models for Client-Specific Federated Personalization(https://arxiv.org/abs/2509.10521)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, federate</a></li>
<li><strong>Abstract: </strong>Personalized federated learning (PFL) often fails under label skew and non-stationarity because a single global parameterization ignores client-specific geometry. We introduce VGM$^2$ (Variational Gaussian Mixture Manifold), a geometry-centric PFL framework that (i) learns client-specific parametric UMAP embeddings, (ii) models latent pairwise distances with mixture relation markers for same and different class pairs, and (iii) exchanges only variational, uncertainty-aware marker statistics. Each client maintains a Dirichlet-Normal-Inverse-Gamma (Dir-NIG) posterior over marker weights, means, and variances; the server aggregates via conjugate moment matching to form global priors that guide subsequent rounds. We prove that this aggregation minimizes the summed reverse Kullback-Leibler divergence from client posteriors within the conjugate family, yielding stability under heterogeneity. We further incorporate a calibration term for distance-to-similarity mapping and report communication and compute budgets. Across eight vision datasets with non-IID label shards, VGM$^2$ achieves competitive or superior test F1 scores compared to strong baselines while communicating only small geometry summaries. Privacy is strengthened through secure aggregation and optional differential privacy noise, and we provide a membership-inference stress test. Code and configurations will be released to ensure full reproducibility.</li>
</ul>

<h3>Title: Multimodal Deep Learning for ATCO Command Lifecycle Modeling and Workload Prediction</h3>
<ul>
<li><strong>Authors: </strong>Kaizhen Tan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10522">https://arxiv.org/abs/2509.10522</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10522">https://arxiv.org/pdf/2509.10522</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10522]] Multimodal Deep Learning for ATCO Command Lifecycle Modeling and Workload Prediction(https://arxiv.org/abs/2509.10522)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Air traffic controllers (ATCOs) issue high-intensity voice commands in dense airspace, where accurate workload modeling is critical for safety and efficiency. This paper proposes a multimodal deep learning framework that integrates structured data, trajectory sequences, and image features to estimate two key parameters in the ATCO command lifecycle: the time offset between a command and the resulting aircraft maneuver, and the command duration. A high-quality dataset was constructed, with maneuver points detected using sliding window and histogram-based methods. A CNN-Transformer ensemble model was developed for accurate, generalizable, and interpretable predictions. By linking trajectories to voice commands, this work offers the first model of its kind to support intelligent command generation and provides practical value for workload assessment, staffing, and scheduling.</li>
</ul>

<h3>Title: From Predictions to Explanations: Explainable AI for Autism Diagnosis and Identification of Critical Brain Regions</h3>
<ul>
<li><strong>Authors: </strong>Kush Gupta, Amir Aly, Emmanuel Ifeachor, Rohit Shankar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10523">https://arxiv.org/abs/2509.10523</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10523">https://arxiv.org/pdf/2509.10523</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10523]] From Predictions to Explanations: Explainable AI for Autism Diagnosis and Identification of Critical Brain Regions(https://arxiv.org/abs/2509.10523)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Autism spectrum disorder (ASD) is a neurodevelopmental condition characterized by atypical brain maturation. However, the adaptation of transfer learning paradigms in machine learning for ASD research remains notably limited. In this study, we propose a computer-aided diagnostic framework with two modules. This chapter presents a two-module framework combining deep learning and explainable AI for ASD diagnosis. The first module leverages a deep learning model fine-tuned through cross-domain transfer learning for ASD classification. The second module focuses on interpreting the model decisions and identifying critical brain regions. To achieve this, we employed three explainable AI (XAI) techniques: saliency mapping, Gradient-weighted Class Activation Mapping, and SHapley Additive exPlanations (SHAP) analysis. This framework demonstrates that cross-domain transfer learning can effectively address data scarcity in ASD research. In addition, by applying three established explainability techniques, the approach reveals how the model makes diagnostic decisions and identifies brain regions most associated with ASD. These findings were compared against established neurobiological evidence, highlighting strong alignment and reinforcing the clinical relevance of the proposed approach.</li>
</ul>

<h3>Title: Mitigating Catastrophic Forgetting and Mode Collapse in Text-to-Image Diffusion via Latent Replay</h3>
<ul>
<li><strong>Authors: </strong>Aoi Otani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10529">https://arxiv.org/abs/2509.10529</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10529">https://arxiv.org/pdf/2509.10529</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10529]] Mitigating Catastrophic Forgetting and Mode Collapse in Text-to-Image Diffusion via Latent Replay(https://arxiv.org/abs/2509.10529)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Continual learning -- the ability to acquire knowledge incrementally without forgetting previous skills -- is fundamental to natural intelligence. While the human brain excels at this, artificial neural networks struggle with "catastrophic forgetting," where learning new tasks erases previously acquired knowledge. This challenge is particularly severe for text-to-image diffusion models, which generate images from textual prompts. Additionally, these models face "mode collapse," where their outputs become increasingly repetitive over time. To address these challenges, we apply Latent Replay, a neuroscience-inspired approach, to diffusion models. Traditional replay methods mitigate forgetting by storing and revisiting past examples, typically requiring large collections of images. Latent Replay instead retains only compact, high-level feature representations extracted from the model's internal architecture. This mirrors the hippocampal process of storing neural activity patterns rather than raw sensory inputs, reducing memory usage while preserving critical information. Through experiments with five sequentially learned visual concepts, we demonstrate that Latent Replay significantly outperforms existing methods in maintaining model versatility. After learning all concepts, our approach retained 77.59% Image Alignment (IA) on the earliest concept, 14% higher than baseline methods, while maintaining diverse outputs. Surprisingly, random selection of stored latent examples outperforms similarity-based strategies. Our findings suggest that Latent Replay enables efficient continual learning for generative AI models, paving the way for personalized text-to-image models that evolve with user needs without excessive computational costs.</li>
</ul>

<h3>Title: Dynamic Adaptive Shared Experts with Grouped Multi-Head Attention Mixture of Experts</h3>
<ul>
<li><strong>Authors: </strong>Cheng Li, Jiexiong Liu, Yixuan Chen, Jie ji</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10530">https://arxiv.org/abs/2509.10530</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10530">https://arxiv.org/pdf/2509.10530</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10530]] Dynamic Adaptive Shared Experts with Grouped Multi-Head Attention Mixture of Experts(https://arxiv.org/abs/2509.10530)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformer models based on the Mixture of Experts (MoE) architecture have made significant progress in long-sequence modeling, but existing models still have shortcomings in computational efficiency and the ability to capture long-range dependencies, especially in terms of the dynamic adaptability of expert resource allocation. In this paper, we propose a Dynamic Adaptive Shared Expert and Grouped Multi-Head Attention Hybrid Model (DASG-MoE) to enhance long-sequence modeling capabilities by integrating three modules. First, we employ the Grouped Multi-Head Attention (GMHA) mechanism to effectively reduce the computational complexity of long sequences. By parallel processing through sequence grouping, local sliding window attention, and feature aggregation, we address long-range dependency issues and the model's lack of generalization for local information. Second, we design a Dual-Scale Shared Expert Structure (DSSE), where shallow experts use lightweight computations to quickly respond to low-dimensional features, while deep experts process high-dimensional complex semantics through pre-training transfer and post-training optimization, achieving a dynamic balance between efficiency and accuracy. Third, we propose a hierarchical Adaptive Dynamic Routing (ADR) mechanism that dynamically selects expert levels based on feature complexity and task requirements, and optimizes resource allocation through a local expert activation strategy. Experiments on multiple long-sequence benchmark datasets demonstrate that our DASG-MoE model outperforms state-of-the-art models.</li>
</ul>

<h3>Title: Decoupling the "What" and "Where" With Polar Coordinate Positional Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Anand Gopalakrishnan, Robert Csordás, Jürgen Schmidhuber, Michael C. Mozer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10534">https://arxiv.org/abs/2509.10534</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10534">https://arxiv.org/pdf/2509.10534</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10534]] Decoupling the "What" and "Where" With Polar Coordinate Positional Embeddings(https://arxiv.org/abs/2509.10534)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The attention mechanism in a Transformer architecture matches key to query based on both content -- the what -- and position in a sequence -- the where. We present an analysis indicating that what and where are entangled in the popular RoPE rotary position embedding. This entanglement can impair performance particularly when decisions require independent matches on these two factors. We propose an improvement to RoPE, which we call Polar Coordinate Position Embeddings or PoPE, that eliminates the what-where confound. PoPE is far superior on a diagnostic task requiring indexing solely by position or by content. On autoregressive sequence modeling in music, genomic, and natural language domains, Transformers using PoPE as the positional encoding scheme outperform baselines using RoPE with respect to evaluation loss (perplexity) and downstream task performance. On language modeling, these gains persist across model scale, from 124M to 774M parameters. Crucially, PoPE shows strong zero-shot length extrapolation capabilities, whereas RoPE's performance degrades significantly on longer sequences at test time without fine tuning or the use of position-interpolation methods.</li>
</ul>

<h3>Title: Semantic-guided LoRA Parameters Generation</h3>
<ul>
<li><strong>Authors: </strong>Miaoge Li, Yang Chen, Zhijie Rao, Can Jiang, Jingcai Guo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10535">https://arxiv.org/abs/2509.10535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10535">https://arxiv.org/pdf/2509.10535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10535]] Semantic-guided LoRA Parameters Generation(https://arxiv.org/abs/2509.10535)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Low-Rank Adaptation (LoRA) has demonstrated strong generalization capabilities across a variety of tasks for efficiently fine-tuning AI models, especially on resource-constrained edges. However, in real-world applications, edge users often exhibit task-specific preferences that are difficult to handle with a unified model trained under a closed-world assumption, and the challenge may further increase when there are significant domain shifts between training and deployment. Meanwhile, retraining/fine-tuning models for each user is also impractical due to its cost-intensive nature and privacy concerns over raw data utilization from edges. To address these challenges, we propose Semantic-guided LoRA Parameter Generation (SG-LoRA), the first of its kind framework to efficiently produce user-specific LoRA parameters without any additional training on user tasks or access to user-specific data. Concretely, SG-LoRA uses task descriptions as the semantic bridge, measuring their proximity to a set of known expert tasks in a shared embedding space. Based on this semantic guidance, it models the target task's LoRA parameter distribution to generate high-performing parameters for novel tasks. SG-LoRA enables the real-time construction of LoRA models aligned with individual intents by distilling knowledge from prominent LoRA experts and, meanwhile, offering a privacy-preserving solution for personalized model adaptation in a novel zero-shot open-world setting proposed in this work. Extensive experiments on multiple challenging tasks confirm the superior performance and remarkable adaptability of SG-LoRA. Code is available at this https URL.</li>
</ul>

<h3>Title: On Using Large-Batches in Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Sahil Tyagi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10537">https://arxiv.org/abs/2509.10537</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10537">https://arxiv.org/pdf/2509.10537</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10537]] On Using Large-Batches in Federated Learning(https://arxiv.org/abs/2509.10537)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Efficient Federated learning (FL) is crucial for training deep networks over devices with limited compute resources and bounded networks. With the advent of big data, devices either generate or collect multimodal data to train either generic or local-context aware networks, particularly when data privacy and locality is vital. FL algorithms generally trade-off between parallel and statistical performance, improving model quality at the cost of higher communication frequency, or vice versa. Under frequent synchronization settings, FL over a large cluster of devices may perform more work per-training iteration by processing a larger global batch-size, thus attaining considerable training speedup. However, this may result in poor test performance (i.e., low test loss or accuracy) due to generalization degradation issues associated with large-batch training. To address these challenges with large-batches, this work proposes our vision of exploiting the trade-offs between small and large-batch training, and explore new directions to enjoy both the parallel scaling of large-batches and good generalizability of small-batch training. For the same number of iterations, we observe that our proposed large-batch training technique attains about 32.33% and 3.74% higher test accuracy than small-batch training in ResNet50 and VGG11 models respectively.</li>
</ul>

<h3>Title: DualAlign: Generating Clinically Grounded Synthetic Data</h3>
<ul>
<li><strong>Authors: </strong>Rumeng Li, Xun Wang, Hong Yu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10538">https://arxiv.org/abs/2509.10538</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10538">https://arxiv.org/pdf/2509.10538</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10538]] DualAlign: Generating Clinically Grounded Synthetic Data(https://arxiv.org/abs/2509.10538)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Synthetic clinical data are increasingly important for advancing AI in healthcare, given strict privacy constraints on real-world EHRs, limited availability of annotated rare-condition data, and systemic biases in observational datasets. While large language models (LLMs) can generate fluent clinical text, producing synthetic data that is both realistic and clinically meaningful remains challenging. We introduce DualAlign, a framework that enhances statistical fidelity and clinical plausibility through dual alignment: (1) statistical alignment, which conditions generation on patient demographics and risk factors; and (2) semantic alignment, which incorporates real-world symptom trajectories to guide content generation. Using Alzheimer's disease (AD) as a case study, DualAlign produces context-grounded symptom-level sentences that better reflect real-world clinical documentation. Fine-tuning an LLaMA 3.1-8B model with a combination of DualAlign-generated and human-annotated data yields substantial performance gains over models trained on gold data alone or unguided synthetic baselines. While DualAlign does not fully capture longitudinal complexity, it offers a practical approach for generating clinically grounded, privacy-preserving synthetic data to support low-resource clinical text analysis.</li>
</ul>

<h3>Title: EchoLeak: The First Real-World Zero-Click Prompt Injection Exploit in a Production LLM System</h3>
<ul>
<li><strong>Authors: </strong>Pavan Reddy, Aditya Sanjay Gujral</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10540">https://arxiv.org/abs/2509.10540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10540">https://arxiv.org/pdf/2509.10540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10540]] EchoLeak: The First Real-World Zero-Click Prompt Injection Exploit in a Production LLM System(https://arxiv.org/abs/2509.10540)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense, large language model</a></li>
<li><strong>Abstract: </strong>Large language model (LLM) assistants are increasingly integrated into enterprise workflows, raising new security concerns as they bridge internal and external data sources. This paper presents an in-depth case study of EchoLeak (CVE-2025-32711), a zero-click prompt injection vulnerability in Microsoft 365 Copilot that enabled remote, unauthenticated data exfiltration via a single crafted email. By chaining multiple bypasses-evading Microsofts XPIA (Cross Prompt Injection Attempt) classifier, circumventing link redaction with reference-style Markdown, exploiting auto-fetched images, and abusing a Microsoft Teams proxy allowed by the content security policy-EchoLeak achieved full privilege escalation across LLM trust boundaries without user interaction. We analyze why existing defenses failed, and outline a set of engineering mitigations including prompt partitioning, enhanced input/output filtering, provenance-based access control, and strict content security policies. Beyond the specific exploit, we derive generalizable lessons for building secure AI copilots, emphasizing the principle of least privilege, defense-in-depth architectures, and continuous adversarial testing. Our findings establish prompt injection as a practical, high-severity vulnerability class in production AI systems and provide a blueprint for defending against future AI-native threats.</li>
</ul>

<h3>Title: Robust DDoS-Attack Classification with 3D CNNs Against Adversarial Methods</h3>
<ul>
<li><strong>Authors: </strong>Landon Bragg, Nathan Dorsey, Josh Prior, John Ajit, Ben Kim, Nate Willis, Pablo Rivas</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10543">https://arxiv.org/abs/2509.10543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10543">https://arxiv.org/pdf/2509.10543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10543]] Robust DDoS-Attack Classification with 3D CNNs Against Adversarial Methods(https://arxiv.org/abs/2509.10543)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Distributed Denial-of-Service (DDoS) attacks remain a serious threat to online infrastructure, often bypassing detection by altering traffic in subtle ways. We present a method using hive-plot sequences of network data and a 3D convolutional neural network (3D CNN) to classify DDoS traffic with high accuracy. Our system relies on three main ideas: (1) using spatio-temporal hive-plot encodings to set a pattern-recognition baseline, (2) applying adversarial training with FGSM and PGD alongside spatial noise and image shifts, and (3) analyzing frame-wise predictions to find early signals. On a benchmark dataset, our method lifts adversarial accuracy from 50-55% to over 93% while maintaining clean-sample performance. Frames 3-4 offer strong predictive signals, showing early-stage classification is possible.</li>
</ul>

<h3>Title: Decentralized Identity Management on Ripple: A Conceptual Framework for High-Speed, Low-Cost Identity Transactions in Attestation-Based Attribute-Based Identity</h3>
<ul>
<li><strong>Authors: </strong>Ruwanga Konara, Kasun De Zoysa, Asanka Sayakkara</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10545">https://arxiv.org/abs/2509.10545</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10545">https://arxiv.org/pdf/2509.10545</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10545]] Decentralized Identity Management on Ripple: A Conceptual Framework for High-Speed, Low-Cost Identity Transactions in Attestation-Based Attribute-Based Identity(https://arxiv.org/abs/2509.10545)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>Recent years have seen many industrial implementations and much scholastic research, i.e., prototypes and theoretical frameworks, in Decentralized Identity Management Systems (DIDMS). It is safe to say that Attestation-Based Attribute-Based Decentralized IDM (ABABDIDM) has not received anywhere near the same level of attention in the literature as general Attribute-Based DIDMs (ABDIDM), i.e, decentralized Attribute-Based Access Control (ABAC). The use of decentralization, i.e., DIDM, is to improve upon the security and privacy-related issues of centralized Identity Management Systems (IDM) and Attribute-Based IDMs (ABIDM). And blockchain is the framework used for decentralization in all these schemes. Many DIDMs - even ABDIDMs - have been defined on popular blockchains such as Hyperledger, Ethereum, and Bitcoin. However, despite the characteristics of Ripple that makes it appealing for an ABIDM, there is a lack of research to develop an Identity Management System (IDMS) on Ripple in literature. We have attempted to conceptualize an ABABDIDM on Ripple.</li>
</ul>

<h3>Title: Uncovering the Vulnerability of Large Language Models in the Financial Domain via Risk Concealment</h3>
<ul>
<li><strong>Authors: </strong>Gang Cheng, Haibo Jin, Wenbin Zhang, Haohan Wang, Jun Zhuang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10546">https://arxiv.org/abs/2509.10546</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10546">https://arxiv.org/pdf/2509.10546</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10546]] Uncovering the Vulnerability of Large Language Models in the Financial Domain via Risk Concealment(https://arxiv.org/abs/2509.10546)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly integrated into financial applications, yet existing red-teaming research primarily targets harmful content, largely neglecting regulatory risks. In this work, we aim to investigate the vulnerability of financial LLMs through red-teaming approaches. We introduce Risk-Concealment Attacks (RCA), a novel multi-turn framework that iteratively conceals regulatory risks to provoke seemingly compliant yet regulatory-violating responses from LLMs. To enable systematic evaluation, we construct FIN-Bench, a domain-specific benchmark for assessing LLM safety in financial contexts. Extensive experiments on FIN-Bench demonstrate that RCA effectively bypasses nine mainstream LLMs, achieving an average attack success rate (ASR) of 93.18%, including 98.28% on GPT-4.1 and 97.56% on OpenAI o1. These findings reveal a critical gap in current alignment techniques and underscore the urgent need for stronger moderation mechanisms in financial domains. We hope this work offers practical insights for advancing robust and domain-aware LLM alignment.</li>
</ul>

<h3>Title: A Hybrid Encryption Framework Combining Classical, Post-Quantum, and QKD Methods</h3>
<ul>
<li><strong>Authors: </strong>Amal Raj, Vivek Balachandran</a></li>
<li><strong>Subjects: </strong>cs.CR, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10551">https://arxiv.org/abs/2509.10551</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10551">https://arxiv.org/pdf/2509.10551</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10551]] A Hybrid Encryption Framework Combining Classical, Post-Quantum, and QKD Methods(https://arxiv.org/abs/2509.10551)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect</a></li>
<li><strong>Abstract: </strong>This paper introduces a hybrid encryption framework combining classical cryptography (EdDSA, ECDH), post-quantum cryptography (ML-DSA-6x5, ML-KEM-768), and Quantum Key Distribution (QKD) via Guardian to counter quantum computing threats. Our prototype implements this integration, using a key derivation function to generate secure symmetric and HMAC keys, and evaluates its performance across execution time and network metrics. The approach improves data protection by merging classical efficiency with PQC's quantum resilience and QKD's key security, offering a practical transition path for cryptographic systems. This research lays the foundation for future adoption of PQC in securing digital communication.</li>
</ul>

<h3>Title: GTS_Forecaster: a novel deep learning based geodetic time series forecasting toolbox with python</h3>
<ul>
<li><strong>Authors: </strong>Xuechen Liang, Xiaoxing He, Shengdao Wang, Jean-Philippe Montillet, Zhengkai Huang, Gaël Kermarrec, Shunqiang Hu, Yu Zhou, Jiahui Huang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10560">https://arxiv.org/abs/2509.10560</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10560">https://arxiv.org/pdf/2509.10560</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10560]] GTS_Forecaster: a novel deep learning based geodetic time series forecasting toolbox with python(https://arxiv.org/abs/2509.10560)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Geodetic time series -- such as Global Navigation Satellite System (GNSS) positions, satellite altimetry-derived sea surface height (SSH), and tide gauge (TG) records -- is essential for monitoring surface deformation and sea level change. Accurate forecasts of these variables can enhance early warning systems and support hazard mitigation for earthquakes, landslides, coastal storm surge, and long-term sea level. However, the nonlinear, non-stationary, and incomplete nature of such variables presents significant challenges for classic models, which often fail to capture long-term dependencies and complex spatiotemporal dynamics. We introduce GTS Forecaster, an open-source Python package for geodetic time series forecasting. It integrates advanced deep learning models -- including kernel attention networks (KAN), graph neural network-based gated recurrent units (GNNGRU), and time-aware graph neural networks (TimeGNN) -- to effectively model nonlinear spatial-temporal patterns. The package also provides robust preprocessing tools, including outlier detection and a reinforcement learning-based gap-filling algorithm, the Kalman-TransFusion Interpolation Framework (KTIF). GTS Forecaster currently supports forecasting, visualization, and evaluation of GNSS, SSH, and TG datasets, and is adaptable to general time series applications. By combining cutting-edge models with an accessible interface, it facilitates the application of deep learning in geodetic forecasting tasks.</li>
</ul>

<h3>Title: AVEC: Bootstrapping Privacy for Local LLMs</h3>
<ul>
<li><strong>Authors: </strong>Madhava Gaikwad</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10561">https://arxiv.org/abs/2509.10561</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10561">https://arxiv.org/pdf/2509.10561</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10561]] AVEC: Bootstrapping Privacy for Local LLMs(https://arxiv.org/abs/2509.10561)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>This position paper presents AVEC (Adaptive Verifiable Edge Control), a framework for bootstrapping privacy for local language models by enforcing privacy at the edge with explicit verifiability for delegated queries. AVEC introduces an adaptive budgeting algorithm that allocates per-query differential privacy parameters based on sensitivity, local confidence, and historical usage, and uses verifiable transformation with on-device integrity checks. We formalize guarantees using Rényi differential privacy with odometer-based accounting, and establish utility ceilings, delegation-leakage bounds, and impossibility results for deterministic gating and hash-only certification. Our evaluation is simulation-based by design to study mechanism behavior and accounting; we do not claim deployment readiness or task-level utility with live LLMs. The contribution is a conceptual architecture and theoretical foundation that chart a pathway for empirical follow-up on privately bootstrapping local LLMs.</li>
</ul>

<h3>Title: Enhancing IoMT Security with Explainable Machine Learning: A Case Study on the CICIOMT2024 Dataset</h3>
<ul>
<li><strong>Authors: </strong>Mohammed Yacoubi, Omar Moussaoui, C. Drocourt (UPJV, MIS)</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10563">https://arxiv.org/abs/2509.10563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10563">https://arxiv.org/pdf/2509.10563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10563]] Enhancing IoMT Security with Explainable Machine Learning: A Case Study on the CICIOMT2024 Dataset(https://arxiv.org/abs/2509.10563)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack, interpretability</a></li>
<li><strong>Abstract: </strong>Explainable Artificial Intelligence (XAI) enhances the transparency and interpretability of AI models, addressing their inherent opacity. In cybersecurity, particularly within the Internet of Medical Things (IoMT), the black-box nature of AI-driven threat detection poses a significant challenge. Cybersecurity professionals must not only detect attacks but also understand the reasoning behind AI decisions to ensure trust and accountability. The rapid increase in cyberattacks targeting connected medical devices threatens patient safety and data privacy, necessitating advanced AI-driven solutions. This study compares two ensemble learning techniques, bagging and boosting, for cyber-attack classification in IoMT environments. We selected Random Forest for bagging and CatBoost for boosting. Random Forest helps reduce variance, while CatBoost improves bias by combining weak classifiers into a strong ensemble model, making them effective for detecting sophisticated attacks. However, their complexity often reduces transparency, making it difficult for cybersecurity professionals to interpret and trust their decisions. To address this issue, we apply XAI models to generate local and global explanations, providing insights into AI decision-making. Using techniques like SHAP (Shapley Additive Explanations) and LIME (Local Interpretable Model-agnostic Explanations), we highlight feature importance to help stakeholders understand the key factors driving cyber threat detection.</li>
</ul>

<h3>Title: SG-ML: Smart Grid Cyber Range Modelling Language</h3>
<ul>
<li><strong>Authors: </strong>Muhammad M. Roomi, Suhail S.M. Hussain, Daisuke Mashima</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10568">https://arxiv.org/abs/2509.10568</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10568">https://arxiv.org/pdf/2509.10568</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10568]] SG-ML: Smart Grid Cyber Range Modelling Language(https://arxiv.org/abs/2509.10568)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>This work provides a detailed specification of the Smart Grid Modelling Language (SG-ML), which is designed for the automated generation of smart grid cyber ranges. SG-ML is defined as a set of XML schemas that describe a smart grid's configuration in both machine-readable and human-friendly ways, thereby bridging the gap between system modelling and automated deployment. Unlike prior ad-hoc approaches to cyber range design, SG-ML provides a unified methodology that integrates both power system and cyber network representations. The SG-ML model can be customized by users to meet specific requirements, such as emulating physical or cyber topologies and configuring network devices. An SG-ML Processor then parses this configured model to instantiate the cyber range environment. The modelling language leverages established standards like the IEC 61850 Substation Configuration Language (SCL) and IEC 61131 PLCopen XML to define power system topology, cyber network topology, and device configurations. This approach allows for the reuse of existing assets, reducing the effort needed to create the SG-ML model. To address gaps not covered by these standards such as attack injection parameters, scenario-specific metadata, and additional network constraints, SG-ML introduces proprietary schemas that complement standard models. Overall, SG-ML enables reproducible, scalable, and automated generation of realistic smart grid cyber ranges for research, training, and security assessment.</li>
</ul>

<h3>Title: MarkDiffusion: An Open-Source Toolkit for Generative Watermarking of Latent Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Leyi Pan, Sheng Guan, Zheyu Fu, Luyang Si, Zian Wang, Xuming Hu, Irwin King, Philip S. Yu, Aiwei Liu, Lijie Wen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10569">https://arxiv.org/abs/2509.10569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10569">https://arxiv.org/pdf/2509.10569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10569]] MarkDiffusion: An Open-Source Toolkit for Generative Watermarking of Latent Diffusion Models(https://arxiv.org/abs/2509.10569)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, watermark, diffusion, generative</a></li>
<li><strong>Abstract: </strong>We introduce MarkDiffusion, an open-source Python toolkit for generative watermarking of latent diffusion models. It comprises three key components: a unified implementation framework for streamlined watermarking algorithm integrations and user-friendly interfaces; a mechanism visualization suite that intuitively showcases added and extracted watermark patterns to aid public understanding; and a comprehensive evaluation module offering standard implementations of 24 tools across three essential aspects - detectability, robustness, and output quality - plus 8 automated evaluation pipelines. Through MarkDiffusion, we seek to assist researchers, enhance public awareness and engagement in generative watermarking, and promote consensus while advancing research and applications.</li>
</ul>

<h3>Title: The Coding Limits of Robust Watermarking for Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Danilo Francati, Yevin Nikhel Goonatilake, Shubham Pawar, Daniele Venturi, Giuseppe Ateniese</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10577">https://arxiv.org/abs/2509.10577</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10577">https://arxiv.org/pdf/2509.10577</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10577]] The Coding Limits of Robust Watermarking for Generative Models(https://arxiv.org/abs/2509.10577)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, watermark, generative</a></li>
<li><strong>Abstract: </strong>We prove a sharp threshold for the robustness of cryptographic watermarking for generative models. This is achieved by introducing a coding abstraction, which we call messageless secret-key codes, that formalizes sufficient and necessary requirements of robust watermarking: soundness, tamper detection, and pseudorandomness. Thus, we establish that robustness has a precise limit: For binary outputs no scheme can survive if more than half of the encoded bits are modified, and for an alphabet of size q the corresponding threshold is $(1-1/q)$ of the symbols. Complementing this impossibility, we give explicit constructions that meet the bound up to a constant slack. For every ${\delta} > 0$, assuming pseudorandom functions and access to a public counter, we build linear-time codes that tolerate up to $(1/2)(1-{\delta})$ errors in the binary case and $(1-1/q)(1-{\delta})$ errors in the $q$-ary case. Together with the lower bound, these yield the maximum robustness achievable under standard cryptographic assumptions. We then test experimentally whether this limit appears in practice by looking at the recent watermarking for images of Gunn, Zhao, and Song (ICLR 2025). We show that a simple crop and resize operation reliably flipped about half of the latent signs and consistently prevented belief-propagation decoding from recovering the codeword, erasing the watermark while leaving the image visually intact. These results provide a complete characterization of robust watermarking, identifying the threshold at which robustness fails, constructions that achieve it, and an experimental confirmation that the threshold is already reached in practice.</li>
</ul>

<h3>Title: Multi-channel secure communication framework for wireless IoT (MCSC-WoT): enhancing security in Internet of Things</h3>
<ul>
<li><strong>Authors: </strong>Prokash Barman, Ratul Chowdhury, Banani Saha</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10581">https://arxiv.org/abs/2509.10581</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10581">https://arxiv.org/pdf/2509.10581</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10581]] Multi-channel secure communication framework for wireless IoT (MCSC-WoT): enhancing security in Internet of Things(https://arxiv.org/abs/2509.10581)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect, attack</a></li>
<li><strong>Abstract: </strong>In modern smart systems, the convergence of the Internet of Things (IoT) and Wireless of Things (WoT) have been revolutionized by offering a broad level of wireless connectivity and communication among various devices. Hitherto, this greater interconnectivity poses important security problems, including the question of how to securely interconnect different networks, preserve secure communication channels, and maintain data integrity. However, the traditional cryptographic method and frequency hopping technique, although they provide some protection, are not sufficient to defend against Man-In-The-Middle, jamming, and replay attacks. In addition, synchronization issues in multi-channel communication systems result in increased latency and energy consumption, which make them unsuitable for resource-constrained IoT and WoT devices. This work presents the Multi-Channel Secure Communication (MCSC) framework, which integrates advanced cryptographic protocols with dynamic channel-hopping strategies to enhance security with reduced synchronization overhead. The MCSC framework maximizes the critical performance metrics, such as packet delivery ratio, latency, throughput, and energy efficiency, and fulfills the specific requirements of the IoT and WoT networks. A comprehensive comparison of MCSC with well-established methods, including Frequency Hop Spread Spectrum, single channel Advanced Encryption Standard, and various Elliptic Curve Cryptography-based schemes, indicates that MCSC has lower error rates and is more resilient to a wider range of cyber attacks. The efficiency of the proposed solution to secure IoT and WoT networks without compromising the operational performance is validated under various interference conditions.</li>
</ul>

<h3>Title: SME-TEAM: Leveraging Trust and Ethics for Secure and Responsible Use of AI and LLMs in SMEs</h3>
<ul>
<li><strong>Authors: </strong>Iqbal H. Sarker, Helge Janicke, Ahmad Mohsin, Leandros Maglaras</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10594">https://arxiv.org/abs/2509.10594</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10594">https://arxiv.org/pdf/2509.10594</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10594]] SME-TEAM: Leveraging Trust and Ethics for Secure and Responsible Use of AI and LLMs in SMEs(https://arxiv.org/abs/2509.10594)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, large language model</a></li>
<li><strong>Abstract: </strong>Artificial Intelligence (AI) and Large Language Models (LLMs) are reshaping today's business practices, however, their adoption within small and medium-sized enterprises (SMEs) raises significant technical, ethical and trust issues. This paper proposes a structured, multi-phased framework designed to embed trust and ethical principles throughout the AI lifecycle for their secure and responsible use in SMEs. Structured around four pillars, i.e., Data, Algorithms, Human oversight, and Model Architecture, the framework bridges theoretical ethical principles with operational practice, enhancing AI capabilities in diverse SME applications. Ultimately, this paper offers a structured roadmap for responsible AI adoption, framing trust and ethics as a catalyst for resilience, competitiveness, and sustainable innovation in SMEs.</li>
</ul>

<h3>Title: pySigLib - Fast Signature-Based Computations on CPU and GPU</h3>
<ul>
<li><strong>Authors: </strong>Daniil Shmelev, Cristopher Salvi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.MS, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10613">https://arxiv.org/abs/2509.10613</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10613">https://arxiv.org/pdf/2509.10613</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10613]] pySigLib - Fast Signature-Based Computations on CPU and GPU(https://arxiv.org/abs/2509.10613)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Signature-based methods have recently gained significant traction in machine learning for sequential data. In particular, signature kernels have emerged as powerful discriminators and training losses for generative models on time-series, notably in quantitative finance. However, existing implementations do not scale to the dataset sizes and sequence lengths encountered in practice. We present pySigLib, a high-performance Python library offering optimised implementations of signatures and signature kernels on CPU and GPU, fully compatible with PyTorch's automatic differentiation. Beyond an efficient software stack for large-scale signature-based computation, we introduce a novel differentiation scheme for signature kernels that delivers accurate gradients at a fraction of the runtime of existing libraries.</li>
</ul>

<h3>Title: Building a General SimCLR Self-Supervised Foundation Model Across Neurological Diseases to Advance 3D Brain MRI Diagnoses</h3>
<ul>
<li><strong>Authors: </strong>Emily Kaczmarek, Justin Szeto, Brennan Nichyporuk, Tal Arbel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10620">https://arxiv.org/abs/2509.10620</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10620">https://arxiv.org/pdf/2509.10620</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10620]] Building a General SimCLR Self-Supervised Foundation Model Across Neurological Diseases to Advance 3D Brain MRI Diagnoses(https://arxiv.org/abs/2509.10620)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>3D structural Magnetic Resonance Imaging (MRI) brain scans are commonly acquired in clinical settings to monitor a wide range of neurological conditions, including neurodegenerative disorders and stroke. While deep learning models have shown promising results analyzing 3D MRI across a number of brain imaging tasks, most are highly tailored for specific tasks with limited labeled data, and are not able to generalize across tasks and/or populations. The development of self-supervised learning (SSL) has enabled the creation of large medical foundation models that leverage diverse, unlabeled datasets ranging from healthy to diseased data, showing significant success in 2D medical imaging applications. However, even the very few foundation models for 3D brain MRI that have been developed remain limited in resolution, scope, or accessibility. In this work, we present a general, high-resolution SimCLR-based SSL foundation model for 3D brain structural MRI, pre-trained on 18,759 patients (44,958 scans) from 11 publicly available datasets spanning diverse neurological diseases. We compare our model to Masked Autoencoders (MAE), as well as two supervised baselines, on four diverse downstream prediction tasks in both in-distribution and out-of-distribution settings. Our fine-tuned SimCLR model outperforms all other models across all tasks. Notably, our model still achieves superior performance when fine-tuned using only 20% of labeled training samples for predicting Alzheimer's disease. We use publicly available code and data, and release our trained model at this https URL, contributing a broadly applicable and accessible foundation model for clinical brain MRI analysis.</li>
</ul>

<h3>Title: No Answer Needed: Predicting LLM Answer Accuracy from Question-Only Linear Probes</h3>
<ul>
<li><strong>Authors: </strong>Iván Vicente Moreno Cencerrado, Arnau Padrés Masdemont, Anton Gonzalvez Hawthorne, David Demitri Africa, Lorenzo Pacchiardi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10625">https://arxiv.org/abs/2509.10625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10625">https://arxiv.org/pdf/2509.10625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10625]] No Answer Needed: Predicting LLM Answer Accuracy from Question-Only Linear Probes(https://arxiv.org/abs/2509.10625)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Do large language models (LLMs) anticipate when they will answer correctly? To study this, we extract activations after a question is read but before any tokens are generated, and train linear probes to predict whether the model's forthcoming answer will be correct. Across three open-source model families ranging from 7 to 70 billion parameters, projections on this "in-advance correctness direction" trained on generic trivia questions predict success in distribution and on diverse out-of-distribution knowledge datasets, outperforming black-box baselines and verbalised predicted confidence. Predictive power saturates in intermediate layers, suggesting that self-assessment emerges mid-computation. Notably, generalisation falters on questions requiring mathematical reasoning. Moreover, for models responding "I don't know", doing so strongly correlates with the probe score, indicating that the same direction also captures confidence. By complementing previous results on truthfulness and other behaviours obtained with probes and sparse auto-encoders, our work contributes essential findings to elucidate LLM internals.</li>
</ul>

<h3>Title: Interpretable neural network system identification method for two families of second-order systems based on characteristic curves</h3>
<ul>
<li><strong>Authors: </strong>Federico J. Gonzalez, Luis P. Lara</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10632">https://arxiv.org/abs/2509.10632</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10632">https://arxiv.org/pdf/2509.10632</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10632]] Interpretable neural network system identification method for two families of second-order systems based on characteristic curves(https://arxiv.org/abs/2509.10632)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Nonlinear system identification often involves a fundamental trade-off between interpretability and flexibility, often requiring the incorporation of physical constraints. We propose a unified data-driven framework that combines the mathematical structure of the governing differential equations with the flexibility of neural networks (NNs). At the core of our approach is the concept of characteristic curves (CCs), which represent individual nonlinear functions (e.g., friction and restoring components) of the system. Each CC is modeled by a dedicated NN, enabling a modular and interpretable representation of the system equation. To demonstrate the versatility of the CC-based formalism, we introduce three identification strategies: (1) SINDy-CC, which extends the sparse regression approach of SINDy by incorporating the mathematical structure of the governing equations as constraints; (2) Poly-CC, which represents each CC using high-degree polynomials; and (3) NN-CC, which uses NNs without requiring prior assumptions about basis functions. Our results show that all three approaches are well-suited for systems with simple polynomial nonlinearities, such as the van der Pol oscillator. In contrast, NN-CC demonstrates superior performance in modeling systems with complex nonlinearities and discontinuities, such as those observed in stick-slip systems. The key contribution of this work is to demonstrate that the CC-based framework, particularly the NN-CC approach, can capture complex nonlinearities while maintaining interpretability through the explicit representation of the CCs. This balance makes it well-suited for modeling systems with discontinuities and complex nonlinearities that are challenging to assess using traditional polynomial or sparse regression methods, providing a powerful tool for nonlinear system identification.</li>
</ul>

<h3>Title: Accurate and Private Diagnosis of Rare Genetic Syndromes from Facial Images with Federated Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Ali Burak Ünal, Cem Ata Baykara, Peter Krawitz, Mete Akgün</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10635">https://arxiv.org/abs/2509.10635</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10635">https://arxiv.org/pdf/2509.10635</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10635]] Accurate and Private Diagnosis of Rare Genetic Syndromes from Facial Images with Federated Deep Learning(https://arxiv.org/abs/2509.10635)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>Machine learning has shown promise in facial dysmorphology, where characteristic facial features provide diagnostic clues for rare genetic disorders. GestaltMatcher, a leading framework in this field, has demonstrated clinical utility across multiple studies, but its reliance on centralized datasets limits further development, as patient data are siloed across institutions and subject to strict privacy regulations. We introduce a federated GestaltMatcher service based on a cross-silo horizontal federated learning framework, which allows hospitals to collaboratively train a global ensemble feature extractor without sharing patient images. Patient data are mapped into a shared latent space, and a privacy-preserving kernel matrix computation framework enables syndrome inference and discovery while safeguarding confidentiality. New participants can directly benefit from and contribute to the system by adopting the global feature extractor and kernel configuration from previous training rounds. Experiments show that the federated service retains over 90% of centralized performance and remains robust to both varying silo numbers and heterogeneous data distributions.</li>
</ul>

<h3>Title: Test-Time Warmup for Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Nikita Rajaneesh, Thomas Zollo, Richard Zemel</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10641">https://arxiv.org/abs/2509.10641</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10641">https://arxiv.org/pdf/2509.10641</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10641]] Test-Time Warmup for Multimodal Large Language Models(https://arxiv.org/abs/2509.10641)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) hold great promise for advanced reasoning at the intersection of text and images, yet they have not fully realized this potential. MLLMs typically integrate an LLM, a vision encoder, and a connector that maps the vision encoder's embeddings into the LLM's text embedding space. Although each component is pretrained on massive datasets with billions of samples, the entire multimodal model is typically trained on only thousands (or a few million) samples, which can result in weak performance on complex reasoning tasks. To address these shortcomings, instead of relying on extensive labeled datasets for fine-tuning, we propose a Test-Time Warmup method that adapts the MLLM per test instance by leveraging data from weakly supervised auxiliary tasks. With our approach, we observe a relative performance improvement of 4.03% on MMMU, 5.28% on VQA-Rad, and 1.63% on GQA on the Llama-Vision-Instruct model. Our method demonstrates that 'warming up' before inference can enhance MLLMs' robustness across diverse reasoning tasks.</li>
</ul>

<h3>Title: Interdisciplinary Research in Conversation: A Case Study in Computational Morphology for Language Documentation</h3>
<ul>
<li><strong>Authors: </strong>Enora Rice, Katharina von der Wense, Alexis Palmer</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10644">https://arxiv.org/abs/2509.10644</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10644">https://arxiv.org/pdf/2509.10644</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10644]] Interdisciplinary Research in Conversation: A Case Study in Computational Morphology for Language Documentation(https://arxiv.org/abs/2509.10644)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Computational morphology has the potential to support language documentation through tasks like morphological segmentation and the generation of Interlinear Glossed Text (IGT). However, our research outputs have seen limited use in real-world language documentation settings. This position paper situates the disconnect between computational morphology and language documentation within a broader misalignment between research and practice in NLP and argues that the field risks becoming decontextualized and ineffectual without systematic integration of User-Centered Design (UCD). To demonstrate how principles from UCD can reshape the research agenda, we present a case study of GlossLM, a state-of-the-art multilingual IGT generation model. Through a small-scale user study with three documentary linguists, we find that despite strong metric based performance, the system fails to meet core usability needs in real documentation contexts. These insights raise new research questions around model constraints, label standardization, segmentation, and personalization. We argue that centering users not only produces more effective tools, but surfaces richer, more relevant research directions</li>
</ul>

<h3>Title: Safety and Security Analysis of Large Language Models: Risk Profile and Harm Potential</h3>
<ul>
<li><strong>Authors: </strong>Charankumar Akiri, Harrison Simpson, Kshitiz Aryal, Aarav Khanna, Maanak Gupta</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10655">https://arxiv.org/abs/2509.10655</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10655">https://arxiv.org/pdf/2509.10655</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10655]] Safety and Security Analysis of Large Language Models: Risk Profile and Harm Potential(https://arxiv.org/abs/2509.10655)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>While the widespread deployment of Large Language Models (LLMs) holds great potential for society, their vulnerabilities to adversarial manipulation and exploitation can pose serious safety, security, and ethical risks. As new threats continue to emerge, it becomes critically necessary to assess the landscape of LLMs' safety and security against evolving adversarial prompt techniques. To understand the behavior of LLMs, this research provides an empirical analysis and risk profile of nine prominent LLMs, Claude Opus 4, DeepSeek V3 (both open-source and online), Gemini 2.5 Flash, GPT-4o, Grok 3, Llama 4 Scout, Mistral 7B, and Qwen 3 1.7B, against 24 different security and safety categories. These LLMs are evaluated on their ability to produce harmful responses for adversarially crafted prompts (dataset has been made public) for a broad range of safety and security topics, such as promotion of violent criminal behavior, promotion of non-violent criminal activity, societal harms related to safety, illegal sexual content, dangerous code generation, and cybersecurity threats beyond code. Our study introduces the Risk Severity Index (RSI), an agile and scalable evaluation score, to quantify and compare the security posture and creating a risk profile of LLMs. As the LLM development landscape progresses, the RSI is intended to be a valuable metric for comparing the risks of LLMs across evolving threats. This research finds widespread vulnerabilities in the safety filters of the LLMs tested and highlights the urgent need for stronger alignment, responsible deployment practices, and model governance, particularly for open-access and rapidly iterated models.</li>
</ul>

<h3>Title: M4GN: Mesh-based Multi-segment Hierarchical Graph Network for Dynamic Simulations</h3>
<ul>
<li><strong>Authors: </strong>Bo Lei, Victor M. Castillo, Yeping Hu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10659">https://arxiv.org/abs/2509.10659</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10659">https://arxiv.org/pdf/2509.10659</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10659]] M4GN: Mesh-based Multi-segment Hierarchical Graph Network for Dynamic Simulations(https://arxiv.org/abs/2509.10659)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Mesh-based graph neural networks (GNNs) have become effective surrogates for PDE simulations, yet their deep message passing incurs high cost and over-smoothing on large, long-range meshes; hierarchical GNNs shorten propagation paths but still face two key obstacles: (i) building coarse graphs that respect mesh topology, geometry, and physical discontinuities, and (ii) maintaining fine-scale accuracy without sacrificing the speed gained from coarsening. We tackle these challenges with M4GN, a three-tier, segment-centric hierarchical network. M4GN begins with a hybrid segmentation strategy that pairs a fast graph partitioner with a superpixel-style refinement guided by modal-decomposition features, producing contiguous segments of dynamically consistent nodes. These segments are encoded by a permutation-invariant aggregator, avoiding the order sensitivity and quadratic cost of aggregation approaches used in prior works. The resulting information bridges a micro-level GNN, which captures local dynamics, and a macro-level transformer that reasons efficiently across segments, achieving a principled balance between accuracy and efficiency. Evaluated on multiple representative benchmark datasets, M4GN improves prediction accuracy by up to 56% while achieving up to 22% faster inference than state-of-the-art baselines.</li>
</ul>

<h3>Title: Context Copying Modulation: The Role of Entropy Neurons in Managing Parametric and Contextual Knowledge Conflicts</h3>
<ul>
<li><strong>Authors: </strong>Zineddine Tighidet, Andrea Mogini, Hedi Ben-younes, Jiali Mei, Patrick Gallinari, Benjamin Piwowarski</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10663">https://arxiv.org/abs/2509.10663</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10663">https://arxiv.org/pdf/2509.10663</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10663]] Context Copying Modulation: The Role of Entropy Neurons in Managing Parametric and Contextual Knowledge Conflicts(https://arxiv.org/abs/2509.10663)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>The behavior of Large Language Models (LLMs) when facing contextual information that conflicts with their internal parametric knowledge is inconsistent, with no generally accepted explanation for the expected outcome distribution. Recent work has identified in autoregressive transformer models a class of neurons -- called entropy neurons -- that produce a significant effect on the model output entropy while having an overall moderate impact on the ranking of the predicted tokens. In this paper, we investigate the preliminary claim that these neurons are involved in inhibiting context copying behavior in transformers by looking at their role in resolving conflicts between contextual and parametric information. We show that entropy neurons are responsible for suppressing context copying across a range of LLMs, and that ablating them leads to a significant change in the generation process. These results enhance our understanding of the internal dynamics of LLMs when handling conflicting information.</li>
</ul>

<h3>Title: LLM in the Middle: A Systematic Review of Threats and Mitigations to Real-World LLM-based Systems</h3>
<ul>
<li><strong>Authors: </strong>Vitor Hugo Galhardo Moia, Igor Jochem Sanz, Gabriel Antonio Fontes Rebello, Rodrigo Duarte de Meneses, Briland Hitaj, Ulf Lindqvist</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL, cs.ET, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10682">https://arxiv.org/abs/2509.10682</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10682">https://arxiv.org/pdf/2509.10682</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10682]] LLM in the Middle: A Systematic Review of Threats and Mitigations to Real-World LLM-based Systems(https://arxiv.org/abs/2509.10682)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, defense, attack, steal, generative, large language model</a></li>
<li><strong>Abstract: </strong>The success and wide adoption of generative AI (GenAI), particularly large language models (LLMs), has attracted the attention of cybercriminals seeking to abuse models, steal sensitive data, or disrupt services. Moreover, providing security to LLM-based systems is a great challenge, as both traditional threats to software applications and threats targeting LLMs and their integration must be mitigated. In this survey, we shed light on security and privacy concerns of such LLM-based systems by performing a systematic review and comprehensive categorization of threats and defensive strategies considering the entire software and LLM life cycles. We analyze real-world scenarios with distinct characteristics of LLM usage, spanning from development to operation. In addition, threats are classified according to their severity level and to which scenarios they pertain, facilitating the identification of the most relevant threats. Recommended defense strategies are systematically categorized and mapped to the corresponding life cycle phase and possible attack strategies they attenuate. This work paves the way for consumers and vendors to understand and efficiently mitigate risks during integration of LLMs in their respective solutions or organizations. It also enables the research community to benefit from the discussion of open challenges and edge cases that may hinder the secure and privacy-preserving adoption of LLM-based systems.</li>
</ul>

<h3>Title: A Comparison and Evaluation of Fine-tuned Convolutional Neural Networks to Large Language Models for Image Classification and Segmentation of Brain Tumors on MRI</h3>
<ul>
<li><strong>Authors: </strong>Felicia Liu, Jay J. Yoo, Farzad Khalvati</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10683">https://arxiv.org/abs/2509.10683</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10683">https://arxiv.org/pdf/2509.10683</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10683]] A Comparison and Evaluation of Fine-tuned Convolutional Neural Networks to Large Language Models for Image Classification and Segmentation of Brain Tumors on MRI(https://arxiv.org/abs/2509.10683)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown strong performance in text-based healthcare tasks. However, their utility in image-based applications remains unexplored. We investigate the effectiveness of LLMs for medical imaging tasks, specifically glioma classification and segmentation, and compare their performance to that of traditional convolutional neural networks (CNNs). Using the BraTS 2020 dataset of multi-modal brain MRIs, we evaluated a general-purpose vision-language LLM (LLaMA 3.2 Instruct) both before and after fine-tuning, and benchmarked its performance against custom 3D CNNs. For glioma classification (Low-Grade vs. High-Grade), the CNN achieved 80% accuracy and balanced precision and recall. The general LLM reached 76% accuracy but suffered from a specificity of only 18%, often misclassifying Low-Grade tumors. Fine-tuning improved specificity to 55%, but overall performance declined (e.g., accuracy dropped to 72%). For segmentation, three methods - center point, bounding box, and polygon extraction, were implemented. CNNs accurately localized gliomas, though small tumors were sometimes missed. In contrast, LLMs consistently clustered predictions near the image center, with no distinction of glioma size, location, or placement. Fine-tuning improved output formatting but failed to meaningfully enhance spatial accuracy. The bounding polygon method yielded random, unstructured outputs. Overall, CNNs outperformed LLMs in both tasks. LLMs showed limited spatial understanding and minimal improvement from fine-tuning, indicating that, in their current form, they are not well-suited for image-based tasks. More rigorous fine-tuning or alternative training strategies may be needed for LLMs to achieve better performance, robustness, and utility in the medical space.</li>
</ul>

<h3>Title: Pluralistic Alignment for Healthcare: A Role-Driven Framework</h3>
<ul>
<li><strong>Authors: </strong>Jiayou Zhong, Anudeex Shetty, Chao Jia, Xuanrui Lin, Usman Naseem</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10685">https://arxiv.org/abs/2509.10685</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10685">https://arxiv.org/pdf/2509.10685</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10685]] Pluralistic Alignment for Healthcare: A Role-Driven Framework(https://arxiv.org/abs/2509.10685)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As large language models are increasingly deployed in sensitive domains such as healthcare, ensuring their outputs reflect the diverse values and perspectives held across populations is critical. However, existing alignment approaches, including pluralistic paradigms like Modular Pluralism, often fall short in the health domain, where personal, cultural, and situational factors shape pluralism. Motivated by the aforementioned healthcare challenges, we propose a first lightweight, generalizable, pluralistic alignment approach, EthosAgents, designed to simulate diverse perspectives and values. We empirically show that it advances the pluralistic alignment for all three modes across seven varying-sized open and closed models. Our findings reveal that health-related pluralism demands adaptable and normatively aware approaches, offering insights into how these models can better respect diversity in other high-stakes domains.</li>
</ul>

<h3>Title: Stable Part Diffusion 4D: Multi-View RGB and Kinematic Parts Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Hao Zhang, Chun-Han Yao, Simon Donné, Narendra Ahuja, Varun Jampani</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10687">https://arxiv.org/abs/2509.10687</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10687">https://arxiv.org/pdf/2509.10687</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10687]] Stable Part Diffusion 4D: Multi-View RGB and Kinematic Parts Video Generation(https://arxiv.org/abs/2509.10687)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>We present Stable Part Diffusion 4D (SP4D), a framework for generating paired RGB and kinematic part videos from monocular inputs. Unlike conventional part segmentation methods that rely on appearance-based semantic cues, SP4D learns to produce kinematic parts - structural components aligned with object articulation and consistent across views and time. SP4D adopts a dual-branch diffusion model that jointly synthesizes RGB frames and corresponding part segmentation maps. To simplify the architecture and flexibly enable different part counts, we introduce a spatial color encoding scheme that maps part masks to continuous RGB-like images. This encoding allows the segmentation branch to share the latent VAE from the RGB branch, while enabling part segmentation to be recovered via straightforward post-processing. A Bidirectional Diffusion Fusion (BiDiFuse) module enhances cross-branch consistency, supported by a contrastive part consistency loss to promote spatial and temporal alignment of part predictions. We demonstrate that the generated 2D part maps can be lifted to 3D to derive skeletal structures and harmonic skinning weights with few manual adjustments. To train and evaluate SP4D, we construct KinematicParts20K, a curated dataset of over 20K rigged objects selected and processed from Objaverse XL (Deitke et al., 2023), each paired with multi-view RGB and part video sequences. Experiments show that SP4D generalizes strongly to diverse scenarios, including real-world videos, novel generated objects, and rare articulated poses, producing kinematic-aware outputs suitable for downstream animation and motion-related tasks.</li>
</ul>

<h3>Title: Privacy-Preserving Decentralized Federated Learning via Explainable Adaptive Differential Privacy</h3>
<ul>
<li><strong>Authors: </strong>Fardin Jalil Piran, Zhiling Chen, Yang Zhang, Qianyu Zhou, Jiong Tang, Farhad Imani</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10691">https://arxiv.org/abs/2509.10691</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10691">https://arxiv.org/pdf/2509.10691</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10691]] Privacy-Preserving Decentralized Federated Learning via Explainable Adaptive Differential Privacy(https://arxiv.org/abs/2509.10691)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, protect, attack, membership infer, federate, transformer</a></li>
<li><strong>Abstract: </strong>Decentralized federated learning faces privacy risks because model updates can leak data through inference attacks and membership inference, a concern that grows over many client exchanges. Differential privacy offers principled protection by injecting calibrated noise so confidential information remains secure on resource-limited IoT devices. Yet without transparency, black-box training cannot track noise already injected by previous clients and rounds, which forces worst-case additions and harms accuracy. We propose PrivateDFL, an explainable framework that joins hyperdimensional computing with differential privacy and keeps an auditable account of cumulative noise so each client adds only the difference between the required noise and what has already been accumulated. We evaluate on MNIST, ISOLET, and UCI-HAR to span image, signal, and tabular modalities, and we benchmark against transformer-based and deep learning-based baselines trained centrally with Differentially Private Stochastic Gradient Descent (DP-SGD) and Renyi Differential Privacy (RDP). PrivateDFL delivers higher accuracy, lower latency, and lower energy across IID and non-IID partitions while preserving formal (epsilon, delta) guarantees and operating without a central server. For example, under non-IID partitions, PrivateDFL achieves 24.42% higher accuracy than the Vision Transformer on MNIST while using about 10x less training time, 76x lower inference latency, and 11x less energy, and on ISOLET it exceeds Transformer accuracy by more than 80% with roughly 10x less training time, 40x lower inference latency, and 36x less training energy. Future work will extend the explainable accounting to adversarial clients and adaptive topologies with heterogeneous privacy budgets.</li>
</ul>

<h3>Title: Kalman Bayesian Transformer</h3>
<ul>
<li><strong>Authors: </strong>Haoming Jing, Oren Wright, José M. F. Moura, Yorie Nakahira</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10695">https://arxiv.org/abs/2509.10695</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10695">https://arxiv.org/pdf/2509.10695</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10695]] Kalman Bayesian Transformer(https://arxiv.org/abs/2509.10695)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Sequential fine-tuning of transformers is useful when new data arrive sequentially, especially with shifting distributions. Unlike batch learning, sequential learning demands that training be stabilized despite a small amount of data by balancing new information and previously learned knowledge in the pre-trained models. This challenge is further complicated when training is to be completed in latency-critical environments and learning must additionally quantify and be mediated by uncertainty. Motivated by these challenges, we propose a novel method that frames sequential fine-tuning as a posterior inference problem within a Bayesian framework. Our approach integrates closed-form moment propagation of random variables, Kalman Bayesian Neural Networks, and Taylor approximations of the moments of softmax functions. By explicitly accounting for pre-trained models as priors and adaptively balancing them against new information based on quantified uncertainty, our method achieves robust and data-efficient sequential learning. The effectiveness of our method is demonstrated through numerical simulations involving sequential adaptation of a decision transformer to tasks characterized by distribution shifts and limited memory resources.</li>
</ul>

<h3>Title: Struct-Bench: A Benchmark for Differentially Private Structured Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Shuaiqi Wang, Vikas Raunak, Arturs Backurs, Victor Reis, Pei Zhou, Sihao Chen, Longqi Yang, Zinan Lin, Sergey Yekhanin, Giulia Fanti</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10696">https://arxiv.org/abs/2509.10696</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10696">https://arxiv.org/pdf/2509.10696</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10696]] Struct-Bench: A Benchmark for Differentially Private Structured Text Generation(https://arxiv.org/abs/2509.10696)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Differentially private (DP) synthetic data generation is a promising technique for utilizing private datasets that otherwise cannot be exposed for model training or other analytics. While much research literature has focused on generating private unstructured text and image data, in enterprise settings, structured data (e.g., tabular) is more common, often including natural language fields or components. Existing synthetic data evaluation techniques (e.g., FID) struggle to capture the structural properties and correlations of such datasets. In this work, we propose Struct-Bench, a framework and benchmark for evaluating synthetic datasets derived from structured datasets that contain natural language data. The Struct-Bench framework requires users to provide a representation of their dataset structure as a Context-Free Grammar (CFG). Our benchmark comprises 5 real-world and 2 synthetically generated datasets, each annotated with CFGs. We show that these datasets demonstrably present a great challenge even for state-of-the-art DP synthetic data generation methods. Struct-Bench also includes reference implementations of different metrics and a leaderboard, thereby providing researchers a standardized evaluation platform to benchmark and investigate privacy-preserving synthetic data generation methods. Further, we also present a case study showing how to use Struct-Bench to improve the synthetic data quality of Private Evolution (PE) on structured data. The benchmark and the leaderboard have been publicly made available at this https URL.</li>
</ul>

<h3>Title: A Survey on Retrieval And Structuring Augmented Generation with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Pengcheng Jiang, Siru Ouyang, Yizhu Jiao, Ming Zhong, Runchu Tian, Jiawei Han</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10697">https://arxiv.org/abs/2509.10697</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10697">https://arxiv.org/pdf/2509.10697</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10697]] A Survey on Retrieval And Structuring Augmented Generation with Large Language Models(https://arxiv.org/abs/2509.10697)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have revolutionized natural language processing with their remarkable capabilities in text generation and reasoning. However, these models face critical challenges when deployed in real-world applications, including hallucination generation, outdated knowledge, and limited domain expertise. Retrieval And Structuring (RAS) Augmented Generation addresses these limitations by integrating dynamic information retrieval with structured knowledge representations. This survey (1) examines retrieval mechanisms including sparse, dense, and hybrid approaches for accessing external knowledge; (2) explore text structuring techniques such as taxonomy construction, hierarchical classification, and information extraction that transform unstructured text into organized representations; and (3) investigate how these structured representations integrate with LLMs through prompt-based methods, reasoning frameworks, and knowledge embedding techniques. It also identifies technical challenges in retrieval efficiency, structure quality, and knowledge integration, while highlighting research opportunities in multimodal retrieval, cross-lingual structures, and interactive systems. This comprehensive overview provides researchers and practitioners with insights into RAS methods, applications, and future directions.</li>
</ul>

<h3>Title: CrunchLLM: Multitask LLMs for Structured Business Reasoning and Outcome Prediction</h3>
<ul>
<li><strong>Authors: </strong>Rabeya Tus Sadia, Qiang Cheng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10698">https://arxiv.org/abs/2509.10698</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10698">https://arxiv.org/pdf/2509.10698</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10698]] CrunchLLM: Multitask LLMs for Structured Business Reasoning and Outcome Prediction(https://arxiv.org/abs/2509.10698)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Predicting the success of start-up companies, defined as achieving an exit through acquisition or IPO, is a critical problem in entrepreneurship and innovation research. Datasets such as Crunchbase provide both structured information (e.g., funding rounds, industries, investor networks) and unstructured text (e.g., company descriptions), but effectively leveraging this heterogeneous data for prediction remains challenging. Traditional machine learning approaches often rely only on structured features and achieve moderate accuracy, while large language models (LLMs) offer rich reasoning abilities but struggle to adapt directly to domain-specific business data. We present \textbf{CrunchLLM}, a domain-adapted LLM framework for startup success prediction. CrunchLLM integrates structured company attributes with unstructured textual narratives and applies parameter-efficient fine-tuning strategies alongside prompt optimization to specialize foundation models for entrepreneurship data. Our approach achieves accuracy exceeding 80\% on Crunchbase startup success prediction, significantly outperforming traditional classifiers and baseline LLMs. Beyond predictive performance, CrunchLLM provides interpretable reasoning traces that justify its predictions, enhancing transparency and trustworthiness for financial and policy decision makers. This work demonstrates how adapting LLMs with domain-aware fine-tuning and structured--unstructured data fusion can advance predictive modeling of entrepreneurial outcomes. CrunchLLM contributes a methodological framework and a practical tool for data-driven decision making in venture capital and innovation policy.</li>
</ul>

<h3>Title: Side-channel Inference of User Activities in AR/VR Using GPU Profiling</h3>
<ul>
<li><strong>Authors: </strong>Seonghun Son, Chandrika Mukherjee, Reham Mohamed Aburas, Berk Gulmezoglu, Z. Berkay Celik</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10703">https://arxiv.org/abs/2509.10703</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10703">https://arxiv.org/pdf/2509.10703</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10703]] Side-channel Inference of User Activities in AR/VR Using GPU Profiling(https://arxiv.org/abs/2509.10703)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, protect</a></li>
<li><strong>Abstract: </strong>Over the past decade, AR/VR devices have drastically changed how we interact with the digital world. Users often share sensitive information, such as their location, browsing history, and even financial data, within third-party apps installed on these devices, assuming a secure environment protected from malicious actors. Recent research has revealed that malicious apps can exploit such capabilities and monitor benign apps to track user activities, leveraging fine-grained profiling tools, such as performance counter APIs. However, app-to-app monitoring is not feasible on all AR/VR devices (e.g., Meta Quest), as a concurrent standalone app execution is disabled. In this paper, we present OVRWatcher, a novel side-channel primitive for AR/VR devices that infers user activities by monitoring low-resolution (1Hz) GPU usage via a background script, unlike prior work that relies on high-resolution profiling. OVRWatcher captures correlations between GPU metrics and 3D object interactions under varying speeds, distances, and rendering scenarios, without requiring concurrent app execution, access to application data, or additional SDK installations. We demonstrate the efficacy of OVRWatcher in fingerprinting both standalone AR/VR and WebXR applications. OVRWatcher also distinguishes virtual objects, such as products in immersive shopping apps selected by real users and the number of participants in virtual meetings, thereby revealing users' product preferences and potentially exposing confidential information from those meetings. OVRWatcher achieves over 99% accuracy in app fingerprinting and over 98% accuracy in object-level inference.</li>
</ul>

<h3>Title: SearchInstruct: Enhancing Domain Adaptation via Retrieval-Based Instruction Dataset Creation</h3>
<ul>
<li><strong>Authors: </strong>Iman Barati, Mostafa Amiri, Heshaam Faili</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10708">https://arxiv.org/abs/2509.10708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10708">https://arxiv.org/pdf/2509.10708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10708]] SearchInstruct: Enhancing Domain Adaptation via Retrieval-Based Instruction Dataset Creation(https://arxiv.org/abs/2509.10708)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Supervised Fine-Tuning (SFT) is essential for training large language models (LLMs), significantly enhancing critical capabilities such as instruction following and in-context learning. Nevertheless, creating suitable training datasets tailored for specific domains remains challenging due to unique domain constraints and data scarcity. In this paper, we propose SearchInstruct, an innovative method explicitly designed to construct high quality instruction datasets for SFT. Our approach begins with a limited set of domain specific, human generated questions, which are systematically expanded using a large language model. Subsequently, domain relevant resources are dynamically retrieved to generate accurate and contextually appropriate answers for each augmented question. Experimental evaluation demonstrates that SearchInstruct enhances both the diversity and quality of SFT datasets, leading to measurable improvements in LLM performance within specialized domains. Additionally, we show that beyond dataset generation, the proposed method can also effectively facilitate tasks such as model editing, enabling efficient updates to existing models. To facilitate reproducibility and community adoption, we provide full implementation details, the complete set of generated instruction response pairs, and the source code in a publicly accessible Git repository: [this https URL](this https URL)</li>
</ul>

<h3>Title: Feature-Centric Approaches to Android Malware Analysis: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Shama Maganur, Yili Jiang, Jiaqi Huang, Fangtian Zhong</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10709">https://arxiv.org/abs/2509.10709</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10709">https://arxiv.org/pdf/2509.10709</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10709]] Feature-Centric Approaches to Android Malware Analysis: A Survey(https://arxiv.org/abs/2509.10709)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, extraction</a></li>
<li><strong>Abstract: </strong>Sophisticated malware families exploit the openness of the Android platform to infiltrate IoT networks, enabling large-scale disruption, data exfiltration, and denial-of-service attacks. This systematic literature review (SLR) examines cutting-edge approaches to Android malware analysis with direct implications for securing IoT infrastructures. We analyze feature extraction techniques across static, dynamic, hybrid, and graph-based methods, highlighting their trade-offs: static analysis offers efficiency but is easily evaded through obfuscation; dynamic analysis provides stronger resistance to evasive behaviors but incurs high computational costs, often unsuitable for lightweight IoT devices; hybrid approaches balance accuracy with resource considerations; and graph-based methods deliver superior semantic modeling and adversarial robustness. This survey contributes a structured comparison of existing methods, exposes research gaps, and outlines a roadmap for future directions to enhance scalability, adaptability, and long-term security in IoT-driven Android malware detection.</li>
</ul>

<h3>Title: SegSLR: Promptable Video Segmentation for Isolated Sign Language Recognition</h3>
<ul>
<li><strong>Authors: </strong>Sven Schreiber, Noha Sarhan, Simone Frintrop, Christian Wilms</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10710">https://arxiv.org/abs/2509.10710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10710">https://arxiv.org/pdf/2509.10710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10710]] SegSLR: Promptable Video Segmentation for Isolated Sign Language Recognition(https://arxiv.org/abs/2509.10710)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Isolated Sign Language Recognition (ISLR) approaches primarily rely on RGB data or signer pose information. However, combining these modalities often results in the loss of crucial details, such as hand shape and orientation, due to imprecise representations like bounding boxes. Therefore, we propose the ISLR system SegSLR, which combines RGB and pose information through promptable zero-shot video segmentation. Given the rough localization of the hands and the signer's body from pose information, we segment the respective parts through the video to maintain all relevant shape information. Subsequently, the segmentations focus the processing of the RGB data on the most relevant body parts for ISLR. This effectively combines RGB and pose information. Our evaluation on the complex ChaLearn249 IsoGD dataset shows that SegSLR outperforms state-of-the-art methods. Furthermore, ablation studies indicate that SegSLR strongly benefits from focusing on the signer's body and hands, justifying our design choices.</li>
</ul>

<h3>Title: Security theory for data flow and access control: From partial orders to lattices and back, a half-century trip</h3>
<ul>
<li><strong>Authors: </strong>Luigi Logrippo</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10727">https://arxiv.org/abs/2509.10727</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10727">https://arxiv.org/pdf/2509.10727</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10727]] Security theory for data flow and access control: From partial orders to lattices and back, a half-century trip(https://arxiv.org/abs/2509.10727)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>The multi level Bell La Padula model for secure data access and data flow control, formulated in the 1970s, was based on the theory of partial orders. Since then, another model, based on lattice theory, has prevailed. We present reasons why the partial order model is more appropriate. We also show, by example, how non lattice data flow networks can be easily implemented by using Attribute-based access control (ABAC).</li>
</ul>

<h3>Title: Using LLMs for Late Multimodal Sensor Fusion for Activity Recognition</h3>
<ul>
<li><strong>Authors: </strong>Ilker Demirel, Karan Thakkar, Benjamin Elizalde, Miquel Espi Marques, Shirley Ren, Jaya Narain</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10729">https://arxiv.org/abs/2509.10729</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10729">https://arxiv.org/pdf/2509.10729</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10729]] Using LLMs for Late Multimodal Sensor Fusion for Activity Recognition(https://arxiv.org/abs/2509.10729)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Sensor data streams provide valuable information around activities and context for downstream applications, though integrating complementary information can be challenging. We show that large language models (LLMs) can be used for late fusion for activity classification from audio and motion time series data. We curated a subset of data for diverse activity recognition across contexts (e.g., household activities, sports) from the Ego4D dataset. Evaluated LLMs achieved 12-class zero- and one-shot classification F1-scores significantly above chance, with no task-specific training. Zero-shot classification via LLM-based fusion from modality-specific models can enable multimodal temporal applications where there is limited aligned training data for learning a shared embedding space. Additionally, LLM-based fusion can enable model deploying without requiring additional memory and computation for targeted application-specific multimodal models.</li>
</ul>

<h3>Title: PolyTruth: Multilingual Disinformation Detection using Transformer-Based Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zaur Gouliev, Jennifer Waters, Chengqian Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10737">https://arxiv.org/abs/2509.10737</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10737">https://arxiv.org/pdf/2509.10737</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10737]] PolyTruth: Multilingual Disinformation Detection using Transformer-Based Language Models(https://arxiv.org/abs/2509.10737)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Disinformation spreads rapidly across linguistic boundaries, yet most AI models are still benchmarked only on English. We address this gap with a systematic comparison of five multilingual transformer models: mBERT, XLM, XLM-RoBERTa, RemBERT, and mT5 on a common fake-vs-true machine learning classification task. While transformer-based language models have demonstrated notable success in detecting disinformation in English, their effectiveness in multilingual contexts still remains up for debate. To facilitate evaluation, we introduce PolyTruth Disinfo Corpus, a novel corpus of 60,486 statement pairs (false claim vs. factual correction) spanning over twenty five languages that collectively cover five language families and a broad topical range from politics, health, climate, finance, and conspiracy, half of which are fact-checked disinformation claims verified by an augmented MindBugs Discovery dataset. Our experiments revealed performance variations. Models such as RemBERT achieved better overall accuracy, particularly excelling in low-resource languages, whereas models like mBERT and XLM exhibit considerable limitations when training data is scarce. We provide a discussion of these performance patterns and implications for real-world deployment. The dataset is publicly available on our GitHub repository to encourage further experimentation and advancement. Our findings illuminate both the potential and the current limitations of AI systems for multilingual disinformation detection.</li>
</ul>

<h3>Title: Reasoning Under Uncertainty: Exploring Probabilistic Reasoning Capabilities of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Mobina Pournemat, Keivan Rezaei, Gaurang Sriramanan, Arman Zarei, Jiaxiang Fu, Yang Wang, Hamid Eghbalzadeh, Soheil Feizi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10739">https://arxiv.org/abs/2509.10739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10739">https://arxiv.org/pdf/2509.10739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10739]] Reasoning Under Uncertainty: Exploring Probabilistic Reasoning Capabilities of LLMs(https://arxiv.org/abs/2509.10739)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Despite widespread success in language understanding and generation, large language models (LLMs) exhibit unclear and often inconsistent behavior when faced with tasks that require probabilistic reasoning. In this work, we present the first comprehensive study of the reasoning capabilities of LLMs over explicit discrete probability distributions. Given observations from a probability distribution, we evaluate models on three carefully designed tasks, mode identification, maximum likelihood estimation, and sample generation, by prompting them to provide responses to queries about either the joint distribution or its conditionals. These tasks thus probe a range of probabilistic skills, including frequency analysis, marginalization, and generative behavior. Through comprehensive empirical evaluations, we demonstrate that there exists a clear performance gap between smaller and larger models, with the latter demonstrating stronger inference and surprising capabilities in sample generation. Furthermore, our investigations reveal notable limitations, including sensitivity to variations in the notation utilized to represent probabilistic outcomes and performance degradation of over 60% as context length increases. Together, our results provide a detailed understanding of the probabilistic reasoning abilities of LLMs and identify key directions for future improvement.</li>
</ul>

<h3>Title: RECAP: Transparent Inference-Time Emotion Alignment for Medical Dialogue Systems</h3>
<ul>
<li><strong>Authors: </strong>Adarsh Srinivasan, Jacob Dineen, Muhammad Umar Afzal, Muhammad Uzair Sarfraz, Irbaz B. Riaz, Ben Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10746">https://arxiv.org/abs/2509.10746</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10746">https://arxiv.org/pdf/2509.10746</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10746]] RECAP: Transparent Inference-Time Emotion Alignment for Medical Dialogue Systems(https://arxiv.org/abs/2509.10746)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models in healthcare often miss critical emotional cues, delivering medically sound but emotionally flat advice. This is especially problematic in clinical contexts where patients are distressed and vulnerable, and require empathic communication to support safety, adherence, and trust. We present RECAP (Reflect-Extract-Calibrate-Align-Produce), an inference-time framework that adds structured emotional reasoning without retraining. By decomposing empathy into transparent appraisal-theoretic stages and exposing per-dimension Likert signals, RECAP produces nuanced, auditable responses. Across EmoBench, SECEU, and EQ-Bench, RECAP improves emotional reasoning by 22-28% on 8B models and 10-13% on larger models over zero-shot baselines. Clinician evaluations further confirm superior empathetic communication. RECAP shows that modular, theory-grounded prompting can systematically enhance emotional intelligence in medical AI while preserving the accountability required for deployment.</li>
</ul>

<h3>Title: SCOPE: Speech-guided COllaborative PErception Framework for Surgical Scene Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Jecia Z.Y. Mao, Francis X Creighton, Russell H Taylor, Manish Sahu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10748">https://arxiv.org/abs/2509.10748</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10748">https://arxiv.org/pdf/2509.10748</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10748]] SCOPE: Speech-guided COllaborative PErception Framework for Surgical Scene Segmentation(https://arxiv.org/abs/2509.10748)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Accurate segmentation and tracking of relevant elements of the surgical scene is crucial to enable context-aware intraoperative assistance and decision making. Current solutions remain tethered to domain-specific, supervised models that rely on labeled data and required domain-specific data to adapt to new surgical scenarios and beyond predefined label categories. Recent advances in prompt-driven vision foundation models (VFM) have enabled open-set, zero-shot segmentation across heterogeneous medical images. However, dependence of these models on manual visual or textual cues restricts their deployment in introperative surgical settings. We introduce a speech-guided collaborative perception (SCOPE) framework that integrates reasoning capabilities of large language model (LLM) with perception capabilities of open-set VFMs to support on-the-fly segmentation, labeling and tracking of surgical instruments and anatomy in intraoperative video streams. A key component of this framework is a collaborative perception agent, which generates top candidates of VFM-generated segmentation and incorporates intuitive speech feedback from clinicians to guide the segmentation of surgical instruments in a natural human-machine collaboration paradigm. Afterwards, instruments themselves serve as interactive pointers to label additional elements of the surgical scene. We evaluated our proposed framework on a subset of publicly available Cataract1k dataset and an in-house ex-vivo skull-base dataset to demonstrate its potential to generate on-the-fly segmentation and tracking of surgical scene. Furthermore, we demonstrate its dynamic capabilities through a live mock ex-vivo experiment. This human-AI collaboration paradigm showcase the potential of developing adaptable, hands-free, surgeon-centric tools for dynamic operating-room environments.</li>
</ul>

<h3>Title: HalluField: Detecting LLM Hallucinations via Field-Theoretic Modeling</h3>
<ul>
<li><strong>Authors: </strong>Minh Vu, Brian K. Tran, Syed A. Shah, Geigh Zollicoffer, Nhat Hoang-Xuan, Manish Bhattarai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10753">https://arxiv.org/abs/2509.10753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10753">https://arxiv.org/pdf/2509.10753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10753]] HalluField: Detecting LLM Hallucinations via Field-Theoretic Modeling(https://arxiv.org/abs/2509.10753)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) exhibit impressive reasoning and question-answering capabilities. However, they often produce inaccurate or unreliable content known as hallucinations. This unreliability significantly limits their deployment in high-stakes applications. Thus, there is a growing need for a general-purpose method to detect hallucinations in LLMs. In this work, we introduce HalluField, a novel field-theoretic approach for hallucination detection based on a parametrized variational principle and thermodynamics. Inspired by thermodynamics, HalluField models an LLM's response to a given query and temperature setting as a collection of discrete likelihood token paths, each associated with a corresponding energy and entropy. By analyzing how energy and entropy distributions vary across token paths under changes in temperature and likelihood, HalluField quantifies the semantic stability of a response. Hallucinations are then detected by identifying unstable or erratic behavior in this energy landscape. HalluField is computationally efficient and highly practical: it operates directly on the model's output logits without requiring fine-tuning or auxiliary neural networks. Notably, the method is grounded in a principled physical interpretation, drawing analogies to the first law of thermodynamics. Remarkably, by modeling LLM behavior through this physical lens, HalluField achieves state-of-the-art hallucination detection performance across models and datasets.</li>
</ul>

<h3>Title: Five Minutes of DDoS Brings down Tor: DDoS Attacks on the Tor Directory Protocol and Mitigations</h3>
<ul>
<li><strong>Authors: </strong>Zhongtang Luo, Jianting Zhang, Akshat Neerati, Aniket Kate</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10755">https://arxiv.org/abs/2509.10755</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10755">https://arxiv.org/pdf/2509.10755</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10755]] Five Minutes of DDoS Brings down Tor: DDoS Attacks on the Tor Directory Protocol and Mitigations(https://arxiv.org/abs/2509.10755)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>The Tor network offers network anonymity to its users by routing their traffic through a sequence of relays. A group of nine directory authorities maintains information about all available relay nodes using a distributed directory protocol. We observe that the current protocol makes a steep synchrony assumption, which makes it vulnerable to natural as well as adversarial non-synchronous communication scenarios over the Internet. In this paper, we show that it is possible to cause a failure in the Tor directory protocol by targeting a majority of the authorities for only five minutes using a well-executed distributed denial-of-service (DDoS) attack. We demonstrate this attack in a controlled environment and show that it is cost-effective for as little as \$53.28 per month to disrupt the protocol and to effectively bring down the entire Tor network. To mitigate this problem, we consider the popular partial synchrony assumption for the Tor directory protocol that ensures that the protocol security is hampered even when the network delays are large and unknown. We design a new Tor directory protocol that leverages any standard partial-synchronous consensus protocol to solve this problem, while also proving its security. We have implemented a prototype in Rust, demonstrating comparable performance to the current protocol while resisting similar attacks.</li>
</ul>

<h3>Title: A Content-dependent Watermark for Safeguarding Image Attribution</h3>
<ul>
<li><strong>Authors: </strong>Tong Zhou, Ruyi Ding, Gaowen Liu, Charles Fleming, Ramana Rao Kompella, Yunsi Fei, Xiaolin Xu, Shaolei Ren</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10766">https://arxiv.org/abs/2509.10766</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10766">https://arxiv.org/pdf/2509.10766</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10766]] A Content-dependent Watermark for Safeguarding Image Attribution(https://arxiv.org/abs/2509.10766)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect, robust, watermark</a></li>
<li><strong>Abstract: </strong>The rapid growth of digital and AI-generated images has amplified the need for secure and verifiable methods of image attribution. While digital watermarking offers more robust protection than metadata-based approaches--which can be easily stripped--current watermarking techniques remain vulnerable to forgery, creating risks of misattribution that can damage the reputations of AI model developers and the rights of digital artists. These vulnerabilities arise from two key issues: (1) content-agnostic watermarks, which, once learned or leaked, can be transferred across images to fake attribution, and (2) reliance on detector-based verification, which is unreliable since detectors can be tricked. We present MetaSeal, a novel framework for content-dependent watermarking with cryptographic security guarantees to safeguard image attribution. Our design provides (1) forgery resistance, preventing unauthorized replication and enforcing cryptographic verification; (2) robust, self-contained protection, embedding attribution directly into images while maintaining resilience against benign transformations; and (3) evidence of tampering, making malicious alterations visually detectable. Experiments demonstrate that MetaSeal effectively mitigates forgery attempts and applies to both natural and AI-generated images, establishing a new standard for secure image attribution.</li>
</ul>

<h3>Title: Enhancement Without Contrast: Stability-Aware Multicenter Machine Learning for Glioma MRI Imaging</h3>
<ul>
<li><strong>Authors: </strong>Sajad Amiri, Shahram Taeb, Sara Gharibi, Setareh Dehghanfard, Somayeh Sadat Mehrnia, Mehrdad Oveisi, Ilker Hacihaliloglu, Arman Rahmim, Mohammad R. Salmanpour</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10767">https://arxiv.org/abs/2509.10767</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10767">https://arxiv.org/pdf/2509.10767</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10767]] Enhancement Without Contrast: Stability-Aware Multicenter Machine Learning for Glioma MRI Imaging(https://arxiv.org/abs/2509.10767)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Gadolinium-based contrast agents (GBCAs) are central to glioma imaging but raise safety, cost, and accessibility concerns. Predicting contrast enhancement from non-contrast MRI using machine learning (ML) offers a safer alternative, as enhancement reflects tumor aggressiveness and informs treatment planning. Yet scanner and cohort variability hinder robust model selection. We propose a stability-aware framework to identify reproducible ML pipelines for multicenter prediction of glioma MRI contrast enhancement. We analyzed 1,446 glioma cases from four TCIA datasets (UCSF-PDGM, UPENN-GB, BRATS-Africa, BRATS-TCGA-LGG). Non-contrast T1WI served as input, with enhancement derived from paired post-contrast T1WI. Using PyRadiomics under IBSI standards, 108 features were extracted and combined with 48 dimensionality reduction methods and 25 classifiers, yielding 1,200 pipelines. Rotational validation was trained on three datasets and tested on the fourth. Cross-validation prediction accuracies ranged from 0.91 to 0.96, with external testing achieving 0.87 (UCSF-PDGM), 0.98 (UPENN-GB), and 0.95 (BRATS-Africa), with an average of 0.93. F1, precision, and recall were stable (0.87 to 0.96), while ROC-AUC varied more widely (0.50 to 0.82), reflecting cohort heterogeneity. The MI linked with ETr pipeline consistently ranked highest, balancing accuracy and stability. This framework demonstrates that stability-aware model selection enables reliable prediction of contrast enhancement from non-contrast glioma MRI, reducing reliance on GBCAs and improving generalizability across centers. It provides a scalable template for reproducible ML in neuro-oncology and beyond.</li>
</ul>

<h3>Title: Contextual Budget Bandit for Food Rescue Volunteer Engagement</h3>
<ul>
<li><strong>Authors: </strong>Ariana Tang, Naveen Raman, Fei Fang, Zheyuan Ryan Shi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10777">https://arxiv.org/abs/2509.10777</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10777">https://arxiv.org/pdf/2509.10777</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10777]] Contextual Budget Bandit for Food Rescue Volunteer Engagement(https://arxiv.org/abs/2509.10777)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Volunteer-based food rescue platforms tackle food waste by matching surplus food to communities in need. These platforms face the dual problem of maintaining volunteer engagement and maximizing the food rescued. Existing algorithms to improve volunteer engagement exacerbate geographical disparities, leaving some communities systematically disadvantaged. We address this issue by proposing Contextual Budget Bandit. Contextual Budget Bandit incorporates context-dependent budget allocation in restless multi-armed bandits, a model of decision-making which allows for stateful arms. By doing so, we can allocate higher budgets to communities with lower match rates, thereby alleviating geographical disparities. To tackle this problem, we develop an empirically fast heuristic algorithm. Because the heuristic algorithm can achieve a poor approximation when active volunteers are scarce, we design the Mitosis algorithm, which is guaranteed to compute the optimal budget allocation. Empirically, we demonstrate that our algorithms outperform baselines on both synthetic and real-world food rescue datasets, and show how our algorithm achieves geographical fairness in food rescue.</li>
</ul>

<h3>Title: GoldenTransformer: A Modular Fault Injection Framework for Transformer Robustness Research</h3>
<ul>
<li><strong>Authors: </strong>Luke Howard</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10790">https://arxiv.org/abs/2509.10790</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10790">https://arxiv.org/pdf/2509.10790</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10790]] GoldenTransformer: A Modular Fault Injection Framework for Transformer Robustness Research(https://arxiv.org/abs/2509.10790)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Transformers have become the foundation for a wide range of state--of--the--art models across natural language processing, computer vision, and other machine learning domains. Despite their widespread deployment, the robustness of these models under fault conditions remains underexplored. We present GoldenTransformer, a modular and extensible fault injection framework designed to evaluate the resiliency of Large Language Models to induced hardware faults. GoldenTransformer offers a unified Python-based platform for injecting diverse classes of faults--such as weight corruption, activation injections, and attention--level disruptions--into pretrained transformer--based models. Inspired by the GoldenEye simulator for DNNs, our framework focuses on the unique challenges of working with large transformer architectures, including considerations such as structural complexity, latent dependencies, and nonuniform layer definitions. GoldenTransformer is built atop PyTorch and HuggingFace Transformers, and it supports experiment reproducibility, metric logging, and visualization out of the box. We detail the technical design and use of GoldenTransformer and demonstrate through several example experiments on classification and generation tasks. By enabling controlled injection of faults at multiple logical and structural points in a transformer, GoldenTransformer offers researchers and practitioners a valuable tool for model robustness analysis and for guiding dependable system design in real-world LLM applications.</li>
</ul>

<h3>Title: ORQ: Complex Analytics on Private Data with Strong Security Guarantees</h3>
<ul>
<li><strong>Authors: </strong>Eli Baum, Sam Buxbaum, Nitin Mathai, Muhammad Faisal, Vasiliki Kalavri, Mayank Varia, John Liagouris</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10793">https://arxiv.org/abs/2509.10793</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10793">https://arxiv.org/pdf/2509.10793</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10793]] ORQ: Complex Analytics on Private Data with Strong Security Guarantees(https://arxiv.org/abs/2509.10793)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect</a></li>
<li><strong>Abstract: </strong>We present ORQ, a system that enables collaborative analysis of large private datasets using cryptographically secure multi-party computation (MPC). ORQ protects data against semi-honest or malicious parties and can efficiently evaluate relational queries with multi-way joins and aggregations that have been considered notoriously expensive under MPC. To do so, ORQ eliminates the quadratic cost of secure joins by leveraging the fact that, in practice, the structure of many real queries allows us to join records and apply the aggregations "on the fly" while keeping the result size bounded. On the system side, ORQ contributes generic oblivious operators, a data-parallel vectorized query engine, a communication layer that amortizes MPC network costs, and a dataflow API for expressing relational analytics -- all built from the ground up. We evaluate ORQ in LAN and WAN deployments on a diverse set of workloads, including complex queries with multiple joins and custom aggregations. When compared to state-of-the-art solutions, ORQ significantly reduces MPC execution times and can process one order of magnitude larger datasets. For our most challenging workload, the full TPC-H benchmark, we report results entirely under MPC with Scale Factor 10 -- a scale that had previously been achieved only with information leakage or the use of trusted third parties.</li>
</ul>

<h3>Title: Judge Q: Trainable Queries for Optimized Information Retention in KV Cache Eviction</h3>
<ul>
<li><strong>Authors: </strong>Yijun Liu, Yixuan Wang, Yuzhuang Xu, Shiyu Ji, Yang Xu, Qingfu Zhu, Wanxiang Che</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10798">https://arxiv.org/abs/2509.10798</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10798">https://arxiv.org/pdf/2509.10798</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10798]] Judge Q: Trainable Queries for Optimized Information Retention in KV Cache Eviction(https://arxiv.org/abs/2509.10798)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) utilize key-value (KV) cache to store historical information during sequence processing. The size of KV cache grows linearly as the length of the sequence extends, which seriously affects memory usage and decoding efficiency. Current methods for KV cache eviction typically utilize the last window from the pre-filling phase as queries to compute the KV importance scores for eviction. Although this scheme is simple to implement, it tends to overly focus on local information, potentially leading to the neglect or omission of crucial global information. To mitigate this issue, we propose Judge Q, a novel training method which incorporates a soft token list. This method only tunes the model's embedding layer at a low training cost. By concatenating the soft token list at the end of the input sequence, we train these tokens' attention map to the original input sequence to align with that of the actual decoded tokens. In this way, the queries corresponding to the soft tokens can effectively capture global information and better evaluate the importance of the keys and values within the KV cache, thus maintaining decoding quality when KV cache is evicted. Under the same eviction budget, our method exhibits less performance degradation compared to existing eviction approaches. We validate our approach through experiments conducted on models such as Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3, using benchmarks including LongBench, RULER, and Needle-in-a-Haystack. Results indicate an improvement of approximately 1 point on the LongBench and over 3 points on RULER. This proposed methodology can be seamlessly integrated into existing open-source models with minimal training overhead, thereby enhancing performance in KV cache eviction scenarios.</li>
</ul>

<h3>Title: Rethinking Sparse Autoencoders: Select-and-Project for Fairness and Control from Encoder Features Alone</h3>
<ul>
<li><strong>Authors: </strong>Antonio Bărbălau, Cristian Daniel Păduraru, Teodor Poncu, Alexandru Tifrea, Elena Burceanu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10809">https://arxiv.org/abs/2509.10809</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10809">https://arxiv.org/pdf/2509.10809</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10809]] Rethinking Sparse Autoencoders: Select-and-Project for Fairness and Control from Encoder Features Alone(https://arxiv.org/abs/2509.10809)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Sparse Autoencoders (SAEs) have proven valuable due to their ability to provide interpretable and steerable representations. Current debiasing methods based on SAEs manipulate these sparse activations presuming that feature representations are housed within decoder weights. We challenge this fundamental assumption and introduce an encoder-focused alternative for representation debiasing, contributing three key findings: (i) we highlight an unconventional SAE feature selection strategy, (ii) we propose a novel SAE debiasing methodology that orthogonalizes input embeddings against encoder weights, and (iii) we establish a performance-preserving mechanism during debiasing through encoder weight interpolation. Our Selection and Projection framework, termed S\&P TopK, surpasses conventional SAE usage in fairness metrics by a factor of up to 3.2 and advances state-of-the-art test-time VLM debiasing results by a factor of up to 1.8 while maintaining downstream performance.</li>
</ul>

<h3>Title: Automatic Generation of a Cryptography Misuse Taxonomy Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yang Zhang, Wenyi Ouyang, Yi Zhang, Liang Cheng, Chen Wu, Wenxin Hu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10814">https://arxiv.org/abs/2509.10814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10814">https://arxiv.org/pdf/2509.10814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10814]] Automatic Generation of a Cryptography Misuse Taxonomy Using Large Language Models(https://arxiv.org/abs/2509.10814)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>The prevalence of cryptographic API misuse (CAM) is compromising the effectiveness of cryptography and in turn the security of modern systems and applications. Despite extensive efforts to develop CAM detection tools, these tools typically rely on a limited set of predefined rules from human-curated knowledge. This rigid, rule-based approach hinders adaptation to evolving CAM patterns in real practices. We propose leveraging large language models (LLMs), trained on publicly available cryptography-related data, to automatically detect and classify CAMs in real-world code to address this limitation. Our method enables the development and continuous expansion of a CAM taxonomy, supporting developers and detection tools in tracking and understanding emerging CAM patterns. Specifically, we develop an LLM-agnostic prompt engineering method to guide LLMs in detecting CAM instances from C/C++, Java, Python, and Go code, and then classifying them into a hierarchical taxonomy. Using a data set of 3,492 real-world software programs, we demonstrate the effectiveness of our approach with mainstream LLMs, including GPT, Llama, Gemini, and Claude. It also allows us to quantitatively measure and compare the performance of these LLMs in analyzing CAM in realistic code. Our evaluation produced a taxonomy with 279 base CAM categories, 36 of which are not addressed by existing taxonomies. To validate its practical value, we encode 11 newly identified CAM types into detection rules and integrate them into existing tools. Experiments show that such integration expands the tools' detection capabilities.</li>
</ul>

<h3>Title: From Paradigm Shift to Audit Rift: Exploring Vulnerabilities and Audit Tips for TON Smart Contracts</h3>
<ul>
<li><strong>Authors: </strong>Yury Yanovich, Sergey Sobolev, Yash Madhwal, Kirill Ziborov, Vladimir Gorgadze, Victoria Kovalevskay, Elizaveta Smirnova, Matvey Mishuris, Subodh Sharma</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10823">https://arxiv.org/abs/2509.10823</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10823">https://arxiv.org/pdf/2509.10823</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10823]] From Paradigm Shift to Audit Rift: Exploring Vulnerabilities and Audit Tips for TON Smart Contracts(https://arxiv.org/abs/2509.10823)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, robust</a></li>
<li><strong>Abstract: </strong>The Open Network (TON) is a high-performance blockchain platform designed for scalability and efficiency, leveraging an asynchronous execution model and a multi-layered architecture. While TON's design offers significant advantages, it also introduces unique challenges for smart contract development and security. This paper introduces a comprehensive audit checklist for TON smart contracts, based on an analysis of 34 professional audit reports containing 233 real-world vulnerabilities. The checklist addresses TON-specific challenges, such as asynchronous message handling, and provides actionable insights for developers and auditors. We also present detailed case studies of vulnerabilities in TON smart contracts, highlighting their implications and offering lessons learned. By adopting this checklist, developers and auditors can systematically identify and mitigate vulnerabilities, enhancing the security and reliability of TON-based projects. Our work bridges the gap between Ethereum's mature audit methodologies and the emerging needs of the TON ecosystem, fostering a more secure and robust blockchain environment.</li>
</ul>

<h3>Title: Multi-Task Diffusion Approach For Prediction of Glioma Tumor Progression</h3>
<ul>
<li><strong>Authors: </strong>Aghiles Kebaili, Romain Modzelewski, Jérôme Lapuyade-Lahorgue, Maxime Fontanilles, Sébastien Thureau, Su Ruan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10824">https://arxiv.org/abs/2509.10824</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10824">https://arxiv.org/pdf/2509.10824</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10824]] Multi-Task Diffusion Approach For Prediction of Glioma Tumor Progression(https://arxiv.org/abs/2509.10824)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Glioma, an aggressive brain malignancy characterized by rapid progression and its poor prognosis, poses significant challenges for accurate evolution prediction. These challenges are exacerbated by sparse, irregularly acquired longitudinal MRI data in clinical practice, where incomplete follow-up sequences create data imbalances and make reliable modeling difficult. In this paper, we present a multitask diffusion framework for time-agnostic, pixel-wise prediction of glioma progression. The model simultaneously generates future FLAIR sequences at any chosen time point and estimates spatial probabilistic tumor evolution maps derived using signed distance fields (SDFs), allowing uncertainty quantification. To capture temporal dynamics of tumor evolution across arbitrary intervals, we integrate a pretrained deformation module that models inter-scan changes using deformation fields. Regarding the common clinical limitation of data scarcity, we implement a targeted augmentation pipeline that synthesizes complete sequences of three follow-up scans and imputes missing MRI modalities from available patient studies, improving the stability and accuracy of predictive models. Based on merely two follow-up scans at earlier timepoints, our framework produces flexible time-depending probability maps, enabling clinicians to interrogate tumor progression risks at any future temporal milestone. We further introduce a radiotherapy-weighted focal loss term that leverages radiation dose maps, as these highlight regions of greater clinical importance during model training. The proposed method was trained on a public dataset and evaluated on an internal private dataset, achieving promising results in both cases</li>
</ul>

<h3>Title: Towards Automated Error Discovery: A Study in Conversational AI</h3>
<ul>
<li><strong>Authors: </strong>Dominic Petrak, Thy Thy Tran, Iryna Gurevych</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10833">https://arxiv.org/abs/2509.10833</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10833">https://arxiv.org/pdf/2509.10833</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10833]] Towards Automated Error Discovery: A Study in Conversational AI(https://arxiv.org/abs/2509.10833)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Although LLM-based conversational agents demonstrate strong fluency and coherence, they still produce undesirable behaviors (errors) that are challenging to prevent from reaching users during deployment. Recent research leverages large language models (LLMs) to detect errors and guide response-generation models toward improvement. However, current LLMs struggle to identify errors not explicitly specified in their instructions, such as those arising from updates to the response-generation model or shifts in user behavior. In this work, we introduce Automated Error Discovery, a framework for detecting and defining errors in conversational AI, and propose SEEED (Soft Clustering Extended Encoder-Based Error Detection), as an encoder-based approach to its implementation. We enhance the Soft Nearest Neighbor Loss by amplifying distance weighting for negative samples and introduce Label-Based Sample Ranking to select highly contrastive examples for better representation learning. SEEED outperforms adapted baselines -- including GPT-4o and Phi-4 -- across multiple error-annotated dialogue datasets, improving the accuracy for detecting unknown errors by up to 8 points and demonstrating strong generalization to unknown intent detection.</li>
</ul>

<h3>Title: Point-Plane Projections for Accurate LiDAR Semantic Segmentation in Small Data Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Simone Mosco, Daniel Fusaro, Wanmeng Li, Emanuele Menegatti, Alberto Pretto</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10841">https://arxiv.org/abs/2509.10841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10841">https://arxiv.org/pdf/2509.10841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10841]] Point-Plane Projections for Accurate LiDAR Semantic Segmentation in Small Data Scenarios(https://arxiv.org/abs/2509.10841)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>LiDAR point cloud semantic segmentation is essential for interpreting 3D environments in applications such as autonomous driving and robotics. Recent methods achieve strong performance by exploiting different point cloud representations or incorporating data from other sensors, such as cameras or external datasets. However, these approaches often suffer from high computational complexity and require large amounts of training data, limiting their generalization in data-scarce scenarios. In this paper, we improve the performance of point-based methods by effectively learning features from 2D representations through point-plane projections, enabling the extraction of complementary information while relying solely on LiDAR data. Additionally, we introduce a geometry-aware technique for data augmentation that aligns with LiDAR sensor properties and mitigates class imbalance. We implemented and evaluated our method that applies point-plane projections onto multiple informative 2D representations of the point cloud. Experiments demonstrate that this approach leads to significant improvements in limited-data scenarios, while also achieving competitive results on two publicly available standard datasets, as SemanticKITTI and PandaSet. The code of our method is available at this https URL</li>
</ul>

<h3>Title: OpenUrban3D: Annotation-Free Open-Vocabulary Semantic Segmentation of Large-Scale Urban Point Clouds</h3>
<ul>
<li><strong>Authors: </strong>Chongyu Wang, Kunlei Jing, Jihua Zhu, Di Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10842">https://arxiv.org/abs/2509.10842</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10842">https://arxiv.org/pdf/2509.10842</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10842]] OpenUrban3D: Annotation-Free Open-Vocabulary Semantic Segmentation of Large-Scale Urban Point Clouds(https://arxiv.org/abs/2509.10842)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Open-vocabulary semantic segmentation enables models to recognize and segment objects from arbitrary natural language descriptions, offering the flexibility to handle novel, fine-grained, or functionally defined categories beyond fixed label sets. While this capability is crucial for large-scale urban point clouds that support applications such as digital twins, smart city management, and urban analytics, it remains largely unexplored in this domain. The main obstacles are the frequent absence of high-quality, well-aligned multi-view imagery in large-scale urban point cloud datasets and the poor generalization of existing three-dimensional (3D) segmentation pipelines across diverse urban environments with substantial variation in geometry, scale, and appearance. To address these challenges, we present OpenUrban3D, the first 3D open-vocabulary semantic segmentation framework for large-scale urban scenes that operates without aligned multi-view images, pre-trained point cloud segmentation networks, or manual annotations. Our approach generates robust semantic features directly from raw point clouds through multi-view, multi-granularity rendering, mask-level vision-language feature extraction, and sample-balanced fusion, followed by distillation into a 3D backbone model. This design enables zero-shot segmentation for arbitrary text queries while capturing both semantic richness and geometric priors. Extensive experiments on large-scale urban benchmarks, including SensatUrban and SUM, show that OpenUrban3D achieves significant improvements in both segmentation accuracy and cross-scene generalization over existing methods, demonstrating its potential as a flexible and scalable solution for 3D urban scene understanding.</li>
</ul>

<h3>Title: Evaluating Large Language Models for Evidence-Based Clinical Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Can Wang, Yiqun Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10843">https://arxiv.org/abs/2509.10843</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10843">https://arxiv.org/pdf/2509.10843</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10843]] Evaluating Large Language Models for Evidence-Based Clinical Question Answering(https://arxiv.org/abs/2509.10843)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated substantial progress in biomedical and clinical applications, motivating rigorous evaluation of their ability to answer nuanced, evidence-based questions. We curate a multi-source benchmark drawing from Cochrane systematic reviews and clinical guidelines, including structured recommendations from the American Heart Association and narrative guidance used by insurers. Using GPT-4o-mini and GPT-5, we observe consistent performance patterns across sources and clinical domains: accuracy is highest on structured guideline recommendations (90%) and lower on narrative guideline and systematic review questions (60--70%). We also find a strong correlation between accuracy and the citation count of the underlying systematic reviews, where each doubling of citations is associated with roughly a 30% increase in the odds of a correct answer. Models show moderate ability to reason about evidence quality when contextual information is supplied. When we incorporate retrieval-augmented prompting, providing the gold-source abstract raises accuracy on previously incorrect items to 0.79; providing top 3 PubMed abstracts (ranked by semantic relevance) improves accuracy to 0.23, while random abstracts reduce accuracy (0.10, within temperature variation). These effects are mirrored in GPT-4o-mini, underscoring that source clarity and targeted retrieval -- not just model size -- drive performance. Overall, our results highlight both the promise and current limitations of LLMs for evidence-based clinical question answering. Retrieval-augmented prompting emerges as a useful strategy to improve factual accuracy and alignment with source evidence, while stratified evaluation by specialty and question type remains essential to understand current knowledge access and to contextualize model performance.</li>
</ul>

<h3>Title: Text2Sign Diffusion: A Generative Approach for Gloss-Free Sign Language Production</h3>
<ul>
<li><strong>Authors: </strong>Liqian Feng, Lintao Wang, Kun Hu, Dehui Kong, Zhiyong Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10845">https://arxiv.org/abs/2509.10845</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10845">https://arxiv.org/pdf/2509.10845</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10845]] Text2Sign Diffusion: A Generative Approach for Gloss-Free Sign Language Production(https://arxiv.org/abs/2509.10845)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Sign language production (SLP) aims to translate spoken language sentences into a sequence of pose frames in a sign language, bridging the communication gap and promoting digital inclusion for deaf and hard-of-hearing communities. Existing methods typically rely on gloss, a symbolic representation of sign language words or phrases that serves as an intermediate step in SLP. This limits the flexibility and generalization of SLP, as gloss annotations are often unavailable and language-specific. Therefore, we present a novel diffusion-based generative approach - Text2Sign Diffusion (Text2SignDiff) for gloss-free SLP. Specifically, a gloss-free latent diffusion model is proposed to generate sign language sequences from noisy latent sign codes and spoken text jointly, reducing the potential error accumulation through a non-autoregressive iterative denoising process. We also design a cross-modal signing aligner that learns a shared latent space to bridge visual and textual content in sign and spoken languages. This alignment supports the conditioned diffusion-based process, enabling more accurate and contextually relevant sign language generation without gloss. Extensive experiments on the commonly used PHOENIX14T and How2Sign datasets demonstrate the effectiveness of our method, achieving the state-of-the-art performance.</li>
</ul>

<h3>Title: Neurosymbolic AI Transfer Learning Improves Network Intrusion Detection</h3>
<ul>
<li><strong>Authors: </strong>Huynh T. T. Tran, Jacob Sander, Achraf Cohen, Brian Jalaian, Nathaniel D. Bastian</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10850">https://arxiv.org/abs/2509.10850</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10850">https://arxiv.org/pdf/2509.10850</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10850]] Neurosymbolic AI Transfer Learning Improves Network Intrusion Detection(https://arxiv.org/abs/2509.10850)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Transfer learning is commonly utilized in various fields such as computer vision, natural language processing, and medical imaging due to its impressive capability to address subtasks and work with different datasets. However, its application in cybersecurity has not been thoroughly explored. In this paper, we present an innovative neurosymbolic AI framework designed for network intrusion detection systems, which play a crucial role in combating malicious activities in cybersecurity. Our framework leverages transfer learning and uncertainty quantification. The findings indicate that transfer learning models, trained on large and well-structured datasets, outperform neural-based models that rely on smaller datasets, paving the way for a new era in cybersecurity solutions.</li>
</ul>

<h3>Title: Large Language Models for Security Operations Centers: A Comprehensive Survey</h3>
<ul>
<li><strong>Authors: </strong>Ali Habibzadeh, Farid Feyzi, Reza Ebrahimi Atani</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10858">https://arxiv.org/abs/2509.10858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10858">https://arxiv.org/pdf/2509.10858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10858]] Large Language Models for Security Operations Centers: A Comprehensive Survey(https://arxiv.org/abs/2509.10858)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, generative, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have emerged as powerful tools capable of understanding and generating human-like text, offering transformative potential across diverse domains. The Security Operations Center (SOC), responsible for safeguarding digital infrastructure, represents one of these domains. SOCs serve as the frontline of defense in cybersecurity, tasked with continuous monitoring, detection, and response to incidents. However, SOCs face persistent challenges such as high alert volumes, limited resources, high demand for experts with advanced knowledge, delayed response times, and difficulties in leveraging threat intelligence effectively. In this context, LLMs can offer promising solutions by automating log analysis, streamlining triage, improving detection accuracy, and providing the required knowledge in less time. This survey systematically explores the integration of generative AI and more specifically LLMs into SOC workflow, providing a structured perspective on its capabilities, challenges, and future directions. We believe that this survey offers researchers and SOC managers a broad overview of the current state of LLM integration within academic study. To the best of our knowledge, this is the first comprehensive study to examine LLM applications in SOCs in details.</li>
</ul>

<h3>Title: Quantifier Scope Interpretation in Language Learners and LLMs</h3>
<ul>
<li><strong>Authors: </strong>Shaohua Fang, Yue Li, Yan Cong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10860">https://arxiv.org/abs/2509.10860</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10860">https://arxiv.org/pdf/2509.10860</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10860]] Quantifier Scope Interpretation in Language Learners and LLMs(https://arxiv.org/abs/2509.10860)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Sentences with multiple quantifiers often lead to interpretive ambiguities, which can vary across languages. This study adopts a cross-linguistic approach to examine how large language models (LLMs) handle quantifier scope interpretation in English and Chinese, using probabilities to assess interpretive likelihood. Human similarity (HS) scores were used to quantify the extent to which LLMs emulate human performance across language groups. Results reveal that most LLMs prefer the surface scope interpretations, aligning with human tendencies, while only some differentiate between English and Chinese in the inverse scope preferences, reflecting human-similar patterns. HS scores highlight variability in LLMs' approximation of human behavior, but their overall potential to align with humans is notable. Differences in model architecture, scale, and particularly models' pre-training data language background, significantly influence how closely LLMs approximate human quantifier scope interpretations.</li>
</ul>

<h3>Title: CogGNN: Cognitive Graph Neural Networks in Generative Connectomics</h3>
<ul>
<li><strong>Authors: </strong>Mayssa Soussia, Yijun Lin, Mohamed Ali Mahjoub, Islem Rekik</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10864">https://arxiv.org/abs/2509.10864</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10864">https://arxiv.org/pdf/2509.10864</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10864]] CogGNN: Cognitive Graph Neural Networks in Generative Connectomics(https://arxiv.org/abs/2509.10864)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative learning has advanced network neuroscience, enabling tasks like graph super-resolution, temporal graph prediction, and multimodal brain graph fusion. However, current methods, mainly based on graph neural networks (GNNs), focus solely on structural and topological properties, neglecting cognitive traits. To address this, we introduce the first cognified generative model, CogGNN, which endows GNNs with cognitive capabilities (e.g., visual memory) to generate brain networks that preserve cognitive features. While broadly applicable, we present CogGNN, a specific variant designed to integrate visual input, a key factor in brain functions like pattern recognition and memory recall. As a proof of concept, we use our model to learn connectional brain templates (CBTs), population-level fingerprints from multi-view brain networks. Unlike prior work that overlooks cognitive properties, CogGNN generates CBTs that are both cognitively and structurally meaningful. Our contributions are: (i) a novel cognition-aware generative model with a visual-memory-based loss; (ii) a CBT-learning framework with a co-optimization strategy to yield well-centered, discriminative, cognitively enhanced templates. Extensive experiments show that CogGNN outperforms state-of-the-art methods, establishing a strong foundation for cognitively grounded brain network modeling.</li>
</ul>

<h3>Title: GTHNA: Local-global Graph Transformer with Memory Reconstruction for Holistic Node Anomaly Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Mingkang Li, Xuexiong Luo, Yue Zhang, Yaoyang Li, Fu Lin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10869">https://arxiv.org/abs/2509.10869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10869">https://arxiv.org/pdf/2509.10869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10869]] GTHNA: Local-global Graph Transformer with Memory Reconstruction for Holistic Node Anomaly Evaluation(https://arxiv.org/abs/2509.10869)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Anomaly detection in graph-structured data is an inherently challenging problem, as it requires the identification of rare nodes that deviate from the majority in both their structural and behavioral characteristics. Existing methods, such as those based on graph convolutional networks (GCNs), often suffer from over-smoothing, which causes the learned node representations to become indistinguishable. Furthermore, graph reconstruction-based approaches are vulnerable to anomalous node interference during the reconstruction process, leading to inaccurate anomaly detection. In this work, we propose a novel and holistic anomaly evaluation framework that integrates three key components: a local-global Transformer encoder, a memory-guided reconstruction mechanism, and a multi-scale representation matching strategy. These components work synergistically to enhance the model's ability to capture both local and global structural dependencies, suppress the influence of anomalous nodes, and assess anomalies from multiple levels of granularity. Anomaly scores are computed by combining reconstruction errors and memory matching signals, resulting in a more robust evaluation. Extensive experiments on seven benchmark datasets demonstrate that our method outperforms existing state-of-the-art approaches, offering a comprehensive and generalizable solution for anomaly detection across various graph domains.</li>
</ul>

<h3>Title: Term2Note: Synthesising Differentially Private Clinical Notes from Medical Terms</h3>
<ul>
<li><strong>Authors: </strong>Yuping Wu, Viktor Schlegel, Warren Del-Pinto, Srinivasan Nandakumar, Iqra Zahid, Yidan Sun, Usama Farghaly Omar, Amirah Jasmine, Arun-Kumar Kaliya-Perumal, Chun Shen Tham, Gabriel Connors, Anil A Bharath, Goran Nenadic</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10882">https://arxiv.org/abs/2509.10882</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10882">https://arxiv.org/pdf/2509.10882</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10882]] Term2Note: Synthesising Differentially Private Clinical Notes from Medical Terms(https://arxiv.org/abs/2509.10882)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>Training data is fundamental to the success of modern machine learning models, yet in high-stakes domains such as healthcare, the use of real-world training data is severely constrained by concerns over privacy leakage. A promising solution to this challenge is the use of differentially private (DP) synthetic data, which offers formal privacy guarantees while maintaining data utility. However, striking the right balance between privacy protection and utility remains challenging in clinical note synthesis, given its domain specificity and the complexity of long-form text generation. In this paper, we present Term2Note, a methodology to synthesise long clinical notes under strong DP constraints. By structurally separating content and form, Term2Note generates section-wise note content conditioned on DP medical terms, with each governed by separate DP constraints. A DP quality maximiser further enhances synthetic notes by selecting high-quality outputs. Experimental results show that Term2Note produces synthetic notes with statistical properties closely aligned with real clinical notes, demonstrating strong fidelity. In addition, multi-label classification models trained on these synthetic notes perform comparably to those trained on real data, confirming their high utility. Compared to existing DP text generation baselines, Term2Note achieves substantial improvements in both fidelity and utility while operating under fewer assumptions, suggesting its potential as a viable privacy-preserving alternative to using sensitive clinical notes.</li>
</ul>

<h3>Title: CultureSynth: A Hierarchical Taxonomy-Guided and Retrieval-Augmented Framework for Cultural Question-Answer Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Zhang, Pei Zhang, Shuang Luo, Jialong Tang, Yu Wan, Baosong Yang, Fei Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10886">https://arxiv.org/abs/2509.10886</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10886">https://arxiv.org/pdf/2509.10886</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10886]] CultureSynth: A Hierarchical Taxonomy-Guided and Retrieval-Augmented Framework for Cultural Question-Answer Synthesis(https://arxiv.org/abs/2509.10886)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Cultural competence, defined as the ability to understand and adapt to multicultural contexts, is increasingly vital for large language models (LLMs) in global environments. While several cultural benchmarks exist to assess LLMs' cultural competence, current evaluations suffer from fragmented taxonomies, domain specificity, and heavy reliance on manual data annotation. To address these limitations, we introduce CultureSynth, a novel framework comprising (1) a comprehensive hierarchical multilingual cultural taxonomy covering 12 primary and 130 secondary topics, and (2) a Retrieval-Augmented Generation (RAG)-based methodology leveraging factual knowledge to synthesize culturally relevant question-answer pairs. The CultureSynth-7 synthetic benchmark contains 19,360 entries and 4,149 manually verified entries across 7 languages. Evaluation of 14 prevalent LLMs of different sizes reveals clear performance stratification led by ChatGPT-4o-Latest and Qwen2.5-72B-Instruct. The results demonstrate that a 3B-parameter threshold is necessary for achieving basic cultural competence, models display varying architectural biases in knowledge processing, and significant geographic disparities exist across models. We believe that CultureSynth offers a scalable framework for developing culturally aware AI systems while reducing reliance on manual annotation\footnote{Benchmark is available at this https URL.}.</li>
</ul>

<h3>Title: AutoOEP - A Multi-modal Framework for Online Exam Proctoring</h3>
<ul>
<li><strong>Authors: </strong>Aryan Kashyap Naveen, Bhuvanesh Singla, Raajan Wankhade, Shreesha M, Ramu S, Ram Mohana Reddy Guddeti</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10887">https://arxiv.org/abs/2509.10887</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10887">https://arxiv.org/pdf/2509.10887</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10887]] AutoOEP - A Multi-modal Framework for Online Exam Proctoring(https://arxiv.org/abs/2509.10887)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The burgeoning of online education has created an urgent need for robust and scalable systems to ensure academic integrity during remote examinations. Traditional human proctoring is often not feasible at scale, while existing automated solutions can be intrusive or fail to detect a wide range of cheating behaviors. This paper introduces AutoOEP (Automated Online Exam Proctoring), a comprehensive, multi-modal framework that leverages computer vision and machine learning to provide effective, automated proctoring. The system utilizes a dual-camera setup to capture both a frontal view of the examinee and a side view of the workspace, minimizing blind spots. Our approach integrates several parallel analyses: the Face Module performs continuous identity verification using ArcFace, along with head pose estimation, gaze tracking, and mouth movement analysis to detect suspicious cues. Concurrently, the Hand Module employs a fine-tuned YOLOv11 model for detecting prohibited items (e.g., mobile phones, notes) and tracks hand proximity to these objects. Features from these modules are aggregated and fed into a Long Short-Term Memory (LSTM) network that analyzes temporal patterns to calculate a real-time cheating probability score. We evaluate AutoOEP on a custom-collected dataset simulating diverse exam conditions. Our system achieves an accuracy of 90.7% in classifying suspicious activities. The object detection component obtains a mean Average Precision (mAP@.5) of 0.57 for prohibited items, and the entire framework processes video streams at approximately 2.4 frames per second without a GPU. The results demonstrate that AutoOEP is an effective and resource-efficient solution for automated proctoring, significantly reducing the need for human intervention and enhancing the integrity of online assessments.</li>
</ul>

<h3>Title: Finding SSH Strict Key Exchange Violations by State Learning</h3>
<ul>
<li><strong>Authors: </strong>Fabian Bäumer, Marcel Maehren, Marcus Brinkmann, Jörg Schwenk</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10895">https://arxiv.org/abs/2509.10895</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10895">https://arxiv.org/pdf/2509.10895</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10895]] Finding SSH Strict Key Exchange Violations by State Learning(https://arxiv.org/abs/2509.10895)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>SSH is an important protocol for secure remote shell access to servers on the Internet. At USENIX 2024, Bäumer et al. presented the Terrapin attack on SSH, which relies on the attacker injecting optional messages during the key exchange. To mitigate this attack, SSH vendors adopted an extension developed by OpenSSH called strict key exchange ("strict KEX"). With strict KEX, optional messages are forbidden during the handshake, preventing the attack. In practice, this should simplify the state machine of an SSH handshake to a linear message flow similar to that of TLS. In this work, we analyze the design, implementation, and security of strict KEX in popular SSH servers, using black-box state learning, which can uncover the hidden state machine of an implementation. In practice, it is limited by the number of learned messages and the complexity of the state machine. Thus, learning the complete state machine of SSH is infeasible. Previous research on SSH, therefore, excluded optional messages, learning only a partial state machine. However, these messages are a critical part of the Terrapin attack. We propose to instead learn the complete state machine of the handshake phase of an SSH server, but with strict KEX enabled. We investigate the security of ten SSH implementations supporting strict KEX for up to five key exchange algorithms. In total, we learn 33 state machines, revealing significant differences in the implementations. We show that seven implementations violate the strict KEX specification and find two critical security vulnerabilities. One results in a rogue session attack in the proprietary Tectia SSH implementation. Another affects the official SSH implementation of the Erlang Open Telecom Platform, and enables unauthenticated remote code execution in the security context of the SSH server.</li>
</ul>

<h3>Title: Total Variation Subgradient Guided Image Fusion for Dual-Camera CASSI System</h3>
<ul>
<li><strong>Authors: </strong>Weiqiang Zhao, Tianzhu Liu, Yuzhe Gui, Yanfeng Gu</a></li>
<li><strong>Subjects: </strong>cs.CV, physics.optics</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10897">https://arxiv.org/abs/2509.10897</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10897">https://arxiv.org/pdf/2509.10897</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10897]] Total Variation Subgradient Guided Image Fusion for Dual-Camera CASSI System(https://arxiv.org/abs/2509.10897)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Spectral imaging technology has long-faced fundamental challenges in balancing spectral, spatial, and temporal reso- lutions. While compressive sensing-based Coded Aperture Snapshot Spectral Imaging (CASSI) mitigates this trade-off through optical encoding, high compression ratios result in ill-posed reconstruction problems. Traditional model-based methods exhibit limited performance due to reliance on handcrafted inherent image priors, while deep learning approaches are constrained by their black-box nature, which compromises physical interpretability. To address these limitations, we propose a dual-camera CASSI reconstruction framework that integrates total variation (TV) subgradient theory. By es- tablishing an end-to-end SD-CASSI mathematical model, we reduce the computational complexity of solving the inverse problem and provide a mathematically well-founded framework for analyzing multi-camera systems. A dynamic regular- ization strategy is introduced, incorporating normalized gradient constraints from RGB/panchromatic-derived reference images, which constructs a TV subgradient similarity function with strict convex optimization guarantees. Leveraging spatial priors from auxiliary cameras, an adaptive reference generation and updating mechanism is designed to provide subgradient guidance. Experimental results demonstrate that the proposed method effectively preserves spatial-spectral structural consistency. The theoretical framework establishes an interpretable mathematical foundation for computational spectral imaging, demonstrating robust performance across diverse reconstruction scenarios. The source code is available at this https URL.</li>
</ul>

<h3>Title: Robustifying Diffusion-Denoised Smoothing Against Covariate Shift</h3>
<ul>
<li><strong>Authors: </strong>Ali Hedayatnia, Mostafa Tavassolipour, Babak Nadjar Araabi, Abdol-Hossein Vahabie</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10913">https://arxiv.org/abs/2509.10913</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10913">https://arxiv.org/pdf/2509.10913</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10913]] Robustifying Diffusion-Denoised Smoothing Against Covariate Shift(https://arxiv.org/abs/2509.10913)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Randomized smoothing is a well-established method for achieving certified robustness against l2-adversarial perturbations. By incorporating a denoiser before the base classifier, pretrained classifiers can be seamlessly integrated into randomized smoothing without significant performance degradation. Among existing methods, Diffusion Denoised Smoothing - where a pretrained denoising diffusion model serves as the denoiser - has produced state-of-the-art results. However, we show that employing a denoising diffusion model introduces a covariate shift via misestimation of the added noise, ultimately degrading the smoothed classifier's performance. To address this issue, we propose a novel adversarial objective function focused on the added noise of the denoising diffusion model. This approach is inspired by our understanding of the origin of the covariate shift. Our goal is to train the base classifier to ensure it is robust against the covariate shift introduced by the denoiser. Our method significantly improves certified accuracy across three standard classification benchmarks - MNIST, CIFAR-10, and ImageNet - achieving new state-of-the-art performance in l2-adversarial perturbations. Our implementation is publicly available at this https URL</li>
</ul>

<h3>Title: ToMA: Token Merge with Attention for Image Generation with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Wenbo Lu, Shaoyi Zheng, Yuxuan Xia, Shengjie Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10918">https://arxiv.org/abs/2509.10918</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10918">https://arxiv.org/pdf/2509.10918</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10918]] ToMA: Token Merge with Attention for Image Generation with Diffusion Models(https://arxiv.org/abs/2509.10918)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Diffusion models excel in high-fidelity image generation but face scalability limits due to transformers' quadratic attention complexity. Plug-and-play token reduction methods like ToMeSD and ToFu reduce FLOPs by merging redundant tokens in generated images but rely on GPU-inefficient operations (e.g., sorting, scattered writes), introducing overheads that negate theoretical speedups when paired with optimized attention implementations (e.g., FlashAttention). To bridge this gap, we propose Token Merge with Attention (ToMA), an off-the-shelf method that redesigns token reduction for GPU-aligned efficiency, with three key contributions: 1) a reformulation of token merge as a submodular optimization problem to select diverse tokens; 2) merge/unmerge as an attention-like linear transformation via GPU-friendly matrix operations; and 3) exploiting latent locality and sequential redundancy (pattern reuse) to minimize overhead. ToMA reduces SDXL/Flux generation latency by 24%/23%, respectively (with DINO $\Delta < 0.07$), outperforming prior methods. This work bridges the gap between theoretical and practical efficiency for transformers in diffusion.</li>
</ul>

<h3>Title: Aligning ESG Controversy Data with International Guidelines through Semi-Automatic Ontology Construction</h3>
<ul>
<li><strong>Authors: </strong>Tsuyoshi Iwata, Guillaume Comte, Melissa Flores, Ryoma Kondo, Ryohei Hisano</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10922">https://arxiv.org/abs/2509.10922</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10922">https://arxiv.org/pdf/2509.10922</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10922]] Aligning ESG Controversy Data with International Guidelines through Semi-Automatic Ontology Construction(https://arxiv.org/abs/2509.10922)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The growing importance of environmental, social, and governance data in regulatory and investment contexts has increased the need for accurate, interpretable, and internationally aligned representations of non-financial risks, particularly those reported in unstructured news sources. However, aligning such controversy-related data with principle-based normative frameworks, such as the United Nations Global Compact or Sustainable Development Goals, presents significant challenges. These frameworks are typically expressed in abstract language, lack standardized taxonomies, and differ from the proprietary classification systems used by commercial data providers. In this paper, we present a semi-automatic method for constructing structured knowledge representations of environmental, social, and governance events reported in the news. Our approach uses lightweight ontology design, formal pattern modeling, and large language models to convert normative principles into reusable templates expressed in the Resource Description Framework. These templates are used to extract relevant information from news content and populate a structured knowledge graph that links reported incidents to specific framework principles. The result is a scalable and transparent framework for identifying and interpreting non-compliance with international sustainability guidelines.</li>
</ul>

<h3>Title: Clarifying Model Transparency: Interpretability versus Explainability in Deep Learning with MNIST and IMDB Examples</h3>
<ul>
<li><strong>Authors: </strong>Mitali Raj</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10929">https://arxiv.org/abs/2509.10929</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10929">https://arxiv.org/pdf/2509.10929</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10929]] Clarifying Model Transparency: Interpretability versus Explainability in Deep Learning with MNIST and IMDB Examples(https://arxiv.org/abs/2509.10929)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability</a></li>
<li><strong>Abstract: </strong>The impressive capabilities of deep learning models are often counterbalanced by their inherent opacity, commonly termed the "black box" problem, which impedes their widespread acceptance in high-trust domains. In response, the intersecting disciplines of interpretability and explainability, collectively falling under the Explainable AI (XAI) umbrella, have become focal points of research. Although these terms are frequently used as synonyms, they carry distinct conceptual weights. This document offers a comparative exploration of interpretability and explainability within the deep learning paradigm, carefully outlining their respective definitions, objectives, prevalent methodologies, and inherent difficulties. Through illustrative examinations of the MNIST digit classification task and IMDB sentiment analysis, we substantiate a key argument: interpretability generally pertains to a model's inherent capacity for human comprehension of its operational mechanisms (global understanding), whereas explainability is more commonly associated with post-hoc techniques designed to illuminate the basis for a model's individual predictions or behaviors (local explanations). For example, feature attribution methods can reveal why a specific MNIST image is recognized as a '7', and word-level importance can clarify an IMDB sentiment outcome. However, these local insights do not render the complex underlying model globally transparent. A clear grasp of this differentiation, as demonstrated by these standard datasets, is vital for fostering dependable and sound artificial intelligence.</li>
</ul>

<h3>Title: Introducing Spotlight: A Novel Approach for Generating Captivating Key Information from Documents</h3>
<ul>
<li><strong>Authors: </strong>Ankan Mullick, Sombit Bose, Rounak Saha, Ayan Kumar Bhowmick, Aditya Vempaty, Prasenjit Dey, Ravi Kokku, Pawan Goyal, Niloy Ganguly</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10935">https://arxiv.org/abs/2509.10935</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10935">https://arxiv.org/pdf/2509.10935</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10935]] Introducing Spotlight: A Novel Approach for Generating Captivating Key Information from Documents(https://arxiv.org/abs/2509.10935)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce Spotlight, a novel paradigm for information extraction that produces concise, engaging narratives by highlighting the most compelling aspects of a document. Unlike traditional summaries, which prioritize comprehensive coverage, spotlights selectively emphasize intriguing content to foster deeper reader engagement with the source material. We formally differentiate spotlights from related constructs and support our analysis with a detailed benchmarking study using new datasets curated for this work. To generate high-quality spotlights, we propose a two-stage approach: fine-tuning a large language model on our benchmark data, followed by alignment via Direct Preference Optimization (DPO). Our comprehensive evaluation demonstrates that the resulting model not only identifies key elements with precision but also enhances readability and boosts the engagement value of the original document.</li>
</ul>

<h3>Title: An Interpretable Benchmark for Clickbait Detection and Tactic Attribution</h3>
<ul>
<li><strong>Authors: </strong>Lihi Nofar, Tomer Portal, Aviv Elbaz, Alexander Apartsin, Yehudit Aperstein</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10937">https://arxiv.org/abs/2509.10937</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10937">https://arxiv.org/pdf/2509.10937</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10937]] An Interpretable Benchmark for Clickbait Detection and Tactic Attribution(https://arxiv.org/abs/2509.10937)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, large language model</a></li>
<li><strong>Abstract: </strong>The proliferation of clickbait headlines poses significant challenges to the credibility of information and user trust in digital media. While recent advances in machine learning have improved the detection of manipulative content, the lack of explainability limits their practical adoption. This paper presents a model for explainable clickbait detection that not only identifies clickbait titles but also attributes them to specific linguistic manipulation strategies. We introduce a synthetic dataset generated by systematically augmenting real news headlines using a predefined catalogue of clickbait strategies. This dataset enables controlled experimentation and detailed analysis of model behaviour. We present a two-stage framework for automatic clickbait analysis comprising detection and tactic attribution. In the first stage, we compare a fine-tuned BERT classifier with large language models (LLMs), specifically GPT-4.0 and Gemini 2.4 Flash, under both zero-shot prompting and few-shot prompting enriched with illustrative clickbait headlines and their associated persuasive tactics. In the second stage, a dedicated BERT-based classifier predicts the specific clickbait strategies present in each headline. This work advances the development of transparent and trustworthy AI systems for combating manipulative media content. We share the dataset with the research community at this https URL</li>
</ul>

<h3>Title: Simulating Sinogram-Domain Motion and Correcting Image-Domain Artifacts Using Deep Learning in HR-pQCT Bone Imaging</h3>
<ul>
<li><strong>Authors: </strong>Farhan Sadik, Christopher L. Newman, Stuart J. Warden, Rachel K. Surowiec</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10961">https://arxiv.org/abs/2509.10961</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10961">https://arxiv.org/pdf/2509.10961</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10961]] Simulating Sinogram-Domain Motion and Correcting Image-Domain Artifacts Using Deep Learning in HR-pQCT Bone Imaging(https://arxiv.org/abs/2509.10961)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Rigid-motion artifacts, such as cortical bone streaking and trabecular smearing, hinder in vivo assessment of bone microstructures in high-resolution peripheral quantitative computed tomography (HR-pQCT). Despite various motion grading techniques, no motion correction methods exist due to the lack of standardized degradation models. We optimize a conventional sinogram-based method to simulate motion artifacts in HR-pQCT images, creating paired datasets of motion-corrupted images and their corresponding ground truth, which enables seamless integration into supervised learning frameworks for motion correction. As such, we propose an Edge-enhanced Self-attention Wasserstein Generative Adversarial Network with Gradient Penalty (ESWGAN-GP) to address motion artifacts in both simulated (source) and real-world (target) datasets. The model incorporates edge-enhancing skip connections to preserve trabecular edges and self-attention mechanisms to capture long-range dependencies, facilitating motion correction. A visual geometry group (VGG)-based perceptual loss is used to reconstruct fine micro-structural features. The ESWGAN-GP achieves a mean signal-to-noise ratio (SNR) of 26.78, structural similarity index measure (SSIM) of 0.81, and visual information fidelity (VIF) of 0.76 for the source dataset, while showing improved performance on the target dataset with an SNR of 29.31, SSIM of 0.87, and VIF of 0.81. The proposed methods address a simplified representation of real-world motion that may not fully capture the complexity of in vivo motion artifacts. Nevertheless, because motion artifacts present one of the foremost challenges to more widespread adoption of this modality, these methods represent an important initial step toward implementing deep learning-based motion correction in HR-pQCT.</li>
</ul>

<h3>Title: The Psychogenic Machine: Simulating AI Psychosis, Delusion Reinforcement and Harm Enablement in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Joshua Au Yeung, Jacopo Dalmasso, Luca Foschini, Richard JB Dobson, Zeljko Kraljevic</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10970">https://arxiv.org/abs/2509.10970</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10970">https://arxiv.org/pdf/2509.10970</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10970]] The Psychogenic Machine: Simulating AI Psychosis, Delusion Reinforcement and Harm Enablement in Large Language Models(https://arxiv.org/abs/2509.10970)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Background: Emerging reports of "AI psychosis" are on the rise, where user-LLM interactions may exacerbate or induce psychosis or adverse psychological symptoms. The sycophantic and agreeable nature of LLMs can beneficial, it can become a vector for harm by reinforcing delusional beliefs in vulnerable users. Methods: We introduce psychosis-bench, a novel benchmark designed to systematically evaluate the psychogenicity of LLMs comprimising 16 structured, 12-turn conversational scenarios simulating the progression of delusional themes(Erotic Delusions, Grandiose/Messianic Delusions, Referential Delusions) and potential harms. We evaluated eight prominent LLMs for Delusion Confirmation (DCS), Harm Enablement (HES), and Safety Intervention(SIS) across explicit and implicit conversational contexts. Findings: Across 1,536 simulated conversation turns, all LLMs demonstrated psychogenic potential, showing a strong tendency to perpetuate rather than challenge delusions (mean DCS of 0.91 $\pm$0.88). Models frequently enabled harmful user requests (mean HES of 0.69 $\pm$0.84) and offered safety interventions in only roughly a third of applicable turns (mean SIS of 0.37 $\pm$0.48). 51 / 128 (39.8%) of scenarios had no safety interventions offered. Performance was significantly worse in implicit scenarios, models were more likely to confirm delusions and enable harm while offering fewer interventions (p < .001). A strong correlation was found between DCS and HES (rs = .77). Model performance varied widely, indicating that safety is not an emergent property of scale alone. Conclusion: This study establishes LLM psychogenicity as a quantifiable risk and underscores the urgent need for re-thinking how we train LLMs. We frame this issue not merely as a technical challenge but as a public health imperative requiring collaboration between developers, policymakers, and healthcare professionals.</li>
</ul>

<h3>Title: PHLoRA: data-free Post-hoc Low-Rank Adapter extraction from full-rank checkpoint</h3>
<ul>
<li><strong>Authors: </strong>Bhoomit Vasani, Jack FitzGerald, Anjie Fang, Sushmit Vaish</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10971">https://arxiv.org/abs/2509.10971</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10971">https://arxiv.org/pdf/2509.10971</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10971]] PHLoRA: data-free Post-hoc Low-Rank Adapter extraction from full-rank checkpoint(https://arxiv.org/abs/2509.10971)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, data-free</a></li>
<li><strong>Abstract: </strong>We introduce PHLoRA (Pronounced "flora"). (Post-hoc LoRA), a simple yet powerful method to extract low-rank adaptation adapters from full-rank fine-tuned models without requiring access to training data or gradients. By computing the low-rank decomposition of weight differences between a base model and its fine-tuned counterpart, our method reconstructs adapter modules that can be merged or dynamically routed at inference time via S-LoRA, or served in scalable, industry settings using platforms like NVIDIA NIM. This approach amortizes latency overhead across requests and yields substantial cost savings. Unlike prior work that trains each adapter explicitly, our approach decouples fine-tuning from adapter generation, allowing adapter extraction from existing full-rank models or third-party checkpoints. Experiments on text, image, and video benchmarks using the Amazon Nova model family demonstrate that extracted adapters preserve high energy from the full weight delta, can be pruned safely, and yield negligible degradation in downstream task performance when re-merged. Overall, PHLoRA provides a practical path for making all existing full-rank checkpoints adapter-ready, democratizing scalable inference for all models.</li>
</ul>

<h3>Title: TrueSkin: Towards Fair and Accurate Skin Tone Recognition and Generation</h3>
<ul>
<li><strong>Authors: </strong>Haoming Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.10980">https://arxiv.org/abs/2509.10980</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.10980">https://arxiv.org/pdf/2509.10980</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.10980]] TrueSkin: Towards Fair and Accurate Skin Tone Recognition and Generation(https://arxiv.org/abs/2509.10980)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, generative</a></li>
<li><strong>Abstract: </strong>Skin tone recognition and generation play important roles in model fairness, healthcare, and generative AI, yet they remain challenging due to the lack of comprehensive datasets and robust methodologies. Compared to other human image analysis tasks, state-of-the-art large multimodal models (LMMs) and image generation models struggle to recognize and synthesize skin tones accurately. To address this, we introduce TrueSkin, a dataset with 7299 images systematically categorized into 6 classes, collected under diverse lighting conditions, camera angles, and capture settings. Using TrueSkin, we benchmark existing recognition and generation approaches, revealing substantial biases: LMMs tend to misclassify intermediate skin tones as lighter ones, whereas generative models struggle to accurately produce specified skin tones when influenced by inherent biases from unrelated attributes in the prompts, such as hairstyle or environmental context. We further demonstrate that training a recognition model on TrueSkin improves classification accuracy by more than 20\% compared to LMMs and conventional approaches, and fine-tuning with TrueSkin significantly improves skin tone fidelity in image generation models. Our findings highlight the need for comprehensive datasets like TrueSkin, which not only serves as a benchmark for evaluating existing models but also provides a valuable training resource to enhance fairness and accuracy in skin tone recognition and generation tasks.</li>
</ul>

<h3>Title: A Range-Based Sharding (RBS) Protocol for Scalable Enterprise Blockchain</h3>
<ul>
<li><strong>Authors: </strong>M.Z. Haider, M. Dias de Assuncao, Kaiwen Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11006">https://arxiv.org/abs/2509.11006</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11006">https://arxiv.org/pdf/2509.11006</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11006]] A Range-Based Sharding (RBS) Protocol for Scalable Enterprise Blockchain(https://arxiv.org/abs/2509.11006)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, fair</a></li>
<li><strong>Abstract: </strong>Blockchain technology offers decentralization and security but struggles with scalability, particularly in enterprise settings where efficiency and controlled access are paramount. Sharding is a promising solution for private blockchains, yet existing approaches face challenges in coordinating shards, ensuring fault tolerance with limited nodes, and minimizing the high overhead of consensus mechanisms like PBFT. This paper proposes the Range-Based Sharding (RBS) Protocol, a novel sharding mechanism tailored for enterprise blockchains, implemented on Quorum. Unlike traditional sharding models such as OmniLedger and non-sharding Corda framework, RBS employs a commit-reveal scheme for secure and unbiased shard allocation, ensuring fair validator distribution while reducing cross-shard transaction delays. Our approach enhances scalability by balancing computational loads across shards, reducing consensus overhead, and improving parallel transaction execution. Experimental evaluations demonstrate that RBS achieves significantly higher throughput and lower latency compared to existing enterprise sharding frameworks, making it a viable and efficient solution for largescale blockchain deployments.</li>
</ul>

<h3>Title: Improving Fungi Prototype Representations for Few-Shot Classification</h3>
<ul>
<li><strong>Authors: </strong>Abdarahmane Traore, Éric Hervet, Andy Couturier</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11020">https://arxiv.org/abs/2509.11020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11020">https://arxiv.org/pdf/2509.11020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11020]] Improving Fungi Prototype Representations for Few-Shot Classification(https://arxiv.org/abs/2509.11020)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The FungiCLEF 2025 competition addresses the challenge of automatic fungal species recognition using realistic, field-collected observational data. Accurate identification tools support both mycologists and citizen scientists, greatly enhancing large-scale biodiversity monitoring. Effective recognition systems in this context must handle highly imbalanced class distributions and provide reliable performance even when very few training samples are available for many species, especially rare and under-documented taxa that are often missing from standard training sets. According to competition organizers, about 20\% of all verified fungi observations, representing nearly 20,000 instances, are associated with these rarely recorded species. To tackle this challenge, we propose a robust deep learning method based on prototypical networks, which enhances prototype representations for few-shot fungal classification. Our prototypical network approach exceeds the competition baseline by more than 30 percentage points in Recall@5 on both the public (PB) and private (PR) leaderboards. This demonstrates strong potential for accurately identifying both common and rare fungal species, supporting the main objectives of FungiCLEF 2025.</li>
</ul>

<h3>Title: Cluster-Level Sparse Multi-Instance Learning for Whole-Slide Images</h3>
<ul>
<li><strong>Authors: </strong>Yuedi Zhang, Zhixiang Xia, Guosheng Yin, Bin Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11034">https://arxiv.org/abs/2509.11034</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11034">https://arxiv.org/pdf/2509.11034</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11034]] Cluster-Level Sparse Multi-Instance Learning for Whole-Slide Images(https://arxiv.org/abs/2509.11034)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Multi-Instance Learning (MIL) is pivotal for analyzing complex, weakly labeled datasets, such as whole-slide images (WSIs) in computational pathology, where bags comprise unordered collections of instances with sparse diagnostic relevance. Traditional MIL approaches, including early statistical methods and recent attention-based frameworks, struggle with instance redundancy and lack explicit mechanisms for discarding non-informative instances, limiting their robustness and interpretability. We propose Cluster-level Sparse MIL (csMIL), a novel framework that integrates global-local instance clustering, within-cluster attention, and cluster-level sparsity induction to address these challenges. Our csMIL first performs global clustering across all bags to establish $K$ cluster centers, followed by local clustering within each bag to assign cluster labels. Attention scores are computed within each cluster, and sparse regularization is applied to cluster weights, enabling the selective retention of diagnostically relevant clusters while discarding irrelevant ones. This approach enhances robustness to noisy instances, improves interpretability by identifying critical regions, and reduces computational complexity. Theoretical analysis demonstrates that csMIL requires $O(s log K)$ bags to recover $s$ relevant clusters, aligning with compressed sensing principles. Empirically, csMIL achieves state-of-the-art performance on two public histopathology benchmarks (CAMELYON16, TCGA-NSCLC).</li>
</ul>

<h3>Title: Data-Efficient Ensemble Weather Forecasting with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Kevin Valencia, Ziyang Liu, Justin Cui</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11047">https://arxiv.org/abs/2509.11047</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11047">https://arxiv.org/pdf/2509.11047</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11047]] Data-Efficient Ensemble Weather Forecasting with Diffusion Models(https://arxiv.org/abs/2509.11047)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Although numerical weather forecasting methods have dominated the field, recent advances in deep learning methods, such as diffusion models, have shown promise in ensemble weather forecasting. However, such models are typically autoregressive and are thus computationally expensive. This is a challenge in climate science, where data can be limited, costly, or difficult to work with. In this work, we explore the impact of curated data selection on these autoregressive diffusion models. We evaluate several data sampling strategies and show that a simple time stratified sampling approach achieves performance similar to or better than full-data training. Notably, it outperforms the full-data model on certain metrics and performs only slightly worse on others while using only 20% of the training data. Our results demonstrate the feasibility of data-efficient diffusion training, especially for weather forecasting, and motivates future work on adaptive or model-aware sampling methods that go beyond random or purely temporal sampling.</li>
</ul>

<h3>Title: An Advanced Convolutional Neural Network for Bearing Fault Diagnosis under Limited Data</h3>
<ul>
<li><strong>Authors: </strong>Shengke Sun, Shuzhen Han, Ziqian Luan, Xinghao Qin, Jiao Yin, Zhanshan Zhao, Jinli Cao, Hua Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11053">https://arxiv.org/abs/2509.11053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11053">https://arxiv.org/pdf/2509.11053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11053]] An Advanced Convolutional Neural Network for Bearing Fault Diagnosis under Limited Data(https://arxiv.org/abs/2509.11053)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, generative</a></li>
<li><strong>Abstract: </strong>In the area of bearing fault diagnosis, deep learning (DL) methods have been widely used recently. However, due to the high cost or privacy concerns, high-quality labeled data are scarce in real world scenarios. While few-shot learning has shown promise in addressing data scarcity, existing methods still face significant limitations in this domain. Traditional data augmentation techniques often suffer from mode collapse and generate low-quality samples that fail to capture the diversity of bearing fault patterns. Moreover, conventional convolutional neural networks (CNNs) with local receptive fields makes them inadequate for extracting global features from complex vibration signals. Additionally, existing methods fail to model the intricate relationships between limited training samples. To solve these problems, we propose an advanced data augmentation and contrastive fourier convolution framework (DAC-FCF) for bearing fault diagnosis under limited data. Firstly, a novel conditional consistent latent representation and reconstruction generative adversarial network (CCLR-GAN) is proposed to generate more diverse data. Secondly, a contrastive learning based joint optimization mechanism is utilized to better model the relations between the available training data. Finally, we propose a 1D fourier convolution neural network (1D-FCNN) to achieve a global-aware of the input data. Experiments demonstrate that DAC-FCF achieves significant improvements, outperforming baselines by up to 32\% on case western reserve university (CWRU) dataset and 10\% on a self-collected test bench. Extensive ablation experiments prove the effectiveness of the proposed components. Thus, the proposed DAC-FCF offers a promising solution for bearing fault diagnosis under limited data.</li>
</ul>

<h3>Title: Action Hints: Semantic Typicality and Context Uniqueness for Generalizable Skeleton-based Video Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Canhui Tang, Sanping Zhou, Haoyue Shi, Le Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11058">https://arxiv.org/abs/2509.11058</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11058">https://arxiv.org/pdf/2509.11058</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11058]] Action Hints: Semantic Typicality and Context Uniqueness for Generalizable Skeleton-based Video Anomaly Detection(https://arxiv.org/abs/2509.11058)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Zero-Shot Video Anomaly Detection (ZS-VAD) requires temporally localizing anomalies without target domain training data, which is a crucial task due to various practical concerns, e.g., data privacy or new surveillance deployments. Skeleton-based approach has inherent generalizable advantages in achieving ZS-VAD as it eliminates domain disparities both in background and human appearance. However, existing methods only learn low-level skeleton representation and rely on the domain-limited normality boundary, which cannot generalize well to new scenes with different normal and abnormal behavior patterns. In this paper, we propose a novel zero-shot video anomaly detection framework, unlocking the potential of skeleton data via action typicality and uniqueness learning. Firstly, we introduce a language-guided semantic typicality modeling module that projects skeleton snippets into action semantic space and distills LLM's knowledge of typical normal and abnormal behaviors during training. Secondly, we propose a test-time context uniqueness analysis module to finely analyze the spatio-temporal differences between skeleton snippets and then derive scene-adaptive boundaries. Without using any training samples from the target domain, our method achieves state-of-the-art results against skeleton-based methods on four large-scale VAD datasets: ShanghaiTech, UBnormal, NWPU, and UCF-Crime, featuring over 100 unseen surveillance scenes.</li>
</ul>

<h3>Title: Organoid Tracker: A SAM2-Powered Platform for Zero-shot Cyst Analysis in Human Kidney Organoid Videos</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyu Huang, Lauren M Maxson, Trang Nguyen, Cheng Jack Song, Yuankai Huo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11063">https://arxiv.org/abs/2509.11063</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11063">https://arxiv.org/pdf/2509.11063</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11063]] Organoid Tracker: A SAM2-Powered Platform for Zero-shot Cyst Analysis in Human Kidney Organoid Videos(https://arxiv.org/abs/2509.11063)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Recent advances in organoid models have revolutionized the study of human kidney disease mechanisms and drug discovery by enabling scalable, cost-effective research without the need for animal sacrifice. Here, we present a kidney organoid platform optimized for efficient screening in polycystic kidney disease (PKD). While these systems generate rich spatial-temporal microscopy video datasets, current manual approaches to analysis remain limited to coarse classifications (e.g., hit vs. non-hit), often missing valuable pixel-level and longitudinal information. To help overcome this bottleneck, we developed Organoid Tracker, a graphical user interface (GUI) platform designed with a modular plugin architecture, which empowers researchers to extract detailed, quantitative metrics without programming expertise. Built on the cutting-edge vision foundation model Segment Anything Model 2 (SAM2), Organoid Tracker enables zero-shot segmentation and automated analysis of spatial-temporal microscopy videos. It quantifies key metrics such as cyst formation rate, growth velocity, and morphological changes, while generating comprehensive reports. By providing an extensible, open-source framework, Organoid Tracker offers a powerful solution for improving and accelerating research in kidney development, PKD modeling, and therapeutic discovery. The platform is publicly available as open-source software at this https URL.</li>
</ul>

<h3>Title: Machine Learning Framework for Audio-Based Equipment Condition Monitoring: A Comparative Study of Classification Algorithms</h3>
<ul>
<li><strong>Authors: </strong>Srijesh Pillai, Yodhin Agarwal, Zaheeruddin Ahmed</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11075">https://arxiv.org/abs/2509.11075</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11075">https://arxiv.org/pdf/2509.11075</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11075]] Machine Learning Framework for Audio-Based Equipment Condition Monitoring: A Comparative Study of Classification Algorithms(https://arxiv.org/abs/2509.11075)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Audio-based equipment condition monitoring suffers from a lack of standardized methodologies for algorithm selection, hindering reproducible research. This paper addresses this gap by introducing a comprehensive framework for the systematic and statistically rigorous evaluation of machine learning models. Leveraging a rich 127-feature set across time, frequency, and time-frequency domains, our methodology is validated on both synthetic and real-world datasets. Results demonstrate that an ensemble method achieves superior performance (94.2% accuracy, 0.942 F1-score), with statistical testing confirming its significant outperformance of individual algorithms by 8-15%. Ultimately, this work provides a validated benchmarking protocol and practical guidelines for selecting robust monitoring solutions in industrial settings.</li>
</ul>

<h3>Title: Mars Traversability Prediction: A Multi-modal Self-supervised Approach for Costmap Generation</h3>
<ul>
<li><strong>Authors: </strong>Zongwu Xie, Kaijie Yun, Yang Liu, Yiming Ji, Han Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11082">https://arxiv.org/abs/2509.11082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11082">https://arxiv.org/pdf/2509.11082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11082]] Mars Traversability Prediction: A Multi-modal Self-supervised Approach for Costmap Generation(https://arxiv.org/abs/2509.11082)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We present a robust multi-modal framework for predicting traversability costmaps for planetary rovers. Our model fuses camera and LiDAR data to produce a bird's-eye-view (BEV) terrain costmap, trained self-supervised using IMU-derived labels. Key updates include a DINOv3-based image encoder, FiLM-based sensor fusion, and an optimization loss combining Huber and smoothness terms. Experimental ablations (removing image color, occluding inputs, adding noise) show only minor changes in MAE/MSE (e.g. MAE increases from ~0.0775 to 0.0915 when LiDAR is sparsified), indicating that geometry dominates the learned cost and the model is highly robust. We attribute the small performance differences to the IMU labeling primarily reflecting terrain geometry rather than semantics and to limited data diversity. Unlike prior work claiming large gains, we emphasize our contributions: (1) a high-fidelity, reproducible simulation environment; (2) a self-supervised IMU-based labeling pipeline; and (3) a strong multi-modal BEV costmap prediction model. We discuss limitations and future work such as domain generalization and dataset expansion.</li>
</ul>

<h3>Title: End-to-End Visual Autonomous Parking via Control-Aided Attention</h3>
<ul>
<li><strong>Authors: </strong>Chao Chen, Shunyu Yao, Yuanwu He, Tao Feng, Ruojing Song, Yuliang Guo, Xinyu Huang, Chenxu Wu, Ren Liu, Chen Feng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11090">https://arxiv.org/abs/2509.11090</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11090">https://arxiv.org/pdf/2509.11090</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11090]] End-to-End Visual Autonomous Parking via Control-Aided Attention(https://arxiv.org/abs/2509.11090)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Precise parking requires an end-to-end system where perception adaptively provides policy-relevant details-especially in critical areas where fine control decisions are essential. End-to-end learning offers a unified framework by directly mapping sensor inputs to control actions, but existing approaches lack effective synergy between perception and control. We find that transformer-based self-attention, when used alone, tends to produce unstable and temporally inconsistent spatial attention, which undermines the reliability of downstream policy decisions over time. Instead, we propose CAA-Policy, an end-to-end imitation learning system that allows control signal to guide the learning of visual attention via a novel Control-Aided Attention (CAA) mechanism. For the first time, we train such an attention module in a self-supervised manner, using backpropagated gradients from the control outputs instead of from the training loss. This strategy encourages the attention to focus on visual features that induce high variance in action outputs, rather than merely minimizing the training loss-a shift we demonstrate leads to a more robust and generalizable policy. To further enhance stability, CAA-Policy integrates short-horizon waypoint prediction as an auxiliary task, and introduces a separately trained motion prediction module to robustly track the target spot over time. Extensive experiments in the CARLA simulator show that \titlevariable~consistently surpasses both the end-to-end learning baseline and the modular BEV segmentation + hybrid A* pipeline, achieving superior accuracy, robustness, and interpretability. Code is released at this https URL.</li>
</ul>

<h3>Title: PanoLora: Bridging Perspective and Panoramic Video Generation with LoRA Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Zeyu Dong, Yuyang Yin, Yuqi Li, Eric Li, Hao-Xiang Guo, Yikai Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11092">https://arxiv.org/abs/2509.11092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11092">https://arxiv.org/pdf/2509.11092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11092]] PanoLora: Bridging Perspective and Panoramic Video Generation with LoRA Adaptation(https://arxiv.org/abs/2509.11092)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Generating high-quality 360° panoramic videos remains a significant challenge due to the fundamental differences between panoramic and traditional perspective-view projections. While perspective videos rely on a single viewpoint with a limited field of view, panoramic content requires rendering the full surrounding environment, making it difficult for standard video generation models to adapt. Existing solutions often introduce complex architectures or large-scale training, leading to inefficiency and suboptimal results. Motivated by the success of Low-Rank Adaptation (LoRA) in style transfer tasks, we propose treating panoramic video generation as an adaptation problem from perspective views. Through theoretical analysis, we demonstrate that LoRA can effectively model the transformation between these projections when its rank exceeds the degrees of freedom in the task. Our approach efficiently fine-tunes a pretrained video diffusion model using only approximately 1,000 videos while achieving high-quality panoramic generation. Experimental results demonstrate that our method maintains proper projection geometry and surpasses previous state-of-the-art approaches in visual quality, left-right consistency, and motion diversity.</li>
</ul>

<h3>Title: GCN-TULHOR: Trajectory-User Linking Leveraging GCNs and Higher-Order Spatial Representations</h3>
<ul>
<li><strong>Authors: </strong>Khoa Tran, Pranav Gupta, Manos Papagelis</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11095">https://arxiv.org/abs/2509.11095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11095">https://arxiv.org/pdf/2509.11095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11095]] GCN-TULHOR: Trajectory-User Linking Leveraging GCNs and Higher-Order Spatial Representations(https://arxiv.org/abs/2509.11095)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, robust, transformer</a></li>
<li><strong>Abstract: </strong>Trajectory-user linking (TUL) aims to associate anonymized trajectories with the users who generated them, which is crucial for personalized recommendations, privacy-preserving analytics, and secure location-based services. Existing methods struggle with sparse data, incomplete routes, and limited modeling of complex spatial dependencies, often relying on low-level check-in data or ignoring spatial patterns. In this paper, we introduced GCN-TULHOR, a method that transforms raw location data into higher-order mobility flow representations using hexagonal tessellation, reducing data sparsity and capturing richer spatial semantics, and integrating Graph Convolutional Networks (GCNs). Our approach converts both sparse check-in and continuous GPS trajectory data into unified higher-order flow representations, mitigating sparsity while capturing deeper semantic information. The GCN layer explicitly models complex spatial relationships and non-local dependencies without requiring side information such as timestamps or points of interest. Experiments on six real-world datasets show consistent improvements over classical baselines, RNN- and Transformer-based models, and the TULHOR method in accuracy, precision, recall, and F1-score. GCN-TULHOR achieves 1-8% relative gains in accuracy and F1. Sensitivity analysis identifies an optimal setup with a single GCN layer and 512-dimensional embeddings. The integration of GCNs enhances spatial learning and improves generalizability across mobility data. This work highlights the value of combining graph-based spatial learning with sequential modeling, offering a robust and scalable solution for TUL with applications in recommendations, urban planning, and security.</li>
</ul>

<h3>Title: 3DAeroRelief: The first 3D Benchmark UAV Dataset for Post-Disaster Assessment</h3>
<ul>
<li><strong>Authors: </strong>Nhut Le, Ehsan Karimi, Maryam Rahnemoonfar</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11097">https://arxiv.org/abs/2509.11097</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11097">https://arxiv.org/pdf/2509.11097</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11097]] 3DAeroRelief: The first 3D Benchmark UAV Dataset for Post-Disaster Assessment(https://arxiv.org/abs/2509.11097)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Timely assessment of structural damage is critical for disaster response and recovery. However, most prior work in natural disaster analysis relies on 2D imagery, which lacks depth, suffers from occlusions, and provides limited spatial context. 3D semantic segmentation offers a richer alternative, but existing 3D benchmarks focus mainly on urban or indoor scenes, with little attention to disaster-affected areas. To address this gap, we present 3DAeroRelief--the first 3D benchmark dataset specifically designed for post-disaster assessment. Collected using low-cost unmanned aerial vehicles (UAVs) over hurricane-damaged regions, the dataset features dense 3D point clouds reconstructed via Structure-from-Motion and Multi-View Stereo techniques. Semantic annotations were produced through manual 2D labeling and projected into 3D space. Unlike existing datasets, 3DAeroRelief captures 3D large-scale outdoor environments with fine-grained structural damage in real-world disaster contexts. UAVs enable affordable, flexible, and safe data collection in hazardous areas, making them particularly well-suited for emergency scenarios. To demonstrate the utility of 3DAeroRelief, we evaluate several state-of-the-art 3D segmentation models on the dataset to highlight both the challenges and opportunities of 3D scene understanding in disaster response. Our dataset serves as a valuable resource for advancing robust 3D vision systems in real-world applications for post-disaster scenarios.</li>
</ul>

<h3>Title: EmoBench-Reddit: A Hierarchical Benchmark for Evaluating the Emotional Intelligence of Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haokun Li, Yazhou Zhang, Jizhi Ding, Qiuchi Li, Peng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11101">https://arxiv.org/abs/2509.11101</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11101">https://arxiv.org/pdf/2509.11101</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11101]] EmoBench-Reddit: A Hierarchical Benchmark for Evaluating the Emotional Intelligence of Multimodal Large Language Models(https://arxiv.org/abs/2509.11101)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of Multimodal Large Language Models (MLLMs), they have demonstrated exceptional capabilities across a variety of vision-language tasks. However, current evaluation benchmarks predominantly focus on objective visual question answering or captioning, inadequately assessing the models' ability to understand complex and subjective human emotions. To bridge this gap, we introduce EmoBench-Reddit, a novel, hierarchical benchmark for multimodal emotion understanding. The dataset comprises 350 meticulously curated samples from the social media platform Reddit, each containing an image, associated user-provided text, and an emotion category (sad, humor, sarcasm, happy) confirmed by user flairs. We designed a hierarchical task framework that progresses from basic perception to advanced cognition, with each data point featuring six multiple-choice questions and one open-ended question of increasing difficulty. Perception tasks evaluate the model's ability to identify basic visual elements (e.g., colors, objects), while cognition tasks require scene reasoning, intent understanding, and deep empathy integrating textual context. We ensured annotation quality through a combination of AI assistance (Claude 4) and manual verification.</li>
</ul>

<h3>Title: Filling the Gaps: A Multitask Hybrid Multiscale Generative Framework for Missing Modality in Remote Sensing Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Nhi Kieu, Kien Nguyen, Arnold Wiliem, Clinton Fookes, Sridha Sridharan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11102">https://arxiv.org/abs/2509.11102</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11102">https://arxiv.org/pdf/2509.11102</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11102]] Filling the Gaps: A Multitask Hybrid Multiscale Generative Framework for Missing Modality in Remote Sensing Semantic Segmentation(https://arxiv.org/abs/2509.11102)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, segmentation</a></li>
<li><strong>Abstract: </strong>Multimodal learning has shown significant performance boost compared to ordinary unimodal models across various domains. However, in real-world scenarios, multimodal signals are susceptible to missing because of sensor failures and adverse weather conditions, which drastically deteriorates models' operation and performance. Generative models such as AutoEncoder (AE) and Generative Adversarial Network (GAN) are intuitive solutions aiming to reconstruct missing modality from available ones. Yet, their efficacy in remote sensing semantic segmentation remains underexplored. In this paper, we first examine the limitations of existing generative approaches in handling the heterogeneity of multimodal remote sensing data. They inadequately capture semantic context in complex scenes with large intra-class and small inter-class variation. In addition, traditional generative models are susceptible to heavy dependence on the dominant modality, introducing bias that affects model robustness under missing modality conditions. To tackle these limitations, we propose a novel Generative-Enhanced MultiModal learning Network (GEMMNet) with three key components: (1) Hybrid Feature Extractor (HyFEx) to effectively learn modality-specific representations, (2) Hybrid Fusion with Multiscale Awareness (HyFMA) to capture modality-synergistic semantic context across scales and (3) Complementary Loss (CoLoss) scheme to alleviate the inherent bias by encouraging consistency across modalities and tasks. Our method, GEMMNet, outperforms both generative baselines AE, cGAN (conditional GAN), and state-of-the-art non-generative approaches - mmformer and shaspec - on two challenging semantic segmentation remote sensing datasets (Vaihingen and Potsdam). Source code is made available.</li>
</ul>

<h3>Title: BIGNet: Pretrained Graph Neural Network for Embedding Semantic, Spatial, and Topological Data in BIM Models</h3>
<ul>
<li><strong>Authors: </strong>Jin Han, Xin-Zheng Lu, Jia-Rui Lin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11104">https://arxiv.org/abs/2509.11104</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11104">https://arxiv.org/pdf/2509.11104</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11104]] BIGNet: Pretrained Graph Neural Network for Embedding Semantic, Spatial, and Topological Data in BIM Models(https://arxiv.org/abs/2509.11104)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Large Foundation Models (LFMs) have demonstrated significant advantages in civil engineering, but they primarily focus on textual and visual data, overlooking the rich semantic, spatial, and topological features in BIM (Building Information Modelling) models. Therefore, this study develops the first large-scale graph neural network (GNN), BIGNet, to learn, and reuse multidimensional design features embedded in BIM models. Firstly, a scalable graph representation is introduced to encode the "semantic-spatial-topological" features of BIM components, and a dataset with nearly 1 million nodes and 3.5 million edges is created. Subsequently, BIGNet is proposed by introducing a new message-passing mechanism to GraphMAE2 and further pretrained with a node masking strategy. Finally, BIGNet is evaluated in various transfer learning tasks for BIM-based design checking. Results show that: 1) homogeneous graph representation outperforms heterogeneous graph in learning design features, 2) considering local spatial relationships in a 30 cm radius enhances performance, and 3) BIGNet with GAT (Graph Attention Network)-based feature extraction achieves the best transfer learning results. This innovation leads to a 72.7% improvement in Average F1-score over non-pretrained models, demonstrating its effectiveness in learning and transferring BIM design features and facilitating their automated application in future design and lifecycle management.</li>
</ul>

<h3>Title: WildSmoke: Ready-to-Use Dynamic 3D Smoke Assets from a Single Video in the Wild</h3>
<ul>
<li><strong>Authors: </strong>Yuqiu Liu, Jialin Song, Manolis Savva, Wuyang Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11114">https://arxiv.org/abs/2509.11114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11114">https://arxiv.org/pdf/2509.11114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11114]] WildSmoke: Ready-to-Use Dynamic 3D Smoke Assets from a Single Video in the Wild(https://arxiv.org/abs/2509.11114)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>We propose a pipeline to extract and reconstruct dynamic 3D smoke assets from a single in-the-wild video, and further integrate interactive simulation for smoke design and editing. Recent developments in 3D vision have significantly improved reconstructing and rendering fluid dynamics, supporting realistic and temporally consistent view synthesis. However, current fluid reconstructions rely heavily on carefully controlled clean lab environments, whereas real-world videos captured in the wild are largely underexplored. We pinpoint three key challenges of reconstructing smoke in real-world videos and design targeted techniques, including smoke extraction with background removal, initialization of smoke particles and camera poses, and inferring multi-view videos. Our method not only outperforms previous reconstruction and generation methods with high-quality smoke reconstructions (+2.22 average PSNR on wild videos), but also enables diverse and realistic editing of fluid dynamics by simulating our smoke assets. We provide our models, data, and 4D smoke assets at [this https URL](this https URL).</li>
</ul>

<h3>Title: We Argue to Agree: Towards Personality-Driven Argumentation-Based Negotiation Dialogue Systems for Tourism</h3>
<ul>
<li><strong>Authors: </strong>Priyanshu Priya, Saurav Dudhate, Desai Vishesh Yasheshbhai, Asif Ekbal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11118">https://arxiv.org/abs/2509.11118</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11118">https://arxiv.org/pdf/2509.11118</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11118]] We Argue to Agree: Towards Personality-Driven Argumentation-Based Negotiation Dialogue Systems for Tourism(https://arxiv.org/abs/2509.11118)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Integrating argumentation mechanisms into negotiation dialogue systems improves conflict resolution through exchanges of arguments and critiques. Moreover, incorporating personality attributes enhances adaptability by aligning interactions with individuals' preferences and styles. To advance these capabilities in negotiation dialogue systems, we propose a novel Personality-driven Argumentation-based Negotiation Dialogue Generation (PAN-DG) task. To support this task, we introduce PACT, a dataset of Personality-driven Argumentation-based negotiation Conversations for Tourism sector. This dataset, generated using Large Language Models (LLMs), features three distinct personality profiles, viz. Argumentation Profile, Preference Profile, and Buying Style Profile to simulate a variety of negotiation scenarios involving diverse personalities. Thorough automatic and manual evaluations indicate that the dataset comprises high-quality dialogues. Further, we conduct comparative experiments between pre-trained and fine-tuned LLMs for the PAN-DG task. Multi-dimensional evaluation demonstrates that the fine-tuned LLMs effectively generate personality-driven rational responses during negotiations. This underscores the effectiveness of PACT in enhancing personalization and reasoning capabilities in negotiation dialogue systems, thereby establishing a foundation for future research in this domain.</li>
</ul>

<h3>Title: SoK: How Sensor Attacks Disrupt Autonomous Vehicles: An End-to-end Analysis, Challenges, and Missed Threats</h3>
<ul>
<li><strong>Authors: </strong>Qingzhao Zhang, Shaocheng Luo, Z. Morley Mao, Miroslav Pajic, Michael K. Reiter</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11120">https://arxiv.org/abs/2509.11120</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11120">https://arxiv.org/pdf/2509.11120</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11120]] SoK: How Sensor Attacks Disrupt Autonomous Vehicles: An End-to-end Analysis, Challenges, and Missed Threats(https://arxiv.org/abs/2509.11120)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>Autonomous vehicles, including self-driving cars, robotic ground vehicles, and drones, rely on complex sensor pipelines to ensure safe and reliable operation. However, these safety-critical systems remain vulnerable to adversarial sensor attacks that can compromise their performance and mission success. While extensive research has demonstrated various sensor attack techniques, critical gaps remain in understanding their feasibility in real-world, end-to-end systems. This gap largely stems from the lack of a systematic perspective on how sensor errors propagate through interconnected modules in autonomous systems when autonomous vehicles interact with the physical world. To bridge this gap, we present a comprehensive survey of autonomous vehicle sensor attacks across platforms, sensor modalities, and attack methods. Central to our analysis is the System Error Propagation Graph (SEPG), a structured demonstration tool that illustrates how sensor attacks propagate through system pipelines, exposing the conditions and dependencies that determine attack feasibility. With the aid of SEPG, our study distills seven key findings that highlight the feasibility challenges of sensor attacks and uncovers eleven previously overlooked attack vectors exploiting inter-module interactions, several of which we validate through proof-of-concept experiments. Additionally, we demonstrate how large language models (LLMs) can automate aspects of SEPG construction and cross-validate expert analysis, showcasing the promise of AI-assisted security evaluation.</li>
</ul>

<h3>Title: ODoQ: Oblivious DNS-over-QUIC</h3>
<ul>
<li><strong>Authors: </strong>Aditya Kulkarni, Tamal Das, Vivek Balachandran</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11123">https://arxiv.org/abs/2509.11123</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11123">https://arxiv.org/pdf/2509.11123</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11123]] ODoQ: Oblivious DNS-over-QUIC(https://arxiv.org/abs/2509.11123)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, attack</a></li>
<li><strong>Abstract: </strong>The Domain Name System (DNS), which converts domain names to their respective IP addresses, has advanced enhancements aimed at safeguarding DNS data and users' identity from attackers. The recent privacy-focused advancements have enabled the IETF to standardize several protocols. Nevertheless, these protocols tend to focus on either strengthening user privacy (like Oblivious DNS and Oblivious DNS-over-HTTPS) or reducing resolution latency (as demonstrated by DNS-over-QUIC). Achieving both within a single protocol remains a key challenge, which we address in this paper. Our proposed protocol -- 'Oblivious DNS-over-QUIC' (ODoQ) -- leverages the benefits of the QUIC protocol and incorporates an intermediary proxy server to protect the client's identity from exposure to the recursive resolver.</li>
</ul>

<h3>Title: Joint Effects of Argumentation Theory, Audio Modality and Data Enrichment on LLM-Based Fallacy Classification</h3>
<ul>
<li><strong>Authors: </strong>Hongxu Zhou, Hylke Westerdijk, Khondoker Ittehadul Islam</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11127">https://arxiv.org/abs/2509.11127</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11127">https://arxiv.org/pdf/2509.11127</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11127]] Joint Effects of Argumentation Theory, Audio Modality and Data Enrichment on LLM-Based Fallacy Classification(https://arxiv.org/abs/2509.11127)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>This study investigates how context and emotional tone metadata influence large language model (LLM) reasoning and performance in fallacy classification tasks, particularly within political debate settings. Using data from U.S. presidential debates, we classify six fallacy types through various prompting strategies applied to the Qwen-3 (8B) model. We introduce two theoretically grounded Chain-of-Thought frameworks: Pragma-Dialectics and the Periodic Table of Arguments, and evaluate their effectiveness against a baseline prompt under three input settings: text-only, text with context, and text with both context and audio-based emotional tone metadata. Results suggest that while theoretical prompting can improve interpretability and, in some cases, accuracy, the addition of context and especially emotional tone metadata often leads to lowered performance. Emotional tone metadata biases the model toward labeling statements as \textit{Appeal to Emotion}, worsening logical reasoning. Overall, basic prompts often outperformed enhanced ones, suggesting that attention dilution from added inputs may worsen rather than improve fallacy classification in LLMs.</li>
</ul>

<h3>Title: When Smiley Turns Hostile: Interpreting How Emojis Trigger LLMs' Toxicity</h3>
<ul>
<li><strong>Authors: </strong>Shiyao Cui, Xijia Feng, Yingkang Wang, Junxiao Yang, Zhexin Zhang, Biplab Sikdar, Hongning Wang, Han Qiu, Minlie Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11141">https://arxiv.org/abs/2509.11141</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11141">https://arxiv.org/pdf/2509.11141</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11141]] When Smiley Turns Hostile: Interpreting How Emojis Trigger LLMs' Toxicity(https://arxiv.org/abs/2509.11141)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Emojis are globally used non-verbal cues in digital communication, and extensive research has examined how large language models (LLMs) understand and utilize emojis across contexts. While usually associated with friendliness or playfulness, it is observed that emojis may trigger toxic content generation in LLMs. Motivated by such a observation, we aim to investigate: (1) whether emojis can clearly enhance the toxicity generation in LLMs and (2) how to interpret this phenomenon. We begin with a comprehensive exploration of emoji-triggered LLM toxicity generation by automating the construction of prompts with emojis to subtly express toxic intent. Experiments across 5 mainstream languages on 7 famous LLMs along with jailbreak tasks demonstrate that prompts with emojis could easily induce toxicity generation. To understand this phenomenon, we conduct model-level interpretations spanning semantic cognition, sequence generation and tokenization, suggesting that emojis can act as a heterogeneous semantic channel to bypass the safety mechanisms. To pursue deeper insights, we further probe the pre-training corpus and uncover potential correlation between the emoji-related data polution with the toxicity generation behaviors. Supplementary materials provide our implementation code and data. (Warning: This paper contains potentially sensitive contents)</li>
</ul>

<h3>Title: Text2Mem: A Unified Memory Operation Language for Memory Operating System</h3>
<ul>
<li><strong>Authors: </strong>Felix Wang, Boyu Chen, Kerun Xu, Bo Tang, Feiyu Xiong, Zhiyu Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11145">https://arxiv.org/abs/2509.11145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11145">https://arxiv.org/pdf/2509.11145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11145]] Text2Mem: A Unified Memory Operation Language for Memory Operating System(https://arxiv.org/abs/2509.11145)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language model agents increasingly depend on memory to sustain long horizon interaction, but existing frameworks remain limited. Most expose only a few basic primitives such as encode, retrieve, and delete, while higher order operations like merge, promote, demote, split, lock, and expire are missing or inconsistently supported. Moreover, there is no formal and executable specification for memory commands, leaving scope and lifecycle rules implicit and causing unpredictable behavior across systems. We introduce Text2Mem, a unified memory operation language that provides a standardized pathway from natural language to reliable execution. Text2Mem defines a compact yet expressive operation set aligned with encoding, storage, and retrieval. Each instruction is represented as a JSON based schema instance with required fields and semantic invariants, which a parser transforms into typed operation objects with normalized parameters. A validator ensures correctness before execution, while adapters map typed objects either to a SQL prototype backend or to real memory frameworks. Model based services such as embeddings or summarization are integrated when required. All results are returned through a unified execution contract. This design ensures safety, determinism, and portability across heterogeneous backends. We also outline Text2Mem Bench, a planned benchmark that separates schema generation from backend execution to enable systematic evaluation. Together, these components establish the first standardized foundation for memory control in agents.</li>
</ul>

<h3>Title: Feature Space Topology Control via Hopkins Loss</h3>
<ul>
<li><strong>Authors: </strong>Einari Vaaras, Manu Airaksinen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11154">https://arxiv.org/abs/2509.11154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11154">https://arxiv.org/pdf/2509.11154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11154]] Feature Space Topology Control via Hopkins Loss(https://arxiv.org/abs/2509.11154)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, generative</a></li>
<li><strong>Abstract: </strong>Feature space topology refers to the organization of samples within the feature space. Modifying this topology can be beneficial in machine learning applications, including dimensionality reduction, generative modeling, transfer learning, and robustness to adversarial attacks. This paper introduces a novel loss function, Hopkins loss, which leverages the Hopkins statistic to enforce a desired feature space topology, which is in contrast to existing topology-related methods that aim to preserve input feature topology. We evaluate the effectiveness of Hopkins loss on speech, text, and image data in two scenarios: classification and dimensionality reduction using nonlinear bottleneck autoencoders. Our experiments show that integrating Hopkins loss into classification or dimensionality reduction has only a small impact on classification performance while providing the benefit of modifying feature topology.</li>
</ul>

<h3>Title: AQUA: Attention via QUery mAgnitudes for Memory and Compute Efficient Inference in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Santhosh G S, Saurav Prakash, Balaraman Ravindran</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11155">https://arxiv.org/abs/2509.11155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11155">https://arxiv.org/pdf/2509.11155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11155]] AQUA: Attention via QUery mAgnitudes for Memory and Compute Efficient Inference in LLMs(https://arxiv.org/abs/2509.11155)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The quadratic complexity of the attention mechanism remains a fundamental barrier to scaling Large Language Models (LLMs) to longer contexts, creating a critical bottleneck in both computation and memory. To address this, we introduce AQUA (Attention via QUery mAgnitudes) a novel and versatile approximation strategy that significantly reduces the cost of attention with a graceful performance trade-off. Our method operates in two phases: an efficient offline step where we compute a universal, language agnostic projection matrix via SVD on a calibration dataset, and an online inference step where we project query and key vectors and dynamically select a sparse subset of dimensions based on the query's magnitude. We provide a formal theoretical analysis of AQUA, establishing the break-even point at which it becomes more computationally efficient than standard attention. Our empirical evaluations on state-of-the-art models like Llama-3.1-8B demonstrate that a 25% reduction in the attention dot-product computation can be achieved with a statistically insignificant impact on performance across a wide range of benchmarks. We further showcase the versatility of AQUA by demonstrating its ability to synergistically accelerate existing token eviction methods like H2O and to directly reduce KV-cache memory size. By offering a controllable knob to balance efficiency and accuracy, AQUA provides a practical and powerful tool for making large-scale LLM inference more accessible and sustainable.</li>
</ul>

<h3>Title: Cryptanalysis and design for a family of plaintext non-delayed chaotic ciphers</h3>
<ul>
<li><strong>Authors: </strong>Qianxue Wang, Simin Yu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11158">https://arxiv.org/abs/2509.11158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11158">https://arxiv.org/pdf/2509.11158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11158]] Cryptanalysis and design for a family of plaintext non-delayed chaotic ciphers(https://arxiv.org/abs/2509.11158)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, diffusion</a></li>
<li><strong>Abstract: </strong>Plaintext non-delayed chaotic cipher (PNDCC) means that in the diffusion equation, plaintext has no delay terms while ciphertext has a feedback term. In existing literature, chaotic cipher diffusions invariably take this form. Since its introduction, PNDCC has attracted attention but also doubts. Designers of chaotic ciphers usually claim PNDCC security by statistical tests, while rigorous cryptographic proofs are absent. Thus, it is necessary to re-examine its design rationale and empirical security. To address this issue, we present a typical example of a three-stage permutation-diffusion-permutation PNDCC, which contains multiple security vulnerabilities. Although all of its statistical indicators show good performance, we are able to break it using four different attacks. The first is a differential attack based on homogeneous operations; the second is an S-PTC attack; the third is a novel impulse-step-based differential attack (ISBDA), proposed in this paper, and the fourth is a novel chain attack, also introduced here. These results demonstrate that the fulfilment of statistical criteria is not a sufficient condition for the security of PNDCC. Then, based on a mathematical model of multi-stage PNDCC, we show that the proposed chain attack can successfully break a class of multi-stage PNDCCs. The key technique of the chain attack depends on how to reveal all permutations. To address this key problem, we summarize the chaining rules and show that, from the attacker's perspective, if the same decryption chain can be reconstructed then all permutations can be deciphered. To that end, the entire diffusion process can be broken by solving a system of simultaneous equations. Finally, as a secure improvement, we propose a new scheme termed plaintext-delayed chaotic cipher (PDCC) that can resist various cryptanalytic attacks.</li>
</ul>

<h3>Title: Stabilizing Data-Free Model Extraction</h3>
<ul>
<li><strong>Authors: </strong>Dat-Thinh Nguyen, Kim-Hung Le, Nhien-An Le-Khac</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11159">https://arxiv.org/abs/2509.11159</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11159">https://arxiv.org/pdf/2509.11159</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11159]] Stabilizing Data-Free Model Extraction(https://arxiv.org/abs/2509.11159)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, extraction, data-free</a></li>
<li><strong>Abstract: </strong>Model extraction is a severe threat to Machine Learning-as-a-Service systems, especially through data-free approaches, where dishonest users can replicate the functionality of a black-box target model without access to realistic data. Despite recent advancements, existing data-free model extraction methods suffer from the oscillating accuracy of the substitute model. This oscillation, which could be attributed to the constant shift in the generated data distribution during the attack, makes the attack impractical since the optimal substitute model cannot be determined without access to the target model's in-distribution data. Hence, we propose MetaDFME, a novel data-free model extraction method that employs meta-learning in the generator training to reduce the distribution shift, aiming to mitigate the substitute model's accuracy oscillation. In detail, we train our generator to iteratively capture the meta-representations of the synthetic data during the attack. These meta-representations can be adapted with a few steps to produce data that facilitates the substitute model to learn from the target model while reducing the effect of distribution shifts. Our experiments on popular baseline image datasets, MNIST, SVHN, CIFAR-10, and CIFAR-100, demonstrate that MetaDFME outperforms the current state-of-the-art data-free model extraction method while exhibiting a more stable substitute model's accuracy during the attack.</li>
</ul>

<h3>Title: GK-SMOTE: A Hyperparameter-free Noise-Resilient Gaussian KDE-Based Oversampling Approach</h3>
<ul>
<li><strong>Authors: </strong>Mahabubur Rahman Miraj, Hongyu Huang, Ting Yang, Jinxue Zhao, Nankun Mu, Xinyu Lei</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11163">https://arxiv.org/abs/2509.11163</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11163">https://arxiv.org/pdf/2509.11163</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11163]] GK-SMOTE: A Hyperparameter-free Noise-Resilient Gaussian KDE-Based Oversampling Approach(https://arxiv.org/abs/2509.11163)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust</a></li>
<li><strong>Abstract: </strong>Imbalanced classification is a significant challenge in machine learning, especially in critical applications like medical diagnosis, fraud detection, and cybersecurity. Traditional oversampling techniques, such as SMOTE, often fail to handle label noise and complex data distributions, leading to reduced classification accuracy. In this paper, we propose GK-SMOTE, a hyperparameter-free, noise-resilient extension of SMOTE, built on Gaussian Kernel Density Estimation (KDE). GK-SMOTE enhances class separability by generating synthetic samples in high-density minority regions, while effectively avoiding noisy or ambiguous areas. This self-adaptive approach uses Gaussian KDE to differentiate between safe and noisy regions, ensuring more accurate sample generation without requiring extensive parameter tuning. Our extensive experiments on diverse binary classification datasets demonstrate that GK-SMOTE outperforms existing state-of-the-art oversampling techniques across key evaluation metrics, including MCC, Balanced Accuracy, and AUPRC. The proposed method offers a robust, efficient solution for imbalanced classification tasks, especially in noisy data environments, making it an attractive choice for real-world applications.</li>
</ul>

<h3>Title: Traffic-MLLM: A Spatio-Temporal MLLM with Retrieval-Augmented Generation for Causal Inference in Traffic</h3>
<ul>
<li><strong>Authors: </strong>Waikit Xiu, Qiang Lu, Xiying Li, Chen Hu, Shengbo Sun</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11165">https://arxiv.org/abs/2509.11165</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11165">https://arxiv.org/pdf/2509.11165</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11165]] Traffic-MLLM: A Spatio-Temporal MLLM with Retrieval-Augmented Generation for Causal Inference in Traffic(https://arxiv.org/abs/2509.11165)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As intelligent transportation systems advance, traffic video understanding plays an increasingly pivotal role in comprehensive scene perception and causal analysis. Yet, existing approaches face notable challenges in accurately modeling spatiotemporal causality and integrating domain-specific knowledge, limiting their effectiveness in complex scenarios. To address these limitations, we propose Traffic-MLLM, a multimodal large language model tailored for fine-grained traffic analysis. Built on the Qwen2.5-VL backbone, our model leverages high-quality traffic-specific multimodal datasets and uses Low-Rank Adaptation (LoRA) for lightweight fine-tuning, significantly enhancing its capacity to model continuous spatiotemporal features in video sequences. Furthermore, we introduce an innovative knowledge prompting module fusing Chain-of-Thought (CoT) reasoning with Retrieval-Augmented Generation (RAG), enabling precise injection of detailed traffic regulations and domain knowledge into the inference process. This design markedly boosts the model's logical reasoning and knowledge adaptation capabilities. Experimental results on TrafficQA and DriveQA benchmarks show Traffic-MLLM achieves state-of-the-art performance, validating its superior ability to process multimodal traffic data. It also exhibits remarkable zero-shot reasoning and cross-scenario generalization capabilities.</li>
</ul>

<h3>Title: Harnessing Optimization Dynamics for Curvature-Informed Model Merging</h3>
<ul>
<li><strong>Authors: </strong>Pouria Mahdavinia, Hamed Mahdavi, Niloofar Mireshghallah, Mehrdad Mahdavi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11167">https://arxiv.org/abs/2509.11167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11167">https://arxiv.org/pdf/2509.11167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11167]] Harnessing Optimization Dynamics for Curvature-Informed Model Merging(https://arxiv.org/abs/2509.11167)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Model merging is an effective post-training strategy for composing capabilities in large language models without joint retraining. We study this in the supervised fine-tuning (SFT) stage, where multiple capability-based SFT checkpoints -- spanning math, code, precise instruction following, general instruction following, and knowledge recall -- must be consolidated into a single model. We introduce Optimization Trajectory Aware (OTA) Merging, a curvature-aware aggregation that leverages optimizer second-moment statistics as a diagonal curvature proxy to reweight parameter edits and mitigate interference. Complementing OTA, we propose Fast Fisher Grafting (FFG), a curvature-driven task-localization step that sparsifies conflicting or low-importance edits. FFG induces extremely low-rank masks concentrated in early attention query/key projections and token embeddings, exploiting shared curvature across capabilities. We further develop a memory-light compression of the second moments that preserves OTA's effect. Across diverse capability-based SFT checkpoints, OTA+FFG improves merged-model quality over strong weight-space baselines, reduces negative transfer, and remains robust across sparsity levels. Analyses reveal substantial curvature overlap between checkpoints, offering a novel lens on why simple linear merging can be effective in practice. Ablations confirm that FFG is critical for reducing task interference and that the compressed second moments retain the gains of the full formulation. To facilitate reproducibility, we open-source all code, training and evaluation scripts, visualization artifacts, and capability-specific SFT checkpoints at this https URL.</li>
</ul>

<h3>Title: Your Compiler is Backdooring Your Model: Understanding and Exploiting Compilation Inconsistency Vulnerabilities in Deep Learning Compilers</h3>
<ul>
<li><strong>Authors: </strong>Simin Chen, Jinjun Peng, Yixin He, Junfeng Yang, Baishakhi Ray</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11173">https://arxiv.org/abs/2509.11173</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11173">https://arxiv.org/pdf/2509.11173</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11173]] Your Compiler is Backdooring Your Model: Understanding and Exploiting Compilation Inconsistency Vulnerabilities in Deep Learning Compilers(https://arxiv.org/abs/2509.11173)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>Deep learning (DL) compilers are core infrastructure in modern DL systems, offering flexibility and scalability beyond vendor-specific libraries. This work uncovers a fundamental vulnerability in their design: can an official, unmodified compiler alter a model's semantics during compilation and introduce hidden backdoors? We study both adversarial and natural settings. In the adversarial case, we craft benign models where triggers have no effect pre-compilation but become effective backdoors after compilation. Tested on six models, three commercial compilers, and two hardware platforms, our attack yields 100% success on triggered inputs while preserving normal accuracy and remaining undetected by state-of-the-art detectors. The attack generalizes across compilers, hardware, and floating-point settings. In the natural setting, we analyze the top 100 HuggingFace models (including one with 220M+ downloads) and find natural triggers in 31 models. This shows that compilers can introduce risks even without adversarial manipulation. Our results reveal an overlooked threat: unmodified DL compilers can silently alter model semantics. To our knowledge, this is the first work to expose inherent security risks in DL compiler design, opening a new direction for secure and trustworthy ML.</li>
</ul>

<h3>Title: Differentially-private text generation degrades output language quality</h3>
<ul>
<li><strong>Authors: </strong>Erion Çano, Ivan Habernal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11176">https://arxiv.org/abs/2509.11176</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11176">https://arxiv.org/pdf/2509.11176</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11176]] Differentially-private text generation degrades output language quality(https://arxiv.org/abs/2509.11176)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Ensuring user privacy by synthesizing data from large language models (LLMs) tuned under differential privacy (DP) has become popular recently. However, the impact of DP fine-tuned LLMs on the quality of the language and the utility of the texts they produce has not been investigated. In this work, we tune five LLMs with three corpora under four levels of privacy and assess the length, the grammatical correctness, and the lexical diversity of the text outputs they produce. We also probe the utility of the synthetic outputs in downstream classification tasks such as book genre recognition based on book descriptions and cause of death recognition based on verbal autopsies. The results indicate that LLMs tuned under stronger privacy constrains produce texts that are shorter by at least 77 %, that are less grammatically correct by at least 9 %, and are less diverse by at least 10 % in bi-gram diversity. Furthermore, the accuracy they reach in downstream classification tasks decreases, which might be detrimental to the usefulness of the generated synthetic data.</li>
</ul>

<h3>Title: Optimal Brain Restoration for Joint Quantization and Sparsification of LLMs</h3>
<ul>
<li><strong>Authors: </strong>Hang Guo, Yawei Li, Luca Benini</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11177">https://arxiv.org/abs/2509.11177</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11177">https://arxiv.org/pdf/2509.11177</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11177]] Optimal Brain Restoration for Joint Quantization and Sparsification of LLMs(https://arxiv.org/abs/2509.11177)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in Large Language Model (LLM) compression, such as quantization and pruning, have achieved notable success. However, as these techniques gradually approach their respective limits, relying on a single method for further compression has become increasingly challenging. In this work, we explore an alternative solution by combining quantization and sparsity. This joint approach, though promising, introduces new difficulties due to the inherently conflicting requirements on weight distributions: quantization favors compact ranges, while pruning benefits from high variance. To attack this problem, we propose Optimal Brain Restoration (OBR), a general and training-free framework that aligns pruning and quantization by error compensation between both. OBR minimizes performance degradation on downstream tasks by building on a second-order Hessian objective, which is then reformulated into a tractable problem through surrogate approximation and ultimately reaches a closed-form solution via group error compensation. Experiments show that OBR enables aggressive W4A4KV4 quantization with 50% sparsity on existing LLMs, and delivers up to 4.72x speedup and 6.4x memory reduction compared to the FP16-dense baseline.</li>
</ul>

<h3>Title: StegOT: Trade-offs in Steganography via Optimal Transport</h3>
<ul>
<li><strong>Authors: </strong>Chengde Lin, Xuezhu Gong, Shuxue Ding, Mingzhe Yang, Xijun Lu, Chengjun Mo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11178">https://arxiv.org/abs/2509.11178</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11178">https://arxiv.org/pdf/2509.11178</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11178]] StegOT: Trade-offs in Steganography via Optimal Transport(https://arxiv.org/abs/2509.11178)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Image hiding is often referred to as steganography, which aims to hide a secret image in a cover image of the same resolution. Many steganography models are based on genera-tive adversarial networks (GANs) and variational autoencoders (VAEs). However, most existing models suffer from mode collapse. Mode collapse will lead to an information imbalance between the cover and secret images in the stego image and further affect the subsequent extraction. To address these challenges, this paper proposes StegOT, an autoencoder-based steganography model incorporating optimal transport theory. We designed the multiple channel optimal transport (MCOT) module to transform the feature distribution, which exhibits multiple peaks, into a single peak to achieve the trade-off of information. Experiments demonstrate that we not only achieve a trade-off between the cover and secret images but also enhance the quality of both the stego and recovery images. The source code will be released on this https URL.</li>
</ul>

<h3>Title: The Impact of Skin Tone Label Granularity on the Performance and Fairness of AI Based Dermatology Image Classification Models</h3>
<ul>
<li><strong>Authors: </strong>Partha Shah, Durva Sankhe, Maariyah Rashid, Zakaa Khaled, Esther Puyol-Antón, Tiarna Lee, Maram Alqarni, Sweta Rai, Andrew P. King</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11184">https://arxiv.org/abs/2509.11184</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11184">https://arxiv.org/pdf/2509.11184</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11184]] The Impact of Skin Tone Label Granularity on the Performance and Fairness of AI Based Dermatology Image Classification Models(https://arxiv.org/abs/2509.11184)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Artificial intelligence (AI) models to automatically classify skin lesions from dermatology images have shown promising performance but also susceptibility to bias by skin tone. The most common way of representing skin tone information is the Fitzpatrick Skin Tone (FST) scale. The FST scale has been criticised for having greater granularity in its skin tone categories for lighter-skinned subjects. This paper conducts an investigation of the impact (on performance and bias) on AI classification models of granularity in the FST scale. By training multiple AI models to classify benign vs. malignant lesions using FST-specific data of differing granularity, we show that: (i) when training models using FST-specific data based on three groups (FST 1/2, 3/4 and 5/6), performance is generally better for models trained on FST-specific data compared to a general model trained on FST-balanced data; (ii) reducing the granularity of FST scale information (from 1/2 and 3/4 to 1/2/3/4) can have a detrimental effect on performance. Our results highlight the importance of the granularity of FST groups when training lesion classification models. Given the question marks over possible human biases in the choice of categories in the FST scale, this paper provides evidence for a move away from the FST scale in fair AI research and a transition to an alternative scale that better represents the diversity of human skin tones.</li>
</ul>

<h3>Title: DMLDroid: Deep Multimodal Fusion Framework for Android Malware Detection with Resilience to Code Obfuscation and Adversarial Perturbations</h3>
<ul>
<li><strong>Authors: </strong>Doan Minh Trung, Tien Duc Anh Hao, Luong Hoang Minh, Nghi Hoang Khoa, Nguyen Tan Cam, Van-Hau Pham, Phan The Duy</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11187">https://arxiv.org/abs/2509.11187</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11187">https://arxiv.org/pdf/2509.11187</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11187]] DMLDroid: Deep Multimodal Fusion Framework for Android Malware Detection with Resilience to Code Obfuscation and Adversarial Perturbations(https://arxiv.org/abs/2509.11187)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>In recent years, learning-based Android malware detection has seen significant advancements, with detectors generally falling into three categories: string-based, image-based, and graph-based approaches. While these methods have shown strong detection performance, they often struggle to sustain robustness in real-world settings, particularly when facing code obfuscation and adversarial examples (AEs). Deep multimodal learning has emerged as a promising solution, leveraging the strengths of multiple feature types to enhance robustness and generalization. However, a systematic investigation of multimodal fusion for both accuracy and resilience remains underexplored. In this study, we propose DMLDroid, an Android malware detection based on multimodal fusion that leverages three different representations of malware features, including permissions & intents (tabular-based), DEX file representations (image-based), and API calls (graph-derived sequence-based). We conduct exhaustive experiments independently on each feature, as well as in combination, using different fusion strategies. Experimental results on the CICMalDroid 2020 dataset demonstrate that our multimodal approach with the dynamic weighted fusion mechanism achieves high performance, reaching 97.98% accuracy and 98.67% F1-score on original malware detection. Notably, the proposed method maintains strong robustness, sustaining over 98% accuracy and 98% F1-score under both obfuscation and adversarial attack scenarios. Our findings highlight the benefits of multimodal fusion in improving both detection accuracy and robustness against evolving Android malware threats.</li>
</ul>

<h3>Title: RanAT4BIE: Random Adversarial Training for Biomedical Information Extraction</h3>
<ul>
<li><strong>Authors: </strong>Jian Chen, Shengyi Lv, Leilei Su</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11191">https://arxiv.org/abs/2509.11191</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11191">https://arxiv.org/pdf/2509.11191</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11191]] RanAT4BIE: Random Adversarial Training for Biomedical Information Extraction(https://arxiv.org/abs/2509.11191)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>We introduce random adversarial training (RAT), a novel framework successfully applied to biomedical information extraction (BioIE) tasks. Building on PubMedBERT as the foundational architecture, our study first validates the effectiveness of conventional adversarial training in enhancing pre-trained language models' performance on BioIE tasks. While adversarial training yields significant improvements across various performance metrics, it also introduces considerable computational overhead. To address this limitation, we propose RAT as an efficiency solution for biomedical information extraction. This framework strategically integrates random sampling mechanisms with adversarial training principles, achieving dual objectives: enhanced model generalization and robustness while significantly reducing computational costs. Through comprehensive evaluations, RAT demonstrates superior performance compared to baseline models in BioIE tasks. The results highlight RAT's potential as a transformative framework for biomedical natural language processing, offering a balanced solution to the model performance and computational efficiency.</li>
</ul>

<h3>Title: Federated Recommender System with Data Valuation for E-commerce Platform</h3>
<ul>
<li><strong>Authors: </strong>Jongwon Park, Minku Kang, Wooseok Sim, Soyoung Lee, Hogun Park</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11196">https://arxiv.org/abs/2509.11196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11196">https://arxiv.org/pdf/2509.11196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11196]] Federated Recommender System with Data Valuation for E-commerce Platform(https://arxiv.org/abs/2509.11196)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) is gaining prominence in machine learning as privacy concerns grow. This paradigm allows each client (e.g., an individual online store) to train a recommendation model locally while sharing only model updates, without exposing the raw interaction logs to a central server, thereby preserving privacy in a decentralized environment. Nonetheless, most existing FL-based recommender systems still rely solely on each client's private data, despite the abundance of publicly available datasets that could be leveraged to enrich local training; this potential remains largely underexplored. To this end, we consider a realistic scenario wherein a large shopping platform collaborates with multiple small online stores to build a global recommender system. The platform possesses global data, such as shareable user and item lists, while each store holds a portion of interaction data privately (or locally). Although integrating global data can help mitigate the limitations of sparse and biased clients' local data, it also introduces additional challenges: simply combining all global interactions can amplify noise and irrelevant patterns, worsening personalization and increasing computational costs. To address these challenges, we propose FedGDVE, which selectively augments each client's local graph with semantically aligned samples from the global dataset. FedGDVE employs: (i) a pre-trained graph encoder to extract global structural features, (ii) a local valid predictor to assess client-specific relevance, (iii) a reinforcement-learning-based probability estimator to filter and sample only the most pertinent global interactions. FedGDVE improves performance by up to 34.86% on recognized benchmarks in FL environments.</li>
</ul>

<h3>Title: Scaling Up Forest Vision with Synthetic Data</h3>
<ul>
<li><strong>Authors: </strong>Yihang She, Andrew Blake, David Coomes, Srinivasan Keshav</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11201">https://arxiv.org/abs/2509.11201</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11201">https://arxiv.org/pdf/2509.11201</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11201]] Scaling Up Forest Vision with Synthetic Data(https://arxiv.org/abs/2509.11201)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Accurate tree segmentation is a key step in extracting individual tree metrics from forest laser scans, and is essential to understanding ecosystem functions in carbon cycling and beyond. Over the past decade, tree segmentation algorithms have advanced rapidly due to developments in AI. However existing, public, 3D forest datasets are not large enough to build robust tree segmentation systems. Motivated by the success of synthetic data in other domains such as self-driving, we investigate whether similar approaches can help with tree segmentation. In place of expensive field data collection and annotation, we use synthetic data during pretraining, and then require only minimal, real forest plot annotation for fine-tuning. We have developed a new synthetic data generation pipeline to do this for forest vision tasks, integrating advances in game-engines with physics-based LiDAR simulation. As a result, we have produced a comprehensive, diverse, annotated 3D forest dataset on an unprecedented scale. Extensive experiments with a state-of-the-art tree segmentation algorithm and a popular real dataset show that our synthetic data can substantially reduce the need for labelled real data. After fine-tuning on just a single, real, forest plot of less than 0.1 hectare, the pretrained model achieves segmentations that are competitive with a model trained on the full scale real data. We have also identified critical factors for successful use of synthetic data: physics, diversity, and scale, paving the way for more robust 3D forest vision systems in the future. Our data generation pipeline and the resulting dataset are available at this https URL.</li>
</ul>

<h3>Title: Beyond Sliders: Mastering the Art of Diffusion-based Image Manipulation</h3>
<ul>
<li><strong>Authors: </strong>Yufei Tang, Daiheng Gao, Pingyu Wu, Wenbo Zhou, Bang Zhang, Weiming Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11213">https://arxiv.org/abs/2509.11213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11213">https://arxiv.org/pdf/2509.11213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11213]] Beyond Sliders: Mastering the Art of Diffusion-based Image Manipulation(https://arxiv.org/abs/2509.11213)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>In the realm of image generation, the quest for realism and customization has never been more pressing. While existing methods like concept sliders have made strides, they often falter when it comes to no-AIGC images, particularly images captured in real world settings. To bridge this gap, we introduce Beyond Sliders, an innovative framework that integrates GANs and diffusion models to facilitate sophisticated image manipulation across diverse image categories. Improved upon concept sliders, our method refines the image through fine grained guidance both textual and visual in an adversarial manner, leading to a marked enhancement in image quality and realism. Extensive experimental validation confirms the robustness and versatility of Beyond Sliders across a spectrum of applications.</li>
</ul>

<h3>Title: Geometrically Constrained and Token-Based Probabilistic Spatial Transformers</h3>
<ul>
<li><strong>Authors: </strong>Johann Schmidt, Sebastian Stober</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11218">https://arxiv.org/abs/2509.11218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11218">https://arxiv.org/pdf/2509.11218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11218]] Geometrically Constrained and Token-Based Probabilistic Spatial Transformers(https://arxiv.org/abs/2509.11218)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Fine-grained visual classification (FGVC) remains highly sensitive to geometric variability, where objects appear under arbitrary orientations, scales, and perspective distortions. While equivariant architectures address this issue, they typically require substantial computational resources and restrict the hypothesis space. We revisit Spatial Transformer Networks (STNs) as a canonicalization tool for transformer-based vision pipelines, emphasizing their flexibility, backbone-agnostic nature, and lack of architectural constraints. We propose a probabilistic, component-wise extension that improves robustness. Specifically, we decompose affine transformations into rotation, scaling, and shearing, and regress each component under geometric constraints using a shared localization encoder. To capture uncertainty, we model each component with a Gaussian variational posterior and perform sampling-based canonicalization during inference.A novel component-wise alignment loss leverages augmentation parameters to guide spatial alignment. Experiments on challenging moth classification benchmarks demonstrate that our method consistently improves robustness compared to other STNs.</li>
</ul>

<h3>Title: CCoMAML: Efficient Cattle Identification Using Cooperative Model-Agnostic Meta-Learning</h3>
<ul>
<li><strong>Authors: </strong>Rabin Dulal, Lihong Zheng, Ashad Kabir</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11219">https://arxiv.org/abs/2509.11219</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11219">https://arxiv.org/pdf/2509.11219</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11219]] CCoMAML: Efficient Cattle Identification Using Cooperative Model-Agnostic Meta-Learning(https://arxiv.org/abs/2509.11219)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, biometric</a></li>
<li><strong>Abstract: </strong>Cattle identification is critical for efficient livestock farming management, currently reliant on radio-frequency identification (RFID) ear tags. However, RFID-based systems are prone to failure due to loss, damage, tampering, and vulnerability to external attacks. As a robust alternative, biometric identification using cattle muzzle patterns similar to human fingerprints has emerged as a promising solution. Deep learning techniques have demonstrated success in leveraging these unique patterns for accurate identification. But deep learning models face significant challenges, including limited data availability, disruptions during data collection, and dynamic herd compositions that require frequent model retraining. To address these limitations, this paper proposes a novel few-shot learning framework for real-time cattle identification using Cooperative Model-Agnostic Meta-Learning (CCoMAML) with Multi-Head Attention Feature Fusion (MHAFF) as a feature extractor model. This model offers great model adaptability to new data through efficient learning from few data samples without retraining. The proposed approach has been rigorously evaluated against current state-of-the-art few-shot learning techniques applied in cattle identification. Comprehensive experimental results demonstrate that our proposed CCoMAML with MHAFF has superior cattle identification performance with 98.46% and 97.91% F1 scores.</li>
</ul>

<h3>Title: ANROT-HELANet: Adverserially and Naturally Robust Attention-Based Aggregation Network via The Hellinger Distance for Few-Shot Classification</h3>
<ul>
<li><strong>Authors: </strong>Gao Yu Lee, Tanmoy Dam, Md Meftahul Ferdaus, Daniel Puiu Poenar, Vu N.Duong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11220">https://arxiv.org/abs/2509.11220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11220">https://arxiv.org/pdf/2509.11220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11220]] ANROT-HELANet: Adverserially and Naturally Robust Attention-Based Aggregation Network via The Hellinger Distance for Few-Shot Classification(https://arxiv.org/abs/2509.11220)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Few-Shot Learning (FSL), which involves learning to generalize using only a few data samples, has demonstrated promising and superior performances to ordinary CNN methods. While Bayesian based estimation approaches using Kullback-Leibler (KL) divergence have shown improvements, they remain vulnerable to adversarial attacks and natural noises. We introduce ANROT-HELANet, an Adversarially and Naturally RObusT Hellinger Aggregation Network that significantly advances the state-of-the-art in FSL robustness and performance. Our approach implements an adversarially and naturally robust Hellinger distance-based feature class aggregation scheme, demonstrating resilience to adversarial perturbations up to $\epsilon=0.30$ and Gaussian noise up to $\sigma=0.30$. The network achieves substantial improvements across benchmark datasets, including gains of 1.20\% and 1.40\% for 1-shot and 5-shot scenarios on miniImageNet respectively. We introduce a novel Hellinger Similarity contrastive loss function that generalizes cosine similarity contrastive loss for variational few-shot inference scenarios. Our approach also achieves superior image reconstruction quality with a FID score of 2.75, outperforming traditional VAE (3.43) and WAE (3.38) approaches. Extensive experiments conducted on four few-shot benchmarked datasets verify that ANROT-HELANet's combination of Hellinger distance-based feature aggregation, attention mechanisms, and our novel loss function establishes new state-of-the-art performance while maintaining robustness against both adversarial and natural perturbations. Our code repository will be available at this https URL.</li>
</ul>

<h3>Title: MIS-LSTM: Multichannel Image-Sequence LSTM for Sleep Quality and Stress Prediction</h3>
<ul>
<li><strong>Authors: </strong>Seongwan Park, Jieun Woo, Siheon Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11232">https://arxiv.org/abs/2509.11232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11232">https://arxiv.org/pdf/2509.11232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11232]] MIS-LSTM: Multichannel Image-Sequence LSTM for Sleep Quality and Stress Prediction(https://arxiv.org/abs/2509.11232)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper presents MIS-LSTM, a hybrid framework that joins CNN encoders with an LSTM sequence model for sleep quality and stress prediction at the day level from multimodal lifelog data. Continuous sensor streams are first partitioned into N-hour blocks and rendered as multi-channel images, while sparse discrete events are encoded with a dedicated 1D-CNN. A Convolutional Block Attention Module fuses the two modalities into refined block embeddings, which an LSTM then aggregates to capture long-range temporal dependencies. To further boost robustness, we introduce UALRE, an uncertainty-aware ensemble that overrides lowconfidence majority votes with high-confidence individual predictions. Experiments on the 2025 ETRI Lifelog Challenge dataset show that Our base MISLSTM achieves Macro-F1 0.615; with the UALRE ensemble, the score improves to 0.647, outperforming strong LSTM, 1D-CNN, and CNN baselines. Ablations confirm (i) the superiority of multi-channel over stacked-vertical imaging, (ii) the benefit of a 4-hour block granularity, and (iii) the efficacy of modality-specific discrete encoding.</li>
</ul>

<h3>Title: TransZero: Parallel Tree Expansion in MuZero using Transformer Networks</h3>
<ul>
<li><strong>Authors: </strong>Emil Malmsten, Wendelin Böhmer</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11233">https://arxiv.org/abs/2509.11233</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11233">https://arxiv.org/pdf/2509.11233</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11233]] TransZero: Parallel Tree Expansion in MuZero using Transformer Networks(https://arxiv.org/abs/2509.11233)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We present TransZero, a model-based reinforcement learning algorithm that removes the sequential bottleneck in Monte Carlo Tree Search (MCTS). Unlike MuZero, which constructs its search tree step by step using a recurrent dynamics model, TransZero employs a transformer-based network to generate multiple latent future states simultaneously. Combined with the Mean-Variance Constrained (MVC) evaluator that eliminates dependence on inherently sequential visitation counts, our approach enables the parallel expansion of entire subtrees during planning. Experiments in MiniGrid and LunarLander show that TransZero achieves up to an eleven-fold speedup in wall-clock time compared to MuZero while maintaining sample efficiency. These results demonstrate that parallel tree construction can substantially accelerate model-based reinforcement learning, bringing real-time decision-making in complex environments closer to practice. The code is publicly available on GitHub.</li>
</ul>

<h3>Title: Implementation of Learning with Errors in Non-Commuting Multiplicative Groups</h3>
<ul>
<li><strong>Authors: </strong>Aleksejus Mihalkovič, Lina Dindiene, Eligijus Sakalauskas</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11237">https://arxiv.org/abs/2509.11237</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11237">https://arxiv.org/pdf/2509.11237</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11237]] Implementation of Learning with Errors in Non-Commuting Multiplicative Groups(https://arxiv.org/abs/2509.11237)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>In this paper, we demonstrate a way to generalize learning with errors (LWE) to the family of so-called modular-maximal cyclic groups which are non-commuting. Since the group M2t has two cycles of maximal multiplicative order, we use this fact to construct an accurate criterion for restoring the message bit with overwhelming probability. Furthermore, we implement the original idea by O. Regev in the considered group to gain benefits from the non-commutativity of M2t . Also we prove that using this approach we can achieve a level of security comparable to the original idea.</li>
</ul>

<h3>Title: Exploring and Exploiting the Resource Isolation Attack Surface of WebAssembly Containers</h3>
<ul>
<li><strong>Authors: </strong>Zhaofeng Yu (1), Dongyang Zhan (1), Lin Ye (1), Haining Yu (1), Hongli Zhang (1), Zhihong Tian (2) ((1) Harbin Institute of Technology, (2) Guangzhou University)</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11242">https://arxiv.org/abs/2509.11242</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11242">https://arxiv.org/pdf/2509.11242</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11242]] Exploring and Exploiting the Resource Isolation Attack Surface of WebAssembly Containers(https://arxiv.org/abs/2509.11242)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect, attack, robust</a></li>
<li><strong>Abstract: </strong>Recently, the WebAssembly (or Wasm) technology has been rapidly evolving, with many runtimes actively under development, providing cross-platform secure sandboxes for Wasm modules to run as portable containers. Compared with Docker, which isolates applications at the operating system level, Wasm runtimes provide more security mechanisms, such as linear memory, type checking, and protected call stacks. Although Wasm is designed with security in mind and considered to be a more secure container runtime, various security challenges have arisen, and researchers have focused on the security of Wasm runtimes, such as discovering vulnerabilities or proposing new security mechanisms to achieve robust isolation. However, we have observed that the resource isolation is not well protected by the current Wasm runtimes, and attackers can exhaust the host's resources to interfere with the execution of other container instances by exploiting the WASI/WASIX interfaces. And the attack surface has not been well explored and measured. In this paper, we explore the resource isolation attack surface of Wasm runtimes systematically by proposing several static Wasm runtime analysis approaches. Based on the analysis results, we propose several exploitation strategies to break the resource isolation of Wasm runtimes. The experimental results show that malicious Wasm instances can not only consume large amounts of system resources on their own but also introduce high workloads into other components of the underlying operating system, leading to a substantial performance degradation of the whole system. In addition, the mitigation approaches have also been discussed.</li>
</ul>

<h3>Title: Contextualized Multimodal Lifelong Person Re-Identification in Hybrid Clothing States</h3>
<ul>
<li><strong>Authors: </strong>Robert Long, Rongxin Jiang, Mingrui Yan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11247">https://arxiv.org/abs/2509.11247</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11247">https://arxiv.org/pdf/2509.11247</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11247]] Contextualized Multimodal Lifelong Person Re-Identification in Hybrid Clothing States(https://arxiv.org/abs/2509.11247)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Person Re-Identification (ReID) has several challenges in real-world surveillance systems due to clothing changes (CCReID) and the need for maintaining continual learning (LReID). Previous existing methods either develop models specifically for one application, which is mostly a same-cloth (SC) setting or treat CCReID as its own separate sub-problem. In this work, we will introduce the LReID-Hybrid task with the goal of developing a model to achieve both SC and CC while learning in a continual setting. Mismatched representations and forgetting from one task to the next are significant issues, we address this with CMLReID, a CLIP-based framework composed of two novel tasks: (1) Context-Aware Semantic Prompt (CASP) that generates adaptive prompts, and also incorporates context to align richly multi-grained visual cues with semantic text space; and (2) Adaptive Knowledge Fusion and Projection (AKFP) which produces robust SC/CC prototypes through the use of a dual-path learner that aligns features with our Clothing-State-Aware Projection Loss. Experiments performed on a wide range of datasets and illustrate that CMLReID outperforms all state-of-the-art methods with strong robustness and generalization despite clothing variations and a sophisticated process of sequential learning.</li>
</ul>

<h3>Title: Make Identity Unextractable yet Perceptible: Synthesis-Based Privacy Protection for Subject Faces in Photos</h3>
<ul>
<li><strong>Authors: </strong>Tao Wang, Yushu Zhang, Xiangli Xiao, Kun Xu, Lin Yuan, Wenying Wen, Yuming Fang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11249">https://arxiv.org/abs/2509.11249</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11249">https://arxiv.org/pdf/2509.11249</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11249]] Make Identity Unextractable yet Perceptible: Synthesis-Based Privacy Protection for Subject Faces in Photos(https://arxiv.org/abs/2509.11249)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, extraction</a></li>
<li><strong>Abstract: </strong>Deep learning-based face recognition (FR) technology exacerbates privacy concerns in photo sharing. In response, the research community developed a suite of anti-FR methods to block identity extraction by unauthorized FR systems. Benefiting from quasi-imperceptible alteration, perturbation-based methods are well-suited for privacy protection of subject faces in photos, as they allow familiar persons to recognize subjects via naked eyes. However, we reveal that perturbation-based methods provide a false sense of privacy through theoretical analysis and experimental validation. Therefore, new alternative solutions should be found to protect subject faces. In this paper, we explore synthesis-based methods as a promising solution, whose challenge is to enable familiar persons to recognize subjects. To solve the challenge, we present a key insight: In most photo sharing scenarios, familiar persons recognize subjects through identity perception rather than meticulous face analysis. Based on the insight, we propose the first synthesis-based method dedicated to subject faces, i.e., PerceptFace, which can make identity unextractable yet perceptible. To enhance identity perception, a new perceptual similarity loss is designed for faces, reducing the alteration in regions of high sensitivity to human vision. As a synthesis-based method, PerceptFace can inherently provide reliable identity protection. Meanwhile, out of the confine of meticulous face analysis, PerceptFace focuses on identity perception from a more practical scenario, which is also enhanced by the designed perceptual similarity loss. Sufficient experiments show that PerceptFace achieves a superior trade-off between identity protection and identity perception compared to existing methods. We provide a public API of PerceptFace and believe that it has great potential to become a practical anti-FR tool.</li>
</ul>

<h3>Title: Realistic Environmental Injection Attacks on GUI Agents</h3>
<ul>
<li><strong>Authors: </strong>Yitong Zhang, Ximo Li, Liyi Cai, Jia Li</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11250">https://arxiv.org/abs/2509.11250</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11250">https://arxiv.org/pdf/2509.11250</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11250]] Realistic Environmental Injection Attacks on GUI Agents(https://arxiv.org/abs/2509.11250)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>GUI agents built on LVLMs are increasingly used to interact with websites. However, their exposure to open-world content makes them vulnerable to Environmental Injection Attacks (EIAs) that hijack agent behavior via webpage elements. Many recent studies assume the attacker to be a regular user who can only upload a single trigger image, which is more realistic than earlier assumptions of website-level administrative control. However, these works still fall short of realism: (1) the trigger's position and surrounding context remain largely fixed between training and testing, failing to capture the dynamic nature of real webpages and (2) the trigger often occupies an unrealistically large area, whereas real-world images are typically small. To better reflect real-world scenarios, we introduce a more realistic threat model where the attacker is a regular user and the trigger image is small and embedded within a dynamically changing environment. As a result, existing attacks prove largely ineffective under this threat model. To better expose the vulnerabilities of GUI agents, we propose Chameleon, an attack framework with two main novelties. The first is LLM-Driven Environment Simulation, which automatically generates diverse and high-fidelity webpage simulations. The second is Attention Black Hole, which transforms attention weights into explicit supervisory signals that guide the agent's focus toward the trigger region. We evaluate Chameleon on 6 realistic websites and 4 representative LVLM-powered GUI agents, where it significantly outperforms existing methods. Ablation studies confirm that both novelties are critical to performance. Our findings reveal underexplored vulnerabilities in modern GUI agents and establish a robust foundation for future research on defense in open-world GUI agent systems. The code is publicly available at this https URL.</li>
</ul>

<h3>Title: Gradient Free Deep Reinforcement Learning With TabPFN</h3>
<ul>
<li><strong>Authors: </strong>David Schiff, Ofir Lindenbaum, Yonathan Efroni</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11259">https://arxiv.org/abs/2509.11259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11259">https://arxiv.org/pdf/2509.11259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11259]] Gradient Free Deep Reinforcement Learning With TabPFN(https://arxiv.org/abs/2509.11259)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Gradient based optimization is fundamental to most modern deep reinforcement learning algorithms, however, it introduces significant sensitivity to hyperparameters, unstable training dynamics, and high computational costs. We propose TabPFN RL, a novel gradient free deep RL framework that repurposes the meta trained transformer TabPFN as a Q function approximator. Originally developed for tabular classification, TabPFN is a transformer pre trained on millions of synthetic datasets to perform inference on new unseen datasets via in context learning. Given an in context dataset of sample label pairs and new unlabeled data, it predicts the most likely labels in a single forward pass, without gradient updates or task specific fine tuning. We use TabPFN to predict Q values using inference only, thereby eliminating the need for back propagation at both training and inference. To cope with the model's fixed context budget, we design a high reward episode gate that retains only the top 5% of trajectories. Empirical evaluations on the Gymnasium classic control suite demonstrate that TabPFN RL matches or surpasses Deep Q Network on CartPole v1, MountainCar v0, and Acrobot v1, without applying gradient descent or any extensive hyperparameter tuning. We discuss the theoretical aspects of how bootstrapped targets and non stationary visitation distributions violate the independence assumptions encoded in TabPFN's prior, yet the model retains a surprising generalization capacity. We further formalize the intrinsic context size limit of in context RL algorithms and propose principled truncation strategies that enable continual learning when the context is full. Our results establish prior fitted networks such as TabPFN as a viable foundation for fast and computationally efficient RL, opening new directions for gradient free RL with large pre trained transformers.</li>
</ul>

<h3>Title: SelectMix: Enhancing Label Noise Robustness through Targeted Sample Mixing</h3>
<ul>
<li><strong>Authors: </strong>Qiuhao Liu, Ling Li, Yao Lu, Qi Xuan, Zhaowei Zhu, Jiaheng Wei</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11265">https://arxiv.org/abs/2509.11265</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11265">https://arxiv.org/pdf/2509.11265</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11265]] SelectMix: Enhancing Label Noise Robustness through Targeted Sample Mixing(https://arxiv.org/abs/2509.11265)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Deep neural networks tend to memorize noisy labels, severely degrading their generalization performance. Although Mixup has demonstrated effectiveness in improving generalization and robustness, existing Mixup-based methods typically perform indiscriminate mixing without principled guidance on sample selection and mixing strategy, inadvertently propagating noisy supervision. To overcome these limitations, we propose SelectMix, a confidence-guided mixing framework explicitly tailored for noisy labels. SelectMix first identifies potentially noisy or ambiguous samples through confidence based mismatch analysis using K-fold cross-validation, then selectively blends identified uncertain samples with confidently predicted peers from their potential classes. Furthermore, SelectMix employs soft labels derived from all classes involved in the mixing process, ensuring the labels accurately represent the composition of the mixed samples, thus aligning supervision signals closely with the actual mixed inputs. Through extensive theoretical analysis and empirical evaluations on multiple synthetic (MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100) and real-world benchmark datasets (CIFAR-N, MNIST and Clothing1M), we demonstrate that SelectMix consistently outperforms strong baseline methods, validating its effectiveness and robustness in learning with noisy labels.</li>
</ul>

<h3>Title: Protected Probabilistic Classification Library</h3>
<ul>
<li><strong>Authors: </strong>Ivan Petej</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11267">https://arxiv.org/abs/2509.11267</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11267">https://arxiv.org/pdf/2509.11267</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11267]] Protected Probabilistic Classification Library(https://arxiv.org/abs/2509.11267)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>This paper introduces a new Python package specifically designed to address calibration of probabilistic classifiers under dataset shift. The method is demonstrated in binary and multi-class settings and its effectiveness is measured against a number of existing post-hoc calibration methods. The empirical results are promising and suggest that our technique can be helpful in a variety of settings for batch and online learning classification problems where the underlying data distribution changes between the training and test sets.</li>
</ul>

<h3>Title: Synthetic Dataset Evaluation Based on Generalized Cross Validation</h3>
<ul>
<li><strong>Authors: </strong>Zhihang Song, Dingyi Yao, Ruibo Ming, Lihui Peng, Danya Yao, Yi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11273">https://arxiv.org/abs/2509.11273</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11273">https://arxiv.org/pdf/2509.11273</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11273]] Synthetic Dataset Evaluation Based on Generalized Cross Validation(https://arxiv.org/abs/2509.11273)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of synthetic dataset generation techniques, evaluating the quality of synthetic data has become a critical research focus. Robust evaluation not only drives innovations in data generation methods but also guides researchers in optimizing the utilization of these synthetic resources. However, current evaluation studies for synthetic datasets remain limited, lacking a universally accepted standard framework. To address this, this paper proposes a novel evaluation framework integrating generalized cross-validation experiments and domain transfer learning principles, enabling generalizable and comparable assessments of synthetic dataset quality. The framework involves training task-specific models (e.g., YOLOv5s) on both synthetic datasets and multiple real-world benchmarks (e.g., KITTI, BDD100K), forming a cross-performance matrix. Following normalization, a Generalized Cross-Validation (GCV) Matrix is constructed to quantify domain transferability. The framework introduces two key metrics. One measures the simulation quality by quantifying the similarity between synthetic data and real-world datasets, while another evaluates the transfer quality by assessing the diversity and coverage of synthetic data across various real-world scenarios. Experimental validation on Virtual KITTI demonstrates the effectiveness of our proposed framework and metrics in assessing synthetic data fidelity. This scalable and quantifiable evaluation solution overcomes traditional limitations, providing a principled approach to guide synthetic dataset optimization in artificial intelligence research.</li>
</ul>

<h3>Title: PINGS: Physics-Informed Neural Network for Fast Generative Sampling</h3>
<ul>
<li><strong>Authors: </strong>Achmad Ardani Prasha, Clavino Ourizqi Rachmadi, Muhamad Fauzan Ibnu Syahlan, Naufal Rahfi Anugerah, Nanda Garin Raditya, Putri Amelia, Sabrina Laila Mutiara, Hilman Syachr Ramadhan</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11284">https://arxiv.org/abs/2509.11284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11284">https://arxiv.org/pdf/2509.11284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11284]] PINGS: Physics-Informed Neural Network for Fast Generative Sampling(https://arxiv.org/abs/2509.11284)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We introduce PINGS (Physics-Informed Neural Network for Fast Generative Sampling), a framework that amortizes diffusion sampling by training a physics-informed network to approximate reverse-time probability-flow dynamics, reducing sampling to a single forward pass (NFE = 1). As a proof of concept, we learn a direct map from a 3D standard normal to a non-Gaussian Gaussian Mixture Model (GMM). PINGS preserves the target's distributional structure (multi-bandwidth kernel $MMD^2 = 1.88 \times 10^{-2}$ with small errors in mean, covariance, skewness, and excess kurtosis) and achieves constant-time generation: $10^4$ samples in $16.54 \pm 0.56$ millisecond on an RTX 3090, versus 468-843 millisecond for DPM-Solver (10/20) and 960 millisecond for DDIM (50) under matched conditions. We also sanity-check the PINN/automatic-differentiation pipeline on a damped harmonic oscillator, obtaining MSEs down to $\mathcal{O}(10^{-5})$. Compared to fast but iterative ODE solvers and direct-map families (Flow, Rectified-Flow, Consistency), PINGS frames generative sampling as a PINN-style residual problem with endpoint anchoring, yielding a white-box, differentiable map with NFE = 1. These proof-of-concept results position PINGS as a promising route to fast, function-based generative sampling with potential extensions to scientific simulation (e.g., fast calorimetry).</li>
</ul>

<h3>Title: Leveraging Geometric Priors for Unaligned Scene Change Detection</h3>
<ul>
<li><strong>Authors: </strong>Ziling Liu, Ziwei Chen, Mingqi Gao, Jinyu Yang, Feng Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11292">https://arxiv.org/abs/2509.11292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11292">https://arxiv.org/pdf/2509.11292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11292]] Leveraging Geometric Priors for Unaligned Scene Change Detection(https://arxiv.org/abs/2509.11292)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Unaligned Scene Change Detection aims to detect scene changes between image pairs captured at different times without assuming viewpoint alignment. To handle viewpoint variations, current methods rely solely on 2D visual cues to establish cross-image correspondence to assist change detection. However, large viewpoint changes can alter visual observations, causing appearance-based matching to drift or fail. Additionally, supervision limited to 2D change masks from small-scale SCD datasets restricts the learning of generalizable multi-view knowledge, making it difficult to reliably identify visual overlaps and handle occlusions. This lack of explicit geometric reasoning represents a critical yet overlooked limitation. In this work, we are the first to leverage geometric priors from a Geometric Foundation Model to address the core challenges of unaligned SCD, including reliable identification of visual overlaps, robust correspondence establishment, and explicit occlusion detection. Building on these priors, we propose a training-free framework that integrates them with the powerful representations of a visual foundation model to enable reliable change detection under viewpoint misalignment. Through extensive evaluation on the PSCD, ChangeSim, and PASLCD datasets, we demonstrate that our approach achieves superior and robust performance. Our code will be released at this https URL.</li>
</ul>

<h3>Title: The Prompt Engineering Report Distilled: Quick Start Guide for Life Sciences</h3>
<ul>
<li><strong>Authors: </strong>Valentin Romanov, Steven A Niederer</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11295">https://arxiv.org/abs/2509.11295</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11295">https://arxiv.org/pdf/2509.11295</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11295]] The Prompt Engineering Report Distilled: Quick Start Guide for Life Sciences(https://arxiv.org/abs/2509.11295)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Developing effective prompts demands significant cognitive investment to generate reliable, high-quality responses from Large Language Models (LLMs). By deploying case-specific prompt engineering techniques that streamline frequently performed life sciences workflows, researchers could achieve substantial efficiency gains that far exceed the initial time investment required to master these techniques. The Prompt Report published in 2025 outlined 58 different text-based prompt engineering techniques, highlighting the numerous ways prompts could be constructed. To provide actionable guidelines and reduce the friction of navigating these various approaches, we distil this report to focus on 6 core techniques: zero-shot, few-shot approaches, thought generation, ensembling, self-criticism, and decomposition. We breakdown the significance of each approach and ground it in use cases relevant to life sciences, from literature summarization and data extraction to editorial tasks. We provide detailed recommendations for how prompts should and shouldn't be structured, addressing common pitfalls including multi-turn conversation degradation, hallucinations, and distinctions between reasoning and non-reasoning models. We examine context window limitations, agentic tools like Claude Code, while analyzing the effectiveness of Deep Research tools across OpenAI, Google, Anthropic and Perplexity platforms, discussing current limitations. We demonstrate how prompt engineering can augment rather than replace existing established individual practices around data processing and document editing. Our aim is to provide actionable guidance on core prompt engineering principles, and to facilitate the transition from opportunistic prompting to an effective, low-friction systematic practice that contributes to higher quality research.</li>
</ul>

<h3>Title: UnLoc: Leveraging Depth Uncertainties for Floorplan Localization</h3>
<ul>
<li><strong>Authors: </strong>Matthias Wüest, Francis Engelmann, Ondrej Miksik, Marc Pollefeys, Daniel Barath</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11301">https://arxiv.org/abs/2509.11301</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11301">https://arxiv.org/pdf/2509.11301</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11301]] UnLoc: Leveraging Depth Uncertainties for Floorplan Localization(https://arxiv.org/abs/2509.11301)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We propose UnLoc, an efficient data-driven solution for sequential camera localization within floorplans. Floorplan data is readily available, long-term persistent, and robust to changes in visual appearance. We address key limitations of recent methods, such as the lack of uncertainty modeling in depth predictions and the necessity for custom depth networks trained for each environment. We introduce a novel probabilistic model that incorporates uncertainty estimation, modeling depth predictions as explicit probability distributions. By leveraging off-the-shelf pre-trained monocular depth models, we eliminate the need to rely on per-environment-trained depth networks, enhancing generalization to unseen spaces. We evaluate UnLoc on large-scale synthetic and real-world datasets, demonstrating significant improvements over existing methods in terms of accuracy and robustness. Notably, we achieve $2.7$ times higher localization recall on long sequences (100 frames) and $16.7$ times higher on short ones (15 frames) than the state of the art on the challenging LaMAR HGE dataset.</li>
</ul>

<h3>Title: Motion Estimation for Multi-Object Tracking using KalmanNet with Semantic-Independent Encoding</h3>
<ul>
<li><strong>Authors: </strong>Jian Song, Wei Mei, Yunfeng Xu, Qiang Fu, Renke Kou, Lina Bu, Yucheng Long</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11323">https://arxiv.org/abs/2509.11323</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11323">https://arxiv.org/pdf/2509.11323</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11323]] Motion Estimation for Multi-Object Tracking using KalmanNet with Semantic-Independent Encoding(https://arxiv.org/abs/2509.11323)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Motion estimation is a crucial component in multi-object tracking (MOT). It predicts the trajectory of objects by analyzing the changes in their positions in consecutive frames of images, reducing tracking failures and identity switches. The Kalman filter (KF) based on the linear constant-velocity model is one of the most commonly used methods in MOT. However, it may yield unsatisfactory results when KF's parameters are mismatched and objects move in non-stationary. In this work, we utilize the learning-aided filter to handle the motion estimation of MOT. In particular, we propose a novel method named Semantic-Independent KalmanNet (SIKNet), which encodes the state vector (the input feature) using a Semantic-Independent Encoder (SIE) by two steps. First, the SIE uses a 1D convolution with a kernel size of 1, which convolves along the dimension of homogeneous-semantic elements across different state vectors to encode independent semantic information. Then it employs a fully-connected layer and a nonlinear activation layer to encode nonlinear and cross-dependency information between heterogeneous-semantic elements. To independently evaluate the performance of the motion estimation module in MOT, we constructed a large-scale semi-simulated dataset from several open-source MOT datasets. Experimental results demonstrate that the proposed SIKNet outperforms the traditional KF and achieves superior robustness and accuracy than existing learning-aided filters. The code is available at (this https URL and this https URL).</li>
</ul>

<h3>Title: Toward Next-generation Medical Vision Backbones: Modeling Finer-grained Long-range Visual Dependency</h3>
<ul>
<li><strong>Authors: </strong>Mingyuan Meng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11328">https://arxiv.org/abs/2509.11328</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11328">https://arxiv.org/pdf/2509.11328</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11328]] Toward Next-generation Medical Vision Backbones: Modeling Finer-grained Long-range Visual Dependency(https://arxiv.org/abs/2509.11328)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Medical Image Computing (MIC) is a broad research topic covering both pixel-wise (e.g., segmentation, registration) and image-wise (e.g., classification, regression) vision tasks. Effective analysis demands models that capture both global long-range context and local subtle visual characteristics, necessitating fine-grained long-range visual dependency modeling. Compared to Convolutional Neural Networks (CNNs) that are limited by intrinsic locality, transformers excel at long-range modeling; however, due to the high computational loads of self-attention, transformers typically cannot process high-resolution features (e.g., full-scale image features before downsampling or patch embedding) and thus face difficulties in modeling fine-grained dependency among subtle medical image details. Concurrently, Multi-layer Perceptron (MLP)-based visual models are recognized as computation/memory-efficient alternatives in modeling long-range visual dependency but have yet to be widely investigated in the MIC community. This doctoral research advances deep learning-based MIC by investigating effective long-range visual dependency modeling. It first presents innovative use of transformers for both pixel- and image-wise medical vision tasks. The focus then shifts to MLPs, pioneeringly developing MLP-based visual models to capture fine-grained long-range visual dependency in medical images. Extensive experiments confirm the critical role of long-range dependency modeling in MIC and reveal a key finding: MLPs provide feasibility in modeling finer-grained long-range dependency among higher-resolution medical features containing enriched anatomical/pathological details. This finding establishes MLPs as a superior paradigm over transformers/CNNs, consistently enhancing performance across various medical vision tasks and paving the way for next-generation medical vision backbones.</li>
</ul>

<h3>Title: MatQnA: A Benchmark Dataset for Multi-modal Large Language Models in Materials Characterization and Analysis</h3>
<ul>
<li><strong>Authors: </strong>Yonghao Weng, Liqiang Gao, Linwu Zhu, Jian Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11335">https://arxiv.org/abs/2509.11335</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11335">https://arxiv.org/pdf/2509.11335</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11335]] MatQnA: A Benchmark Dataset for Multi-modal Large Language Models in Materials Characterization and Analysis(https://arxiv.org/abs/2509.11335)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recently, large language models (LLMs) have achieved remarkable breakthroughs in general domains such as programming and writing, and have demonstrated strong potential in various scientific research scenarios. However, the capabilities of AI models in the highly specialized field of materials characterization and analysis have not yet been systematically or sufficiently validated. To address this gap, we present MatQnA, the first multi-modal benchmark dataset specifically designed for material characterization techniques. MatQnA includes ten mainstream characterization methods, such as X-ray Photoelectron Spectroscopy (XPS), X-ray Diffraction (XRD), Scanning Electron Microscopy (SEM), Transmission Electron Microscopy (TEM), etc. We employ a hybrid approach combining LLMs with human-in-the-loop validation to construct high-quality question-answer pairs, integrating both multiple-choice and subjective questions. Our preliminary evaluation results show that the most advanced multi-modal AI models (e.g., GPT-4.1, Claude 4, Gemini 2.5, and Doubao Vision Pro 32K) have already achieved nearly 90% accuracy on objective questions in materials data interpretation and analysis tasks, demonstrating strong potential for applications in materials characterization and analysis. The MatQnA dataset is publicly available at this https URL.</li>
</ul>

<h3>Title: On the Escaping Efficiency of Distributed Adversarial Training Algorithms</h3>
<ul>
<li><strong>Authors: </strong>Ying Cao, Kun Yuan, Ali H. Sayed</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11337">https://arxiv.org/abs/2509.11337</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11337">https://arxiv.org/pdf/2509.11337</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11337]] On the Escaping Efficiency of Distributed Adversarial Training Algorithms(https://arxiv.org/abs/2509.11337)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, diffusion</a></li>
<li><strong>Abstract: </strong>Adversarial training has been widely studied in recent years due to its role in improving model robustness against adversarial attacks. This paper focuses on comparing different distributed adversarial training algorithms--including centralized and decentralized strategies--within multi-agent learning environments. Previous studies have highlighted the importance of model flatness in determining robustness. To this end, we develop a general theoretical framework to study the escaping efficiency of these algorithms from local minima, which is closely related to the flatness of the resulting models. We show that when the perturbation bound is sufficiently small (i.e., when the attack strength is relatively mild) and a large batch size is used, decentralized adversarial training algorithms--including consensus and diffusion--are guaranteed to escape faster from local minima than the centralized strategy, thereby favoring flatter minima. However, as the perturbation bound increases, this trend may no longer hold. In the simulation results, we illustrate our theoretical findings and systematically compare the performance of models obtained through decentralized and centralized adversarial training algorithms. The results highlight the potential of decentralized strategies to enhance the robustness of models in distributed settings.</li>
</ul>

<h3>Title: Beyond Instance Consistency: Investigating View Diversity in Self-supervised Learning</h3>
<ul>
<li><strong>Authors: </strong>Huaiyuan Qin, Muli Yang, Siyuan Hu, Peng Hu, Yu Zhang, Chen Gong, Hongyuan Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11344">https://arxiv.org/abs/2509.11344</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11344">https://arxiv.org/pdf/2509.11344</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11344]] Beyond Instance Consistency: Investigating View Diversity in Self-supervised Learning(https://arxiv.org/abs/2509.11344)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Self-supervised learning (SSL) conventionally relies on the instance consistency paradigm, assuming that different views of the same image can be treated as positive pairs. However, this assumption breaks down for non-iconic data, where different views may contain distinct objects or semantic information. In this paper, we investigate the effectiveness of SSL when instance consistency is not guaranteed. Through extensive ablation studies, we demonstrate that SSL can still learn meaningful representations even when positive pairs lack strict instance consistency. Furthermore, our analysis further reveals that increasing view diversity, by enforcing zero overlapping or using smaller crop scales, can enhance downstream performance on classification and dense prediction tasks. However, excessive diversity is found to reduce effectiveness, suggesting an optimal range for view diversity. To quantify this, we adopt the Earth Mover's Distance (EMD) as an estimator to measure mutual information between views, finding that moderate EMD values correlate with improved SSL learning, providing insights for future SSL framework design. We validate our findings across a range of settings, highlighting their robustness and applicability on diverse data sources.</li>
</ul>

<h3>Title: Promoting Shape Bias in CNNs: Frequency-Based and Contrastive Regularization for Corruption Robustness</h3>
<ul>
<li><strong>Authors: </strong>Robin Narsingh Ranabhat, Longwei Wang, Amit Kumar Patel, KC santosh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11355">https://arxiv.org/abs/2509.11355</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11355">https://arxiv.org/pdf/2509.11355</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11355]] Promoting Shape Bias in CNNs: Frequency-Based and Contrastive Regularization for Corruption Robustness(https://arxiv.org/abs/2509.11355)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Convolutional Neural Networks (CNNs) excel at image classification but remain vulnerable to common corruptions that humans handle with ease. A key reason for this fragility is their reliance on local texture cues rather than global object shapes -- a stark contrast to human perception. To address this, we propose two complementary regularization strategies designed to encourage shape-biased representations and enhance robustness. The first introduces an auxiliary loss that enforces feature consistency between original and low-frequency filtered inputs, discouraging dependence on high-frequency textures. The second incorporates supervised contrastive learning to structure the feature space around class-consistent, shape-relevant representations. Evaluated on the CIFAR-10-C benchmark, both methods improve corruption robustness without degrading clean accuracy. Our results suggest that loss-level regularization can effectively steer CNNs toward more shape-aware, resilient representations.</li>
</ul>

<h3>Title: PersonaX: Multimodal Datasets with LLM-Inferred Behavior Traits</h3>
<ul>
<li><strong>Authors: </strong>Loka Li, Wong Yu Kang, Minghao Fu, Guangyi Chen, Zhenhao Chen, Gongxu Luo, Yuewen Sun, Salman Khan, Peter Spirtes, Kun Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11362">https://arxiv.org/abs/2509.11362</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11362">https://arxiv.org/pdf/2509.11362</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11362]] PersonaX: Multimodal Datasets with LLM-Inferred Behavior Traits(https://arxiv.org/abs/2509.11362)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Understanding human behavior traits is central to applications in human-computer interaction, computational social science, and personalized AI systems. Such understanding often requires integrating multiple modalities to capture nuanced patterns and relationships. However, existing resources rarely provide datasets that combine behavioral descriptors with complementary modalities such as facial attributes and biographical information. To address this gap, we present PersonaX, a curated collection of multimodal datasets designed to enable comprehensive analysis of public traits across modalities. PersonaX consists of (1) CelebPersona, featuring 9444 public figures from diverse occupations, and (2) AthlePersona, covering 4181 professional athletes across 7 major sports leagues. Each dataset includes behavioral trait assessments inferred by three high-performing large language models, alongside facial imagery and structured biographical features. We analyze PersonaX at two complementary levels. First, we abstract high-level trait scores from text descriptions and apply five statistical independence tests to examine their relationships with other modalities. Second, we introduce a novel causal representation learning (CRL) framework tailored to multimodal and multi-measurement data, providing theoretical identifiability guarantees. Experiments on both synthetic and real-world data demonstrate the effectiveness of our approach. By unifying structured and unstructured analysis, PersonaX establishes a foundation for studying LLM-inferred behavioral traits in conjunction with visual and biographical attributes, advancing multimodal trait analysis and causal reasoning.</li>
</ul>

<h3>Title: !MSA at AraHealthQA 2025 Shared Task: Enhancing LLM Performance for Arabic Clinical Question Answering through Prompt Engineering and Ensemble Learning</h3>
<ul>
<li><strong>Authors: </strong>Mohamed Tarek, Seif Ahmed, Mohamed Basem</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11365">https://arxiv.org/abs/2509.11365</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11365">https://arxiv.org/pdf/2509.11365</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11365]] !MSA at AraHealthQA 2025 Shared Task: Enhancing LLM Performance for Arabic Clinical Question Answering through Prompt Engineering and Ensemble Learning(https://arxiv.org/abs/2509.11365)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>We present our systems for Track 2 (General Arabic Health QA, MedArabiQ) of the AraHealthQA-2025 shared task, where our methodology secured 2nd place in both Sub-Task 1 (multiple-choice question answering) and Sub-Task 2 (open-ended question answering) in Arabic clinical contexts. For Sub-Task 1, we leverage the Gemini 2.5 Flash model with few-shot prompting, dataset preprocessing, and an ensemble of three prompt configurations to improve classification accuracy on standard, biased, and fill-in-the-blank questions. For Sub-Task 2, we employ a unified prompt with the same model, incorporating role-playing as an Arabic medical expert, few-shot examples, and post-processing to generate concise responses across fill-in-the-blank, patient-doctor Q&A, GEC, and paraphrased variants.</li>
</ul>

<h3>Title: Decoding Musical Origins: Distinguishing Human and AI Composers</h3>
<ul>
<li><strong>Authors: </strong>Cheng-Yang Tsai, Tzu-Wei Huang, Shao-Yu Wei, Guan-Wei Chen, Hung-Ying Chu, Yu-Cheng Lin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11369">https://arxiv.org/abs/2509.11369</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11369">https://arxiv.org/pdf/2509.11369</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11369]] Decoding Musical Origins: Distinguishing Human and AI Composers(https://arxiv.org/abs/2509.11369)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of Large Language Models (LLMs), AI-driven music generation has become a vibrant and fruitful area of research. However, the representation of musical data remains a significant challenge. To address this, a novel, machine-learning-friendly music notation system, YNote, was developed. This study leverages YNote to train an effective classification model capable of distinguishing whether a piece of music was composed by a human (Native), a rule-based algorithm (Algorithm Generated), or an LLM (LLM Generated). We frame this as a text classification problem, applying the Term Frequency-Inverse Document Frequency (TF-IDF) algorithm to extract structural features from YNote sequences and using the Synthetic Minority Over-sampling Technique (SMOTE) to address data imbalance. The resulting model achieves an accuracy of 98.25%, successfully demonstrating that YNote retains sufficient stylistic information for analysis. More importantly, the model can identify the unique " technological fingerprints " left by different AI generation techniques, providing a powerful tool for tracing the origins of AI-generated content.</li>
</ul>

<h3>Title: Transformer Enhanced Relation Classification: A Comparative Analysis of Contextuality, Data Efficiency and Sequence Complexity</h3>
<ul>
<li><strong>Authors: </strong>Bowen Jing, Yang Cui, Tianpeng Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11374">https://arxiv.org/abs/2509.11374</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11374">https://arxiv.org/pdf/2509.11374</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11374]] Transformer Enhanced Relation Classification: A Comparative Analysis of Contextuality, Data Efficiency and Sequence Complexity(https://arxiv.org/abs/2509.11374)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer, large language model</a></li>
<li><strong>Abstract: </strong>In the era of large language model, relation extraction (RE) plays an important role in information extraction through the transformation of unstructured raw text into structured data (Wadhwa et al., 2023). In this paper, we systematically compare the performance of deep supervised learning approaches without transformers and those with transformers. We used a series of non-transformer architectures such as PA-LSTM(Zhang et al., 2017), C-GCN(Zhang et al., 2018), and AGGCN(attention guide GCN)(Guo et al., 2019), and a series of transformer architectures such as BERT, RoBERTa, and R-BERT(Wu and He, 2019). Our comparison included traditional metrics like micro F1, as well as evaluations in different scenarios, varying sentence lengths, and different percentages of the dataset for training. Our experiments were conducted on TACRED, TACREV, and RE-TACRED. The results show that transformer-based models outperform non-transformer models, achieving micro F1 scores of 80-90% compared to 64-67% for non-transformer models. Additionally, we briefly review the research journey in supervised relation classification and discuss the role and current status of large language models (LLMs) in relation extraction.</li>
</ul>

<h3>Title: Intelligent Reservoir Decision Support: An Integrated Framework Combining Large Language Models, Advanced Prompt Engineering, and Multimodal Data Fusion for Real-Time Petroleum Operations</h3>
<ul>
<li><strong>Authors: </strong>Seyed Kourosh Mahjour, Seyed Saman Mahjour</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11376">https://arxiv.org/abs/2509.11376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11376">https://arxiv.org/pdf/2509.11376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11376]] Intelligent Reservoir Decision Support: An Integrated Framework Combining Large Language Models, Advanced Prompt Engineering, and Multimodal Data Fusion for Real-Time Petroleum Operations(https://arxiv.org/abs/2509.11376)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>The petroleum industry faces unprecedented challenges in reservoir management, requiring rapid integration of complex multimodal datasets for real-time decision support. This study presents a novel integrated framework combining state-of-the-art large language models (GPT-4o, Claude 4 Sonnet, Gemini 2.5 Pro) with advanced prompt engineering techniques and multimodal data fusion for comprehensive reservoir analysis. The framework implements domain-specific retrieval-augmented generation (RAG) with over 50,000 petroleum engineering documents, chain-of-thought reasoning, and few-shot learning for rapid field adaptation. Multimodal integration processes seismic interpretations, well logs, and production data through specialized AI models with vision transformers. Field validation across 15 diverse reservoir environments demonstrates exceptional performance: 94.2% reservoir characterization accuracy, 87.6% production forecasting precision, and 91.4% well placement optimization success rate. The system achieves sub-second response times while maintaining 96.2% safety reliability with no high-risk incidents during evaluation. Economic analysis reveals 62-78% cost reductions (mean 72%) relative to traditional methods with 8-month payback period. Few-shot learning reduces field adaptation time by 72%, while automated prompt optimization achieves 89% improvement in reasoning quality. The framework processed real-time data streams with 96.2% anomaly detection accuracy and reduced environmental incidents by 45%. We provide detailed experimental protocols, baseline comparisons, ablation studies, and statistical significance testing to ensure reproducibility. This research demonstrates practical integration of cutting-edge AI technologies with petroleum domain expertise for enhanced operational efficiency, safety, and economic performance.</li>
</ul>

<h3>Title: Enhancing ML Models Interpretability for Credit Scoring</h3>
<ul>
<li><strong>Authors: </strong>Sagi Schwartz, Qinling Wang, Fang Fang</a></li>
<li><strong>Subjects: </strong>cs.LG, q-fin.RM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11389">https://arxiv.org/abs/2509.11389</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11389">https://arxiv.org/pdf/2509.11389</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11389]] Enhancing ML Models Interpretability for Credit Scoring(https://arxiv.org/abs/2509.11389)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Predicting default is essential for banks to ensure profitability and financial stability. While modern machine learning methods often outperform traditional regression techniques, their lack of transparency limits their use in regulated environments. Explainable artificial intelligence (XAI) has emerged as a solution in domains like credit scoring. However, most XAI research focuses on post-hoc interpretation of black-box models, which does not produce models lightweight or transparent enough to meet regulatory requirements, such as those for Internal Ratings-Based (IRB) models. This paper proposes a hybrid approach: post-hoc interpretations of black-box models guide feature selection, followed by training glass-box models that maintain both predictive power and transparency. Using the Lending Club dataset, we demonstrate that this approach achieves performance comparable to a benchmark black-box model while using only 10 features - an 88.5% reduction. In our example, SHapley Additive exPlanations (SHAP) is used for feature selection, eXtreme Gradient Boosting (XGBoost) serves as the benchmark and the base black-box model, and Explainable Boosting Machine (EBM) and Penalized Logistic Tree Regression (PLTR) are the investigated glass-box models. We also show that model refinement using feature interaction analysis, correlation checks, and expert input can further enhance model interpretability and robustness.</li>
</ul>

<h3>Title: From Firewalls to Frontiers: AI Red-Teaming is a Domain-Specific Evolution of Cyber Red-Teaming</h3>
<ul>
<li><strong>Authors: </strong>Anusha Sinha, Keltin Grimes, James Lucassen, Michael Feffer, Nathan VanHoudnos, Zhiwei Steven Wu, Hoda Heidari</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11398">https://arxiv.org/abs/2509.11398</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11398">https://arxiv.org/pdf/2509.11398</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11398]] From Firewalls to Frontiers: AI Red-Teaming is a Domain-Specific Evolution of Cyber Red-Teaming(https://arxiv.org/abs/2509.11398)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>A red team simulates adversary attacks to help defenders find effective strategies to defend their systems in a real-world operational setting. As more enterprise systems adopt AI, red-teaming will need to evolve to address the unique vulnerabilities and risks posed by AI systems. We take the position that AI systems can be more effectively red-teamed if AI red-teaming is recognized as a domain-specific evolution of cyber red-teaming. Specifically, we argue that existing Cyber Red Teams who adopt this framing will be able to better evaluate systems with AI components by recognizing that AI poses new risks, has new failure modes to exploit, and often contains unpatchable bugs that re-prioritize disclosure and mitigation strategies. Similarly, adopting a cybersecurity framing will allow existing AI Red Teams to leverage a well-tested structure to emulate realistic adversaries, promote mutual accountability with formal rules of engagement, and provide a pattern to mature the tooling necessary for repeatable, scalable engagements. In these ways, the merging of AI and Cyber Red Teams will create a robust security ecosystem and best position the community to adapt to the rapidly changing threat landscape.</li>
</ul>

<h3>Title: No Modality Left Behind: Dynamic Model Generation for Incomplete Medical Data</h3>
<ul>
<li><strong>Authors: </strong>Christoph Fürböck, Paul Weiser, Branko Mitic, Philipp Seeböck, Thomas Helbich, Georg Langs</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11406">https://arxiv.org/abs/2509.11406</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11406">https://arxiv.org/pdf/2509.11406</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11406]] No Modality Left Behind: Dynamic Model Generation for Incomplete Medical Data(https://arxiv.org/abs/2509.11406)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In real world clinical environments, training and applying deep learning models on multi-modal medical imaging data often struggles with partially incomplete data. Standard approaches either discard missing samples, require imputation or repurpose dropout learning schemes, limiting robustness and generalizability. To address this, we propose a hypernetwork-based method that dynamically generates task-specific classification models conditioned on the set of available modalities. Instead of training a fixed model, a hypernetwork learns to predict the parameters of a task model adapted to available modalities, enabling training and inference on all samples, regardless of completeness. We compare this approach with (1) models trained only on complete data, (2) state of the art channel dropout methods, and (3) an imputation-based method, using artificially incomplete datasets to systematically analyze robustness to missing modalities. Results demonstrate superior adaptability of our method, outperforming state of the art approaches with an absolute increase in accuracy of up to 8% when trained on a dataset with 25% completeness (75% of training data with missing modalities). By enabling a single model to generalize across all modality configurations, our approach provides an efficient solution for real-world multi-modal medical data analysis.</li>
</ul>

<h3>Title: Long-time dynamics and universality of nonconvex gradient descent</h3>
<ul>
<li><strong>Authors: </strong>Qiyang Han</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IT, math.OC, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11426">https://arxiv.org/abs/2509.11426</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11426">https://arxiv.org/pdf/2509.11426</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11426]] Long-time dynamics and universality of nonconvex gradient descent(https://arxiv.org/abs/2509.11426)</code><input type="text"></li>
<li><strong>Keywords: </strong>data-free</a></li>
<li><strong>Abstract: </strong>This paper develops a general approach to characterize the long-time trajectory behavior of nonconvex gradient descent in generalized single-index models in the large aspect ratio regime. In this regime, we show that for each iteration the gradient descent iterate concentrates around a deterministic vector called the `Gaussian theoretical gradient descent', whose dynamics can be tracked by a state evolution system of two recursive equations for two scalars. Our concentration guarantees hold universally for a broad class of design matrices and remain valid over long time horizons until algorithmic convergence or divergence occurs. Moreover, our approach reveals that gradient descent iterates are in general approximately independent of the data and strongly incoherent with the feature vectors, a phenomenon previously known as the `implicit regularization' effect of gradient descent in specific models under Gaussian data. As an illustration of the utility of our general theory, we present two applications of different natures in the regression setting. In the first, we prove global convergence of nonconvex gradient descent with general independent initialization for a broad class of structured link functions, and establish universality of randomly initialized gradient descent in phase retrieval for large aspect ratios. In the second, we develop a data-free iterative algorithm for estimating state evolution parameters along the entire gradient descent trajectory, thereby providing a low-cost yet statistically valid tool for practical tasks such as hyperparameter tuning and runtime determination. As a by-product of our analysis, we show that in the large aspect ratio regime, the Gaussian theoretical gradient descent coincides with a recent line of dynamical mean-field theory for gradient descent over the constant-time horizon.</li>
</ul>

<h3>Title: Thunderhammer: Rowhammer Bitflips via PCIe and Thunderbolt (USB-C)</h3>
<ul>
<li><strong>Authors: </strong>Robert Dumitru, Junpeng Wan, Daniel Genkin, Rick Kennell, Dave (Jing)Tian, Yuval Yarom</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11440">https://arxiv.org/abs/2509.11440</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11440">https://arxiv.org/pdf/2509.11440</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11440]] Thunderhammer: Rowhammer Bitflips via PCIe and Thunderbolt (USB-C)(https://arxiv.org/abs/2509.11440)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>In recent years, Rowhammer has attracted significant attention from academia and industry alike. This technique, first published in 2014, flips bits in memory by repeatedly accessing neighbouring memory locations. Since its discovery, researchers have developed a substantial body of work exploiting Rowhammer and proposing countermeasures. These works demonstrate that Rowhammer can be mounted not only through native code, but also via remote code execution, such as JavaScript in browsers, and over networks. In this work, we uncover a previously unexplored Rowhammer vector. We present Thunderhammer, an attack that induces DRAM bitflips from malicious peripherals connected via PCIe or Thunderbolt (which tunnels PCIe). On modern DDR4 systems, we observe that triggering bitflips through PCIe requests requires precisely timed access patterns tailored to the target system. We design a custom device to reverse engineer critical architectural parameters that shape PCIe request scheduling, and to execute effective hammering access patterns. Leveraging this knowledge, we successfully demonstrate Rowhammer-induced bitflips in DDR4 memory modules via both PCIe slot connections and Thunderbolt ports tunnelling PCIe.</li>
</ul>

<h3>Title: MultiMAE for Brain MRIs: Robustness to Missing Inputs Using Multi-Modal Masked Autoencoder</h3>
<ul>
<li><strong>Authors: </strong>Ayhan Can Erdur, Christian Beischl, Daniel Scholz, Jiazhen Pan, Benedikt Wiestler, Daniel Rueckert, Jan C Peeken</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11442">https://arxiv.org/abs/2509.11442</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11442">https://arxiv.org/pdf/2509.11442</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11442]] MultiMAE for Brain MRIs: Robustness to Missing Inputs Using Multi-Modal Masked Autoencoder(https://arxiv.org/abs/2509.11442)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Missing input sequences are common in medical imaging data, posing a challenge for deep learning models reliant on complete input data. In this work, inspired by MultiMAE [2], we develop a masked autoencoder (MAE) paradigm for multi-modal, multi-task learning in 3D medical imaging with brain MRIs. Our method treats each MRI sequence as a separate input modality, leveraging a late-fusion-style transformer encoder to integrate multi-sequence information (multi-modal) and individual decoder streams for each modality for multi-task reconstruction. This pretraining strategy guides the model to learn rich representations per modality while also equipping it to handle missing inputs through cross-sequence reasoning. The result is a flexible and generalizable encoder for brain MRIs that infers missing sequences from available inputs and can be adapted to various downstream applications. We demonstrate the performance and robustness of our method against an MAE-ViT baseline in downstream segmentation and classification tasks, showing absolute improvement of $10.1$ overall Dice score and $0.46$ MCC over the baselines with missing input sequences. Our experiments demonstrate the strength of this pretraining strategy. The implementation is made available.</li>
</ul>

<h3>Title: A Transformer-Based Cross-Platform Analysis of Public Discourse on the 15-Minute City Paradigm</h3>
<ul>
<li><strong>Authors: </strong>Gaurab Chhetri, Darrell Anderson, Boniphace Kutela, Subasish Das</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11443">https://arxiv.org/abs/2509.11443</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11443">https://arxiv.org/pdf/2509.11443</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11443]] A Transformer-Based Cross-Platform Analysis of Public Discourse on the 15-Minute City Paradigm(https://arxiv.org/abs/2509.11443)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This study presents the first multi-platform sentiment analysis of public opinion on the 15-minute city concept across Twitter, Reddit, and news media. Using compressed transformer models and Llama-3-8B for annotation, we classify sentiment across heterogeneous text domains. Our pipeline handles long-form and short-form text, supports consistent annotation, and enables reproducible evaluation. We benchmark five models (DistilRoBERTa, DistilBERT, MiniLM, ELECTRA, TinyBERT) using stratified 5-fold cross-validation, reporting F1-score, AUC, and training time. DistilRoBERTa achieved the highest F1 (0.8292), TinyBERT the best efficiency, and MiniLM the best cross-platform consistency. Results show News data yields inflated performance due to class imbalance, Reddit suffers from summarization loss, and Twitter offers moderate challenge. Compressed models perform competitively, challenging assumptions that larger models are necessary. We identify platform-specific trade-offs and propose directions for scalable, real-world sentiment classification in urban planning discourse.</li>
</ul>

<h3>Title: CognitiveSky: Scalable Sentiment and Narrative Analysis for Decentralized Social Media</h3>
<ul>
<li><strong>Authors: </strong>Gaurab Chhetri, Anandi Dutta, Subasish Das</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11444">https://arxiv.org/abs/2509.11444</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11444">https://arxiv.org/pdf/2509.11444</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11444]] CognitiveSky: Scalable Sentiment and Narrative Analysis for Decentralized Social Media(https://arxiv.org/abs/2509.11444)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate, transformer, large language model</a></li>
<li><strong>Abstract: </strong>The emergence of decentralized social media platforms presents new opportunities and challenges for real-time analysis of public discourse. This study introduces CognitiveSky, an open-source and scalable framework designed for sentiment, emotion, and narrative analysis on Bluesky, a federated Twitter or this http URL alternative. By ingesting data through Bluesky's Application Programming Interface (API), CognitiveSky applies transformer-based models to annotate large-scale user-generated content and produces structured and analyzable outputs. These summaries drive a dynamic dashboard that visualizes evolving patterns in emotion, activity, and conversation topics. Built entirely on free-tier infrastructure, CognitiveSky achieves both low operational cost and high accessibility. While demonstrated here for monitoring mental health discourse, its modular design enables applications across domains such as disinformation detection, crisis response, and civic sentiment analysis. By bridging large language models with decentralized networks, CognitiveSky offers a transparent, extensible tool for computational social science in an era of shifting digital ecosystems.</li>
</ul>

<h3>Title: Tabular Data with Class Imbalance: Predicting Electric Vehicle Crash Severity with Pretrained Transformers (TabPFN) and Mamba-Based Models</h3>
<ul>
<li><strong>Authors: </strong>Shriyank Somvanshi, Pavan Hebli, Gaurab Chhetri, Subasish Das</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11449">https://arxiv.org/abs/2509.11449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11449">https://arxiv.org/pdf/2509.11449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11449]] Tabular Data with Class Imbalance: Predicting Electric Vehicle Crash Severity with Pretrained Transformers (TabPFN) and Mamba-Based Models(https://arxiv.org/abs/2509.11449)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This study presents a deep tabular learning framework for predicting crash severity in electric vehicle (EV) collisions using real-world crash data from Texas (2017-2023). After filtering for electric-only vehicles, 23,301 EV-involved crash records were analyzed. Feature importance techniques using XGBoost and Random Forest identified intersection relation, first harmful event, person age, crash speed limit, and day of week as the top predictors, along with advanced safety features like automatic emergency braking. To address class imbalance, Synthetic Minority Over-sampling Technique and Edited Nearest Neighbors (SMOTEENN) resampling was applied. Three state-of-the-art deep tabular models, TabPFN, MambaNet, and MambaAttention, were benchmarked for severity prediction. While TabPFN demonstrated strong generalization, MambaAttention achieved superior performance in classifying severe injury cases due to its attention-based feature reweighting. The findings highlight the potential of deep tabular architectures for improving crash severity prediction and enabling data-driven safety interventions in EV crash contexts.</li>
</ul>

<h3>Title: MAUI: Reconstructing Private Client Data in Federated Transfer Learning</h3>
<ul>
<li><strong>Authors: </strong>Ahaan Dabholkar, Atul Sharma, Z. Berkay Celik, Saurabh Bagchi</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11451">https://arxiv.org/abs/2509.11451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11451">https://arxiv.org/pdf/2509.11451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11451]] MAUI: Reconstructing Private Client Data in Federated Transfer Learning(https://arxiv.org/abs/2509.11451)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, steal, federate, transformer</a></li>
<li><strong>Abstract: </strong>Recent works in federated learning (FL) have shown the utility of leveraging transfer learning for balancing the benefits of FL and centralized learning. In this setting, federated training happens after a stable point has been reached through conventional training. Global model weights are first centrally pretrained by the server on a public dataset following which only the last few linear layers (the classification head) of the model are finetuned across clients. In this scenario, existing data reconstruction attacks (DRAs) in FL show two key weaknesses. First, strongly input-correlated gradient information from the initial model layers is never shared, significantly degrading reconstruction accuracy. Second, DRAs in which the server makes highly specific, handcrafted manipulations to the model structure or parameters (for e.g., layers with all zero weights, identity mappings and rows with identical weight patterns) are easily detectable by an active client. Improving on these, we propose MAUI, a stealthy DRA that does not require any overt manipulations to the model architecture or weights, and relies solely on the gradients of the classification head. MAUI first extracts "robust" feature representations of the input batch from the gradients of the classification head and subsequently inverts these representations to the original inputs. We report highly accurate reconstructions on the CIFAR10 and ImageNet datasets on a variety of model architectures including convolution networks (CNN, VGG11), ResNets (18, 50), ShuffleNet-V2 and Vision Transformer (ViT B-32), regardless of the batch size. MAUI significantly outperforms prior DRAs in reconstruction quality, achieving 40-120% higher PSNR scores.</li>
</ul>

<h3>Title: Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting</h3>
<ul>
<li><strong>Authors: </strong>Yining Lu, Zilong Wang, Shiyang Li, Xin Liu, Changlong Yu, Qingyu Yin, Zhan Shi, Zixuan Zhang, Meng Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11452">https://arxiv.org/abs/2509.11452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11452">https://arxiv.org/pdf/2509.11452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11452]] Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting(https://arxiv.org/abs/2509.11452)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Prior works in multi-objective reinforcement learning typically use linear reward scalarization with fixed weights, which provably fail to capture non-convex Pareto fronts and thus yield suboptimal results. This limitation becomes especially critical in online preference alignment for large language models. Here, stochastic trajectories generated by parameterized policies create highly non-linear and non-convex mappings from parameters to objectives that no single static weighting scheme can find optimal trade-offs. We address this limitation by introducing dynamic reward weighting, which adaptively adjusts reward weights during the online reinforcement learning process. Unlike existing approaches that rely on fixed-weight interpolation, our dynamic weighting continuously balances and prioritizes objectives in training, facilitating effective exploration of Pareto fronts in objective space. We introduce two approaches of increasing sophistication and generalizability: (1) hypervolume-guided weight adaptation and (2) gradient-based weight optimization, offering a versatile toolkit for online multi-objective alignment. Our extensive experiments demonstrate their compatibility with commonly used online reinforcement learning algorithms (including GRPO, REINFORCE, and RLOO), effectiveness across multiple mathematical reasoning datasets, and applicability to different model families, consistently achieving Pareto dominant solutions with fewer training steps than fixed-weight linear scalarization baselines.</li>
</ul>

<h3>Title: Beyond Frame-wise Tracking: A Trajectory-based Paradigm for Efficient Point Cloud Tracking</h3>
<ul>
<li><strong>Authors: </strong>BaiChen Fan, Sifan Zhou, Jian Li, Shibo Zhao, Muqing Cao, Qin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11453">https://arxiv.org/abs/2509.11453</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11453">https://arxiv.org/pdf/2509.11453</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11453]] Beyond Frame-wise Tracking: A Trajectory-based Paradigm for Efficient Point Cloud Tracking(https://arxiv.org/abs/2509.11453)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>LiDAR-based 3D single object tracking (3D SOT) is a critical task in robotics and autonomous systems. Existing methods typically follow frame-wise motion estimation or a sequence-based paradigm. However, the two-frame methods are efficient but lack long-term temporal context, making them vulnerable in sparse or occluded scenes, while sequence-based methods that process multiple point clouds gain robustness at a significant computational cost. To resolve this dilemma, we propose a novel trajectory-based paradigm and its instantiation, TrajTrack. TrajTrack is a lightweight framework that enhances a base two-frame tracker by implicitly learning motion continuity from historical bounding box trajectories alone-without requiring additional, costly point cloud inputs. It first generates a fast, explicit motion proposal and then uses an implicit motion modeling module to predict the future trajectory, which in turn refines and corrects the initial proposal. Extensive experiments on the large-scale NuScenes benchmark show that TrajTrack achieves new state-of-the-art performance, dramatically improving tracking precision by 4.48% over a strong baseline while running at 56 FPS. Besides, we also demonstrate the strong generalizability of TrajTrack across different base trackers. Video is available at this https URL.</li>
</ul>

<h3>Title: CEMTM: Contextual Embedding-based Multimodal Topic Modeling</h3>
<ul>
<li><strong>Authors: </strong>Amirhossein Abaskohi, Raymond Li, Chuyuan Li, Shafiq Joty, Giuseppe Carenini</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11465">https://arxiv.org/abs/2509.11465</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11465">https://arxiv.org/pdf/2509.11465</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11465]] CEMTM: Contextual Embedding-based Multimodal Topic Modeling(https://arxiv.org/abs/2509.11465)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>We introduce CEMTM, a context-enhanced multimodal topic model designed to infer coherent and interpretable topic structures from both short and long documents containing text and images. CEMTM builds on fine-tuned large vision language models (LVLMs) to obtain contextualized embeddings, and employs a distributional attention mechanism to weight token-level contributions to topic inference. A reconstruction objective aligns topic-based representations with the document embedding, encouraging semantic consistency across modalities. Unlike existing approaches, CEMTM can process multiple images per document without repeated encoding and maintains interpretability through explicit word-topic and document-topic distributions. Extensive experiments on six multimodal benchmarks show that CEMTM consistently outperforms unimodal and multimodal baselines, achieving a remarkable average LLM score of 2.61. Further analysis shows its effectiveness in downstream few-shot retrieval and its ability to capture visually grounded semantics in complex domains such as scientific articles.</li>
</ul>

<h3>Title: Improving LLMs' Learning for Coreference Resolution</h3>
<ul>
<li><strong>Authors: </strong>Yujian Gan, Yuan Liang, Yanni Lin, Juntao Yu, Massimo Poesio</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11466">https://arxiv.org/abs/2509.11466</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11466">https://arxiv.org/pdf/2509.11466</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11466]] Improving LLMs' Learning for Coreference Resolution(https://arxiv.org/abs/2509.11466)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Coreference Resolution (CR) is crucial for many NLP tasks, but existing LLMs struggle with hallucination and under-performance. In this paper, we investigate the limitations of existing LLM-based approaches to CR-specifically the Question-Answering (QA) Template and Document Template methods and propose two novel techniques: Reversed Training with Joint Inference and Iterative Document Generation. Our experiments show that Reversed Training improves the QA Template method, while Iterative Document Generation eliminates hallucinations in the generated source text and boosts coreference resolution. Integrating these methods and techniques offers an effective and robust solution to LLM-based coreference resolution.</li>
</ul>

<h3>Title: Modality-Aware Infrared and Visible Image Fusion with Target-Aware Supervision</h3>
<ul>
<li><strong>Authors: </strong>Tianyao Sun, Dawei Xiang, Tianqi Ding, Xiang Fang, Yijiashun Qi, Zunduo Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11476">https://arxiv.org/abs/2509.11476</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11476">https://arxiv.org/pdf/2509.11476</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11476]] Modality-Aware Infrared and Visible Image Fusion with Target-Aware Supervision(https://arxiv.org/abs/2509.11476)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Infrared and visible image fusion (IVIF) is a fundamental task in multi-modal perception that aims to integrate complementary structural and textural cues from different spectral domains. In this paper, we propose FusionNet, a novel end-to-end fusion framework that explicitly models inter-modality interaction and enhances task-critical regions. FusionNet introduces a modality-aware attention mechanism that dynamically adjusts the contribution of infrared and visible features based on their discriminative capacity. To achieve fine-grained, interpretable fusion, we further incorporate a pixel-wise alpha blending module, which learns spatially-varying fusion weights in an adaptive and content-aware manner. Moreover, we formulate a target-aware loss that leverages weak ROI supervision to preserve semantic consistency in regions containing important objects (e.g., pedestrians, vehicles). Experiments on the public M3FD dataset demonstrate that FusionNet generates fused images with enhanced semantic preservation, high perceptual quality, and clear interpretability. Our framework provides a general and extensible solution for semantic-aware multi-modal image fusion, with benefits for downstream tasks such as object detection and scene understanding.</li>
</ul>

<h3>Title: ClaimIQ at CheckThat! 2025: Comparing Prompted and Fine-Tuned Language Models for Verifying Numerical Claims</h3>
<ul>
<li><strong>Authors: </strong>Anirban Saha Anik, Md Fahimul Kabir Chowdhury, Andrew Wyckoff, Sagnik Ray Choudhury</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11492">https://arxiv.org/abs/2509.11492</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11492">https://arxiv.org/pdf/2509.11492</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11492]] ClaimIQ at CheckThat! 2025: Comparing Prompted and Fine-Tuned Language Models for Verifying Numerical Claims(https://arxiv.org/abs/2509.11492)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>This paper presents our system for Task 3 of the CLEF 2025 CheckThat! Lab, which focuses on verifying numerical and temporal claims using retrieved evidence. We explore two complementary approaches: zero-shot prompting with instruction-tuned large language models (LLMs) and supervised fine-tuning using parameter-efficient LoRA. To enhance evidence quality, we investigate several selection strategies, including full-document input and top-k sentence filtering using BM25 and MiniLM. Our best-performing model LLaMA fine-tuned with LoRA achieves strong performance on the English validation set. However, a notable drop in the test set highlights a generalization challenge. These findings underscore the importance of evidence granularity and model adaptation for robust numerical fact verification.</li>
</ul>

<h3>Title: AKCIT-FN at CheckThat! 2025: Switching Fine-Tuned SLMs and LLM Prompting for Multilingual Claim Normalization</h3>
<ul>
<li><strong>Authors: </strong>Fabrycio Leite Nakano Almada, Kauan Divino Pouso Mariano, Maykon Adriell Dutra, Victor Emanuel da Silva Monteiro, Juliana Resplande Sant'Anna Gomes, Arlindo Rodrigues Galvão Filho, Anderson da Silva Soares</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11496">https://arxiv.org/abs/2509.11496</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11496">https://arxiv.org/pdf/2509.11496</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11496]] AKCIT-FN at CheckThat! 2025: Switching Fine-Tuned SLMs and LLM Prompting for Multilingual Claim Normalization(https://arxiv.org/abs/2509.11496)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Claim normalization, the transformation of informal social media posts into concise, self-contained statements, is a crucial step in automated fact-checking pipelines. This paper details our submission to the CLEF-2025 CheckThat! Task~2, which challenges systems to perform claim normalization across twenty languages, divided into thirteen supervised (high-resource) and seven zero-shot (no training data) tracks. Our approach, leveraging fine-tuned Small Language Models (SLMs) for supervised languages and Large Language Model (LLM) prompting for zero-shot scenarios, achieved podium positions (top three) in fifteen of the twenty languages. Notably, this included second-place rankings in eight languages, five of which were among the seven designated zero-shot languages, underscoring the effectiveness of our LLM-based zero-shot strategy. For Portuguese, our initial development language, our system achieved an average METEOR score of 0.5290, ranking third. All implementation artifacts, including inference, training, evaluation scripts, and prompt configurations, are publicly available at this https URL.</li>
</ul>

<h3>Title: PeruMedQA: Benchmarking Large Language Models (LLMs) on Peruvian Medical Exams - Dataset Construction and Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Rodrigo M. Carrillo-Larco, Jesus Lovón Melgarejo, Manuel Castillo-Cara, Gusseppe Bravo-Rocca</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11517">https://arxiv.org/abs/2509.11517</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11517">https://arxiv.org/pdf/2509.11517</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11517]] PeruMedQA: Benchmarking Large Language Models (LLMs) on Peruvian Medical Exams - Dataset Construction and Evaluation(https://arxiv.org/abs/2509.11517)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>BACKGROUND: Medical large language models (LLMS) have demonstrated remarkable performance in answering medical examinations. However, the extent to which this high performance is transferable to medical questions in Spanish and from a Latin American country remains unexplored. This knowledge is crucial as LLM-based medical applications gain traction in Latin America. AIMS: to build a dataset of questions from medical examinations taken by Peruvian physicians pursuing specialty training; to fine-tune a LLM on this dataset; to evaluate and compare the performance in terms of accuracy between vanilla LLMs and the fine-tuned LLM. METHODS: We curated PeruMedQA, a multiple-choice question-answering (MCQA) datasets containing 8,380 questions spanning 12 medical domains (2018-2025). We selected eight medical LLMs including medgemma-4b-it and medgemma-27b-text-it, and developed zero-shot task-specific prompts to answer the questions appropriately. We employed parameter-efficient fine tuning (PEFT)and low-rant adaptation (LoRA) to fine-tune medgemma-4b-it utilizing all questions except those from 2025 (test set). RESULTS: medgemma-27b-text-it outperformed all other models, achieving a proportion of correct answers exceeding 90% in several instances. LLMs with <10 billion parameters exhibited <60% of correct answers, while some exams yielded results <50%. The fine-tuned version of medgemma-4b-it emerged victorious agains all LLMs with <10 billion parameters and rivaled a LLM with 70 billion parameters across various examinations. CONCLUSIONS: For medical AI application and research that require knowledge bases from Spanish-speaking countries and those exhibiting similar epidemiological profiles to Peru's, interested parties should utilize medgemma-27b-text-it or a fine-tuned version of medgemma-4b-it.</li>
</ul>

<h3>Title: DARD: Dice Adversarial Robustness Distillation against Adversarial Attacks</h3>
<ul>
<li><strong>Authors: </strong>Jing Zou, Shungeng Zhang, Meikang Qiu, Chong Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11525">https://arxiv.org/abs/2509.11525</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11525">https://arxiv.org/pdf/2509.11525</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11525]] DARD: Dice Adversarial Robustness Distillation against Adversarial Attacks(https://arxiv.org/abs/2509.11525)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Deep learning models are vulnerable to adversarial exam- ples, posing critical security challenges in real-world applications. While Adversarial Training (AT ) is a widely adopted defense mechanism to enhance robustness, it often incurs a trade-off by degrading performance on unperturbed, natural data. Recent efforts have highlighted that larger models exhibit enhanced robustness over their smaller counterparts. In this paper, we empirically demonstrate that such robustness can be sys- tematically distilled from large teacher models into compact student models. To achieve better performance, we introduce Dice Adversarial Robustness Distillation (DARD), a novel method designed to transfer robustness through a tailored knowledge distillation paradigm. Addition- ally, we propose Dice Projected Gradient Descent (DPGD), an adversar- ial example generalization method optimized for effective attack. Our ex- tensive experiments demonstrate that the DARD approach consistently outperforms adversarially trained networks with the same architecture, achieving superior robustness and standard accuracy.</li>
</ul>

<h3>Title: On the Distinctive Co-occurrence Characteristics of Antonymy</h3>
<ul>
<li><strong>Authors: </strong>Zhihan Cao, Hiroaki Yamada, Takenobu Tokunaga</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11534">https://arxiv.org/abs/2509.11534</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11534">https://arxiv.org/pdf/2509.11534</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11534]] On the Distinctive Co-occurrence Characteristics of Antonymy(https://arxiv.org/abs/2509.11534)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Antonymy has long received particular attention in lexical semantics. Previous studies have shown that antonym pairs frequently co-occur in text, across genres and parts of speech, more often than would be expected by chance. However, whether this co-occurrence pattern is distinctive of antonymy remains unclear, due to a lack of comparison with other semantic relations. This work fills the gap by comparing antonymy with three other relations across parts of speech using robust co-occurrence metrics. We find that antonymy is distinctive in three respects: antonym pairs co-occur with high strength, in a preferred linear order, and within short spans. All results are available online.</li>
</ul>

<h3>Title: HARP: Hallucination Detection via Reasoning Subspace Projection</h3>
<ul>
<li><strong>Authors: </strong>Junjie Hu, Gang Tu, ShengYu Cheng, Jinxin Li, Jinting Wang, Rui Chen, Zhilong Zhou, Dongbo Shan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11536">https://arxiv.org/abs/2509.11536</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11536">https://arxiv.org/pdf/2509.11536</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11536]] HARP: Hallucination Detection via Reasoning Subspace Projection(https://arxiv.org/abs/2509.11536)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Hallucinations in Large Language Models (LLMs) pose a major barrier to their reliable use in critical decision-making. Although existing hallucination detection methods have improved accuracy, they still struggle with disentangling semantic and reasoning information and maintaining robustness. To address these challenges, we propose HARP (Hallucination detection via reasoning subspace projection), a novel hallucination detection framework. HARP establishes that the hidden state space of LLMs can be decomposed into a direct sum of a semantic subspace and a reasoning subspace, where the former encodes linguistic expression and the latter captures internal reasoning processes. Moreover, we demonstrate that the Unembedding layer can disentangle these subspaces, and by applying Singular Value Decomposition (SVD) to its parameters, the basis vectors spanning the semantic and reasoning subspaces are obtained. Finally, HARP projects hidden states onto the basis vectors of the reasoning subspace, and the resulting projections are then used as input features for hallucination detection in LLMs. By using these projections, HARP reduces the dimension of the feature to approximately 5% of the original, filters out most noise, and achieves enhanced robustness. Experiments across multiple datasets show that HARP achieves state-of-the-art hallucination detection performance; in particular, it achieves an AUROC of 92.8% on TriviaQA, outperforming the previous best method by 7.5%.</li>
</ul>

<h3>Title: Dstack: A Zero Trust Framework for Confidential Containers</h3>
<ul>
<li><strong>Authors: </strong>Shunfan Zhou, Kevin Wang, Hang Yin</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11555">https://arxiv.org/abs/2509.11555</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11555">https://arxiv.org/pdf/2509.11555</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11555]] Dstack: A Zero Trust Framework for Confidential Containers(https://arxiv.org/abs/2509.11555)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>Web3 applications require execution platforms that maintain confidentiality and integrity without relying on centralized trust authorities. While Trusted Execution Environments (TEEs) offer promising capabilities for confidential computing, current implementations face significant limitations when applied to Web3 contexts, particularly in security reliability, censorship resistance, and vendor independence. This paper presents dstack, a comprehensive framework that transforms raw TEE technology into a true Zero Trust platform. We introduce three key innovations: (1) Portable Confidential Containers that enable seamless workload migration across heterogeneous TEE environments while maintaining security guarantees, (2) Decentralized Code Management that leverages smart contracts for transparent governance of TEE applications, and (3) Verifiable Domain Management that ensures secure and verifiable application identity without centralized authorities. These innovations are implemented through three core components: dstack-OS, dstack-KMS, and dstack-Gateway. Together, they demonstrate how to achieve both the performance advantages of VM-level TEE solutions and the trustless guarantees required by Web3 applications. Our evaluation shows that dstack provides comprehensive security guarantees while maintaining practical usability for real-world applications.</li>
</ul>

<h3>Title: D$^2$HScore: Reasoning-Aware Hallucination Detection via Semantic Breadth and Depth Analysis in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Yue Ding, Xiaofang Zhu, Tianze Xia, Junfei Wu, Xinlong Chen, Qiang Liu, Liang Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11569">https://arxiv.org/abs/2509.11569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11569">https://arxiv.org/pdf/2509.11569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11569]] D$^2$HScore: Reasoning-Aware Hallucination Detection via Semantic Breadth and Depth Analysis in LLMs(https://arxiv.org/abs/2509.11569)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Although large Language Models (LLMs) have achieved remarkable success, their practical application is often hindered by the generation of non-factual content, which is called "hallucination". Ensuring the reliability of LLMs' outputs is a critical challenge, particularly in high-stakes domains such as finance, security, and healthcare. In this work, we revisit hallucination detection from the perspective of model architecture and generation dynamics. Leveraging the multi-layer structure and autoregressive decoding process of LLMs, we decompose hallucination signals into two complementary dimensions: the semantic breadth of token representations within each layer, and the semantic depth of core concepts as they evolve across layers. Based on this insight, we propose \textbf{D$^2$HScore (Dispersion and Drift-based Hallucination Score)}, a training-free and label-free framework that jointly measures: (1) \textbf{Intra-Layer Dispersion}, which quantifies the semantic diversity of token representations within each layer; and (2) \textbf{Inter-Layer Drift}, which tracks the progressive transformation of key token representations across layers. To ensure drift reflects the evolution of meaningful semantics rather than noisy or redundant tokens, we guide token selection using attention signals. By capturing both the horizontal and vertical dynamics of representation during inference, D$^2$HScore provides an interpretable and lightweight proxy for hallucination detection. Extensive experiments across five open-source LLMs and five widely used benchmarks demonstrate that D$^2$HScore consistently outperforms existing training-free baselines.</li>
</ul>

<h3>Title: Bhaasha, Bhasa, Zaban: A Survey for Low-Resourced Languages in South Asia - Current Stage and Challenges</h3>
<ul>
<li><strong>Authors: </strong>Sampoorna Poria, Xiaolei Huang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11570">https://arxiv.org/abs/2509.11570</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11570">https://arxiv.org/pdf/2509.11570</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11570]] Bhaasha, Bhasa, Zaban: A Survey for Low-Resourced Languages in South Asia - Current Stage and Challenges(https://arxiv.org/abs/2509.11570)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Rapid developments of large language models have revolutionized many NLP tasks for English data. Unfortunately, the models and their evaluations for low-resource languages are being overlooked, especially for languages in South Asia. Although there are more than 650 languages in South Asia, many of them either have very limited computational resources or are missing from existing language models. Thus, a concrete question to be answered is: Can we assess the current stage and challenges to inform our NLP community and facilitate model developments for South Asian languages? In this survey, we have comprehensively examined current efforts and challenges of NLP models for South Asian languages by retrieving studies since 2020, with a focus on transformer-based models, such as BERT, T5, & GPT. We present advances and gaps across 3 essential aspects: data, models, & tasks, such as available data sources, fine-tuning strategies, & domain applications. Our findings highlight substantial issues, including missing data in critical domains (e.g., health), code-mixing, and lack of standardized evaluation benchmarks. Our survey aims to raise awareness within the NLP community for more targeted data curation, unify benchmarks tailored to cultural and linguistic nuances of South Asia, and encourage an equitable representation of South Asian languages. The complete list of resources is available at: this https URL.</li>
</ul>

<h3>Title: MVQA-68K: A Multi-dimensional and Causally-annotated Dataset with Quality Interpretability for Video Assessment</h3>
<ul>
<li><strong>Authors: </strong>Yanyun Pu, Kehan Li, Zeyi Huang, Zhijie Zhong, Kaixiang Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11589">https://arxiv.org/abs/2509.11589</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11589">https://arxiv.org/pdf/2509.11589</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11589]] MVQA-68K: A Multi-dimensional and Causally-annotated Dataset with Quality Interpretability for Video Assessment(https://arxiv.org/abs/2509.11589)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of video generation models such as Sora, video quality assessment (VQA) is becoming increasingly crucial for selecting high-quality videos from large-scale datasets used in pre-training. Traditional VQA methods, typically producing single numerical scores, often lack comprehensiveness and interpretability. To address these challenges, we introduce MVQA-68K, a novel multi-dimensional VQA dataset comprising over 68,000 carefully annotated videos, covering seven essential quality dimensions: overall aesthetics, camera movement, dynamic degree, texture detail, composition, visual quality, and factual consistency. Each annotation includes detailed chain-of-thought reasoning to facilitate interpretability and comprehensive understanding. Extensive experiments demonstrate that MVQA-68K significantly enhances the performance of various multimodal large language models (MLLMs) on the VQA task, achieving state-of-the-art results not only on our internal test set (Fig.1) but also on public benchmarks including LSVQ-test, LSVQ-1080p, and LIVE-VQC. Meantime, incorporating explicit reasoning process during VQA training substantially boosts the zero-shot generalization. Code and dataset will be available at github: this https URL</li>
</ul>

<h3>Title: Analyzing Information-Seeking Behaviors in a Hakka AI Chatbot: A Cognitive-Pragmatic Study</h3>
<ul>
<li><strong>Authors: </strong>Chu-Hsuan Lee, Chen-Chi Chang, Hung-Shin Lee, Yun-Hsiang Hsu, Ching-Yuan Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11591">https://arxiv.org/abs/2509.11591</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11591">https://arxiv.org/pdf/2509.11591</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11591]] Analyzing Information-Seeking Behaviors in a Hakka AI Chatbot: A Cognitive-Pragmatic Study(https://arxiv.org/abs/2509.11591)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>With many endangered languages at risk of disappearing, efforts to preserve them now rely more than ever on using technology alongside culturally informed teaching strategies. This study examines user behaviors in TALKA, a generative AI-powered chatbot designed for Hakka language engagement, by employing a dual-layered analytical framework grounded in Bloom's Taxonomy of cognitive processes and dialogue act categorization. We analyzed 7,077 user utterances, each carefully annotated according to six cognitive levels and eleven dialogue act types. These included a variety of functions, such as asking for information, requesting translations, making cultural inquiries, and using language creatively. Pragmatic classifications further highlight how different types of dialogue acts--such as feedback, control commands, and social greetings--align with specific cognitive intentions. The results suggest that generative AI chatbots can support language learning in meaningful ways--especially when they are designed with an understanding of how users think and communicate. They may also help learners express themselves more confidently and connect with their cultural identity. The TALKA case provides empirical insights into how AI-mediated dialogue facilitates cognitive development in low-resource language learners, as well as pragmatic negotiation and socio-cultural affiliation. By focusing on AI-assisted language learning, this study offers new insights into how technology can support language preservation and educational practice.</li>
</ul>

<h3>Title: Disentangling Content from Style to Overcome Shortcut Learning: A Hybrid Generative-Discriminative Learning Framework</h3>
<ul>
<li><strong>Authors: </strong>Siming Fu, Sijun Dong, Xiaoliang Meng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11598">https://arxiv.org/abs/2509.11598</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11598">https://arxiv.org/pdf/2509.11598</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11598]] Disentangling Content from Style to Overcome Shortcut Learning: A Hybrid Generative-Discriminative Learning Framework(https://arxiv.org/abs/2509.11598)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Despite the remarkable success of Self-Supervised Learning (SSL), its generalization is fundamentally hindered by Shortcut Learning, where models exploit superficial features like texture instead of intrinsic structure. We experimentally verify this flaw within the generative paradigm (e.g., MAE) and argue it is a systemic issue also affecting discriminative methods, identifying it as the root cause of their failure on unseen domains. While existing methods often tackle this at a surface level by aligning or separating domain-specific features, they fail to alter the underlying learning mechanism that fosters shortcut dependency. To address this at its core, we propose HyGDL (Hybrid Generative-Discriminative Learning Framework), a hybrid framework that achieves explicit content-style disentanglement. Our approach is guided by the Invariance Pre-training Principle: forcing a model to learn an invariant essence by systematically varying a bias (e.g., style) at the input while keeping the supervision signal constant. HyGDL operates on a single encoder and analytically defines style as the component of a representation that is orthogonal to its style-invariant content, derived via vector projection.</li>
</ul>

<h3>Title: Dynamic Adaptive Parsing of Temporal and Cross-Variable Patterns for Network State Classification</h3>
<ul>
<li><strong>Authors: </strong>Yuan Gao, Xuelong Wang, Zhenguo Dong, Yong Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11601">https://arxiv.org/abs/2509.11601</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11601">https://arxiv.org/pdf/2509.11601</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11601]] Dynamic Adaptive Parsing of Temporal and Cross-Variable Patterns for Network State Classification(https://arxiv.org/abs/2509.11601)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, extraction</a></li>
<li><strong>Abstract: </strong>Effective network state classification is a primary task for ensuring network security and optimizing performance. Existing deep learning models have shown considerable progress in this area. Some methods excel at analyzing the complex temporal periodicities found in traffic data, while graph-based approaches are adept at modeling the dynamic dependencies between different variables. However, a key trade-off remains, as these methods struggle to capture both characteristics simultaneously. Models focused on temporal patterns often overlook crucial variable dependencies, whereas those centered on dependencies may fail to capture fine-grained temporal details. To address this trade-off, we introduce DAPNet, a framework based on a Mixture-of-Experts architecture. DAPNet integrates three specialized networks for periodic analysis, dynamic cross-variable correlation modeling, and hybrid temporal feature extraction. A learnable gating network dynamically assigns weights to experts based on the input sample and computes a weighted fusion of their outputs. Furthermore, a hybrid regularization loss function ensures stable training and addresses the common issue of class imbalance. Extensive experiments on two large-scale network intrusion detection datasets (CICIDS2017/2018) validate DAPNet's higher accuracy for its target application. The generalizability of the architectural design is evaluated across ten public UEA benchmark datasets, positioning DAPNet as a specialized framework for network state classification.</li>
</ul>

<h3>Title: Dynamic Span Interaction and Graph-Aware Memory for Entity-Level Sentiment Classification</h3>
<ul>
<li><strong>Authors: </strong>Md. Mithun Hossain, Sanjara, Md. Shakil Hossain, Sudipto Chaki</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11604">https://arxiv.org/abs/2509.11604</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11604">https://arxiv.org/pdf/2509.11604</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11604]] Dynamic Span Interaction and Graph-Aware Memory for Entity-Level Sentiment Classification(https://arxiv.org/abs/2509.11604)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Entity-level sentiment classification involves identifying the sentiment polarity linked to specific entities within text. This task poses several challenges: effectively modeling the subtle and complex interactions between entities and their surrounding sentiment expressions; capturing dependencies that may span across sentences; and ensuring consistent sentiment predictions for multiple mentions of the same entity through coreference resolution. Additionally, linguistic phenomena such as negation, ambiguity, and overlapping opinions further complicate the analysis. These complexities make entity-level sentiment classification a difficult problem, especially in real-world, noisy textual data. To address these issues, we propose SpanEIT, a novel framework integrating dynamic span interaction and graph-aware memory mechanisms for enhanced entity-sentiment relational modeling. SpanEIT builds span-based representations for entities and candidate sentiment phrases, employs bidirectional attention for fine-grained interactions, and uses a graph attention network to capture syntactic and co-occurrence relations. A coreference-aware memory module ensures entity-level consistency across documents. Experiments on FSAD, BARU, and IMDB datasets show SpanEIT outperforms state-of-the-art transformer and hybrid baselines in accuracy and F1 scores. Ablation and interpretability analyses validate the effectiveness of our approach, underscoring its potential for fine-grained sentiment analysis in applications like social media monitoring and customer feedback analysis.</li>
</ul>

<h3>Title: Cyber Threat Hunting: Non-Parametric Mining of Attack Patterns from Cyber Threat Intelligence for Precise Threats Attribution</h3>
<ul>
<li><strong>Authors: </strong>Rimsha Kanwal, Umara Noor, Zafar Iqbal, Zahid Rashid</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11615">https://arxiv.org/abs/2509.11615</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11615">https://arxiv.org/pdf/2509.11615</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11615]] Cyber Threat Hunting: Non-Parametric Mining of Attack Patterns from Cyber Threat Intelligence for Precise Threats Attribution(https://arxiv.org/abs/2509.11615)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>With the ever-changing landscape of cyber threats, identifying their origin has become paramount, surpassing the simple task of attack classification. Cyber threat attribution gives security analysts the insights they need to device effective threat mitigation strategies. Such strategies empower enterprises to proactively detect and defend against future cyber-attacks. However, existing approaches exhibit limitations in accurately identifying threat actors, leading to low precision and a significant occurrence of false positives. Machine learning offers the potential to automate certain aspects of cyber threat attribution. The distributed nature of information regarding cyber threat actors and their intricate attack methodologies has hindered substantial progress in this domain. Cybersecurity analysts deal with an ever-expanding collection of cyber threat intelligence documents. While these documents hold valuable insights, their sheer volume challenges efficient organization and retrieval of pertinent information. To assist the cybersecurity analyst activities, we propose a machine learning based approach featuring visually interactive analytics tool named the Cyber-Attack Pattern Explorer (CAPE), designed to facilitate efficient information discovery by employing interactive visualization and mining techniques. In the proposed system, a non-parametric mining technique is proposed to create a dataset for identifying the attack patterns within cyber threat intelligence documents. These attack patterns align semantically with commonly employed themes ensuring ease of interpretation. The extracted dataset is used for training of proposed machine learning algorithms that enables the attribution of cyber threats with respective to the actors.</li>
</ul>

<h3>Title: HalluDetect: Detecting, Mitigating, and Benchmarking Hallucinations in Conversational Systems</h3>
<ul>
<li><strong>Authors: </strong>Spandan Anaokar, Shrey Ganatra, Harshvivek Kashid, Swapnil Bhattacharyya, Shruti Nair, Reshma Sekhar, Siddharth Manohar, Rahul Hemrajani, Pushpak Bhattacharyya</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11619">https://arxiv.org/abs/2509.11619</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11619">https://arxiv.org/pdf/2509.11619</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11619]] HalluDetect: Detecting, Mitigating, and Benchmarking Hallucinations in Conversational Systems(https://arxiv.org/abs/2509.11619)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are widely used in industry but remain prone to hallucinations, limiting their reliability in critical applications. This work addresses hallucination reduction in consumer grievance chatbots built using LLaMA 3.1 8B Instruct, a compact model frequently used in industry. We develop HalluDetect, an LLM-based hallucination detection system that achieves an F1 score of 69% outperforming baseline detectors by 25.44%. Benchmarking five chatbot architectures, we find that out of them, AgentBot minimizes hallucinations to 0.4159 per turn while maintaining the highest token accuracy (96.13%), making it the most effective mitigation strategy. Our findings provide a scalable framework for hallucination mitigation, demonstrating that optimized inference strategies can significantly improve factual accuracy. While applied to consumer law, our approach generalizes to other high-risk domains, enhancing trust in LLM-driven assistants. We will release the code and dataset</li>
</ul>

<h3>Title: AesBiasBench: Evaluating Bias and Alignment in Multimodal Language Models for Personalized Image Aesthetic Assessment</h3>
<ul>
<li><strong>Authors: </strong>Kun Li, Lai-Man Po, Hongzheng Yang, Xuyuan Xu, Kangcheng Liu, Yuzhi Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11620">https://arxiv.org/abs/2509.11620</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11620">https://arxiv.org/pdf/2509.11620</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11620]] AesBiasBench: Evaluating Bias and Alignment in Multimodal Language Models for Personalized Image Aesthetic Assessment(https://arxiv.org/abs/2509.11620)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) are increasingly applied in Personalized Image Aesthetic Assessment (PIAA) as a scalable alternative to expert evaluations. However, their predictions may reflect subtle biases influenced by demographic factors such as gender, age, and education. In this work, we propose AesBiasBench, a benchmark designed to evaluate MLLMs along two complementary dimensions: (1) stereotype bias, quantified by measuring variations in aesthetic evaluations across demographic groups; and (2) alignment between model outputs and genuine human aesthetic preferences. Our benchmark covers three subtasks (Aesthetic Perception, Assessment, Empathy) and introduces structured metrics (IFD, NRD, AAS) to assess both bias and alignment. We evaluate 19 MLLMs, including proprietary models (e.g., GPT-4o, Claude-3.5-Sonnet) and open-source models (e.g., InternVL-2.5, Qwen2.5-VL). Results indicate that smaller models exhibit stronger stereotype biases, whereas larger models align more closely with human preferences. Incorporating identity information often exacerbates bias, particularly in emotional judgments. These findings underscore the importance of identity-aware evaluation frameworks in subjective vision-language tasks.</li>
</ul>

<h3>Title: A Controllable 3D Deepfake Generation Framework with Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Wending Liu, Siyun Liang, Huy H. Nguyen, Isao Echizen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11624">https://arxiv.org/abs/2509.11624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11624">https://arxiv.org/pdf/2509.11624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11624]] A Controllable 3D Deepfake Generation Framework with Gaussian Splatting(https://arxiv.org/abs/2509.11624)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>We propose a novel 3D deepfake generation framework based on 3D Gaussian Splatting that enables realistic, identity-preserving face swapping and reenactment in a fully controllable 3D space. Compared to conventional 2D deepfake approaches that suffer from geometric inconsistencies and limited generalization to novel view, our method combines a parametric head model with dynamic Gaussian representations to support multi-view consistent rendering, precise expression control, and seamless background integration. To address editing challenges in point-based representations, we explicitly separate the head and background Gaussians and use pre-trained 2D guidance to optimize the facial region across views. We further introduce a repair module to enhance visual consistency under extreme poses and expressions. Experiments on NeRSemble and additional evaluation videos demonstrate that our method achieves comparable performance to state-of-the-art 2D approaches in identity preservation, as well as pose and expression consistency, while significantly outperforming them in multi-view rendering quality and 3D consistency. Our approach bridges the gap between 3D modeling and deepfake synthesis, enabling new directions for scene-aware, controllable, and immersive visual forgeries, revealing the threat that emerging 3D Gaussian Splatting technique could be used for manipulation attacks.</li>
</ul>

<h3>Title: Inducing Uncertainty for Test-Time Privacy</h3>
<ul>
<li><strong>Authors: </strong>Muhammad H. Ashiq, Peter Triantafillou, Hung Yun Tseng, Grigoris G. Chrysos</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11625">https://arxiv.org/abs/2509.11625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11625">https://arxiv.org/pdf/2509.11625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11625]] Inducing Uncertainty for Test-Time Privacy(https://arxiv.org/abs/2509.11625)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, defense</a></li>
<li><strong>Abstract: </strong>Unlearning is the predominant method for removing the influence of data in machine learning models. However, even after unlearning, models often continue to produce the same predictions on the unlearned data with high confidence. This persistent behavior can be exploited by adversaries using confident model predictions on incorrect or obsolete data to harm users. We call this threat model, which unlearning fails to protect against, *test-time privacy*. In particular, an adversary with full model access can bypass any naive defenses which ensure test-time privacy. To address this threat, we introduce an algorithm which perturbs model weights to induce maximal uncertainty on protected instances while preserving accuracy on the rest of the instances. Our core algorithm is based on finetuning with a Pareto optimal objective that explicitly balances test-time privacy against utility. We also provide a certifiable approximation algorithm which achieves $(\varepsilon, \delta)$ guarantees without convexity assumptions. We then prove a tight, non-vacuous bound that characterizes the privacy-utility tradeoff that our algorithms incur. Empirically, our method obtains $>3\times$ stronger uncertainty than pretraining with $<0.2\%$ drops in accuracy on various image recognition benchmarks. Altogether, this framework provides a tool to guarantee additional protection to end users.</li>
</ul>

<h3>Title: SpeCa: Accelerating Diffusion Transformers with Speculative Feature Caching</h3>
<ul>
<li><strong>Authors: </strong>Jiacheng Liu, Chang Zou, Yuanhuiyi Lyu, Fei Ren, Shaobo Wang, Kaixin Li, Linfeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11628">https://arxiv.org/abs/2509.11628</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11628">https://arxiv.org/pdf/2509.11628</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11628]] SpeCa: Accelerating Diffusion Transformers with Speculative Feature Caching(https://arxiv.org/abs/2509.11628)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Diffusion models have revolutionized high-fidelity image and video synthesis, yet their computational demands remain prohibitive for real-time applications. These models face two fundamental challenges: strict temporal dependencies preventing parallelization, and computationally intensive forward passes required at each denoising step. Drawing inspiration from speculative decoding in large language models, we present SpeCa, a novel 'Forecast-then-verify' acceleration framework that effectively addresses both limitations. SpeCa's core innovation lies in introducing Speculative Sampling to diffusion models, predicting intermediate features for subsequent timesteps based on fully computed reference timesteps. Our approach implements a parameter-free verification mechanism that efficiently evaluates prediction reliability, enabling real-time decisions to accept or reject each prediction while incurring negligible computational overhead. Furthermore, SpeCa introduces sample-adaptive computation allocation that dynamically modulates resources based on generation complexity, allocating reduced computation for simpler samples while preserving intensive processing for complex instances. Experiments demonstrate 6.34x acceleration on FLUX with minimal quality degradation (5.5% drop), 7.3x speedup on DiT while preserving generation fidelity, and 79.84% VBench score at 6.1x acceleration for HunyuanVideo. The verification mechanism incurs minimal overhead (1.67%-3.5% of full inference costs), establishing a new paradigm for efficient diffusion model inference while maintaining generation quality even at aggressive acceleration ratios. Our codes have been released in Github: \textbf{this https URL}</li>
</ul>

<h3>Title: Reasoned Safety Alignment: Ensuring Jailbreak Defense via Answer-Then-Check</h3>
<ul>
<li><strong>Authors: </strong>Chentao Cao, Xiaojun Xu, Bo Han, Hang Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11629">https://arxiv.org/abs/2509.11629</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11629">https://arxiv.org/pdf/2509.11629</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11629]] Reasoned Safety Alignment: Ensuring Jailbreak Defense via Answer-Then-Check(https://arxiv.org/abs/2509.11629)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) continue to advance in capabilities, ensuring their safety against jailbreak attacks remains a critical challenge. In this paper, we introduce a novel safety alignment approach called Answer-Then-Check, which enhances LLM robustness against malicious prompts by applying thinking ability to mitigate jailbreaking problems before producing a final answer to the user. Our method enables models to directly answer the question in their thought and then critically evaluate its safety before deciding whether to provide it. To implement this approach, we construct the Reasoned Safety Alignment (ReSA) dataset, comprising 80K examples that teach models to reason through direct responses and then analyze their safety. Experimental results demonstrate that our approach achieves the Pareto frontier with superior safety capability while decreasing over-refusal rates on over-refusal benchmarks. Notably, the model fine-tuned with ReSA maintains general reasoning capabilities on benchmarks like MMLU, MATH500, and HumanEval. Besides, our method equips models with the ability to perform safe completion. Unlike post-hoc methods that can only reject harmful queries, our model can provide helpful and safe alternative responses for sensitive topics (e.g., self-harm). Furthermore, we discover that training on a small subset of just 500 examples can achieve comparable performance to using the full dataset, suggesting that safety alignment may require less data than previously assumed.</li>
</ul>

<h3>Title: Adaptive-GraphSketch: Real-Time Edge Anomaly Detection via Multi-Layer Tensor Sketching and Temporal Decay</h3>
<ul>
<li><strong>Authors: </strong>Ocheme Anthony Ekle, William Eberle</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11633">https://arxiv.org/abs/2509.11633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11633">https://arxiv.org/pdf/2509.11633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11633]] Adaptive-GraphSketch: Real-Time Edge Anomaly Detection via Multi-Layer Tensor Sketching and Temporal Decay(https://arxiv.org/abs/2509.11633)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, interpretability</a></li>
<li><strong>Abstract: </strong>Anomaly detection in dynamic graphs is essential for identifying malicious activities, fraud, and unexpected behaviors in real-world systems such as cybersecurity and power grids. However, existing approaches struggle with scalability, probabilistic interpretability, and adaptability to evolving traffic patterns. In this paper, we propose ADAPTIVE-GRAPHSKETCH, a lightweight and scalable framework for real-time anomaly detection in streaming edge data. Our method integrates temporal multi-tensor sketching with Count-Min Sketch using Conservative Update (CMS-CU) to compactly track edge frequency patterns with bounded memory, while mitigating hash collision issues. We incorporate Bayesian inference for probabilistic anomaly scoring and apply Exponentially Weighted Moving Average (EWMA) for adaptive thresholding tuned to burst intensity. Extensive experiments on four real-world intrusion detection datasets demonstrate that ADAPTIVE-GRAPHSKETCH outperforms state-of-the-art baselines such as ANOEDGE-G/L, MIDAS-R, and F-FADE, achieving up to 6.5% AUC gain on CIC-IDS2018 and up to 15.6% on CIC-DDoS2019, while processing 20 million edges in under 3.4 seconds using only 10 hash functions. Our results show that ADAPTIVE-GRAPHSKETCH is practical and effective for fast, accurate anomaly detection in large-scale streaming graphs. Keywords: Anomaly Detection, Streaming, Real-time, Dynamic Graphs, Edge Streams, Tensor Sketching</li>
</ul>

<h3>Title: IS-Diff: Improving Diffusion-Based Inpainting with Better Initial Seed</h3>
<ul>
<li><strong>Authors: </strong>Yongzhe Lyu, Yu Wu, Yutian Lin, Bo Du</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11638">https://arxiv.org/abs/2509.11638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11638">https://arxiv.org/pdf/2509.11638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11638]] IS-Diff: Improving Diffusion-Based Inpainting with Better Initial Seed(https://arxiv.org/abs/2509.11638)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have shown promising results in free-form inpainting. Recent studies based on refined diffusion samplers or novel architectural designs led to realistic results and high data consistency. However, random initialization seed (noise) adopted in vanilla diffusion process may introduce mismatched semantic information in masked regions, leading to biased inpainting results, e.g., low consistency and low coherence with the other unmasked area. To address this issue, we propose the Initial Seed refined Diffusion Model (IS-Diff), a completely training-free approach incorporating distributional harmonious seeds to produce harmonious results. Specifically, IS-Diff employs initial seeds sampled from unmasked areas to imitate the masked data distribution, thereby setting a promising direction for the diffusion procedure. Moreover, a dynamic selective refinement mechanism is proposed to detect severe unharmonious inpaintings in intermediate latent and adjust the strength of our initialization prior dynamically. We validate our method on both standard and large-mask inpainting tasks using the CelebA-HQ, ImageNet, and Places2 datasets, demonstrating its effectiveness across all metrics compared to state-of-the-art inpainting methods.</li>
</ul>

<h3>Title: WeatherBench: A Real-World Benchmark Dataset for All-in-One Adverse Weather Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Qiyuan Guan, Qianfeng Yang, Xiang Chen, Tianyu Song, Guiyue Jin, Jiyu Jin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11642">https://arxiv.org/abs/2509.11642</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11642">https://arxiv.org/pdf/2509.11642</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11642]] WeatherBench: A Real-World Benchmark Dataset for All-in-One Adverse Weather Image Restoration(https://arxiv.org/abs/2509.11642)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Existing all-in-one image restoration approaches, which aim to handle multiple weather degradations within a single framework, are predominantly trained and evaluated using mixed single-weather synthetic datasets. However, these datasets often differ significantly in resolution, style, and domain characteristics, leading to substantial domain gaps that hinder the development and fair evaluation of unified models. Furthermore, the lack of a large-scale, real-world all-in-one weather restoration dataset remains a critical bottleneck in advancing this field. To address these limitations, we present a real-world all-in-one adverse weather image restoration benchmark dataset, which contains image pairs captured under various weather conditions, including rain, snow, and haze, as well as diverse outdoor scenes and illumination settings. The resulting dataset provides precisely aligned degraded and clean images, enabling supervised learning and rigorous evaluation. We conduct comprehensive experiments by benchmarking a variety of task-specific, task-general, and all-in-one restoration methods on our dataset. Our dataset offers a valuable foundation for advancing robust and practical all-in-one image restoration in real-world scenarios. The dataset has been publicly released and is available at this https URL.</li>
</ul>

<h3>Title: EthicsMH: A Pilot Benchmark for Ethical Reasoning in Mental Health AI</h3>
<ul>
<li><strong>Authors: </strong>Sai Kartheek Reddy Kasu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11648">https://arxiv.org/abs/2509.11648</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11648">https://arxiv.org/pdf/2509.11648</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11648]] EthicsMH: A Pilot Benchmark for Ethical Reasoning in Mental Health AI(https://arxiv.org/abs/2509.11648)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>The deployment of large language models (LLMs) in mental health and other sensitive domains raises urgent questions about ethical reasoning, fairness, and responsible alignment. Yet, existing benchmarks for moral and clinical decision-making do not adequately capture the unique ethical dilemmas encountered in mental health practice, where confidentiality, autonomy, beneficence, and bias frequently intersect. To address this gap, we introduce Ethical Reasoning in Mental Health (EthicsMH), a pilot dataset of 125 scenarios designed to evaluate how AI systems navigate ethically charged situations in therapeutic and psychiatric contexts. Each scenario is enriched with structured fields, including multiple decision options, expert-aligned reasoning, expected model behavior, real-world impact, and multi-stakeholder viewpoints. This structure enables evaluation not only of decision accuracy but also of explanation quality and alignment with professional norms. Although modest in scale and developed with model-assisted generation, EthicsMH establishes a task framework that bridges AI ethics and mental health decision-making. By releasing this dataset, we aim to provide a seed resource that can be expanded through community and expert contributions, fostering the development of AI systems capable of responsibly handling some of society's most delicate decisions.</li>
</ul>

<h3>Title: Joint-octamamba:an octa joint segmentation network based on feature enhanced mamba</h3>
<ul>
<li><strong>Authors: </strong>Chuang Liu, Nan Guo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11649">https://arxiv.org/abs/2509.11649</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11649">https://arxiv.org/pdf/2509.11649</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11649]] Joint-octamamba:an octa joint segmentation network based on feature enhanced mamba(https://arxiv.org/abs/2509.11649)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>OCTA is a crucial non-invasive imaging technique for diagnosing and monitoring retinal diseases like diabetic retinopathy, age-related macular degeneration, and glaucoma. Current 2D-based methods for retinal vessel (RV) segmentation offer insufficient accuracy. To address this, we propose RVMamba, a novel architecture integrating multiple feature extraction modules with the Mamba state-space model. Moreover, existing joint segmentation models for OCTA data exhibit performance imbalance between different tasks. To simultaneously improve the segmentation of the foveal avascular zone (FAZ) and mitigate this imbalance, we introduce FAZMamba and a unified Joint-OCTAMamba framework. Experimental results on the OCTA-500 dataset demonstrate that Joint-OCTAMamba outperforms existing models across evaluation this http URL code is available at this https URL.</li>
</ul>

<h3>Title: DTGen: Generative Diffusion-Based Few-Shot Data Augmentation for Fine-Grained Dirty Tableware Recognition</h3>
<ul>
<li><strong>Authors: </strong>Lifei Hao, Yue Cheng, Baoqi Huang, Bing Jia, Xuandong Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11661">https://arxiv.org/abs/2509.11661</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11661">https://arxiv.org/pdf/2509.11661</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11661]] DTGen: Generative Diffusion-Based Few-Shot Data Augmentation for Fine-Grained Dirty Tableware Recognition(https://arxiv.org/abs/2509.11661)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Intelligent tableware cleaning is a critical application in food safety and smart homes, but existing methods are limited by coarse-grained classification and scarcity of few-shot data, making it difficult to meet industrialization requirements. We propose DTGen, a few-shot data augmentation scheme based on generative diffusion models, specifically designed for fine-grained dirty tableware recognition. DTGen achieves efficient domain specialization through LoRA, generates diverse dirty images via structured prompts, and ensures data quality through CLIP-based cross-modal filtering. Under extremely limited real few-shot conditions, DTGen can synthesize virtually unlimited high-quality samples, significantly improving classifier performance and supporting fine-grained dirty tableware recognition. We further elaborate on lightweight deployment strategies, promising to transfer DTGen's benefits to embedded dishwashers and integrate with cleaning programs to intelligently regulate energy consumption and detergent usage. Research results demonstrate that DTGen not only validates the value of generative AI in few-shot industrial vision but also provides a feasible deployment path for automated tableware cleaning and food safety monitoring.</li>
</ul>

<h3>Title: MindVL: Towards Efficient and Effective Training of Multimodal Large Language Models on Ascend NPUs</h3>
<ul>
<li><strong>Authors: </strong>Feilong Chen, Yijiang Liu, Yi Huang, Hao Wang, Miren Tian, Ya-Qi Yu, Minghui Liao, Jihao Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11662">https://arxiv.org/abs/2509.11662</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11662">https://arxiv.org/pdf/2509.11662</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11662]] MindVL: Towards Efficient and Effective Training of Multimodal Large Language Models on Ascend NPUs(https://arxiv.org/abs/2509.11662)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>We propose MindVL, a multimodal large langauge model trained on Ascend NPUs. Similar to Qwen2.5-VL, MindVL adopts native-resolution Vision Transformers, which enables it to process images at their original variable resolutions. This design avoids the degradation caused by fixed-resolution tiling while preserving fine-grained details and global layouts, which is crucial for visually dense content such as complex charts and diagrams. To ensure the smooth training of MindVL on Ascend NPUs, we develop Mindspeed-MLLM, a distributed multimodal training framework tailored for Ascend NPUs. To maintain training accuracy, we implement equivalent replacements for certain operators. MindVL undergoes a three-phase training process, namely the warm-up phase, multitask training phase, and supervised instruction tuning phase, to gradually enhance its capabilities. This process starts with basic visual and multimodal pre-training, followed by large-scale multiask trainging and instruction tuning. We also adopt multimodal data packaging and hybrid parallelism techniques, which significantly improve end-to-end training speed. To further boost model performance, we specifically introduce test-time resolution search and model weight averaging. Notably, despite using about 1/10 of the training data required by Qwen2.5-VL, MindVL achieves performance on par with Qwen2.5-VL in evaluations of general multimodal understanding and document/table comprehension. Beyond overall scores, MindVL also delivers leading performance in OCR assessments.</li>
</ul>

<h3>Title: Cyber Attack Mitigation Framework for Denial of Service (DoS) Attacks in Fog Computing</h3>
<ul>
<li><strong>Authors: </strong>Fizza Khurshid, Umara Noor, Zahid Rashid</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11668">https://arxiv.org/abs/2509.11668</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11668">https://arxiv.org/pdf/2509.11668</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11668]] Cyber Attack Mitigation Framework for Denial of Service (DoS) Attacks in Fog Computing(https://arxiv.org/abs/2509.11668)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Innovative solutions to cyber security issues are shaped by the ever-changing landscape of cyber threats. Automating the mitigation of these threats can be achieved through a new methodology that addresses the domain of mitigation automation, which is often overlooked. This literature overview emphasizes the lack of scholarly work focusing specifically on automated cyber threat mitigation, particularly in addressing challenges beyond detection. The proposed methodology comprise of the development of an automatic cyber threat mitigation framework tailored for Distributed Denial-of-Service (DDoS) attacks. This framework adopts a multi-layer security approach, utilizing smart devices at the device layer, and leveraging fog network and cloud computing layers for deeper understanding and technological adaptability. Initially, firewall rule-based packet inspection is conducted on simulated attack traffic to filter out DoS packets, forwarding legitimate packets to the fog. The methodology emphasizes the integration of fog detection through statistical and behavioral analysis, specification-based detection, and deep packet inspection, resulting in a comprehensive cyber protection system. Furthermore, cloud-level inspection is performed to confirm and mitigate attacks using firewalls, enhancing strategic defense and increasing robustness against cyber threats. These enhancements enhance understanding of the research framework's practical implementation and assessment strategies, substantiating its importance in addressing current cyber security challenges and shaping future automation mitigation approaches.</li>
</ul>

<h3>Title: RouteExtract: A Modular Pipeline for Extracting Routes from Paper Maps</h3>
<ul>
<li><strong>Authors: </strong>Bjoern Kremser, Yusuke Matsui</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11674">https://arxiv.org/abs/2509.11674</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11674">https://arxiv.org/pdf/2509.11674</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11674]] RouteExtract: A Modular Pipeline for Extracting Routes from Paper Maps(https://arxiv.org/abs/2509.11674)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Paper maps remain widely used for hiking and sightseeing because they contain curated trails and locally relevant annotations that are often missing from digital navigation applications such as Google Maps. We propose a pipeline to extract navigable trails from scanned maps, enabling their use in GPS-based navigation. Our method combines georeferencing, U-Net-based binary segmentation, graph construction, and an iterative refinement procedure using a routing engine. We evaluate the full end-to-end pipeline as well as individual components, showing that the approach can robustly recover trail networks from diverse map styles and generate GPS routes suitable for practical use.</li>
</ul>

<h3>Title: IMD: A 6-DoF Pose Estimation Benchmark for Industrial Metallic Objects</h3>
<ul>
<li><strong>Authors: </strong>Ruimin Ma, Sebastian Zudaire, Zhen Li, Chi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11680">https://arxiv.org/abs/2509.11680</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11680">https://arxiv.org/pdf/2509.11680</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11680]] IMD: A 6-DoF Pose Estimation Benchmark for Industrial Metallic Objects(https://arxiv.org/abs/2509.11680)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Object 6DoF (6D) pose estimation is essential for robotic perception, especially in industrial settings. It enables robots to interact with the environment and manipulate objects. However, existing benchmarks on object 6D pose estimation primarily use everyday objects with rich textures and low-reflectivity, limiting model generalization to industrial scenarios where objects are often metallic, texture-less, and highly reflective. To address this gap, we propose a novel dataset and benchmark namely \textit{Industrial Metallic Dataset (IMD)}, tailored for industrial applications. Our dataset comprises 45 true-to-scale industrial components, captured with an RGB-D camera under natural indoor lighting and varied object arrangements to replicate real-world conditions. The benchmark supports three tasks, including video object segmentation, 6D pose tracking, and one-shot 6D pose estimation. We evaluate existing state-of-the-art models, including XMem and SAM2 for segmentation, and BundleTrack and BundleSDF for pose estimation, to assess model performance in industrial contexts. Evaluation results show that our industrial dataset is more challenging than existing household object datasets. This benchmark provides the baseline for developing and comparing segmentation and pose estimation algorithms that better generalize to industrial robotics scenarios.</li>
</ul>

<h3>Title: An Unsupervised Learning Approach For A Reliable Profiling Of Cyber Threat Actors Reported Globally Based On Complete Contextual Information Of Cyber Attacks</h3>
<ul>
<li><strong>Authors: </strong>Sawera Shahid, Umara Noor, Zahid Rashid</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11683">https://arxiv.org/abs/2509.11683</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11683">https://arxiv.org/pdf/2509.11683</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11683]] An Unsupervised Learning Approach For A Reliable Profiling Of Cyber Threat Actors Reported Globally Based On Complete Contextual Information Of Cyber Attacks(https://arxiv.org/abs/2509.11683)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, defense, attack</a></li>
<li><strong>Abstract: </strong>Cyber attacks are rapidly increasing with the advancement of technology and there is no protection for our information. To prevent future cyberattacks it is critical to promptly recognize cyberattacks and establish strong defense mechanisms against them. To respond to cybersecurity threats immediately, it is essential to examine the attackers skills, knowledge, and behaviors with the goal of evaluating their impact on the system and comprehending the traits associated with these attacks. Creating a profile of cyber threat actors based on their traits or patterns of behavior can help to create effective defenses against cyberattacks in advance. In the current literature, multiple supervised machine learning based approaches considered a smaller number of features for attacker profiling that are reported in textual cyber threat incident documents although these profiles have been developed based on the security experts own perception, we cannot rely on them. Supervised machine learning approaches strictly depend upon the structure data set. This usually leads to a two step process where we first have to establish a structured data set before we can analyze it and then employ it to construct defense mechanisms, which takes time. In this paper, an unsupervised efficient agglomerative hierarchal clustering technique is proposed for profiling cybercriminal groups based on their comprehensive contextual threat information in order to address the aforementioned issues. The main objective of this report is to identify the relationship between cyber threat actors based on their common features, aggregate them, and also profile cyber criminal groups.</li>
</ul>

<h3>Title: A Dynamic Knowledge Update-Driven Model with Large Language Models for Fake News Detection</h3>
<ul>
<li><strong>Authors: </strong>Di Jin, Jun Yang, Xiaobao Wang, Junwei Zhang, Shuqi Li, Dongxiao He</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11687">https://arxiv.org/abs/2509.11687</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11687">https://arxiv.org/pdf/2509.11687</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11687]] A Dynamic Knowledge Update-Driven Model with Large Language Models for Fake News Detection(https://arxiv.org/abs/2509.11687)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As the Internet and social media evolve rapidly, distinguishing credible news from a vast amount of complex information poses a significant challenge. Due to the suddenness and instability of news events, the authenticity labels of news can potentially shift as events develop, making it crucial for fake news detection to obtain the latest event updates. Existing methods employ retrieval-augmented generation to fill knowledge gaps, but they suffer from issues such as insufficient credibility of retrieved content and interference from noisy information. We propose a dynamic knowledge update-driven model for fake news detection (DYNAMO), which leverages knowledge graphs to achieve continuous updating of new knowledge and integrates with large language models to fulfill dual functions: news authenticity detection and verification of new knowledge correctness, solving the two key problems of ensuring the authenticity of new knowledge and deeply mining news semantics. Specifically, we first construct a news-domain-specific knowledge graph. Then, we use Monte Carlo Tree Search to decompose complex news and verify them step by step. Finally, we extract and update new knowledge from verified real news texts and reasoning paths. Experimental results demonstrate that DYNAMO achieves the best performance on two real-world datasets.</li>
</ul>

<h3>Title: Uncertainty-Aware Retinal Vessel Segmentation via Ensemble Distillation</h3>
<ul>
<li><strong>Authors: </strong>Jeremiah Fadugba, Petru Manescu, Bolanle Oladejo, Delmiro Fernandez-Reyes, Philipp Berens</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11689">https://arxiv.org/abs/2509.11689</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11689">https://arxiv.org/pdf/2509.11689</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11689]] Uncertainty-Aware Retinal Vessel Segmentation via Ensemble Distillation(https://arxiv.org/abs/2509.11689)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Uncertainty estimation is critical for reliable medical image segmentation, particularly in retinal vessel analysis, where accurate predictions are essential for diagnostic applications. Deep Ensembles, where multiple networks are trained individually, are widely used to improve medical image segmentation performance. However, training and testing costs increase with the number of ensembles. In this work, we propose Ensemble Distillation as a robust alternative to commonly used uncertainty estimation techniques by distilling the knowledge of multiple ensemble models into a single model. Through extensive experiments on the DRIVE and FIVES datasets, we demonstrate that Ensemble Distillation achieves comparable performance via calibration and segmentation metrics, while significantly reducing computational complexity. These findings suggest that Ensemble distillation provides an efficient and reliable approach for uncertainty estimation in the segmentation of the retinal vessels, making it a promising tool for medical imaging applications.</li>
</ul>

<h3>Title: Time-Based State-Management of Hash-Based Signature CAs for VPN-Authentication</h3>
<ul>
<li><strong>Authors: </strong>Daniel Herzinger, Linus Heise, Daniel Loebenberger, Matthias Söllner</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11695">https://arxiv.org/abs/2509.11695</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11695">https://arxiv.org/pdf/2509.11695</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11695]] Time-Based State-Management of Hash-Based Signature CAs for VPN-Authentication(https://arxiv.org/abs/2509.11695)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>Advances in quantum computing necessitate migrating the entire technology stack to post-quantum cryptography. This includes IPsec-based VPN connection authentication. Although there is an RFC draft for post-quantum authentication in this setting, the draft does not consider (stateful) hash-based signatures despite their small signature size and trusted long-term security. We propose a design with time-based state-management that assigns VPN devices a certificate authority (CA) based on the hash-based signature scheme XMSS. The CA then issues leaf certificates which are based on classical cryptography but have a short validity time, e. g., four hours. It is to be expected that even large quantum computers will take significantly longer to break the cryptography, making the design quantum-secure. We propose strategies to make the timekeeping more resilient to faults and tampering, as well as strategies to recognize a wrong system time, minimize its potential damage, and quickly recover. The result is an OpenBSD implementation of a quantum-safe and, regarding the leaf certificates, highly flexible VPN authentication design that requires significantly less bandwidth and computational resources compared to existing alternatives.</li>
</ul>

<h3>Title: A Holistic Approach to E-Commerce Innovation: Redefining Security and User Experience</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Olid Ali Akash (1), Priyangana Saha (1) ((1) North South University)</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11712">https://arxiv.org/abs/2509.11712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11712">https://arxiv.org/pdf/2509.11712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11712]] A Holistic Approach to E-Commerce Innovation: Redefining Security and User Experience(https://arxiv.org/abs/2509.11712)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect</a></li>
<li><strong>Abstract: </strong>In the modern, fast-moving world of e-commerce, many Android apps face challenges in providing a simple and secure shopping experience. Many of these apps, often enough, have complicated designs that prevent users from finding what they want quickly, thus frustrating them and wasting their precious time. Another major issue is that of security; with the limitation of payment options and weak authentication mechanisms, users' sensitive information can be compromised. This research presents a new e-commerce platform that responds to the above challenges with an intuitive interface and strong security measures. The platform makes online shopping easy with well-organized categories of products and a fast, efficient checkout process. It also gives priority to security by incorporating features such as Google authentication and SSL-secured payment gateways to protect user data and ensure secure transactions. This paper discusses how a focus on user-friendliness, security, and personalization steps up the game for e-commerce platforms, providing workable frameworks that match modern user needs and expectations. The findings show the e-commerce user experience can be remodelled by the platform, hence opening ways for future developments in that respect.</li>
</ul>

<h3>Title: Beyond Regularity: Modeling Chaotic Mobility Patterns for Next Location Prediction</h3>
<ul>
<li><strong>Authors: </strong>Yuqian Wu, Yuhong Peng, Jiapeng Yu, Xiangyu Liu, Zeting Yan, Kang Lin, Weifeng Su, Bingqing Qu, Raymond Lee, Dingqi Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11713">https://arxiv.org/abs/2509.11713</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11713">https://arxiv.org/pdf/2509.11713</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11713]] Beyond Regularity: Modeling Chaotic Mobility Patterns for Next Location Prediction(https://arxiv.org/abs/2509.11713)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Next location prediction is a key task in human mobility analysis, crucial for applications like smart city resource allocation and personalized navigation services. However, existing methods face two significant challenges: first, they fail to address the dynamic imbalance between periodic and chaotic mobile patterns, leading to inadequate adaptation over sparse trajectories; second, they underutilize contextual cues, such as temporal regularities in arrival times, which persist even in chaotic patterns and offer stronger predictability than spatial forecasts due to reduced search spaces. To tackle these challenges, we propose \textbf{\method}, a \underline{\textbf{C}}h\underline{\textbf{A}}otic \underline{\textbf{N}}eural \underline{\textbf{O}}scillator n\underline{\textbf{E}}twork for next location prediction, which introduces a biologically inspired Chaotic Neural Oscillatory Attention mechanism to inject adaptive variability into traditional attention, enabling balanced representation of evolving mobility behaviors, and employs a Tri-Pair Interaction Encoder along with a Cross Context Attentive Decoder to fuse multimodal ``who-when-where'' contexts in a joint framework for enhanced prediction performance. Extensive experiments on two real-world datasets demonstrate that CANOE consistently and significantly outperforms a sizeable collection of state-of-the-art baselines, yielding 3.17\%-13.11\% improvement over the best-performing baselines across different cases. In particular, CANOE can make robust predictions over mobility trajectories of different mobility chaotic levels. A series of ablation studies also supports our key design choices. Our code is available at: this https URL.</li>
</ul>

<h3>Title: DRAG: Data Reconstruction Attack using Guided Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Wa-Kin Lei, Jun-Cheng Chen, Shang-Tse Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11724">https://arxiv.org/abs/2509.11724</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11724">https://arxiv.org/pdf/2509.11724</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11724]] DRAG: Data Reconstruction Attack using Guided Diffusion(https://arxiv.org/abs/2509.11724)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, attack, robust, diffusion</a></li>
<li><strong>Abstract: </strong>With the rise of large foundation models, split inference (SI) has emerged as a popular computational paradigm for deploying models across lightweight edge devices and cloud servers, addressing data privacy and computational cost concerns. However, most existing data reconstruction attacks have focused on smaller CNN classification models, leaving the privacy risks of foundation models in SI settings largely unexplored. To address this gap, we propose a novel data reconstruction attack based on guided diffusion, which leverages the rich prior knowledge embedded in a latent diffusion model (LDM) pre-trained on a large-scale dataset. Our method performs iterative reconstruction on the LDM's learned image prior, effectively generating high-fidelity images resembling the original data from their intermediate representations (IR). Extensive experiments demonstrate that our approach significantly outperforms state-of-the-art methods, both qualitatively and quantitatively, in reconstructing data from deep-layer IRs of the vision foundation model. The results highlight the urgent need for more robust privacy protection mechanisms for large models in SI scenarios. Code is available at: this https URL.</li>
</ul>

<h3>Title: Microsurgical Instrument Segmentation for Robot-Assisted Surgery</h3>
<ul>
<li><strong>Authors: </strong>Tae Kyeong Jeong, Garam Kim, Juyoun Park</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11727">https://arxiv.org/abs/2509.11727</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11727">https://arxiv.org/pdf/2509.11727</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11727]] Microsurgical Instrument Segmentation for Robot-Assisted Surgery(https://arxiv.org/abs/2509.11727)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Accurate segmentation of thin structures is critical for microsurgical scene understanding but remains challenging due to resolution loss, low contrast, and class imbalance. We propose Microsurgery Instrument Segmentation for Robotic Assistance(MISRA), a segmentation framework that augments RGB input with luminance channels, integrates skip attention to preserve elongated features, and employs an Iterative Feedback Module(IFM) for continuity restoration across multiple passes. In addition, we introduce a dedicated microsurgical dataset with fine-grained annotations of surgical instruments including thin objects, providing a benchmark for robust evaluation Dataset available at this https URL. Experiments demonstrate that MISRA achieves competitive performance, improving the mean class IoU by 5.37% over competing methods, while delivering more stable predictions at instrument contacts and overlaps. These results position MISRA as a promising step toward reliable scene parsing for computer-assisted and robotic microsurgery.</li>
</ul>

<h3>Title: Fast and Interpretable Machine Learning Modelling of Atmospheric Molecular Clusters</h3>
<ul>
<li><strong>Authors: </strong>Lauri Seppäläinen, Jakub Kubečka, Jonas Elm, Kai Puolamäki</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11728">https://arxiv.org/abs/2509.11728</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11728">https://arxiv.org/pdf/2509.11728</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11728]] Fast and Interpretable Machine Learning Modelling of Atmospheric Molecular Clusters(https://arxiv.org/abs/2509.11728)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Understanding how atmospheric molecular clusters form and grow is key to resolving one of the biggest uncertainties in climate modelling: the formation of new aerosol particles. While quantum chemistry offers accurate insights into these early-stage clusters, its steep computational costs limit large-scale exploration. In this work, we present a fast, interpretable, and surprisingly powerful alternative: $k$-nearest neighbour ($k$-NN) regression model. By leveraging chemically informed distance metrics, including a kernel-induced metric and one learned via metric learning for kernel regression (MLKR), we show that simple $k$-NN models can rival more complex kernel ridge regression (KRR) models in accuracy, while reducing computational time by orders of magnitude. We perform this comparison with the well-established Faber-Christensen-Huang-Lilienfeld (FCHL19) molecular descriptor, but other descriptors (e.g., FCHL18, MBDF, and CM) can be shown to have similar performance. Applied to both simple organic molecules in the QM9 benchmark set and large datasets of atmospheric molecular clusters (sulphuric acid-water and sulphuric-multibase -base systems), our $k$-NN models achieve near-chemical accuracy, scale seamlessly to datasets with over 250,000 entries, and even appears to extrapolate to larger unseen clusters with minimal error (often nearing 1 kcal/mol). With built-in interpretability and straightforward uncertainty estimation, this work positions $k$-NN as a potent tool for accelerating discovery in atmospheric chemistry and beyond.</li>
</ul>

<h3>Title: Bridging the Gap Between Sparsity and Redundancy: A Dual-Decoding Framework with Global Context for Map Inference</h3>
<ul>
<li><strong>Authors: </strong>Yudong Shen, Wenyu Wu, Jiali Mao, Yixiao Tong, Guoping Liu, Chaoya Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11731">https://arxiv.org/abs/2509.11731</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11731">https://arxiv.org/pdf/2509.11731</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11731]] Bridging the Gap Between Sparsity and Redundancy: A Dual-Decoding Framework with Global Context for Map Inference(https://arxiv.org/abs/2509.11731)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Trajectory data has become a key resource for automated map in-ference due to its low cost, broad coverage, and continuous availability. However, uneven trajectory density often leads to frag-mented roads in sparse areas and redundant segments in dense regions, posing significant challenges for existing methods. To address these issues, we propose DGMap, a dual-decoding framework with global context awareness, featuring Multi-scale Grid Encoding, Mask-enhanced Keypoint Extraction, and Global Context-aware Relation Prediction. By integrating global semantic context with local geometric features, DGMap improves keypoint detection accuracy to reduce road fragmentation in sparse-trajectory areas. Additionally, the Global Context-aware Relation Prediction module suppresses false connections in dense-trajectory regions by modeling long-range trajectory patterns. Experimental results on three real-world datasets show that DGMap outperforms state-of-the-art methods by 5% in APLS, with notable performance gains on trajectory data from the Didi Chuxing platform</li>
</ul>

<h3>Title: Removal Attack and Defense on AI-generated Content Latent-based Watermarking</h3>
<ul>
<li><strong>Authors: </strong>De Zhang Lee, Han Fang, Hanyi Wang, Ee-Chien Chang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11745">https://arxiv.org/abs/2509.11745</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11745">https://arxiv.org/pdf/2509.11745</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11745]] Removal Attack and Defense on AI-generated Content Latent-based Watermarking(https://arxiv.org/abs/2509.11745)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust, watermark, diffusion</a></li>
<li><strong>Abstract: </strong>Digital watermarks can be embedded into AI-generated content (AIGC) by initializing the generation process with starting points sampled from a secret distribution. When combined with pseudorandom error-correcting codes, such watermarked outputs can remain indistinguishable from unwatermarked objects, while maintaining robustness under whitenoise. In this paper, we go beyond indistinguishability and investigate security under removal attacks. We demonstrate that indistinguishability alone does not necessarily guarantee resistance to adversarial removal. Specifically, we propose a novel attack that exploits boundary information leaked by the locations of watermarked objects. This attack significantly reduces the distortion required to remove watermarks -- by up to a factor of $15 \times$ compared to a baseline whitenoise attack under certain settings. To mitigate such attacks, we introduce a defense mechanism that applies a secret transformation to hide the boundary, and prove that the secret transformation effectively rendering any attacker's perturbations equivalent to those of a naive whitenoise adversary. Our empirical evaluations, conducted on multiple versions of Stable Diffusion, validate the effectiveness of both the attack and the proposed defense, highlighting the importance of addressing boundary leakage in latent-based watermarking schemes.</li>
</ul>

<h3>Title: A Fully Open and Generalizable Foundation Model for Ultrasound Clinical Applications</h3>
<ul>
<li><strong>Authors: </strong>Hongyuan Zhang, Yuheng Wu, Mingyang Zhao, Zhiwei Chen, Rebecca Li, Fei Zhu, Haohan Zhao, Xiaohua Yuan, Meng Yang, Chunli Qiu, Xiang Cong, Haiyan Chen, Lina Luan, Randolph H.L. Wong, Huai Liao, Colin A Graham, Shi Chang, Guowei Tao, Dong Yi, Zhen Lei, Nassir Navab, Sebastien Ourselin, Jiebo Luo, Hongbin Liu, Gaofeng Meng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11752">https://arxiv.org/abs/2509.11752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11752">https://arxiv.org/pdf/2509.11752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11752]] A Fully Open and Generalizable Foundation Model for Ultrasound Clinical Applications(https://arxiv.org/abs/2509.11752)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Artificial intelligence (AI) that can effectively learn ultrasound representations by integrating multi-source data holds significant promise for advancing clinical care. However, the scarcity of large labeled datasets in real-world clinical environments and the limited generalizability of task-specific models have hindered the development of generalizable clinical AI models for ultrasound applications. In this study, we present EchoCare, a novel ultrasound foundation model for generalist clinical use, developed via self-supervised learning on our curated, publicly available, large-scale dataset EchoCareData. EchoCareData comprises 4.5 million ultrasound images, sourced from over 23 countries across 5 continents and acquired via a diverse range of distinct imaging devices, thus encompassing global cohorts that are multi-center, multi-device, and multi-ethnic. Unlike prior studies that adopt off-the-shelf vision foundation model architectures, we introduce a hierarchical classifier into EchoCare to enable joint learning of pixel-level and representation-level features, capturing both global anatomical contexts and local ultrasound characteristics. With minimal training, EchoCare outperforms state-of-the-art comparison models across 10 representative ultrasound benchmarks of varying diagnostic difficulties, spanning disease diagnosis, lesion segmentation, organ detection, landmark prediction, quantitative regression, imaging enhancement and report generation. The code and pretrained model are publicly released, rendering EchoCare accessible for fine-tuning and local adaptation, supporting extensibility to additional applications. EchoCare provides a fully open and generalizable foundation model to boost the development of AI technologies for diverse clinical ultrasound applications.</li>
</ul>

<h3>Title: On Spatial-Provenance Recovery in Wireless Networks with Relaxed-Privacy Constraints</h3>
<ul>
<li><strong>Authors: </strong>Manish Bansal, Pramsu Shrivastava, J. Harshan</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.IT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11761">https://arxiv.org/abs/2509.11761</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11761">https://arxiv.org/pdf/2509.11761</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11761]] On Spatial-Provenance Recovery in Wireless Networks with Relaxed-Privacy Constraints(https://arxiv.org/abs/2509.11761)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack</a></li>
<li><strong>Abstract: </strong>In Vehicle-to-Everything (V2X) networks with multi-hop communication, Road Side Units (RSUs) intend to gather location data from the vehicles to offer various location-based services. Although vehicles use the Global Positioning System (GPS) for navigation, they may refrain from sharing their exact GPS coordinates to the RSUs due to privacy considerations. Thus, to address the localization expectations of the RSUs and the privacy concerns of the vehicles, we introduce a relaxed-privacy model wherein the vehicles share their partial location information in order to avail the location-based services. To implement this notion of relaxed-privacy, we propose a low-latency protocol for spatial-provenance recovery, wherein vehicles use correlated linear Bloom filters to embed their position information. Our proposed spatial-provenance recovery process takes into account the resolution of localization, the underlying ad hoc protocol, and the coverage range of the wireless technology used by the vehicles. Through a rigorous theoretical analysis, we present extensive analysis on the underlying trade-off between relaxed-privacy and the communication-overhead of the protocol. Finally, using a wireless testbed, we show that our proposed method requires a few bits in the packet header to provide security features such as localizing a low-power jammer executing a denial-of-service attack.</li>
</ul>

<h3>Title: MSMA: Multi-Scale Feature Fusion For Multi-Attribute 3D Face Reconstruction From Unconstrained Images</h3>
<ul>
<li><strong>Authors: </strong>Danling Cao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11763">https://arxiv.org/abs/2509.11763</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11763">https://arxiv.org/pdf/2509.11763</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11763]] MSMA: Multi-Scale Feature Fusion For Multi-Attribute 3D Face Reconstruction From Unconstrained Images(https://arxiv.org/abs/2509.11763)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Reconstructing 3D face from a single unconstrained image remains a challenging problem due to diverse conditions in unconstrained environments. Recently, learning-based methods have achieved notable results by effectively capturing complex facial structures and details across varying conditions. Consequently, many existing approaches employ projection-based losses between generated and input images to constrain model training. However, learning-based methods for 3D face reconstruction typically require substantial amounts of 3D facial data, which is difficult and costly to obtain. Consequently, to reduce reliance on labeled 3D face datasets, many existing approaches employ projection-based losses between generated and input images to constrain model training. Nonetheless, despite these advancements, existing approaches frequently struggle to capture detailed and multi-scale features under diverse facial attributes and conditions, leading to incomplete or less accurate reconstructions. In this paper, we propose a Multi-Scale Feature Fusion with Multi-Attribute (MSMA) framework for 3D face reconstruction from unconstrained images. Our method integrates multi-scale feature fusion with a focus on multi-attribute learning and leverages a large-kernel attention module to enhance the precision of feature extraction across scales, enabling accurate 3D facial parameter estimation from a single 2D image. Comprehensive experiments on the MICC Florence, Facewarehouse and custom-collect datasets demonstrate that our approach achieves results on par with current state-of-the-art methods, and in some instances, surpasses SOTA performance across challenging conditions.</li>
</ul>

<h3>Title: Seg2Track-SAM2: SAM2-based Multi-object Tracking and Segmentation for Zero-shot Generalization</h3>
<ul>
<li><strong>Authors: </strong>Diogo Mendonça, Tiago Barros, Cristiano Premebida, Urbano J. Nunes</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11772">https://arxiv.org/abs/2509.11772</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11772">https://arxiv.org/pdf/2509.11772</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11772]] Seg2Track-SAM2: SAM2-based Multi-object Tracking and Segmentation for Zero-shot Generalization(https://arxiv.org/abs/2509.11772)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Autonomous systems require robust Multi-Object Tracking (MOT) capabilities to operate reliably in dynamic environments. MOT ensures consistent object identity assignment and precise spatial delineation. Recent advances in foundation models, such as SAM2, have demonstrated strong zero-shot generalization for video segmentation, but their direct application to MOTS (MOT+Segmentation) remains limited by insufficient identity management and memory efficiency. This work introduces Seg2Track-SAM2, a framework that integrates pre-trained object detectors with SAM2 and a novel Seg2Track module to address track initialization, track management, and reinforcement. The proposed approach requires no fine-tuning and remains detector-agnostic. Experimental results on KITTI MOT and KITTI MOTS benchmarks show that Seg2Track-SAM2 achieves state-of-the-art (SOTA) performance, ranking fourth overall in both car and pedestrian classes on KITTI MOTS, while establishing a new benchmark in association accuracy (AssA). Furthermore, a sliding-window memory strategy reduces memory usage by up to 75% with negligible performance degradation, supporting deployment under resource constraints. These results confirm that Seg2Track-SAM2 advances MOTS by combining robust zero-shot tracking, enhanced identity preservation, and efficient memory utilization. The code is available at this https URL</li>
</ul>

<h3>Title: An Agentic Toolkit for Adaptive Information Extraction from Regulatory Documents</h3>
<ul>
<li><strong>Authors: </strong>Gaye Colakoglu, Gürkan Solmaz, Jonathan Fürst</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11773">https://arxiv.org/abs/2509.11773</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11773">https://arxiv.org/pdf/2509.11773</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11773]] An Agentic Toolkit for Adaptive Information Extraction from Regulatory Documents(https://arxiv.org/abs/2509.11773)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Declaration of Performance (DoP) documents, mandated by EU regulation, certify the performance of construction products. While some of their content is standardized, DoPs vary widely in layout, language, schema, and format, posing challenges for automated key-value pair extraction (KVP) and question answering (QA). Existing static or LLM-only IE pipelines often hallucinate and fail to adapt to this structural diversity. Our domain-specific, stateful agentic system addresses these challenges through a planner-executor-responder architecture. The system infers user intent, detects document modality, and orchestrates tools dynamically for robust, traceable reasoning while avoiding tool misuse or execution loops. Evaluation on a curated DoP dataset demonstrates improved robustness across formats and languages, offering a scalable solution for structured data extraction in regulated workflows.</li>
</ul>

<h3>Title: SA-UNetv2: Rethinking Spatial Attention U-Net for Retinal Vessel Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Changlu Guo, Anders Nymark Christensen, Anders Bjorholm Dahl, Yugen Yi, Morten Rieger Hannemose</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11774">https://arxiv.org/abs/2509.11774</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11774">https://arxiv.org/pdf/2509.11774</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11774]] SA-UNetv2: Rethinking Spatial Attention U-Net for Retinal Vessel Segmentation(https://arxiv.org/abs/2509.11774)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, segmentation</a></li>
<li><strong>Abstract: </strong>Retinal vessel segmentation is essential for early diagnosis of diseases such as diabetic retinopathy, hypertension, and neurodegenerative disorders. Although SA-UNet introduces spatial attention in the bottleneck, it underuses attention in skip connections and does not address the severe foreground-background imbalance. We propose SA-UNetv2, a lightweight model that injects cross-scale spatial attention into all skip connections to strengthen multi-scale feature fusion and adopts a weighted Binary Cross-Entropy (BCE) plus Matthews Correlation Coefficient (MCC) loss to improve robustness to class imbalance. On the public DRIVE and STARE datasets, SA-UNetv2 achieves state-of-the-art performance with only 1.2MB memory and 0.26M parameters (less than 50% of SA-UNet), and 1 second CPU inference on 592 x 592 x 3 images, demonstrating strong efficiency and deployability in resource-constrained, CPU-only settings.</li>
</ul>

<h3>Title: User eXperience Perception Insights Dataset (UXPID): Synthetic User Feedback from Public Industrial Forums</h3>
<ul>
<li><strong>Authors: </strong>Mikhail Kulyabin, Jan Joosten, Choro Ulan uulu, Nuno Miguel Martins Pacheco, Fabian Ries, Filippos Petridis, Jan Bosch, Helena Holmström Olsson</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11777">https://arxiv.org/abs/2509.11777</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11777">https://arxiv.org/pdf/2509.11777</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11777]] User eXperience Perception Insights Dataset (UXPID): Synthetic User Feedback from Public Industrial Forums(https://arxiv.org/abs/2509.11777)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, extraction, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Customer feedback in industrial forums reflect a rich but underexplored source of insight into real-world product experience. These publicly shared discussions offer an organic view of user expectations, frustrations, and success stories shaped by the specific contexts of use. Yet, harnessing this information for systematic analysis remains challenging due to the unstructured and domain-specific nature of the content. The lack of structure and specialized vocabulary makes it difficult for traditional data analysis techniques to accurately interpret, categorize, and quantify the feedback, thereby limiting its potential to inform product development and support strategies. To address these challenges, this paper presents the User eXperience Perception Insights Dataset (UXPID), a collection of 7130 artificially synthesized and anonymized user feedback branches extracted from a public industrial automation forum. Each JavaScript object notation (JSON) record contains multi-post comments related to specific hardware and software products, enriched with metadata and contextual conversation data. Leveraging a large language model (LLM), each branch is systematically analyzed and annotated for UX insights, user expectations, severity and sentiment ratings, and topic classifications. The UXPID dataset is designed to facilitate research in user requirements, user experience (UX) analysis, and AI-driven feedback processing, particularly where privacy and licensing restrictions limit access to real-world data. UXPID supports the training and evaluation of transformer-based models for tasks such as issue detection, sentiment analysis, and requirements extraction in the context of technical forums.</li>
</ul>

<h3>Title: Multimodal Regression for Enzyme Turnover Rates Prediction</h3>
<ul>
<li><strong>Authors: </strong>Bozhen Hu, Cheng Tan, Siyuan Li, Jiangbin Zheng, Sizhe Qiu, Jun Xia, Stan Z. Li</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11782">https://arxiv.org/abs/2509.11782</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11782">https://arxiv.org/pdf/2509.11782</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11782]] Multimodal Regression for Enzyme Turnover Rates Prediction(https://arxiv.org/abs/2509.11782)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The enzyme turnover rate is a fundamental parameter in enzyme kinetics, reflecting the catalytic efficiency of enzymes. However, enzyme turnover rates remain scarce across most organisms due to the high cost and complexity of experimental measurements. To address this gap, we propose a multimodal framework for predicting the enzyme turnover rate by integrating enzyme sequences, substrate structures, and environmental factors. Our model combines a pre-trained language model and a convolutional neural network to extract features from protein sequences, while a graph neural network captures informative representations from substrate molecules. An attention mechanism is incorporated to enhance interactions between enzyme and substrate representations. Furthermore, we leverage symbolic regression via Kolmogorov-Arnold Networks to explicitly learn mathematical formulas that govern the enzyme turnover rate, enabling interpretable and accurate predictions. Extensive experiments demonstrate that our framework outperforms both traditional and state-of-the-art deep learning approaches. This work provides a robust tool for studying enzyme kinetics and holds promise for applications in enzyme engineering, biotechnology, and industrial biocatalysis.</li>
</ul>

<h3>Title: Anomaly Detection in Industrial Control Systems Based on Cross-Domain Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Dongyang Zhan, Wenqi Zhang, Lin Ye, Xiangzhan Yu, Hongli Zhang, Zheng He</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11786">https://arxiv.org/abs/2509.11786</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11786">https://arxiv.org/pdf/2509.11786</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11786]] Anomaly Detection in Industrial Control Systems Based on Cross-Domain Representation Learning(https://arxiv.org/abs/2509.11786)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Industrial control systems (ICSs) are widely used in industry, and their security and stability are very important. Once the ICS is attacked, it may cause serious damage. Therefore, it is very important to detect anomalies in ICSs. ICS can monitor and manage physical devices remotely using communication networks. The existing anomaly detection approaches mainly focus on analyzing the security of network traffic or sensor data. However, the behaviors of different domains (e.g., network traffic and sensor physical status) of ICSs are correlated, so it is difficult to comprehensively identify anomalies by analyzing only a single domain. In this paper, an anomaly detection approach based on cross-domain representation learning in ICSs is proposed, which can learn the joint features of multi-domain behaviors and detect anomalies within different domains. After constructing a cross-domain graph that can represent the behaviors of multiple domains in ICSs, our approach can learn the joint features of them by leveraging graph neural networks. Since anomalies behave differently in different domains, we leverage a multi-task learning approach to identify anomalies in different domains separately and perform joint training. The experimental results show that the performance of our approach is better than existing approaches for identifying anomalies in ICSs.</li>
</ul>

<h3>Title: Watch Your Step: A Cost-Sensitive Framework for Accelerometer-Based Fall Detection in Real-World Streaming Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Timilehin B. Aderinola, Luca Palmerini, Ilaria D'Ascanio, Lorenzo Chiari, Jochen Klenk, Clemens Becker, Brian Caulfield, Georgiana Ifrim</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11789">https://arxiv.org/abs/2509.11789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11789">https://arxiv.org/pdf/2509.11789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11789]] Watch Your Step: A Cost-Sensitive Framework for Accelerometer-Based Fall Detection in Real-World Streaming Scenarios(https://arxiv.org/abs/2509.11789)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Real-time fall detection is crucial for enabling timely interventions and mitigating the severe health consequences of falls, particularly in older adults. However, existing methods often rely on simulated data or assumptions such as prior knowledge of fall events, limiting their real-world applicability. Practical deployment also requires efficient computation and robust evaluation metrics tailored to continuous monitoring. This paper presents a real-time fall detection framework for continuous monitoring without prior knowledge of fall events. Using over 60 hours of inertial measurement unit (IMU) data from the FARSEEING real-world falls dataset, we employ recent efficient classifiers to compute fall probabilities in streaming mode. To enhance robustness, we introduce a cost-sensitive learning strategy that tunes the decision threshold using a cost function reflecting the higher risk of missed falls compared to false alarms. Unlike many methods that achieve high recall only at the cost of precision, our framework achieved Recall of 1.00, Precision of 0.84, and an F1 score of 0.91 on FARSEEING, detecting all falls while keeping false alarms low, with average inference time below 5 ms per sample. These results demonstrate that cost-sensitive threshold tuning enhances the robustness of accelerometer-based fall detection. They also highlight the potential of our computationally efficient framework for deployment in real-time wearable sensor systems for continuous monitoring.</li>
</ul>

<h3>Title: FineQuest: Adaptive Knowledge-Assisted Sports Video Understanding via Agent-of-Thoughts Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Haodong Chen, Haojian Huang, XinXiang Yin, Dian Shao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11796">https://arxiv.org/abs/2509.11796</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11796">https://arxiv.org/pdf/2509.11796</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11796]] FineQuest: Adaptive Knowledge-Assisted Sports Video Understanding via Agent-of-Thoughts Reasoning(https://arxiv.org/abs/2509.11796)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Video Question Answering (VideoQA) based on Large Language Models (LLMs) has shown potential in general video understanding but faces significant challenges when applied to the inherently complex domain of sports videos. In this work, we propose FineQuest, the first training-free framework that leverages dual-mode reasoning inspired by cognitive science: i) Reactive Reasoning for straightforward sports queries and ii) Deliberative Reasoning for more complex ones. To bridge the knowledge gap between general-purpose models and domain-specific sports understanding, FineQuest incorporates SSGraph, a multimodal sports knowledge scene graph spanning nine sports, which encodes both visual instances and domain-specific terminology to enhance reasoning accuracy. Furthermore, we introduce two new sports VideoQA benchmarks, Gym-QA and Diving-QA, derived from the FineGym and FineDiving datasets, enabling diverse and comprehensive evaluation. FineQuest achieves state-of-the-art performance on these benchmarks as well as the existing SPORTU dataset, while maintains strong general VideoQA capabilities.</li>
</ul>

<h3>Title: Pseudo-D: Informing Multi-View Uncertainty Estimation with Calibrated Neural Training Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Ang Nan Gu, Michael Tsang, Hooman Vaseli, Purang Abolmaesumi, Teresa Tsang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11800">https://arxiv.org/abs/2509.11800</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11800">https://arxiv.org/pdf/2509.11800</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11800]] Pseudo-D: Informing Multi-View Uncertainty Estimation with Calibrated Neural Training Dynamics(https://arxiv.org/abs/2509.11800)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Computer-aided diagnosis systems must make critical decisions from medical images that are often noisy, ambiguous, or conflicting, yet today's models are trained on overly simplistic labels that ignore diagnostic uncertainty. One-hot labels erase inter-rater variability and force models to make overconfident predictions, especially when faced with incomplete or artifact-laden inputs. We address this gap by introducing a novel framework that brings uncertainty back into the label space. Our method leverages neural network training dynamics (NNTD) to assess the inherent difficulty of each training sample. By aggregating and calibrating model predictions during training, we generate uncertainty-aware pseudo-labels that reflect the ambiguity encountered during learning. This label augmentation approach is architecture-agnostic and can be applied to any supervised learning pipeline to enhance uncertainty estimation and robustness. We validate our approach on a challenging echocardiography classification benchmark, demonstrating superior performance over specialized baselines in calibration, selective classification, and multi-view fusion.</li>
</ul>

<h3>Title: When Curiosity Signals Danger: Predicting Health Crises Through Online Medication Inquiries</h3>
<ul>
<li><strong>Authors: </strong>Dvora Goncharok, Arbel Shifman, Alexander Apartsin, Yehudit Aperstein</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11802">https://arxiv.org/abs/2509.11802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11802">https://arxiv.org/pdf/2509.11802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11802]] When Curiosity Signals Danger: Predicting Health Crises Through Online Medication Inquiries(https://arxiv.org/abs/2509.11802)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Online medical forums are a rich and underutilized source of insight into patient concerns, especially regarding medication use. Some of the many questions users pose may signal confusion, misuse, or even the early warning signs of a developing health crisis. Detecting these critical questions that may precede severe adverse events or life-threatening complications is vital for timely intervention and improving patient safety. This study introduces a novel annotated dataset of medication-related questions extracted from online forums. Each entry is manually labelled for criticality based on clinical risk factors. We benchmark the performance of six traditional machine learning classifiers using TF-IDF textual representations, alongside three state-of-the-art large language model (LLM)-based classification approaches that leverage deep contextual understanding. Our results highlight the potential of classical and modern methods to support real-time triage and alert systems in digital health spaces. The curated dataset is made publicly available to encourage further research at the intersection of patient-generated data, natural language processing, and early warning systems for critical health events. The dataset and benchmark are available at: this https URL.</li>
</ul>

<h3>Title: From Fuzzy Speech to Medical Insight: Benchmarking LLMs on Noisy Patient Narratives</h3>
<ul>
<li><strong>Authors: </strong>Eden Mama, Liel Sheri, Yehudit Aperstein, Alexander Apartsin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11803">https://arxiv.org/abs/2509.11803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11803">https://arxiv.org/pdf/2509.11803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11803]] From Fuzzy Speech to Medical Insight: Benchmarking LLMs on Noisy Patient Narratives(https://arxiv.org/abs/2509.11803)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The widespread adoption of large language models (LLMs) in healthcare raises critical questions about their ability to interpret patient-generated narratives, which are often informal, ambiguous, and noisy. Existing benchmarks typically rely on clean, structured clinical text, offering limited insight into model performance under realistic conditions. In this work, we present a novel synthetic dataset designed to simulate patient self-descriptions characterized by varying levels of linguistic noise, fuzzy language, and layperson terminology. Our dataset comprises clinically consistent scenarios annotated with ground-truth diagnoses, spanning a spectrum of communication clarity to reflect diverse real-world reporting styles. Using this benchmark, we fine-tune and evaluate several state-of-the-art models (LLMs), including BERT-based and encoder-decoder T5 models. To support reproducibility and future research, we release the Noisy Diagnostic Benchmark (NDB), a structured dataset of noisy, synthetic patient descriptions designed to stress-test and compare the diagnostic capabilities of large language models (LLMs) under realistic linguistic conditions. We made the benchmark available for the community: this https URL</li>
</ul>

<h3>Title: LFRA-Net: A Lightweight Focal and Region-Aware Attention Network for Retinal Vessel Segmentatio</h3>
<ul>
<li><strong>Authors: </strong>Mehwish Mehmood, Shahzaib Iqbal, Tariq Mahmood Khan, Ivor Spence, Muhammad Fahim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11811">https://arxiv.org/abs/2509.11811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11811">https://arxiv.org/pdf/2509.11811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11811]] LFRA-Net: A Lightweight Focal and Region-Aware Attention Network for Retinal Vessel Segmentatio(https://arxiv.org/abs/2509.11811)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Retinal vessel segmentation is critical for the early diagnosis of vision-threatening and systemic diseases, especially in real-world clinical settings with limited computational resources. Although significant improvements have been made in deep learning-based segmentation methods, current models still face challenges in extracting tiny vessels and suffer from high computational costs. In this study, we present LFRA-Net by incorporating focal modulation attention at the encoder-decoder bottleneck and region-aware attention in the selective skip connections. LFRA-Net is a lightweight network optimized for precise and effective retinal vascular segmentation. It enhances feature representation and regional focus by efficiently capturing local and global dependencies. LFRA-Net outperformed many state-of-the-art models while maintaining lightweight characteristics with only 0.17 million parameters, 0.66 MB memory size, and 10.50 GFLOPs. We validated it on three publicly available datasets: DRIVE, STARE, and CHASE\_DB. It performed better in terms of Dice score (84.28\%, 88.44\%, and 85.50\%) and Jaccard index (72.86\%, 79.31\%, and 74.70\%) on the DRIVE, STARE, and CHASE\_DB datasets, respectively. LFRA-Net provides an ideal ratio between segmentation accuracy and computational cost compared to existing deep learning methods, which makes it suitable for real-time clinical applications in areas with limited resources. The code can be found at this https URL.</li>
</ul>

<h3>Title: SpecVLM: Fast Speculative Decoding in Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haiduo Huang, Fuwei Yang, Zhenhua Liu, Xuanwu Yin, Dong Li, Pengju Ren, Emad Barsoum</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11815">https://arxiv.org/abs/2509.11815</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11815">https://arxiv.org/pdf/2509.11815</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11815]] SpecVLM: Fast Speculative Decoding in Vision-Language Models(https://arxiv.org/abs/2509.11815)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Speculative decoding is a powerful way to accelerate autoregressive large language models (LLMs), but directly porting it to vision-language models (VLMs) faces unique systems constraints: the prefill stage is dominated by visual tokens whose count scales with image resolution and video length, inflating both compute and memory, especially the key-value (KV) cache. We study speculative decoding for VLMs and introduce SpecVLM, a practical system that (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering 1.5--2.3x end-to-end speedups over full autoregressive inference, and (2) further accelerates VLM inference with an elastic visual compressor that adaptively selects among pruning, pooling, convolution, and resampler primitives to balance FLOPs/parameters and accuracy per input. To avoid costly offline distillation corpora, we propose an online-logit distillation protocol that trains the draft model with on-the-fly teacher logits and penultimate features using a combined cross-entropy and Smooth L1 objective, eliminating storage and preprocessing while remaining compute-efficient. This protocol reveals a training-time scaling effect: longer online training monotonically increases the draft model's average accepted length, improving speculative efficiency. Empirically, SpecVLM achieves additional acceleration, culminating in 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU, consistently over resolutions and task difficulties, while preserving the target model's output distribution (lossless decoding). Our code is available at this https URL.</li>
</ul>

<h3>Title: Collapse of Irrelevant Representations (CIR) Ensures Robust and Non-Disruptive LLM Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Filip Sondej, Yushi Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11816">https://arxiv.org/abs/2509.11816</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11816">https://arxiv.org/pdf/2509.11816</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11816]] Collapse of Irrelevant Representations (CIR) Ensures Robust and Non-Disruptive LLM Unlearning(https://arxiv.org/abs/2509.11816)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Current unlearning techniques and safety training consistently fail to remove dangerous knowledge from language models. We analyze the root causes and propose a highly selective technique which unlearns robustly and without disrupting general performance. We perform PCA on activations and module output gradients to identify subspaces containing common representations, and collapse them before calculating unlearning updates. This way we avoid unlearning general representations, and only target those specific to the unlearned facts. When unlearning WMDP dataset facts from Llama-3.1-8B, we drop post-attack accuracy 80x more than our best baseline (Circuit Breakers) on biohazardous facts and 30x more on cyberhazardous facts. Despite this, we disrupt general performance 30x less (only 0.1% WikiText loss increase), while requiring less than 3 GPU-seconds per fact.</li>
</ul>

<h3>Title: MAFS: Masked Autoencoder for Infrared-Visible Image Fusion and Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Liying Wang, Xiaoli Zhang, Chuanmin Jia, Siwei Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11817">https://arxiv.org/abs/2509.11817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11817">https://arxiv.org/pdf/2509.11817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11817]] MAFS: Masked Autoencoder for Infrared-Visible Image Fusion and Semantic Segmentation(https://arxiv.org/abs/2509.11817)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Infrared-visible image fusion methods aim at generating fused images with good visual quality and also facilitate the performance of high-level tasks. Indeed, existing semantic-driven methods have considered semantic information injection for downstream applications. However, none of them investigates the potential for reciprocal promotion between pixel-wise image fusion and cross-modal feature fusion perception tasks from a macroscopic task-level perspective. To address this limitation, we propose a unified network for image fusion and semantic segmentation. MAFS is a parallel structure, containing a fusion sub-network and a segmentation sub-network. On the one hand, We devise a heterogeneous feature fusion strategy to enhance semantic-aware capabilities for image fusion. On the other hand, by cascading the fusion sub-network and a segmentation backbone, segmentation-related knowledge is transferred to promote feature-level fusion-based segmentation. Within the framework, we design a novel multi-stage Transformer decoder to aggregate fine-grained multi-scale fused features efficiently. Additionally, a dynamic factor based on the max-min fairness allocation principle is introduced to generate adaptive weights of two tasks and guarantee smooth training in a multi-task manner. Extensive experiments demonstrate that our approach achieves competitive results compared with state-of-the-art methods. The code is available at this https URL.</li>
</ul>

<h3>Title: SCDTour: Embedding Axis Ordering and Merging for Interpretable Semantic Change Detection</h3>
<ul>
<li><strong>Authors: </strong>Taichi Aida, Danushka Bollegala</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11818">https://arxiv.org/abs/2509.11818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11818">https://arxiv.org/pdf/2509.11818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11818]] SCDTour: Embedding Axis Ordering and Merging for Interpretable Semantic Change Detection(https://arxiv.org/abs/2509.11818)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>In Semantic Change Detection (SCD), it is a common problem to obtain embeddings that are both interpretable and high-performing. However, improving interpretability often leads to a loss in the SCD performance, and vice versa. To address this problem, we propose SCDTour, a method that orders and merges interpretable axes to alleviate the performance degradation of SCD. SCDTour considers both (a) semantic similarity between axes in the embedding space, as well as (b) the degree to which each axis contributes to semantic change. Experimental results show that SCDTour preserves performance in semantic change detection while maintaining high interpretability. Moreover, agglomerating the sorted axes produces a more refined set of word senses, which achieves comparable or improved performance against the original full-dimensional embeddings in the SCD task. These findings demonstrate that SCDTour effectively balances interpretability and SCD performance, enabling meaningful interpretation of semantic shifts through a small number of refined axes. Source code is available at this https URL .</li>
</ul>

<h3>Title: FedDAF: Federated Domain Adaptation Using Model Functional Distance</h3>
<ul>
<li><strong>Authors: </strong>Mrinmay Sen, Ankita Das, Sidhant Nair, C Krishna Mohan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11819">https://arxiv.org/abs/2509.11819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11819">https://arxiv.org/pdf/2509.11819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11819]] FedDAF: Federated Domain Adaptation Using Model Functional Distance(https://arxiv.org/abs/2509.11819)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated Domain Adaptation (FDA) is a federated learning (FL) approach that improves model performance at the target client by collaborating with source clients while preserving data privacy. FDA faces two primary challenges: domain shifts between source and target data and limited labeled data at the target. Most existing FDA methods focus on domain shifts, assuming ample target data, yet often neglect the combined challenges of both domain shifts and data scarcity. Moreover, approaches that address both challenges fail to prioritize sharing relevant information from source clients according to the target's objective. In this paper, we propose FedDAF, a novel approach addressing both challenges in FDA. FedDAF uses similarity-based aggregation of the global source model and target model by calculating model functional distance from their mean gradient fields computed on target data. This enables effective model aggregation based on the target objective, constructed using target data, even with limited data. While computing model functional distance between these two models, FedDAF computes the angle between their mean gradient fields and then normalizes with the Gompertz function. To construct the global source model, all the local source models are aggregated using simple average in the server. Experiments on real-world datasets demonstrate FedDAF's superiority over existing FL, PFL, and FDA methods in terms of achieving better test accuracy.</li>
</ul>

<h3>Title: Off-Path TCP Exploits: PMTUD Breaks TCP Connection Isolation in IP Address Sharing Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Xuewei Feng, Zhaoxi Li, Qi Li, Ziqiang Wang, Kun Sun, Ke Xu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11833">https://arxiv.org/abs/2509.11833</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11833">https://arxiv.org/pdf/2509.11833</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11833]] Off-Path TCP Exploits: PMTUD Breaks TCP Connection Isolation in IP Address Sharing Scenarios(https://arxiv.org/abs/2509.11833)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Path MTU Discovery (PMTUD) and IP address sharing are integral aspects of modern Internet infrastructure. In this paper, we investigate the security vulnerabilities associated with PMTUD within the context of prevalent IP address sharing practices. We reveal that PMTUD is inadequately designed to handle IP address sharing, creating vulnerabilities that attackers can exploit to perform off-path TCP hijacking attacks. We demonstrate that by observing the path MTU value determined by a server for a public IP address (shared among multiple devices), an off-path attacker on the Internet, in collaboration with a malicious device, can infer the sequence numbers of TCP connections established by other legitimate devices sharing the same IP address. This vulnerability enables the attacker to perform off-path TCP hijacking attacks, significantly compromising the security of the affected TCP connections. Our attack involves first identifying a target TCP connection originating from the shared IP address, followed by inferring the sequence numbers of the identified connection. We thoroughly assess the impacts of our attack under various network configurations. Experimental results reveal that the attack can be executed within an average time of 220 seconds, achieving a success rate of 70%.Case studies, including SSH DoS, FTP traffic poisoning, and HTTP injection, highlight the threat it poses to various applications. Additionally, we evaluate our attack across 50 real-world networks with IP address sharing--including public Wi-Fi, VPNs, and 5G--and find 38 vulnerable. Finally, we responsibly disclose the vulnerabilities, receive recognition from organizations such as IETF, Linux, and Cisco, and propose our countermeasures.</li>
</ul>

<h3>Title: A Practical Adversarial Attack against Sequence-based Deep Learning Malware Classifiers</h3>
<ul>
<li><strong>Authors: </strong>Kai Tan, Dongyang Zhan, Lin Ye, Hongli Zhang, Binxing Fang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11836">https://arxiv.org/abs/2509.11836</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11836">https://arxiv.org/pdf/2509.11836</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11836]] A Practical Adversarial Attack against Sequence-based Deep Learning Malware Classifiers(https://arxiv.org/abs/2509.11836)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Sequence-based deep learning models (e.g., RNNs), can detect malware by analyzing its behavioral sequences. Meanwhile, these models are susceptible to adversarial attacks. Attackers can create adversarial samples that alter the sequence characteristics of behavior sequences to deceive malware classifiers. The existing methods for generating adversarial samples typically involve deleting or replacing crucial behaviors in the original data sequences, or inserting benign behaviors that may violate the behavior constraints. However, these methods that directly manipulate sequences make adversarial samples difficult to implement or apply in practice. In this paper, we propose an adversarial attack approach based on Deep Q-Network and a heuristic backtracking search strategy, which can generate perturbation sequences that satisfy practical conditions for successful attacks. Subsequently, we utilize a novel transformation approach that maps modifications back to the source code, thereby avoiding the need to directly modify the behavior log sequences. We conduct an evaluation of our approach, and the results confirm its effectiveness in generating adversarial samples from real-world malware behavior sequences, which have a high success rate in evading anomaly detection models. Furthermore, our approach is practical and can generate adversarial samples while maintaining the functionality of the modified software.</li>
</ul>

<h3>Title: Probabilistic Robustness Analysis in High Dimensional Space: Application to Semantic Segmentation Network</h3>
<ul>
<li><strong>Authors: </strong>Navid Hashemi, Samuel Sasaki, Diego Manzanas Lopez, Ipek Oguz, Meiyi Ma, Taylor T. Johnson</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11838">https://arxiv.org/abs/2509.11838</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11838">https://arxiv.org/pdf/2509.11838</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11838]] Probabilistic Robustness Analysis in High Dimensional Space: Application to Semantic Segmentation Network(https://arxiv.org/abs/2509.11838)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Semantic segmentation networks (SSNs) play a critical role in domains such as medical imaging, autonomous driving, and environmental monitoring, where safety hinges on reliable model behavior under uncertainty. Yet, existing probabilistic verification approaches struggle to scale with the complexity and dimensionality of modern segmentation tasks, often yielding guarantees that are too conservative to be practical. We introduce a probabilistic verification framework that is both architecture-agnostic and scalable to high-dimensional outputs. Our approach combines sampling-based reachability analysis with conformal inference (CI) to deliver provable guarantees while avoiding the excessive conservatism of prior methods. To counteract CI's limitations in high-dimensional settings, we propose novel strategies that reduce conservatism without compromising rigor. Empirical evaluation on large-scale segmentation models across CamVid, OCTA-500, Lung Segmentation, and Cityscapes demonstrates that our method provides reliable safety guarantees while substantially tightening bounds compared to SOTA. We also provide a toolbox implementing this technique, available on Github.</li>
</ul>

<h3>Title: Synthetic Captions for Open-Vocabulary Zero-Shot Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Tim Lebailly, Vijay Veerabadran, Satwik Kottur, Karl Ridgeway, Michael Louis Iuzzolino</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11840">https://arxiv.org/abs/2509.11840</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11840">https://arxiv.org/pdf/2509.11840</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11840]] Synthetic Captions for Open-Vocabulary Zero-Shot Segmentation(https://arxiv.org/abs/2509.11840)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, segmentation</a></li>
<li><strong>Abstract: </strong>Generative vision-language models (VLMs) exhibit strong high-level image understanding but lack spatially dense alignment between vision and language modalities, as our findings indicate. Orthogonal to advancements in generative VLMs, another line of research has focused on representation learning for vision-language alignment, targeting zero-shot inference for dense tasks like segmentation. In this work, we bridge these two directions by densely aligning images with synthetic descriptions generated by VLMs. Synthetic captions are inexpensive, scalable, and easy to generate, making them an excellent source of high-level semantic understanding for dense alignment methods. Empirically, our approach outperforms prior work on standard zero-shot open-vocabulary segmentation benchmarks/datasets, while also being more data-efficient.</li>
</ul>

<h3>Title: Transparent and Fair Profiling in Employment Services: Evidence from Switzerland</h3>
<ul>
<li><strong>Authors: </strong>Tim Räz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11847">https://arxiv.org/abs/2509.11847</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11847">https://arxiv.org/pdf/2509.11847</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11847]] Transparent and Fair Profiling in Employment Services: Evidence from Switzerland(https://arxiv.org/abs/2509.11847)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, interpretability</a></li>
<li><strong>Abstract: </strong>Long-term unemployment (LTU) is a challenge for both jobseekers and public employment services. Statistical profiling tools are increasingly used to predict LTU risk. Some profiling tools are opaque, black-box machine learning models, which raise issues of transparency and fairness. This paper investigates whether interpretable models could serve as an alternative, using administrative data from Switzerland. Traditional statistical, interpretable, and black-box models are compared in terms of predictive performance, interpretability, and fairness. It is shown that explainable boosting machines, a recent interpretable model, perform nearly as well as the best black-box models. It is also shown how model sparsity, feature smoothing, and fairness mitigation can enhance transparency and fairness with only minor losses in performance. These findings suggest that interpretable profiling provides an accountable and trustworthy alternative to black-box models without compromising performance.</li>
</ul>

<h3>Title: Segmentation-Driven Initialization for Sparse-view 3D Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Yi-Hsin Li, Thomas Sikora, Sebastian Knorr, Måarten Sjöström</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11853">https://arxiv.org/abs/2509.11853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11853">https://arxiv.org/pdf/2509.11853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11853]] Segmentation-Driven Initialization for Sparse-view 3D Gaussian Splatting(https://arxiv.org/abs/2509.11853)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Sparse-view synthesis remains a challenging problem due to the difficulty of recovering accurate geometry and appearance from limited observations. While recent advances in 3D Gaussian Splatting (3DGS) have enabled real-time rendering with competitive quality, existing pipelines often rely on Structure-from-Motion (SfM) for camera pose estimation, an approach that struggles in genuinely sparse-view settings. Moreover, several SfM-free methods replace SfM with multi-view stereo (MVS) models, but generate massive numbers of 3D Gaussians by back-projecting every pixel into 3D space, leading to high memory costs. We propose Segmentation-Driven Initialization for Gaussian Splatting (SDI-GS), a method that mitigates inefficiency by leveraging region-based segmentation to identify and retain only structurally significant regions. This enables selective downsampling of the dense point cloud, preserving scene fidelity while substantially reducing Gaussian count. Experiments across diverse benchmarks show that SDI-GS reduces Gaussian count by up to 50% and achieves comparable or superior rendering quality in PSNR and SSIM, with only marginal degradation in LPIPS. It further enables faster training and lower memory footprint, advancing the practicality of 3DGS for constrained-view scenarios.</li>
</ul>

<h3>Title: MOOM: Maintenance, Organization and Optimization of Memory in Ultra-Long Role-Playing Dialogues</h3>
<ul>
<li><strong>Authors: </strong>Weishu Chen, Jinyi Tang, Zhouhui Hou, Shihao Han, Mingjie Zhan, Zhiyuan Huang, Delong Liu, Jiawei Guo, Zhicheng Zhao, Fei Su</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11860">https://arxiv.org/abs/2509.11860</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11860">https://arxiv.org/pdf/2509.11860</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11860]] MOOM: Maintenance, Organization and Optimization of Memory in Ultra-Long Role-Playing Dialogues(https://arxiv.org/abs/2509.11860)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Memory extraction is crucial for maintaining coherent ultra-long dialogues in human-robot role-playing scenarios. However, existing methods often exhibit uncontrolled memory growth. To address this, we propose MOOM, the first dual-branch memory plugin that leverages literary theory by modeling plot development and character portrayal as core storytelling elements. Specifically, one branch summarizes plot conflicts across multiple time scales, while the other extracts the user's character profile. MOOM further integrates a forgetting mechanism, inspired by the ``competition-inhibition'' memory theory, to constrain memory capacity and mitigate uncontrolled growth. Furthermore, we present ZH-4O, a Chinese ultra-long dialogue dataset specifically designed for role-playing, featuring dialogues that average 600 turns and include manually annotated memory information. Experimental results demonstrate that MOOM outperforms all state-of-the-art memory extraction methods, requiring fewer large language model invocations while maintaining a controllable memory capacity.</li>
</ul>

<h3>Title: Bridging Vision Language Models and Symbolic Grounding for Video Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Haodi Ma, Vyom Pathak, Daisy Zhe Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11862">https://arxiv.org/abs/2509.11862</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11862">https://arxiv.org/pdf/2509.11862</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11862]] Bridging Vision Language Models and Symbolic Grounding for Video Question Answering(https://arxiv.org/abs/2509.11862)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Video Question Answering (VQA) requires models to reason over spatial, temporal, and causal cues in videos. Recent vision language models (VLMs) achieve strong results but often rely on shallow correlations, leading to weak temporal grounding and limited interpretability. We study symbolic scene graphs (SGs) as intermediate grounding signals for VQA. SGs provide structured object-relation representations that complement VLMs holistic reasoning. We introduce SG-VLM, a modular framework that integrates frozen VLMs with scene graph grounding via prompting and visual localization. Across three benchmarks (NExT-QA, iVQA, ActivityNet-QA) and multiple VLMs (QwenVL, InternVL), SG-VLM improves causal and temporal reasoning and outperforms prior baselines, though gains over strong VLMs are limited. These findings highlight both the promise and current limitations of symbolic grounding, and offer guidance for future hybrid VLM-symbolic approaches in video understanding.</li>
</ul>

<h3>Title: NeuroStrike: Neuron-Level Attacks on Aligned LLMs</h3>
<ul>
<li><strong>Authors: </strong>Lichao Wu, Sasha Behrouzi, Mohamadreza Rostami, Maximilian Thang, Stjepan Picek, Ahmad-Reza Sadeghi</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11864">https://arxiv.org/abs/2509.11864</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11864">https://arxiv.org/pdf/2509.11864</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11864]] NeuroStrike: Neuron-Level Attacks on Aligned LLMs(https://arxiv.org/abs/2509.11864)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Safety alignment is critical for the ethical deployment of large language models (LLMs), guiding them to avoid generating harmful or unethical content. Current alignment techniques, such as supervised fine-tuning and reinforcement learning from human feedback, remain fragile and can be bypassed by carefully crafted adversarial prompts. Unfortunately, such attacks rely on trial and error, lack generalizability across models, and are constrained by scalability and reliability. This paper presents NeuroStrike, a novel and generalizable attack framework that exploits a fundamental vulnerability introduced by alignment techniques: the reliance on sparse, specialized safety neurons responsible for detecting and suppressing harmful inputs. We apply NeuroStrike to both white-box and black-box settings: In the white-box setting, NeuroStrike identifies safety neurons through feedforward activation analysis and prunes them during inference to disable safety mechanisms. In the black-box setting, we propose the first LLM profiling attack, which leverages safety neuron transferability by training adversarial prompt generators on open-weight surrogate models and then deploying them against black-box and proprietary targets. We evaluate NeuroStrike on over 20 open-weight LLMs from major LLM developers. By removing less than 0.6% of neurons in targeted layers, NeuroStrike achieves an average attack success rate (ASR) of 76.9% using only vanilla malicious prompts. Moreover, Neurostrike generalizes to four multimodal LLMs with 100% ASR on unsafe image inputs. Safety neurons transfer effectively across architectures, raising ASR to 78.5% on 11 fine-tuned models and 77.7% on five distilled models. The black-box LLM profiling attack achieves an average ASR of 63.7% across five black-box models, including the Google Gemini family.</li>
</ul>

<h3>Title: Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding</h3>
<ul>
<li><strong>Authors: </strong>Meng Luo, Shengqiong Wu, Liqiang Jing, Tianjie Ju, Li Zheng, Jinxiang Lai, Tianlong Wu, Xinya Du, Jian Li, Siyuan Yan, Jiebo Luo, William Yang Wang, Hao Fei, Mong-Li Lee, Wynne Hsu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11866">https://arxiv.org/abs/2509.11866</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11866">https://arxiv.org/pdf/2509.11866</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11866]] Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding(https://arxiv.org/abs/2509.11866)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Recent advancements in large video models (LVMs) have significantly enhance video understanding. However, these models continue to suffer from hallucinations, producing content that conflicts with input videos. To address this issue, we propose Dr.V, a hierarchical framework covering perceptive, temporal, and cognitive levels to diagnose video hallucination by fine-grained spatial-temporal grounding. Dr.V comprises of two key components: a benchmark dataset Dr.V-Bench and a satellite video agent Dr.V-Agent. Dr.V-Bench includes 10k instances drawn from 4,974 videos spanning diverse tasks, each enriched with detailed spatial-temporal annotation. Dr.V-Agent detects hallucinations in LVMs by systematically applying fine-grained spatial-temporal grounding at the perceptive and temporal levels, followed by cognitive level reasoning. This step-by-step pipeline mirrors human-like video comprehension and effectively identifies hallucinations. Extensive experiments demonstrate that Dr.V-Agent is effective in diagnosing hallucination while enhancing interpretability and reliability, offering a practical blueprint for robust video understanding in real-world scenarios. All our data and code are available at this https URL.</li>
</ul>

<h3>Title: Growing Perspectives: Modelling Embodied Perspective Taking and Inner Narrative Development Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sabrina Patania, Luca Annese, Anna Lambiase, Anita Pellegrini, Tom Foulsham, Azzurra Ruggeri, Silvia Rossi, Silvia Serino, Dimitri Ognibene</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11868">https://arxiv.org/abs/2509.11868</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11868">https://arxiv.org/pdf/2509.11868</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11868]] Growing Perspectives: Modelling Embodied Perspective Taking and Inner Narrative Development Using Large Language Models(https://arxiv.org/abs/2509.11868)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Language and embodied perspective taking are essential for human collaboration, yet few computational models address both simultaneously. This work investigates the PerspAct system [1], which integrates the ReAct (Reason and Act) paradigm with Large Language Models (LLMs) to simulate developmental stages of perspective taking, grounded in Selman's theory [2]. Using an extended director task, we evaluate GPT's ability to generate internal narratives aligned with specified developmental stages, and assess how these influence collaborative performance both qualitatively (action selection) and quantitatively (task efficiency). Results show that GPT reliably produces developmentally-consistent narratives before task execution but often shifts towards more advanced stages during interaction, suggesting that language exchanges help refine internal representations. Higher developmental stages generally enhance collaborative effectiveness, while earlier stages yield more variable outcomes in complex contexts. These findings highlight the potential of integrating embodied perspective taking and language in LLMs to better model developmental dynamics and stress the importance of evaluating internal speech during combined linguistic and embodied tasks.</li>
</ul>

<h3>Title: Efficient Byzantine-Robust Privacy-Preserving Federated Learning via Dimension Compression</h3>
<ul>
<li><strong>Authors: </strong>Xian Qin, Xue Yang, Xiaohu Tang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11870">https://arxiv.org/abs/2509.11870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11870">https://arxiv.org/pdf/2509.11870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11870]] Efficient Byzantine-Robust Privacy-Preserving Federated Learning via Dimension Compression(https://arxiv.org/abs/2509.11870)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, defense, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) allows collaborative model training across distributed clients without sharing raw data, thus preserving privacy. However, the system remains vulnerable to privacy leakage from gradient updates and Byzantine attacks from malicious clients. Existing solutions face a critical trade-off among privacy preservation, Byzantine robustness, and computational efficiency. We propose a novel scheme that effectively balances these competing objectives by integrating homomorphic encryption with dimension compression based on the Johnson-Lindenstrauss transformation. Our approach employs a dual-server architecture that enables secure Byzantine defense in the ciphertext domain while dramatically reducing computational overhead through gradient compression. The dimension compression technique preserves the geometric relationships necessary for Byzantine defence while reducing computation complexity from $O(dn)$ to $O(kn)$ cryptographic operations, where $k \ll d$. Extensive experiments across diverse datasets demonstrate that our approach maintains model accuracy comparable to non-private FL while effectively defending against Byzantine clients comprising up to $40\%$ of the network.</li>
</ul>

<h3>Title: Do It Yourself (DIY): Modifying Images for Poems in a Zero-Shot Setting Using Weighted Prompt Manipulation</h3>
<ul>
<li><strong>Authors: </strong>Sofia Jamil, Kotla Sai Charan, Sriparna Saha, Koustava Goswami, K J Joseph</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11878">https://arxiv.org/abs/2509.11878</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11878">https://arxiv.org/pdf/2509.11878</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11878]] Do It Yourself (DIY): Modifying Images for Poems in a Zero-Shot Setting Using Weighted Prompt Manipulation(https://arxiv.org/abs/2509.11878)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>Poetry is an expressive form of art that invites multiple interpretations, as readers often bring their own emotions, experiences, and cultural backgrounds into their understanding of a poem. Recognizing this, we aim to generate images for poems and improve these images in a zero-shot setting, enabling audiences to modify images as per their requirements. To achieve this, we introduce a novel Weighted Prompt Manipulation (WPM) technique, which systematically modifies attention weights and text embeddings within diffusion models. By dynamically adjusting the importance of specific words, WPM enhances or suppresses their influence in the final generated image, leading to semantically richer and more contextually accurate visualizations. Our approach exploits diffusion models and large language models (LLMs) such as GPT in conjunction with existing poetry datasets, ensuring a comprehensive and structured methodology for improved image generation in the literary domain. To the best of our knowledge, this is the first attempt at integrating weighted prompt manipulation for enhancing imagery in poetic language.</li>
</ul>

<h3>Title: BREA-Depth: Bronchoscopy Realistic Airway-geometric Depth Estimation</h3>
<ul>
<li><strong>Authors: </strong>Francis Xiatian Zhang, Emile Mackute, Mohammadreza Kasaei, Kevin Dhaliwal, Robert Thomson, Mohsen Khadem</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11885">https://arxiv.org/abs/2509.11885</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11885">https://arxiv.org/pdf/2509.11885</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11885]] BREA-Depth: Bronchoscopy Realistic Airway-geometric Depth Estimation(https://arxiv.org/abs/2509.11885)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Monocular depth estimation in bronchoscopy can significantly improve real-time navigation accuracy and enhance the safety of interventions in complex, branching airways. Recent advances in depth foundation models have shown promise for endoscopic scenarios, yet these models often lack anatomical awareness in bronchoscopy, overfitting to local textures rather than capturing the global airway structure, particularly under ambiguous depth cues and poor lighting. To address this, we propose Brea-Depth, a novel framework that integrates airway-specific geometric priors into foundation model adaptation for bronchoscopic depth estimation. Our method introduces a depth-aware CycleGAN, refining the translation between real bronchoscopic images and airway geometries from anatomical data, effectively bridging the domain gap. In addition, we introduce an airway structure awareness loss to enforce depth consistency within the airway lumen while preserving smooth transitions and structural integrity. By incorporating anatomical priors, Brea-Depth enhances model generalization and yields more robust, accurate 3D airway reconstructions. To assess anatomical realism, we introduce Airway Depth Structure Evaluation, a new metric for structural consistency. We validate BREA-Depth on a collected ex vivo human lung dataset and an open bronchoscopic dataset, where it outperforms existing methods in anatomical depth preservation.</li>
</ul>

<h3>Title: Logit Mixture Outlier Exposure for Fine-grained Out-of-Distribution Detection</h3>
<ul>
<li><strong>Authors: </strong>Akito Shinohara, Kohei Fukuda, Hiroaki Aizawa</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11892">https://arxiv.org/abs/2509.11892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11892">https://arxiv.org/pdf/2509.11892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11892]] Logit Mixture Outlier Exposure for Fine-grained Out-of-Distribution Detection(https://arxiv.org/abs/2509.11892)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The ability to detect out-of-distribution data is essential not only for ensuring robustness against unknown or unexpected input data but also for improving the generalization performance of the model. Among various out-of-distribution detection methods, Outlier Exposure and Mixture Outlier Exposure are promising approaches that enhance out-of-distribution detection performance by exposing the outlier data during training. However, even with these sophisticated techniques, it remains challenging for models to learn the relationships between classes effectively and to distinguish data sampling from in-distribution and out-of-distribution clearly. Therefore, we focus on the logit space, where the properties between class-wise distributions are distinctly separated from those in the input or feature spaces. Specifically, we propose a linear interpolation technique in the logit space that mixes in-distribution and out-of-distribution data to facilitate smoothing logits between classes and improve the out-of-distribution detection performance, particularly for out-of-distribution data that lie close to the in-distribution data. Additionally, we enforce consistency between the logits obtained through mixing in the logit space and those generated via mixing in the input space. Our experiments demonstrate that our logit-space mixing technique reduces the abrupt fluctuations in the model outputs near the decision boundaries, resulting in smoother and more reliable separation between in-distribution and out-of-distribution data. Furthermore, we evaluate the effectiveness of the proposed method on a fine-grained out-of-distribution detection task.</li>
</ul>

<h3>Title: Uncertainty in Authorship: Why Perfect AI Detection Is Mathematically Impossible</h3>
<ul>
<li><strong>Authors: </strong>Aadil Gani Ganie</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11915">https://arxiv.org/abs/2509.11915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11915">https://arxiv.org/pdf/2509.11915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11915]] Uncertainty in Authorship: Why Perfect AI Detection Is Mathematically Impossible(https://arxiv.org/abs/2509.11915)</code><input type="text"></li>
<li><strong>Keywords: </strong>watermark, large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) become more advanced, it is increasingly difficult to distinguish between human-written and AI-generated text. This paper draws a conceptual parallel between quantum uncertainty and the limits of authorship detection in natural language. We argue that there is a fundamental trade-off: the more confidently one tries to identify whether a text was written by a human or an AI, the more one risks disrupting the text's natural flow and authenticity. This mirrors the tension between precision and disturbance found in quantum systems. We explore how current detection methods--such as stylometry, watermarking, and neural classifiers--face inherent limitations. Enhancing detection accuracy often leads to changes in the AI's output, making other features less reliable. In effect, the very act of trying to detect AI authorship introduces uncertainty elsewhere in the text. Our analysis shows that when AI-generated text closely mimics human writing, perfect detection becomes not just technologically difficult but theoretically impossible. We address counterarguments and discuss the broader implications for authorship, ethics, and policy. Ultimately, we suggest that the challenge of AI-text detection is not just a matter of better tools--it reflects a deeper, unavoidable tension in the nature of language itself.</li>
</ul>

<h3>Title: NeuroGaze-Distill: Brain-informed Distillation and Depression-Inspired Geometric Priors for Robust Facial Emotion Recognition</h3>
<ul>
<li><strong>Authors: </strong>Zilin Li, Weiwei Xu, Xuanqi Zhao, Yiran Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11916">https://arxiv.org/abs/2509.11916</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11916">https://arxiv.org/pdf/2509.11916</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11916]] NeuroGaze-Distill: Brain-informed Distillation and Depression-Inspired Geometric Priors for Robust Facial Emotion Recognition(https://arxiv.org/abs/2509.11916)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Facial emotion recognition (FER) models trained only on pixels often fail to generalize across datasets because facial appearance is an indirect and biased proxy for underlying affect. We present NeuroGaze-Distill, a cross-modal distillation framework that transfers brain-informed priors into an image-only FER student via static Valence/Arousal (V/A) prototypes and a depression-inspired geometric prior (D-Geo). A teacher trained on EEG topographic maps from DREAMER (with MAHNOB-HCI as unlabeled support) produces a consolidated 5x5 V/A prototype grid that is frozen and reused; no EEG-face pairing and no non-visual signals at deployment are required. The student (ResNet-18/50) is trained on FERPlus with conventional CE/KD and two lightweight regularizers: (i) Proto-KD (cosine) aligns student features to the static prototypes; (ii) D-Geo softly shapes the embedding geometry in line with affective findings often reported in depression research (e.g., anhedonia-like contraction in high-valence regions). We evaluate both within-domain (FERPlus validation) and cross-dataset protocols (AffectNet-mini; optional CK+), reporting standard 8-way scores alongside present-only Macro-F1 and balanced accuracy to fairly handle label-set mismatch. Ablations attribute consistent gains to prototypes and D-Geo, and favor 5x5 over denser grids for stability. The method is simple, deployable, and improves robustness without architectural complexity.</li>
</ul>

<h3>Title: Designing LLMs for cultural sensitivity: Evidence from English-Japanese translation</h3>
<ul>
<li><strong>Authors: </strong>Helene Tenzer, Oumnia Abidi, Stefan Feuerriegel</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11921">https://arxiv.org/abs/2509.11921</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11921">https://arxiv.org/pdf/2509.11921</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11921]] Designing LLMs for cultural sensitivity: Evidence from English-Japanese translation(https://arxiv.org/abs/2509.11921)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly used in everyday communication, including multilingual interactions across different cultural contexts. While LLMs can now generate near-perfect literal translations, it remains unclear whether LLMs support culturally appropriate communication. In this paper, we analyze the cultural sensitivity of different LLM designs when applied to English-Japanese translations of workplace e-mails. Here, we vary the prompting strategies: (1) naive "just translate" prompts, (2) audience-targeted prompts specifying the recipient's cultural background, and (3) instructional prompts with explicit guidance on Japanese communication norms. Using a mixed-methods study, we then analyze culture-specific language patterns to evaluate how well translations adapt to cultural norms. Further, we examine the appropriateness of the tone of the translations as perceived by native speakers. We find that culturally-tailored prompting can improve cultural fit, based on which we offer recommendations for designing culturally inclusive LLMs in multilingual settings.</li>
</ul>

<h3>Title: zkToken: Empowering Holders to Limit Revocation Checks for Verifiable Credentials</h3>
<ul>
<li><strong>Authors: </strong>Praveensankar Manimaran, Mayank Raikwar, Thiago Garrett, Arlindo F. da Conceição, Leander Jehl, Roman Vitenberg</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11934">https://arxiv.org/abs/2509.11934</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11934">https://arxiv.org/pdf/2509.11934</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11934]] zkToken: Empowering Holders to Limit Revocation Checks for Verifiable Credentials(https://arxiv.org/abs/2509.11934)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Systems managing Verifiable Credentials are becoming increasingly popular. Unfortunately, their support for revoking previously issued credentials allows verifiers to effectively monitor the validity of the credentials, which is sensitive information. While the issue started to gain recognition, no adequate solution has been proposed so far. In this work, we propose a novel framework for time-limited continuous verification. The holder is able to individually configure the verification period when sharing information with the verifier, and the system guarantees proven untraceability of the revocation status after the verification period expires. Different from existing systems, the implementation adopts a more scalable blacklist approach where tokens corresponding to revoked credentials are stored in the registry. The approach employs ZK proofs that allow holders to prove non-membership in the blacklist. In addition to theoretically proving security, we evaluate the approach analytically and experimentally and show that it significantly improves bandwidth consumption on the holder while being on par with state-of-the-art solutions with respect to the other performance metrics.</li>
</ul>

<h3>Title: Sphere-GAN: a GAN-based Approach for Saliency Estimation in 360° Videos</h3>
<ul>
<li><strong>Authors: </strong>Mahmoud Z. A. Wahba, Sara Baldoni, Federica Battisti</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11948">https://arxiv.org/abs/2509.11948</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11948">https://arxiv.org/pdf/2509.11948</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11948]] Sphere-GAN: a GAN-based Approach for Saliency Estimation in 360° Videos(https://arxiv.org/abs/2509.11948)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The recent success of immersive applications is pushing the research community to define new approaches to process 360° images and videos and optimize their transmission. Among these, saliency estimation provides a powerful tool that can be used to identify visually relevant areas and, consequently, adapt processing algorithms. Although saliency estimation has been widely investigated for 2D content, very few algorithms have been proposed for 360° saliency estimation. Towards this goal, we introduce Sphere-GAN, a saliency detection model for 360° videos that leverages a Generative Adversarial Network with spherical convolutions. Extensive experiments were conducted using a public 360° video saliency dataset, and the results demonstrate that Sphere-GAN outperforms state-of-the-art models in accurately predicting saliency maps.</li>
</ul>

<h3>Title: CLAIRE: A Dual Encoder Network with RIFT Loss and Phi-3 Small Language Model Based Interpretability for Cross-Modality Synthetic Aperture Radar and Optical Land Cover Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Debopom Sutradhar, Arefin Ittesafun Abian, Mohaimenul Azam Khan Raiaan, Reem E. Mohamed, Sheikh Izzal Azid, Sami Azam</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11952">https://arxiv.org/abs/2509.11952</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11952">https://arxiv.org/pdf/2509.11952</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11952]] CLAIRE: A Dual Encoder Network with RIFT Loss and Phi-3 Small Language Model Based Interpretability for Cross-Modality Synthetic Aperture Radar and Optical Land Cover Segmentation(https://arxiv.org/abs/2509.11952)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, segmentation</a></li>
<li><strong>Abstract: </strong>Accurate land cover classification from satellite imagery is crucial in environmental monitoring and sustainable resource management. However, it remains challenging due to the complexity of natural landscapes, the visual similarity between classes, and the significant class imbalance in the available datasets. To address these issues, we propose a dual encoder architecture that independently extracts modality-specific features from optical and Synthetic Aperture Radar (SAR) imagery, which are then fused using a cross-modality attention-fusion module named Cross-modality Land cover segmentation with Attention and Imbalance- aware Reasoning-Enhanced Explanations (CLAIRE). This fusion mechanism highlights complementary spatial and textural features, enabling the network to better capture detailed and diverse land cover patterns. We incorporate a hybrid loss function that utilizes Weighted Focal Loss and Tversky Loss named RIFT (Rare-Instance Focal-Tversky) to address class imbalance and improve segmentation performance across underrepresented categories. Our model achieves competitive performance across multiple benchmarks: a mean Intersection over Union (mIoU) of 56.02% and Overall Accuracy (OA) of 84.56% on the WHU-OPT-SAR dataset; strong generalization with a mIoU of 59.89% and OA of 73.92% on the OpenEarthMap-SAR dataset; and remarkable robustness under cloud-obstructed conditions, achieving an mIoU of 86.86% and OA of 94.58% on the PIE-RGB-SAR dataset. Additionally, we introduce a metric-driven reasoning module generated by a Small Language Model (Phi-3), which generates expert-level, sample-specific justifications for model predictions, thereby enhancing transparency and interpretability.</li>
</ul>

<h3>Title: Learning to Generate 4D LiDAR Sequences</h3>
<ul>
<li><strong>Authors: </strong>Ao Liang, Youquan Liu, Yu Yang, Dongyue Lu, Linfeng Li, Lingdong Kong, Huaici Zhao, Wei Tsang Ooi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11959">https://arxiv.org/abs/2509.11959</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11959">https://arxiv.org/pdf/2509.11959</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11959]] Learning to Generate 4D LiDAR Sequences(https://arxiv.org/abs/2509.11959)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, diffusion, generative</a></li>
<li><strong>Abstract: </strong>While generative world models have advanced video and occupancy-based data synthesis, LiDAR generation remains underexplored despite its importance for accurate 3D perception. Extending generation to 4D LiDAR data introduces challenges in controllability, temporal stability, and evaluation. We present LiDARCrafter, a unified framework that converts free-form language into editable LiDAR sequences. Instructions are parsed into ego-centric scene graphs, which a tri-branch diffusion model transforms into object layouts, trajectories, and shapes. A range-image diffusion model generates the initial scan, and an autoregressive module extends it into a temporally coherent sequence. The explicit layout design further supports object-level editing, such as insertion or relocation. To enable fair assessment, we provide EvalSuite, a benchmark spanning scene-, object-, and sequence-level metrics. On nuScenes, LiDARCrafter achieves state-of-the-art fidelity, controllability, and temporal consistency, offering a foundation for LiDAR-based simulation and data augmentation.</li>
</ul>

<h3>Title: ToolRM: Outcome Reward Models for Tool-Calling Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mayank Agarwal, Ibrahim Abdelaziz, Kinjal Basu, Merve Unuvar, Luis A. Lastras, Yara Rizk, Pavan Kapanipathi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11963">https://arxiv.org/abs/2509.11963</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11963">https://arxiv.org/pdf/2509.11963</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11963]] ToolRM: Outcome Reward Models for Tool-Calling Large Language Models(https://arxiv.org/abs/2509.11963)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) increasingly interact with external tools, reward modeling for tool use has become a critical yet underexplored area. Existing reward models, trained primarily on natural language outputs, struggle to evaluate tool-based reasoning and execution. To quantify this gap, we introduce FC-RewardBench, the first benchmark designed to systematically assess reward models' performance in tool-calling scenarios. Our analysis shows that current reward models often miss key signals of effective tool use, highlighting the need for domain-specific modeling. To address this, we propose a training framework for outcome-based reward models using data synthesized from permissively licensed, open-weight LLMs. We train models ranging from 1.7B to 14B parameters and evaluate them across seven out-of-domain benchmarks. These models consistently outperform general-purpose baselines, achieving up to 25\% average improvement in downstream task performance and enabling data-efficient fine-tuning through reward-guided filtering.</li>
</ul>

<h3>Title: Deep operator network for surrogate modeling of poroelasticity with random permeability fields</h3>
<ul>
<li><strong>Authors: </strong>Sangjoon Park, Yeonjong Shin, Jinhyun Choo</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.geo-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11966">https://arxiv.org/abs/2509.11966</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11966">https://arxiv.org/pdf/2509.11966</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11966]] Deep operator network for surrogate modeling of poroelasticity with random permeability fields(https://arxiv.org/abs/2509.11966)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Poroelasticity -- coupled fluid flow and elastic deformation in porous media -- often involves spatially variable permeability, especially in subsurface systems. In such cases, simulations with random permeability fields are widely used for probabilistic analysis, uncertainty quantification, and inverse problems. These simulations require repeated forward solves that are often prohibitively expensive, motivating the development of efficient surrogate models. However, efficient surrogate modeling techniques for poroelasticity with random permeability fields remain scarce. In this study, we propose a surrogate modeling framework based on the deep operator network (DeepONet), a neural architecture designed to learn mappings between infinite-dimensional function spaces. The proposed surrogate model approximates the solution operator that maps random permeability fields to transient poroelastic responses. To enhance predictive accuracy and stability, we integrate three strategies: nondimensionalization of the governing equations, input dimensionality reduction via Karhunen--Loéve expansion, and a two-step training procedure that decouples the optimization of branch and trunk networks. The methodology is evaluated on two benchmark problems in poroelasticity: soil consolidation and ground subsidence induced by groundwater extraction. In both cases, the DeepONet achieves substantial speedup in inference while maintaining high predictive accuracy across a wide range of permeability statistics. These results highlight the potential of the proposed approach as a scalable and efficient surrogate modeling technique for poroelastic systems with random permeability fields.</li>
</ul>

<h3>Title: MillStone: How Open-Minded Are LLMs?</h3>
<ul>
<li><strong>Authors: </strong>Harold Triedman, Vitaly Shmatikov</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11967">https://arxiv.org/abs/2509.11967</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11967">https://arxiv.org/pdf/2509.11967</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11967]] MillStone: How Open-Minded Are LLMs?(https://arxiv.org/abs/2509.11967)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models equipped with Web search, information retrieval tools, and other agentic capabilities are beginning to supplant traditional search engines. As users start to rely on LLMs for information on many topics, including controversial and debatable issues, it is important to understand how the stances and opinions expressed in LLM outputs are influenced by the documents they use as their information sources. In this paper, we present MillStone, the first benchmark that aims to systematically measure the effect of external arguments on the stances that LLMs take on controversial issues (not all of them political). We apply MillStone to nine leading LLMs and measure how ``open-minded'' they are to arguments supporting opposite sides of these issues, whether different LLMs agree with each other, which arguments LLMs find most persuasive, and whether these arguments are the same for different LLMs. In general, we find that LLMs are open-minded on most issues. An authoritative source of information can easily sway an LLM's stance, highlighting the importance of source selection and the risk that LLM-based information retrieval and search systems can be manipulated.</li>
</ul>

<h3>Title: Poison to Detect: Detection of Targeted Overfitting in Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Soumia Zohra El Mestari, Maciej Krzysztof Zuziak, Gabriele Lenzini</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11974">https://arxiv.org/abs/2509.11974</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11974">https://arxiv.org/pdf/2509.11974</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11974]] Poison to Detect: Detection of Targeted Overfitting in Federated Learning(https://arxiv.org/abs/2509.11974)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) enables collaborative model training across decentralised clients while keeping local data private, making it a widely adopted privacy-enhancing technology (PET). Despite its privacy benefits, FL remains vulnerable to privacy attacks, including those targeting specific clients. In this paper, we study an underexplored threat where a dishonest orchestrator intentionally manipulates the aggregation process to induce targeted overfitting in the local models of specific clients. Whereas many studies in this area predominantly focus on reducing the amount of information leakage during training, we focus on enabling an early client-side detection of targeted overfitting, thereby allowing clients to disengage before significant harm occurs. In line with this, we propose three detection techniques - (a) label flipping, (b) backdoor trigger injection, and (c) model fingerprinting - that enable clients to verify the integrity of the global aggregation. We evaluated our methods on multiple datasets under different attack scenarios. Our results show that the three methods reliably detect targeted overfitting induced by the orchestrator, but they differ in terms of computational complexity, detection latency, and false-positive rates.</li>
</ul>

<h3>Title: Learning from Uncertain Similarity and Unlabeled Data</h3>
<ul>
<li><strong>Authors: </strong>Meng Wei, Zhongnian Li, Peng Ying, Xinzheng Xu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11984">https://arxiv.org/abs/2509.11984</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11984">https://arxiv.org/pdf/2509.11984</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11984]] Learning from Uncertain Similarity and Unlabeled Data(https://arxiv.org/abs/2509.11984)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Existing similarity-based weakly supervised learning approaches often rely on precise similarity annotations between data pairs, which may inadvertently expose sensitive label information and raise privacy risks. To mitigate this issue, we propose Uncertain Similarity and Unlabeled Learning (USimUL), a novel framework where each similarity pair is embedded with an uncertainty component to reduce label leakage. In this paper, we propose an unbiased risk estimator that learns from uncertain similarity and unlabeled data. Additionally, we theoretically prove that the estimator achieves statistically optimal parametric convergence rates. Extensive experiments on both benchmark and real-world datasets show that our method achieves superior classification performance compared to conventional similarity-based approaches.</li>
</ul>

<h3>Title: Text Adaptation to Plain Language and Easy Read via Automatic Post-Editing Cycles</h3>
<ul>
<li><strong>Authors: </strong>Jesús Calleja, David Ponce, Thierry Etchegoyhen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.11991">https://arxiv.org/abs/2509.11991</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.11991">https://arxiv.org/pdf/2509.11991</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.11991]] Text Adaptation to Plain Language and Easy Read via Automatic Post-Editing Cycles(https://arxiv.org/abs/2509.11991)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We describe Vicomtech's participation in the CLEARS challenge on text adaptation to Plain Language and Easy Read in Spanish. Our approach features automatic post-editing of different types of initial Large Language Model adaptations, where successive adaptations are generated iteratively until readability and similarity metrics indicate that no further adaptation refinement can be successfully performed. Taking the average of all official metrics, our submissions achieved first and second place in Plain language and Easy Read adaptation, respectively.</li>
</ul>

<h3>Title: AMQ: Enabling AutoML for Mixed-precision Weight-Only Quantization of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sangjun Lee, Seung-taek Woo, Jungyu Jin, Changhun Lee, Eunhyeok Park</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12019">https://arxiv.org/abs/2509.12019</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12019">https://arxiv.org/pdf/2509.12019</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12019]] AMQ: Enabling AutoML for Mixed-precision Weight-Only Quantization of Large Language Models(https://arxiv.org/abs/2509.12019)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>To enable broader deployment of Large Language Models (LLMs), it is essential to identify the best-performing model under strict memory constraints. We present AMQ, Automated Mixed-Precision Weight-Only Quantization, a framework that assigns layer-wise quantization bit-widths to optimally balance model quality and memory usage. However, the combinatorial search space, with over 10^{100} possible configurations, makes conventional black-box optimization infeasible. AMQ overcomes this challenge through four key innovations:(1) search space pruning using prior knowledge to exclude unpromising configurations, (2) quantization proxy to bypass costly format conversions during search, (3) quality predictor to minimize evaluation overhead, and (4) iterative search-and-update strategy for fast and stable convergence. By integrating these components, AMQ efficiently explores the quality-efficiency landscape, reaching the Pareto frontier and yielding LLMs that are both compact and high-performing. Our code is available at this https URL.</li>
</ul>

<h3>Title: Robust Concept Erasure in Diffusion Models: A Theoretical Perspective on Security and Robustness</h3>
<ul>
<li><strong>Authors: </strong>Zixuan Fu, Yan Ren, Finn Carter, Chenyue Wen, Le Ku, Daheng Yu, Emily Davis, Bo Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12024">https://arxiv.org/abs/2509.12024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12024">https://arxiv.org/pdf/2509.12024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12024]] Robust Concept Erasure in Diffusion Models: A Theoretical Perspective on Security and Robustness(https://arxiv.org/abs/2509.12024)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, privacy, robust, fair, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have achieved unprecedented success in image generation but pose increasing risks in terms of privacy, fairness, and security. A growing demand exists to \emph{erase} sensitive or harmful concepts (e.g., NSFW content, private individuals, artistic styles) from these models while preserving their overall generative capabilities. We introduce \textbf{SCORE} (Secure and Concept-Oriented Robust Erasure), a novel framework for robust concept removal in diffusion models. SCORE formulates concept erasure as an \emph{adversarial independence} problem, theoretically guaranteeing that the model's outputs become statistically independent of the erased concept. Unlike prior heuristic methods, SCORE minimizes the mutual information between a target concept and generated outputs, yielding provable erasure guarantees. We provide formal proofs establishing convergence properties and derive upper bounds on residual concept leakage. Empirically, we evaluate SCORE on Stable Diffusion and FLUX across four challenging benchmarks: object erasure, NSFW removal, celebrity face suppression, and artistic style unlearning. SCORE consistently outperforms state-of-the-art methods including EraseAnything, ANT, MACE, ESD, and UCE, achieving up to \textbf{12.5\%} higher erasure efficacy while maintaining comparable or superior image quality. By integrating adversarial optimization, trajectory consistency, and saliency-driven fine-tuning, SCORE sets a new standard for secure and robust concept erasure in diffusion models.</li>
</ul>

<h3>Title: RAM++: Robust Representation Learning via Adaptive Mask for All-in-One Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Zilong Zhang, Chujie Qin, Chunle Guo, Yong Zhang, Chao Xue, Ming-Ming Cheng, Chongyi Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12039">https://arxiv.org/abs/2509.12039</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12039">https://arxiv.org/pdf/2509.12039</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12039]] RAM++: Robust Representation Learning via Adaptive Mask for All-in-One Image Restoration(https://arxiv.org/abs/2509.12039)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>This work presents Robust Representation Learning via Adaptive Mask (RAM++), a two-stage framework for all-in-one image restoration. RAM++ integrates high-level semantic understanding with low-level texture generation to achieve content-oriented robust restoration. It addresses the limitations of existing degradation-oriented methods in extreme scenarios (e.g., degradations strongly coupled with image structures). RAM++ also mitigates common challenges such as unbalanced performance across tasks, overfitting to seen degradations, and weak generalization to unseen ones through three key designs: 1) Adaptive Semantic-Aware Mask (AdaSAM): a pretraining strategy that applies pixel-level masks to semantically rich and textured regions. This design enables the network to learn both generative priors and image content priors from various degradations. 2) Mask Attribute Conductance (MAC): a selective fine-tuning strategy that adjusts the layers with higher contributions to bridge the integrity gap between masked pretraining and full-image fine-tuning while retaining learned priors. 3) Robust Feature Regularization (RFR): a strategy that leverages DINOv2's semantically consistent and degradation-invariant representations, together with efficient feature fusion, to achieve faithful and semantically coherent restoration. With these designs, RAM++ achieves robust, well-balanced, and state-of-the-art performance across seen, unseen, extreme, and mixed degradations. Our code and model will be released at this https URL</li>
</ul>

<h3>Title: Exploring Efficient Open-Vocabulary Segmentation in the Remote Sensing</h3>
<ul>
<li><strong>Authors: </strong>Bingyu Li, Haocheng Dong, Da Zhang, Zhiyuan Zhao, Junyu Gao, Xuelong Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12040">https://arxiv.org/abs/2509.12040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12040">https://arxiv.org/pdf/2509.12040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12040]] Exploring Efficient Open-Vocabulary Segmentation in the Remote Sensing(https://arxiv.org/abs/2509.12040)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Open-Vocabulary Remote Sensing Image Segmentation (OVRSIS), an emerging task that adapts Open-Vocabulary Segmentation (OVS) to the remote sensing (RS) domain, remains underexplored due to the absence of a unified evaluation benchmark and the domain gap between natural and RS images. To bridge these gaps, we first establish a standardized OVRSIS benchmark (\textbf{OVRSISBench}) based on widely-used RS segmentation datasets, enabling consistent evaluation across methods. Using this benchmark, we comprehensively evaluate several representative OVS/OVRSIS models and reveal their limitations when directly applied to remote sensing scenarios. Building on these insights, we propose \textbf{RSKT-Seg}, a novel open-vocabulary segmentation framework tailored for remote sensing. RSKT-Seg integrates three key components: (1) a Multi-Directional Cost Map Aggregation (RS-CMA) module that captures rotation-invariant visual cues by computing vision-language cosine similarities across multiple directions; (2) an Efficient Cost Map Fusion (RS-Fusion) transformer, which jointly models spatial and semantic dependencies with a lightweight dimensionality reduction strategy; and (3) a Remote Sensing Knowledge Transfer (RS-Transfer) module that injects pre-trained knowledge and facilitates domain adaptation via enhanced upsampling. Extensive experiments on the benchmark show that RSKT-Seg consistently outperforms strong OVS baselines by +3.8 mIoU and +5.9 mACC, while achieving 2x faster inference through efficient aggregation. Our code is \href{this https URL}{\textcolor{blue}{here}}.</li>
</ul>

<h3>Title: Travel Time and Weather-Aware Traffic Forecasting in a Conformal Graph Neural Network Framework</h3>
<ul>
<li><strong>Authors: </strong>Mayur Patil, Qadeer Ahmed, Shawn Midlam-Mohler</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12043">https://arxiv.org/abs/2509.12043</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12043">https://arxiv.org/pdf/2509.12043</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12043]] Travel Time and Weather-Aware Traffic Forecasting in a Conformal Graph Neural Network Framework(https://arxiv.org/abs/2509.12043)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Traffic flow forecasting is essential for managing congestion, improving safety, and optimizing various transportation systems. However, it remains a prevailing challenge due to the stochastic nature of urban traffic and environmental factors. Better predictions require models capable of accommodating the traffic variability influenced by multiple dynamic and complex interdependent factors. In this work, we propose a Graph Neural Network (GNN) framework to address the stochasticity by leveraging adaptive adjacency matrices using log-normal distributions and Coefficient of Variation (CV) values to reflect real-world travel time variability. Additionally, weather factors such as temperature, wind speed, and precipitation adjust edge weights and enable GNN to capture evolving spatio-temporal dependencies across traffic stations. This enhancement over the static adjacency matrix allows the model to adapt effectively to traffic stochasticity and changing environmental conditions. Furthermore, we utilize the Adaptive Conformal Prediction (ACP) framework to provide reliable uncertainty quantification, achieving target coverage while maintaining acceptable prediction intervals. Experimental results demonstrate that the proposed model, in comparison with baseline methods, showed better prediction accuracy and uncertainty bounds. We, then, validate this method by constructing traffic scenarios in SUMO and applying Monte-Carlo simulation to derive a travel time distribution for a Vehicle Under Test (VUT) to reflect real-world variability. The simulated mean travel time of the VUT falls within the intervals defined by INRIX historical data, verifying the model's robustness.</li>
</ul>

<h3>Title: A Computer Vision Pipeline for Individual-Level Behavior Analysis: Benchmarking on the Edinburgh Pig Dataset</h3>
<ul>
<li><strong>Authors: </strong>Haiyu Yang, Enhong Liu, Jennifer Sun, Sumit Sharma, Meike van Leerdam, Sebastien Franceschini, Puchun Niu, Miel Hostens</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12047">https://arxiv.org/abs/2509.12047</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12047">https://arxiv.org/pdf/2509.12047</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12047]] A Computer Vision Pipeline for Individual-Level Behavior Analysis: Benchmarking on the Edinburgh Pig Dataset(https://arxiv.org/abs/2509.12047)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Animal behavior analysis plays a crucial role in understanding animal welfare, health status, and productivity in agricultural settings. However, traditional manual observation methods are time-consuming, subjective, and limited in scalability. We present a modular pipeline that leverages open-sourced state-of-the-art computer vision techniques to automate animal behavior analysis in a group housing environment. Our approach combines state-of-the-art models for zero-shot object detection, motion-aware tracking and segmentation, and advanced feature extraction using vision transformers for robust behavior recognition. The pipeline addresses challenges including animal occlusions and group housing scenarios as demonstrated in indoor pig monitoring. We validated our system on the Edinburgh Pig Behavior Video Dataset for multiple behavioral tasks. Our temporal model achieved 94.2% overall accuracy, representing a 21.2 percentage point improvement over existing methods. The pipeline demonstrated robust tracking capabilities with 93.3% identity preservation score and 89.3% object detection precision. The modular design suggests potential for adaptation to other contexts, though further validation across species would be required. The open-source implementation provides a scalable solution for behavior monitoring, contributing to precision pig farming and welfare assessment through automated, objective, and continuous analysis.</li>
</ul>

<h3>Title: AvatarSync: Rethinking Talking-Head Animation through Autoregressive Perspective</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Deng, Xiuyang Wu, Hai-Tao Zheng, Suiyang Zhang, Yi He, Yuxing Han</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12052">https://arxiv.org/abs/2509.12052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12052">https://arxiv.org/pdf/2509.12052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12052]] AvatarSync: Rethinking Talking-Head Animation through Autoregressive Perspective(https://arxiv.org/abs/2509.12052)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Existing talking-head animation approaches based on Generative Adversarial Networks (GANs) or diffusion models often suffer from inter-frame flicker, identity drift, and slow inference. These limitations inherent to their video generation pipelines restrict their suitability for applications. To address this, we introduce AvatarSync, an autoregressive framework on phoneme representations that generates realistic and controllable talking-head animations from a single reference image, driven directly text or audio input. In addition, AvatarSync adopts a two-stage generation strategy, decoupling semantic modeling from visual dynamics, which is a deliberate "Divide and Conquer" design. The first stage, Facial Keyframe Generation (FKG), focuses on phoneme-level semantic representation by leveraging the many-to-one mapping from text or audio to phonemes. A Phoneme-to-Visual Mapping is constructed to anchor abstract phonemes to character-level units. Combined with a customized Text-Frame Causal Attention Mask, the keyframes are generated. The second stage, inter-frame interpolation, emphasizes temporal coherence and visual smoothness. We introduce a timestamp-aware adaptive strategy based on a selective state space model, enabling efficient bidirectional context reasoning. To support deployment, we optimize the inference pipeline to reduce latency without compromising visual fidelity. Extensive experiments show that AvatarSync outperforms existing talking-head animation methods in visual fidelity, temporal consistency, and computational efficiency, providing a scalable and controllable solution.</li>
</ul>

<h3>Title: Foundational theory for optimal decision tree problems. II. Optimal hypersurface decision tree algorithm</h3>
<ul>
<li><strong>Authors: </strong>Xi He</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DM, cs.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12057">https://arxiv.org/abs/2509.12057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12057">https://arxiv.org/pdf/2509.12057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12057]] Foundational theory for optimal decision tree problems. II. Optimal hypersurface decision tree algorithm(https://arxiv.org/abs/2509.12057)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Decision trees are a ubiquitous model for classification and regression tasks due to their interpretability and efficiency. However, solving the optimal decision tree (ODT) problem remains a challenging combinatorial optimization task. Even for the simplest splitting rules--axis-parallel hyperplanes--it is NP-hard to optimize. In Part I of this series, we rigorously defined the proper decision tree model through four axioms and, based on these, introduced four formal definitions of the ODT problem. From these definitions, we derived four generic algorithms capable of solving ODT problems for arbitrary decision trees satisfying the axioms. We also analyzed the combinatorial geometric properties of hypersurfaces, showing that decision trees defined by polynomial hypersurface splitting rules satisfy the proper axioms that we proposed. In this second paper (Part II) of this two-part series, building on the algorithmic and geometric foundations established in Part I, we introduce the first hypersurface decision tree (HODT) algorithm. To the best of our knowledge, existing optimal decision tree methods are, to date, limited to hyperplane splitting rules--a special case of hypersurfaces--and rely on general-purpose solvers. In contrast, our HODT algorithm addresses the general hypersurface decision tree model without requiring external solvers. Using synthetic datasets generated from ground-truth hyperplane decision trees, we vary tree size, data size, dimensionality, and label and feature noise. Results showing that our algorithm recovers the ground truth more accurately than axis-parallel trees and exhibits greater robustness to noise. We also analyzed generalization performance across 30 real-world datasets, showing that HODT can achieve up to 30% higher accuracy than the state-of-the-art optimal axis-parallel decision tree algorithm when tree complexity is properly controlled.</li>
</ul>

<h3>Title: Robust Fetal Pose Estimation across Gestational Ages via Cross-Population Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Sebastian Diaz, Benjamin Billot, Neel Dey, Molin Zhang, Esra Abaci Turk, P. Ellen Grant, Polina Golland, Elfar Adalsteinsson</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12062">https://arxiv.org/abs/2509.12062</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12062">https://arxiv.org/pdf/2509.12062</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12062]] Robust Fetal Pose Estimation across Gestational Ages via Cross-Population Augmentation(https://arxiv.org/abs/2509.12062)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Fetal motion is a critical indicator of neurological development and intrauterine health, yet its quantification remains challenging, particularly at earlier gestational ages (GA). Current methods track fetal motion by predicting the location of annotated landmarks on 3D echo planar imaging (EPI) time-series, primarily in third-trimester fetuses. The predicted landmarks enable simplification of the fetal body for downstream analysis. While these methods perform well within their training age distribution, they consistently fail to generalize to early GAs due to significant anatomical changes in both mother and fetus across gestation, as well as the difficulty of obtaining annotated early GA EPI data. In this work, we develop a cross-population data augmentation framework that enables pose estimation models to robustly generalize to younger GA clinical cohorts using only annotated images from older GA cohorts. Specifically, we introduce a fetal-specific augmentation strategy that simulates the distinct intrauterine environment and fetal positioning of early GAs. Our experiments find that cross-population augmentation yields reduced variability and significant improvements across both older GA and challenging early GA cases. By enabling more reliable pose estimation across gestation, our work potentially facilitates early clinical detection and intervention in challenging 4D fetal imaging settings. Code is available at this https URL.</li>
</ul>

<h3>Title: Steering Language Models in Multi-Token Generation: A Case Study on Tense and Aspect</h3>
<ul>
<li><strong>Authors: </strong>Alina Klerings, Jannik Brinkmann, Daniel Ruffinelli, Simone Ponzetto</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12065">https://arxiv.org/abs/2509.12065</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12065">https://arxiv.org/pdf/2509.12065</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12065]] Steering Language Models in Multi-Token Generation: A Case Study on Tense and Aspect(https://arxiv.org/abs/2509.12065)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are able to generate grammatically well-formed text, but how do they encode their syntactic knowledge internally? While prior work has focused largely on binary grammatical contrasts, in this work, we study the representation and control of two multidimensional hierarchical grammar phenomena - verb tense and aspect - and for each, identify distinct, orthogonal directions in residual space using linear discriminant analysis. Next, we demonstrate causal control over both grammatical features through concept steering across three generation tasks. Then, we use these identified features in a case study to investigate factors influencing effective steering in multi-token generation. We find that steering strength, location, and duration are crucial parameters for reducing undesirable side effects such as topic shift and degeneration. Our findings suggest that models encode tense and aspect in structurally organized, human-like ways, but effective control of such features during generation is sensitive to multiple factors and requires manual tuning or automated optimization.</li>
</ul>

<h3>Title: U-Mamba2: Scaling State Space Models for Dental Anatomy Segmentation in CBCT</h3>
<ul>
<li><strong>Authors: </strong>Zhi Qin Tan, Xiatian Zhu, Owen Addison, Yunpeng Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12069">https://arxiv.org/abs/2509.12069</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12069">https://arxiv.org/pdf/2509.12069</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12069]] U-Mamba2: Scaling State Space Models for Dental Anatomy Segmentation in CBCT(https://arxiv.org/abs/2509.12069)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, segmentation</a></li>
<li><strong>Abstract: </strong>Cone-Beam Computed Tomography (CBCT) is a widely used 3D imaging technique in dentistry, providing volumetric information about the anatomical structures of jaws and teeth. Accurate segmentation of these anatomies is critical for clinical applications such as diagnosis and surgical planning, but remains time-consuming and challenging. In this paper, we present U-Mamba2, a new neural network architecture designed for multi-anatomy CBCT segmentation in the context of the ToothFairy3 challenge. U-Mamba2 integrates the Mamba2 state space models into the U-Net architecture, enforcing stronger structural constraints for higher efficiency without compromising performance. In addition, we integrate interactive click prompts with cross-attention blocks, pre-train U-Mamba2 using self-supervised learning, and incorporate dental domain knowledge into the model design to address key challenges of dental anatomy segmentation in CBCT. Extensive experiments, including independent tests, demonstrate that U-Mamba2 is both effective and efficient, securing top 3 places in both tasks of the Toothfairy3 challenge. In Task 1, U-Mamba2 achieved a mean Dice of 0.792, HD95 of 93.19 with the held-out test data, with an average inference time of XX (TBC during the ODIN workshop). In Task 2, U-Mamba2 achieved the mean Dice of 0.852 and HD95 of 7.39 with the held-out test data. The code is publicly available at this https URL.</li>
</ul>

<h3>Title: Progressive Flow-inspired Unfolding for Spectral Compressive Imaging</h3>
<ul>
<li><strong>Authors: </strong>Xiaodong Wang, Ping Wang, Zijun He, Mengjie Qin, Xin Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12079">https://arxiv.org/abs/2509.12079</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12079">https://arxiv.org/pdf/2509.12079</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12079]] Progressive Flow-inspired Unfolding for Spectral Compressive Imaging(https://arxiv.org/abs/2509.12079)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Coded aperture snapshot spectral imaging (CASSI) retrieves a 3D hyperspectral image (HSI) from a single 2D compressed measurement, which is a highly challenging reconstruction task. Recent deep unfolding networks (DUNs), empowered by explicit data-fidelity updates and implicit deep denoisers, have achieved the state of the art in CASSI reconstruction. However, existing unfolding approaches suffer from uncontrollable reconstruction trajectories, leading to abrupt quality jumps and non-gradual refinement across stages. Inspired by diffusion trajectories and flow matching, we propose a novel trajectory-controllable unfolding framework that enforces smooth, continuous optimization paths from noisy initial estimates to high-quality reconstructions. To achieve computational efficiency, we design an efficient spatial-spectral Transformer tailored for hyperspectral reconstruction, along with a frequency-domain fusion module to gurantee feature consistency. Experiments on simulation and real data demonstrate that our method achieves better reconstruction quality and efficiency than prior state-of-the-art approaches.</li>
</ul>

<h3>Title: A Time-Series Foundation Model by Universal Delay Embedding</h3>
<ul>
<li><strong>Authors: </strong>Zijian Wang, Peng Tao, Jifan Shi, Rui Bao, Rui Liu, Luonan Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12080">https://arxiv.org/abs/2509.12080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12080">https://arxiv.org/pdf/2509.12080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12080]] A Time-Series Foundation Model by Universal Delay Embedding(https://arxiv.org/abs/2509.12080)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>This study introduces Universal Delay Embedding (UDE), a pretrained foundation model designed to revolutionize time-series forecasting through principled integration of delay embedding representation and Koopman operator prediction. Leveraging Takens' embedding theorem, UDE as a dynamical representation of observed data constructs two-dimensional subspace patches from Hankel matrices, theoretically preserving dynamical and topological properties of underlying dynamical systems. Such patches are viewed as images, which can be efficiently processed by exploiting advanced deep learning technologies. Computationally, these patches further serve as tokens for learning a self-attention encoder, thus enabling accurate prediction of nonlinear time-series by a finite-dimensional Koopman operator in a linear manner in a latent space. Extensive evaluations across various benchmarks and real-world climate datasets demonstrate over 20% average reduction in mean squared error versus state-of-the-art foundation models, alongside superior generalization in fine-tuning scenarios. In particular, the learned dynamical representations and Koopman operator prediction forms from the patches exhibit exceptional interpretability, with consistent identification of topologically informative subspaces and robust encoding of domain-invariant dynamics, establishing UDE as a scalable, interpretable framework for universal time-series modeling and forecasting with broad scientific and industrial applicability.</li>
</ul>

<h3>Title: Is 'Hope' a person or an idea? A pilot benchmark for NER: comparing traditional NLP tools and large language models on ambiguous entities</h3>
<ul>
<li><strong>Authors: </strong>Payam Latifi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12098">https://arxiv.org/abs/2509.12098</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12098">https://arxiv.org/pdf/2509.12098</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12098]] Is 'Hope' a person or an idea? A pilot benchmark for NER: comparing traditional NLP tools and large language models on ambiguous entities(https://arxiv.org/abs/2509.12098)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This pilot study presents a small-scale but carefully annotated benchmark of Named Entity Recognition (NER) performance across six systems: three non-LLM NLP tools (NLTK, spaCy, Stanza) and three general-purpose large language models (LLMs: Gemini-1.5-flash, DeepSeek-V3, Qwen-3-4B). The dataset contains 119 tokens covering five entity types (PERSON, LOCATION, ORGANIZATION, DATE, TIME). We evaluated each system's output against the manually annotated gold standard dataset using F1-score. The results show that LLMs generally outperform conventional tools in recognizing context-sensitive entities like person names, with Gemini achieving the highest average F1-score. However, traditional systems like Stanza demonstrate greater consistency in structured tags such as LOCATION and DATE. We also observed variability among LLMs, particularly in handling temporal expressions and multi-word organizations. Our findings highlight that while LLMs offer improved contextual understanding, traditional tools remain competitive in specific tasks, informing model selection.</li>
</ul>

<h3>Title: FS-SAM2: Adapting Segment Anything Model 2 for Few-Shot Semantic Segmentation via Low-Rank Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Bernardo Forni, Gabriele Lombardi, Federico Pozzi, Mirco Planamente</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12105">https://arxiv.org/abs/2509.12105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12105">https://arxiv.org/pdf/2509.12105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12105]] FS-SAM2: Adapting Segment Anything Model 2 for Few-Shot Semantic Segmentation via Low-Rank Adaptation(https://arxiv.org/abs/2509.12105)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Few-shot semantic segmentation has recently attracted great attention. The goal is to develop a model capable of segmenting unseen classes using only a few annotated samples. Most existing approaches adapt a pre-trained model by training from scratch an additional module. Achieving optimal performance with these approaches requires extensive training on large-scale datasets. The Segment Anything Model 2 (SAM2) is a foundational model for zero-shot image and video segmentation with a modular design. In this paper, we propose a Few-Shot segmentation method based on SAM2 (FS-SAM2), where SAM2's video capabilities are directly repurposed for the few-shot task. Moreover, we apply a Low-Rank Adaptation (LoRA) to the original modules in order to handle the diverse images typically found in standard datasets, unlike the temporally connected frames used in SAM2's pre-training. With this approach, only a small number of parameters is meta-trained, which effectively adapts SAM2 while benefiting from its impressive segmentation performance. Our method supports any K-shot configuration. We evaluate FS-SAM2 on the PASCAL-5$^i$, COCO-20$^i$ and FSS-1000 datasets, achieving remarkable results and demonstrating excellent computational efficiency during inference. Code is available at this https URL</li>
</ul>

<h3>Title: GTA: Supervised-Guided Reinforcement Learning for Text Classification with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Min Zeng, Jinfei Sun, Xueyou Luo, Caiquan Liu, Shiqi Zhang, Li Xie, Xiaoxin Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12108">https://arxiv.org/abs/2509.12108</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12108">https://arxiv.org/pdf/2509.12108</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12108]] GTA: Supervised-Guided Reinforcement Learning for Text Classification with Large Language Models(https://arxiv.org/abs/2509.12108)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In natural language processing tasks, pure reinforcement learning (RL) fine-tuning methods often suffer from inefficient exploration and slow convergence; while supervised fine-tuning (SFT) methods, although efficient in training, have limited performance ceiling and less solid theoretical foundation compared to RL. To address efficiency-capability trade-off, we propose the Guess-Think-Answer (GTA) framework that combines the efficiency of SFT with the capability gains of RL in a unified training paradigm. GTA works by having the model first produce a provisional guess (optimized via cross-entropy loss), then reflect on this guess before generating the final answer, with RL rewards shaping both the final output and the format of the entire GTA structure. This hybrid approach achieves both faster convergence than pure RL and higher performance ceiling than pure SFT. To mitigate gradient conflicts between the two training signals, we employ loss masking and gradient constraints. Empirical results on four text classification benchmarks demonstrate that GTA substantially accelerates convergence while outperforming both standalone SFT and RL baselines.</li>
</ul>

<h3>Title: CBP-Tuning: Efficient Local Customization for Black-box Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiaxuan Zhao, Naibin Gu, Yuchen Feng, Xiyu Liu, Peng Fu, Zheng Lin, Weiping Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12112">https://arxiv.org/abs/2509.12112</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12112">https://arxiv.org/pdf/2509.12112</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12112]] CBP-Tuning: Efficient Local Customization for Black-box Large Language Models(https://arxiv.org/abs/2509.12112)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>The high costs of customizing large language models (LLMs) fundamentally limit their adaptability to user-specific needs. Consequently, LLMs are increasingly offered as cloud-based services, a paradigm that introduces critical limitations: providers struggle to support personalized customization at scale, while users face privacy risks when exposing sensitive data. To address this dual challenge, we propose Customized Black-box Prompt Tuning (CBP-Tuning), a novel framework that facilitates efficient local customization while preserving bidirectional privacy. Specifically, we design a two-stage framework: (1) a prompt generator trained on the server-side to capture domain-specific and task-agnostic capabilities, and (2) user-side gradient-free optimization that tailors soft prompts for individual tasks. This approach eliminates the need for users to access model weights or upload private data, requiring only a single customized vector per task while achieving effective adaptation. Furthermore, the evaluation of CBP-Tuning in the commonsense reasoning, medical and financial domain settings demonstrates superior performance compared to baselines, showcasing its advantages in task-agnostic processing and privacy preservation.</li>
</ul>

<h3>Title: RailSafeNet: Visual Scene Understanding for Tram Safety</h3>
<ul>
<li><strong>Authors: </strong>Ing. Ondrej Valach, Ing. Ivan Gruber</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12125">https://arxiv.org/abs/2509.12125</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12125">https://arxiv.org/pdf/2509.12125</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12125]] RailSafeNet: Visual Scene Understanding for Tram Safety(https://arxiv.org/abs/2509.12125)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Tram-human interaction safety is an important challenge, given that trams frequently operate in densely populated areas, where collisions can range from minor injuries to fatal outcomes. This paper addresses the issue from the perspective of designing a solution leveraging digital image processing, deep learning, and artificial intelligence to improve the safety of pedestrians, drivers, cyclists, pets, and tram passengers. We present RailSafeNet, a real-time framework that fuses semantic segmentation, object detection and a rule-based Distance Assessor to highlight track intrusions. Using only monocular video, the system identifies rails, localises nearby objects and classifies their risk by comparing projected distances with the standard 1435mm rail gauge. Experiments on the diverse RailSem19 dataset show that a class-filtered SegFormer B3 model achieves 65% intersection-over-union (IoU), while a fine-tuned YOLOv8 attains 75.6% mean average precision (mAP) calculated at an intersection over union (IoU) threshold of 0.50. RailSafeNet therefore delivers accurate, annotation-light scene understanding that can warn drivers before dangerous situations escalate. Code available at this https URL.</li>
</ul>

<h3>Title: XplaiNLP at CheckThat! 2025: Multilingual Subjectivity Detection with Finetuned Transformers and Prompt-Based Inference with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ariana Sahitaj, Jiaao Li, Pia Wenzel Neves, Fedor Splitt, Premtim Sahitaj, Charlott Jakob, Veronika Solopova, Vera Schmitt</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12130">https://arxiv.org/abs/2509.12130</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12130">https://arxiv.org/pdf/2509.12130</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12130]] XplaiNLP at CheckThat! 2025: Multilingual Subjectivity Detection with Finetuned Transformers and Prompt-Based Inference with Large Language Models(https://arxiv.org/abs/2509.12130)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>This notebook reports the XplaiNLP submission to the CheckThat! 2025 shared task on multilingual subjectivity detection. We evaluate two approaches: (1) supervised fine-tuning of transformer encoders, EuroBERT, XLM-RoBERTa, and German-BERT, on monolingual and machine-translated training data; and (2) zero-shot prompting using two LLMs: o3-mini for Annotation (rule-based labelling) and gpt-4.1-mini for DoubleDown (contrastive rewriting) and Perspective (comparative reasoning). The Annotation Approach achieves 1st place in the Italian monolingual subtask with an F_1 score of 0.8104, outperforming the baseline of 0.6941. In the Romanian zero-shot setting, the fine-tuned XLM-RoBERTa model obtains an F_1 score of 0.7917, ranking 3rd and exceeding the baseline of 0.6461. The same model also performs reliably in the multilingual task and improves over the baseline in Greek. For German, a German-BERT model fine-tuned on translated training data from typologically related languages yields competitive performance over the baseline. In contrast, performance in the Ukrainian and Polish zero-shot settings falls slightly below the respective baselines, reflecting the challenge of generalization in low-resource cross-lingual scenarios.</li>
</ul>

<h3>Title: 3DViT-GAT: A Unified Atlas-Based 3D Vision Transformer and Graph Learning Framework for Major Depressive Disorder Detection Using Structural MRI Data</h3>
<ul>
<li><strong>Authors: </strong>Nojod M. Alotaibi, Areej M. Alhothali, Manar S. Ali</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12143">https://arxiv.org/abs/2509.12143</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12143">https://arxiv.org/pdf/2509.12143</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12143]] 3DViT-GAT: A Unified Atlas-Based 3D Vision Transformer and Graph Learning Framework for Major Depressive Disorder Detection Using Structural MRI Data(https://arxiv.org/abs/2509.12143)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Major depressive disorder (MDD) is a prevalent mental health condition that negatively impacts both individual well-being and global public health. Automated detection of MDD using structural magnetic resonance imaging (sMRI) and deep learning (DL) methods holds increasing promise for improving diagnostic accuracy and enabling early intervention. Most existing methods employ either voxel-level features or handcrafted regional representations built from predefined brain atlases, limiting their ability to capture complex brain patterns. This paper develops a unified pipeline that utilizes Vision Transformers (ViTs) for extracting 3D region embeddings from sMRI data and Graph Neural Network (GNN) for classification. We explore two strategies for defining regions: (1) an atlas-based approach using predefined structural and functional brain atlases, and (2) an cube-based method by which ViTs are trained directly to identify regions from uniformly extracted 3D patches. Further, cosine similarity graphs are generated to model interregional relationships, and guide GNN-based classification. Extensive experiments were conducted using the REST-meta-MDD dataset to demonstrate the effectiveness of our model. With stratified 10-fold cross-validation, the best model obtained 78.98% accuracy, 76.54% sensitivity, 81.58% specificity, 81.58% precision, and 78.98% F1-score. Further, atlas-based models consistently outperformed the cube-based approach, highlighting the importance of using domain-specific anatomical priors for MDD detection.</li>
</ul>

<h3>Title: Open-ended Hierarchical Streaming Video Understanding with Vision Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hyolim Kang, Yunsu Park, Youngbeom Yoo, Yeeun Choi, Seon Joo Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12145">https://arxiv.org/abs/2509.12145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12145">https://arxiv.org/pdf/2509.12145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12145]] Open-ended Hierarchical Streaming Video Understanding with Vision Language Models(https://arxiv.org/abs/2509.12145)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce Hierarchical Streaming Video Understanding, a task that combines online temporal action localization with free-form description generation. Given the scarcity of datasets with hierarchical and fine-grained temporal annotations, we demonstrate that LLMs can effectively group atomic actions into higher-level events, enriching existing datasets. We then propose OpenHOUSE (Open-ended Hierarchical Online Understanding System for Events), which extends streaming action perception beyond action classification. OpenHOUSE features a specialized streaming module that accurately detects boundaries between closely adjacent actions, nearly doubling the performance of direct extensions of existing methods. We envision the future of streaming action perception in the integration of powerful generative models, with OpenHOUSE representing a key step in that direction.</li>
</ul>

<h3>Title: Multi Anatomy X-Ray Foundation Model</h3>
<ul>
<li><strong>Authors: </strong>Nishank Singla, Krisztian Koos, Farzin Haddadpour, Amin Honarmandi Shandiz, Lovish Chum, Xiaojian Xu, Qing Jin, Erhan Bas</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12146">https://arxiv.org/abs/2509.12146</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12146">https://arxiv.org/pdf/2509.12146</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12146]] Multi Anatomy X-Ray Foundation Model(https://arxiv.org/abs/2509.12146)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>X-ray imaging is a ubiquitous in radiology, yet most existing AI foundation models are limited to chest anatomy and fail to generalize across broader clinical tasks. In this work, we introduce XR-0, the multi-anatomy X-ray foundation model using self-supervised learning on a large, private dataset of 1.15 million images spanning diverse anatomical regions and evaluated across 12 datasets and 20 downstream tasks, including classification, retrieval, segmentation, localization, visual grounding, and report generation. XR-0 achieves state-of-the-art performance on most multi-anatomy tasks and remains competitive on chest-specific benchmarks. Our results demonstrate that anatomical diversity and supervision are critical for building robust, general-purpose medical vision models, paving the way for scalable and adaptable AI systems in radiology.</li>
</ul>

<h3>Title: Do machine learning climate models work in changing climate dynamics?</h3>
<ul>
<li><strong>Authors: </strong>Maria Conchita Agana Navarro, Geng Li, Theo Wolf, María Pérez-Ortiz</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12147">https://arxiv.org/abs/2509.12147</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12147">https://arxiv.org/pdf/2509.12147</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12147]] Do machine learning climate models work in changing climate dynamics?(https://arxiv.org/abs/2509.12147)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Climate change is accelerating the frequency and severity of unprecedented events, deviating from established patterns. Predicting these out-of-distribution (OOD) events is critical for assessing risks and guiding climate adaptation. While machine learning (ML) models have shown promise in providing precise, high-speed climate predictions, their ability to generalize under distribution shifts remains a significant limitation that has been underexplored in climate contexts. This research systematically evaluates state-of-the-art ML-based climate models in diverse OOD scenarios by adapting established OOD evaluation methodologies to climate data. Experiments on large-scale datasets reveal notable performance variability across scenarios, shedding light on the strengths and limitations of current models. These findings underscore the importance of robust evaluation frameworks and provide actionable insights to guide the reliable application of ML for climate risk forecasting.</li>
</ul>

<h3>Title: LoRA-fine-tuned Large Vision Models for Automated Assessment of Post-SBRT Lung Injury</h3>
<ul>
<li><strong>Authors: </strong>M. Bolhassani, B. Veasey, E. Daugherty, S. Keltner, N. Kumar, N. Dunlap, A. Amini</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12155">https://arxiv.org/abs/2509.12155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12155">https://arxiv.org/pdf/2509.12155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12155]] LoRA-fine-tuned Large Vision Models for Automated Assessment of Post-SBRT Lung Injury(https://arxiv.org/abs/2509.12155)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This study investigates the efficacy of Low-Rank Adaptation (LoRA) for fine-tuning large Vision Models, DinoV2 and SwinV2, to diagnose Radiation-Induced Lung Injury (RILI) from X-ray CT scans following Stereotactic Body Radiation Therapy (SBRT). To evaluate the robustness and efficiency of this approach, we compare LoRA with traditional full fine-tuning and inference-only (no fine-tuning) methods. Cropped images of two sizes (50 mm3 and 75 mm3), centered at the treatment isocenter, in addition to different adaptation techniques for adapting the 2D LVMs for 3D data were used to determine the sensitivity of the models to spatial context. Experimental results show that LoRA achieves comparable or superior performance to traditional fine-tuning while significantly reducing computational costs and training times by requiring fewer trainable parameters.</li>
</ul>

<h3>Title: Pun Unintended: LLMs and the Illusion of Humor Understanding</h3>
<ul>
<li><strong>Authors: </strong>Alessandro Zangari, Matteo Marcuzzo, Andrea Albarelli, Mohammad Taher Pilehvar, Jose Camacho-Collados</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12158">https://arxiv.org/abs/2509.12158</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12158">https://arxiv.org/pdf/2509.12158</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12158]] Pun Unintended: LLMs and the Illusion of Humor Understanding(https://arxiv.org/abs/2509.12158)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Puns are a form of humorous wordplay that exploits polysemy and phonetic similarity. While LLMs have shown promise in detecting puns, we show in this paper that their understanding often remains shallow, lacking the nuanced grasp typical of human interpretation. By systematically analyzing and reformulating existing pun benchmarks, we demonstrate how subtle changes in puns are sufficient to mislead LLMs. Our contributions include comprehensive and nuanced pun detection benchmarks, human evaluation across recent LLMs, and an analysis of the robustness challenges these models face in processing puns.</li>
</ul>

<h3>Title: RAGs to Riches: RAG-like Few-shot Learning for Large Language Model Role-playing</h3>
<ul>
<li><strong>Authors: </strong>Timothy Rupprecht, Enfu Nan, Arash Akbari, Arman Akbari, Lei Lu, Priyanka Maan, Sean Duffy, Pu Zhao, Yumei He, David Kaeli, Yanzhi Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12168">https://arxiv.org/abs/2509.12168</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12168">https://arxiv.org/pdf/2509.12168</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12168]] RAGs to Riches: RAG-like Few-shot Learning for Large Language Model Role-playing(https://arxiv.org/abs/2509.12168)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Role-playing Large language models (LLMs) are increasingly deployed in high-stakes domains such as healthcare, education, and governance, where failures can directly impact user trust and well-being. A cost effective paradigm for LLM role-playing is few-shot learning, but existing approaches often cause models to break character in unexpected and potentially harmful ways, especially when interacting with hostile users. Inspired by Retrieval-Augmented Generation (RAG), we reformulate LLM role-playing into a text retrieval problem and propose a new prompting framework called RAGs-to-Riches, which leverages curated reference demonstrations to condition LLM responses. We evaluate our framework with LLM-as-a-judge preference voting and introduce two novel token-level ROUGE metrics: Intersection over Output (IOO) to quantity how much an LLM improvises and Intersection over References (IOR) to measure few-shot demonstrations utilization rate during the evaluation tasks. When simulating interactions with a hostile user, our prompting strategy incorporates in its responses during inference an average of 35% more tokens from the reference demonstrations. As a result, across 453 role-playing interactions, our models are consistently judged as being more authentic, and remain in-character more often than zero-shot and in-context Learning (ICL) methods. Our method presents a scalable strategy for building robust, human-aligned LLM role-playing frameworks.</li>
</ul>

<h3>Title: Preservation of Language Understanding Capabilities in Speech-aware Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Marek Kubis, Paweł Skórzewski, Iwona Christop, Mateusz Czyżnikiewicz, Jakub Kubiak, Łukasz Bondaruk, Marcin Lewandowski</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12171">https://arxiv.org/abs/2509.12171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12171">https://arxiv.org/pdf/2509.12171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12171]] Preservation of Language Understanding Capabilities in Speech-aware Large Language Models(https://arxiv.org/abs/2509.12171)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, large language model</a></li>
<li><strong>Abstract: </strong>The paper presents C3T (Cross-modal Capabilities Conservation Test), a new benchmark for assessing the performance of speech-aware large language models. The benchmark utilizes textual tasks and a voice cloning text-to-speech model to quantify the extent to which language understanding capabilities are preserved when the model is accessed via speech input. C3T quantifies the fairness of the model for different categories of speakers and its robustness across text and speech modalities.</li>
</ul>

<h3>Title: From Autoencoders to CycleGAN: Robust Unpaired Face Manipulation via Adversarial Learning</h3>
<ul>
<li><strong>Authors: </strong>Collin Guo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12176">https://arxiv.org/abs/2509.12176</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12176">https://arxiv.org/pdf/2509.12176</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12176]] From Autoencoders to CycleGAN: Robust Unpaired Face Manipulation via Adversarial Learning(https://arxiv.org/abs/2509.12176)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Human face synthesis and manipulation are increasingly important in entertainment and AI, with a growing demand for highly realistic, identity-preserving images even when only unpaired, unaligned datasets are available. We study unpaired face manipulation via adversarial learning, moving from autoencoder baselines to a robust, guided CycleGAN framework. While autoencoders capture coarse identity, they often miss fine details. Our approach integrates spectral normalization for stable training, identity- and perceptual-guided losses to preserve subject identity and high-level structure, and landmark-weighted cycle constraints to maintain facial geometry across pose and illumination changes. Experiments show that our adversarial trained CycleGAN improves realism (FID), perceptual quality (LPIPS), and identity preservation (ID-Sim) over autoencoders, with competitive cycle-reconstruction SSIM and practical inference times, which achieved high quality without paired datasets and approaching pix2pix on curated paired subsets. These results demonstrate that guided, spectrally normalized CycleGANs provide a practical path from autoencoders to robust unpaired face manipulation.</li>
</ul>

<h3>Title: All that structure matches does not glitter</h3>
<ul>
<li><strong>Authors: </strong>Maya M. Martirossyan, Thomas Egg, Philipp Hoellmer, George Karypis, Mark Transtrum, Adrian Roitberg, Mingjie Liu, Richard G. Hennig, Ellad B. Tadmor, Stefano Martiniani</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12178">https://arxiv.org/abs/2509.12178</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12178">https://arxiv.org/pdf/2509.12178</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12178]] All that structure matches does not glitter(https://arxiv.org/abs/2509.12178)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Generative models for materials, especially inorganic crystals, hold potential to transform the theoretical prediction of novel compounds and structures. Advancement in this field depends critically on robust benchmarks and minimal, information-rich datasets that enable meaningful model evaluation. This paper critically examines common datasets and reported metrics for a crystal structure prediction task$\unicode{x2014}$generating the most likely structures given the chemical composition of a material. We focus on three key issues: First, materials datasets should contain unique crystal structures; for example, we show that the widely-utilized carbon-24 dataset only contains $\approx$40% unique structures. Second, materials datasets should not be split randomly if polymorphs of many different compositions are numerous, which we find to be the case for the perov-5 dataset. Third, benchmarks can mislead if used uncritically, e.g., reporting a match rate metric without considering the structural variety exhibited by identical building blocks. To address these oft-overlooked issues, we introduce several fixes. We provide revised versions of the carbon-24 dataset: one with duplicates removed, one deduplicated and split by number of atoms $N$, and two containing only identical structures but with different unit cells. We also propose a new split for the perov-5 dataset which ensures polymorphs are grouped within each split subset, setting a more sensible standard for benchmarking model performance. Finally, we present METRe and cRMSE, new model evaluation metrics that can correct existing issues with the match rate metric.</li>
</ul>

<h3>Title: LOKI: Proactively Discovering Online Scam Websites by Mining Toxic Search Queries</h3>
<ul>
<li><strong>Authors: </strong>Pujan Paudel, Gianluca Stringhini</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12181">https://arxiv.org/abs/2509.12181</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12181">https://arxiv.org/pdf/2509.12181</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12181]] LOKI: Proactively Discovering Online Scam Websites by Mining Toxic Search Queries(https://arxiv.org/abs/2509.12181)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Online e-commerce scams, ranging from shopping scams to pet scams, globally cause millions of dollars in financial damage every year. In response, the security community has developed highly accurate detection systems able to determine if a website is fraudulent. However, finding candidate scam websites that can be passed as input to these downstream detection systems is challenging: relying on user reports is inherently reactive and slow, and proactive systems issuing search engine queries to return candidate websites suffer from low coverage and do not generalize to new scam types. In this paper, we present LOKI, a system designed to identify search engine queries likely to return a high fraction of fraudulent websites. LOKI implements a keyword scoring model grounded in Learning Under Privileged Information (LUPI) and feature distillation from Search Engine Result Pages (SERPs). We rigorously validate LOKI across 10 major scam categories and demonstrate a 20.58 times improvement in discovery over both heuristic and data- driven baselines across all categories. Leveraging a small seed set of only 1,663 known scam sites, we use the keywords identified by our method to discover 52,493 previously unreported scams in the wild. Finally, we show that LOKI generalizes to previously-unseen scam categories, highlighting its utility in surfacing emerging threats.</li>
</ul>

<h3>Title: HoloGarment: 360° Novel View Synthesis of In-the-Wild Garments</h3>
<ul>
<li><strong>Authors: </strong>Johanna Karras, Yingwei Li, Yasamin Jafarian, Ira Kemelmacher-Shlizerman</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12187">https://arxiv.org/abs/2509.12187</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12187">https://arxiv.org/pdf/2509.12187</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12187]] HoloGarment: 360° Novel View Synthesis of In-the-Wild Garments(https://arxiv.org/abs/2509.12187)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Novel view synthesis (NVS) of in-the-wild garments is a challenging task due significant occlusions, complex human poses, and cloth deformations. Prior methods rely on synthetic 3D training data consisting of mostly unoccluded and static objects, leading to poor generalization on real-world clothing. In this paper, we propose HoloGarment (Hologram-Garment), a method that takes 1-3 images or a continuous video of a person wearing a garment and generates 360° novel views of the garment in a canonical pose. Our key insight is to bridge the domain gap between real and synthetic data with a novel implicit training paradigm leveraging a combination of large-scale real video data and small-scale synthetic 3D data to optimize a shared garment embedding space. During inference, the shared embedding space further enables dynamic video-to-360° NVS through the construction of a garment "atlas" representation by finetuning a garment embedding on a specific real-world video. The atlas captures garment-specific geometry and texture across all viewpoints, independent of body pose or motion. Extensive experiments show that HoloGarment achieves state-of-the-art performance on NVS of in-the-wild garments from images and videos. Notably, our method robustly handles challenging real-world artifacts -- such as wrinkling, pose variation, and occlusion -- while maintaining photorealism, view consistency, fine texture details, and accurate geometry. Visit our project page for additional results: this https URL</li>
</ul>

<h3>Title: Dynamic Relational Priming Improves Transformer in Multivariate Time Series</h3>
<ul>
<li><strong>Authors: </strong>Hunjae Lee, Corey Clark</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12196">https://arxiv.org/abs/2509.12196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12196">https://arxiv.org/pdf/2509.12196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12196]] Dynamic Relational Priming Improves Transformer in Multivariate Time Series(https://arxiv.org/abs/2509.12196)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Standard attention mechanisms in transformers employ static token representations that remain unchanged across all pair-wise computations in each layer. This limits their representational alignment with the potentially diverse relational dynamics of each token-pair interaction. While they excel in domains with relatively homogeneous relationships, standard attention's static relational learning struggles to capture the diverse, heterogeneous inter-channel dependencies of multivariate time series (MTS) data--where different channel-pair interactions within a single system may be governed by entirely different physical laws or temporal dynamics. To better align the attention mechanism for such domain phenomena, we propose attention with dynamic relational priming (prime attention). Unlike standard attention where each token presents an identical representation across all of its pair-wise interactions, prime attention tailors each token dynamically (or per interaction) through learnable modulations to best capture the unique relational dynamics of each token pair, optimizing each pair-wise interaction for that specific relationship. This representational plasticity of prime attention enables effective extraction of relationship-specific information in MTS while maintaining the same asymptotic computational complexity as standard attention. Our results demonstrate that prime attention consistently outperforms standard attention across benchmarks, achieving up to 6.5\% improvement in forecasting accuracy. In addition, we find that prime attention achieves comparable or superior performance using up to 40\% less sequence length compared to standard attention, further demonstrating its superior relational modeling capabilities.</li>
</ul>

<h3>Title: 3D Human Pose and Shape Estimation from LiDAR Point Clouds: A Review</h3>
<ul>
<li><strong>Authors: </strong>Salma Galaaoui, Eduardo Valle, David Picard, Nermin Samet</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12197">https://arxiv.org/abs/2509.12197</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12197">https://arxiv.org/pdf/2509.12197</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12197]] 3D Human Pose and Shape Estimation from LiDAR Point Clouds: A Review(https://arxiv.org/abs/2509.12197)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>In this paper, we present a comprehensive review of 3D human pose estimation and human mesh recovery from in-the-wild LiDAR point clouds. We compare existing approaches across several key dimensions, and propose a structured taxonomy to classify these methods. Following this taxonomy, we analyze each method's strengths, limitations, and design choices. In addition, (i) we perform a quantitative comparison of the three most widely used datasets, detailing their characteristics; (ii) we compile unified definitions of all evaluation metrics; and (iii) we establish benchmark tables for both tasks on these datasets to enable fair comparisons and promote progress in the field. We also outline open challenges and research directions critical for advancing LiDAR-based 3D human understanding. Moreover, we maintain an accompanying webpage that organizes papers according to our taxonomy and continuously update it with new studies: this https URL</li>
</ul>

<h3>Title: OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling</h3>
<ul>
<li><strong>Authors: </strong>Yang Zhou, Yifan Wang, Jianjun Zhou, Wenzheng Chang, Haoyu Guo, Zizun Li, Kaijing Ma, Xinyue Li, Yating Wang, Haoyi Zhu, Mingyu Liu, Dingning Liu, Jiange Yang, Zhoujie Fu, Junyi Chen, Chunhua Shen, Jiangmiao Pang, Kaipeng Zhang, Tong He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12201">https://arxiv.org/abs/2509.12201</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12201">https://arxiv.org/pdf/2509.12201</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12201]] OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling(https://arxiv.org/abs/2509.12201)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The field of 4D world modeling - aiming to jointly capture spatial geometry and temporal dynamics - has witnessed remarkable progress in recent years, driven by advances in large-scale generative models and multimodal learning. However, the development of truly general 4D world models remains fundamentally constrained by the availability of high-quality data. Existing datasets and benchmarks often lack the dynamic complexity, multi-domain diversity, and spatial-temporal annotations required to support key tasks such as 4D geometric reconstruction, future prediction, and camera-control video generation. To address this gap, we introduce OmniWorld, a large-scale, multi-domain, multi-modal dataset specifically designed for 4D world modeling. OmniWorld consists of a newly collected OmniWorld-Game dataset and several curated public datasets spanning diverse domains. Compared with existing synthetic datasets, OmniWorld-Game provides richer modality coverage, larger scale, and more realistic dynamic interactions. Based on this dataset, we establish a challenging benchmark that exposes the limitations of current state-of-the-art (SOTA) approaches in modeling complex 4D environments. Moreover, fine-tuning existing SOTA methods on OmniWorld leads to significant performance gains across 4D reconstruction and video generation tasks, strongly validating OmniWorld as a powerful resource for training and evaluation. We envision OmniWorld as a catalyst for accelerating the development of general-purpose 4D world models, ultimately advancing machines' holistic understanding of the physical world.</li>
</ul>

<h3>Title: LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion Transformers via Explicit Correspondence</h3>
<ul>
<li><strong>Authors: </strong>Zixin Yin, Xili Dai, Duomin Wang, Xianfang Zeng, Lionel M. Ni, Gang Yu, Heung-Yeung Shum</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12203">https://arxiv.org/abs/2509.12203</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12203">https://arxiv.org/pdf/2509.12203</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12203]] LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion Transformers via Explicit Correspondence(https://arxiv.org/abs/2509.12203)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, generative</a></li>
<li><strong>Abstract: </strong>The reliance on implicit point matching via attention has become a core bottleneck in drag-based editing, resulting in a fundamental compromise on weakened inversion strength and costly test-time optimization (TTO). This compromise severely limits the generative capabilities of diffusion models, suppressing high-fidelity inpainting and text-guided creation. In this paper, we introduce LazyDrag, the first drag-based image editing method for Multi-Modal Diffusion Transformers, which directly eliminates the reliance on implicit point matching. In concrete terms, our method generates an explicit correspondence map from user drag inputs as a reliable reference to boost the attention control. This reliable reference opens the potential for a stable full-strength inversion process, which is the first in the drag-based editing task. It obviates the necessity for TTO and unlocks the generative capability of models. Therefore, LazyDrag naturally unifies precise geometric control with text guidance, enabling complex edits that were previously out of reach: opening the mouth of a dog and inpainting its interior, generating new objects like a ``tennis ball'', or for ambiguous drags, making context-aware changes like moving a hand into a pocket. Additionally, LazyDrag supports multi-round workflows with simultaneous move and scale operations. Evaluated on the DragBench, our method outperforms baselines in drag accuracy and perceptual quality, as validated by VIEScore and human evaluation. LazyDrag not only establishes new state-of-the-art performance, but also paves a new way to editing paradigms.</li>
</ul>

<h3>Title: Character-Centric Understanding of Animated Movies</h3>
<ul>
<li><strong>Authors: </strong>Zhongrui Gui, Junyu Xie, Tengda Han, Weidi Xie, Andrew Zisserman</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.12204">https://arxiv.org/abs/2509.12204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.12204">https://arxiv.org/pdf/2509.12204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.12204]] Character-Centric Understanding of Animated Movies(https://arxiv.org/abs/2509.12204)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Animated movies are captivating for their unique character designs and imaginative storytelling, yet they pose significant challenges for existing recognition systems. Unlike the consistent visual patterns detected by conventional face recognition methods, animated characters exhibit extreme diversity in their appearance, motion, and deformation. In this work, we propose an audio-visual pipeline to enable automatic and robust animated character recognition, and thereby enhance character-centric understanding of animated movies. Central to our approach is the automatic construction of an audio-visual character bank from online sources. This bank contains both visual exemplars and voice (audio) samples for each character, enabling subsequent multi-modal character recognition despite long-tailed appearance distributions. Building on accurate character recognition, we explore two downstream applications: Audio Description (AD) generation for visually impaired audiences, and character-aware subtitling for the hearing impaired. To support research in this domain, we introduce CMD-AM, a new dataset of 75 animated movies with comprehensive annotations. Our character-centric pipeline demonstrates significant improvements in both accessibility and narrative comprehension for animated content over prior face-detection-based approaches. For the code and dataset, visit this https URL.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
