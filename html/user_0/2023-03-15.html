<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: GazePair: Efficient Pairing of Augmented Reality Devices Using Gaze Tracking. (arXiv:2303.07404v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.07404">http://arxiv.org/abs/2303.07404</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.07404] GazePair: Efficient Pairing of Augmented Reality Devices Using Gaze Tracking](http://arxiv.org/abs/2303.07404) #secure</code></li>
<li>Summary: <p>As Augmented Reality (AR) devices become more prevalent and commercially
viable, the need for quick, efficient, and secure schemes for pairing these
devices has become more pressing. Current methods to securely exchange
holograms require users to send this information through large data centers,
creating security and privacy concerns. Existing techniques to pair these
devices on a local network and share information fall short in terms of
usability and scalability. These techniques either require hardware not
available on AR devices, intricate physical gestures, removal of the device
from the head, do not scale to multiple pairing partners, or rely on methods
with low entropy to create encryption keys. To that end, we propose a novel
pairing system, called GazePair, that improves on all existing local pairing
techniques by creating an efficient, effective, and intuitive pairing protocol.
GazePair uses eye gaze tracking and a spoken key sequence cue (KSC) to generate
identical, independently generated symmetric encryption keys with 64 bits of
entropy. GazePair also achieves improvements in pairing success rates and times
over current methods. Additionally, we show that GazePair can extend to
multiple users. Finally, we assert that GazePair can be used on any Mixed
Reality (MR) device equipped with eye gaze tracking.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Improving Java Deserialization Gadget Chain Mining via Overriding-Guided Object Generation. (arXiv:2303.07593v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.07593">http://arxiv.org/abs/2303.07593</a></li>
<li>Code URL: <a href="https://github.com/gcminer/gcminer">https://github.com/gcminer/gcminer</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.07593] Improving Java Deserialization Gadget Chain Mining via Overriding-Guided Object Generation](http://arxiv.org/abs/2303.07593) #security</code></li>
<li>Summary: <p>Java (de)serialization is prone to causing security-critical vulnerabilities
that attackers can invoke existing methods (gadgets) on the application's
classpath to construct a gadget chain to perform malicious behaviors. Several
techniques have been proposed to statically identify suspicious gadget chains
and dynamically generate injection objects for fuzzing. However, due to their
incomplete support for dynamic program features (e.g., Java runtime
polymorphism) and ineffective injection object generation for fuzzing, the
existing techniques are still far from satisfactory.
</p></li>
</ul>

<p>In this paper, we first performed an empirical study to investigate the
characteristics of Java deserialization vulnerabilities based on our manually
collected 86 publicly known gadget chains. The empirical results show that 1)
Java deserialization gadgets are usually exploited by abusing runtime
polymorphism, which enables attackers to reuse serializable overridden methods;
and 2) attackers usually invoke exploitable overridden methods (gadgets) via
dynamic binding to generate injection objects for gadget chain construction.
Based on our empirical findings, we propose a novel gadget chain mining
approach, \emph{GCMiner}, which captures both explicit and implicit method
calls to identify more gadget chains, and adopts an overriding-guided object
generation approach to generate valid injection objects for fuzzing. The
evaluation results show that \emph{GCMiner} significantly outperforms the
state-of-the-art techniques, and discovers 56 unique gadget chains that cannot
be identified by the baseline approaches.
</p>

<h3>Title: Software-based security approach for networked embedded devices. (arXiv:2303.07975v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.07975">http://arxiv.org/abs/2303.07975</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.07975] Software-based security approach for networked embedded devices](http://arxiv.org/abs/2303.07975) #security</code></li>
<li>Summary: <p>As the Internet of Things (IoT) continues to expand, data security has become
increasingly important for ensuring privacy and safety, especially given the
sensitive and, sometimes, critical nature of the data handled by IoT devices.
There exist hardware-based trusted execution environments used to protect data,
but they are not compatible with low-cost devices that lack hardware-assisted
security features. The research in this paper presents software-based
protection and encryption mechanisms explicitly designed for embedded devices.
The proposed architecture is designed to work with low-cost, low-end devices
without requiring the usual changes on the underlying hardware. It protects
against hardware attacks and supports runtime updates, enabling devices to
write data in protected memory. The proposed solution is an alternative data
security approach for low-cost IoT devices without compromising performance or
functionality. Our work underscores the importance of developing secure and
cost-effective solutions for protecting data in the context of IoT.
</p></li>
</ul>

<h3>Title: Half-Day Vulnerabilities: A study of the First Days of CVE Entries. (arXiv:2303.07990v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.07990">http://arxiv.org/abs/2303.07990</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.07990] Half-Day Vulnerabilities: A study of the First Days of CVE Entries](http://arxiv.org/abs/2303.07990) #security</code></li>
<li>Summary: <p>The National Vulnerability Disclosure Database is an invaluable source of
information for security professionals and researchers. However, in some cases,
a vulnerability report is initially published with incomplete information, a
situation that complicates incident response and mitigation. In this paper, we
perform an empirical study of vulnerabilities that are initially submitted with
an incomplete report, and present key findings related to their frequency,
nature, and the time needed to update them. We further present a novel
ticketing process that is tailored to addressing the problems related to such
vulnerabilities and demonstrate the use of this system with a real-life use
case.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Schr\"odinger's Camera: First Steps Towards a Quantum-Based Privacy Preserving Camera. (arXiv:2303.07510v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.07510">http://arxiv.org/abs/2303.07510</a></li>
<li>Code URL: <a href="https://github.com/teawizardry/schrodingers-camera">https://github.com/teawizardry/schrodingers-camera</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.07510] Schr\"odinger's Camera: First Steps Towards a Quantum-Based Privacy Preserving Camera](http://arxiv.org/abs/2303.07510) #privacy</code></li>
<li>Summary: <p>Privacy-preserving vision must overcome the dual challenge of utility and
privacy. Too much anonymity renders the images useless, but too little privacy
does not protect sensitive data. We propose a novel design for privacy
preservation, where the imagery is stored in quantum states. In the future,
this will be enabled by quantum imaging cameras, and, currently, storing very
low resolution imagery in quantum states is possible. Quantum state imagery has
the advantage of being both private and non-private till the point of
measurement. This occurs even when images are manipulated, since every quantum
action is fully reversible. We propose a control algorithm, based on double
deep Q-learning, to learn how to anonymize the image before measurement. After
learning, the RL weights are fixed, and new attack neural networks are trained
from scratch to break the system's privacy. Although all our results are in
simulation, we demonstrate, with these first steps, that it is possible to
control both privacy and utility in a quantum-based manner.
</p></li>
</ul>

<h3>Title: Data-Free Sketch-Based Image Retrieval. (arXiv:2303.07775v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.07775">http://arxiv.org/abs/2303.07775</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.07775] Data-Free Sketch-Based Image Retrieval](http://arxiv.org/abs/2303.07775) #privacy</code></li>
<li>Summary: <p>Rising concerns about privacy and anonymity preservation of deep learning
models have facilitated research in data-free learning (DFL). For the first
time, we identify that for data-scarce tasks like Sketch-Based Image Retrieval
(SBIR), where the difficulty in acquiring paired photos and hand-drawn sketches
limits data-dependent cross-modal learning algorithms, DFL can prove to be a
much more practical paradigm. We thus propose Data-Free (DF)-SBIR, where,
unlike existing DFL problems, pre-trained, single-modality classification
models have to be leveraged to learn a cross-modal metric-space for retrieval
without access to any training data. The widespread availability of pre-trained
classification models, along with the difficulty in acquiring paired
photo-sketch datasets for SBIR justify the practicality of this setting. We
present a methodology for DF-SBIR, which can leverage knowledge from models
independently trained to perform classification on photos and sketches. We
evaluate our model on the Sketchy, TU-Berlin, and QuickDraw benchmarks,
designing a variety of baselines based on state-of-the-art DFL literature, and
observe that our method surpasses all of them by significant margins. Our
method also achieves mAPs competitive with data-dependent approaches, all the
while requiring no training data. Implementation is available at
\url{https://github.com/abhrac/data-free-sbir}.
</p></li>
</ul>

<h3>Title: Inferential Privacy: From Impossibility to Database Privacy. (arXiv:2303.07782v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.07782">http://arxiv.org/abs/2303.07782</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.07782] Inferential Privacy: From Impossibility to Database Privacy](http://arxiv.org/abs/2303.07782) #privacy</code></li>
<li>Summary: <p>We investigate the possibility of guaranteeing inferential privacy for
mechanisms that release useful information about some data containing sensitive
information, denoted by $X$. We describe a general model of utility and privacy
in which utility is achieved by disclosing the value of low-entropy features of
$X$, while privacy is maintained by keeping high-entropy features of $X$
secret. Adopting this model, we prove that meaningful inferential privacy
guarantees can be obtained, even though this is commonly considered to be
impossible by the well-known result of Dwork and Naor. Then, we specifically
discuss a privacy measure called pointwise maximal leakage (PML) whose
guarantees are of the inferential type. We use PML to show that differential
privacy admits an inferential formulation: it describes the information leaking
about a single entry in a database assuming that every other entry is known,
and considering the worst-case distribution on the data. Moreover, we define
inferential instance privacy (IIP) as a bound on the (non-conditional)
information leaking about a single entry in the database under the worst-case
distribution, and show that it is equivalent to free-lunch privacy. Overall,
our approach to privacy unifies, formalizes, and explains many existing ideas,
e.g., why the informed adversary assumption may lead to underestimating the
information leaking about each entry in the database. Furthermore, insights
obtained from our results suggest general methods for improving privacy
analyses; for example, we argue that smaller privacy parameters can be obtained
by excluding low-entropy prior distributions from protection.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h2>attack</h2>
<h3>Title: Can Adversarial Examples Be Parsed to Reveal Victim Model Information?. (arXiv:2303.07474v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.07474">http://arxiv.org/abs/2303.07474</a></li>
<li>Code URL: <a href="https://github.com/optml-group/red-adv">https://github.com/optml-group/red-adv</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.07474] Can Adversarial Examples Be Parsed to Reveal Victim Model Information?](http://arxiv.org/abs/2303.07474) #attack</code></li>
<li>Summary: <p>Numerous adversarial attack methods have been developed to generate
imperceptible image perturbations that can cause erroneous predictions of
state-of-the-art machine learning (ML) models, in particular, deep neural
networks (DNNs). Despite intense research on adversarial attacks, little effort
was made to uncover 'arcana' carried in adversarial attacks. In this work, we
ask whether it is possible to infer data-agnostic victim model (VM) information
(i.e., characteristics of the ML model or DNN used to generate adversarial
attacks) from data-specific adversarial instances. We call this 'model parsing
of adversarial attacks' - a task to uncover 'arcana' in terms of the concealed
VM information in attacks. We approach model parsing via supervised learning,
which correctly assigns classes of VM's model attributes (in terms of
architecture type, kernel size, activation function, and weight sparsity) to an
attack instance generated from this VM. We collect a dataset of adversarial
attacks across 7 attack types generated from 135 victim models (configured by 5
architecture types, 3 kernel size setups, 3 activation function types, and 3
weight sparsity ratios). We show that a simple, supervised model parsing
network (MPN) is able to infer VM attributes from unseen adversarial attacks if
their attack settings are consistent with the training setting (i.e.,
in-distribution generalization assessment). We also provide extensive
experiments to justify the feasibility of VM parsing from adversarial attacks,
and the influence of training and evaluation factors in the parsing performance
(e.g., generalization challenge raised in out-of-distribution evaluation). We
further demonstrate how the proposed MPN can be used to uncover the source VM
attributes from transfer attacks, and shed light on a potential connection
between model parsing and attack transferability.
</p></li>
</ul>

<h3>Title: BODEGA: Benchmark for Adversarial Example Generation in Credibility Assessment. (arXiv:2303.08032v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.08032">http://arxiv.org/abs/2303.08032</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.08032] BODEGA: Benchmark for Adversarial Example Generation in Credibility Assessment](http://arxiv.org/abs/2303.08032) #attack</code></li>
<li>Summary: <p>Text classification methods have been widely investigated as a way to detect
content of low credibility: fake news, social media bots, propaganda, etc.
Quite accurate models (likely based on deep neural networks) help in moderating
public electronic platforms and often cause content creators to face rejection
of their submissions or removal of already published texts. Having the
incentive to evade further detection, content creators try to come up with a
slightly modified version of the text (known as an attack with an adversarial
example) that exploit the weaknesses of classifiers and result in a different
output. Here we introduce BODEGA: a benchmark for testing both victim models
and attack methods on four misinformation detection tasks in an evaluation
framework designed to simulate real use-cases of content moderation. We also
systematically test the robustness of popular text classifiers against
available attacking techniques and discover that, indeed, in some cases barely
significant changes in input text can mislead the models. We openly share the
BODEGA code and data in hope of enhancing the comparability and replicability
of further research in this area.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Efficient Self-supervised Continual Learning with Progressive Task-correlated Layer Freezing. (arXiv:2303.07477v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.07477">http://arxiv.org/abs/2303.07477</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.07477] Efficient Self-supervised Continual Learning with Progressive Task-correlated Layer Freezing](http://arxiv.org/abs/2303.07477) #robust</code></li>
<li>Summary: <p>Inspired by the success of Self-supervised learning (SSL) in learning visual
representations from unlabeled data, a few recent works have studied SSL in the
context of continual learning (CL), where multiple tasks are learned
sequentially, giving rise to a new paradigm, namely self-supervised continual
learning (SSCL). It has been shown that the SSCL outperforms supervised
continual learning (SCL) as the learned representations are more informative
and robust to catastrophic forgetting. However, if not designed intelligently,
the training complexity of SSCL may be prohibitively high due to the inherent
training cost of SSL. In this work, by investigating the task correlations in
SSCL setup first, we discover an interesting phenomenon that, with the
SSL-learned background model, the intermediate features are highly correlated
between tasks. Based on this new finding, we propose a new SSCL method with
layer-wise freezing which progressively freezes partial layers with the highest
correlation ratios for each task to improve training computation efficiency and
memory efficiency. Extensive experiments across multiple datasets are
performed, where our proposed method shows superior performance against the
SoTA SSCL methods under various SSL frameworks. For example, compared to LUMP,
our method achieves 12\%/14\%/12\% GPU training time reduction, 23\%/26\%/24\%
memory reduction, 35\%/34\%/33\% backward FLOPs reduction, and
1.31\%/1.98\%/1.21\% forgetting reduction without accuracy degradation on three
datasets, respectively.
</p></li>
</ul>

<h3>Title: MRET: Multi-resolution Transformer for Video Quality Assessment. (arXiv:2303.07489v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.07489">http://arxiv.org/abs/2303.07489</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.07489] MRET: Multi-resolution Transformer for Video Quality Assessment](http://arxiv.org/abs/2303.07489) #robust</code></li>
<li>Summary: <p>No-reference video quality assessment (NR-VQA) for user generated content
(UGC) is crucial for understanding and improving visual experience. Unlike
video recognition tasks, VQA tasks are sensitive to changes in input
resolution. Since large amounts of UGC videos nowadays are 720p or above, the
fixed and relatively small input used in conventional NR-VQA methods results in
missing high-frequency details for many videos. In this paper, we propose a
novel Transformer-based NR-VQA framework that preserves the high-resolution
quality information. With the multi-resolution input representation and a novel
multi-resolution patch sampling mechanism, our method enables a comprehensive
view of both the global video composition and local high-resolution details.
The proposed approach can effectively aggregate quality information across
different granularities in spatial and temporal dimensions, making the model
robust to input resolution variations. Our method achieves state-of-the-art
performance on large-scale UGC VQA datasets LSVQ and LSVQ-1080p, and on
KoNViD-1k and LIVE-VQC without fine-tuning.
</p></li>
</ul>

<h3>Title: HazardNet: Road Debris Detection by Augmentation of Synthetic Models. (arXiv:2303.07547v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.07547">http://arxiv.org/abs/2303.07547</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.07547] HazardNet: Road Debris Detection by Augmentation of Synthetic Models](http://arxiv.org/abs/2303.07547) #robust</code></li>
<li>Summary: <p>We present an algorithm to detect unseen road debris using a small set of
synthetic models. Early detection of road debris is critical for safe
autonomous or assisted driving, yet the development of a robust road debris
detection model has not been widely discussed. There are two main challenges to
building a road debris detector: first, data collection of road debris is
challenging since hazardous objects on the road are rare to encounter in real
driving scenarios; second, the variability of road debris is broad, ranging
from a very small brick to a large fallen tree. To overcome these challenges,
we propose a novel approach to few-shot learning of road debris that uses
semantic augmentation and domain randomization to augment real road images with
synthetic models. We constrain the problem domain to uncommon objects on the
road and allow the deep neural network, HazardNet, to learn the semantic
meaning of road debris to eventually detect unseen road debris. Our results
demonstrate that HazardNet is able to accurately detect real road debris when
only trained on synthetic objects in augmented images.
</p></li>
</ul>

<h3>Title: Modeling Continuous Motion for 3D Point Cloud Object Tracking. (arXiv:2303.07605v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.07605">http://arxiv.org/abs/2303.07605</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.07605] Modeling Continuous Motion for 3D Point Cloud Object Tracking](http://arxiv.org/abs/2303.07605) #robust</code></li>
<li>Summary: <p>The task of 3D single object tracking (SOT) with LiDAR point clouds is
crucial for various applications, such as autonomous driving and robotics.
However, existing approaches have primarily relied on appearance matching or
motion modeling within only two successive frames, thereby overlooking the
long-range continuous motion property of objects in 3D space. To address this
issue, this paper presents a novel approach that views each tracklet as a
continuous stream: at each timestamp, only the current frame is fed into the
network to interact with multi-frame historical features stored in a memory
bank, enabling efficient exploitation of sequential information. To achieve
effective cross-frame message passing, a hybrid attention mechanism is designed
to account for both long-range relation modeling and local geometric feature
extraction. Furthermore, to enhance the utilization of multi-frame features for
robust tracking, a contrastive sequence enhancement strategy is designed, which
uses ground truth tracklets to augment training sequences and promote
discrimination against false positives in a contrastive manner. Extensive
experiments demonstrate that the proposed method outperforms the
state-of-the-art method by significant margins (approximately 8%, 6%, and 12%
improvements in the success performance on KITTI, nuScenes, and Waymo,
respectively).
</p></li>
</ul>

<h3>Title: Training Robust Spiking Neural Networks with ViewPoint Transform and SpatioTemporal Stretching. (arXiv:2303.07609v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.07609">http://arxiv.org/abs/2303.07609</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.07609] Training Robust Spiking Neural Networks with ViewPoint Transform and SpatioTemporal Stretching](http://arxiv.org/abs/2303.07609) #robust</code></li>
<li>Summary: <p>Neuromorphic vision sensors (event cameras) simulate biological visual
perception systems and have the advantages of high temporal resolution, less
data redundancy, low power consumption, and large dynamic range. Since both
events and spikes are modeled from neural signals, event cameras are inherently
suitable for spiking neural networks (SNNs), which are considered promising
models for artificial intelligence (AI) and theoretical neuroscience. However,
the unconventional visual signals of these cameras pose a great challenge to
the robustness of spiking neural networks. In this paper, we propose a novel
data augmentation method, ViewPoint Transform and SpatioTemporal Stretching
(VPT-STS). It improves the robustness of SNNs by transforming the rotation
centers and angles in the spatiotemporal domain to generate samples from
different viewpoints. Furthermore, we introduce the spatiotemporal stretching
to avoid potential information loss in viewpoint transformation. Extensive
experiments on prevailing neuromorphic datasets demonstrate that VPT-STS is
broadly effective on multi-event representations and significantly outperforms
pure spatial geometric transformations. Notably, the SNNs model with VPT-STS
achieves a state-of-the-art accuracy of 84.4\% on the DVS-CIFAR10 dataset.
</p></li>
</ul>

<h3>Title: Context Normalization for Robust Image Classification. (arXiv:2303.07651v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.07651">http://arxiv.org/abs/2303.07651</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.07651] Context Normalization for Robust Image Classification](http://arxiv.org/abs/2303.07651) #robust</code></li>
<li>Summary: <p>Normalization is a pre-processing step that converts the data into a more
usable representation. As part of the deep neural networks (DNNs), the batch
normalization (BN) technique uses normalization to address the problem of
internal covariate shift. It can be packaged as general modules, which have
been extensively integrated into various DNNs, to stabilize and accelerate
training, presumably leading to improved generalization. However, the effect of
BN is dependent on the mini-batch size and it does not take into account any
groups or clusters that may exist in the dataset when estimating population
statistics. This study proposes a new normalization technique, called context
normalization, for image data. This approach adjusts the scaling of features
based on the characteristics of each sample, which improves the model's
convergence speed and performance by adapting the data values to the context of
the target task. The effectiveness of context normalization is demonstrated on
various datasets, and its performance is compared to other standard
normalization techniques.
</p></li>
</ul>

<h3>Title: NEF: Neural Edge Fields for 3D Parametric Curve Reconstruction from Multi-view Images. (arXiv:2303.07653v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.07653">http://arxiv.org/abs/2303.07653</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.07653] NEF: Neural Edge Fields for 3D Parametric Curve Reconstruction from Multi-view Images](http://arxiv.org/abs/2303.07653) #robust</code></li>
<li>Summary: <p>We study the problem of reconstructing 3D feature curves of an object from a
set of calibrated multi-view images. To do so, we learn a neural implicit field
representing the density distribution of 3D edges which we refer to as Neural
Edge Field (NEF). Inspired by NeRF, NEF is optimized with a view-based
rendering loss where a 2D edge map is rendered at a given view and is compared
to the ground-truth edge map extracted from the image of that view. The
rendering-based differentiable optimization of NEF fully exploits 2D edge
detection, without needing a supervision of 3D edges, a 3D geometric operator
or cross-view edge correspondence. Several technical designs are devised to
ensure learning a range-limited and view-independent NEF for robust edge
extraction. The final parametric 3D curves are extracted from NEF with an
iterative optimization method. On our benchmark with synthetic data, we
demonstrate that NEF outperforms existing state-of-the-art methods on all
metrics. Project page: https://yunfan1202.github.io/NEF/.
</p></li>
</ul>

<h3>Title: HALOS: Hallucination-free Organ Segmentation after Organ Resection Surgery. (arXiv:2303.07717v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.07717">http://arxiv.org/abs/2303.07717</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.07717] HALOS: Hallucination-free Organ Segmentation after Organ Resection Surgery](http://arxiv.org/abs/2303.07717) #robust</code></li>
<li>Summary: <p>The wide range of research in deep learning-based medical image segmentation
pushed the boundaries in a multitude of applications. A clinically relevant
problem that received less attention is the handling of scans with irregular
anatomy, e.g., after organ resection. State-of-the-art segmentation models
often lead to organ hallucinations, i.e., false-positive predictions of organs,
which cannot be alleviated by oversampling or post-processing. Motivated by the
increasing need to develop robust deep learning models, we propose HALOS for
abdominal organ segmentation in MR images that handles cases after organ
resection surgery. To this end, we combine missing organ classification and
multi-organ segmentation tasks into a multi-task model, yielding a
classification-assisted segmentation pipeline. The segmentation network learns
to incorporate knowledge about organ existence via feature fusion modules.
Extensive experiments on a small labeled test set and large-scale UK Biobank
data demonstrate the effectiveness of our approach in terms of higher
segmentation Dice scores and near-to-zero false positive prediction rate.
</p></li>
</ul>

<h3>Title: Imbalanced Domain Generalization for Robust Single Cell Classification in Hematological Cytomorphology. (arXiv:2303.07771v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.07771">http://arxiv.org/abs/2303.07771</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.07771] Imbalanced Domain Generalization for Robust Single Cell Classification in Hematological Cytomorphology](http://arxiv.org/abs/2303.07771) #robust</code></li>
<li>Summary: <p>Accurate morphological classification of white blood cells (WBCs) is an
important step in the diagnosis of leukemia, a disease in which nonfunctional
blast cells accumulate in the bone marrow. Recently, deep convolutional neural
networks (CNNs) have been successfully used to classify leukocytes by training
them on single-cell images from a specific domain. Most CNN models assume that
the distributions of the training and test data are similar, i.e., that the
data are independently and identically distributed. Therefore, they are not
robust to different staining protocols, magnifications, resolutions, scanners,
or imaging protocols, as well as variations in clinical centers or patient
cohorts. In addition, domain-specific data imbalances affect the generalization
performance of classifiers. Here, we train a robust CNN for WBC classification
by addressing cross-domain data imbalance and domain shifts. To this end, we
use two loss functions and demonstrate the effectiveness on out-of-distribution
(OOD) generalization. Our approach achieves the best F1 macro score compared to
other existing methods, and is able to consider rare cell types. This is the
first demonstration of imbalanced domain generalization in hematological
cytomorphology and paves the way for robust single cell classification methods
for the application in laboratories and clinics.
</p></li>
</ul>

<h3>Title: OVRL-V2: A simple state-of-art baseline for ImageNav and ObjectNav. (arXiv:2303.07798v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.07798">http://arxiv.org/abs/2303.07798</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.07798] OVRL-V2: A simple state-of-art baseline for ImageNav and ObjectNav](http://arxiv.org/abs/2303.07798) #robust</code></li>
<li>Summary: <p>We present a single neural network architecture composed of task-agnostic
components (ViTs, convolutions, and LSTMs) that achieves state-of-art results
on both the ImageNav ("go to location in <this picture>") and ObjectNav ("find
a chair") tasks without any task-specific modules like object detection,
segmentation, mapping, or planning modules. Such general-purpose methods offer
advantages of simplicity in design, positive scaling with available compute,
and versatile applicability to multiple tasks. Our work builds upon the recent
success of self-supervised learning (SSL) for pre-training vision transformers
(ViT). However, while the training recipes for convolutional networks are
mature and robust, the recipes for ViTs are contingent and brittle, and in the
case of ViTs for visual navigation, yet to be fully discovered. Specifically,
we find that vanilla ViTs do not outperform ResNets on visual navigation. We
propose the use of a compression layer operating over ViT patch representations
to preserve spatial information along with policy training improvements. These
improvements allow us to demonstrate positive scaling laws for the first time
in visual navigation tasks. Consequently, our model advances state-of-the-art
performance on ImageNav from 54.2% to 82.0% success and performs competitively
against concurrent state-of-art on ObjectNav with success rate of 64.0% vs.
65.0%. Overall, this work does not present a fundamentally new approach, but
rather recommendations for training a general-purpose architecture that
achieves state-of-art performance today and could serve as a strong baseline
for future methods.
</p></li>
</ul>

<h3>Title: Kinematic Data-Based Action Segmentation for Surgical Applications. (arXiv:2303.07814v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.07814">http://arxiv.org/abs/2303.07814</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.07814] Kinematic Data-Based Action Segmentation for Surgical Applications](http://arxiv.org/abs/2303.07814) #robust</code></li>
<li>Summary: <p>Action segmentation is a challenging task in high-level process analysis,
typically performed on video or kinematic data obtained from various sensors.
In the context of surgical procedures, action segmentation is critical for
workflow analysis algorithms. This work presents two contributions related to
action segmentation on kinematic data. Firstly, we introduce two multi-stage
architectures, MS-TCN-BiLSTM and MS-TCN-BiGRU, specifically designed for
kinematic data. The architectures consist of a prediction generator with
intra-stage regularization and Bidirectional LSTM or GRU-based refinement
stages. Secondly, we propose two new data augmentation techniques, World Frame
Rotation and Horizontal-Flip, which utilize the strong geometric structure of
kinematic data to improve algorithm performance and robustness. We evaluate our
models on three datasets of surgical suturing tasks: the Variable Tissue
Simulation (VTS) Dataset and the newly introduced Bowel Repair Simulation (BRS)
Dataset, both of which are open surgery simulation datasets collected by us, as
well as the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS), a
well-known benchmark in robotic surgery. Our methods achieve state-of-the-art
performance on all benchmark datasets and establish a strong baseline for the
BRS dataset.
</p></li>
</ul>

<h3>Title: Object-based SLAM utilizing unambiguous pose parameters considering general symmetry types. (arXiv:2303.07872v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.07872">http://arxiv.org/abs/2303.07872</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.07872] Object-based SLAM utilizing unambiguous pose parameters considering general symmetry types](http://arxiv.org/abs/2303.07872) #robust</code></li>
<li>Summary: <p>Existence of symmetric objects, whose observation at different viewpoints can
be identical, can deteriorate the performance of simultaneous localization and
mapping(SLAM). This work proposes a system for robustly optimizing the pose of
cameras and objects even in the presence of symmetric objects. We classify
objects into three categories depending on their symmetry characteristics,
which is efficient and effective in that it allows to deal with general objects
and the objects in the same category can be associated with the same type of
ambiguity. Then we extract only the unambiguous parameters corresponding to
each category and use them in data association and joint optimization of the
camera and object pose. The proposed approach provides significant robustness
to the SLAM performance by removing the ambiguous parameters and utilizing as
much useful geometric information as possible. Comparison with baseline
algorithms confirms the superior performance of the proposed system in terms of
object tracking and pose estimation, even in challenging scenarios where the
baseline fails.
</p></li>
</ul>

<h3>Title: Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D Generation. (arXiv:2303.07937v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.07937">http://arxiv.org/abs/2303.07937</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.07937] Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D Generation](http://arxiv.org/abs/2303.07937) #robust</code></li>
<li>Summary: <p>Text-to-3D generation has shown rapid progress in recent days with the advent
of score distillation, a methodology of using pretrained text-to-2D diffusion
models to optimize neural radiance field (NeRF) in the zero-shot setting.
However, the lack of 3D awareness in the 2D diffusion models destabilizes score
distillation-based methods from reconstructing a plausible 3D scene. To address
this issue, we propose \ours, a novel framework that incorporates 3D awareness
into pretrained 2D diffusion models, enhancing the robustness and 3D
consistency of score distillation-based methods. We realize this by first
constructing a coarse 3D structure of a given text prompt and then utilizing
projected, view-specific depth map as a condition for the diffusion model.
Additionally, we introduce a training strategy that enables the 2D diffusion
model learns to handle the errors and sparsity within the coarse 3D structure
for robust generation, as well as a method for ensuring semantic consistency
throughout all viewpoints of the scene. Our framework surpasses the limitations
of prior arts, and has significant implications for 3D consistent generation of
2D diffusion models.
</p></li>
</ul>

<h3>Title: Non-Contrastive Unsupervised Learning of Physiological Signals from Video. (arXiv:2303.07944v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.07944">http://arxiv.org/abs/2303.07944</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.07944] Non-Contrastive Unsupervised Learning of Physiological Signals from Video](http://arxiv.org/abs/2303.07944) #robust</code></li>
<li>Summary: <p>Subtle periodic signals such as blood volume pulse and respiration can be
extracted from RGB video, enabling remote health monitoring at low cost.
Advancements in remote pulse estimation -- or remote photoplethysmography
(rPPG) -- are currently driven by deep learning solutions. However, modern
approaches are trained and evaluated on benchmark datasets with associated
ground truth from contact-PPG sensors. We present the first non-contrastive
unsupervised learning framework for signal regression to break free from the
constraints of labelled video data. With minimal assumptions of periodicity and
finite bandwidth, our approach is capable of discovering the blood volume pulse
directly from unlabelled videos. We find that encouraging sparse power spectra
within normal physiological bandlimits and variance over batches of power
spectra is sufficient for learning visual features of periodic signals. We
perform the first experiments utilizing unlabelled video data not specifically
created for rPPG to train robust pulse rate estimators. Given the limited
inductive biases and impressive empirical results, the approach is
theoretically capable of discovering other periodic signals from video,
enabling multiple physiological measurements without the need for ground truth
signals. Codes to fully reproduce the experiments are made available along with
the paper.
</p></li>
</ul>

<h3>Title: RoCNet: 3D Robust Registration of Point-Clouds using Deep Learning. (arXiv:2303.07963v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.07963">http://arxiv.org/abs/2303.07963</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.07963] RoCNet: 3D Robust Registration of Point-Clouds using Deep Learning](http://arxiv.org/abs/2303.07963) #robust</code></li>
<li>Summary: <p>This paper introduces a new method for 3D point cloud registration based on
deep learning. The architecture is composed of three distinct blocs: (i) an
encoder composed of a convolutional graph-based descriptor that encodes the
immediate neighbourhood of each point and an attention mechanism that encodes
the variations of the surface normals. Such descriptors are refined by
highlighting attention between the points of the same set and then between the
points of the two sets. (ii) a matching process that estimates a matrix of
correspondences using the Sinkhorn algorithm. (iii) Finally, the rigid
transformation between the two point clouds is calculated by RANSAC using the
Kc best scores from the correspondence matrix. We conduct experiments on the
ModelNet40 dataset, and our proposed architecture shows very promising results,
outperforming state-of-the-art methods in most of the simulated configurations,
including partial overlap and data augmentation with Gaussian noise.
</p></li>
</ul>

<h3>Title: ISimDL: Importance Sampling-Driven Acceleration of Fault Injection Simulations for Evaluating the Robustness of Deep Learning. (arXiv:2303.08035v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.08035">http://arxiv.org/abs/2303.08035</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.08035] ISimDL: Importance Sampling-Driven Acceleration of Fault Injection Simulations for Evaluating the Robustness of Deep Learning](http://arxiv.org/abs/2303.08035) #robust</code></li>
<li>Summary: <p>Deep Learning (DL) systems have proliferated in many applications, requiring
specialized hardware accelerators and chips. In the nano-era, devices have
become increasingly more susceptible to permanent and transient faults.
Therefore, we need an efficient methodology for analyzing the resilience of
advanced DL systems against such faults, and understand how the faults in
neural accelerator chips manifest as errors at the DL application level, where
faults can lead to undetectable and unrecoverable errors. Using fault
injection, we can perform resilience investigations of the DL system by
modifying neuron weights and outputs at the software-level, as if the hardware
had been affected by a transient fault. Existing fault models reduce the search
space, allowing faster analysis, but requiring a-priori knowledge on the model,
and not allowing further analysis of the filtered-out search space. Therefore,
we propose ISimDL, a novel methodology that employs neuron sensitivity to
generate importance sampling-based fault-scenarios. Without any a-priori
knowledge of the model-under-test, ISimDL provides an equivalent reduction of
the search space as existing works, while allowing long simulations to cover
all the possible faults, improving on existing model requirements. Our
experiments show that the importance sampling provides up to 15x higher
precision in selecting critical faults than the random uniform sampling,
reaching such precision in less than 100 faults. Additionally, we showcase
another practical use-case for importance sampling for reliable DNN design,
namely Fault Aware Training (FAT). By using ISimDL to select the faults leading
to errors, we can insert the faults during the DNN training process to harden
the DNN against such faults. Using importance sampling in FAT reduces the
overhead required for finding faults that lead to a predetermined drop in
accuracy by more than 12x.
</p></li>
</ul>

<h3>Title: Alias-Free Convnets: Fractional Shift Invariance via Polynomial Activations. (arXiv:2303.08085v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.08085">http://arxiv.org/abs/2303.08085</a></li>
<li>Code URL: <a href="https://github.com/hmichaeli/alias_free_convnets">https://github.com/hmichaeli/alias_free_convnets</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.08085] Alias-Free Convnets: Fractional Shift Invariance via Polynomial Activations](http://arxiv.org/abs/2303.08085) #robust</code></li>
<li>Summary: <p>Although CNNs are believed to be invariant to translations, recent works have
shown this is not the case, due to aliasing effects that stem from downsampling
layers. The existing architectural solutions to prevent aliasing are partial
since they do not solve these effects, that originate in non-linearities. We
propose an extended anti-aliasing method that tackles both downsampling and
non-linear layers, thus creating truly alias-free, shift-invariant CNNs. We
show that the presented model is invariant to integer as well as fractional
(i.e., sub-pixel) translations, thus outperforming other shift-invariant
methods in terms of robustness to adversarial translations.
</p></li>
</ul>

<h3>Title: InstMove: Instance Motion for Object-centric Video Segmentation. (arXiv:2303.08132v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.08132">http://arxiv.org/abs/2303.08132</a></li>
<li>Code URL: <a href="https://github.com/wjf5203/vnext">https://github.com/wjf5203/vnext</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.08132] InstMove: Instance Motion for Object-centric Video Segmentation](http://arxiv.org/abs/2303.08132) #robust</code></li>
<li>Summary: <p>Despite significant efforts, cutting-edge video segmentation methods still
remain sensitive to occlusion and rapid movement, due to their reliance on the
appearance of objects in the form of object embeddings, which are vulnerable to
these disturbances. A common solution is to use optical flow to provide motion
information, but essentially it only considers pixel-level motion, which still
relies on appearance similarity and hence is often inaccurate under occlusion
and fast movement. In this work, we study the instance-level motion and present
InstMove, which stands for Instance Motion for Object-centric Video
Segmentation. In comparison to pixel-wise motion, InstMove mainly relies on
instance-level motion information that is free from image feature embeddings,
and features physical interpretations, making it more accurate and robust
toward occlusion and fast-moving objects. To better fit in with the video
segmentation tasks, InstMove uses instance masks to model the physical presence
of an object and learns the dynamic model through a memory network to predict
its position and shape in the next frame. With only a few lines of code,
InstMove can be integrated into current SOTA methods for three different video
segmentation tasks and boost their performance. Specifically, we improve the
previous arts by 1.5 AP on OVIS dataset, which features heavy occlusions, and
4.9 AP on YouTubeVIS-Long dataset, which mainly contains fast-moving objects.
These results suggest that instance-level motion is robust and accurate, and
hence serving as a powerful solution in complex scenarios for object-centric
video segmentation.
</p></li>
</ul>

<h3>Title: Improving Accented Speech Recognition with Multi-Domain Training. (arXiv:2303.07924v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.07924">http://arxiv.org/abs/2303.07924</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.07924] Improving Accented Speech Recognition with Multi-Domain Training](http://arxiv.org/abs/2303.07924) #robust</code></li>
<li>Summary: <p>Thanks to the rise of self-supervised learning, automatic speech recognition
(ASR) systems now achieve near-human performance on a wide variety of datasets.
However, they still lack generalization capability and are not robust to domain
shifts like accent variations. In this work, we use speech audio representing
four different French accents to create fine-tuning datasets that improve the
robustness of pre-trained ASR models. By incorporating various accents in the
training set, we obtain both in-domain and out-of-domain improvements. Our
numerical experiments show that we can reduce error rates by up to 25%
(relative) on African and Belgian accents compared to single-domain training
while keeping a good performance on standard French.
</p></li>
</ul>

<h3>Title: Fractional dynamics foster deep learning of COPD stage prediction. (arXiv:2303.07537v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.07537">http://arxiv.org/abs/2303.07537</a></li>
<li>Code URL: <a href="https://github.com/chenzhoy/fractional-dynamics-foster-deep-learning-of-copdstage-prediction">https://github.com/chenzhoy/fractional-dynamics-foster-deep-learning-of-copdstage-prediction</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.07537] Fractional dynamics foster deep learning of COPD stage prediction](http://arxiv.org/abs/2303.07537) #robust</code></li>
<li>Summary: <p>Chronic obstructive pulmonary disease (COPD) is one of the leading causes of
death worldwide. Current COPD diagnosis (i.e., spirometry) could be unreliable
because the test depends on an adequate effort from the tester and testee.
Moreover, the early diagnosis of COPD is challenging. We address COPD detection
by constructing two novel physiological signals datasets (4432 records from 54
patients in the WestRo COPD dataset and 13824 medical records from 534 patients
in the WestRo Porti COPD dataset). The authors demonstrate their complex
coupled fractal dynamical characteristics and perform a fractional-order
dynamics deep learning analysis to diagnose COPD. The authors found that the
fractional-order dynamical modeling can extract distinguishing signatures from
the physiological signals across patients with all COPD stages from stage 0
(healthy) to stage 4 (very severe). They use the fractional signatures to
develop and train a deep neural network that predicts COPD stages based on the
input features (such as thorax breathing effort, respiratory rate, or oxygen
saturation). The authors show that the fractional dynamic deep learning model
(FDDLM) achieves a COPD prediction accuracy of 98.66% and can serve as a robust
alternative to spirometry. The FDDLM also has high accuracy when validated on a
dataset with different physiological signals.
</p></li>
</ul>

<h3>Title: Lifelong Learning for Anomaly Detection: New Challenges, Perspectives, and Insights. (arXiv:2303.07557v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.07557">http://arxiv.org/abs/2303.07557</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.07557] Lifelong Learning for Anomaly Detection: New Challenges, Perspectives, and Insights](http://arxiv.org/abs/2303.07557) #robust</code></li>
<li>Summary: <p>Anomaly detection is of paramount importance in many real-world domains,
characterized by evolving behavior. Lifelong learning represents an emerging
trend, answering the need for machine learning models that continuously adapt
to new challenges in dynamic environments while retaining past knowledge.
However, limited efforts are dedicated to building foundations for lifelong
anomaly detection, which provides intrinsically different challenges compared
to the more widely explored classification setting. In this paper, we face this
issue by exploring, motivating, and discussing lifelong anomaly detection,
trying to build foundations for its wider adoption. First, we explain why
lifelong anomaly detection is relevant, defining challenges and opportunities
to design anomaly detection methods that deal with lifelong learning
complexities. Second, we characterize learning settings and a scenario
generation procedure that enables researchers to experiment with lifelong
anomaly detection using existing datasets. Third, we perform experiments with
popular anomaly detection methods on proposed lifelong scenarios, emphasizing
the gap in performance that could be gained with the adoption of lifelong
learning. Overall, we conclude that the adoption of lifelong anomaly detection
is important to design more robust models that provide a comprehensive view of
the environment, as well as simultaneous adaptation and knowledge retention.
</p></li>
</ul>

<h3>Title: Sample-efficient Adversarial Imitation Learning. (arXiv:2303.07846v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.07846">http://arxiv.org/abs/2303.07846</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.07846] Sample-efficient Adversarial Imitation Learning](http://arxiv.org/abs/2303.07846) #robust</code></li>
<li>Summary: <p>Imitation learning, in which learning is performed by demonstration, has been
studied and advanced for sequential decision-making tasks in which a reward
function is not predefined. However, imitation learning methods still require
numerous expert demonstration samples to successfully imitate an expert's
behavior. To improve sample efficiency, we utilize self-supervised
representation learning, which can generate vast training signals from the
given data. In this study, we propose a self-supervised representation-based
adversarial imitation learning method to learn state and action representations
that are robust to diverse distortions and temporally predictive, on non-image
control tasks. In particular, in comparison with existing self-supervised
learning methods for tabular data, we propose a different corruption method for
state and action representations that is robust to diverse distortions. We
theoretically and empirically observe that making an informative feature
manifold with less sample complexity significantly improves the performance of
imitation learning. The proposed method shows a 39% relative improvement over
existing adversarial imitation learning methods on MuJoCo in a setting limited
to 100 expert state-action pairs. Moreover, we conduct comprehensive ablations
and additional experiments using demonstrations with varying optimality to
provide insights into a range of factors.
</p></li>
</ul>

<h3>Title: Understanding Model Complexity for temporal tabular and multi-variate time series, case study with Numerai data science tournament. (arXiv:2303.07925v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.07925">http://arxiv.org/abs/2303.07925</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.07925] Understanding Model Complexity for temporal tabular and multi-variate time series, case study with Numerai data science tournament](http://arxiv.org/abs/2303.07925) #robust</code></li>
<li>Summary: <p>In this paper, we explore the use of different feature engineering and
dimensionality reduction methods in multi-variate time-series modelling. Using
a feature-target cross correlation time series dataset created from Numerai
tournament, we demonstrate under over-parameterised regime, both the
performance and predictions from different feature engineering methods converge
to the same equilibrium, which can be characterised by the reproducing kernel
Hilbert space. We suggest a new Ensemble method, which combines different
random non-linear transforms followed by ridge regression for modelling high
dimensional time-series. Compared to some commonly used deep learning models
for sequence modelling, such as LSTM and transformers, our method is more
robust (lower model variance over different random seeds and less sensitive to
the choice of architecture) and more efficient. An additional advantage of our
method is model simplicity as there is no need to use sophisticated deep
learning frameworks such as PyTorch. The learned feature rankings are then
applied to the temporal tabular prediction problem in the Numerai tournament,
and the predictive power of feature rankings obtained from our method is better
than the baseline prediction model based on moving averages
</p></li>
</ul>

<h2>biometric</h2>
<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Rethinking Image-based Table Recognition Using Weakly Supervised Methods. (arXiv:2303.07641v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.07641">http://arxiv.org/abs/2303.07641</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.07641] Rethinking Image-based Table Recognition Using Weakly Supervised Methods](http://arxiv.org/abs/2303.07641) #extraction</code></li>
<li>Summary: <p>Most of the previous methods for table recognition rely on training datasets
containing many richly annotated table images. Detailed table image annotation,
e.g., cell or text bounding box annotation, however, is costly and often
subjective. In this paper, we propose a weakly supervised model named WSTabNet
for table recognition that relies only on HTML (or LaTeX) code-level
annotations of table images. The proposed model consists of three main parts:
an encoder for feature extraction, a structure decoder for generating table
structure, and a cell decoder for predicting the content of each cell in the
table. Our system is trained end-to-end by stochastic gradient descent
algorithms, requiring only table images and their ground-truth HTML (or LaTeX)
representations. To facilitate table recognition with deep learning, we create
and release WikiTableSet, the largest publicly available image-based table
recognition dataset built from Wikipedia. WikiTableSet contains nearly 4
million English table images, 590K Japanese table images, and 640k French table
images with corresponding HTML representation and cell bounding boxes. The
extensive experiments on WikiTableSet and two large-scale datasets: FinTabNet
and PubTabNet demonstrate that the proposed weakly supervised model achieves
better, or similar accuracies compared to the state-of-the-art models on all
benchmark datasets.
</p></li>
</ul>

<h3>Title: A Simple Baseline for Supervised Surround-view Depth Estimation. (arXiv:2303.07759v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.07759">http://arxiv.org/abs/2303.07759</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.07759] A Simple Baseline for Supervised Surround-view Depth Estimation](http://arxiv.org/abs/2303.07759) #extraction</code></li>
<li>Summary: <p>Depth estimation has been widely studied and serves as the fundamental step
of 3D perception for autonomous driving. Though significant progress has been
made for monocular depth estimation in the past decades, these attempts are
mainly conducted on the KITTI benchmark with only front-view cameras, which
ignores the correlations across surround-view cameras. In this paper, we
propose S3Depth, a Simple Baseline for Supervised Surround-view Depth
Estimation, to jointly predict the depth maps across multiple surrounding
cameras. Specifically, we employ a global-to-local feature extraction module
which combines CNN with transformer layers for enriched representations.
Further, the Adjacent-view Attention mechanism is proposed to enable the
intra-view and inter-view feature propagation. The former is achieved by the
self-attention module within each view, while the latter is realized by the
adjacent attention module, which computes the attention across multi-cameras to
exchange the multi-scale representations across surround-view feature maps.
Extensive experiments show that our method achieves superior performance over
existing state-of-the-art methods on both DDAD and nuScenes datasets.
</p></li>
</ul>

<h3>Title: Style Feature Extraction Using Contrastive Conditioned Variational Autoencoders with Mutual Information Constraints. (arXiv:2303.08068v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.08068">http://arxiv.org/abs/2303.08068</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.08068] Style Feature Extraction Using Contrastive Conditioned Variational Autoencoders with Mutual Information Constraints](http://arxiv.org/abs/2303.08068) #extraction</code></li>
<li>Summary: <p>It is crucial to extract fine-grained features such as styles from unlabeled
data in data analysis. Unsupervised methods, such as variational autoencoders
(VAEs), can extract styles, but the extracted styles are usually mixed with
other features. We can isolate the styles using VAEs conditioned by class
labels, known as conditional VAEs (CVAEs). However, methods to extract only
styles using unlabeled data are not established. In this paper, we construct a
CVAE-based method that extracts style features using only unlabeled data. The
proposed model roughly consists of two parallel parts; a contrastive learning
(CL) part that extracts style-independent features and a CVAE part that
extracts style features. CL models generally learn representations independent
of data augmentation, which can be seen as a perturbation in styles, in a
self-supervised way. Taking the style-independent features as a condition, the
CVAE learns to extract only styles. In the training procedure, a CL model is
trained beforehand, and then the CVAE is trained while the CL model is fixed.
Additionally, to prevent the CVAE from learning to ignore the condition and
failing to extract only styles, we introduce a constraint based on mutual
information between the CL features and the VAE features. Experiments on two
simple datasets, MNIST and an original dataset based on Google Fonts, show that
the proposed method efficiently extracts style features. Further experiments
using real-world natural image datasets also show the method's extendability.
</p></li>
</ul>

<h3>Title: Finding the Needle in a Haystack: Unsupervised Rationale Extraction from Long Text Classifiers. (arXiv:2303.07991v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.07991">http://arxiv.org/abs/2303.07991</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.07991] Finding the Needle in a Haystack: Unsupervised Rationale Extraction from Long Text Classifiers](http://arxiv.org/abs/2303.07991) #extraction</code></li>
<li>Summary: <p>Long-sequence transformers are designed to improve the representation of
longer texts by language models and their performance on downstream
document-level tasks. However, not much is understood about the quality of
token-level predictions in long-form models. We investigate the performance of
such architectures in the context of document classification with unsupervised
rationale extraction. We find standard soft attention methods to perform
significantly worse when combined with the Longformer language model. We
propose a compositional soft attention architecture that applies RoBERTa
sentence-wise to extract plausible rationales at the token-level. We find this
method to significantly outperform Longformer-driven baselines on sentiment
classification datasets, while also exhibiting significantly lower runtimes.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Network Anomaly Detection Using Federated Learning. (arXiv:2303.07452v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.07452">http://arxiv.org/abs/2303.07452</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.07452] Network Anomaly Detection Using Federated Learning](http://arxiv.org/abs/2303.07452) #federate</code></li>
<li>Summary: <p>Due to the veracity and heterogeneity in network traffic, detecting anomalous
events is challenging. The computational load on global servers is a
significant challenge in terms of efficiency, accuracy, and scalability. Our
primary motivation is to introduce a robust and scalable framework that enables
efficient network anomaly detection. We address the issue of scalability and
efficiency for network anomaly detection by leveraging federated learning, in
which multiple participants train a global model jointly. Unlike centralized
training architectures, federated learning does not require participants to
upload their training data to the server, preventing attackers from exploiting
the training data. Moreover, most prior works have focused on traditional
centralized machine learning, making federated machine learning under-explored
in network anomaly detection. Therefore, we propose a deep neural network
framework that could work on low to mid-end devices detecting network anomalies
while checking if a request from a specific IP address is malicious or not.
Compared to multiple traditional centralized machine learning models, the deep
neural federated model reduces training time overhead. The proposed method
performs better than baseline machine learning techniques on the UNSW-NB15 data
set as measured by experiments conducted with an accuracy of 97.21% and a
faster computation time.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Demographic Parity Inspector: Fairness Audits via the Explanation Space. (arXiv:2303.08040v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.08040">http://arxiv.org/abs/2303.08040</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.08040] Demographic Parity Inspector: Fairness Audits via the Explanation Space](http://arxiv.org/abs/2303.08040) #fair</code></li>
<li>Summary: <p>Even if deployed with the best intentions, machine learning methods can
perpetuate, amplify or even create social biases. Measures of (un-)fairness
have been proposed as a way to gauge the (non-)discriminatory nature of machine
learning models. However, proxies of protected attributes causing
discriminatory effects remain challenging to address. In this work, we propose
a new algorithmic approach that measures group-wise demographic parity
violations and allows us to inspect the causes of inter-group discrimination.
Our method relies on the novel idea of measuring the dependence of a model on
the protected attribute based on the explanation space, an informative space
that allows for more sensitive audits than the primary space of input data or
prediction distributions, and allowing for the assertion of theoretical
demographic parity auditing guarantees. We provide a mathematical analysis,
synthetic examples, and experimental evaluation of real-world data. We release
an open-source Python package with methods, routines, and tutorials.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Sr-init: An interpretable layer pruning method. (arXiv:2303.07677v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.07677">http://arxiv.org/abs/2303.07677</a></li>
<li>Code URL: <a href="https://github.com/huitang-zjut/sr-init">https://github.com/huitang-zjut/sr-init</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.07677] Sr-init: An interpretable layer pruning method](http://arxiv.org/abs/2303.07677) #interpretability</code></li>
<li>Summary: <p>Despite the popularization of deep neural networks (DNNs) in many fields, it
is still challenging to deploy state-of-the-art models to resource-constrained
devices due to high computational overhead. Model pruning provides a feasible
solution to the aforementioned challenges. However, the interpretation of
existing pruning criteria is always overlooked. To counter this issue, we
propose a novel layer pruning method by exploring the Stochastic
Re-initialization. Our SR-init method is inspired by the discovery that the
accuracy drop due to stochastic re-initialization of layer parameters differs
in various layers. On the basis of this observation, we come up with a layer
pruning criterion, i.e., those layers that are not sensitive to stochastic
re-initialization (low accuracy drop) produce less contribution to the model
and could be pruned with acceptable loss. Afterward, we experimentally verify
the interpretability of SR-init via feature visualization. The visual
explanation demonstrates that SR-init is theoretically feasible, thus we
compare it with state-of-the-art methods to further evaluate its
practicability. As for ResNet56 on CIFAR-10 and CIFAR-100, SR-init achieves a
great reduction in parameters (63.98% and 37.71%) with an ignorable drop in
top-1 accuracy (-0.56% and 0.8%). With ResNet50 on ImageNet, we achieve a
15.59% FLOPs reduction by removing 39.29% of the parameters, with only a drop
of 0.6% in top-1 accuracy. Our code is available at
https://github.com/huitang-zjut/SRinit.
</p></li>
</ul>

<h3>Title: ICICLE: Interpretable Class Incremental Continual Learning. (arXiv:2303.07811v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.07811">http://arxiv.org/abs/2303.07811</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.07811] ICICLE: Interpretable Class Incremental Continual Learning](http://arxiv.org/abs/2303.07811) #interpretability</code></li>
<li>Summary: <p>Continual learning enables incremental learning of new tasks without
forgetting those previously learned, resulting in positive knowledge transfer
that can enhance performance on both new and old tasks. However, continual
learning poses new challenges for interpretability, as the rationale behind
model predictions may change over time, leading to interpretability concept
drift. We address this problem by proposing Interpretable Class-InCremental
LEarning (ICICLE), an exemplar-free approach that adopts a prototypical
part-based approach. It consists of three crucial novelties: interpretability
regularization that distills previously learned concepts while preserving
user-friendly positive reasoning; proximity-based prototype initialization
strategy dedicated to the fine-grained setting; and task-recency bias
compensation devoted to prototypical parts. Our experimental results
demonstrate that ICICLE reduces the interpretability concept drift and
outperforms the existing exemplar-free methods of common class-incremental
learning when applied to concept-based models. We make the code available.
</p></li>
</ul>

<h3>Title: ViperGPT: Visual Inference via Python Execution for Reasoning. (arXiv:2303.08128v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.08128">http://arxiv.org/abs/2303.08128</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.08128] ViperGPT: Visual Inference via Python Execution for Reasoning](http://arxiv.org/abs/2303.08128) #interpretability</code></li>
<li>Summary: <p>Answering visual queries is a complex task that requires both visual
processing and reasoning. End-to-end models, the dominant approach for this
task, do not explicitly differentiate between the two, limiting
interpretability and generalization. Learning modular programs presents a
promising alternative, but has proven challenging due to the difficulty of
learning both the programs and modules simultaneously. We introduce ViperGPT, a
framework that leverages code-generation models to compose vision-and-language
models into subroutines to produce a result for any query. ViperGPT utilizes a
provided API to access the available modules, and composes them by generating
Python code that is later executed. This simple approach requires no further
training, and achieves state-of-the-art results across various complex visual
tasks.
</p></li>
</ul>

<h2>explainability</h2>
<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Text-to-image Diffusion Model in Generative AI: A Survey. (arXiv:2303.07909v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.07909">http://arxiv.org/abs/2303.07909</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.07909] Text-to-image Diffusion Model in Generative AI: A Survey](http://arxiv.org/abs/2303.07909) #diffusion</code></li>
<li>Summary: <p>This survey reviews text-to-image diffusion models in the context that
diffusion models have emerged to be popular for a wide range of generative
tasks. As a self-contained work, this survey starts with a brief introduction
of how a basic diffusion model works for image synthesis, followed by how
condition or guidance improves learning. Based on that, we present a review of
state-of-the-art methods on text-conditioned image synthesis, i.e.,
text-to-image. We further summarize applications beyond text-to-image
generation: text-guided creative generation and text-guided image editing.
Beyond the progress made so far, we discuss existing challenges and promising
future directions.
</p></li>
</ul>

<h3>Title: Controllable Mesh Generation Through Sparse Latent Point Diffusion Models. (arXiv:2303.07938v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.07938">http://arxiv.org/abs/2303.07938</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.07938] Controllable Mesh Generation Through Sparse Latent Point Diffusion Models](http://arxiv.org/abs/2303.07938) #diffusion</code></li>
<li>Summary: <p>Mesh generation is of great value in various applications involving computer
graphics and virtual content, yet designing generative models for meshes is
challenging due to their irregular data structure and inconsistent topology of
meshes in the same category. In this work, we design a novel sparse latent
point diffusion model for mesh generation. Our key insight is to regard point
clouds as an intermediate representation of meshes, and model the distribution
of point clouds instead. While meshes can be generated from point clouds via
techniques like Shape as Points (SAP), the challenges of directly generating
meshes can be effectively avoided. To boost the efficiency and controllability
of our mesh generation method, we propose to further encode point clouds to a
set of sparse latent points with point-wise semantic meaningful features, where
two DDPMs are trained in the space of sparse latent points to respectively
model the distribution of the latent point positions and features at these
latent points. We find that sampling in this latent space is faster than
directly sampling dense point clouds. Moreover, the sparse latent points also
enable us to explicitly control both the overall structures and local details
of the generated meshes. Extensive experiments are conducted on the ShapeNet
dataset, where our proposed sparse latent point diffusion model achieves
superior performance in terms of generation quality and controllability when
compared to existing methods.
</p></li>
</ul>

<h3>Title: Edit-A-Video: Single Video Editing with Object-Aware Consistency. (arXiv:2303.07945v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.07945">http://arxiv.org/abs/2303.07945</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.07945] Edit-A-Video: Single Video Editing with Object-Aware Consistency](http://arxiv.org/abs/2303.07945) #diffusion</code></li>
<li>Summary: <p>Despite the fact that text-to-video (TTV) model has recently achieved
remarkable success, there have been few approaches on TTV for its extension to
video editing. Motivated by approaches on TTV models adapting from
diffusion-based text-to-image (TTI) models, we suggest the video editing
framework given only a pretrained TTI model and a single <text, video> pair,
which we term Edit-A-Video. The framework consists of two stages: (1) inflating
the 2D model into the 3D model by appending temporal modules and tuning on the
source video (2) inverting the source video into the noise and editing with
target text prompt and attention map injection. Each stage enables the temporal
modeling and preservation of semantic attributes of the source video. One of
the key challenges for video editing include a background inconsistency
problem, where the regions not included for the edit suffer from undesirable
and inconsistent temporal alterations. To mitigate this issue, we also
introduce a novel mask blending method, termed as sparse-causal blending (SC
Blending). We improve previous mask blending methods to reflect the temporal
consistency so that the area where the editing is applied exhibits smooth
transition while also achieving spatio-temporal consistency of the unedited
regions. We present extensive experimental results over various types of text
and videos, and demonstrate the superiority of the proposed method compared to
baselines in terms of background consistency, text alignment, and video editing
quality.
</p></li>
</ul>

<h3>Title: Interpretable ODE-style Generative Diffusion Model via Force Field Construction. (arXiv:2303.08063v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.08063">http://arxiv.org/abs/2303.08063</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.08063] Interpretable ODE-style Generative Diffusion Model via Force Field Construction](http://arxiv.org/abs/2303.08063) #diffusion</code></li>
<li>Summary: <p>For a considerable time, researchers have focused on developing a method that
establishes a deep connection between the generative diffusion model and
mathematical physics. Despite previous efforts, progress has been limited to
the pursuit of a single specialized method. In order to advance the
interpretability of diffusion models and explore new research directions, it is
essential to establish a unified ODE-style generative diffusion model. Such a
model should draw inspiration from physical models and possess a clear
geometric meaning. This paper aims to identify various physical models that are
suitable for constructing ODE-style generative diffusion models accurately from
a mathematical perspective. We then summarize these models into a unified
method. Additionally, we perform a case study where we use the theoretical
model identified by our method to develop a range of new diffusion model
methods, and conduct experiments. Our experiments on CIFAR-10 demonstrate the
effectiveness of our approach. We have constructed a computational framework
that attains highly proficient results with regards to image generation speed,
alongside an additional model that demonstrates exceptional performance in both
Inception score and FID score. These results underscore the significance of our
method in advancing the field of diffusion models.
</p></li>
</ul>

<h3>Title: Editing Implicit Assumptions in Text-to-Image Diffusion Models. (arXiv:2303.08084v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.08084">http://arxiv.org/abs/2303.08084</a></li>
<li>Code URL: <a href="https://github.com/bahjat-kawar/time-diffusion">https://github.com/bahjat-kawar/time-diffusion</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.08084] Editing Implicit Assumptions in Text-to-Image Diffusion Models](http://arxiv.org/abs/2303.08084) #diffusion</code></li>
<li>Summary: <p>Text-to-image diffusion models often make implicit assumptions about the
world when generating images. While some assumptions are useful (e.g., the sky
is blue), they can also be outdated, incorrect, or reflective of social biases
present in the training data. Thus, there is a need to control these
assumptions without requiring explicit user input or costly re-training. In
this work, we aim to edit a given implicit assumption in a pre-trained
diffusion model. Our Text-to-Image Model Editing method, TIME for short,
receives a pair of inputs: a "source" under-specified prompt for which the
model makes an implicit assumption (e.g., "a pack of roses"), and a
"destination" prompt that describes the same setting, but with a specified
desired attribute (e.g., "a pack of blue roses"). TIME then updates the model's
cross-attention layers, as these layers assign visual meaning to textual
tokens. We edit the projection matrices in these layers such that the source
prompt is projected close to the destination prompt. Our method is highly
efficient, as it modifies a mere 2.2% of the model's parameters in under one
second. To evaluate model editing approaches, we introduce TIMED (TIME
Dataset), containing 147 source and destination prompt pairs from various
domains. Our experiments (using Stable Diffusion) show that TIME is successful
in model editing, generalizes well for related prompts unseen during editing,
and imposes minimal effect on unrelated generations.
</p></li>
</ul>

<h3>Title: LayoutDM: Discrete Diffusion Model for Controllable Layout Generation. (arXiv:2303.08137v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.08137">http://arxiv.org/abs/2303.08137</a></li>
<li>Code URL: <a href="https://github.com/CyberAgentAILab/layout-dm">https://github.com/CyberAgentAILab/layout-dm</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2303.08137] LayoutDM: Discrete Diffusion Model for Controllable Layout Generation](http://arxiv.org/abs/2303.08137) #diffusion</code></li>
<li>Summary: <p>Controllable layout generation aims at synthesizing plausible arrangement of
element bounding boxes with optional constraints, such as type or position of a
specific element. In this work, we try to solve a broad range of layout
generation tasks in a single model that is based on discrete state-space
diffusion models. Our model, named LayoutDM, naturally handles the structured
layout data in the discrete representation and learns to progressively infer a
noiseless layout from the initial input, where we model the layout corruption
process by modality-wise discrete diffusion. For conditional generation, we
propose to inject layout constraints in the form of masking or logit adjustment
during inference. We show in the experiments that our LayoutDM successfully
generates high-quality layouts and outperforms both task-specific and
task-agnostic baselines on several layout tasks.
</p></li>
</ul>

<h3>Title: Diffusion Models in NLP: A Survey. (arXiv:2303.07576v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2303.07576">http://arxiv.org/abs/2303.07576</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2303.07576] Diffusion Models in NLP: A Survey](http://arxiv.org/abs/2303.07576) #diffusion</code></li>
<li>Summary: <p>Diffusion models have become a powerful family of deep generative models,
with record-breaking performance in many applications. This paper first gives
an overview and derivation of the basic theory of diffusion models, then
reviews the research results of diffusion models in the field of natural
language processing, from text generation, text-driven image generation and
other four aspects, and analyzes and summarizes the relevant literature
materials sorted out, and finally records the experience and feelings of this
topic literature review research.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
