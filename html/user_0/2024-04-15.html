<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-04-15</h1>
<h3>Title: A Multi-Level Framework for Accelerating Training Transformer Models</h3>
<ul>
<li><strong>Authors: </strong>Longwei Zou, Han Zhang, Yangdong Deng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.07999">https://arxiv.org/abs/2404.07999</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.07999">https://arxiv.org/pdf/2404.07999</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.07999]] A Multi-Level Framework for Accelerating Training Transformer Models(https://arxiv.org/abs/2404.07999)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The fast growing capabilities of large-scale deep learning models, such as Bert, GPT and ViT, are revolutionizing the landscape of NLP, CV and many other domains. Training such models, however, poses an unprecedented demand for computing power, which incurs exponentially increasing energy cost and carbon dioxide emissions. It is thus critical to develop efficient training solutions to reduce the training costs. Motivated by a set of key observations of inter- and intra-layer similarities among feature maps and attentions that can be identified from typical training processes, we propose a multi-level framework for training acceleration. Specifically, the framework is based on three basic operators, Coalescing, De-coalescing and Interpolation, which can be orchestrated to build a multi-level training framework. The framework consists of a V-cycle training process, which progressively down- and up-scales the model size and projects the parameters between adjacent levels of models via coalescing and de-coalescing. The key idea is that a smaller model that can be trained for fast convergence and the trained parameters provides high-qualities intermediate solutions for the next level larger network. The interpolation operator is designed to break the symmetry of neurons incurred by de-coalescing for better convergence performance. Our experiments on transformer-based language models (e.g. Bert, GPT) as well as a vision model (e.g. DeiT) prove that the proposed framework reduces the computational cost by about 20% on training BERT/GPT-Base models and up to 51.6% on training the BERT-Large model while preserving the performance.</li>
</ul>

<h3>Title: Asynchronous Federated Reinforcement Learning with Policy Gradient  Updates: Algorithm Design and Convergence Analysis</h3>
<ul>
<li><strong>Authors: </strong>Guangchen Lan, Dong-Jun Han, Abolfazl Hashemi, Vaneet Aggarwal, Christopher G. Brinton</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08003">https://arxiv.org/abs/2404.08003</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08003">https://arxiv.org/pdf/2404.08003</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08003]] Asynchronous Federated Reinforcement Learning with Policy Gradient  Updates: Algorithm Design and Convergence Analysis(https://arxiv.org/abs/2404.08003)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>To improve the efficiency of reinforcement learning, we propose a novel asynchronous federated reinforcement learning framework termed AFedPG, which constructs a global model through collaboration among $N$ agents using policy gradient (PG) updates. To handle the challenge of lagged policies in asynchronous settings, we design delay-adaptive lookahead and normalized update techniques that can effectively handle the heterogeneous arrival times of policy gradients. We analyze the theoretical global convergence bound of AFedPG, and characterize the advantage of the proposed algorithm in terms of both the sample complexity and time complexity. Specifically, our AFedPG method achieves $\mathcal{O}(\frac{{\epsilon}^{-2.5}}{N})$ sample complexity at each agent on average. Compared to the single agent setting with $\mathcal{O}(\epsilon^{-2.5})$ sample complexity, it enjoys a linear speedup with respect to the number of agents. Moreover, compared to synchronous FedPG, AFedPG improves the time complexity from $\mathcal{O}(\frac{t_{\max}}{N})$ to $\mathcal{O}(\frac{1}{\sum_{i=1}^{N} \frac{1}{t_{i}}})$, where $t_{i}$ denotes the time consumption in each iteration at the agent $i$, and $t_{\max}$ is the largest one. The latter complexity $\mathcal{O}(\frac{1}{\sum_{i=1}^{N} \frac{1}{t_{i}}})$ is always smaller than the former one, and this improvement becomes significant in large-scale federated settings with heterogeneous computing powers ($t_{\max}\gg t_{\min}$). Finally, we empirically verify the improved performances of AFedPG in three MuJoCo environments with varying numbers of agents. We also demonstrate the improvements with different computing heterogeneity.</li>
</ul>

<h3>Title: GRANP: A Graph Recurrent Attentive Neural Process Model for Vehicle  Trajectory Prediction</h3>
<ul>
<li><strong>Authors: </strong>Yuhao Luo, Kehua Chen, Meixin Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08004">https://arxiv.org/abs/2404.08004</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08004">https://arxiv.org/pdf/2404.08004</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08004]] GRANP: A Graph Recurrent Attentive Neural Process Model for Vehicle  Trajectory Prediction(https://arxiv.org/abs/2404.08004)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, generative</a></li>
<li><strong>Abstract: </strong>As a vital component in autonomous driving, accurate trajectory prediction effectively prevents traffic accidents and improves driving efficiency. To capture complex spatial-temporal dynamics and social interactions, recent studies developed models based on advanced deep-learning methods. On the other hand, recent studies have explored the use of deep generative models to further account for trajectory uncertainties. However, the current approaches demonstrating indeterminacy involve inefficient and time-consuming practices such as sampling from trained models. To fill this gap, we proposed a novel model named Graph Recurrent Attentive Neural Process (GRANP) for vehicle trajectory prediction while efficiently quantifying prediction uncertainty. In particular, GRANP contains an encoder with deterministic and latent paths, and a decoder for prediction. The encoder, including stacked Graph Attention Networks, LSTM and 1D convolutional layers, is employed to extract spatial-temporal relationships. The decoder is used to learn a latent distribution and thus quantify prediction uncertainty. To reveal the effectiveness of our model, we evaluate the performance of GRANP on the highD dataset. Extensive experiments show that GRANP achieves state-of-the-art results and can efficiently quantify uncertainties. Additionally, we undertake an intuitive case study that showcases the interpretability of the proposed approach. The code is available at https://github.com/joy-driven/GRANP.</li>
</ul>

<h3>Title: Sample-Efficient Human Evaluation of Large Language Models via Maximum  Discrepancy Competition</h3>
<ul>
<li><strong>Authors: </strong>Kehua Feng, Keyan Ding, Kede Ma, Zhihua Wang, Qiang Zhang, Huajun Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08008">https://arxiv.org/abs/2404.08008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08008">https://arxiv.org/pdf/2404.08008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08008]] Sample-Efficient Human Evaluation of Large Language Models via Maximum  Discrepancy Competition(https://arxiv.org/abs/2404.08008)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The past years have witnessed a proliferation of large language models (LLMs). Yet, automated and unbiased evaluation of LLMs is challenging due to the inaccuracy of standard metrics in reflecting human preferences and the inefficiency in sampling informative and diverse test examples. While human evaluation remains the gold standard, it is expensive and time-consuming, especially when dealing with a large number of testing samples. To address this problem, we propose a sample-efficient human evaluation method based on MAximum Discrepancy (MAD) competition. MAD automatically selects a small set of informative and diverse instructions, each adapted to two LLMs, whose responses are subject to three-alternative forced choice by human subjects. The pairwise comparison results are then aggregated into a global ranking using the Elo rating system. We select eight representative LLMs and compare them in terms of four skills: knowledge understanding, mathematical reasoning, writing, and coding. Experimental results show that the proposed method achieves a reliable and sensible ranking of LLMs' capabilities, identifies their relative strengths and weaknesses, and offers valuable insights for further LLM advancement.</li>
</ul>

<h3>Title: An inclusive review on deep learning techniques and their scope in  handwriting recognition</h3>
<ul>
<li><strong>Authors: </strong>Sukhdeep Singh, Sudhir Rohilla, Anuj Sharma</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08011">https://arxiv.org/abs/2404.08011</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08011">https://arxiv.org/pdf/2404.08011</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08011]] An inclusive review on deep learning techniques and their scope in  handwriting recognition(https://arxiv.org/abs/2404.08011)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Deep learning expresses a category of machine learning algorithms that have the capability to combine raw inputs into intermediate features layers. These deep learning algorithms have demonstrated great results in different fields. Deep learning has particularly witnessed for a great achievement of human level performance across a number of domains in computer vision and pattern recognition. For the achievement of state-of-the-art performances in diverse domains, the deep learning used different architectures and these architectures used activation functions to perform various computations between hidden and output layers of any architecture. This paper presents a survey on the existing studies of deep learning in handwriting recognition field. Even though the recent progress indicates that the deep learning methods has provided valuable means for speeding up or proving accurate results in handwriting recognition, but following from the extensive literature survey, the present study finds that the deep learning has yet to revolutionize more and has to resolve many of the most pressing challenges in this field, but promising advances have been made on the prior state of the art. Additionally, an inadequate availability of labelled data to train presents problems in this domain. Nevertheless, the present handwriting recognition survey foresees deep learning enabling changes at both bench and bedside with the potential to transform several domains as image processing, speech recognition, computer vision, machine translation, robotics and control, medical imaging, medical information processing, bio-informatics, natural language processing, cyber security, and many others.</li>
</ul>

<h3>Title: AI-Guided Feature Segmentation Techniques to Model Features from Single  Crystal Diamond Growth</h3>
<ul>
<li><strong>Authors: </strong>Rohan Reddy Mekala, Elias Garratt, Matthias Muehle, Arjun Srinivasan, Adam Porter, Mikael Lindvall</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08017">https://arxiv.org/abs/2404.08017</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08017">https://arxiv.org/pdf/2404.08017</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08017]] AI-Guided Feature Segmentation Techniques to Model Features from Single  Crystal Diamond Growth(https://arxiv.org/abs/2404.08017)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Process refinement to consistently produce high-quality material over a large area of the grown crystal, enabling various applications from optics crystals to quantum detectors, has long been a goal for diamond growth. Machine learning offers a promising path toward this goal, but faces challenges such as the complexity of features within datasets, their time-dependency, and the volume of data produced per growth run. Accurate spatial feature extraction from image to image for real-time monitoring of diamond growth is crucial yet complicated due to the low-volume and high feature complexity nature of the datasets. This paper compares various traditional and machine learning-driven approaches for feature extraction in the diamond growth domain, proposing a novel deep learning-driven semantic segmentation approach to isolate and classify accurate pixel masks of geometric features like diamond, pocket holder, and background, along with their derivative features based on shape and size. Using an annotation-focused human-in-the-loop software architecture for training datasets, with modules for selective data labeling using active learning, data augmentations, and model-assisted labeling, our approach achieves effective annotation accuracy and drastically reduces labeling time and cost. Deep learning algorithms prove highly efficient in accurately learning complex representations from datasets with many features. Our top-performing model, based on the DeeplabV3plus architecture, achieves outstanding accuracy in classifying features of interest, with accuracies of 96.31% for pocket holder, 98.60% for diamond top, and 91.64% for diamond side features.</li>
</ul>

<h3>Title: The OxMat dataset: a multimodal resource for the development of  AI-driven technologies in maternal and newborn child health</h3>
<ul>
<li><strong>Authors: </strong>M. Jaleed Khan, Ioana Duta, Beth Albert, William Cooke, Manu Vatish, Gabriel Davis Jones</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08024">https://arxiv.org/abs/2404.08024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08024">https://arxiv.org/pdf/2404.08024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08024]] The OxMat dataset: a multimodal resource for the development of  AI-driven technologies in maternal and newborn child health(https://arxiv.org/abs/2404.08024)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The rapid advancement of Artificial Intelligence (AI) in healthcare presents a unique opportunity for advancements in obstetric care, particularly through the analysis of cardiotocography (CTG) for fetal monitoring. However, the effectiveness of such technologies depends upon the availability of large, high-quality datasets that are suitable for machine learning. This paper introduces the Oxford Maternity (OxMat) dataset, the world's largest curated dataset of CTGs, featuring raw time series CTG data and extensive clinical data for both mothers and babies, which is ideally placed for machine learning. The OxMat dataset addresses the critical gap in women's health data by providing over 177,211 unique CTG recordings from 51,036 pregnancies, carefully curated and reviewed since 1991. The dataset also comprises over 200 antepartum, intrapartum and postpartum clinical variables, ensuring near-complete data for crucial outcomes such as stillbirth and acidaemia. While this dataset also covers the intrapartum stage, around 94% of the constituent CTGS are antepartum. This allows for a unique focus on the underserved antepartum period, in which early detection of at-risk fetuses can significantly improve health outcomes. Our comprehensive review of existing datasets reveals the limitations of current datasets: primarily, their lack of sufficient volume, detailed clinical data and antepartum data. The OxMat dataset lays a foundation for future AI-driven prenatal care, offering a robust resource for developing and testing algorithms aimed at improving maternal and fetal health outcomes.</li>
</ul>

<h3>Title: FedAuxHMTL: Federated Auxiliary Hard-Parameter Sharing Multi-Task  Learning for Network Edge Traffic Classification</h3>
<ul>
<li><strong>Authors: </strong>Faisal Ahmed, Myungjin Lee, Suresh Subramaniam, Motoharu Matsuura, Hiroshi Hasegawa, Shih-Chun Lin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08028">https://arxiv.org/abs/2404.08028</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08028">https://arxiv.org/pdf/2404.08028</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08028]] FedAuxHMTL: Federated Auxiliary Hard-Parameter Sharing Multi-Task  Learning for Network Edge Traffic Classification(https://arxiv.org/abs/2404.08028)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) has garnered significant interest recently due to its potential as an effective solution for tackling many challenges in diverse application scenarios, for example, data privacy in network edge traffic classification. Despite its recognized advantages, FL encounters obstacles linked to statistical data heterogeneity and labeled data scarcity during the training of single-task models for machine learning-based traffic classification, leading to hindered learning performance. In response to these challenges, adopting a hard-parameter sharing multi-task learning model with auxiliary tasks proves to be a suitable approach. Such a model has the capability to reduce communication and computation costs, navigate statistical complexities inherent in FL contexts, and overcome labeled data scarcity by leveraging knowledge derived from interconnected auxiliary tasks. This paper introduces a new framework for federated auxiliary hard-parameter sharing multi-task learning, namely, FedAuxHMTL. The introduced framework incorporates model parameter exchanges between edge server and base stations, enabling base stations from distributed areas to participate in the FedAuxHMTL process and enhance the learning performance of the main task-network edge traffic classification. Empirical experiments are conducted to validate and demonstrate the FedAuxHMTL's effectiveness in terms of accuracy, total global loss, communication costs, computing time, and energy consumption compared to its counterparts.</li>
</ul>

<h3>Title: A Multi-Expert Large Language Model Architecture for Verilog Code  Generation</h3>
<ul>
<li><strong>Authors: </strong>Bardia Nadimi, Hao Zheng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.PL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08029">https://arxiv.org/abs/2404.08029</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08029">https://arxiv.org/pdf/2404.08029</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08029]] A Multi-Expert Large Language Model Architecture for Verilog Code  Generation(https://arxiv.org/abs/2404.08029)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recently, there has been a surging interest in using large language models (LLMs) for Verilog code generation. However, the existing approaches are limited in terms of the quality of the generated Verilog code. To address such limitations, this paper introduces an innovative multi-expert LLM architecture for Verilog code generation (MEV-LLM). Our architecture uniquely integrates multiple LLMs, each specifically fine-tuned with a dataset that is categorized with respect to a distinct level of design complexity. It allows more targeted learning, directly addressing the nuances of generating Verilog code for each category. Empirical evidence from experiments highlights notable improvements in terms of the percentage of generated Verilog outputs that are syntactically and functionally correct. These findings underscore the efficacy of our approach, promising a forward leap in the field of automated hardware design through machine learning.</li>
</ul>

<h3>Title: Rethinking Artistic Copyright Infringements in the Era of Text-to-Image  Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Mazda Moayeri, Samyadeep Basu, Sriram Balasubramanian, Priyatham Kattakinda, Atoosa Chengini, Robert Brauneis, Soheil Feizi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08030">https://arxiv.org/abs/2404.08030</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08030">https://arxiv.org/pdf/2404.08030</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08030]] Rethinking Artistic Copyright Infringements in the Era of Text-to-Image  Generative Models(https://arxiv.org/abs/2404.08030)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent text-to-image generative models such as Stable Diffusion are extremely adept at mimicking and generating copyrighted content, raising concerns amongst artists that their unique styles may be improperly copied. Understanding how generative models copy "artistic style" is more complex than duplicating a single image, as style is comprised by a set of elements (or signature) that frequently co-occurs across a body of work, where each individual work may vary significantly. In our paper, we first reformulate the problem of "artistic copyright infringement" to a classification problem over image sets, instead of probing image-wise similarities. We then introduce ArtSavant, a practical (i.e., efficient and easy to understand) tool to (i) determine the unique style of an artist by comparing it to a reference dataset of works from 372 artists curated from WikiArt, and (ii) recognize if the identified style reappears in generated images. We leverage two complementary methods to perform artistic style classification over image sets, includingTagMatch, which is a novel inherently interpretable and attributable method, making it more suitable for broader use by non-technical stake holders (artists, lawyers, judges, etc). Leveraging ArtSavant, we then perform a large-scale empirical study to provide quantitative insight on the prevalence of artistic style copying across 3 popular text-to-image generative models. Namely, amongst a dataset of prolific artists (including many famous ones), only 20% of them appear to have their styles be at a risk of copying via simple prompting of today's popular text-to-image generative models.</li>
</ul>

<h3>Title: Latent Guard: a Safety Framework for Text-to-image Generation</h3>
<ul>
<li><strong>Authors: </strong>Runtao Liu, Ashkan Khakzar, Jindong Gu, Qifeng Chen, Philip Torr, Fabio Pizzati</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08031">https://arxiv.org/abs/2404.08031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08031">https://arxiv.org/pdf/2404.08031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08031]] Latent Guard: a Safety Framework for Text-to-image Generation(https://arxiv.org/abs/2404.08031)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the ability to generate high-quality images, text-to-image (T2I) models can be exploited for creating inappropriate content. To prevent misuse, existing safety measures are either based on text blacklists, which can be easily circumvented, or harmful content classification, requiring large datasets for training and offering low flexibility. Hence, we propose Latent Guard, a framework designed to improve safety measures in text-to-image generation. Inspired by blacklist-based approaches, Latent Guard learns a latent space on top of the T2I model's text encoder, where it is possible to check the presence of harmful concepts in the input text embeddings. Our proposed framework is composed of a data generation pipeline specific to the task using large language models, ad-hoc architectural components, and a contrastive learning strategy to benefit from the generated data. The effectiveness of our method is verified on three datasets and against four baselines. Code and data will be shared at https://github.com/rt219/LatentGuard.</li>
</ul>

<h3>Title: MSciNLI: A Diverse Benchmark for Scientific Natural Language Inference</h3>
<ul>
<li><strong>Authors: </strong>Mobashir Sadat, Cornelia Caragea</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08066">https://arxiv.org/abs/2404.08066</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08066">https://arxiv.org/pdf/2404.08066</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08066]] MSciNLI: A Diverse Benchmark for Scientific Natural Language Inference(https://arxiv.org/abs/2404.08066)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The task of scientific Natural Language Inference (NLI) involves predicting the semantic relation between two sentences extracted from research articles. This task was recently proposed along with a new dataset called SciNLI derived from papers published in the computational linguistics domain. In this paper, we aim to introduce diversity in the scientific NLI task and present MSciNLI, a dataset containing 132,320 sentence pairs extracted from five new scientific domains. The availability of multiple domains makes it possible to study domain shift for scientific NLI. We establish strong baselines on MSciNLI by fine-tuning Pre-trained Language Models (PLMs) and prompting Large Language Models (LLMs). The highest Macro F1 scores of PLM and LLM baselines are 77.21% and 51.77%, respectively, illustrating that MSciNLI is challenging for both types of models. Furthermore, we show that domain shift degrades the performance of scientific NLI models which demonstrates the diverse characteristics of different domains in our dataset. Finally, we use both scientific NLI datasets in an intermediate task transfer learning setting and show that they can improve the performance of downstream tasks in the scientific domain. We make our dataset and code available on Github.</li>
</ul>

<h3>Title: WildGraph: Realistic Graph-based Trajectory Generation for Wildlife</h3>
<ul>
<li><strong>Authors: </strong>Ali Al-Lawati, Elsayed Eshra, Prasenjit Mitra</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08068">https://arxiv.org/abs/2404.08068</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08068">https://arxiv.org/pdf/2404.08068</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08068]] WildGraph: Realistic Graph-based Trajectory Generation for Wildlife(https://arxiv.org/abs/2404.08068)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Trajectory generation is an important task in movement studies; it circumvents the privacy, ethical, and technical challenges of collecting real trajectories from the target population. In particular, real trajectories in the wildlife domain are scarce as a result of ethical and environmental constraints of the collection process. In this paper, we consider the problem of generating long-horizon trajectories, akin to wildlife migration, based on a small set of real samples. We propose a hierarchical approach to learn the global movement characteristics of the real dataset and recursively refine localized regions. Our solution, WildGraph, discretizes the geographic path into a prototype network of H3 (https://www.uber.com/blog/h3/) regions and leverages a recurrent variational auto-encoder to probabilistically generate paths over the regions, based on occupancy. WildGraph successfully generates realistic months-long trajectories using a sample size as small as 60. Experiments performed on two wildlife migration datasets demonstrate that our proposed method improves the generalization of the generated trajectories in comparison to existing work while achieving superior or comparable performance in several benchmark metrics. Our code is published on the following repository: \url{https://github.com/aliwister/wildgraph}.</li>
</ul>

<h3>Title: Persistent Classification: A New Approach to Stability of Data and  Adversarial Examples</h3>
<ul>
<li><strong>Authors: </strong>Brian Bell, Michael Geyer, David Glickenstein, Keaton Hamm, Carlos Scheidegger, Amanda Fernandez, Juston Moore</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08069">https://arxiv.org/abs/2404.08069</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08069">https://arxiv.org/pdf/2404.08069</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08069]] Persistent Classification: A New Approach to Stability of Data and  Adversarial Examples(https://arxiv.org/abs/2404.08069)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>There are a number of hypotheses underlying the existence of adversarial examples for classification problems. These include the high-dimensionality of the data, high codimension in the ambient space of the data manifolds of interest, and that the structure of machine learning models may encourage classifiers to develop decision boundaries close to data points. This article proposes a new framework for studying adversarial examples that does not depend directly on the distance to the decision boundary. Similarly to the smoothed classifier literature, we define a (natural or adversarial) data point to be $(\gamma,\sigma)$-stable if the probability of the same classification is at least $\gamma$ for points sampled in a Gaussian neighborhood of the point with a given standard deviation $\sigma$. We focus on studying the differences between persistence metrics along interpolants of natural and adversarial points. We show that adversarial examples have significantly lower persistence than natural examples for large neural networks in the context of the MNIST and ImageNet datasets. We connect this lack of persistence with decision boundary geometry by measuring angles of interpolants with respect to decision boundaries. Finally, we connect this approach with robustness by developing a manifold alignment gradient metric and demonstrating the increase in robustness that can be achieved when training with the addition of this metric.</li>
</ul>

<h3>Title: SQBC: Active Learning using LLM-Generated Synthetic Data for Stance  Detection in Online Political Discussions</h3>
<ul>
<li><strong>Authors: </strong>Stefan Sylvius Wagner, Maike Behrendt, Marc Ziegele, Stefan Harmeling</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08078">https://arxiv.org/abs/2404.08078</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08078">https://arxiv.org/pdf/2404.08078</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08078]] SQBC: Active Learning using LLM-Generated Synthetic Data for Stance  Detection in Online Political Discussions(https://arxiv.org/abs/2404.08078)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Stance detection is an important task for many applications that analyse or support online political discussions. Common approaches include fine-tuning transformer based models. However, these models require a large amount of labelled data, which might not be available. In this work, we present two different ways to leverage LLM-generated synthetic data to train and improve stance detection agents for online political discussions: first, we show that augmenting a small fine-tuning dataset with synthetic data can improve the performance of the stance detection model. Second, we propose a new active learning method called SQBC based on the "Query-by-Comittee" approach. The key idea is to use LLM-generated synthetic data as an oracle to identify the most informative unlabelled samples, that are selected for manual labelling. Comprehensive experiments show that both ideas can improve the stance detection performance. Curiously, we observed that fine-tuning on actively selected samples can exceed the performance of using the full dataset.</li>
</ul>

<h3>Title: Real-Time Detection and Analysis of Vehicles and Pedestrians using Deep  Learning</h3>
<ul>
<li><strong>Authors: </strong>Md Nahid Sadik, Tahmim Hossain, Faisal Sayeed</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08081">https://arxiv.org/abs/2404.08081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08081">https://arxiv.org/pdf/2404.08081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08081]] Real-Time Detection and Analysis of Vehicles and Pedestrians using Deep  Learning(https://arxiv.org/abs/2404.08081)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Computer vision, particularly vehicle and pedestrian identification is critical to the evolution of autonomous driving, artificial intelligence, and video surveillance. Current traffic monitoring systems confront major difficulty in recognizing small objects and pedestrians effectively in real-time, posing a serious risk to public safety and contributing to traffic inefficiency. Recognizing these difficulties, our project focuses on the creation and validation of an advanced deep-learning framework capable of processing complex visual input for precise, real-time recognition of cars and people in a variety of environmental situations. On a dataset representing complicated urban settings, we trained and evaluated different versions of the YOLOv8 and RT-DETR models. The YOLOv8 Large version proved to be the most effective, especially in pedestrian recognition, with great precision and robustness. The results, which include Mean Average Precision and recall rates, demonstrate the model's ability to dramatically improve traffic monitoring and safety. This study makes an important addition to real-time, reliable detection in computer vision, establishing new benchmarks for traffic management systems.</li>
</ul>

<h3>Title: Visual Context-Aware Person Fall Detection</h3>
<ul>
<li><strong>Authors: </strong>Aleksander Nagaj, Zenjie Li, Dim P. Papadopoulos, Kamal Nasrollahi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08088">https://arxiv.org/abs/2404.08088</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08088">https://arxiv.org/pdf/2404.08088</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08088]] Visual Context-Aware Person Fall Detection(https://arxiv.org/abs/2404.08088)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>As the global population ages, the number of fall-related incidents is on the rise. Effective fall detection systems, specifically in healthcare sector, are crucial to mitigate the risks associated with such events. This study evaluates the role of visual context, including background objects, on the accuracy of fall detection classifiers. We present a segmentation pipeline to semi-automatically separate individuals and objects in images. Well-established models like ResNet-18, EfficientNetV2-S, and Swin-Small are trained and evaluated. During training, pixel-based transformations are applied to segmented objects, and the models are then evaluated on raw images without segmentation. Our findings highlight the significant influence of visual context on fall detection. The application of Gaussian blur to the image background notably improves the performance and generalization capabilities of all models. Background objects such as beds, chairs, or wheelchairs can challenge fall detection systems, leading to false positive alarms. However, we demonstrate that object-specific contextual transformations during training effectively mitigate this challenge. Further analysis using saliency maps supports our observation that visual context is crucial in classification tasks. We create both dataset processing API and segmentation pipeline, available at https://github.com/A-NGJ/image-segmentation-cli.</li>
</ul>

<h3>Title: Efficient Duple Perturbation Robustness in Low-rank MDPs</h3>
<ul>
<li><strong>Authors: </strong>Yang Hu, Haitong Ma, Bo Dai, Na Li</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08089">https://arxiv.org/abs/2404.08089</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08089">https://arxiv.org/pdf/2404.08089</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08089]] Efficient Duple Perturbation Robustness in Low-rank MDPs(https://arxiv.org/abs/2404.08089)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The pursuit of robustness has recently been a popular topic in reinforcement learning (RL) research, yet the existing methods generally suffer from efficiency issues that obstruct their real-world implementation. In this paper, we introduce duple perturbation robustness, i.e. perturbation on both the feature and factor vectors for low-rank Markov decision processes (MDPs), via a novel characterization of $(\xi,\eta)$-ambiguity sets. The novel robust MDP formulation is compatible with the function representation view, and therefore, is naturally applicable to practical RL problems with large or even continuous state-action spaces. Meanwhile, it also gives rise to a provably efficient and practical algorithm with theoretical convergence rate guarantee. Examples are designed to justify the new robustness concept, and algorithmic efficiency is supported by both theoretical bounds and numerical simulations.</li>
</ul>

<h3>Title: Data-Augmentation-Based Dialectal Adaptation for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Fahim Faisal, Antonios Anastasopoulos</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08092">https://arxiv.org/abs/2404.08092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08092">https://arxiv.org/pdf/2404.08092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08092]] Data-Augmentation-Based Dialectal Adaptation for LLMs(https://arxiv.org/abs/2404.08092)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This report presents GMUNLP's participation to the Dialect-Copa shared task at VarDial 2024, which focuses on evaluating the commonsense reasoning capabilities of large language models (LLMs) on South Slavic micro-dialects. The task aims to assess how well LLMs can handle non-standard dialectal varieties, as their performance on standard languages is already well-established. We propose an approach that combines the strengths of different types of language models and leverages data augmentation techniques to improve task performance on three South Slavic dialects: Chakavian, Cherkano, and Torlak. We conduct experiments using a language-family-focused encoder-based model (BERTi\'c) and a domain-agnostic multilingual model (AYA-101). Our results demonstrate that the proposed data augmentation techniques lead to substantial performance gains across all three test datasets in the open-source model category. This work highlights the practical utility of data augmentation and the potential of LLMs in handling non-standard dialectal varieties, contributing to the broader goal of advancing natural language understanding in low-resource and dialectal settings. Code:https://github.com/ffaisal93/dialect_copa</li>
</ul>

<h3>Title: LLM Agents can Autonomously Exploit One-day Vulnerabilities</h3>
<ul>
<li><strong>Authors: </strong>Richard Fang, Rohan Bindu, Akul Gupta, Daniel Kang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08144">https://arxiv.org/abs/2404.08144</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08144">https://arxiv.org/pdf/2404.08144</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08144]] LLM Agents can Autonomously Exploit One-day Vulnerabilities(https://arxiv.org/abs/2404.08144)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>LLMs have becoming increasingly powerful, both in their benign and malicious uses. With the increase in capabilities, researchers have been increasingly interested in their ability to exploit cybersecurity vulnerabilities. In particular, recent work has conducted preliminary studies on the ability of LLM agents to autonomously hack websites. However, these studies are limited to simple vulnerabilities. In this work, we show that LLM agents can autonomously exploit one-day vulnerabilities in real-world systems. To show this, we collected a dataset of 15 one-day vulnerabilities that include ones categorized as critical severity in the CVE description. When given the CVE description, GPT-4 is capable of exploiting 87% of these vulnerabilities compared to 0% for every other model we test (GPT-3.5, open-source LLMs) and open-source vulnerability scanners (ZAP and Metasploit). Fortunately, our GPT-4 agent requires the CVE description for high performance: without the description, GPT-4 can exploit only 7% of the vulnerabilities. Our findings raise questions around the widespread deployment of highly capable LLM agents.</li>
</ul>

<h3>Title: Distilling Algorithmic Reasoning from LLMs via Explaining Solution  Programs</h3>
<ul>
<li><strong>Authors: </strong>Jierui Li, Raymond Mooney</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08148">https://arxiv.org/abs/2404.08148</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08148">https://arxiv.org/pdf/2404.08148</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08148]] Distilling Algorithmic Reasoning from LLMs via Explaining Solution  Programs(https://arxiv.org/abs/2404.08148)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Distilling explicit chain-of-thought reasoning paths has emerged as an effective method for improving the reasoning abilities of large language models (LLMs) across various tasks. However, when tackling complex tasks that pose significant challenges for state-of-the-art models, this technique often struggles to produce effective chains of thought that lead to correct answers. In this work, we propose a novel approach to distill reasoning abilities from LLMs by leveraging their capacity to explain solutions. We apply our method to solving competitive-level programming challenges. More specifically, we employ an LLM to generate explanations for a set of <problem, solution-program> pairs, then use <problem, explanation> pairs to fine-tune a smaller language model, which we refer to as the Reasoner, to learn algorithmic reasoning that can generate "how-to-solve" hints for unseen problems. Our experiments demonstrate that learning from explanations enables the Reasoner to more effectively guide program implementation by a Coder, resulting in higher solve rates than strong chain-of-thought baselines on competitive-level programming problems. It also outperforms models that learn directly from <problem, solution-program> pairs. We curated an additional test set in the CodeContests format, which includes 246 more recent problems posted after the models' knowledge cutoff.</li>
</ul>

<h3>Title: Eliminating Catastrophic Overfitting Via Abnormal Adversarial Examples  Regularization</h3>
<ul>
<li><strong>Authors: </strong>Runqi Lin, Chaojian Yu, Tongliang Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08154">https://arxiv.org/abs/2404.08154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08154">https://arxiv.org/pdf/2404.08154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08154]] Eliminating Catastrophic Overfitting Via Abnormal Adversarial Examples  Regularization(https://arxiv.org/abs/2404.08154)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Single-step adversarial training (SSAT) has demonstrated the potential to achieve both efficiency and robustness. However, SSAT suffers from catastrophic overfitting (CO), a phenomenon that leads to a severely distorted classifier, making it vulnerable to multi-step adversarial attacks. In this work, we observe that some adversarial examples generated on the SSAT-trained network exhibit anomalous behaviour, that is, although these training samples are generated by the inner maximization process, their associated loss decreases instead, which we named abnormal adversarial examples (AAEs). Upon further analysis, we discover a close relationship between AAEs and classifier distortion, as both the number and outputs of AAEs undergo a significant variation with the onset of CO. Given this observation, we re-examine the SSAT process and uncover that before the occurrence of CO, the classifier already displayed a slight distortion, indicated by the presence of few AAEs. Furthermore, the classifier directly optimizing these AAEs will accelerate its distortion, and correspondingly, the variation of AAEs will sharply increase as a result. In such a vicious circle, the classifier rapidly becomes highly distorted and manifests as CO within a few iterations. These observations motivate us to eliminate CO by hindering the generation of AAEs. Specifically, we design a novel method, termed Abnormal Adversarial Examples Regularization (AAER), which explicitly regularizes the variation of AAEs to hinder the classifier from becoming distorted. Extensive experiments demonstrate that our method can effectively eliminate CO and further boost adversarial robustness with negligible additional computational overhead.</li>
</ul>

<h3>Title: Graph Integrated Language Transformers for Next Action Prediction in  Complex Phone Calls</h3>
<ul>
<li><strong>Authors: </strong>Amin Hosseiny Marani, Ulie Schnaithmann, Youngseo Son, Akil Iyer, Manas Paldhe, Arushi Raghuvanshi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08155">https://arxiv.org/abs/2404.08155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08155">https://arxiv.org/pdf/2404.08155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08155]] Graph Integrated Language Transformers for Next Action Prediction in  Complex Phone Calls(https://arxiv.org/abs/2404.08155)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Current Conversational AI systems employ different machine learning pipelines, as well as external knowledge sources and business logic to predict the next action. Maintaining various components in dialogue managers' pipeline adds complexity in expansion and updates, increases processing time, and causes additive noise through the pipeline that can lead to incorrect next action prediction. This paper investigates graph integration into language transformers to improve understanding the relationships between humans' utterances, previous, and next actions without the dependency on external sources or components. Experimental analyses on real calls indicate that the proposed Graph Integrated Language Transformer models can achieve higher performance compared to other production level conversational AI systems in driving interactive calls with human users in real-world settings.</li>
</ul>

<h3>Title: A Survey on Security of Ultra/Hyper Reliable Low Latency Communication:  Recent Advancements, Challenges, and Future Directions</h3>
<ul>
<li><strong>Authors: </strong>Annapurna Pradhan, Susmita Das, Md. Jalil Piran, Zhu Han</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08160">https://arxiv.org/abs/2404.08160</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08160">https://arxiv.org/pdf/2404.08160</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08160]] A Survey on Security of Ultra/Hyper Reliable Low Latency Communication:  Recent Advancements, Challenges, and Future Directions(https://arxiv.org/abs/2404.08160)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, robust</a></li>
<li><strong>Abstract: </strong>Ultra-reliable low latency communication (URLLC) is an innovative service offered by fifth-generation (5G) wireless systems. URLLC enables various mission-critical applications by facilitating reliable and low-latency signal transmission to support extreme Quality of Service (QoS) requirements. Apart from reliability and latency, ensuring secure data transmission for URLLC has been a prominent issue for researchers in recent years. Using finite blocklength signals to achieve the stringent reliability and latency criteria in URLLC eliminates the possibility of using conventional complex cryptographic security enhancement techniques based on encoding and decoding of secret keys. Thus, the development of lightweight security mechanisms is of paramount importance for URLLC. Recently, Physical-Layer Security (PLS) techniques have emerged as a powerful alternative to the complex cryptography-based security approaches for facilitating secure URLLC by exploiting the randomness of the wireless channel. Therefore, in this survey, we present a comprehensive and in-depth review of the state-of-the-art PLS enhancements utilized to unleash secure URLLC while analyzing the impact of various system design parameters on its performance. Moreover, the survey incorporates a detailed overview of the recent advancements in ensuring secure URLLC using PLS in various mission-critical applications, and 5G URLLC enabling technologies like non-orthogonal multiple access (NOMA), multi-antenna systems, cooperative communication using unmanned aerial vehicles (UAV), and intelligent reflective surfaces (IRS). Apart from this, we briefly discuss the role of advanced Machine Learning (ML) techniques in designing robust and intelligent PLS schemes for URLLC service.</li>
</ul>

<h3>Title: Lightweight Cryptanalysis of IoT Encryption Algorithms : Is Quota  Sampling the Answer?</h3>
<ul>
<li><strong>Authors: </strong>Jonathan Cook, Sabih ur Rehman, M. Arif Khan</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08165">https://arxiv.org/abs/2404.08165</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08165">https://arxiv.org/pdf/2404.08165</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08165]] Lightweight Cryptanalysis of IoT Encryption Algorithms : Is Quota  Sampling the Answer?(https://arxiv.org/abs/2404.08165)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Rapid growth in the number of small sensor devices known as the Internet of Things (IoT) has seen the development of lightweight encryption algorithms. Two well-known lightweight algorithms are SIMON and SIMECK which have been specifically designed for use on resource-constrained IoT devices. These lightweight encryption algorithms are based on the efficient Feistel block structure which is known to exhibit vulnerabilities to differential cryptanalysis. Consequently, it is necessary to test these algorithms for resilience against such attacks. While existing state-of-the-art research has demonstrated novel heuristic methods of differential cryptanalysis that improve time efficiency on previous techniques, the large state sizes of these encryption algorithms inhibit cryptanalysis time efficiency. In this paper, we introduce Versatile Investigative Sampling Technique for Advanced Cryptanalysis (VISTA-CRYPT) - a time-efficient enhancement of differential cryptanalysis of lightweight encryption algorithms. The proposed technique introduces a simple framework of quota sampling that produces state-of-the-art results with time reductions of up to $76\%$ over existing techniques. Further, we present a preliminary graph-based analysis of the output differentials for the identification of relationships within the data and future research opportunities to further enhance the performance of differential cryptanalysis. The code designed for this work and associated datasets will be available at https://github.com/johncook1979/simon-cryptanalysis.</li>
</ul>

<h3>Title: Pay Attention to Your Neighbours: Training-Free Open-Vocabulary Semantic  Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Sina Hajimiri, Ismail Ben Ayed, Jose Dolz</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08181">https://arxiv.org/abs/2404.08181</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08181">https://arxiv.org/pdf/2404.08181</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08181]] Pay Attention to Your Neighbours: Training-Free Open-Vocabulary Semantic  Segmentation(https://arxiv.org/abs/2404.08181)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Despite the significant progress in deep learning for dense visual recognition problems, such as semantic segmentation, traditional methods are constrained by fixed class sets. Meanwhile, vision-language foundation models, such as CLIP, have showcased remarkable effectiveness in numerous zero-shot image-level tasks, owing to their robust generalizability. Recently, a body of work has investigated utilizing these models in open-vocabulary semantic segmentation (OVSS). However, existing approaches often rely on impractical supervised pre-training or access to additional pre-trained networks. In this work, we propose a strong baseline for training-free OVSS, termed Neighbour-Aware CLIP (NACLIP), representing a straightforward adaptation of CLIP tailored for this scenario. Our method enforces localization of patches in the self-attention of CLIP's vision transformer which, despite being crucial for dense prediction tasks, has been overlooked in the OVSS literature. By incorporating design choices favouring segmentation, our approach significantly improves performance without requiring additional data, auxiliary pre-trained networks, or extensive hyperparameter tuning, making it highly practical for real-world applications. Experiments are performed on 8 popular semantic segmentation benchmarks, yielding state-of-the-art performance on most scenarios. Our code is publicly available at https://github.com/sinahmr/NACLIP .</li>
</ul>

<h3>Title: Adapting CNNs for Fisheye Cameras without Retraining</h3>
<ul>
<li><strong>Authors: </strong>Ryan Griffiths, Donald G. Dansereau</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08187">https://arxiv.org/abs/2404.08187</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08187">https://arxiv.org/pdf/2404.08187</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08187]] Adapting CNNs for Fisheye Cameras without Retraining(https://arxiv.org/abs/2404.08187)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The majority of image processing approaches assume images are in or can be rectified to a perspective projection. However, in many applications it is beneficial to use non conventional cameras, such as fisheye cameras, that have a larger field of view (FOV). The issue arises that these large-FOV images can't be rectified to a perspective projection without significant cropping of the original image. To address this issue we propose Rectified Convolutions (RectConv); a new approach for adapting pre-trained convolutional networks to operate with new non-perspective images, without any retraining. Replacing the convolutional layers of the network with RectConv layers allows the network to see both rectified patches and the entire FOV. We demonstrate RectConv adapting multiple pre-trained networks to perform segmentation and detection on fisheye imagery from two publicly available datasets. Our approach requires no additional data or training, and operates directly on the native image as captured from the camera. We believe this work is a step toward adapting the vast resources available for perspective images to operate across a broad range of camera geometries.</li>
</ul>

<h3>Title: Reducing hallucination in structured outputs via Retrieval-Augmented  Generation</h3>
<ul>
<li><strong>Authors: </strong>Patrice Bchard, Orlando Marquez Ayala</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08189">https://arxiv.org/abs/2404.08189</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08189">https://arxiv.org/pdf/2404.08189</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08189]] Reducing hallucination in structured outputs via Retrieval-Augmented  Generation(https://arxiv.org/abs/2404.08189)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>A common and fundamental limitation of Generative AI (GenAI) is its propensity to hallucinate. While large language models (LLM) have taken the world by storm, without eliminating or at least reducing hallucinations, real-world GenAI systems may face challenges in user adoption. In the process of deploying an enterprise application that produces workflows based on natural language requirements, we devised a system leveraging Retrieval Augmented Generation (RAG) to greatly improve the quality of the structured output that represents such workflows. Thanks to our implementation of RAG, our proposed system significantly reduces hallucinations in the output and improves the generalization of our LLM in out-of-domain settings. In addition, we show that using a small, well-trained retriever encoder can reduce the size of the accompanying LLM, thereby making deployments of LLM-based systems less resource-intensive.</li>
</ul>

<h3>Title: Tackling Ambiguity from Perspective of Uncertainty Inference and  Affinity Diversification for Weakly Supervised Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Zhiwei Yang, Yucong Meng, Kexue Fu, Shuo Wang, Zhijian Song</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08195">https://arxiv.org/abs/2404.08195</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08195">https://arxiv.org/pdf/2404.08195</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08195]] Tackling Ambiguity from Perspective of Uncertainty Inference and  Affinity Diversification for Weakly Supervised Semantic Segmentation(https://arxiv.org/abs/2404.08195)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Weakly supervised semantic segmentation (WSSS) with image-level labels intends to achieve dense tasks without laborious annotations. However, due to the ambiguous contexts and fuzzy regions, the performance of WSSS, especially the stages of generating Class Activation Maps (CAMs) and refining pseudo masks, widely suffers from ambiguity while being barely noticed by previous literature. In this work, we propose UniA, a unified single-staged WSSS framework, to efficiently tackle this issue from the perspective of uncertainty inference and affinity diversification, respectively. When activating class objects, we argue that the false activation stems from the bias to the ambiguous regions during the feature extraction. Therefore, we design a more robust feature representation with a probabilistic Gaussian distribution and introduce the uncertainty estimation to avoid the bias. A distribution loss is particularly proposed to supervise the process, which effectively captures the ambiguity and models the complex dependencies among features. When refining pseudo labels, we observe that the affinity from the prevailing refinement methods intends to be similar among ambiguities. To this end, an affinity diversification module is proposed to promote diversity among semantics. A mutual complementing refinement is proposed to initially rectify the ambiguous affinity with multiple inferred pseudo labels. More importantly, a contrastive affinity loss is further designed to diversify the relations among unrelated semantics, which reliably propagates the diversity into the whole feature representations and helps generate better pseudo masks. Extensive experiments are conducted on PASCAL VOC, MS COCO, and medical ACDC datasets, which validate the efficiency of UniA tackling ambiguity and the superiority over recent single-staged or even most multi-staged competitors.</li>
</ul>

<h3>Title: Uncertain Boundaries: Multidisciplinary Approaches to Copyright Issues  in Generative AI</h3>
<ul>
<li><strong>Authors: </strong>Jocelyn Dzuong, Zichong Wang, Wenbin Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08221">https://arxiv.org/abs/2404.08221</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08221">https://arxiv.org/pdf/2404.08221</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08221]] Uncertain Boundaries: Multidisciplinary Approaches to Copyright Issues  in Generative AI(https://arxiv.org/abs/2404.08221)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, generative</a></li>
<li><strong>Abstract: </strong>In the rapidly evolving landscape of generative artificial intelligence (AI), the increasingly pertinent issue of copyright infringement arises as AI advances to generate content from scraped copyrighted data, prompting questions about ownership and protection that impact professionals across various careers. With this in mind, this survey provides an extensive examination of copyright infringement as it pertains to generative AI, aiming to stay abreast of the latest developments and open problems. Specifically, it will first outline methods of detecting copyright infringement in mediums such as text, image, and video. Next, it will delve an exploration of existing techniques aimed at safeguarding copyrighted works from generative models. Furthermore, this survey will discuss resources and tools for users to evaluate copyright violations. Finally, insights into ongoing regulations and proposals for AI will be explored and compared. Through combining these disciplines, the implications of AI-driven content and copyright are thoroughly illustrated and brought into question.</li>
</ul>

<h3>Title: HCL-MTSAD: Hierarchical Contrastive Consistency Learning for Accurate  Detection of Industrial Multivariate Time Series Anomalies</h3>
<ul>
<li><strong>Authors: </strong>Haili Sun, Yan Huang, Lansheng Han, Cai Fu, Chunjie Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR, cs.IT, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08224">https://arxiv.org/abs/2404.08224</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08224">https://arxiv.org/pdf/2404.08224</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08224]] HCL-MTSAD: Hierarchical Contrastive Consistency Learning for Accurate  Detection of Industrial Multivariate Time Series Anomalies(https://arxiv.org/abs/2404.08224)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Multivariate Time Series (MTS) anomaly detection focuses on pinpointing samples that diverge from standard operational patterns, which is crucial for ensuring the safety and security of industrial applications. The primary challenge in this domain is to develop representations capable of discerning anomalies effectively. The prevalent methods for anomaly detection in the literature are predominantly reconstruction-based and predictive in nature. However, they typically concentrate on a single-dimensional instance level, thereby not fully harnessing the complex associations inherent in industrial MTS. To address this issue, we propose a novel self-supervised hierarchical contrastive consistency learning method for detecting anomalies in MTS, named HCL-MTSAD. It innovatively leverages data consistency at multiple levels inherent in industrial MTS, systematically capturing consistent associations across four latent levels-measurement, sample, channel, and process. By developing a multi-layer contrastive loss, HCL-MTSAD can extensively mine data consistency and spatio-temporal association, resulting in more informative representations. Subsequently, an anomaly discrimination module, grounded in self-supervised hierarchical contrastive learning, is designed to detect timestamp-level anomalies by calculating multi-scale data consistency. Extensive experiments conducted on six diverse MTS datasets retrieved from real cyber-physical systems and server machines, in comparison with 20 baselines, indicate that HCL-MTSAD's anomaly detection capability outperforms the state-of-the-art benchmark models by an average of 1.8\% in terms of F1 score.</li>
</ul>

<h3>Title: Enhancing Fairness and Performance in Machine Learning Models: A  Multi-Task Learning Approach with Monte-Carlo Dropout and Pareto Optimality</h3>
<ul>
<li><strong>Authors: </strong>Khadija Zanna, Akane Sano</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08230">https://arxiv.org/abs/2404.08230</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08230">https://arxiv.org/pdf/2404.08230</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08230]] Enhancing Fairness and Performance in Machine Learning Models: A  Multi-Task Learning Approach with Monte-Carlo Dropout and Pareto Optimality(https://arxiv.org/abs/2404.08230)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, explainability</a></li>
<li><strong>Abstract: </strong>This paper considers the need for generalizable bias mitigation techniques in machine learning due to the growing concerns of fairness and discrimination in data-driven decision-making procedures across a range of industries. While many existing methods for mitigating bias in machine learning have succeeded in specific cases, they often lack generalizability and cannot be easily applied to different data types or models. Additionally, the trade-off between accuracy and fairness remains a fundamental tension in the field. To address these issues, we propose a bias mitigation method based on multi-task learning, utilizing the concept of Monte-Carlo dropout and Pareto optimality from multi-objective optimization. This method optimizes accuracy and fairness while improving the model's explainability without using sensitive information. We test this method on three datasets from different domains and show how it can deliver the most desired trade-off between model fairness and performance. This allows for tuning in specific domains where one metric may be more important than another. With the framework we introduce in this paper, we aim to enhance the fairness-performance trade-off and offer a solution to bias mitigation methods' generalizability issues in machine learning.</li>
</ul>

<h3>Title: Evaluation Framework for Quantum Security Risk Assessment: A  Comprehensive Study for Quantum-Safe Migration</h3>
<ul>
<li><strong>Authors: </strong>Yaser Baseri, Vikas Chouhan, Ali Ghorbani, Aaron Chow</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08231">https://arxiv.org/abs/2404.08231</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08231">https://arxiv.org/pdf/2404.08231</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08231]] Evaluation Framework for Quantum Security Risk Assessment: A  Comprehensive Study for Quantum-Safe Migration(https://arxiv.org/abs/2404.08231)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, robust</a></li>
<li><strong>Abstract: </strong>The rise of large-scale quantum computing poses a significant threat to traditional cryptographic security measures. Quantum attacks undermine current asymmetric cryptographic algorithms, rendering them ineffective. Even symmetric key cryptography is vulnerable, albeit to a lesser extent, suggesting longer keys or extended hash functions for security. Thus, current cryptographic solutions are inadequate against emerging quantum threats. Organizations must transition to quantum-safe environments with robust continuity plans and meticulous risk management. This study explores the challenges of migrating to quantum-safe cryptographic states, introducing a comprehensive security risk assessment framework. We propose a security risk assessment framework that examines vulnerabilities across algorithms, certificates, and protocols throughout the migration process (pre-migration, during migration, post-migration). We link these vulnerabilities to the STRIDE threat model to assess their impact and likelihood. Then, we discuss practical mitigation strategies for critical components like algorithms, public key infrastructures, and protocols. Our study not only identifies potential attacks and vulnerabilities at each layer and migration stage but also suggests possible countermeasures and alternatives to enhance system resilience, empowering organizations to construct a secure infrastructure for the quantum era. Through these efforts, we establish the foundation for enduring security in networked systems amid the challenges of the quantum era.</li>
</ul>

<h3>Title: Navigating Quantum Security Risks in Networked Environments: A  Comprehensive Study of Quantum-Safe Network Protocols</h3>
<ul>
<li><strong>Authors: </strong>Yaser Baseri, Vikas Chouhan, Abdelhakim Hafid</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08232">https://arxiv.org/abs/2404.08232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08232">https://arxiv.org/pdf/2404.08232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08232]] Navigating Quantum Security Risks in Networked Environments: A  Comprehensive Study of Quantum-Safe Network Protocols(https://arxiv.org/abs/2404.08232)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>The emergence of quantum computing poses a formidable security challenge to network protocols traditionally safeguarded by classical cryptographic algorithms. This paper provides an exhaustive analysis of vulnerabilities introduced by quantum computing in a diverse array of widely utilized security protocols across the layers of the TCP/IP model, including TLS, IPsec, SSH, PGP, and more. Our investigation focuses on precisely identifying vulnerabilities susceptible to exploitation by quantum adversaries at various migration stages for each protocol while also assessing the associated risks and consequences for secure communication. We delve deep into the impact of quantum computing on each protocol, emphasizing potential threats posed by quantum attacks and scrutinizing the effectiveness of post-quantum cryptographic solutions. Through carefully evaluating vulnerabilities and risks that network protocols face in the post-quantum era, this study provides invaluable insights to guide the development of appropriate countermeasures. Our findings contribute to a broader comprehension of quantum computing's influence on network security and offer practical guidance for protocol designers, implementers, and policymakers in addressing the challenges stemming from the advancement of quantum computing. This comprehensive study is a crucial step toward fortifying the security of networked environments in the quantum age.</li>
</ul>

<h3>Title: IFViT: Interpretable Fixed-Length Representation for Fingerprint  Matching via Vision Transformer</h3>
<ul>
<li><strong>Authors: </strong>Yuhang Qiu, Honghui Chen, Xingbo Dong, Zheng Lin, Iman Yi Liao, Massimo Tistarelli, Zhe Jin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08237">https://arxiv.org/abs/2404.08237</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08237">https://arxiv.org/pdf/2404.08237</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08237]] IFViT: Interpretable Fixed-Length Representation for Fingerprint  Matching via Vision Transformer(https://arxiv.org/abs/2404.08237)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Determining dense feature points on fingerprints used in constructing deep fixed-length representations for accurate matching, particularly at the pixel level, is of significant interest. To explore the interpretability of fingerprint matching, we propose a multi-stage interpretable fingerprint matching network, namely Interpretable Fixed-length Representation for Fingerprint Matching via Vision Transformer (IFViT), which consists of two primary modules. The first module, an interpretable dense registration module, establishes a Vision Transformer (ViT)-based Siamese Network to capture long-range dependencies and the global context in fingerprint pairs. It provides interpretable dense pixel-wise correspondences of feature points for fingerprint alignment and enhances the interpretability in the subsequent matching stage. The second module takes into account both local and global representations of the aligned fingerprint pair to achieve an interpretable fixed-length representation extraction and matching. It employs the ViTs trained in the first module with the additional fully connected layer and retrains them to simultaneously produce the discriminative fixed-length representation and interpretable dense pixel-wise correspondences of feature points. Extensive experimental results on diverse publicly available fingerprint databases demonstrate that the proposed framework not only exhibits superior performance on dense registration and matching but also significantly promotes the interpretability in deep fixed-length representations-based fingerprint matching.</li>
</ul>

<h3>Title: Balanced Mixed-Type Tabular Data Synthesis with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Zeyu Yang, Peikun Guo, Khadija Zanna, Akane Sano</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08254">https://arxiv.org/abs/2404.08254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08254">https://arxiv.org/pdf/2404.08254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08254]] Balanced Mixed-Type Tabular Data Synthesis with Diffusion Models(https://arxiv.org/abs/2404.08254)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged as a robust framework for various generative tasks, such as image and audio synthesis, and have also demonstrated a remarkable ability to generate mixed-type tabular data comprising both continuous and discrete variables. However, current approaches to training diffusion models on mixed-type tabular data tend to inherit the imbalanced distributions of features present in the training dataset, which can result in biased sampling. In this research, we introduce a fair diffusion model designed to generate balanced data on sensitive attributes. We present empirical evidence demonstrating that our method effectively mitigates the class imbalance in training data while maintaining the quality of the generated samples. Furthermore, we provide evidence that our approach outperforms existing methods for synthesizing tabular data in terms of performance and fairness.</li>
</ul>

<h3>Title: Practical Region-level Attack against Segment Anything Models</h3>
<ul>
<li><strong>Authors: </strong>Yifan Shen, Zhengyuan Li, Gang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08255">https://arxiv.org/abs/2404.08255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08255">https://arxiv.org/pdf/2404.08255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08255]] Practical Region-level Attack against Segment Anything Models(https://arxiv.org/abs/2404.08255)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, segmentation</a></li>
<li><strong>Abstract: </strong>Segment Anything Models (SAM) have made significant advancements in image segmentation, allowing users to segment target portions of an image with a single click (i.e., user prompt). Given its broad applications, the robustness of SAM against adversarial attacks is a critical concern. While recent works have explored adversarial attacks against a pre-defined prompt/click, their threat model is not yet realistic: (1) they often assume the user-click position is known to the attacker (point-based attack), and (2) they often operate under a white-box setting with limited transferability. In this paper, we propose a more practical region-level attack where attackers do not need to know the precise user prompt. The attack remains effective as the user clicks on any point on the target object in the image, hiding the object from SAM. Also, by adapting a spectrum transformation method, we make the attack more transferable under a black-box setting. Both control experiments and testing against real-world SAM services confirm its effectiveness.</li>
</ul>

<h3>Title: Investigating Neural Machine Translation for Low-Resource Languages:  Using Bavarian as a Case Study</h3>
<ul>
<li><strong>Authors: </strong>Wan-Hua Her, Udo Kruschwitz</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08259">https://arxiv.org/abs/2404.08259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08259">https://arxiv.org/pdf/2404.08259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08259]] Investigating Neural Machine Translation for Low-Resource Languages:  Using Bavarian as a Case Study(https://arxiv.org/abs/2404.08259)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Machine Translation has made impressive progress in recent years offering close to human-level performance on many languages, but studies have primarily focused on high-resource languages with broad online presence and resources. With the help of growing Large Language Models, more and more low-resource languages achieve better results through the presence of other languages. However, studies have shown that not all low-resource languages can benefit from multilingual systems, especially those with insufficient training and evaluation data. In this paper, we revisit state-of-the-art Neural Machine Translation techniques to develop automatic translation systems between German and Bavarian. We investigate conditions of low-resource languages such as data scarcity and parameter sensitivity and focus on refined solutions that combat low-resource difficulties and creative solutions such as harnessing language similarity. Our experiment entails applying Back-translation and Transfer Learning to automatically generate more training data and achieve higher translation performance. We demonstrate noisiness in the data and present our approach to carry out text preprocessing extensively. Evaluation was conducted using combined metrics: BLEU, chrF and TER. Statistical significance results with Bonferroni correction show surprisingly high baseline systems, and that Back-translation leads to significant improvement. Furthermore, we present a qualitative analysis of translation errors and system limitations.</li>
</ul>

<h3>Title: Pretraining and Updating Language- and Domain-specific Large Language  Model: A Case Study in Japanese Business Domain</h3>
<ul>
<li><strong>Authors: </strong>Kosuke Takahashi, Takahiro Omi, Kosuke Arima, Tatsuya Ishigaki</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08262">https://arxiv.org/abs/2404.08262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08262">https://arxiv.org/pdf/2404.08262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08262]] Pretraining and Updating Language- and Domain-specific Large Language  Model: A Case Study in Japanese Business Domain(https://arxiv.org/abs/2404.08262)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Several previous studies have considered language- and domain-specific large language models (LLMs) as separate topics. This study explores the combination of a non-English language and a high-demand industry domain, focusing on a Japanese business-specific LLM. This type of a model requires expertise in the business domain, strong language skills, and regular updates of its knowledge. We trained a 13-billion-parameter LLM from scratch using a new dataset of business texts and patents, and continually pretrained it with the latest business documents. Further we propose a new benchmark for Japanese business domain question answering (QA) and evaluate our models on it. The results show that our pretrained model improves QA accuracy without losing general knowledge, and that continual pretraining enhances adaptation to new information. Our pretrained model and business domain benchmark are publicly available.</li>
</ul>

<h3>Title: Transfer Learning Study of Motion Transformer-based Trajectory  Predictions</h3>
<ul>
<li><strong>Authors: </strong>Lars Ullrich, Alex McMaster, Knut Graichen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08271">https://arxiv.org/abs/2404.08271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08271">https://arxiv.org/pdf/2404.08271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08271]] Transfer Learning Study of Motion Transformer-based Trajectory  Predictions(https://arxiv.org/abs/2404.08271)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Trajectory planning in autonomous driving is highly dependent on predicting the emergent behavior of other road users. Learning-based methods are currently showing impressive results in simulation-based challenges, with transformer-based architectures technologically leading the way. Ultimately, however, predictions are needed in the real world. In addition to the shifts from simulation to the real world, many vehicle- and country-specific shifts, i.e. differences in sensor systems, fusion and perception algorithms as well as traffic rules and laws, are on the agenda. Since models that can cover all system setups and design domains at once are not yet foreseeable, model adaptation plays a central role. Therefore, a simulation-based study on transfer learning techniques is conducted on basis of a transformer-based model. Furthermore, the study aims to provide insights into possible trade-offs between computational time and performance to support effective transfers into the real world.</li>
</ul>

<h3>Title: Struggle with Adversarial Defense? Try Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Yujie Li, Yanbin Wang, Haitao xu, Bin Liu, Jianguo Sun, Zhenhao Guo, Wenrui Ma</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08273">https://arxiv.org/abs/2404.08273</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08273">https://arxiv.org/pdf/2404.08273</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08273]] Struggle with Adversarial Defense? Try Diffusion(https://arxiv.org/abs/2404.08273)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Adversarial attacks induce misclassification by introducing subtle perturbations. Recently, diffusion models are applied to the image classifiers to improve adversarial robustness through adversarial training or by purifying adversarial noise. However, diffusion-based adversarial training often encounters convergence challenges and high computational expenses. Additionally, diffusion-based purification inevitably causes data shift and is deemed susceptible to stronger adaptive attacks. To tackle these issues, we propose the Truth Maximization Diffusion Classifier (TMDC), a generative Bayesian classifier that builds upon pre-trained diffusion models and the Bayesian theorem. Unlike data-driven classifiers, TMDC, guided by Bayesian principles, utilizes the conditional likelihood from diffusion models to determine the class probabilities of input images, thereby insulating against the influences of data shift and the limitations of adversarial training. Moreover, to enhance TMDC's resilience against more potent adversarial attacks, we propose an optimization strategy for diffusion classifiers. This strategy involves post-training the diffusion model on perturbed datasets with ground-truth labels as conditions, guiding the diffusion model to learn the data distribution and maximizing the likelihood under the ground-truth labels. The proposed method achieves state-of-the-art performance on the CIFAR10 dataset against heavy white-box attacks and strong adaptive attacks. Specifically, TMDC achieves robust accuracies of 82.81% against $l_{\infty}$ norm-bounded perturbations and 86.05% against $l_{2}$ norm-bounded perturbations, respectively, with $\epsilon=0.05$.</li>
</ul>

<h3>Title: FaceFilterSense: A Filter-Resistant Face Recognition and Facial  Attribute Analysis Framework</h3>
<ul>
<li><strong>Authors: </strong>Shubham Tiwari, Yash Sethia, Ritesh Kumar, Ashwani Tanwar, Rudresh Dwivedi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08277">https://arxiv.org/abs/2404.08277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08277">https://arxiv.org/pdf/2404.08277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08277]] FaceFilterSense: A Filter-Resistant Face Recognition and Facial  Attribute Analysis Framework(https://arxiv.org/abs/2404.08277)</code><input type="text"></li>
<li><strong>Keywords: </strong>biometric</a></li>
<li><strong>Abstract: </strong>With the advent of social media, fun selfie filters have come into tremendous mainstream use affecting the functioning of facial biometric systems as well as image recognition systems. These filters vary from beautification filters and Augmented Reality (AR)-based filters to filters that modify facial landmarks. Hence, there is a need to assess the impact of such filters on the performance of existing face recognition systems. The limitation associated with existing solutions is that these solutions focus more on the beautification filters. However, the current AR-based filters and filters which distort facial key points are in vogue recently and make the faces highly unrecognizable even to the naked eye. Also, the filters considered are mostly obsolete with limited variations. To mitigate these limitations, we aim to perform a holistic impact analysis of the latest filters and propose an user recognition model with the filtered images. We have utilized a benchmark dataset for baseline images, and applied the latest filters over them to generate a beautified/filtered dataset. Next, we have introduced a model FaceFilterNet for beautified user recognition. In this framework, we also utilize our model to comment on various attributes of the person including age, gender, and ethnicity. In addition, we have also presented a filter-wise impact analysis on face recognition, age estimation, gender, and ethnicity prediction. The proposed method affirms the efficacy of our dataset with an accuracy of 87.25% and an optimal accuracy for facial attribute analysis.</li>
</ul>

<h3>Title: Calibration & Reconstruction: Deep Integrated Language for Referring  Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Yichen Yan, Xingjian He, Sihan Chen, Jing Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08281">https://arxiv.org/abs/2404.08281</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08281">https://arxiv.org/pdf/2404.08281</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08281]] Calibration & Reconstruction: Deep Integrated Language for Referring  Image Segmentation(https://arxiv.org/abs/2404.08281)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Referring image segmentation aims to segment an object referred to by natural language expression from an image. The primary challenge lies in the efficient propagation of fine-grained semantic information from textual features to visual features. Many recent works utilize a Transformer to address this challenge. However, conventional transformer decoders can distort linguistic information with deeper layers, leading to suboptimal results. In this paper, we introduce CRFormer, a model that iteratively calibrates multi-modal features in the transformer decoder. We start by generating language queries using vision features, emphasizing different aspects of the input language. Then, we propose a novel Calibration Decoder (CDec) wherein the multi-modal features can iteratively calibrated by the input language features. In the Calibration Decoder, we use the output of each decoder layer and the original language features to generate new queries for continuous calibration, which gradually updates the language features. Based on CDec, we introduce a Language Reconstruction Module and a reconstruction loss. This module leverages queries from the final layer of the decoder to reconstruct the input language and compute the reconstruction loss. This can further prevent the language information from being lost or distorted. Our experiments consistently show the superior performance of our approach across RefCOCO, RefCOCO+, and G-Ref datasets compared to state-of-the-art methods.</li>
</ul>

<h3>Title: A Survey of Neural Network Robustness Assessment in Image Recognition</h3>
<ul>
<li><strong>Authors: </strong>Jie Wang, Jun Ai, Minyan Lu, Haoran Su, Dan Yu, Yutao Zhang, Junda Zhu, Jingyu Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08285">https://arxiv.org/abs/2404.08285</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08285">https://arxiv.org/pdf/2404.08285</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08285]] A Survey of Neural Network Robustness Assessment in Image Recognition(https://arxiv.org/abs/2404.08285)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>In recent years, there has been significant attention given to the robustness assessment of neural networks. Robustness plays a critical role in ensuring reliable operation of artificial intelligence (AI) systems in complex and uncertain environments. Deep learning's robustness problem is particularly significant, highlighted by the discovery of adversarial attacks on image classification models. Researchers have dedicated efforts to evaluate robustness in diverse perturbation conditions for image recognition tasks. Robustness assessment encompasses two main techniques: robustness verification/ certification for deliberate adversarial attacks and robustness testing for random data corruptions. In this survey, we present a detailed examination of both adversarial robustness (AR) and corruption robustness (CR) in neural network assessment. Analyzing current research papers and standards, we provide an extensive overview of robustness assessment in image recognition. Three essential aspects are analyzed: concepts, metrics, and assessment methods. We investigate the perturbation metrics and range representations used to measure the degree of perturbations on images, as well as the robustness metrics specifically for the robustness conditions of classification models. The strengths and limitations of the existing methods are also discussed, and some potential directions for future research are provided.</li>
</ul>

<h3>Title: AdaContour: Adaptive Contour Descriptor with Hierarchical Representation</h3>
<ul>
<li><strong>Authors: </strong>Tianyu Ding, Jinxin Zhou, Tianyi Chen, Zhihui Zhu, Ilya Zharkov, Luming Liang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08292">https://arxiv.org/abs/2404.08292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08292">https://arxiv.org/pdf/2404.08292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08292]] AdaContour: Adaptive Contour Descriptor with Hierarchical Representation(https://arxiv.org/abs/2404.08292)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Existing angle-based contour descriptors suffer from lossy representation for non-starconvex shapes. By and large, this is the result of the shape being registered with a single global inner center and a set of radii corresponding to a polar coordinate parameterization. In this paper, we propose AdaContour, an adaptive contour descriptor that uses multiple local representations to desirably characterize complex shapes. After hierarchically encoding object shapes in a training set and constructing a contour matrix of all subdivided regions, we compute a robust low-rank robust subspace and approximate each local contour by linearly combining the shared basis vectors to represent an object. Experiments show that AdaContour is able to represent shapes more accurately and robustly than other descriptors while retaining effectiveness. We validate AdaContour by integrating it into off-the-shelf detectors to enable instance segmentation which demonstrates faithful performance. The code is available at https://github.com/tding1/AdaContour.</li>
</ul>

<h3>Title: Overcoming Scene Context Constraints for Object Detection in wild using  Defilters</h3>
<ul>
<li><strong>Authors: </strong>Vamshi Krishna Kancharla, Neelam sinha</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08293">https://arxiv.org/abs/2404.08293</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08293">https://arxiv.org/pdf/2404.08293</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08293]] Overcoming Scene Context Constraints for Object Detection in wild using  Defilters(https://arxiv.org/abs/2404.08293)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This paper focuses on improving object detection performance by addressing the issue of image distortions, commonly encountered in uncontrolled acquisition environments. High-level computer vision tasks such as object detection, recognition, and segmentation are particularly sensitive to image distortion. To address this issue, we propose a novel approach employing an image defilter to rectify image distortion prior to object detection. This method enhances object detection accuracy, as models perform optimally when trained on non-distorted images. Our experiments demonstrate that utilizing defiltered images significantly improves mean average precision compared to training object detection models on distorted images. Consequently, our proposed method offers considerable benefits for real-world applications plagued by image distortion. To our knowledge, the contribution lies in employing distortion-removal paradigm for object detection on images captured in natural settings. We achieved an improvement of 0.562 and 0.564 of mean Average precision on validation and test data.</li>
</ul>

<h3>Title: Interference Motion Removal for Doppler Radar Vital Sign Detection Using  Variational Encoder-Decoder Neural Network</h3>
<ul>
<li><strong>Authors: </strong>Mikolaj Czerkawski, Christos Ilioudis, Carmine Clemente, Craig Michie, Ivan Andonovic, Christos Tachtatzis</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08298">https://arxiv.org/abs/2404.08298</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08298">https://arxiv.org/pdf/2404.08298</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08298]] Interference Motion Removal for Doppler Radar Vital Sign Detection Using  Variational Encoder-Decoder Neural Network(https://arxiv.org/abs/2404.08298)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>The treatment of interfering motion contributions remains one of the key challenges in the domain of radar-based vital sign monitoring. Removal of the interference to extract the vital sign contributions is demanding due to overlapping Doppler bands, the complex structure of the interference motions and significant variations in the power levels of their contributions. A novel approach to the removal of interference through the use of a probabilistic deep learning model is presented. Results show that a convolutional encoder-decoder neural network with a variational objective is capable of learning a meaningful representation space of vital sign Doppler-time distribution facilitating their extraction from a mixture signal. The approach is tested on semi-experimental data containing real vital sign signatures and simulated returns from interfering body motions. The application of the proposed network enhances the extraction of the micro-Doppler frequency corresponding to the respiration rate is demonstrated.</li>
</ul>

<h3>Title: Subtoxic Questions: Dive Into Attitude Change of LLM's Response in  Jailbreak Attempts</h3>
<ul>
<li><strong>Authors: </strong>Tianyu Zhang, Zixuan Zhao, Jiaqi Huang, Jingyu Hua, Sheng Zhong</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08309">https://arxiv.org/abs/2404.08309</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08309">https://arxiv.org/pdf/2404.08309</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08309]] Subtoxic Questions: Dive Into Attitude Change of LLM's Response in  Jailbreak Attempts(https://arxiv.org/abs/2404.08309)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, large language model</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) of Prompt Jailbreaking are getting more and more attention, it is of great significance to raise a generalized research paradigm to evaluate attack strengths and a basic model to conduct subtler experiments. In this paper, we propose a novel approach by focusing on a set of target questions that are inherently more sensitive to jailbreak prompts, aiming to circumvent the limitations posed by enhanced LLM security. Through designing and analyzing these sensitive questions, this paper reveals a more effective method of identifying vulnerabilities in LLMs, thereby contributing to the advancement of LLM security. This research not only challenges existing jailbreaking methodologies but also fortifies LLMs against potential exploits.</li>
</ul>

<h3>Title: Manifest V3 Unveiled: Navigating the New Era of Browser Extensions</h3>
<ul>
<li><strong>Authors: </strong>Nikolaos Pantelaios, Alexandros Kapravelos</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08310">https://arxiv.org/abs/2404.08310</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08310">https://arxiv.org/pdf/2404.08310</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08310]] Manifest V3 Unveiled: Navigating the New Era of Browser Extensions(https://arxiv.org/abs/2404.08310)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy</a></li>
<li><strong>Abstract: </strong>Introduced over a decade ago, Chrome extensions now exceed 200,000 in number. In 2020, Google announced a shift in extension development with Manifest Version 3 (V3), aiming to replace the previous Version 2 (V2) by January 2023. This deadline was later extended to January 2025. The company's decision is grounded in enhancing three main pillars: privacy, security, and performance. This paper presents a comprehensive analysis of the Manifest V3 ecosystem. We start by investigating the adoption rate of V3, detailing the percentage of adoption from its announcement up until 2024. Our findings indicate, prior to the 2023 pause, less than 5% of all extensions had transitioned to V3, despite the looming deadline for the complete removal of V2, while currently nine out of ten new extensions are being uploaded in Manifest V3. Furthermore, we compare the security and privacy enhancements between V2 and V3 and we evaluate the improved security attributable to V3's safer APIs, examining how certain APIs, which were vulnerable or facilitated malicious behavior, have been deprecated or removed in V3. We dynamically execute 517 confirmed malicious extensions and we see a 87.8% removal of APIs related to malicious behavior due to the improvements of V3. We discover that only 154 (29.8%) of these extensions remain functional post-conversion. This analysis leads to the conclusion that V3 reduces the avenues for abuse of such APIs. However, despite the reduction in APIs associated with malicious activities, the new Manifest V3 protocol is not immune to such behavior. Our research demonstrates, through a proof of concept, the adaptability of malicious activities to V3. After the proof of concept changes are applied, we showcase 290 (56%) of the examined malicious extensions retain their capability to conduct harmful activities within the V3 framework.</li>
</ul>

<h3>Title: GPN: Generative Point-based NeRF</h3>
<ul>
<li><strong>Authors: </strong>Haipeng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08312">https://arxiv.org/abs/2404.08312</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08312">https://arxiv.org/pdf/2404.08312</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08312]] GPN: Generative Point-based NeRF(https://arxiv.org/abs/2404.08312)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Scanning real-life scenes with modern registration devices typically gives incomplete point cloud representations, primarily due to the limitations of partial scanning, 3D occlusions, and dynamic light conditions. Recent works on processing incomplete point clouds have always focused on point cloud completion. However, these approaches do not ensure consistency between the completed point cloud and the captured images regarding color and geometry. We propose using Generative Point-based NeRF (GPN) to reconstruct and repair a partial cloud by fully utilizing the scanning images and the corresponding reconstructed cloud. The repaired point cloud can achieve multi-view consistency with the captured images at high spatial resolution. For the finetunes of a single scene, we optimize the global latent condition by incorporating an Auto-Decoder architecture while retaining multi-view consistency. As a result, the generated point clouds are smooth, plausible, and geometrically consistent with the partial scanning images. Extensive experiments on ShapeNet demonstrate that our works achieve competitive performances to the other state-of-the-art point cloud-based neural scene rendering and editing performances.</li>
</ul>

<h3>Title: The Integration of Semantic and Structural Knowledge in Knowledge Graph  Entity Typing</h3>
<ul>
<li><strong>Authors: </strong>Muzhi Li, Minda Hu, Irwin King, Ho-fung Leung</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08313">https://arxiv.org/abs/2404.08313</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08313">https://arxiv.org/pdf/2404.08313</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08313]] The Integration of Semantic and Structural Knowledge in Knowledge Graph  Entity Typing(https://arxiv.org/abs/2404.08313)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The Knowledge Graph Entity Typing (KGET) task aims to predict missing type annotations for entities in knowledge graphs. Recent works only utilize the \textit{\textbf{structural knowledge}} in the local neighborhood of entities, disregarding \textit{\textbf{semantic knowledge}} in the textual representations of entities, relations, and types that are also crucial for type inference. Additionally, we observe that the interaction between semantic and structural knowledge can be utilized to address the false-negative problem. In this paper, we propose a novel \textbf{\underline{S}}emantic and \textbf{\underline{S}}tructure-aware KG \textbf{\underline{E}}ntity \textbf{\underline{T}}yping~{(SSET)} framework, which is composed of three modules. First, the \textit{Semantic Knowledge Encoding} module encodes factual knowledge in the KG with a Masked Entity Typing task. Then, the \textit{Structural Knowledge Aggregation} module aggregates knowledge from the multi-hop neighborhood of entities to infer missing types. Finally, the \textit{Unsupervised Type Re-ranking} module utilizes the inference results from the two models above to generate type predictions that are robust to false-negative samples. Extensive experiments show that SSET significantly outperforms existing state-of-the-art methods.</li>
</ul>

<h3>Title: Salience-Based Adaptive Masking: Revisiting Token Dynamics for Enhanced  Pre-training</h3>
<ul>
<li><strong>Authors: </strong>Hyesong Choi, Hyejin Park, Kwang Moo Yi, Sungmin Cha, Dongbo Min</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08327">https://arxiv.org/abs/2404.08327</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08327">https://arxiv.org/pdf/2404.08327</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08327]] Salience-Based Adaptive Masking: Revisiting Token Dynamics for Enhanced  Pre-training(https://arxiv.org/abs/2404.08327)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce Saliency-Based Adaptive Masking (SBAM), a novel and cost-effective approach that significantly enhances the pre-training performance of Masked Image Modeling (MIM) approaches by prioritizing token salience. Our method provides robustness against variations in masking ratios, effectively mitigating the performance instability issues common in existing methods. This relaxes the sensitivity of MIM-based pre-training to masking ratios, which in turn allows us to propose an adaptive strategy for `tailored' masking ratios for each data sample, which no existing method can provide. Toward this goal, we propose an Adaptive Masking Ratio (AMR) strategy that dynamically adjusts the proportion of masking for the unique content of each image based on token salience. We show that our method significantly improves over the state-of-the-art in mask-based pre-training on the ImageNet-1K dataset.</li>
</ul>

<h3>Title: Toward a Theory of Tokenization in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Nived Rajaraman, Jiantao Jiao, Kannan Ramchandran</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08335">https://arxiv.org/abs/2404.08335</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08335">https://arxiv.org/pdf/2404.08335</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08335]] Toward a Theory of Tokenization in LLMs(https://arxiv.org/abs/2404.08335)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>While there has been a large body of research attempting to circumvent tokenization for language modeling (Clark et al., 2022; Xue et al., 2022), the current consensus is that it is a necessary initial step for designing state-of-the-art performant language models. In this paper, we investigate tokenization from a theoretical point of view by studying the behavior of transformers on simple data generating processes. When trained on data drawn from certain simple $k^{\text{th}}$-order Markov processes for $k > 1$, transformers exhibit a surprising phenomenon - in the absence of tokenization, they empirically fail to learn the right distribution and predict characters according to a unigram model (Makkuva et al., 2024). With the addition of tokenization, however, we empirically observe that transformers break through this barrier and are able to model the probabilities of sequences drawn from the source near-optimally, achieving small cross-entropy loss. With this observation as starting point, we study the end-to-end cross-entropy loss achieved by transformers with and without tokenization. With the appropriate tokenization, we show that even the simplest unigram models (over tokens) learnt by transformers are able to model the probability of sequences drawn from $k^{\text{th}}$-order Markov sources near optimally. Our analysis provides a justification for the use of tokenization in practice through studying the behavior of transformers on Markovian data.</li>
</ul>

<h3>Title: Counterfactual Explanations for Face Forgery Detection via Adversarial  Removal of Artifacts</h3>
<ul>
<li><strong>Authors: </strong>Yang Li, Songlin Yang, Wei Wang, Ziwen He, Bo Peng, Jing Dong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08341">https://arxiv.org/abs/2404.08341</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08341">https://arxiv.org/pdf/2404.08341</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08341]] Counterfactual Explanations for Face Forgery Detection via Adversarial  Removal of Artifacts(https://arxiv.org/abs/2404.08341)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, generative</a></li>
<li><strong>Abstract: </strong>Highly realistic AI generated face forgeries known as deepfakes have raised serious social concerns. Although DNN-based face forgery detection models have achieved good performance, they are vulnerable to latest generative methods that have less forgery traces and adversarial attacks. This limitation of generalization and robustness hinders the credibility of detection results and requires more explanations. In this work, we provide counterfactual explanations for face forgery detection from an artifact removal perspective. Specifically, we first invert the forgery images into the StyleGAN latent space, and then adversarially optimize their latent representations with the discrimination supervision from the target detection model. We verify the effectiveness of the proposed explanations from two aspects: (1) Counterfactual Trace Visualization: the enhanced forgery images are useful to reveal artifacts by visually contrasting the original images and two different visualization methods; (2) Transferable Adversarial Attacks: the adversarial forgery images generated by attacking the detection model are able to mislead other detection models, implying the removed artifacts are general. Extensive experiments demonstrate that our method achieves over 90% attack success rate and superior attack transferability. Compared with naive adversarial noise methods, our method adopts both generative and discriminative model priors, and optimize the latent representations in a synthesis-by-analysis way, which forces the search of counterfactual explanations on the natural face manifold. Thus, more general counterfactual traces can be found and better adversarial attack transferability can be achieved.</li>
</ul>

<h3>Title: Let It Flow: Simultaneous Optimization of 3D Flow and Object Clustering</h3>
<ul>
<li><strong>Authors: </strong>Patrik Vacek, David Hurych, Tom Svoboda, Karel Zimmermann</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08363">https://arxiv.org/abs/2404.08363</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08363">https://arxiv.org/pdf/2404.08363</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08363]] Let It Flow: Simultaneous Optimization of 3D Flow and Object Clustering(https://arxiv.org/abs/2404.08363)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We study the problem of self-supervised 3D scene flow estimation from real large-scale raw point cloud sequences, which is crucial to various tasks like trajectory prediction or instance segmentation. In the absence of ground truth scene flow labels, contemporary approaches concentrate on deducing optimizing flow across sequential pairs of point clouds by incorporating structure based regularization on flow and object rigidity. The rigid objects are estimated by a variety of 3D spatial clustering methods. While state-of-the-art methods successfully capture overall scene motion using the Neural Prior structure, they encounter challenges in discerning multi-object motions. We identified the structural constraints and the use of large and strict rigid clusters as the main pitfall of the current approaches and we propose a novel clustering approach that allows for combination of overlapping soft clusters as well as non-overlapping rigid clusters representation. Flow is then jointly estimated with progressively growing non-overlapping rigid clusters together with fixed size overlapping soft clusters. We evaluate our method on multiple datasets with LiDAR point clouds, demonstrating the superior performance over the self-supervised baselines reaching new state of the art results. Our method especially excels in resolving flow in complicated dynamic scenes with multiple independently moving objects close to each other which includes pedestrians, cyclists and other vulnerable road users. Our codes will be publicly available.</li>
</ul>

<h3>Title: Graph data augmentation with Gromow-Wasserstein Barycenters</h3>
<ul>
<li><strong>Authors: </strong>Andrea Ponti</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08376">https://arxiv.org/abs/2404.08376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08376">https://arxiv.org/pdf/2404.08376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08376]] Graph data augmentation with Gromow-Wasserstein Barycenters(https://arxiv.org/abs/2404.08376)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Graphs are ubiquitous in various fields, and deep learning methods have been successful applied in graph classification tasks. However, building large and diverse graph datasets for training can be expensive. While augmentation techniques exist for structured data like images or numerical data, the augmentation of graph data remains challenging. This is primarily due to the complex and non-Euclidean nature of graph data. In this paper, it has been proposed a novel augmentation strategy for graphs that operates in a non-Euclidean space. This approach leverages graphon estimation, which models the generative mechanism of networks sequences. Computational results demonstrate the effectiveness of the proposed augmentation framework in improving the performance of graph classification models. Additionally, using a non-Euclidean distance, specifically the Gromow-Wasserstein distance, results in better approximations of the graphon. This framework also provides a means to validate different graphon estimation approaches, particularly in real-world scenarios where the true graphon is unknown.</li>
</ul>

<h3>Title: Look at the Text: Instruction-Tuned Language Models are More Robust  Multiple Choice Selectors than You Think</h3>
<ul>
<li><strong>Authors: </strong>Xinpeng Wang, Chengzhi Hu, Bolei Ma, Paul Rttger, Barbara Plank</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08382">https://arxiv.org/abs/2404.08382</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08382">https://arxiv.org/pdf/2404.08382</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08382]] Look at the Text: Instruction-Tuned Language Models are More Robust  Multiple Choice Selectors than You Think(https://arxiv.org/abs/2404.08382)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Multiple choice questions (MCQs) are commonly used to evaluate the capabilities of large language models (LLMs). One common way to evaluate the model response is to rank the candidate answers based on the log probability of the first token prediction. An alternative way is to examine the text output. Prior work has shown that first token probabilities lack robustness to changes in MCQ phrasing, and that first token probabilities do not match text answers for instruction-tuned models. Therefore, in this paper, we investigate the robustness of text answers. We show that the text answers are more robust to question perturbations than the first token probabilities, when the first token answers mismatch the text answers. The difference in robustness increases as the mismatch rate becomes greater. As the mismatch reaches over 50\%, the text answer is more robust to option order changes than the debiased first token probabilities using state-of-the-art debiasing methods such as PriDe. Our findings provide further evidence for the benefits of text answer evaluation over first token probability evaluation.</li>
</ul>

<h3>Title: NC-TTT: A Noise Contrastive Approach for Test-Time Training</h3>
<ul>
<li><strong>Authors: </strong>David Osowiechi, Gustavo A. Vargas Hakim, Mehrdad Noori, Milad Cheraghalikhani, Ali Bahri, Moslem Yazdanpanah, Ismail Ben Ayed, Christian Desrosiers</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08392">https://arxiv.org/abs/2404.08392</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08392">https://arxiv.org/pdf/2404.08392</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08392]] NC-TTT: A Noise Contrastive Approach for Test-Time Training(https://arxiv.org/abs/2404.08392)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Despite their exceptional performance in vision tasks, deep learning models often struggle when faced with domain shifts during testing. Test-Time Training (TTT) methods have recently gained popularity by their ability to enhance the robustness of models through the addition of an auxiliary objective that is jointly optimized with the main task. Being strictly unsupervised, this auxiliary objective is used at test time to adapt the model without any access to labels. In this work, we propose Noise-Contrastive Test-Time Training (NC-TTT), a novel unsupervised TTT technique based on the discrimination of noisy feature maps. By learning to classify noisy views of projected feature maps, and then adapting the model accordingly on new domains, classification performance can be recovered by an important margin. Experiments on several popular test-time adaptation baselines demonstrate the advantages of our method compared to recent approaches for this task. The code can be found at:https://github.com/GustavoVargasHakim/NCTTT.git</li>
</ul>

<h3>Title: Mitigating Challenges of the Space Environment for Onboard Artificial  Intelligence: Design Overview of the Imaging Payload on SpIRIT</h3>
<ul>
<li><strong>Authors: </strong>Miguel Ortiz del Castillo, Jonathan Morgan, Jack McRobbie, Clint Therakam, Zaher Joukhadar, Robert Mearns, Simon Barraclough, Richard Sinnott, Andrew Woods, Chris Bayliss, Kris Ehinger, Ben Rubinstein, James Bailey, Airlie Chapman, Michele Trenti</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08399">https://arxiv.org/abs/2404.08399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08399">https://arxiv.org/pdf/2404.08399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08399]] Mitigating Challenges of the Space Environment for Onboard Artificial  Intelligence: Design Overview of the Imaging Payload on SpIRIT(https://arxiv.org/abs/2404.08399)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Artificial intelligence (AI) and autonomous edge computing in space are emerging areas of interest to augment capabilities of nanosatellites, where modern sensors generate orders of magnitude more data than can typically be transmitted to mission control. Here, we present the hardware and software design of an onboard AI subsystem hosted on SpIRIT. The system is optimised for on-board computer vision experiments based on visible light and long wave infrared cameras. This paper highlights the key design choices made to maximise the robustness of the system in harsh space conditions, and their motivation relative to key mission requirements, such as limited compute resources, resilience to cosmic radiation, extreme temperature variations, distribution shifts, and very low transmission bandwidths. The payload, called Loris, consists of six visible light cameras, three infrared cameras, a camera control board and a Graphics Processing Unit (GPU) system-on-module. Loris enables the execution of AI models with on-orbit fine-tuning as well as a next-generation image compression algorithm, including progressive coding. This innovative approach not only enhances the data processing capabilities of nanosatellites but also lays the groundwork for broader applications to remote sensing from space.</li>
</ul>

<h3>Title: Learning representations of learning representations</h3>
<ul>
<li><strong>Authors: </strong>Rita Gonzlez-Mrquez, Dmitry Kobak</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.DL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08403">https://arxiv.org/abs/2404.08403</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08403">https://arxiv.org/pdf/2404.08403</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08403]] Learning representations of learning representations(https://arxiv.org/abs/2404.08403)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The ICLR conference is unique among the top machine learning conferences in that all submitted papers are openly available. Here we present the ICLR dataset consisting of abstracts of all 24 thousand ICLR submissions from 2017-2024 with meta-data, decision scores, and custom keyword-based labels. We find that on this dataset, bag-of-words representation outperforms most dedicated sentence transformer models in terms of $k$NN classification accuracy, and the top performing language models barely outperform TF-IDF. We see this as a challenge for the NLP community. Furthermore, we use the ICLR dataset to study how the field of machine learning has changed over the last seven years, finding some improvement in gender balance. Using a 2D embedding of the abstracts' texts, we describe a shift in research topics from 2017 to 2024 and identify hedgehogs and foxes among the authors with the highest number of ICLR submissions.</li>
</ul>

<h3>Title: MambaDFuse: A Mamba-based Dual-phase Model for Multi-modality Image  Fusion</h3>
<ul>
<li><strong>Authors: </strong>Zhe Li, Haiwei Pan, Kejia Zhang, Yuhua Wang, Fengming Yu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08406">https://arxiv.org/abs/2404.08406</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08406">https://arxiv.org/pdf/2404.08406</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08406]] MambaDFuse: A Mamba-based Dual-phase Model for Multi-modality Image  Fusion(https://arxiv.org/abs/2404.08406)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer</a></li>
<li><strong>Abstract: </strong>Multi-modality image fusion (MMIF) aims to integrate complementary information from different modalities into a single fused image to represent the imaging scene and facilitate downstream visual tasks comprehensively. In recent years, significant progress has been made in MMIF tasks due to advances in deep neural networks. However, existing methods cannot effectively and efficiently extract modality-specific and modality-fused features constrained by the inherent local reductive bias (CNN) or quadratic computational complexity (Transformers). To overcome this issue, we propose a Mamba-based Dual-phase Fusion (MambaDFuse) model. Firstly, a dual-level feature extractor is designed to capture long-range features from single-modality images by extracting low and high-level features from CNN and Mamba blocks. Then, a dual-phase feature fusion module is proposed to obtain fusion features that combine complementary information from different modalities. It uses the channel exchange method for shallow fusion and the enhanced Multi-modal Mamba (M3) blocks for deep fusion. Finally, the fused image reconstruction module utilizes the inverse transformation of the feature extraction to generate the fused result. Through extensive experiments, our approach achieves promising fusion results in infrared-visible image fusion and medical image fusion. Additionally, in a unified benchmark, MambaDFuse has also demonstrated improved performance in downstream tasks such as object detection. Code with checkpoints will be available after the peer-review process.</li>
</ul>

<h3>Title: Seismic First Break Picking in a Higher Dimension Using Deep Graph  Learning</h3>
<ul>
<li><strong>Authors: </strong>Hongtao Wang, Li Long, Jiangshe Zhang, Xiaoli Wei, Chunxia Zhang, Zhenbo Guo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP, physics.geo-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08408">https://arxiv.org/abs/2404.08408</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08408">https://arxiv.org/pdf/2404.08408</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08408]] Seismic First Break Picking in a Higher Dimension Using Deep Graph  Learning(https://arxiv.org/abs/2404.08408)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Contemporary automatic first break (FB) picking methods typically analyze 1D signals, 2D source gathers, or 3D source-receiver gathers. Utilizing higher-dimensional data, such as 2D or 3D, incorporates global features, improving the stability of local picking. Despite the benefits, high-dimensional data requires structured input and increases computational demands. Addressing this, we propose a novel approach using deep graph learning called DGL-FB, constructing a large graph to efficiently extract information. In this graph, each seismic trace is represented as a node, connected by edges that reflect similarities. To manage the size of the graph, we develop a subgraph sampling technique to streamline model training and inference. Our proposed framework, DGL-FB, leverages deep graph learning for FB picking. It encodes subgraphs into global features using a deep graph encoder. Subsequently, the encoded global features are combined with local node signals and fed into a ResUNet-based 1D segmentation network for FB detection. Field survey evaluations of DGL-FB show superior accuracy and stability compared to a 2D U-Net-based benchmark method.</li>
</ul>

<h3>Title: AdapterSwap: Continuous Training of LLMs with Data Removal and  Access-Control Guarantees</h3>
<ul>
<li><strong>Authors: </strong>William Fleshman, Aleem Khan, Marc Marone, Benjamin Van Durme</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08417">https://arxiv.org/abs/2404.08417</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08417">https://arxiv.org/pdf/2404.08417</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08417]] AdapterSwap: Continuous Training of LLMs with Data Removal and  Access-Control Guarantees(https://arxiv.org/abs/2404.08417)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly capable of completing knowledge intensive tasks by recalling information from a static pretraining corpus. Here we are concerned with LLMs in the context of evolving data requirements. For instance: batches of new data that are introduced periodically; subsets of data with user-based access controls; or requirements on dynamic removal of documents with guarantees that associated knowledge cannot be recalled. We wish to satisfy these requirements while at the same time ensuring a model does not forget old information when new data becomes available. To address these issues, we introduce AdapterSwap, a training and inference scheme that organizes knowledge from a data collection into a set of low-rank adapters, which are dynamically composed during inference. Our experiments demonstrate AdapterSwap's ability to support efficient continual learning, while also enabling organizations to have fine-grained control over data access and deletion.</li>
</ul>

<h3>Title: Adapting the Segment Anything Model During Usage in Novel Situations</h3>
<ul>
<li><strong>Authors: </strong>Robin Schn, Julian Lorenz, Katja Ludwig, Rainer Lienhart</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08421">https://arxiv.org/abs/2404.08421</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08421">https://arxiv.org/pdf/2404.08421</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08421]] Adapting the Segment Anything Model During Usage in Novel Situations(https://arxiv.org/abs/2404.08421)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The interactive segmentation task consists in the creation of object segmentation masks based on user interactions. The most common way to guide a model towards producing a correct segmentation consists in clicks on the object and background. The recently published Segment Anything Model (SAM) supports a generalized version of the interactive segmentation problem and has been trained on an object segmentation dataset which contains 1.1B masks. Though being trained extensively and with the explicit purpose of serving as a foundation model, we show significant limitations of SAM when being applied for interactive segmentation on novel domains or object types. On the used datasets, SAM displays a failure rate $\text{FR}_{30}@90$ of up to $72.6 \%$. Since we still want such foundation models to be immediately applicable, we present a framework that can adapt SAM during immediate usage. For this we will leverage the user interactions and masks, which are constructed during the interactive segmentation process. We use this information to generate pseudo-labels, which we use to compute a loss function and optimize a part of the SAM model. The presented method causes a relative reduction of up to $48.1 \%$ in the $\text{FR}_{20}@85$ and $46.6 \%$ in the $\text{FR}_{30}@90$ metrics.</li>
</ul>

<h3>Title: MSSTNet: A Multi-Scale Spatio-Temporal CNN-Transformer Network for  Dynamic Facial Expression Recognition</h3>
<ul>
<li><strong>Authors: </strong>Linhuang Wang, Xin Kang, Fei Ding, Satoshi Nakagawa, Fuji Ren</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08433">https://arxiv.org/abs/2404.08433</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08433">https://arxiv.org/pdf/2404.08433</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08433]] MSSTNet: A Multi-Scale Spatio-Temporal CNN-Transformer Network for  Dynamic Facial Expression Recognition(https://arxiv.org/abs/2404.08433)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Unlike typical video action recognition, Dynamic Facial Expression Recognition (DFER) does not involve distinct moving targets but relies on localized changes in facial muscles. Addressing this distinctive attribute, we propose a Multi-Scale Spatio-temporal CNN-Transformer network (MSSTNet). Our approach takes spatial features of different scales extracted by CNN and feeds them into a Multi-scale Embedding Layer (MELayer). The MELayer extracts multi-scale spatial information and encodes these features before sending them into a Temporal Transformer (T-Former). The T-Former simultaneously extracts temporal information while continually integrating multi-scale spatial information. This process culminates in the generation of multi-scale spatio-temporal features that are utilized for the final classification. Our method achieves state-of-the-art results on two in-the-wild datasets. Furthermore, a series of ablation experiments and visualizations provide further validation of our approach's proficiency in leveraging spatio-temporal information within DFER.</li>
</ul>

<h3>Title: An improved tabular data generator with VAE-GMM integration</h3>
<ul>
<li><strong>Authors: </strong>Patricia A. Apellniz, Juan Parras, Santiago Zazo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08434">https://arxiv.org/abs/2404.08434</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08434">https://arxiv.org/pdf/2404.08434</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08434]] An improved tabular data generator with VAE-GMM integration(https://arxiv.org/abs/2404.08434)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>The rising use of machine learning in various fields requires robust methods to create synthetic tabular data. Data should preserve key characteristics while addressing data scarcity challenges. Current approaches based on Generative Adversarial Networks, such as the state-of-the-art CTGAN model, struggle with the complex structures inherent in tabular data. These data often contain both continuous and discrete features with non-Gaussian distributions. Therefore, we propose a novel Variational Autoencoder (VAE)-based model that addresses these limitations. Inspired by the TVAE model, our approach incorporates a Bayesian Gaussian Mixture model (BGM) within the VAE architecture. This avoids the limitations imposed by assuming a strictly Gaussian latent space, allowing for a more accurate representation of the underlying data distribution during data generation. Furthermore, our model offers enhanced flexibility by allowing the use of various differentiable distributions for individual features, making it possible to handle both continuous and discrete data types. We thoroughly validate our model on three real-world datasets with mixed data types, including two medically relevant ones, based on their resemblance and utility. This evaluation demonstrates significant outperformance against CTGAN and TVAE, establishing its potential as a valuable tool for generating synthetic tabular data in various domains, particularly in healthcare.</li>
</ul>

<h3>Title: Anti-Byzantine Attacks Enabled Vehicle Selection for Asynchronous  Federated Learning in Vehicular Edge Computing</h3>
<ul>
<li><strong>Authors: </strong>Cui Zhang, Xiao Xu, Qiong Wu, Pingyi Fan, Qiang Fan, Huiling Zhu, Jiangzhou Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08444">https://arxiv.org/abs/2404.08444</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08444">https://arxiv.org/pdf/2404.08444</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08444]] Anti-Byzantine Attacks Enabled Vehicle Selection for Asynchronous  Federated Learning in Vehicular Edge Computing(https://arxiv.org/abs/2404.08444)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, federate</a></li>
<li><strong>Abstract: </strong>In vehicle edge computing (VEC), asynchronous federated learning (AFL) is used, where the edge receives a local model and updates the global model, effectively reducing the global aggregation latency.Due to different amounts of local data,computing capabilities and locations of the vehicles, renewing the global model with same weight is inappropriate.The above factors will affect the local calculation time and upload time of the local model, and the vehicle may also be affected by Byzantine attacks, leading to the deterioration of the vehicle data. However, based on deep reinforcement learning (DRL), we can consider these factors comprehensively to eliminate vehicles with poor performance as much as possible and exclude vehicles that have suffered Byzantine attacks before AFL. At the same time, when aggregating AFL, we can focus on those vehicles with better performance to improve the accuracy and safety of the system. In this paper, we proposed a vehicle selection scheme based on DRL in VEC. In this scheme, vehicle s mobility, channel conditions with temporal variations, computational resources with temporal variations, different data amount, transmission channel status of vehicles as well as Byzantine attacks were taken into account.Simulation results show that the proposed scheme effectively improves the safety and accuracy of the global model.</li>
</ul>

<h3>Title: Federated Optimization with Doubly Regularized Drift Correction</h3>
<ul>
<li><strong>Authors: </strong>Xiaowen Jiang, Anton Rodomanov, Sebastian U. Stich</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08447">https://arxiv.org/abs/2404.08447</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08447">https://arxiv.org/pdf/2404.08447</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08447]] Federated Optimization with Doubly Regularized Drift Correction(https://arxiv.org/abs/2404.08447)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Federated learning is a distributed optimization paradigm that allows training machine learning models across decentralized devices while keeping the data localized. The standard method, FedAvg, suffers from client drift which can hamper performance and increase communication costs over centralized methods. Previous works proposed various strategies to mitigate drift, yet none have shown uniformly improved communication-computation trade-offs over vanilla gradient descent. In this work, we revisit DANE, an established method in distributed optimization. We show that (i) DANE can achieve the desired communication reduction under Hessian similarity constraints. Furthermore, (ii) we present an extension, DANE+, which supports arbitrary inexact local solvers and has more freedom to choose how to aggregate the local updates. We propose (iii) a novel method, FedRed, which has improved local computational complexity and retains the same communication complexity compared to DANE/DANE+. This is achieved by using doubly regularized drift correction.</li>
</ul>

<h3>Title: Joint Physical-Digital Facial Attack Detection Via Simulating Spoofing  Clues</h3>
<ul>
<li><strong>Authors: </strong>Xianhua He, Dashuang Liang, Song Yang, Zhanlong Hao, Hui Ma, Binjie Mao, Xi Li, Yao Wang, Pengfei Yan, Ajian Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08450">https://arxiv.org/abs/2404.08450</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08450">https://arxiv.org/pdf/2404.08450</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08450]] Joint Physical-Digital Facial Attack Detection Via Simulating Spoofing  Clues(https://arxiv.org/abs/2404.08450)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Face recognition systems are frequently subjected to a variety of physical and digital attacks of different types. Previous methods have achieved satisfactory performance in scenarios that address physical attacks and digital attacks, respectively. However, few methods are considered to integrate a model that simultaneously addresses both physical and digital attacks, implying the necessity to develop and maintain multiple models. To jointly detect physical and digital attacks within a single model, we propose an innovative approach that can adapt to any network architecture. Our approach mainly contains two types of data augmentation, which we call Simulated Physical Spoofing Clues augmentation (SPSC) and Simulated Digital Spoofing Clues augmentation (SDSC). SPSC and SDSC augment live samples into simulated attack samples by simulating spoofing clues of physical and digital attacks, respectively, which significantly improve the capability of the model to detect "unseen" attack types. Extensive experiments show that SPSC and SDSC can achieve state-of-the-art generalization in Protocols 2.1 and 2.2 of the UniAttackData dataset, respectively. Our method won first place in "Unified Physical-Digital Face Attack Detection" of the 5th Face Anti-spoofing Challenge@CVPR2024. Our final submission obtains 3.75% APCER, 0.93% BPCER, and 2.34% ACER, respectively. Our code is available at https://github.com/Xianhua-He/cvpr2024-face-anti-spoofing-challenge.</li>
</ul>

<h3>Title: MoE-FFD: Mixture of Experts for Generalized and Parameter-Efficient Face  Forgery Detection</h3>
<ul>
<li><strong>Authors: </strong>Chenqi Kong, Anwei Luo, Song Xia, Yi Yu, Haoliang Li, Alex C. Kot</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08452">https://arxiv.org/abs/2404.08452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08452">https://arxiv.org/pdf/2404.08452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08452]] MoE-FFD: Mixture of Experts for Generalized and Parameter-Efficient Face  Forgery Detection(https://arxiv.org/abs/2404.08452)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, transformer</a></li>
<li><strong>Abstract: </strong>Deepfakes have recently raised significant trust issues and security concerns among the public. Compared to CNN face forgery detectors, ViT-based methods take advantage of the expressivity of transformers, achieving superior detection performance. However, these approaches still exhibit the following limitations: (1). Fully fine-tuning ViT-based models from ImageNet weights demands substantial computational and storage resources; (2). ViT-based methods struggle to capture local forgery clues, leading to model bias and limited generalizability. To tackle these challenges, this work introduces Mixture-of-Experts modules for Face Forgery Detection (MoE-FFD), a generalized yet parameter-efficient ViT-based approach. MoE-FFD only updates lightweight Low-Rank Adaptation (LoRA) and Adapter layers while keeping the ViT backbone frozen, thereby achieving parameter-efficient training. Moreover, MoE-FFD leverages the expressivity of transformers and local priors of CNNs to simultaneously extract global and local forgery clues. Additionally, novel MoE modules are designed to scale the model's capacity and select optimal forgery experts, further enhancing forgery detection performance. The proposed MoE learning scheme can be seamlessly adapted to various transformer backbones in a plug-and-play manner. Extensive experimental results demonstrate that the proposed method achieves state-of-the-art face forgery detection performance with reduced parameter overhead. The code will be released upon acceptance.</li>
</ul>

<h3>Title: TSLANet: Rethinking Transformers for Time Series Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, Xiaoli Li</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08472">https://arxiv.org/abs/2404.08472</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08472">https://arxiv.org/pdf/2404.08472</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08472]] TSLANet: Rethinking Transformers for Time Series Representation Learning(https://arxiv.org/abs/2404.08472)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Time series data, characterized by its intrinsic long and short-range dependencies, poses a unique challenge across analytical applications. While Transformer-based models excel at capturing long-range dependencies, they face limitations in noise sensitivity, computational efficiency, and overfitting with smaller datasets. In response, we introduce a novel Time Series Lightweight Adaptive Network (TSLANet), as a universal convolutional model for diverse time series tasks. Specifically, we propose an Adaptive Spectral Block, harnessing Fourier analysis to enhance feature representation and to capture both long-term and short-term interactions while mitigating noise via adaptive thresholding. Additionally, we introduce an Interactive Convolution Block and leverage self-supervised learning to refine the capacity of TSLANet for decoding complex temporal patterns and improve its robustness on different datasets. Our comprehensive experiments demonstrate that TSLANet outperforms state-of-the-art models in various tasks spanning classification, forecasting, and anomaly detection, showcasing its resilience and adaptability across a spectrum of noise levels and data sizes. The code is available at \url{https://github.com/emadeldeen24/TSLANet}</li>
</ul>

<h3>Title: New Efficient Visual OILU Markers</h3>
<ul>
<li><strong>Authors: </strong>Youssef Chahir, Messaoud Mostefai, Hamza Saida</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08477">https://arxiv.org/abs/2404.08477</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08477">https://arxiv.org/pdf/2404.08477</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08477]] New Efficient Visual OILU Markers(https://arxiv.org/abs/2404.08477)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Basic patterns are the source of a wide range of more or less complex geometric structures. We will exploit such patterns to develop new efficient visual markers. Besides being projective invariants, the proposed markers allow producing rich panel of unique identifiers, highly required for resource-intensive navigation and augmented reality applications. The spiral topology of our markers permits the validation of an accurate identification scheme, which is based on level set methods. The robustness of the markers against acquisition and geometric distortions is validated by extensive experimental tests.</li>
</ul>

<h3>Title: Decoding AI: The inside story of data analysis in ChatGPT</h3>
<ul>
<li><strong>Authors: </strong>Ozan Evkaya, Miguel de Carvalho</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, stat.CO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08480">https://arxiv.org/abs/2404.08480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08480">https://arxiv.org/pdf/2404.08480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08480]] Decoding AI: The inside story of data analysis in ChatGPT(https://arxiv.org/abs/2404.08480)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>As a result of recent advancements in generative AI, the field of Data Science is prone to various changes. This review critically examines the Data Analysis (DA) capabilities of ChatGPT assessing its performance across a wide range of tasks. While DA provides researchers and practitioners with unprecedented analytical capabilities, it is far from being perfect, and it is important to recognize and address its limitations.</li>
</ul>

<h3>Title: Thematic Analysis with Large Language Models: does it work with  languages other than English? A targeted test in Italian</h3>
<ul>
<li><strong>Authors: </strong>Stefano De Paoli</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08488">https://arxiv.org/abs/2404.08488</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08488">https://arxiv.org/pdf/2404.08488</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08488]] Thematic Analysis with Large Language Models: does it work with  languages other than English? A targeted test in Italian(https://arxiv.org/abs/2404.08488)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper proposes a test to perform Thematic Analysis (TA) with Large Language Model (LLM) on data which is in a different language than English. While there has been initial promising work on using pre-trained LLMs for TA on data in English, we lack any tests on whether these models can reasonably perform the same analysis with good quality in other language. In this paper a test will be proposed using an open access dataset of semi-structured interviews in Italian. The test shows that a pre-trained model can perform such a TA on the data, also using prompts in Italian. A comparative test shows the model capacity to produce themes which have a good resemblance with those produced independently by human researchers. The main implication of this study is that pre-trained LLMs may thus be suitable to support analysis in multilingual situations, so long as the language is supported by the model used.</li>
</ul>

<h3>Title: SpectralMamba: Efficient Mamba for Hyperspectral Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Jing Yao, Danfeng Hong, Chenyu Li, Jocelyn Chanussot</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08489">https://arxiv.org/abs/2404.08489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08489">https://arxiv.org/pdf/2404.08489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08489]] SpectralMamba: Efficient Mamba for Hyperspectral Image Classification(https://arxiv.org/abs/2404.08489)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Recurrent neural networks and Transformers have recently dominated most applications in hyperspectral (HS) imaging, owing to their capability to capture long-range dependencies from spectrum sequences. However, despite the success of these sequential architectures, the non-ignorable inefficiency caused by either difficulty in parallelization or computationally prohibitive attention still hinders their practicality, especially for large-scale observation in remote sensing scenarios. To address this issue, we herein propose SpectralMamba -- a novel state space model incorporated efficient deep learning framework for HS image classification. SpectralMamba features the simplified but adequate modeling of HS data dynamics at two levels. First, in spatial-spectral space, a dynamical mask is learned by efficient convolutions to simultaneously encode spatial regularity and spectral peculiarity, thus attenuating the spectral variability and confusion in discriminative representation learning. Second, the merged spectrum can then be efficiently operated in the hidden state space with all parameters learned input-dependent, yielding selectively focused responses without reliance on redundant attention or imparallelizable recurrence. To explore the room for further computational downsizing, a piece-wise scanning mechanism is employed in-between, transferring approximately continuous spectrum into sequences with squeezed length while maintaining short- and long-term contextual profiles among hundreds of bands. Through extensive experiments on four benchmark HS datasets acquired by satellite-, aircraft-, and UAV-borne imagers, SpectralMamba surprisingly creates promising win-wins from both performance and efficiency perspectives.</li>
</ul>

<h3>Title: Dataset Reset Policy Optimization for RLHF</h3>
<ul>
<li><strong>Authors: </strong>Jonathan D. Chang, Wenhao Shan, Owen Oertell, Kiant Brantley, Dipendra Misra, Jason D. Lee, Wen Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08495">https://arxiv.org/abs/2404.08495</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08495">https://arxiv.org/pdf/2404.08495</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08495]] Dataset Reset Policy Optimization for RLHF(https://arxiv.org/abs/2404.08495)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning (RL) from Human Preference-based feedback is a popular paradigm for fine-tuning generative models, which has produced impressive models such as GPT-4 and Claude3 Opus. This framework often consists of two steps: learning a reward model from an offline preference dataset followed by running online RL to optimize the learned reward model. In this work, leveraging the idea of reset, we propose a new RLHF algorithm with provable guarantees. Motivated by the fact that offline preference dataset provides informative states (i.e., data that is preferred by the labelers), our new algorithm, Dataset Reset Policy Optimization (DR-PO), integrates the existing offline preference dataset into the online policy training procedure via dataset reset: it directly resets the policy optimizer to the states in the offline dataset, instead of always starting from the initial state distribution. In theory, we show that DR-PO learns to perform at least as good as any policy that is covered by the offline dataset under general function approximation with finite sample complexity. In experiments, we demonstrate that on both the TL;DR summarization and the Anthropic Helpful Harmful (HH) dataset, the generation from DR-PO is better than that from Proximal Policy Optimization (PPO) and Direction Preference Optimization (DPO), under the metric of GPT4 win-rate. Code for this work can be found at https://github.com/Cornell-RL/drpo.</li>
</ul>

<h3>Title: 3D Human Scan With A Moving Event Camera</h3>
<ul>
<li><strong>Authors: </strong>Kai Kohyama, Shintaro Shiba, Yoshimitsu Aoki</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08504">https://arxiv.org/abs/2404.08504</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08504">https://arxiv.org/pdf/2404.08504</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08504]] 3D Human Scan With A Moving Event Camera(https://arxiv.org/abs/2404.08504)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Capturing the 3D human body is one of the important tasks in computer vision with a wide range of applications such as virtual reality and sports analysis. However, conventional frame cameras are limited by their temporal resolution and dynamic range, which imposes constraints in real-world application setups. Event cameras have the advantages of high temporal resolution and high dynamic range (HDR), but the development of event-based methods is necessary to handle data with different characteristics. This paper proposes a novel event-based method for 3D pose estimation and human mesh recovery. Prior work on event-based human mesh recovery require frames (images) as well as event data. The proposed method solely relies on events; it carves 3D voxels by moving the event camera around a stationary body, reconstructs the human pose and mesh by attenuated rays, and fit statistical body models, preserving high-frequency details. The experimental results show that the proposed method outperforms conventional frame-based methods in the estimation accuracy of both pose and body mesh. We also demonstrate results in challenging situations where a conventional camera has motion blur. This is the first to demonstrate event-only human mesh recovery, and we hope that it is the first step toward achieving robust and accurate 3D human body scanning from vision sensors.</li>
</ul>

<h3>Title: LaSagnA: Language-based Segmentation Assistant for Complex Queries</h3>
<ul>
<li><strong>Authors: </strong>Cong Wei, Haoxian Tan, Yujie Zhong, Yujiu Yang, Lin Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08506">https://arxiv.org/abs/2404.08506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08506">https://arxiv.org/pdf/2404.08506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08506]] LaSagnA: Language-based Segmentation Assistant for Complex Queries(https://arxiv.org/abs/2404.08506)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Recent advancements have empowered Large Language Models for Vision (vLLMs) to generate detailed perceptual outcomes, including bounding boxes and masks. Nonetheless, there are two constraints that restrict the further application of these vLLMs: the incapability of handling multiple targets per query and the failure to identify the absence of query objects in the image. In this study, we acknowledge that the main cause of these problems is the insufficient complexity of training queries. Consequently, we define the general sequence format for complex queries. Then we incorporate a semantic segmentation task in the current pipeline to fulfill the requirements of training data. Furthermore, we present three novel strategies to effectively handle the challenges arising from the direct integration of the proposed format. The effectiveness of our model in processing complex queries is validated by the comparable results with conventional methods on both close-set and open-set semantic segmentation datasets. Additionally, we outperform a series of vLLMs in reasoning and referring segmentation, showcasing our model's remarkable capabilities. We release the code at https://github.com/congvvc/LaSagnA.</li>
</ul>

<h3>Title: Masked Image Modeling as a Framework for Self-Supervised Learning across  Eye Movements</h3>
<ul>
<li><strong>Authors: </strong>Robin Weiler, Matthias Brucklacher, Cyriel M. A. Pennartz, Sander M. Boht</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08526">https://arxiv.org/abs/2404.08526</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08526">https://arxiv.org/pdf/2404.08526</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08526]] Masked Image Modeling as a Framework for Self-Supervised Learning across  Eye Movements(https://arxiv.org/abs/2404.08526)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>To make sense of their surroundings, intelligent systems must transform complex sensory inputs to structured codes that are reduced to task-relevant information such as object category. Biological agents achieve this in a largely autonomous manner, presumably via self-\allowbreak super-\allowbreak vised learning. Whereas previous attempts to model the underlying mechanisms were largely discriminative in nature, there is ample evidence that the brain employs a generative model of the world. Here, we propose that eye movements, in combination with the focused nature of primate vision, constitute a generative, self-supervised task of predicting and revealing visual information. We construct a proof-of-principle model starting from the framework of masked image modeling (MIM), a common approach in deep representation learning. To do so, we analyze how core components of MIM such as masking technique and data augmentation influence the formation of category-specific representations. This allows us not only to better understand the principles behind MIM, but to then reassemble a MIM more in line with the focused nature of biological perception. From a theoretical angle, we find that MIM disentangles neurons in latent space, a property that has been suggested to structure visual representations in primates, without explicit regulation. Together with previous findings of invariance learning, this highlights an interesting connection of MIM to latent regularization approaches for self-supervised learning. The source code is available under https://github.com/RobinWeiler/FocusMIM</li>
</ul>

<h3>Title: VertAttack: Taking advantage of Text Classifiers' horizontal vision</h3>
<ul>
<li><strong>Authors: </strong>Jonathan Rusert</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08538">https://arxiv.org/abs/2404.08538</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08538">https://arxiv.org/pdf/2404.08538</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08538]] VertAttack: Taking advantage of Text Classifiers' horizontal vision(https://arxiv.org/abs/2404.08538)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, transformer</a></li>
<li><strong>Abstract: </strong>Text classification systems have continuously improved in performance over the years. However, nearly all current SOTA classifiers have a similar shortcoming, they process text in a horizontal manner. Vertically written words will not be recognized by a classifier. In contrast, humans are easily able to recognize and read words written both horizontally and vertically. Hence, a human adversary could write problematic words vertically and the meaning would still be preserved to other humans. We simulate such an attack, VertAttack. VertAttack identifies which words a classifier is reliant on and then rewrites those words vertically. We find that VertAttack is able to greatly drop the accuracy of 4 different transformer models on 5 datasets. For example, on the SST2 dataset, VertAttack is able to drop RoBERTa's accuracy from 94 to 13%. Furthermore, since VertAttack does not replace the word, meaning is easily preserved. We verify this via a human study and find that crowdworkers are able to correctly label 77% perturbed texts perturbed, compared to 81% of the original texts. We believe VertAttack offers a look into how humans might circumvent classifiers in the future and thus inspire a look into more robust algorithms.</li>
</ul>

<h3>Title: On the Robustness of Language Guidance for Low-Level Vision Tasks:  Findings from Depth Estimation</h3>
<ul>
<li><strong>Authors: </strong>Agneet Chatterjee, Tejas Gokhale, Chitta Baral, Yezhou Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08540">https://arxiv.org/abs/2404.08540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08540">https://arxiv.org/pdf/2404.08540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08540]] On the Robustness of Language Guidance for Low-Level Vision Tasks:  Findings from Depth Estimation(https://arxiv.org/abs/2404.08540)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Recent advances in monocular depth estimation have been made by incorporating natural language as additional guidance. Although yielding impressive results, the impact of the language prior, particularly in terms of generalization and robustness, remains unexplored. In this paper, we address this gap by quantifying the impact of this prior and introduce methods to benchmark its effectiveness across various settings. We generate "low-level" sentences that convey object-centric, three-dimensional spatial relationships, incorporate them as additional language priors and evaluate their downstream impact on depth estimation. Our key finding is that current language-guided depth estimators perform optimally only with scene-level descriptions and counter-intuitively fare worse with low level descriptions. Despite leveraging additional data, these methods are not robust to directed adversarial attacks and decline in performance with an increase in distribution shift. Finally, to provide a foundation for future research, we identify points of failures and offer insights to better understand these shortcomings. With an increasing number of methods using language for depth estimation, our findings highlight the opportunities and pitfalls that require careful consideration for effective deployment in real-world settings</li>
</ul>

<h3>Title: Analyzing Decades-Long Environmental Changes in Namibia Using Archival  Aerial Photography and Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Girmaw Abebe Tadesse, Caleb Robinson, Gilles Quentin Hacheme, Akram Zaytar, Rahul Dodhia, Tsering Wangyal Shawa, Juan M. Lavista Ferres, Emmanuel H. Kreike</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08544">https://arxiv.org/abs/2404.08544</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08544">https://arxiv.org/pdf/2404.08544</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08544]] Analyzing Decades-Long Environmental Changes in Namibia Using Archival  Aerial Photography and Deep Learning(https://arxiv.org/abs/2404.08544)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This study explores object detection in historical aerial photographs of Namibia to identify long-term environmental changes. Specifically, we aim to identify key objects -- \textit{Waterholes}, \textit{Omuti homesteads}, and \textit{Big trees} -- around Oshikango in Namibia using sub-meter gray-scale aerial imagery from 1943 and 1972. In this work, we propose a workflow for analyzing historical aerial imagery using a deep semantic segmentation model on sparse hand-labels. To this end, we employ a number of strategies including class-weighting, pseudo-labeling and empirical p-value-based filtering to balance skewed and sparse representations of objects in the ground truth data. Results demonstrate the benefits of these different training strategies resulting in an average $F_1=0.661$ and $F_1=0.755$ over the three objects of interest for the 1943 and 1972 imagery, respectively. We also identified that the average size of Waterhole and Big trees increased while the average size of Omutis decreased between 1943 and 1972 reflecting some of the local effects of the massive post-Second World War economic, agricultural, demographic, and environmental changes. This work also highlights the untapped potential of historical aerial photographs in understanding long-term environmental changes beyond Namibia (and Africa). With the lack of adequate satellite technology in the past, archival aerial photography offers a great alternative to uncover decades-long environmental changes.</li>
</ul>

<h3>Title: RLHF Deciphered: A Critical Analysis of Reinforcement Learning from  Human Feedback for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Shreyas Chaudhari, Pranjal Aggarwal, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, Karthik Narasimhan, Ameet Deshpande, Bruno Castro da Silva</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08555">https://arxiv.org/abs/2404.08555</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08555">https://arxiv.org/pdf/2404.08555</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08555]] RLHF Deciphered: A Critical Analysis of Reinforcement Learning from  Human Feedback for LLMs(https://arxiv.org/abs/2404.08555)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>State-of-the-art large language models (LLMs) have become indispensable tools for various tasks. However, training LLMs to serve as effective assistants for humans requires careful consideration. A promising approach is reinforcement learning from human feedback (RLHF), which leverages human feedback to update the model in accordance with human preferences and mitigate issues like toxicity and hallucinations. Yet, an understanding of RLHF for LLMs is largely entangled with initial design choices that popularized the method and current research focuses on augmenting those choices rather than fundamentally improving the framework. In this paper, we analyze RLHF through the lens of reinforcement learning principles to develop an understanding of its fundamentals, dedicating substantial focus to the core component of RLHF -- the reward model. Our study investigates modeling choices, caveats of function approximation, and their implications on RLHF training algorithms, highlighting the underlying assumptions made about the expressivity of reward. Our analysis improves the understanding of the role of reward models and methods for their training, concurrently revealing limitations of the current methodology. We characterize these limitations, including incorrect generalization, model misspecification, and the sparsity of feedback, along with their impact on the performance of a language model. The discussion and analysis are substantiated by a categorical review of current literature, serving as a reference for researchers and practitioners to understand the challenges of RLHF and build upon existing efforts.</li>
</ul>

<h3>Title: Scalability in Building Component Data Annotation: Enhancing Facade  Material Classification with Synthetic Data</h3>
<ul>
<li><strong>Authors: </strong>Josie Harrison, Alexander Hollberg, Yinan Yu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08557">https://arxiv.org/abs/2404.08557</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08557">https://arxiv.org/pdf/2404.08557</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08557]] Scalability in Building Component Data Annotation: Enhancing Facade  Material Classification with Synthetic Data(https://arxiv.org/abs/2404.08557)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Computer vision models trained on Google Street View images can create material cadastres. However, current approaches need manually annotated datasets that are difficult to obtain and often have class imbalance. To address these challenges, this paper fine-tuned a Swin Transformer model on a synthetic dataset generated with DALL-E and compared the performance to a similar manually annotated dataset. Although manual annotation remains the gold standard, the synthetic dataset performance demonstrates a reasonable alternative. The findings will ease annotation needed to develop material cadastres, offering architects insights into opportunities for material reuse, thus contributing to the reduction of demolition waste.</li>
</ul>

<h3>Title: Dynamic Neural Control Flow Execution: An Agent-Based Deep Equilibrium  Approach for Binary Vulnerability Detection</h3>
<ul>
<li><strong>Authors: </strong>Litao Li, Steven H. H. Ding, Andrew Walenstein, Philippe Charland, Benjamin C. M. Fung</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08562">https://arxiv.org/abs/2404.08562</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08562">https://arxiv.org/pdf/2404.08562</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08562]] Dynamic Neural Control Flow Execution: An Agent-Based Deep Equilibrium  Approach for Binary Vulnerability Detection(https://arxiv.org/abs/2404.08562)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Software vulnerabilities are a challenge in cybersecurity. Manual security patches are often difficult and slow to be deployed, while new vulnerabilities are created. Binary code vulnerability detection is less studied and more complex compared to source code, and this has important practical implications. Deep learning has become an efficient and powerful tool in the security domain, where it provides end-to-end and accurate prediction. Modern deep learning approaches learn the program semantics through sequence and graph neural networks, using various intermediate representation of programs, such as abstract syntax trees (AST) or control flow graphs (CFG). Due to the complex nature of program execution, the output of an execution depends on the many program states and inputs. Also, a CFG generated from static analysis can be an overestimation of the true program flow. Moreover, the size of programs often does not allow a graph neural network with fixed layers to aggregate global information. To address these issues, we propose DeepEXE, an agent-based implicit neural network that mimics the execution path of a program. We use reinforcement learning to enhance the branching decision at every program state transition and create a dynamic environment to learn the dependency between a vulnerability and certain program states. An implicitly defined neural network enables nearly infinite state transitions until convergence, which captures the structural information at a higher level. The experiments are conducted on two semi-synthetic and two real-world datasets. We show that DeepEXE is an accurate and efficient method and outperforms the state-of-the-art vulnerability detection methods.</li>
</ul>

<h3>Title: Federated Distillation: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Lin Li, Jianping Gou, Baosheng Yu, Lan Du, Zhang Yiand Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08564">https://arxiv.org/abs/2404.08564</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08564">https://arxiv.org/pdf/2404.08564</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08564]] Federated Distillation: A Survey(https://arxiv.org/abs/2404.08564)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) seeks to train a model collaboratively without sharing private training data from individual clients. Despite its promise, FL encounters challenges such as high communication costs for large-scale models and the necessity for uniform model architectures across all clients and the server. These challenges severely restrict the practical applications of FL. To address these limitations, the integration of knowledge distillation (KD) into FL has been proposed, forming what is known as Federated Distillation (FD). FD enables more flexible knowledge transfer between clients and the server, surpassing the mere sharing of model parameters. By eliminating the need for identical model architectures across clients and the server, FD mitigates the communication costs associated with training large-scale models. This paper aims to offer a comprehensive overview of FD, highlighting its latest advancements. It delves into the fundamental principles underlying the design of FD frameworks, delineates FD approaches for tackling various challenges, and provides insights into the diverse applications of FD across different scenarios.</li>
</ul>

<h3>Title: Small Models Are (Still) Effective Cross-Domain Argument Extractors</h3>
<ul>
<li><strong>Authors: </strong>William Gantt, Aaron Steven White</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08579">https://arxiv.org/abs/2404.08579</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08579">https://arxiv.org/pdf/2404.08579</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08579]] Small Models Are (Still) Effective Cross-Domain Argument Extractors(https://arxiv.org/abs/2404.08579)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Effective ontology transfer has been a major goal of recent work on event argument extraction (EAE). Two methods in particular -- question answering (QA) and template infilling (TI) -- have emerged as promising approaches to this problem. However, detailed explorations of these techniques' ability to actually enable this transfer are lacking. In this work, we provide such a study, exploring zero-shot transfer using both techniques on six major EAE datasets at both the sentence and document levels. Further, we challenge the growing reliance on LLMs for zero-shot extraction, showing that vastly smaller models trained on an appropriate source ontology can yield zero-shot performance superior to that of GPT-3.5 or GPT-4.</li>
</ul>

<h3>Title: FashionFail: Addressing Failure Cases in Fashion Object Detection and  Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Riza Velioglu, Robin Chan, Barbara Hammer</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08582">https://arxiv.org/abs/2404.08582</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08582">https://arxiv.org/pdf/2404.08582</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08582]] FashionFail: Addressing Failure Cases in Fashion Object Detection and  Segmentation(https://arxiv.org/abs/2404.08582)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>In the realm of fashion object detection and segmentation for online shopping images, existing state-of-the-art fashion parsing models encounter limitations, particularly when exposed to non-model-worn apparel and close-up shots. To address these failures, we introduce FashionFail; a new fashion dataset with e-commerce images for object detection and segmentation. The dataset is efficiently curated using our novel annotation tool that leverages recent foundation models. The primary objective of FashionFail is to serve as a test bed for evaluating the robustness of models. Our analysis reveals the shortcomings of leading models, such as Attribute-Mask R-CNN and Fashionformer. Additionally, we propose a baseline approach using naive data augmentation to mitigate common failure cases and improve model robustness. Through this work, we aim to inspire and support further research in fashion item detection and segmentation for industrial applications. The dataset, annotation tool, code, and models are available at \url{https://rizavelioglu.github.io/fashionfail/}.</li>
</ul>

<h3>Title: Pathological Primitive Segmentation Based on Visual Foundation Model  with Zero-Shot Mask Generation</h3>
<ul>
<li><strong>Authors: </strong>Abu Bakor Hayat Arnob, Xiangxue Wang, Yiping Jiao, Xiao Gan, Wenlong Ming, Jun Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08584">https://arxiv.org/abs/2404.08584</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08584">https://arxiv.org/pdf/2404.08584</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08584]] Pathological Primitive Segmentation Based on Visual Foundation Model  with Zero-Shot Mask Generation(https://arxiv.org/abs/2404.08584)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Medical image processing usually requires a model trained with carefully crafted datasets due to unique image characteristics and domain-specific challenges, especially in pathology. Primitive detection and segmentation in digitized tissue samples are essential for objective and automated diagnosis and prognosis of cancer. SAM (Segment Anything Model) has recently been developed to segment general objects from natural images with high accuracy, but it requires human prompts to generate masks. In this work, we present a novel approach that adapts pre-trained natural image encoders of SAM for detection-based region proposals. Regions proposed by a pre-trained encoder are sent to cascaded feature propagation layers for projection. Then, local semantic and global context is aggregated from multi-scale for bounding box localization and classification. Finally, the SAM decoder uses the identified bounding boxes as essential prompts to generate a comprehensive primitive segmentation map. The entire base framework, SAM, requires no additional training or fine-tuning but could produce an end-to-end result for two fundamental segmentation tasks in pathology. Our method compares with state-of-the-art models in F1 score for nuclei detection and binary/multiclass panoptic(bPQ/mPQ) and mask quality(dice) for segmentation quality on the PanNuke dataset while offering end-to-end efficiency. Our model also achieves remarkable Average Precision (+4.5%) on the secondary dataset (HuBMAP Kidney) compared to Faster RCNN. The code is publicly available at https://github.com/learner-codec/autoprom_sam.</li>
</ul>

<h3>Title: Advanced wood species identification based on multiple anatomical  sections and using deep feature transfer and fusion</h3>
<ul>
<li><strong>Authors: </strong>Kallil M. Zielinski, Leonardo Scabini, Lucas C. Ribas, Nbia R. da Silva, Hans Beeckman, Jan Verwaeren, Odemir M. Bruno, Bernard De Baets</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08585">https://arxiv.org/abs/2404.08585</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08585">https://arxiv.org/pdf/2404.08585</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08585]] Advanced wood species identification based on multiple anatomical  sections and using deep feature transfer and fusion(https://arxiv.org/abs/2404.08585)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>In recent years, we have seen many advancements in wood species identification. Methods like DNA analysis, Near Infrared (NIR) spectroscopy, and Direct Analysis in Real Time (DART) mass spectrometry complement the long-established wood anatomical assessment of cell and tissue morphology. However, most of these methods have some limitations such as high costs, the need for skilled experts for data interpretation, and the lack of good datasets for professional reference. Therefore, most of these methods, and certainly the wood anatomical assessment, may benefit from tools based on Artificial Intelligence. In this paper, we apply two transfer learning techniques with Convolutional Neural Networks (CNNs) to a multi-view Congolese wood species dataset including sections from different orientations and viewed at different microscopic magnifications. We explore two feature extraction methods in detail, namely Global Average Pooling (GAP) and Random Encoding of Aggregated Deep Activation Maps (RADAM), for efficient and accurate wood species identification. Our results indicate superior accuracy on diverse datasets and anatomical sections, surpassing the results of other methods. Our proposal represents a significant advancement in wood species identification, offering a robust tool to support the conservation of forest ecosystems and promote sustainable forestry practices.</li>
</ul>

<h3>Title: Enhancing Visual Question Answering through Question-Driven Image  Captions as Prompts</h3>
<ul>
<li><strong>Authors: </strong>vg zdemir, Erdem Akagndz</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08589">https://arxiv.org/abs/2404.08589</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08589">https://arxiv.org/pdf/2404.08589</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08589]] Enhancing Visual Question Answering through Question-Driven Image  Captions as Prompts(https://arxiv.org/abs/2404.08589)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Visual question answering (VQA) is known as an AI-complete task as it requires understanding, reasoning, and inferring about the vision and the language content. Over the past few years, numerous neural architectures have been suggested for the VQA problem. However, achieving success in zero-shot VQA remains a challenge due to its requirement for advanced generalization and reasoning skills. This study explores the impact of incorporating image captioning as an intermediary process within the VQA pipeline. Specifically, we explore the efficacy of utilizing image captions instead of images and leveraging large language models (LLMs) to establish a zero-shot setting. Since image captioning is the most crucial step in this process, we compare the impact of state-of-the-art image captioning models on VQA performance across various question types in terms of structure and semantics. We propose a straightforward and efficient question-driven image captioning approach within this pipeline to transfer contextual information into the question-answering (QA) model. This method involves extracting keywords from the question, generating a caption for each image-question pair using the keywords, and incorporating the question-driven caption into the LLM prompt. We evaluate the efficacy of using general-purpose and question-driven image captions in the VQA pipeline. Our study highlights the potential of employing image captions and harnessing the capabilities of LLMs to achieve competitive performance on GQA under the zero-shot setting. Our code is available at \url{https://github.com/ovguyo/captions-in-VQA}.</li>
</ul>

<h3>Title: Improving Referring Image Segmentation using Vision-Aware Text Features</h3>
<ul>
<li><strong>Authors: </strong>Hai Nguyen-Truong, E-Ro Nguyen, Tuan-Anh Vu, Minh-Triet Tran, Binh-Son Hua, Sai-Kit Yeung</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08590">https://arxiv.org/abs/2404.08590</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08590">https://arxiv.org/pdf/2404.08590</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08590]] Improving Referring Image Segmentation using Vision-Aware Text Features(https://arxiv.org/abs/2404.08590)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Referring image segmentation is a challenging task that involves generating pixel-wise segmentation masks based on natural language descriptions. Existing methods have relied mostly on visual features to generate the segmentation masks while treating text features as supporting components. This over-reliance on visual features can lead to suboptimal results, especially in complex scenarios where text prompts are ambiguous or context-dependent. To overcome these challenges, we present a novel framework VATEX to improve referring image segmentation by enhancing object and context understanding with Vision-Aware Text Feature. Our method involves using CLIP to derive a CLIP Prior that integrates an object-centric visual heatmap with text description, which can be used as the initial query in DETR-based architecture for the segmentation task. Furthermore, by observing that there are multiple ways to describe an instance in an image, we enforce feature similarity between text variations referring to the same visual input by two components: a novel Contextual Multimodal Decoder that turns text embeddings into vision-aware text features, and a Meaning Consistency Constraint to ensure further the coherent and consistent interpretation of language expressions with the context understanding obtained from the image. Our method achieves a significant performance improvement on three benchmark datasets RefCOCO, RefCOCO+ and G-Ref. Code is available at: https://nero1342.github.io/VATEX\_RIS.</li>
</ul>

<h3>Title: Generating Synthetic Time Series Data for Cyber-Physical Systems</h3>
<ul>
<li><strong>Authors: </strong>Alexander Sommers, Somayeh Bakhtiari Ramezani, Logan Cummins, Sudip Mittal, Shahram Rahimi, Maria Seale, Joseph Jaboure</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08601">https://arxiv.org/abs/2404.08601</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08601">https://arxiv.org/pdf/2404.08601</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08601]] Generating Synthetic Time Series Data for Cyber-Physical Systems(https://arxiv.org/abs/2404.08601)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Data augmentation is an important facilitator of deep learning applications in the time series domain. A gap is identified in the literature, demonstrating sparse exploration of the transformer, the dominant sequence model, for data augmentation in time series. A architecture hybridizing several successful priors is put forth and tested using a powerful time domain similarity metric. Results suggest the challenge of this domain, and several valuable directions for future work.</li>
</ul>

<h3>Title: Automatic Quantification of Serial PET/CT Images for Pediatric Hodgkin  Lymphoma Patients Using a Longitudinally-Aware Segmentation Network</h3>
<ul>
<li><strong>Authors: </strong>Xin Tie, Muheon Shin, Changhee Lee, Scott B. Perlman, Zachary Huemann, Amy J. Weisman, Sharon M. Castellino, Kara M. Kelly, Kathleen M. McCarten, Adina L. Alazraki, Junjie Hu, Steve Y. Cho, Tyler J. Bradshaw</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, physics.med-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08611">https://arxiv.org/abs/2404.08611</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08611">https://arxiv.org/pdf/2404.08611</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08611]] Automatic Quantification of Serial PET/CT Images for Pediatric Hodgkin  Lymphoma Patients Using a Longitudinally-Aware Segmentation Network(https://arxiv.org/abs/2404.08611)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>$\textbf{Purpose}$: Automatic quantification of longitudinal changes in PET scans for lymphoma patients has proven challenging, as residual disease in interim-therapy scans is often subtle and difficult to detect. Our goal was to develop a longitudinally-aware segmentation network (LAS-Net) that can quantify serial PET/CT images for pediatric Hodgkin lymphoma patients. $\textbf{Materials and Methods}$: This retrospective study included baseline (PET1) and interim (PET2) PET/CT images from 297 patients enrolled in two Children's Oncology Group clinical trials (AHOD1331 and AHOD0831). LAS-Net incorporates longitudinal cross-attention, allowing relevant features from PET1 to inform the analysis of PET2. Model performance was evaluated using Dice coefficients for PET1 and detection F1 scores for PET2. Additionally, we extracted and compared quantitative PET metrics, including metabolic tumor volume (MTV) and total lesion glycolysis (TLG) in PET1, as well as qPET and $\Delta$SUVmax in PET2, against physician measurements. We quantified their agreement using Spearman's $\rho$ correlations and employed bootstrap resampling for statistical analysis. $\textbf{Results}$: LAS-Net detected residual lymphoma in PET2 with an F1 score of 0.606 (precision/recall: 0.615/0.600), outperforming all comparator methods (P<0.01). For baseline segmentation, LAS-Net achieved a mean Dice score of 0.772. In PET quantification, LAS-Net's measurements of qPET, $\Delta$SUVmax, MTV and TLG were strongly correlated with physician measurements, with Spearman's $\rho$ of 0.78, 0.80, 0.93 and 0.96, respectively. The performance remained high, with a slight decrease, in an external testing cohort. $\textbf{Conclusion}$: LAS-Net achieved high performance in quantifying PET metrics across serial scans, highlighting the value of longitudinal awareness in evaluating multi-time-point imaging datasets.</li>
</ul>

<h3>Title: Synthetic Dataset Creation and Fine-Tuning of Transformer Models for  Question Answering in Serbian</h3>
<ul>
<li><strong>Authors: </strong>Aleksa Cvetanovi, Predrag Tadi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08617">https://arxiv.org/abs/2404.08617</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08617">https://arxiv.org/pdf/2404.08617</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08617]] Synthetic Dataset Creation and Fine-Tuning of Transformer Models for  Question Answering in Serbian(https://arxiv.org/abs/2404.08617)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In this paper, we focus on generating a synthetic question answering (QA) dataset using an adapted Translate-Align-Retrieve method. Using this method, we created the largest Serbian QA dataset of more than 87K samples, which we name SQuAD-sr. To acknowledge the script duality in Serbian, we generated both Cyrillic and Latin versions of the dataset. We investigate the dataset quality and use it to fine-tune several pre-trained QA models. Best results were obtained by fine-tuning the BERTi\'c model on our Latin SQuAD-sr dataset, achieving 73.91% Exact Match and 82.97% F1 score on the benchmark XQuAD dataset, which we translated into Serbian for the purpose of evaluation. The results show that our model exceeds zero-shot baselines, but fails to go beyond human performance. We note the advantage of using a monolingual pre-trained model over multilingual, as well as the performance increase gained by using Latin over Cyrillic. By performing additional analysis, we show that questions about numeric values or dates are more likely to be answered correctly than other types of questions. Finally, we conclude that SQuAD-sr is of sufficient quality for fine-tuning a Serbian QA model, in the absence of a manually crafted and annotated dataset.</li>
</ul>

<h3>Title: FCert: Certifiably Robust Few-Shot Classification in the Era of  Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Yanting Wang, Wei Zou, Jinyuan Jia</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08631">https://arxiv.org/abs/2404.08631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08631">https://arxiv.org/pdf/2404.08631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08631]] FCert: Certifiably Robust Few-Shot Classification in the Era of  Foundation Models(https://arxiv.org/abs/2404.08631)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Few-shot classification with foundation models (e.g., CLIP, DINOv2, PaLM-2) enables users to build an accurate classifier with a few labeled training samples (called support samples) for a classification task. However, an attacker could perform data poisoning attacks by manipulating some support samples such that the classifier makes the attacker-desired, arbitrary prediction for a testing input. Empirical defenses cannot provide formal robustness guarantees, leading to a cat-and-mouse game between the attacker and defender. Existing certified defenses are designed for traditional supervised learning, resulting in sub-optimal performance when extended to few-shot classification. In our work, we propose FCert, the first certified defense against data poisoning attacks to few-shot classification. We show our FCert provably predicts the same label for a testing input under arbitrary data poisoning attacks when the total number of poisoned support samples is bounded. We perform extensive experiments on benchmark few-shot classification datasets with foundation models released by OpenAI, Meta, and Google in both vision and text domains. Our experimental results show our FCert: 1) maintains classification accuracy without attacks, 2) outperforms existing state-of-the-art certified defenses for data poisoning attacks, and 3) is efficient and general.</li>
</ul>

<h3>Title: Pre-training Small Base LMs with Fewer Tokens</h3>
<ul>
<li><strong>Authors: </strong>Sunny Sanyal, Sujay Sanghavi, Alexandros G. Dimakis</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08634">https://arxiv.org/abs/2404.08634</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08634">https://arxiv.org/pdf/2404.08634</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08634]] Pre-training Small Base LMs with Fewer Tokens(https://arxiv.org/abs/2404.08634)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We study the effectiveness of a simple approach to develop a small base language model (LM) starting from an existing large base LM: first inherit a few transformer blocks from the larger LM, and then train this smaller model on a very small subset (0.1\%) of the raw pretraining data of the larger model. We call our simple recipe Inheritune and first demonstrate it for building a small base LM with 1.5B parameters using 1B tokens (and a starting few layers of larger LM of 3B parameters); we do this using a single A6000 GPU for less than half a day. Across 9 diverse evaluation datasets as well as the MMLU benchmark, the resulting model compares favorably to publicly available base models of 1B-2B size, some of which have been trained using 50-1000 times more tokens. We investigate Inheritune in a slightly different setting where we train small LMs utilizing larger LMs and their full pre-training dataset. Here we show that smaller LMs trained utilizing some of the layers of GPT2-medium (355M) and GPT-2-large (770M) can effectively match the val loss of their bigger counterparts when trained from scratch for the same number of training steps on OpenWebText dataset with 9B tokens. We analyze our recipe with extensive experiments and demonstrate it efficacy on diverse settings. Our code is available at https://github.com/sanyalsunny111/LLM-Inheritune.</li>
</ul>

<h3>Title: Probing the 3D Awareness of Visual Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Mohamed El Banani, Amit Raj, Kevis-Kokitsi Maninis, Abhishek Kar, Yuanzhen Li, Michael Rubinstein, Deqing Sun, Leonidas Guibas, Justin Johnson, Varun Jampani</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08636">https://arxiv.org/abs/2404.08636</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08636">https://arxiv.org/pdf/2404.08636</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08636]] Probing the 3D Awareness of Visual Foundation Models(https://arxiv.org/abs/2404.08636)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Recent advances in large-scale pretraining have yielded visual foundation models with strong capabilities. Not only can recent models generalize to arbitrary images for their training task, their intermediate representations are useful for other visual tasks such as detection and segmentation. Given that such models can classify, delineate, and localize objects in 2D, we ask whether they also represent their 3D structure? In this work, we analyze the 3D awareness of visual foundation models. We posit that 3D awareness implies that representations (1) encode the 3D structure of the scene and (2) consistently represent the surface across views. We conduct a series of experiments using task-specific probes and zero-shot inference procedures on frozen features. Our experiments reveal several limitations of the current models. Our code and analysis can be found at https://github.com/mbanani/probe3d.</li>
</ul>

<h3>Title: COCONut: Modernizing COCO Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Xueqing Deng, Qihang Yu, Peng Wang, Xiaohui Shen, Liang-Chieh Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08639">https://arxiv.org/abs/2404.08639</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08639">https://arxiv.org/pdf/2404.08639</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08639]] COCONut: Modernizing COCO Segmentation(https://arxiv.org/abs/2404.08639)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>In recent decades, the vision community has witnessed remarkable progress in visual recognition, partially owing to advancements in dataset benchmarks. Notably, the established COCO benchmark has propelled the development of modern detection and segmentation systems. However, the COCO segmentation benchmark has seen comparatively slow improvement over the last decade. Originally equipped with coarse polygon annotations for thing instances, it gradually incorporated coarse superpixel annotations for stuff regions, which were subsequently heuristically amalgamated to yield panoptic segmentation annotations. These annotations, executed by different groups of raters, have resulted not only in coarse segmentation masks but also in inconsistencies between segmentation types. In this study, we undertake a comprehensive reevaluation of the COCO segmentation annotations. By enhancing the annotation quality and expanding the dataset to encompass 383K images with more than 5.18M panoptic masks, we introduce COCONut, the COCO Next Universal segmenTation dataset. COCONut harmonizes segmentation annotations across semantic, instance, and panoptic segmentation with meticulously crafted high-quality masks, and establishes a robust benchmark for all segmentation tasks. To our knowledge, COCONut stands as the inaugural large-scale universal segmentation dataset, verified by human raters. We anticipate that the release of COCONut will significantly contribute to the community's ability to assess the progress of novel neural networks.</li>
</ul>

<h3>Title: EventEgo3D: 3D Human Motion Capture from Egocentric Event Streams</h3>
<ul>
<li><strong>Authors: </strong>Christen Millerdurai, Hiroyasu Akada, Jian Wang, Diogo Luvizon, Christian Theobalt, Vladislav Golyanik</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.08640">https://arxiv.org/abs/2404.08640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.08640">https://arxiv.org/pdf/2404.08640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.08640]] EventEgo3D: 3D Human Motion Capture from Egocentric Event Streams(https://arxiv.org/abs/2404.08640)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Monocular egocentric 3D human motion capture is a challenging and actively researched problem. Existing methods use synchronously operating visual sensors (e.g. RGB cameras) and often fail under low lighting and fast motions, which can be restricting in many applications involving head-mounted devices. In response to the existing limitations, this paper 1) introduces a new problem, i.e., 3D human motion capture from an egocentric monocular event camera with a fisheye lens, and 2) proposes the first approach to it called EventEgo3D (EE3D). Event streams have high temporal resolution and provide reliable cues for 3D human motion capture under high-speed human motions and rapidly changing illumination. The proposed EE3D framework is specifically tailored for learning with event streams in the LNES representation, enabling high 3D reconstruction accuracy. We also design a prototype of a mobile head-mounted device with an event camera and record a real dataset with event observations and the ground-truth 3D human poses (in addition to the synthetic dataset). Our EE3D demonstrates robustness and superior 3D accuracy compared to existing solutions across various challenging experiments while supporting real-time 3D pose update rates of 140Hz.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
