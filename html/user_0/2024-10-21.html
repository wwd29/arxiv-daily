<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-10-21</h1>
<h3>Title: Explaining an image classifier with a generative model conditioned by uncertainty</h3>
<ul>
<li><strong>Authors: </strong>Adrien Le Coz, St√©phane Herbin, Faouzi Adjed</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13871">https://arxiv.org/abs/2410.13871</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13871">https://arxiv.org/pdf/2410.13871</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13871]] Explaining an image classifier with a generative model conditioned by uncertainty(https://arxiv.org/abs/2410.13871)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We propose to condition a generative model by a given image classifier uncertainty in order to analyze and explain its behavior. Preliminary experiments on synthetic data and a corrupted version of MNIST dataset illustrate the idea.</li>
</ul>

<h3>Title: Articulate-Anything: Automatic Modeling of Articulated Objects via a Vision-Language Foundation Model</h3>
<ul>
<li><strong>Authors: </strong>Long Le, Jason Xie, William Liang, Hung-Ju Wang, Yue Yang, Yecheng Jason Ma, Kyle Vedder, Arjun Krishna, Dinesh Jayaraman, Eric Eaton</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13882">https://arxiv.org/abs/2410.13882</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13882">https://arxiv.org/pdf/2410.13882</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13882]] Articulate-Anything: Automatic Modeling of Articulated Objects via a Vision-Language Foundation Model(https://arxiv.org/abs/2410.13882)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Interactive 3D simulated objects are crucial in AR/VR, animations, and robotics, driving immersive experiences and advanced automation. However, creating these articulated objects requires extensive human effort and expertise, limiting their broader applications. To overcome this challenge, we present Articulate-Anything, a system that automates the articulation of diverse, complex objects from many input modalities, including text, images, and videos. Articulate-Anything leverages vision-language models (VLMs) to generate code that can be compiled into an interactable digital twin for use in standard 3D simulators. Our system exploits existing 3D asset datasets via a mesh retrieval mechanism, along with an actor-critic system that iteratively proposes, evaluates, and refines solutions for articulating the objects, self-correcting errors to achieve a robust outcome. Qualitative evaluations demonstrate Articulate-Anything's capability to articulate complex and even ambiguous object affordances by leveraging rich grounded inputs. In extensive quantitative experiments on the standard PartNet-Mobility dataset, Articulate-Anything substantially outperforms prior work, increasing the success rate from 8.7-11.6% to 75% and setting a new bar for state-of-the-art performance. We further showcase the utility of our generated assets by using them to train robotic policies for fine-grained manipulation tasks that go beyond basic pick and place.</li>
</ul>

<h3>Title: Transformers Utilization in Chart Understanding: A Review of Recent Advances & Future Trends</h3>
<ul>
<li><strong>Authors: </strong>Mirna Al-Shetairy, Hanan Hindy, Dina Khattab, Mostafa M. Aref</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.HC, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13883">https://arxiv.org/abs/2410.13883</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13883">https://arxiv.org/pdf/2410.13883</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13883]] Transformers Utilization in Chart Understanding: A Review of Recent Advances & Future Trends(https://arxiv.org/abs/2410.13883)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>In recent years, interest in vision-language tasks has grown, especially those involving chart interactions. These tasks are inherently multimodal, requiring models to process chart images, accompanying text, underlying data tables, and often user queries. Traditionally, Chart Understanding (CU) relied on heuristics and rule-based systems. However, recent advancements that have integrated transformer architectures significantly improved performance. This paper reviews prominent research in CU, focusing on State-of-The-Art (SoTA) frameworks that employ transformers within End-to-End (E2E) solutions. Relevant benchmarking datasets and evaluation techniques are analyzed. Additionally, this article identifies key challenges and outlines promising future directions for advancing CU solutions. Following the PRISMA guidelines, a comprehensive literature search is conducted across Google Scholar, focusing on publications from Jan'20 to Jun'24. After rigorous screening and quality assessment, 32 studies are selected for in-depth analysis. The CU tasks are categorized into a three-layered paradigm based on the cognitive task required. Recent advancements in the frameworks addressing various CU tasks are also reviewed. Frameworks are categorized into single-task or multi-task based on the number of tasks solvable by the E2E solution. Within multi-task frameworks, pre-trained and prompt-engineering-based techniques are explored. This review overviews leading architectures, datasets, and pre-training tasks. Despite significant progress, challenges remain in OCR dependency, handling low-resolution images, and enhancing visual reasoning. Future directions include addressing these challenges, developing robust benchmarks, and optimizing model efficiency. Additionally, integrating explainable AI techniques and exploring the balance between real and synthetic data are crucial for advancing CU research.</li>
</ul>

<h3>Title: Refusal-Trained LLMs Are Easily Jailbroken As Browser Agents</h3>
<ul>
<li><strong>Authors: </strong>Priyanshu Kumar, Elaine Lau, Saranya Vijayakumar, Tu Trinh, Scale Red Team, Elaine Chang, Vaughn Robinson, Sean Hendryx, Shuyan Zhou, Matt Fredrikson, Summer Yue, Zifan Wang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13886">https://arxiv.org/abs/2410.13886</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13886">https://arxiv.org/pdf/2410.13886</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13886]] Refusal-Trained LLMs Are Easily Jailbroken As Browser Agents(https://arxiv.org/abs/2410.13886)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>For safety reasons, large language models (LLMs) are trained to refuse harmful user instructions, such as assisting dangerous activities. We study an open question in this work: does the desired safety refusal, typically enforced in chat contexts, generalize to non-chat and agentic use cases? Unlike chatbots, LLM agents equipped with general-purpose tools, such as web browsers and mobile devices, can directly influence the real world, making it even more crucial to refuse harmful instructions. In this work, we primarily focus on red-teaming browser agents, LLMs that manipulate information via web browsers. To this end, we introduce Browser Agent Red teaming Toolkit (BrowserART), a comprehensive test suite designed specifically for red-teaming browser agents. BrowserART is consist of 100 diverse browser-related harmful behaviors (including original behaviors and ones sourced from HarmBench [Mazeika et al., 2024] and AirBench 2024 [Zeng et al., 2024b]) across both synthetic and real websites. Our empirical study on state-of-the-art browser agents reveals that, while the backbone LLM refuses harmful instructions as a chatbot, the corresponding agent does not. Moreover, attack methods designed to jailbreak refusal-trained LLMs in the chat settings transfer effectively to browser agents. With human rewrites, GPT-4o and o1-preview-based browser agents attempted 98 and 63 harmful behaviors (out of 100), respectively. We publicly release BrowserART and call on LLM developers, policymakers, and agent developers to collaborate on improving agent safety</li>
</ul>

<h3>Title: S$^4$ST: A Strong, Self-transferable, faSt, and Simple Scale Transformation for Transferable Targeted Attack</h3>
<ul>
<li><strong>Authors: </strong>Yongxiang Liu, Bowen Peng, Li Liu, Xiang Li</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13891">https://arxiv.org/abs/2410.13891</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13891">https://arxiv.org/pdf/2410.13891</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13891]] S$^4$ST: A Strong, Self-transferable, faSt, and Simple Scale Transformation for Transferable Targeted Attack(https://arxiv.org/abs/2410.13891)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Transferable targeted adversarial attacks (TTAs) against deep neural networks have been proven significantly more challenging than untargeted ones, yet they remain relatively underexplored. This paper sheds new light on performing highly efficient yet transferable targeted attacks leveraging the simple gradient-based baseline. Our research underscores the critical importance of image transformations within gradient calculations, marking a shift from the prevalent emphasis on loss functions to address the gradient vanishing problem. Moreover, we have developed two effective blind estimators that facilitate the design of transformation strategies to enhance targeted transferability under black-box conditions. The adversarial examples' self-transferability to geometric transformations has been identified as strongly correlated with their black-box transferability, featuring these basic operations as potent yet overlapped proxies for facilitating targeted transferability. The surrogate self-alignment assessments further highlight simple scaling transformation's exceptional efficacy, which rivals that of most advanced methods. Building on these insights, we introduce a scaling-centered transformation strategy termed Strong, Self-transferable, faSt, and Simple Scale Transformation (S4ST) to enhance transferable targeted attacks. In experiments conducted on the ImageNet-Compatible benchmark dataset, our proposed S4ST attains a SOTA average targeted transfer success rate across various challenging black-box models, outperforming the previous leading method by over 14% while requiring only 25% of the execution time. Additionally, our approach eclipses SOTA attacks considerably and exhibits remarkable effectiveness against real-world APIs. This work marks a significant leap forward in TTAs, revealing the realistic threats they pose and providing a practical generation method for future research.</li>
</ul>

<h3>Title: Can LLMs be Scammed? A Baseline Measurement Study</h3>
<ul>
<li><strong>Authors: </strong>Udari Madhushani Sehwag, Kelly Patel, Francesca Mosca, Vineeth Ravi, Jessica Staddon</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13893">https://arxiv.org/abs/2410.13893</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13893">https://arxiv.org/pdf/2410.13893</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13893]] Can LLMs be Scammed? A Baseline Measurement Study(https://arxiv.org/abs/2410.13893)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Despite the importance of developing generative AI models that can effectively resist scams, current literature lacks a structured framework for evaluating their vulnerability to such threats. In this work, we address this gap by constructing a benchmark based on the FINRA taxonomy and systematically assessing Large Language Models' (LLMs') vulnerability to a variety of scam tactics. First, we incorporate 37 well-defined base scam scenarios reflecting the diverse scam categories identified by FINRA taxonomy, providing a focused evaluation of LLMs' scam detection capabilities. Second, we utilize representative proprietary (GPT-3.5, GPT-4) and open-source (Llama) models to analyze their performance in scam detection. Third, our research provides critical insights into which scam tactics are most effective against LLMs and how varying persona traits and persuasive techniques influence these vulnerabilities. We reveal distinct susceptibility patterns across different models and scenarios, underscoring the need for targeted enhancements in LLM design and deployment.</li>
</ul>

<h3>Title: A Formal Framework for Assessing and Mitigating Emergent Security Risks in Generative AI Models: Bridging Theory and Dynamic Risk Mitigation</h3>
<ul>
<li><strong>Authors: </strong>Aviral Srivastava, Sourav Panda</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13897">https://arxiv.org/abs/2410.13897</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13897">https://arxiv.org/pdf/2410.13897</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13897]] A Formal Framework for Assessing and Mitigating Emergent Security Risks in Generative AI Models: Bridging Theory and Dynamic Risk Mitigation(https://arxiv.org/abs/2410.13897)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, diffusion, generative, large language model</a></li>
<li><strong>Abstract: </strong>As generative AI systems, including large language models (LLMs) and diffusion models, advance rapidly, their growing adoption has led to new and complex security risks often overlooked in traditional AI risk assessment frameworks. This paper introduces a novel formal framework for categorizing and mitigating these emergent security risks by integrating adaptive, real-time monitoring, and dynamic risk mitigation strategies tailored to generative models' unique vulnerabilities. We identify previously under-explored risks, including latent space exploitation, multi-modal cross-attack vectors, and feedback-loop-induced model degradation. Our framework employs a layered approach, incorporating anomaly detection, continuous red-teaming, and real-time adversarial simulation to mitigate these risks. We focus on formal verification methods to ensure model robustness and scalability in the face of evolving threats. Though theoretical, this work sets the stage for future empirical validation by establishing a detailed methodology and metrics for evaluating the performance of risk mitigation strategies in generative AI systems. This framework addresses existing gaps in AI safety, offering a comprehensive road map for future research and implementation.</li>
</ul>

<h3>Title: Security of and by Generative AI platforms</h3>
<ul>
<li><strong>Authors: </strong>Hari Hayagreevan, Souvik Khamaru</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13899">https://arxiv.org/abs/2410.13899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13899">https://arxiv.org/pdf/2410.13899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13899]] Security of and by Generative AI platforms(https://arxiv.org/abs/2410.13899)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack, robust, generative</a></li>
<li><strong>Abstract: </strong>This whitepaper highlights the dual importance of securing generative AI (genAI) platforms and leveraging genAI for cybersecurity. As genAI technologies proliferate, their misuse poses significant risks, including data breaches, model tampering, and malicious content generation. Securing these platforms is critical to protect sensitive data, ensure model integrity, and prevent adversarial attacks. Simultaneously, genAI presents opportunities for enhancing security by automating threat detection, vulnerability analysis, and incident response. The whitepaper explores strategies for robust security frameworks around genAI systems, while also showcasing how genAI can empower organizations to anticipate, detect, and mitigate sophisticated cyber threats.</li>
</ul>

<h3>Title: Voting by mail: a Markov chain model for managing the security risks of election systems</h3>
<ul>
<li><strong>Authors: </strong>Carmen A. Haseltine, Laura A. Albert</a></li>
<li><strong>Subjects: </strong>cs.CR, math.PR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13900">https://arxiv.org/abs/2410.13900</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13900">https://arxiv.org/pdf/2410.13900</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13900]] Voting by mail: a Markov chain model for managing the security risks of election systems(https://arxiv.org/abs/2410.13900)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, protect, attack</a></li>
<li><strong>Abstract: </strong>The scrutiny surrounding vote-by-mail (VBM) in the United States has increased in recent years, raising concerns about the integrity and security of absentee voting. This paper addresses these issues by introducing a dynamic mathematical modeling framework for performing a risk assessment of VBM processes. We introduce a discrete-time Markov chain (DTMC) to model the VBM process and assess election performance and risk with a novel layered network approach that considers the interplay between VBM processes, malicious and non-malicious threats, and security mitigations. The time-inhomogeneous DTMC framework captures dynamic risks and evaluates performance over time. The DTMC model accounts for a spectrum of outcomes, from unintended voter errors to sophisticated, targeted attacks, representing a significant advancement in the risk assessment of VBM planning and protection. A case study based on real-world data from Milwaukee County, Wisconsin, is used to evaluate the DTMC model. The analysis includes the development of attack scenarios to assess the system's resilience and the evaluation of security measures. The analysis suggests that ballot drop boxes and automatic ballot notification systems are crucial for ensuring secure and reliable operations.</li>
</ul>

<h3>Title: SoK: Prompt Hacking of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Baha Rababah, Shang (Tommy)Wu, Matthew Kwiatkowski, Carson Leung, Cuneyt Gurcan Akcora</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13901">https://arxiv.org/abs/2410.13901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13901">https://arxiv.org/pdf/2410.13901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13901]] SoK: Prompt Hacking of Large Language Models(https://arxiv.org/abs/2410.13901)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>The safety and robustness of large language models (LLMs) based applications remain critical challenges in artificial intelligence. Among the key threats to these applications are prompt hacking attacks, which can significantly undermine the security and reliability of LLM-based systems. In this work, we offer a comprehensive and systematic overview of three distinct types of prompt hacking: jailbreaking, leaking, and injection, addressing the nuances that differentiate them despite their overlapping characteristics. To enhance the evaluation of LLM-based applications, we propose a novel framework that categorizes LLM responses into five distinct classes, moving beyond the traditional binary classification. This approach provides more granular insights into the AI's behavior, improving diagnostic precision and enabling more targeted enhancements to the system's safety and robustness.</li>
</ul>

<h3>Title: CoreGuard: Safeguarding Foundational Capabilities of LLMs Against Model Stealing in Edge Deployment</h3>
<ul>
<li><strong>Authors: </strong>Qinfeng Li, Yangfan Xie, Tianyu Du, Zhiqiang Shen, Zhenghan Qin, Hao Peng, Xinkui Zhao, Xianwei Zhu, Jianwei Yin, Xuhong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13903">https://arxiv.org/abs/2410.13903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13903">https://arxiv.org/pdf/2410.13903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13903]] CoreGuard: Safeguarding Foundational Capabilities of LLMs Against Model Stealing in Edge Deployment(https://arxiv.org/abs/2410.13903)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, protect, attack, steal, large language model</a></li>
<li><strong>Abstract: </strong>Proprietary large language models (LLMs) demonstrate exceptional generalization ability across various tasks. Additionally, deploying LLMs on edge devices is trending for efficiency and privacy reasons. However, edge deployment of proprietary LLMs introduces new security threats: attackers who obtain an edge-deployed LLM can easily use it as a base model for various tasks due to its high generalization ability, which we call foundational capability stealing. Unfortunately, existing model protection mechanisms are often task-specific and fail to protect general-purpose LLMs, as they mainly focus on protecting task-related parameters using trusted execution environments (TEEs). Although some recent TEE-based methods are able to protect the overall model parameters in a computation-efficient way, they still suffer from prohibitive communication costs between TEE and CPU/GPU, making it impractical to deploy for edge LLMs. To protect the foundational capabilities of edge LLMs, we propose CoreGuard, a computation- and communication-efficient model protection approach against model stealing on edge devices. The core component of CoreGuard is a lightweight and propagative authorization module residing in TEE. Extensive experiments show that CoreGuard achieves the same security protection as the black-box security guarantees with negligible overhead.</li>
</ul>

<h3>Title: NSmark: Null Space Based Black-box Watermarking Defense Framework for Pre-trained Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haodong Zhao, Jinming Hu, Peixuan Li, Fangqi Li, Jinrui Sha, Peixuan Chen, Zhuosheng Zhang, Gongshen Liu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13907">https://arxiv.org/abs/2410.13907</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13907">https://arxiv.org/pdf/2410.13907</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13907]] NSmark: Null Space Based Black-box Watermarking Defense Framework for Pre-trained Language Models(https://arxiv.org/abs/2410.13907)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, defense, attack, robust, extraction, watermark</a></li>
<li><strong>Abstract: </strong>Pre-trained language models (PLMs) have emerged as critical intellectual property (IP) assets that necessitate protection. Although various watermarking strategies have been proposed, they remain vulnerable to Linear Functionality Equivalence Attacks (LFEA), which can invalidate most existing white-box watermarks without prior knowledge of the watermarking scheme or training data. This paper further analyzes and extends the attack scenarios of LFEA to the commonly employed black-box settings for PLMs by considering Last-Layer outputs (dubbed LL-LFEA). We discover that the null space of the output matrix remains invariant against LL-LFEA attacks. Based on this finding, we propose NSmark, a task-agnostic, black-box watermarking scheme capable of resisting LL-LFEA attacks. NSmark consists of three phases: (i) watermark generation using the digital signature of the owner, enhanced by spread spectrum modulation for increased robustness; (ii) watermark embedding through an output mapping extractor that preserves PLM performance while maximizing watermark capacity; (iii) watermark verification, assessed by extraction rate and null space conformity. Extensive experiments on both pre-training and downstream tasks confirm the effectiveness, reliability, fidelity, and robustness of our approach. Code is available at this https URL.</li>
</ul>

<h3>Title: Mitigating the Backdoor Effect for Multi-Task Model Merging via Safety-Aware Subspace</h3>
<ul>
<li><strong>Authors: </strong>Jinluan Yang, Anke Tang, Didi Zhu, Zhengyu Chen, Li Shen, Fei Wu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13910">https://arxiv.org/abs/2410.13910</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13910">https://arxiv.org/pdf/2410.13910</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13910]] Mitigating the Backdoor Effect for Multi-Task Model Merging via Safety-Aware Subspace(https://arxiv.org/abs/2410.13910)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Model merging has gained significant attention as a cost-effective approach to integrate multiple single-task fine-tuned models into a unified one that can perform well on multiple tasks. However, existing model merging techniques primarily focus on resolving conflicts between task-specific models, they often overlook potential security threats, particularly the risk of backdoor attacks in the open-source model ecosystem. In this paper, we first investigate the vulnerabilities of existing model merging methods to backdoor attacks, identifying two critical challenges: backdoor succession and backdoor transfer. To address these issues, we propose a novel Defense-Aware Merging (DAM) approach that simultaneously mitigates task interference and backdoor vulnerabilities. Specifically, DAM employs a meta-learning-based optimization method with dual masks to identify a shared and safety-aware subspace for model merging. These masks are alternately optimized: the Task-Shared mask identifies common beneficial parameters across tasks, aiming to preserve task-specific knowledge while reducing interference, while the Backdoor-Detection mask isolates potentially harmful parameters to neutralize security threats. This dual-mask design allows us to carefully balance the preservation of useful knowledge and the removal of potential vulnerabilities. Compared to existing merging methods, DAM achieves a more favorable balance between performance and security, reducing the attack success rate by 2-10 percentage points while sacrificing only about 1% in accuracy. Furthermore, DAM exhibits robust performance and broad applicability across various types of backdoor attacks and the number of compromised models involved in the merging process. We will release the codes and models soon.</li>
</ul>

<h3>Title: GraspDiffusion: Synthesizing Realistic Whole-body Hand-Object Interaction</h3>
<ul>
<li><strong>Authors: </strong>Patrick Kwon, Hanbyul Joo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13911">https://arxiv.org/abs/2410.13911</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13911">https://arxiv.org/pdf/2410.13911</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13911]] GraspDiffusion: Synthesizing Realistic Whole-body Hand-Object Interaction(https://arxiv.org/abs/2410.13911)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent generative models can synthesize high-quality images but often fail to generate humans interacting with objects using their hands. This arises mostly from the model's misunderstanding of such interactions, and the hardships of synthesizing intricate regions of the body. In this paper, we propose GraspDiffusion, a novel generative method that creates realistic scenes of human-object interaction. Given a 3D object mesh, GraspDiffusion first constructs life-like whole-body poses with control over the object's location relative to the human body. This is achieved by separately leveraging the generative priors for 3D body and hand poses, optimizing them into a joint grasping pose. The resulting pose guides the image synthesis to correctly reflect the intended interaction, allowing the creation of realistic and diverse human-object interaction scenes. We demonstrate that GraspDiffusion can successfully tackle the relatively uninvestigated problem of generating full-bodied human-object interactions while outperforming previous methods. Code and models will be available at this https URL</li>
</ul>

<h3>Title: GBCT: An Efficient and Adaptive Granular-Ball Clustering Algorithm for Complex Data</h3>
<ul>
<li><strong>Authors: </strong>Shuyin Xia, Bolun Shi, Yifan Wang, Jiang Xie, Guoyin Wang, Xinbo Gao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13917">https://arxiv.org/abs/2410.13917</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13917">https://arxiv.org/pdf/2410.13917</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13917]] GBCT: An Efficient and Adaptive Granular-Ball Clustering Algorithm for Complex Data(https://arxiv.org/abs/2410.13917)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Traditional clustering algorithms often focus on the most fine-grained information and achieve clustering by calculating the distance between each pair of data points or implementing other calculations based on points. This way is not inconsistent with the cognitive mechanism of "global precedence" in human brain, resulting in those methods' bad performance in efficiency, generalization ability and robustness. To address this problem, we propose a new clustering algorithm called granular-ball clustering (GBCT) via granular-ball computing. Firstly, GBCT generates a smaller number of granular-balls to represent the original data, and forms clusters according to the relationship between granular-balls, instead of the traditional point relationship. At the same time, its coarse-grained characteristics are not susceptible to noise, and the algorithm is efficient and robust; besides, as granular-balls can fit various complex data, GBCT performs much better in non-spherical data sets than other traditional clustering methods. The completely new coarse granularity representation method of GBCT and cluster formation mode can also used to improve other traditional methods.</li>
</ul>

<h3>Title: Leveraging Fine-Tuned Language Models for Efficient and Accurate Smart Contract Auditing</h3>
<ul>
<li><strong>Authors: </strong>Zhiyuan Wei, Jing Sun, Zijian Zhang, Xianhao Zhang, Meng Li</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13918">https://arxiv.org/abs/2410.13918</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13918">https://arxiv.org/pdf/2410.13918</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13918]] Leveraging Fine-Tuned Language Models for Efficient and Accurate Smart Contract Auditing(https://arxiv.org/abs/2410.13918)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust, large language model</a></li>
<li><strong>Abstract: </strong>The rise of blockchain technologies has greatly accelerated the development and deployment of smart contracts. However, their inherent vulnerabilities and susceptibility to bugs have led to significant financial losses, underscoring the challenges in securing smart contracts. While traditional auditing methods are crucial, they often fall short in addressing the increasing complexity and volume of smart contracts. Recent advancements in Large Language Models (LLMs) offer promising solutions for enhancing software auditing by automatically identifying security vulnerabilities. Despite their potential, the practical application of these models is hindered by substantial computational demands. This paper investigates the feasibility of using smaller, fine-tuned models to achieve comparable or even superior results in smart contract auditing. We introduce the FTSmartAudit framework, which is designed to develop cost-effective, specialized models for smart contract auditing through the fine-tuning of LLMs. Our contributions include: (1) a single-task learning framework that streamlines data preparation, training, evaluation, and continuous learning; (2) a robust dataset generation method utilizing domain-special knowledge distillation to produce high-quality datasets from advanced models like GPT-4o; (3) an adaptive learning strategy to maintain model accuracy and robustness; (4) the proven effectiveness of fine-tuned models in detecting specific vulnerabilities and complex logical errors; and (5) a framework that can be extended to other domains requiring LLM solutions. Our experimental results demonstrate that smaller models can surpass state-of-the-art commercial models and tools in detecting vulnerabilities in smart contracts.</li>
</ul>

<h3>Title: LLM Agent Honeypot: Monitoring AI Hacking Agents in the Wild</h3>
<ul>
<li><strong>Authors: </strong>Reworr, Dmitrii Volkov</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13919">https://arxiv.org/abs/2410.13919</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13919">https://arxiv.org/pdf/2410.13919</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13919]] LLM Agent Honeypot: Monitoring AI Hacking Agents in the Wild(https://arxiv.org/abs/2410.13919)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>We introduce the LLM Honeypot, a system for monitoring autonomous AI hacking agents. We deployed a customized SSH honeypot and applied prompt injections with temporal analysis to identify LLM-based agents among attackers. Over a trial run of a few weeks in a public environment, we collected 800,000 hacking attempts and 6 potential AI agents, which we plan to analyze in depth in future work. Our objectives aim to improve awareness of AI hacking agents and enhance preparedness for their risks.</li>
</ul>

<h3>Title: ARKit LabelMaker: A New Scale for Indoor 3D Scene Understanding</h3>
<ul>
<li><strong>Authors: </strong>Guangda Ji, Silvan Weder, Francis Engelmann, Marc Pollefeys, Hermann Blum</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13924">https://arxiv.org/abs/2410.13924</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13924">https://arxiv.org/pdf/2410.13924</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13924]] ARKit LabelMaker: A New Scale for Indoor 3D Scene Understanding(https://arxiv.org/abs/2410.13924)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>The performance of neural networks scales with both their size and the amount of data they have been trained on. This is shown in both language and image generation. However, this requires scaling-friendly network architectures as well as large-scale datasets. Even though scaling-friendly architectures like transformers have emerged for 3D vision tasks, the GPT-moment of 3D vision remains distant due to the lack of training data. In this paper, we introduce ARKit LabelMaker, the first large-scale, real-world 3D dataset with dense semantic annotations. Specifically, we complement ARKitScenes dataset with dense semantic annotations that are automatically generated at scale. To this end, we extend LabelMaker, a recent automatic annotation pipeline, to serve the needs of large-scale pre-training. This involves extending the pipeline with cutting-edge segmentation models as well as making it robust to the challenges of large-scale processing. Further, we push forward the state-of-the-art performance on ScanNet and ScanNet200 dataset with prevalent 3D semantic segmentation models, demonstrating the efficacy of our generated dataset.</li>
</ul>

<h3>Title: FiTv2: Scalable and Improved Flexible Vision Transformer for Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>ZiDong Wang, Zeyu Lu, Di Huang, Cai Zhou, Wanli Ouyang, and Lei Bai</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13925">https://arxiv.org/abs/2410.13925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13925">https://arxiv.org/pdf/2410.13925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13925]] FiTv2: Scalable and Improved Flexible Vision Transformer for Diffusion Model(https://arxiv.org/abs/2410.13925)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>\textit{Nature is infinitely resolution-free}. In the context of this reality, existing diffusion models, such as Diffusion Transformers, often face challenges when processing image resolutions outside of their trained domain. To address this limitation, we conceptualize images as sequences of tokens with dynamic sizes, rather than traditional methods that perceive images as fixed-resolution grids. This perspective enables a flexible training strategy that seamlessly accommodates various aspect ratios during both training and inference, thus promoting resolution generalization and eliminating biases introduced by image cropping. On this basis, we present the \textbf{Flexible Vision Transformer} (FiT), a transformer architecture specifically designed for generating images with \textit{unrestricted resolutions and aspect ratios}. We further upgrade the FiT to FiTv2 with several innovative designs, includingthe Query-Key vector normalization, the AdaLN-LoRA module, a rectified flow scheduler, and a Logit-Normal sampler. Enhanced by a meticulously adjusted network structure, FiTv2 exhibits $2\times$ convergence speed of FiT. When incorporating advanced training-free extrapolation techniques, FiTv2 demonstrates remarkable adaptability in both resolution extrapolation and diverse resolution generation. Additionally, our exploration of the scalability of the FiTv2 model reveals that larger models exhibit better computational efficiency. Furthermore, we introduce an efficient post-training strategy to adapt a pre-trained model for the high-resolution generation. Comprehensive experiments demonstrate the exceptional performance of FiTv2 across a broad range of resolutions. We have released all the codes and models at \url{this https URL} to promote the exploration of diffusion transformer models for arbitrary-resolution image generation.</li>
</ul>

<h3>Title: Automatically Interpreting Millions of Features in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Gon√ßalo Paulo, Alex Mallen, Caden Juang, Nora Belrose</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13928">https://arxiv.org/abs/2410.13928</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13928">https://arxiv.org/pdf/2410.13928</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13928]] Automatically Interpreting Millions of Features in Large Language Models(https://arxiv.org/abs/2410.13928)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>While the activations of neurons in deep neural networks usually do not have a simple human-understandable interpretation, sparse autoencoders (SAEs) can be used to transform these activations into a higher-dimensional latent space which may be more easily interpretable. However, these SAEs can have millions of distinct latent features, making it infeasible for humans to manually interpret each one. In this work, we build an open-source automated pipeline to generate and evaluate natural language explanations for SAE features using LLMs. We test our framework on SAEs of varying sizes, activation functions, and losses, trained on two different open-weight LLMs. We introduce five new techniques to score the quality of explanations that are cheaper to run than the previous state of the art. One of these techniques, intervention scoring, evaluates the interpretability of the effects of intervening on a feature, which we find explains features that are not recalled by existing methods. We propose guidelines for generating better explanations that remain valid for a broader set of activating contexts, and discuss pitfalls with existing scoring techniques. We use our explanations to measure the semantic similarity of independently trained SAEs, and find that SAEs trained on nearby layers of the residual stream are highly similar. Our large-scale analysis confirms that SAE latents are indeed much more interpretable than neurons, even when neurons are sparsified using top-$k$ postprocessing. Our code is available at this https URL, and our explanations are available at this https URL.</li>
</ul>

<h3>Title: Boosting LLM Translation Skills without General Ability Loss via Rationale Distillation</h3>
<ul>
<li><strong>Authors: </strong>Junhong Wu, Yang Zhao, Yangyifan Xu, Bing Liu, Chengqing Zong</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13944">https://arxiv.org/abs/2410.13944</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13944">https://arxiv.org/pdf/2410.13944</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13944]] Boosting LLM Translation Skills without General Ability Loss via Rationale Distillation(https://arxiv.org/abs/2410.13944)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, generative, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved impressive results across numerous NLP tasks but still encounter difficulties in machine translation. Traditional methods to improve translation have typically involved fine-tuning LLMs using parallel corpora. However, vanilla fine-tuning often leads to catastrophic forgetting of the instruction-following capabilities and alignment with human preferences, compromising their broad general abilities and introducing potential security risks. These abilities, which are developed using proprietary and unavailable training data, make existing continual instruction tuning methods ineffective. To overcome this issue, we propose a novel approach called RaDis (Rationale Distillation). RaDis harnesses the strong generative capabilities of LLMs to create rationales for training data, which are then "replayed" to prevent forgetting. These rationales encapsulate general knowledge and safety principles, acting as self-distillation targets to regulate the training process. By jointly training on both reference translations and self-generated rationales, the model can learn new translation skills while preserving its overall general abilities. Extensive experiments demonstrate that our method enhances machine translation performance while maintaining the broader capabilities of LLMs across other tasks. This work presents a pathway for creating more versatile LLMs that excel in specialized tasks without compromising generality and safety.</li>
</ul>

<h3>Title: Satellite Streaming Video QoE Prediction: A Real-World Subjective Database and Network-Level Prediction Models</h3>
<ul>
<li><strong>Authors: </strong>Bowen Chen, Zaixi Shang, Jae Won Chung, David Lerner, Werner Robitza, Rakesh Rao Ramachandra Rao, Alexander Raake, Alan C. Bovik</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13952">https://arxiv.org/abs/2410.13952</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13952">https://arxiv.org/pdf/2410.13952</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13952]] Satellite Streaming Video QoE Prediction: A Real-World Subjective Database and Network-Level Prediction Models(https://arxiv.org/abs/2410.13952)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Demand for streaming services, including satellite, continues to exhibit unprecedented growth. Internet Service Providers find themselves at the crossroads of technological advancements and rising customer expectations. To stay relevant and competitive, these ISPs must ensure their networks deliver optimal video streaming quality, a key determinant of user satisfaction. Towards this end, it is important to have accurate Quality of Experience prediction models in place. However, achieving robust performance by these models requires extensive data sets labeled by subjective opinion scores on videos impaired by diverse playback disruptions. To bridge this data gap, we introduce the LIVE-Viasat Real-World Satellite QoE Database. This database consists of 179 videos recorded from real-world streaming services affected by various authentic distortion patterns. We also conducted a comprehensive subjective study involving 54 participants, who contributed both continuous-time opinion scores and endpoint (retrospective) QoE scores. Our analysis sheds light on various determinants influencing subjective QoE, such as stall events, spatial resolutions, bitrate, and certain network parameters. We demonstrate the usefulness of this unique new resource by evaluating the efficacy of prevalent QoE-prediction models on it. We also created a new model that maps the network parameters to predicted human perception scores, which can be used by ISPs to optimize the video streaming quality of their networks. Our proposed model, which we call SatQA, is able to accurately predict QoE using only network parameters, without any access to pixel data or video-specific metadata, estimated by Spearman's Rank Order Correlation Coefficient (SROCC), Pearson Linear Correlation Coefficient (PLCC), and Root Mean Squared Error (RMSE), indicating high accuracy and reliability.</li>
</ul>

<h3>Title: On Diffusion Models for Multi-Agent Partial Observability: Shared Attractors, Error Bounds, and Composite Flow</h3>
<ul>
<li><strong>Authors: </strong>Tonghan Wang, Heng Dong, Yanchen Jiang, David C. Parkes, Milind Tambe</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13953">https://arxiv.org/abs/2410.13953</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13953">https://arxiv.org/pdf/2410.13953</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13953]] On Diffusion Models for Multi-Agent Partial Observability: Shared Attractors, Error Bounds, and Composite Flow(https://arxiv.org/abs/2410.13953)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Multiagent systems grapple with partial observability (PO), and the decentralized POMDP (Dec-POMDP) model highlights the fundamental nature of this challenge. Whereas recent approaches to address PO have appealed to deep learning models, providing a rigorous understanding of how these models and their approximation errors affect agents' handling of PO and their interactions remain a challenge. In addressing this challenge, we investigate reconstructing global states from local action-observation histories in Dec-POMDPs using diffusion models. We first find that diffusion models conditioned on local history represent possible states as stable fixed points. In collectively observable (CO) Dec-POMDPs, individual diffusion models conditioned on agents' local histories share a unique fixed point corresponding to the global state, while in non-CO settings, the shared fixed points yield a distribution of possible states given joint history. We further find that, with deep learning approximation errors, fixed points can deviate from true states and the deviation is negatively correlated to the Jacobian rank. Inspired by this low-rank property, we bound the deviation by constructing a surrogate linear regression model that approximates the local behavior of diffusion models. With this bound, we propose a composite diffusion process iterating over agents with theoretical convergence guarantees to the true state.</li>
</ul>

<h3>Title: Benchmarking Transcriptomics Foundation Models for Perturbation Analysis : one PCA still rules them all</h3>
<ul>
<li><strong>Authors: </strong>Ihab Bendidi, Shawn Whitfield, Kian Kenyon-Dean, Hanene Ben Yedder, Yassir El Mesbahi, Emmanuel Noutahi, Alisandra K. Denton</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13956">https://arxiv.org/abs/2410.13956</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13956">https://arxiv.org/pdf/2410.13956</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13956]] Benchmarking Transcriptomics Foundation Models for Perturbation Analysis : one PCA still rules them all(https://arxiv.org/abs/2410.13956)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Understanding the relationships among genes, compounds, and their interactions in living organisms remains limited due to technological constraints and the complexity of biological data. Deep learning has shown promise in exploring these relationships using various data types. However, transcriptomics, which provides detailed insights into cellular states, is still underused due to its high noise levels and limited data availability. Recent advancements in transcriptomics sequencing provide new opportunities to uncover valuable insights, especially with the rise of many new foundation models for transcriptomics, yet no benchmark has been made to robustly evaluate the effectiveness of these rising models for perturbation analysis. This article presents a novel biologically motivated evaluation framework and a hierarchy of perturbation analysis tasks for comparing the performance of pretrained foundation models to each other and to more classical techniques of learning from transcriptomics data. We compile diverse public datasets from different sequencing techniques and cell lines to assess models performance. Our approach identifies scVI and PCA to be far better suited models for understanding biological perturbations in comparison to existing foundation models, especially in their application in real-world scenarios.</li>
</ul>

<h3>Title: From Single to Multi: How LLMs Hallucinate in Multi-Document Summarization</h3>
<ul>
<li><strong>Authors: </strong>Catarina G. Belem, Pouya Pezeskhpour, Hayate Iso, Seiji Maekawa, Nikita Bhutani, Estevam Hruschka</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13961">https://arxiv.org/abs/2410.13961</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13961">https://arxiv.org/pdf/2410.13961</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13961]] From Single to Multi: How LLMs Hallucinate in Multi-Document Summarization(https://arxiv.org/abs/2410.13961)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Although many studies have investigated and reduced hallucinations in large language models (LLMs) for single-document tasks, research on hallucination in multi-document summarization (MDS) tasks remains largely unexplored. Specifically, it is unclear how the challenges arising from handling multiple documents (e.g., repetition and diversity of information) affect models outputs. In this work, we investigate how hallucinations manifest in LLMs when summarizing topic-specific information from multiple documents. Since no benchmarks exist for investigating hallucinations in MDS, we use existing news and conversation datasets, annotated with topic-specific insights, to create two novel multi-document benchmarks. When evaluating 5 LLMs on our benchmarks, we observe that on average, up to 75% of the content in LLM-generated summary is hallucinated, with hallucinations more likely to occur towards the end of the summaries. Moreover, when summarizing non-existent topic-related information, gpt-3.5-turbo and GPT-4o still generate summaries about 79.35% and 44% of the time, raising concerns about their tendency to fabricate content. To understand the characteristics of these hallucinations, we manually evaluate 700+ insights and find that most errors stem from either failing to follow instructions or producing overly generic insights. Motivated by these observations, we investigate the efficacy of simple post-hoc baselines in mitigating hallucinations but find them only moderately effective. Our results underscore the need for more effective approaches to systematically mitigate hallucinations in MDS. We release our dataset and code at this http URL.</li>
</ul>

<h3>Title: Enhancing Generalization in Sparse Mixture of Experts Models: The Case for Increased Expert Activation in Compositional Tasks</h3>
<ul>
<li><strong>Authors: </strong>Jinze Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13964">https://arxiv.org/abs/2410.13964</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13964">https://arxiv.org/pdf/2410.13964</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13964]] Enhancing Generalization in Sparse Mixture of Experts Models: The Case for Increased Expert Activation in Compositional Tasks(https://arxiv.org/abs/2410.13964)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>As Transformer models grow in complexity, their ability to generalize to novel, compositional tasks becomes crucial. This study challenges conventional wisdom about sparse activation in Sparse Mixture of Experts (SMoE) models when faced with increasingly complex compositional tasks. Through experiments on the SRAVEN symbolic reasoning task and SKILL-MIX benchmark, we demonstrate that activating more experts improves performance on difficult tasks, with the optimal number of activated experts scaling with task complexity. Our findings reveal that pretrained SMoE-based Large Language Models achieve better results by increasing experts-per-token on challenging compositional tasks.</li>
</ul>

<h3>Title: Detecting AI-Generated Texts in Cross-Domains</h3>
<ul>
<li><strong>Authors: </strong>You Zhou, Jie Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13966">https://arxiv.org/abs/2410.13966</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13966">https://arxiv.org/pdf/2410.13966</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13966]] Detecting AI-Generated Texts in Cross-Domains(https://arxiv.org/abs/2410.13966)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Existing tools to detect text generated by a large language model (LLM) have met with certain success, but their performance can drop when dealing with texts in new domains. To tackle this issue, we train a ranking classifier called RoBERTa-Ranker, a modified version of RoBERTa, as a baseline model using a dataset we constructed that includes a wider variety of texts written by humans and generated by various LLMs. We then present a method to fine-tune RoBERTa-Ranker that requires only a small amount of labeled data in a new domain. Experiments show that this fine-tuned domain-aware model outperforms the popular DetectGPT and GPTZero on both in-domain and cross-domain texts, where AI-generated texts may either be in a different domain or generated by a different LLM not used to generate the training datasets. This approach makes it feasible and economical to build a single system to detect AI-generated texts across various domains.</li>
</ul>

<h3>Title: Trojan Prompt Attacks on Graph Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Minhua Lin, Zhiwei Zhang, Enyan Dai, Zongyu Wu, Yilong Wang, Xiang Zhang, Suhang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13974">https://arxiv.org/abs/2410.13974</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13974">https://arxiv.org/pdf/2410.13974</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13974]] Trojan Prompt Attacks on Graph Neural Networks(https://arxiv.org/abs/2410.13974)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Graph Prompt Learning (GPL) has been introduced as a promising approach that uses prompts to adapt pre-trained GNN models to specific downstream tasks without requiring fine-tuning of the entire model. Despite the advantages of GPL, little attention has been given to its vulnerability to backdoor attacks, where an adversary can manipulate the model's behavior by embedding hidden triggers. Existing graph backdoor attacks rely on modifying model parameters during training, but this approach is impractical in GPL as GNN encoder parameters are frozen after pre-training. Moreover, downstream users may fine-tune their own task models on clean datasets, further complicating the attack. In this paper, we propose TGPA, a backdoor attack framework designed specifically for GPL. TGPA injects backdoors into graph prompts without modifying pre-trained GNN encoders and ensures high attack success rates and clean accuracy. To address the challenge of model fine-tuning by users, we introduce a finetuning-resistant poisoning approach that maintains the effectiveness of the backdoor even after downstream model adjustments. Extensive experiments on multiple datasets under various settings demonstrate the effectiveness of TGPA in compromising GPL models with fixed GNN encoders.</li>
</ul>

<h3>Title: Debiasing Large Vision-Language Models by Ablating Protected Attribute Representations</h3>
<ul>
<li><strong>Authors: </strong>Neale Ratzlaff, Matthew Lyle Olson, Musashi Hinck, Shao-Yen Tseng, Vasudev Lal, Phillip Howard</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13976">https://arxiv.org/abs/2410.13976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13976">https://arxiv.org/pdf/2410.13976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13976]] Debiasing Large Vision-Language Models by Ablating Protected Attribute Representations(https://arxiv.org/abs/2410.13976)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>Large Vision Language Models (LVLMs) such as LLaVA have demonstrated impressive capabilities as general-purpose chatbots that can engage in conversations about a provided input image. However, their responses are influenced by societal biases present in their training datasets, leading to undesirable differences in how the model responds when presented with images depicting people of different demographics. In this work, we propose a novel debiasing framework for LVLMs by directly ablating biased attributes during text generation to avoid generating text related to protected attributes, or even representing them internally. Our method requires no training and a relatively small amount of representative biased outputs (~1000 samples). Our experiments show that not only can we can minimize the propensity of LVLMs to generate text related to protected attributes, but we can even use synthetic data to inform the ablation while retaining captioning performance on real data such as COCO. Furthermore, we find the resulting generations from a debiased LVLM exhibit similar accuracy as a baseline biased model, showing that debiasing effects can be achieved without sacrificing model performance.</li>
</ul>

<h3>Title: On the Learn-to-Optimize Capabilities of Transformers in In-Context Sparse Recovery</h3>
<ul>
<li><strong>Authors: </strong>Renpu Liu, Ruida Zhou, Cong Shen, Jing Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13981">https://arxiv.org/abs/2410.13981</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13981">https://arxiv.org/pdf/2410.13981</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13981]] On the Learn-to-Optimize Capabilities of Transformers in In-Context Sparse Recovery(https://arxiv.org/abs/2410.13981)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>An intriguing property of the Transformer is its ability to perform in-context learning (ICL), where the Transformer can solve different inference tasks without parameter updating based on the contextual information provided by the corresponding input-output demonstration pairs. It has been theoretically proved that ICL is enabled by the capability of Transformers to perform gradient-descent algorithms (Von Oswald et al., 2023a; Bai et al., 2024). This work takes a step further and shows that Transformers can perform learning-to-optimize (L2O) algorithms. Specifically, for the ICL sparse recovery (formulated as LASSO) tasks, we show that a K-layer Transformer can perform an L2O algorithm with a provable convergence rate linear in K. This provides a new perspective explaining the superior ICL capability of Transformers, even with only a few layers, which cannot be achieved by the standard gradient-descent algorithms. Moreover, unlike the conventional L2O algorithms that require the measurement matrix involved in training to match that in testing, the trained Transformer is able to solve sparse recovery problems generated with different measurement matrices. Besides, Transformers as an L2O algorithm can leverage structural information embedded in the training tasks to accelerate its convergence during ICL, and generalize across different lengths of demonstration pairs, where conventional L2O algorithms typically struggle or fail. Such theoretical findings are supported by our experimental results.</li>
</ul>

<h3>Title: RiTeK: A Dataset for Large Language Models Complex Reasoning over Textual Knowledge Graphs</h3>
<ul>
<li><strong>Authors: </strong>Jiatan Huang, Mingchen Li, Zonghai Yao, Zhichao Yang, Yongkang Xiao, Feiyun Ouyang, Xiaohan Li, Shuo Han, Hong Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13987">https://arxiv.org/abs/2410.13987</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13987">https://arxiv.org/pdf/2410.13987</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13987]] RiTeK: A Dataset for Large Language Models Complex Reasoning over Textual Knowledge Graphs(https://arxiv.org/abs/2410.13987)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Answering complex real-world questions often requires accurate retrieval from textual knowledge graphs (TKGs). The scarcity of annotated data, along with intricate topological structures, makes this task particularly challenging. As the nature of relational path information could enhance the inference ability of Large Language Models (LLMs), efficiently retrieving more complex relational path information from TKGs presents another key challenge. To tackle these challenges, we first develop a Dataset for LLMs Complex Reasoning over Textual Knowledge Graphs (RiTeK) with a broad topological structure this http URL synthesize realistic user queries that integrate diverse topological structures, relational information, and complex textual descriptions. We conduct rigorous expert evaluation to validate the quality of our synthesized queries. And then, we introduce an enhanced Monte Carlo Tree Search (MCTS) method, Relational MCTS, to automatically extract relational path information from textual graphs for specific queries. Our dataset mainly covers the medical domain as the relation types and entity are complex and publicly available. Experimental results indicate that RiTeK poses significant challenges for current retrieval and LLM systems, while the proposed Relational MCTS method enhances LLM inference ability and achieves state-of-the-art performance on RiTeK.</li>
</ul>

<h3>Title: Reproducibility study of "LICO: Explainable Models with Language-Image Consistency"</h3>
<ul>
<li><strong>Authors: </strong>Luan Fletcher, Robert van der Klis, Martin Sedl√°ƒçek, Stefan Vasilev, Christos Athanasiadis</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13989">https://arxiv.org/abs/2410.13989</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13989">https://arxiv.org/pdf/2410.13989</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13989]] Reproducibility study of "LICO: Explainable Models with Language-Image Consistency"(https://arxiv.org/abs/2410.13989)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>The growing reproducibility crisis in machine learning has brought forward a need for careful examination of research findings. This paper investigates the claims made by Lei et al. (2023) regarding their proposed method, LICO, for enhancing post-hoc interpretability techniques and improving image classification performance. LICO leverages natural language supervision from a vision-language model to enrich feature representations and guide the learning process. We conduct a comprehensive reproducibility study, employing (Wide) ResNets and established interpretability methods like Grad-CAM and RISE. We were mostly unable to reproduce the authors' results. In particular, we did not find that LICO consistently led to improved classification performance or improvements in quantitative and qualitative measures of interpretability. Thus, our findings highlight the importance of rigorous evaluation and transparent reporting in interpretability research.</li>
</ul>

<h3>Title: Adversarial Inception for Bounded Backdoor Poisoning in Deep Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Ethan Rathbun, Christopher Amato, Alina Oprea</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.13995">https://arxiv.org/abs/2410.13995</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.13995">https://arxiv.org/pdf/2410.13995</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.13995]] Adversarial Inception for Bounded Backdoor Poisoning in Deep Reinforcement Learning(https://arxiv.org/abs/2410.13995)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Recent works have demonstrated the vulnerability of Deep Reinforcement Learning (DRL) algorithms against training-time, backdoor poisoning attacks. These attacks induce pre-determined, adversarial behavior in the agent upon observing a fixed trigger during deployment while allowing the agent to solve its intended task during training. Prior attacks rely on arbitrarily large perturbations to the agent's rewards to achieve both of these objectives - leaving them open to detection. Thus, in this work, we propose a new class of backdoor attacks against DRL which achieve state of the art performance while minimally altering the agent's rewards. These ``inception'' attacks train the agent to associate the targeted adversarial behavior with high returns by inducing a disjunction between the agent's chosen action and the true action executed in the environment during training. We formally define these attacks and prove they can achieve both adversarial objectives. We then devise an online inception attack which significantly out-performs prior attacks under bounded reward constraints.</li>
</ul>

<h3>Title: Personalized Adaptation via In-Context Preference Learning</h3>
<ul>
<li><strong>Authors: </strong>Allison Lau, Younwoo Choi, Vahid Balazadeh, Keertana Chidambaram, Vasilis Syrgkanis, Rahul G. Krishnan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14001">https://arxiv.org/abs/2410.14001</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14001">https://arxiv.org/pdf/2410.14001</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14001]] Personalized Adaptation via In-Context Preference Learning(https://arxiv.org/abs/2410.14001)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning from Human Feedback (RLHF) is widely used to align Language Models (LMs) with human preferences. However, existing approaches often neglect individual user preferences, leading to suboptimal personalization. We present the Preference Pretrained Transformer (PPT), a novel approach for adaptive personalization using online user feedback. PPT leverages the in-context learning capabilities of transformers to dynamically adapt to individual preferences. Our approach consists of two phases: (1) an offline phase where we train a single policy model using a history-dependent loss function, and (2) an online phase where the model adapts to user preferences through in-context learning. We demonstrate PPT's effectiveness in a contextual bandit setting, showing that it achieves personalized adaptation superior to existing methods while significantly reducing the computational costs. Our results suggest the potential of in-context learning for scalable and efficient personalization in large language models.</li>
</ul>

<h3>Title: Conformal Prediction for Federated Graph Neural Networks with Missing Neighbor Information</h3>
<ul>
<li><strong>Authors: </strong>√ñmer Faruk Akg√ºl, Rajgopal Kannan, Viktor Prasanna</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14010">https://arxiv.org/abs/2410.14010</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14010">https://arxiv.org/pdf/2410.14010</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14010]] Conformal Prediction for Federated Graph Neural Networks with Missing Neighbor Information(https://arxiv.org/abs/2410.14010)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Graphs play a crucial role in data mining and machine learning, representing real-world objects and interactions. As graph datasets grow, managing large, decentralized subgraphs becomes essential, particularly within federated learning frameworks. These frameworks face significant challenges, including missing neighbor information, which can compromise model reliability in safety-critical settings. Deployment of federated learning models trained in such settings necessitates quantifying the uncertainty of the models. This study extends the applicability of Conformal Prediction (CP), a well-established method for uncertainty quantification, to federated graph learning. We specifically tackle the missing links issue in distributed subgraphs to minimize its adverse effects on CP set sizes. We discuss data dependencies across the distributed subgraphs and establish conditions for CP validity and precise test-time coverage. We introduce a Variational Autoencoder-based approach for reconstructing missing neighbors to mitigate the negative impact of missing data. Empirical evaluations on real-world datasets demonstrate the efficacy of our approach, yielding smaller prediction sets while ensuring coverage guarantees.</li>
</ul>

<h3>Title: LLMs are Biased Teachers: Evaluating LLM Bias in Personalized Education</h3>
<ul>
<li><strong>Authors: </strong>Iain Weissburg, Sathvika Anand, Sharon Levy, Haewon Jeong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14012">https://arxiv.org/abs/2410.14012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14012">https://arxiv.org/pdf/2410.14012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14012]] LLMs are Biased Teachers: Evaluating LLM Bias in Personalized Education(https://arxiv.org/abs/2410.14012)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>With the increasing adoption of large language models (LLMs) in education, concerns about inherent biases in these models have gained prominence. We evaluate LLMs for bias in the personalized educational setting, specifically focusing on the models' roles as "teachers". We reveal significant biases in how models generate and select educational content tailored to different demographic groups, including race, ethnicity, sex, gender, disability status, income, and national origin. We introduce and apply two bias score metrics--Mean Absolute Bias (MAB) and Maximum Difference Bias (MDB)--to analyze 9 open and closed state-of-the-art LLMs. Our experiments, which utilize over 17,000 educational explanations across multiple difficulty levels and topics, uncover that models perpetuate both typical and inverted harmful stereotypes.</li>
</ul>

<h3>Title: Probabilistic U-Net with Kendall Shape Spaces for Geometry-Aware Segmentations of Images</h3>
<ul>
<li><strong>Authors: </strong>Jiyoung Park, G√ºnay Doƒüan</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14017">https://arxiv.org/abs/2410.14017</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14017">https://arxiv.org/pdf/2410.14017</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14017]] Probabilistic U-Net with Kendall Shape Spaces for Geometry-Aware Segmentations of Images(https://arxiv.org/abs/2410.14017)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>One of the fundamental problems in computer vision is image segmentation, the task of detecting distinct regions or objects in given images. Deep Neural Networks (DNN) have been shown to be very effective in segmenting challenging images, producing convincing segmentations. There is further need for probabilistic DNNs that can reflect the uncertainties from the input images and the models into the computed segmentations, in other words, new DNNs that can generate multiple plausible segmentations and their distributions depending on the input or the model uncertainties. While there are existing probabilistic segmentation models, many of them do not take into account the geometry or shape underlying the segmented regions. In this paper, we propose a probabilistic image segmentation model that can incorporate the geometry of a segmentation. Our proposed model builds on the Probabilistic U-Net of \cite{kohl2018probabilistic} to generate probabilistic segmentations, i.e.\! multiple likely segmentations for an input image. Our model also adopts the Kendall Shape Variational Auto-Encoder of \cite{vadgama2023kendall} to encode a Kendall shape space in the latent variable layers of the prior and posterior networks of the Probabilistic U-Net. Incorporating the shape space in this manner leads to a more robust segmentation with spatially coherent regions, respecting the underlying geometry in the input images.</li>
</ul>

<h3>Title: Identifying Privacy Personas</h3>
<ul>
<li><strong>Authors: </strong>Olena Hrynenko, Andrea Cavallaro</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14023">https://arxiv.org/abs/2410.14023</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14023">https://arxiv.org/pdf/2410.14023</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14023]] Identifying Privacy Personas(https://arxiv.org/abs/2410.14023)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>Privacy personas capture the differences in user segments with respect to one's knowledge, behavioural patterns, level of self-efficacy, and perception of the importance of privacy protection. Modelling these differences is essential for appropriately choosing personalised communication about privacy (e.g. to increase literacy) and for defining suitable choices for privacy enhancing technologies (PETs). While various privacy personas have been derived in the literature, they group together people who differ from each other in terms of important attributes such as perceived or desired level of control, and motivation to use PET. To address this lack of granularity and comprehensiveness in describing personas, we propose eight personas that we derive by combining qualitative and quantitative analysis of the responses to an interactive educational questionnaire. We design an analysis pipeline that uses divisive hierarchical clustering and Boschloo's statistical test of homogeneity of proportions to ensure that the elicited clusters differ from each other based on a statistical measure. Additionally, we propose a new measure for calculating distances between questionnaire responses, that accounts for the type of the question (closed- vs open-ended) used to derive traits. We show that the proposed privacy personas statistically differ from each other. We statistically validate the proposed personas and also compare them with personas in the literature, showing that they provide a more granular and comprehensive understanding of user segments, which will allow to better assist users with their privacy needs.</li>
</ul>

<h3>Title: Generating Signed Language Instructions in Large-Scale Dialogue Systems</h3>
<ul>
<li><strong>Authors: </strong>Mert ƒ∞nan, Katherine Atwell, Anthony Sicilia, Lorna Quandt, Malihe Alikhani</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14026">https://arxiv.org/abs/2410.14026</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14026">https://arxiv.org/pdf/2410.14026</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14026]] Generating Signed Language Instructions in Large-Scale Dialogue Systems(https://arxiv.org/abs/2410.14026)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce a goal-oriented conversational AI system enhanced with American Sign Language (ASL) instructions, presenting the first implementation of such a system on a worldwide multimodal conversational AI platform. Accessible through a touch-based interface, our system receives input from users and seamlessly generates ASL instructions by leveraging retrieval methods and cognitively based gloss translations. Central to our design is a sign translation module powered by Large Language Models, alongside a token-based video retrieval system for delivering instructional content from recipes and wikiHow guides. Our development process is deeply rooted in a commitment to community engagement, incorporating insights from the Deaf and Hard-of-Hearing community, as well as experts in cognitive and ASL learning sciences. The effectiveness of our signing instructions is validated by user feedback, achieving ratings on par with those of the system in its non-signing variant. Additionally, our system demonstrates exceptional performance in retrieval accuracy and text-generation quality, measured by metrics such as BERTScore. We have made our codebase and datasets publicly accessible at this https URL, and a demo of our signed instruction video retrieval system is available at this https URL.</li>
</ul>

<h3>Title: Measuring and Modifying the Readability of English Texts with GPT-4</h3>
<ul>
<li><strong>Authors: </strong>Sean Trott (1), Pamela D. Rivi√®re (1) ((1) Department of Cognitive Science, University of California San Diego)</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14028">https://arxiv.org/abs/2410.14028</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14028">https://arxiv.org/pdf/2410.14028</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14028]] Measuring and Modifying the Readability of English Texts with GPT-4(https://arxiv.org/abs/2410.14028)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The success of Large Language Models (LLMs) in other domains has raised the question of whether LLMs can reliably assess and manipulate the readability of text. We approach this question empirically. First, using a published corpus of 4,724 English text excerpts, we find that readability estimates produced ``zero-shot'' from GPT-4 Turbo and GPT-4o mini exhibit relatively high correlation with human judgments (r = 0.76 and r = 0.74, respectively), out-performing estimates derived from traditional readability formulas and various psycholinguistic indices. Then, in a pre-registered human experiment (N = 59), we ask whether Turbo can reliably make text easier or harder to read. We find evidence to support this hypothesis, though considerable variance in human judgments remains unexplained. We conclude by discussing the limitations of this approach, including limited scope, as well as the validity of the ``readability'' construct and its dependence on context, audience, and goal.</li>
</ul>

<h3>Title: Auditing and Enforcing Conditional Fairness via Optimal Transport</h3>
<ul>
<li><strong>Authors: </strong>Mohsen Ghassemi, Alan Mishler, Niccolo Dalmasso, Luhao Zhang, Vamsi K. Potluru, Tucker Balch, Manuela Veloso</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14029">https://arxiv.org/abs/2410.14029</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14029">https://arxiv.org/pdf/2410.14029</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14029]] Auditing and Enforcing Conditional Fairness via Optimal Transport(https://arxiv.org/abs/2410.14029)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Conditional demographic parity (CDP) is a measure of the demographic parity of a predictive model or decision process when conditioning on an additional feature or set of features. Many algorithmic fairness techniques exist to target demographic parity, but CDP is much harder to achieve, particularly when the conditioning variable has many levels and/or when the model outputs are continuous. The problem of auditing and enforcing CDP is understudied in the literature. In light of this, we propose novel measures of {conditional demographic disparity (CDD)} which rely on statistical distances borrowed from the optimal transport literature. We further design and evaluate regularization-based approaches based on these CDD measures. Our methods, \fairbit{} and \fairlp{}, allow us to target CDP even when the conditioning variable has many levels. When model outputs are continuous, our methods target full equality of the conditional distributions, unlike other methods that only consider first moments or related proxy quantities. We validate the efficacy of our approaches on real-world datasets.</li>
</ul>

<h3>Title: Latent Weight Diffusion: Generating Policies from Trajectories</h3>
<ul>
<li><strong>Authors: </strong>Shashank Hegde, Gautam Salhotra, Gaurav S. Sukhatme</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14040">https://arxiv.org/abs/2410.14040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14040">https://arxiv.org/pdf/2410.14040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14040]] Latent Weight Diffusion: Generating Policies from Trajectories(https://arxiv.org/abs/2410.14040)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>With the increasing availability of open-source robotic data, imitation learning has emerged as a viable approach for both robot manipulation and locomotion. Currently, large generalized policies are trained to predict controls or trajectories using diffusion models, which have the desirable property of learning multimodal action distributions. However, generalizability comes with a cost - namely, larger model size and slower inference. Further, there is a known trade-off between performance and action horizon for Diffusion Policy (i.e., diffusing trajectories): fewer diffusion queries accumulate greater trajectory tracking errors. Thus, it is common practice to run these models at high inference frequency, subject to robot computational constraints. To address these limitations, we propose Latent Weight Diffusion (LWD), a method that uses diffusion to learn a distribution over policies for robotic tasks, rather than over trajectories. Our approach encodes demonstration trajectories into a latent space and then decodes them into policies using a hypernetwork. We employ a diffusion denoising model within this latent space to learn its distribution. We demonstrate that LWD can reconstruct the behaviors of the original policies that generated the trajectory dataset. LWD offers the benefits of considerably smaller policy networks during inference and requires fewer diffusion model queries. When tested on the Metaworld MT10 benchmark, LWD achieves a higher success rate compared to a vanilla multi-task policy, while using models up to ~18x smaller during inference. Additionally, since LWD generates closed-loop policies, we show that it outperforms Diffusion Policy in long action horizon settings, with reduced diffusion queries during rollout.</li>
</ul>

<h3>Title: Style-Compress: An LLM-Based Prompt Compression Framework Considering Task-Specific Styles</h3>
<ul>
<li><strong>Authors: </strong>Xiao Pu, Tianxing He, Xiaojun Wan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14042">https://arxiv.org/abs/2410.14042</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14042">https://arxiv.org/pdf/2410.14042</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14042]] Style-Compress: An LLM-Based Prompt Compression Framework Considering Task-Specific Styles(https://arxiv.org/abs/2410.14042)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Prompt compression condenses contexts while maintaining their informativeness for different usage scenarios. It not only shortens the inference time and reduces computational costs during the usage of large language models, but also lowers expenses when using closed-source models. In a preliminary study, we discover that when instructing language models to compress prompts, different compression styles (e.g., extractive or abstractive) impact performance of compressed prompts on downstream tasks. Building on this insight, we propose Style-Compress, a lightweight framework that adapts a smaller language model to compress prompts for a larger model on a new task without additional training. Our approach iteratively generates and selects effective compressed prompts as task-specific demonstrations through style variation and in-context learning, enabling smaller models to act as efficient compressors with task-specific examples. Style-Compress outperforms two baseline compression models in four tasks: original prompt reconstruction, text summarization, multi-hop QA, and CoT reasoning. In addition, with only 10 samples and 100 queries for adaptation, prompts compressed by Style-Compress achieve performance on par with or better than original prompts at a compression ratio of 0.25 or 0.5.</li>
</ul>

<h3>Title: Efficient Retrieval of Temporal Event Sequences from Textual Descriptions</h3>
<ul>
<li><strong>Authors: </strong>Zefang Liu, Yinzhu Quan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14043">https://arxiv.org/abs/2410.14043</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14043">https://arxiv.org/pdf/2410.14043</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14043]] Efficient Retrieval of Temporal Event Sequences from Textual Descriptions(https://arxiv.org/abs/2410.14043)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieving temporal event sequences from textual descriptions is essential for applications such as analyzing e-commerce behavior, monitoring social media activities, and tracking criminal incidents. In this paper, we introduce TPP-LLM-Embedding, a unified model for efficiently embedding and retrieving event sequences based on natural language descriptions. Built on the TPP-LLM framework, which integrates large language models with temporal point processes, our model encodes both event types and times, generating a sequence-level representation through pooling. Textual descriptions are embedded using the same architecture, ensuring a shared embedding space for both sequences and descriptions. We optimize a contrastive loss based on similarity between these embeddings, bringing matching pairs closer and separating non-matching ones. TPP-LLM-Embedding enables efficient retrieval and demonstrates superior performance compared to baseline models across diverse datasets.</li>
</ul>

<h3>Title: Learning Metadata-Agnostic Representations for Text-to-SQL In-Context Example Selection</h3>
<ul>
<li><strong>Authors: </strong>Chuhong Mai, Ro-ee Tal, Thahir Mohamed</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14049">https://arxiv.org/abs/2410.14049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14049">https://arxiv.org/pdf/2410.14049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14049]] Learning Metadata-Agnostic Representations for Text-to-SQL In-Context Example Selection(https://arxiv.org/abs/2410.14049)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) is a powerful paradigm where large language models (LLMs) benefit from task demonstrations added to the prompt. Yet, selecting optimal demonstrations is not trivial, especially for complex or multi-modal tasks where input and output distributions differ. We hypothesize that forming task-specific representations of the input is key. In this paper, we propose a method to align representations of natural language questions and those of SQL queries in a shared embedding space. Our technique, dubbed MARLO - Metadata-Agnostic Representation Learning for Text-tO-SQL - uses query structure to model querying intent without over-indexing on underlying database metadata (i.e. tables, columns, or domain-specific entities of a database referenced in the question or query). This allows MARLO to select examples that are structurally and semantically relevant for the task rather than examples that are spuriously related to a certain domain or question phrasing. When used to retrieve examples based on question similarity, MARLO shows superior performance compared to generic embedding models (on average +2.9\%pt. in execution accuracy) on the Spider benchmark. It also outperforms the next best method that masks metadata information by +0.8\%pt. in execution accuracy on average, while imposing a significantly lower inference latency.</li>
</ul>

<h3>Title: Learning Multimodal Cues of Children's Uncertainty</h3>
<ul>
<li><strong>Authors: </strong>Qi Cheng, Mert ƒ∞nan, Rahma Mbarki, Grace Grmek, Theresa Choi, Yiming Sun, Kimele Persaud, Jenny Wang, Malihe Alikhani</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV, cs.CY, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14050">https://arxiv.org/abs/2410.14050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14050">https://arxiv.org/pdf/2410.14050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14050]] Learning Multimodal Cues of Children's Uncertainty(https://arxiv.org/abs/2410.14050)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Understanding uncertainty plays a critical role in achieving common ground (Clark et al.,1983). This is especially important for multimodal AI systems that collaborate with users to solve a problem or guide the user through a challenging concept. In this work, for the first time, we present a dataset annotated in collaboration with developmental and cognitive psychologists for the purpose of studying nonverbal cues of uncertainty. We then present an analysis of the data, studying different roles of uncertainty and its relationship with task difficulty and performance. Lastly, we present a multimodal machine learning model that can predict uncertainty given a real-time video clip of a participant, which we find improves upon a baseline multimodal transformer model. This work informs research on cognitive coordination between human-human and human-AI and has broad implications for gesture understanding and generation. The anonymized version of our data and code will be publicly available upon the completion of the required consent forms and data sheets.</li>
</ul>

<h3>Title: From Isolated Conversations to Hierarchical Schemas: Dynamic Tree Memory Representation for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Alireza Rezazadeh, Zichao Li, Wei Wei, Yujia Bao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14052">https://arxiv.org/abs/2410.14052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14052">https://arxiv.org/pdf/2410.14052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14052]] From Isolated Conversations to Hierarchical Schemas: Dynamic Tree Memory Representation for LLMs(https://arxiv.org/abs/2410.14052)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models have significantly improved their context windows, yet challenges in effective long-term memory management remain. We introduce MemTree, an algorithm that leverages a dynamic, tree-structured memory representation to optimize the organization, retrieval, and integration of information, akin to human cognitive schemas. MemTree organizes memory hierarchically, with each node encapsulating aggregated textual content, corresponding semantic embeddings, and varying abstraction levels across the tree's depths. Our algorithm dynamically adapts this memory structure by computing and comparing semantic embeddings of new and existing information to enrich the model's context-awareness. This approach allows MemTree to handle complex reasoning and extended interactions more effectively than traditional memory augmentation methods, which often rely on flat lookup tables. Evaluations on benchmarks for multi-turn dialogue understanding and document question answering show that MemTree significantly enhances performance in scenarios that demand structured memory management.</li>
</ul>

<h3>Title: Towards Cross-Cultural Machine Translation with Retrieval-Augmented Generation from Multilingual Knowledge Graphs</h3>
<ul>
<li><strong>Authors: </strong>Simone Conia, Daniel Lee, Min Li, Umar Farooq Minhas, Saloni Potdar, Yunyao Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14057">https://arxiv.org/abs/2410.14057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14057">https://arxiv.org/pdf/2410.14057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14057]] Towards Cross-Cultural Machine Translation with Retrieval-Augmented Generation from Multilingual Knowledge Graphs(https://arxiv.org/abs/2410.14057)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Translating text that contains entity names is a challenging task, as cultural-related references can vary significantly across languages. These variations may also be caused by transcreation, an adaptation process that entails more than transliteration and word-for-word translation. In this paper, we address the problem of cross-cultural translation on two fronts: (i) we introduce XC-Translate, the first large-scale, manually-created benchmark for machine translation that focuses on text that contains potentially culturally-nuanced entity names, and (ii) we propose KG-MT, a novel end-to-end method to integrate information from a multilingual knowledge graph into a neural machine translation model by leveraging a dense retrieval mechanism. Our experiments and analyses show that current machine translation systems and large language models still struggle to translate texts containing entity names, whereas KG-MT outperforms state-of-the-art approaches by a large margin, obtaining a 129% and 62% relative improvement compared to NLLB-200 and GPT-4, respectively.</li>
</ul>

<h3>Title: Data-driven rainfall prediction at a regional scale: a case study with Ghana</h3>
<ul>
<li><strong>Authors: </strong>Indrajit Kalita, Lucia Vilallonga, Yves Atchade</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14062">https://arxiv.org/abs/2410.14062</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14062">https://arxiv.org/pdf/2410.14062</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14062]] Data-driven rainfall prediction at a regional scale: a case study with Ghana(https://arxiv.org/abs/2410.14062)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>With a warming planet, tropical regions are expected to experience the brunt of climate change, with more intense and more volatile rainfall events. Currently, state-of-the-art numerical weather prediction (NWP) models are known to struggle to produce skillful rainfall forecasts in tropical regions of Africa. There is thus a pressing need for improved rainfall forecasting in these regions. Over the last decade or so, the increased availability of large-scale meteorological datasets and the development of powerful machine learning models have opened up new opportunities for data-driven weather forecasting. Focusing on Ghana in this study, we use these tools to develop two U-Net convolutional neural network (CNN) models, to predict 24h rainfall at 12h and 30h lead-time. The models were trained using data from the ERA5 reanalysis dataset, and the GPM-IMERG dataset. A special attention was paid to interpretability. We developed a novel statistical methodology that allowed us to probe the relative importance of the meteorological variables input in our model, offering useful insights into the factors that drive precipitation in the Ghana region. Empirically, we found that our 12h lead-time model has performances that match, and in some accounts are better than the 18h lead-time forecasts produced by the ECMWF (as available in the TIGGE dataset). We also found that combining our data-driven model with classical NWP further improves forecast accuracy.</li>
</ul>

<h3>Title: FaceSaliencyAug: Mitigating Geographic, Gender and Stereotypical Biases via Saliency-Based Data Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Teerath Kumar, Alessandra Mileo, Malika Bendechache</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14070">https://arxiv.org/abs/2410.14070</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14070">https://arxiv.org/pdf/2410.14070</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14070]] FaceSaliencyAug: Mitigating Geographic, Gender and Stereotypical Biases via Saliency-Based Data Augmentation(https://arxiv.org/abs/2410.14070)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, transformer</a></li>
<li><strong>Abstract: </strong>Geographical, gender and stereotypical biases in computer vision models pose significant challenges to their performance and fairness. {In this study, we present an approach named FaceSaliencyAug aimed at addressing the gender bias in} {Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). Leveraging the salient regions} { of faces detected by saliency, the propose approach mitigates geographical and stereotypical biases } {in the datasets. FaceSaliencyAug} randomly selects masks from a predefined search space and applies them to the salient region of face images, subsequently restoring the original image with masked salient region. {The proposed} augmentation strategy enhances data diversity, thereby improving model performance and debiasing effects. We quantify dataset diversity using Image Similarity Score (ISS) across five datasets, including Flickr Faces HQ (FFHQ), WIKI, IMDB, Labelled Faces in the Wild (LFW), UTK Faces, and Diverse Dataset. The proposed approach demonstrates superior diversity metrics, as evaluated by ISS-intra and ISS-inter algorithms. Furthermore, we evaluate the effectiveness of our approach in mitigating gender bias on CEO, Engineer, Nurse, and School Teacher datasets. We use the Image-Image Association Score (IIAS) to measure gender bias in these occupations. Our experiments reveal a reduction in gender bias for both CNNs and ViTs, indicating the efficacy of our method in promoting fairness and inclusivity in computer vision models.</li>
</ul>

<h3>Title: Efficient Vision-Language Models by Summarizing Visual Tokens into Compact Registers</h3>
<ul>
<li><strong>Authors: </strong>Yuxin Wen, Qingqing Cao, Qichen Fu, Sachin Mehta, Mahyar Najibi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14072">https://arxiv.org/abs/2410.14072</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14072">https://arxiv.org/pdf/2410.14072</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14072]] Efficient Vision-Language Models by Summarizing Visual Tokens into Compact Registers(https://arxiv.org/abs/2410.14072)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Recent advancements in vision-language models (VLMs) have expanded their potential for real-world applications, enabling these models to perform complex reasoning on images. In the widely used fully autoregressive transformer-based models like LLaVA, projected visual tokens are prepended to textual tokens. Oftentimes, visual tokens are significantly more than prompt tokens, resulting in increased computational overhead during both training and inference. In this paper, we propose Visual Compact Token Registers (Victor), a method that reduces the number of visual tokens by summarizing them into a smaller set of register tokens. Victor adds a few learnable register tokens after the visual tokens and summarizes the visual information into these registers using the first few layers in the language tower of VLMs. After these few layers, all visual tokens are discarded, significantly improving computational efficiency for both training and inference. Notably, our method is easy to implement and requires a small number of new trainable parameters with minimal impact on model performance. In our experiment, with merely 8 visual registers--about 1% of the original tokens--Victor shows less than a 4% accuracy drop while reducing the total training time by 43% and boosting the inference throughput by 3.3X.</li>
</ul>

<h3>Title: FedPAE: Peer-Adaptive Ensemble Learning for Asynchronous and Model-Heterogeneous Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Brianna Mueller, W. Nick Street, Stephen Baek, Qihang Lin, Jingyi Yang, Yankun Huang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14075">https://arxiv.org/abs/2410.14075</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14075">https://arxiv.org/pdf/2410.14075</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14075]] FedPAE: Peer-Adaptive Ensemble Learning for Asynchronous and Model-Heterogeneous Federated Learning(https://arxiv.org/abs/2410.14075)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated learning (FL) enables multiple clients with distributed data sources to collaboratively train a shared model without compromising data privacy. However, existing FL paradigms face challenges due to heterogeneity in client data distributions and system capabilities. Personalized federated learning (pFL) has been proposed to mitigate these problems, but often requires a shared model architecture and a central entity for parameter aggregation, resulting in scalability and communication issues. More recently, model-heterogeneous FL has gained attention due to its ability to support diverse client models, but existing methods are limited by their dependence on a centralized framework, synchronized training, and publicly available datasets. To address these limitations, we introduce Federated Peer-Adaptive Ensemble Learning (FedPAE), a fully decentralized pFL algorithm that supports model heterogeneity and asynchronous learning. Our approach utilizes a peer-to-peer model sharing mechanism and ensemble selection to achieve a more refined balance between local and global information. Experimental results show that FedPAE outperforms existing state-of-the-art pFL algorithms, effectively managing diverse client capabilities and demonstrating robustness against statistical heterogeneity.</li>
</ul>

<h3>Title: Interpreting Inflammation Prediction Model via Tag-based Cohort Explanation</h3>
<ul>
<li><strong>Authors: </strong>Fanyu Meng, Jules Larke, Xin Liu, Zhaodan Kong, Xin Chen, Danielle Lemay, Ilias Tagkopoulos</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14082">https://arxiv.org/abs/2410.14082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14082">https://arxiv.org/pdf/2410.14082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14082]] Interpreting Inflammation Prediction Model via Tag-based Cohort Explanation(https://arxiv.org/abs/2410.14082)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Machine learning is revolutionizing nutrition science by enabling systems to learn from data and make intelligent decisions. However, the complexity of these models often leads to challenges in understanding their decision-making processes, necessitating the development of explainability techniques to foster trust and increase model transparency. An under-explored type of explanation is cohort explanation, which provides explanations to groups of instances with similar characteristics. Unlike traditional methods that focus on individual explanations or global model behavior, cohort explainability bridges the gap by providing unique insights at an intermediate granularity. We propose a novel framework for identifying cohorts within a dataset based on local feature importance scores, aiming to generate concise descriptions of the clusters via tags. We evaluate our framework on a food-based inflammation prediction model and demonstrated that the framework can generate reliable explanations that match domain knowledge.</li>
</ul>

<h3>Title: SAMReg: SAM-enabled Image Registration with ROI-based Correspondence</h3>
<ul>
<li><strong>Authors: </strong>Shiqi Huang, Tingfa Xu, Ziyi Shen, Shaheer Ullah Saeed, Wen Yan, Dean Barratt, Yipeng Hu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14083">https://arxiv.org/abs/2410.14083</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14083">https://arxiv.org/pdf/2410.14083</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14083]] SAMReg: SAM-enabled Image Registration with ROI-based Correspondence(https://arxiv.org/abs/2410.14083)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>This paper describes a new spatial correspondence representation based on paired regions-of-interest (ROIs), for medical image registration. The distinct properties of the proposed ROI-based correspondence are discussed, in the context of potential benefits in clinical applications following image registration, compared with alternative correspondence-representing approaches, such as those based on sampled displacements and spatial transformation functions. These benefits include a clear connection between learning-based image registration and segmentation, which in turn motivates two cases of image registration approaches using (pre-)trained segmentation networks. Based on the segment anything model (SAM), a vision foundation model for segmentation, we develop a new registration algorithm SAMReg, which does not require any training (or training data), gradient-based fine-tuning or prompt engineering. The proposed SAMReg models are evaluated across five real-world applications, including intra-subject registration tasks with cardiac MR and lung CT, challenging inter-subject registration scenarios with prostate MR and retinal imaging, and an additional evaluation with a non-clinical example with aerial image registration. The proposed methods outperform both intensity-based iterative algorithms and DDF-predicting learning-based networks across tested metrics including Dice and target registration errors on anatomical structures, and further demonstrates competitive performance compared to weakly-supervised registration approaches that rely on fully-segmented training data. Open source code and examples are available at: this https URL.</li>
</ul>

<h3>Title: In-context learning and Occam's razor</h3>
<ul>
<li><strong>Authors: </strong>Eric Elmoznino, Tom Marty, Tejas Kasetty, Leo Gagnon, Sarthak Mittal, Mahan Fathi, Dhanya Sridhar, Guillaume Lajoie</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14086">https://arxiv.org/abs/2410.14086</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14086">https://arxiv.org/pdf/2410.14086</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14086]] In-context learning and Occam's razor(https://arxiv.org/abs/2410.14086)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>The goal of machine learning is generalization. While the No Free Lunch Theorem states that we cannot obtain theoretical guarantees for generalization without further assumptions, in practice we observe that simple models which explain the training data generalize best: a principle called Occam's razor. Despite the need for simple models, most current approaches in machine learning only minimize the training error, and at best indirectly promote simplicity through regularization or architecture design. Here, we draw a connection between Occam's razor and in-context learning: an emergent ability of certain sequence models like Transformers to learn at inference time from past observations in a sequence. In particular, we show that the next-token prediction loss used to train in-context learners is directly equivalent to a data compression technique called prequential coding, and that minimizing this loss amounts to jointly minimizing both the training error and the complexity of the model that was implicitly learned from context. Our theory and the empirical experiments we use to support it not only provide a normative account of in-context learning, but also elucidate the shortcomings of current in-context learning methods, suggesting ways in which they can be improved. We make our code available at this https URL.</li>
</ul>

<h3>Title: MMAD-Purify: A Precision-Optimized Framework for Efficient and Scalable Multi-Modal Attacks</h3>
<ul>
<li><strong>Authors: </strong>Xinxin Liu, Zhongliang Guo, Siyuan Huang, Chun Pong Lau</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14089">https://arxiv.org/abs/2410.14089</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14089">https://arxiv.org/pdf/2410.14089</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14089]] MMAD-Purify: A Precision-Optimized Framework for Efficient and Scalable Multi-Modal Attacks(https://arxiv.org/abs/2410.14089)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Neural networks have achieved remarkable performance across a wide range of tasks, yet they remain susceptible to adversarial perturbations, which pose significant risks in safety-critical applications. With the rise of multimodality, diffusion models have emerged as powerful tools not only for generative tasks but also for various applications such as image editing, inpainting, and super-resolution. However, these models still lack robustness due to limited research on attacking them to enhance their resilience. Traditional attack techniques, such as gradient-based adversarial attacks and diffusion model-based methods, are hindered by computational inefficiencies and scalability issues due to their iterative nature. To address these challenges, we introduce an innovative framework that leverages the distilled backbone of diffusion models and incorporates a precision-optimized noise predictor to enhance the effectiveness of our attack framework. This approach not only enhances the attack's potency but also significantly reduces computational costs. Our framework provides a cutting-edge solution for multi-modal adversarial attacks, ensuring reduced latency and the generation of high-fidelity adversarial examples with superior success rates. Furthermore, we demonstrate that our framework achieves outstanding transferability and robustness against purification defenses, outperforming existing gradient-based attack models in both effectiveness and efficiency.</li>
</ul>

<h3>Title: ST-MoE-BERT: A Spatial-Temporal Mixture-of-Experts Framework for Long-Term Cross-City Mobility Prediction</h3>
<ul>
<li><strong>Authors: </strong>Haoyu He, Haozheng Luo, Qi R. Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14099">https://arxiv.org/abs/2410.14099</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14099">https://arxiv.org/pdf/2410.14099</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14099]] ST-MoE-BERT: A Spatial-Temporal Mixture-of-Experts Framework for Long-Term Cross-City Mobility Prediction(https://arxiv.org/abs/2410.14099)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Predicting human mobility across multiple cities presents significant challenges due to the complex and diverse spatial-temporal dynamics inherent in different urban environments. In this study, we propose a robust approach to predict human mobility patterns called ST-MoE-BERT. Compared to existing methods, our approach frames the prediction task as a spatial-temporal classification problem. Our methodology integrates the Mixture-of-Experts architecture with BERT model to capture complex mobility dynamics and perform the downstream human mobility prediction task. Additionally, transfer learning is integrated to solve the challenge of data scarcity in cross-city prediction. We demonstrate the effectiveness of the proposed model on GEO-BLEU and DTW, comparing it to several state-of-the-art methods. Notably, ST-MoE-BERT achieves an average improvement of 8.29%.</li>
</ul>

<h3>Title: Beyond Dataset Watermarking: Model-Level Copyright Protection for Code Summarization Models</h3>
<ul>
<li><strong>Authors: </strong>Jiale Zhang, Haoxuan Li, Di Wu, Xiaobing Sun, Qinghua Lu, Guodong Long</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14102">https://arxiv.org/abs/2410.14102</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14102">https://arxiv.org/pdf/2410.14102</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14102]] Beyond Dataset Watermarking: Model-Level Copyright Protection for Code Summarization Models(https://arxiv.org/abs/2410.14102)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, watermark</a></li>
<li><strong>Abstract: </strong>Code Summarization Model (CSM) has been widely used in code production, such as online and web programming for PHP and Javascript. CSMs are essential tools in code production, enhancing software development efficiency and driving innovation in automated code analysis. However, CSMs face risks of exploitation by unauthorized users, particularly in an online environment where CSMs can be easily shared and disseminated. To address these risks, digital watermarks offer a promising solution by embedding imperceptible signatures within the models to assert copyright ownership and track unauthorized usage. Traditional watermarking for CSM copyright protection faces two main challenges: 1) dataset watermarking methods require separate design of triggers and watermark features based on the characteristics of different programming languages, which not only increases the computation complexity but also leads to a lack of generalization, 2) existing watermarks based on code style transformation are easily identifiable by automated detection, demonstrating poor concealment. To tackle these issues, we propose ModMark , a novel model-level digital watermark embedding method. Specifically, by fine-tuning the tokenizer, ModMark achieves cross-language generalization while reducing the complexity of watermark design. Moreover, we employ code noise injection techniques to effectively prevent trigger detection. Experimental results show that our method can achieve 100% watermark verification rate across various programming languages' CSMs, and the concealment and effectiveness of ModMark can also be guaranteed.</li>
</ul>

<h3>Title: Extreme Precipitation Nowcasting using Multi-Task Latent Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Li Chaorong, Ling Xudong, Yang Qiang, Qin Fengqing, Huang Yuanyuan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14103">https://arxiv.org/abs/2410.14103</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14103">https://arxiv.org/pdf/2410.14103</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14103]] Extreme Precipitation Nowcasting using Multi-Task Latent Diffusion Models(https://arxiv.org/abs/2410.14103)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Deep learning models have made remarkable strides in precipitation prediction, yet they continue to struggle with capturing the spatial details of the features of radar images, particularly over high precipitation intensity areas. This shortcoming is evident in the form of low forecast accuracy in the spatial positioning of radar echo images across varying precipitation intensity regions. To address this challenge, we introduce the multi-task latent diffusion model(MTLDM), a novel approach for precipitation prediction. The basic concept of the MTLDM is based on the understanding that the radar image representing precipitation is the result of multiple factors. Therefore, we adopt a divide-and-conquer approach, that is, we decompose the radar image using decomposition technology and then predict the decomposed sub-images separately. We conceptualize the precipitation image as a composition of various components corresponding to different precipitation intensities. The MTLDM decomposes the precipitation image into these distinct components and employs a dedicated task to predict each one. This method enables spatiotemporally consistent prediction of real-world precipitation areas up to 5-80 min in advance, outperforming existing state-of-the-art techniques across multiple evaluation metrics.</li>
</ul>

<h3>Title: DMGNN: Detecting and Mitigating Backdoor Attacks in Graph Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Hao Sui, Bing Chen, Jiale Zhang, Chengcheng Zhu, Di Wu, Qinghua Lu, Guodong Long</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14105">https://arxiv.org/abs/2410.14105</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14105">https://arxiv.org/pdf/2410.14105</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14105]] DMGNN: Detecting and Mitigating Backdoor Attacks in Graph Neural Networks(https://arxiv.org/abs/2410.14105)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, steal</a></li>
<li><strong>Abstract: </strong>Recent studies have revealed that GNNs are highly susceptible to multiple adversarial attacks. Among these, graph backdoor attacks pose one of the most prominent threats, where attackers cause models to misclassify by learning the backdoored features with injected triggers and modified target labels during the training phase. Based on the features of the triggers, these attacks can be categorized into out-of-distribution (OOD) and in-distribution (ID) graph backdoor attacks, triggers with notable differences from the clean sample feature distributions constitute OOD backdoor attacks, whereas the triggers in ID backdoor attacks are nearly identical to the clean sample feature distributions. Existing methods can successfully defend against OOD backdoor attacks by comparing the feature distribution of triggers and clean samples but fail to mitigate stealthy ID backdoor attacks. Due to the lack of proper supervision signals, the main task accuracy is negatively affected in defending against ID backdoor attacks. To bridge this gap, we propose DMGNN against OOD and ID graph backdoor attacks that can powerfully eliminate stealthiness to guarantee defense effectiveness and improve the model performance. Specifically, DMGNN can easily identify the hidden ID and OOD triggers via predicting label transitions based on counterfactual explanation. To further filter the diversity of generated explainable graphs and erase the influence of the trigger features, we present a reverse sampling pruning method to screen and discard the triggers directly on the data level. Extensive experimental evaluations on open graph datasets demonstrate that DMGNN far outperforms the state-of-the-art (SOTA) defense methods, reducing the attack success rate to 5% with almost negligible degradation in model performance (within 3.5%).</li>
</ul>

<h3>Title: Transfer Learning on Transformers for Building Energy Consumption Forecasting -- A Comparative Study</h3>
<ul>
<li><strong>Authors: </strong>Robert Spencer, Surangika Ranathunga, Mikael Boulic, Andries (Hennie)van Heerden, Teo Susnjak</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14107">https://arxiv.org/abs/2410.14107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14107">https://arxiv.org/pdf/2410.14107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14107]] Transfer Learning on Transformers for Building Energy Consumption Forecasting -- A Comparative Study(https://arxiv.org/abs/2410.14107)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This study investigates the application of Transfer Learning (TL) on Transformer architectures to enhance building energy consumption forecasting. Transformers are a relatively new deep learning architecture, which has served as the foundation for groundbreaking technologies such as ChatGPT. While TL has been studied in the past, these studies considered either one TL strategy or used older deep learning models such as Recurrent Neural Networks or Convolutional Neural Networks. Here, we carry out an extensive empirical study on six different TL strategies and analyse their performance under varying feature spaces. In addition to the vanilla Transformer architecture, we also experiment with Informer and PatchTST, specifically designed for time series forecasting. We use 16 datasets from the Building Data Genome Project 2 to create building energy consumption forecasting models. Experiment results reveal that while TL is generally beneficial, especially when the target domain has no data, careful selection of the exact TL strategy should be made to gain the maximum benefit. This decision largely depends on the feature space properties such as the recorded weather features. We also note that PatchTST outperforms the other two Transformer variants (vanilla Transformer and Informer). We believe our findings would assist researchers in making informed decision in using TL and transformer architectures for building energy consumption forecasting.</li>
</ul>

<h3>Title: Improving Graph Neural Networks by Learning Continuous Edge Directions</h3>
<ul>
<li><strong>Authors: </strong>Seong Ho Pahng, Sahand Hormoz</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14109">https://arxiv.org/abs/2410.14109</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14109">https://arxiv.org/pdf/2410.14109</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14109]] Improving Graph Neural Networks by Learning Continuous Edge Directions(https://arxiv.org/abs/2410.14109)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) traditionally employ a message-passing mechanism that resembles diffusion over undirected graphs, which often leads to homogenization of node features and reduced discriminative power in tasks such as node classification. Our key insight for addressing this limitation is to assign fuzzy edge directions -- that can vary continuously from node $i$ pointing to node $j$ to vice versa -- to the edges of a graph so that features can preferentially flow in one direction between nodes to enable long-range information transmission across the graph. We also introduce a novel complex-valued Laplacian for directed graphs with fuzzy edges where the real and imaginary parts represent information flow in opposite directions. Using this Laplacian, we propose a general framework, called Continuous Edge Direction (CoED) GNN, for learning on graphs with fuzzy edges and prove its expressivity limits using a generalization of the Weisfeiler-Leman (WL) graph isomorphism test for directed graphs with fuzzy edges. Our architecture aggregates neighbor features scaled by the learned edge directions and processes the aggregated messages from in-neighbors and out-neighbors separately alongside the self-features of the nodes. Since continuous edge directions are differentiable, they can be learned jointly with the GNN weights via gradient-based optimization. CoED GNN is particularly well-suited for graph ensemble data where the graph structure remains fixed but multiple realizations of node features are available, such as in gene regulatory networks, web connectivity graphs, and power grids. We demonstrate through extensive experiments on both synthetic and real datasets that learning continuous edge directions significantly improves performance both for undirected and directed graphs compared with existing methods.</li>
</ul>

<h3>Title: A Communication and Computation Efficient Fully First-order Method for Decentralized Bilevel Optimization</h3>
<ul>
<li><strong>Authors: </strong>Min Wen, Chengchang Liu, Ahmed Abdelmoniem, Yipeng Zhou, Yuedong Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14115">https://arxiv.org/abs/2410.14115</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14115">https://arxiv.org/pdf/2410.14115</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14115]] A Communication and Computation Efficient Fully First-order Method for Decentralized Bilevel Optimization(https://arxiv.org/abs/2410.14115)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Bilevel optimization, crucial for hyperparameter tuning, meta-learning and reinforcement learning, remains less explored in the decentralized learning paradigm, such as decentralized federated learning (DFL). Typically, decentralized bilevel methods rely on both gradients and Hessian matrices to approximate hypergradients of upper-level models. However, acquiring and sharing the second-order oracle is compute and communication intensive. % and sharing this information incurs heavy communication overhead. To overcome these challenges, this paper introduces a fully first-order decentralized method for decentralized Bilevel optimization, $\text{C}^2$DFB which is both compute- and communicate-efficient. In $\text{C}^2$DFB, each learning node optimizes a min-min-max problem to approximate hypergradient by exclusively using gradients information. To reduce the traffic load at the inner-loop of solving the lower-level problem, $\text{C}^2$DFB incorporates a lightweight communication protocol for efficiently transmitting compressed residuals of local parameters. % during the inner loops. Rigorous theoretical analysis ensures its convergence % of the algorithm, indicating a first-order oracle calls of $\tilde{\mathcal{O}}(\epsilon^{-4})$. Experiments on hyperparameter tuning and hyper-representation tasks validate the superiority of $\text{C}^2$DFB across various typologies and heterogeneous data distributions.</li>
</ul>

<h3>Title: FedMSE: Federated learning for IoT network intrusion detection</h3>
<ul>
<li><strong>Authors: </strong>Van Tuan Nguyen, Razvan Beuran</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14121">https://arxiv.org/abs/2410.14121</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14121">https://arxiv.org/pdf/2410.14121</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14121]] FedMSE: Federated learning for IoT network intrusion detection(https://arxiv.org/abs/2410.14121)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, robust, federate</a></li>
<li><strong>Abstract: </strong>This paper proposes a novel federated learning approach for improving IoT network intrusion detection. The rise of IoT has expanded the cyber attack surface, making traditional centralized machine learning methods insufficient due to concerns about data availability, computational resources, transfer costs, and especially privacy preservation. A semi-supervised federated learning model was developed to overcome these issues, combining the Shrink Autoencoder and Centroid one-class classifier (SAE-CEN). This approach enhances the performance of intrusion detection by effectively representing normal network data and accurately identifying anomalies in the decentralized strategy. Additionally, a mean square error-based aggregation algorithm (MSEAvg) was introduced to improve global model performance by prioritizing more accurate local models. The results obtained in our experimental setup, which uses various settings relying on the N-BaIoT dataset and Dirichlet distribution, demonstrate significant improvements in real-world heterogeneous IoT networks in detection accuracy from 93.98$\pm$2.90 to 97.30$\pm$0.49, reduced learning costs when requiring only 50\% of gateways participating in the training process, and robustness in large-scale networks.</li>
</ul>

<h3>Title: ViConsFormer: Constituting Meaningful Phrases of Scene Texts using Transformer-based Method in Vietnamese Text-based Visual Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Nghia Hieu Nguyen, Tho Thanh Quan, Ngan Luu-Thuy Nguyen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14132">https://arxiv.org/abs/2410.14132</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14132">https://arxiv.org/pdf/2410.14132</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14132]] ViConsFormer: Constituting Meaningful Phrases of Scene Texts using Transformer-based Method in Vietnamese Text-based Visual Question Answering(https://arxiv.org/abs/2410.14132)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Text-based VQA is a challenging task that requires machines to use scene texts in given images to yield the most appropriate answer for the given question. The main challenge of text-based VQA is exploiting the meaning and information from scene texts. Recent studies tackled this challenge by considering the spatial information of scene texts in images via embedding 2D coordinates of their bounding boxes. In this study, we follow the definition of meaning from linguistics to introduce a novel method that effectively exploits the information from scene texts written in Vietnamese. Experimental results show that our proposed method obtains state-of-the-art results on two large-scale Vietnamese Text-based VQA datasets. The implementation can be found at this link.</li>
</ul>

<h3>Title: Hierarchical Conditional Multi-Task Learning for Streamflow Modeling</h3>
<ul>
<li><strong>Authors: </strong>Shaoming Xu, Arvind Renganathan, Ankush Khandelwal, Rahul Ghosh, Xiang Li, Licheng Liu, Kshitij Tayal, Peter Harrington, Xiaowei Jia, Zhenong Jin, Jonh Nieber, Vipin Kumar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14137">https://arxiv.org/abs/2410.14137</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14137">https://arxiv.org/pdf/2410.14137</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14137]] Hierarchical Conditional Multi-Task Learning for Streamflow Modeling(https://arxiv.org/abs/2410.14137)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Streamflow, vital for water resource management, is governed by complex hydrological systems involving intermediate processes driven by meteorological forces. While deep learning models have achieved state-of-the-art results of streamflow prediction, their end-to-end single-task learning approach often fails to capture the causal relationships within these systems. To address this, we propose Hierarchical Conditional Multi-Task Learning (HCMTL), a hierarchical approach that jointly models soil water and snowpack processes based on their causal connections to streamflow. HCMTL utilizes task embeddings to connect network modules, enhancing flexibility and expressiveness while capturing unobserved processes beyond soil water and snowpack. It also incorporates the Conditional Mini-Batch strategy to improve long time series modeling. We compare HCMTL with five baselines on a global dataset. HCMTL's superior performance across hundreds of drainage basins over extended periods shows that integrating domain-specific causal knowledge into deep learning enhances both prediction accuracy and interpretability. This is essential for advancing our understanding of complex hydrological systems and supporting efficient water resource management to mitigate natural disasters like droughts and floods.</li>
</ul>

<h3>Title: ProReason: Multi-Modal Proactive Reasoning with Decoupled Eyesight and Wisdom</h3>
<ul>
<li><strong>Authors: </strong>Jingqi Zhou, Sheng Wang, Jingwei Dong, Lei Li, Jiahui Gao, Lingpeng Kong, Chuan Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14138">https://arxiv.org/abs/2410.14138</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14138">https://arxiv.org/pdf/2410.14138</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14138]] ProReason: Multi-Modal Proactive Reasoning with Decoupled Eyesight and Wisdom(https://arxiv.org/abs/2410.14138)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large vision-language models (LVLMs) have witnessed significant progress on visual understanding tasks. However, they often prioritize language knowledge over image information on visual reasoning tasks, incurring performance degradation. To tackle this issue, we first identify the drawbacks of existing solutions (i.e., insufficient and irrelevant visual descriptions, and limited multi-modal capacities). We then decompose visual reasoning process into two stages: visual perception (i.e., eyesight) and textual reasoning (i.e., wisdom), and introduce a novel visual reasoning framework named ProReason. This framework features multi-run proactive perception and decoupled vision-reasoning capabilities. Briefly, given a multi-modal question, ProReason iterates proactive information collection and reasoning until the answer can be concluded with necessary and sufficient visual descriptions. Notably, the disassociation of capabilities allows seamless integration of existing large language models (LLMs) to compensate for the reasoning deficits of LVLMs. Our extensive experiments demonstrate that ProReason outperforms both existing multi-step reasoning frameworks and passive peer methods on a wide range of benchmarks for both open-source and closed-source models. In addition, with the assistance of LLMs, ProReason achieves a performance improvement of up to 15% on MMMU benchmark. Our insights into existing solutions and the decoupled perspective for feasible integration of LLMs illuminate future research on visual reasoning techniques, especially LLM-assisted ones.</li>
</ul>

<h3>Title: A Lightweight Multi Aspect Controlled Text Generation Solution For Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chenyang Zhang, Jiayi Lin, Haibo Tong, Bingxuan Hou, Dongyu Zhang, Jialin Li, Junli Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14144">https://arxiv.org/abs/2410.14144</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14144">https://arxiv.org/pdf/2410.14144</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14144]] A Lightweight Multi Aspect Controlled Text Generation Solution For Large Language Models(https://arxiv.org/abs/2410.14144)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) show remarkable abilities with instruction tuning. However, they fail to achieve ideal tasks when lacking high-quality instruction tuning data on target tasks. Multi-Aspect Controllable Text Generation (MCTG) is a representative task for this dilemma, where aspect datasets are usually biased and correlated. Existing work exploits additional model structures and strategies for solutions, limiting adaptability to LLMs. To activate MCTG ability of LLMs, we propose a lightweight MCTG pipeline based on data augmentation. We analyze bias and correlations in traditional datasets, and address these concerns with augmented control attributes and sentences. Augmented datasets are feasible for instruction tuning. In our experiments, LLMs perform better in MCTG after data augmentation, with a 20% accuracy rise and less aspect correlations.</li>
</ul>

<h3>Title: CAPE: A Chinese Dataset for Appraisal-based Emotional Generation using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>June M. Liu, He Cao, Renliang Sun, Rui Wang, Yu Li, Jiaxing Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14145">https://arxiv.org/abs/2410.14145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14145">https://arxiv.org/pdf/2410.14145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14145]] CAPE: A Chinese Dataset for Appraisal-based Emotional Generation using Large Language Models(https://arxiv.org/abs/2410.14145)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Generating emotionally appropriate responses in conversations with large language models presents a significant challenge due to the complexities of human emotions and cognitive processes, which remain largely underexplored in their critical role in social interactions. In this study, we introduce a two-stage automatic data generation framework to create CAPE, a Chinese dataset named Cognitive Appraisal theory-based Emotional corpus. This corpus facilitates the generation of dialogues with contextually appropriate emotional responses by accounting for diverse personal and situational factors. We propose two tasks utilizing this dataset: emotion prediction and next utterance prediction. Both automated and human evaluations demonstrate that agents trained on our dataset can deliver responses that are more aligned with human emotional expressions. Our study shows the potential for advancing emotional expression in conversational agents, paving the way for more nuanced and meaningful human-computer interactions.</li>
</ul>

<h3>Title: Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in Vision-Language Alignment</h3>
<ul>
<li><strong>Authors: </strong>Chenhang Cui, An Zhang, Yiyang Zhou, Zhaorun Chen, Gelei Deng, Huaxiu Yao, Tat-Seng Chua</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14148">https://arxiv.org/abs/2410.14148</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14148">https://arxiv.org/pdf/2410.14148</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14148]] Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in Vision-Language Alignment(https://arxiv.org/abs/2410.14148)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The recent advancements in large language models (LLMs) and pre-trained vision models have accelerated the development of vision-language large models (VLLMs), enhancing the interaction between visual and linguistic modalities. Despite their notable success across various domains, VLLMs face challenges in modality alignment, which can lead to issues like hallucinations and unsafe content generation. Current alignment techniques often rely on coarse feedback and external datasets, limiting scalability and performance. In this paper, we propose FiSAO (Fine-Grained Self-Alignment Optimization), a novel self-alignment method that utilizes the model's own visual encoder as a fine-grained verifier to improve vision-language alignment without the need for additional data. By leveraging token-level feedback from the vision encoder, FiSAO significantly improves vision-language alignment, even surpassing traditional preference tuning methods that require additional data. Through both theoretical analysis and experimental validation, we demonstrate that FiSAO effectively addresses the misalignment problem in VLLMs, marking the first instance of token-level rewards being applied to such models.</li>
</ul>

<h3>Title: SRAP-Agent: Simulating and Optimizing Scarce Resource Allocation Policy with LLM-based Agent</h3>
<ul>
<li><strong>Authors: </strong>Jiarui Ji, Yang Li, Hongtao Liu, Zhicheng Du, Zhewei Wei, Weiran Shen, Qi Qi, Yankai Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14152">https://arxiv.org/abs/2410.14152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14152">https://arxiv.org/pdf/2410.14152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14152]] SRAP-Agent: Simulating and Optimizing Scarce Resource Allocation Policy with LLM-based Agent(https://arxiv.org/abs/2410.14152)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Public scarce resource allocation plays a crucial role in economics as it directly influences the efficiency and equity in society. Traditional studies including theoretical model-based, empirical study-based and simulation-based methods encounter limitations due to the idealized assumption of complete information and individual rationality, as well as constraints posed by limited available data. In this work, we propose an innovative framework, SRAP-Agent (Simulating and Optimizing Scarce Resource Allocation Policy with LLM-based Agent), which integrates Large Language Models (LLMs) into economic simulations, aiming to bridge the gap between theoretical models and real-world dynamics. Using public housing allocation scenarios as a case study, we conduct extensive policy simulation experiments to verify the feasibility and effectiveness of the SRAP-Agent and employ the Policy Optimization Algorithm with certain optimization objectives. The source code can be found in this https URL</li>
</ul>

<h3>Title: Towards Faithful Natural Language Explanations: A Study Using Activation Patching in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Wei Jie Yeo, Ranjan Satapthy, Erik Cambria</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14155">https://arxiv.org/abs/2410.14155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14155">https://arxiv.org/pdf/2410.14155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14155]] Towards Faithful Natural Language Explanations: A Study Using Activation Patching in Large Language Models(https://arxiv.org/abs/2410.14155)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are capable of generating persuasive Natural Language Explanations (NLEs) to justify their answers. However, the faithfulness of these explanations should not be readily trusted at face value. Recent studies have proposed various methods to measure the faithfulness of NLEs, typically by inserting perturbations at the explanation or feature level. We argue that these approaches are neither comprehensive nor correctly designed according to the established definition of faithfulness. Moreover, we highlight the risks of grounding faithfulness findings on out-of-distribution samples. In this work, we leverage a causal mediation technique called activation patching, to measure the faithfulness of an explanation towards supporting the explained answer. Our proposed metric, Causal Faithfulness quantifies the consistency of causal attributions between explanations and the corresponding model outputs as the indicator of faithfulness. We experimented across models varying from 2B to 27B parameters and found that models that underwent alignment tuning tend to produce more faithful and plausible explanations. We find that Causal Faithfulness is a promising improvement over existing faithfulness tests by taking into account the model's internal computations and avoiding out of distribution concerns that could otherwise undermine the validity of faithfulness assessments. We release the code in \url{this https URL}</li>
</ul>

<h3>Title: Beyond Autoregression: Discrete Diffusion for Complex Reasoning and Planning</h3>
<ul>
<li><strong>Authors: </strong>Jiacheng Ye, Jiahui Gao, Shansan Gong, Lin Zheng, Xin Jiang, Zhenguo Li, Lingpeng Kong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14157">https://arxiv.org/abs/2410.14157</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14157">https://arxiv.org/pdf/2410.14157</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14157]] Beyond Autoregression: Discrete Diffusion for Complex Reasoning and Planning(https://arxiv.org/abs/2410.14157)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Autoregressive language models, despite their impressive capabilities, struggle with complex reasoning and long-term planning tasks. We introduce discrete diffusion models as a novel solution to these challenges. Through the lens of subgoal imbalance, we demonstrate how diffusion models effectively learn difficult subgoals that elude autoregressive approaches. We propose Multi-granularity Diffusion Modeling (MDM), which prioritizes subgoals based on difficulty during learning. On complex tasks like Countdown, Sudoku, and Boolean Satisfiability Problems, MDM significantly outperforms autoregressive models without using search techniques. For instance, MDM achieves 91.5\% and 100\% accuracy on Countdown and Sudoku, respectively, compared to 45.8\% and 20.7\% for autoregressive models. Our work highlights the potential of diffusion-based approaches in advancing AI capabilities for sophisticated language understanding and problem-solving tasks.</li>
</ul>

<h3>Title: Assessing Open-world Forgetting in Generative Image Model Customization</h3>
<ul>
<li><strong>Authors: </strong>H√©ctor Laria, Alex Gomez-Villa, Imad Eddine Marouf, Kai Wang, Bogdan Raducanu, Joost van de Weijer</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14159">https://arxiv.org/abs/2410.14159</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14159">https://arxiv.org/pdf/2410.14159</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14159]] Assessing Open-world Forgetting in Generative Image Model Customization(https://arxiv.org/abs/2410.14159)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in diffusion models have significantly enhanced image generation capabilities. However, customizing these models with new classes often leads to unintended consequences that compromise their reliability. We introduce the concept of open-world forgetting to emphasize the vast scope of these unintended alterations, contrasting it with the well-studied closed-world forgetting, which is measurable by evaluating performance on a limited set of classes or skills. Our research presents the first comprehensive investigation into open-world forgetting in diffusion models, focusing on semantic and appearance drift of representations. We utilize zero-shot classification to analyze semantic drift, revealing that even minor model adaptations lead to unpredictable shifts affecting areas far beyond newly introduced concepts, with dramatic drops in zero-shot classification of up to 60%. Additionally, we observe significant changes in texture and color of generated content when analyzing appearance drift. To address these issues, we propose a mitigation strategy based on functional regularization, designed to preserve original capabilities while accommodating new concepts. Our study aims to raise awareness of unintended changes due to model customization and advocates for the analysis of open-world forgetting in future research on model customization and finetuning methods. Furthermore, we provide insights for developing more robust adaptation methodologies.</li>
</ul>

<h3>Title: Automated Genre-Aware Article Scoring and Feedback Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chihang Wang, Yuxin Dong, Zhenhong Zhang, Ruotong Wang, Shuo Wang, Jiajing Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14165">https://arxiv.org/abs/2410.14165</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14165">https://arxiv.org/pdf/2410.14165</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14165]] Automated Genre-Aware Article Scoring and Feedback Using Large Language Models(https://arxiv.org/abs/2410.14165)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper focuses on the development of an advanced intelligent article scoring system that not only assesses the overall quality of written work but also offers detailed feature-based scoring tailored to various article genres. By integrating the pre-trained BERT model with the large language model Chat-GPT, the system gains a deep understanding of both the content and structure of the text, enabling it to provide a thorough evaluation along with targeted suggestions for improvement. Experimental results demonstrate that this system outperforms traditional scoring methods across multiple public datasets, particularly in feature-based assessments, offering a more accurate reflection of the quality of different article types. Moreover, the system generates personalized feedback to assist users in enhancing their writing skills, underscoring the potential and practical value of automated scoring technologies in educational contexts.</li>
</ul>

<h3>Title: LLM The Genius Paradox: A Linguistic and Math Expert's Struggle with Simple Word-based Counting Problems</h3>
<ul>
<li><strong>Authors: </strong>Nan Xu, Xuezhe Ma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14166">https://arxiv.org/abs/2410.14166</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14166">https://arxiv.org/pdf/2410.14166</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14166]] LLM The Genius Paradox: A Linguistic and Math Expert's Struggle with Simple Word-based Counting Problems(https://arxiv.org/abs/2410.14166)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Interestingly, LLMs yet struggle with some basic tasks that humans find trivial to handle, e.g., counting the number of character r's in the word "strawberry". There are several popular conjectures (e.g., tokenization, architecture and training data) regarding the reason for deficiency of LLMs in simple word-based counting problems, sharing the similar belief that such failure stems from model pretraining hence probably inevitable during deployment. In this paper, we carefully design multiple evaluation settings to investigate validity of prevalent conjectures. Meanwhile, we measure transferability of advanced mathematical and coding reasoning capabilities from specialized LLMs to simple counting tasks. Although specialized LLMs suffer from counting problems as well, we find conjectures about inherent deficiency of LLMs invalid and further seek opportunities to elicit knowledge and capabilities from LLMs that are beneficial to counting tasks. Compared with strategies such as finetuning and in-context learning that are commonly adopted to enhance performance on new or challenging tasks, we show that engaging reasoning is the most robust and efficient way to help LLMs better perceive tasks with more accurate responses. We hope our conjecture validation design could provide insights into the study of future critical failure modes of LLMs. Based on challenges in transferring advanced capabilities to much simpler tasks, we call for more attention to model capability acquisition and evaluation. We also highlight the importance of cultivating consciousness of "reasoning before responding" during model pretraining.</li>
</ul>

<h3>Title: Heavy-Tailed Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Kushagra Pandey, Jaideep Pathak, Yilun Xu, Stephan Mandt, Michael Pritchard, Arash Vahdat, Morteza Mardani</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14171">https://arxiv.org/abs/2410.14171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14171">https://arxiv.org/pdf/2410.14171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14171]] Heavy-Tailed Diffusion Models(https://arxiv.org/abs/2410.14171)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models achieve state-of-the-art generation quality across many applications, but their ability to capture rare or extreme events in heavy-tailed distributions remains unclear. In this work, we show that traditional diffusion and flow-matching models with standard Gaussian priors fail to capture heavy-tailed behavior. We address this by repurposing the diffusion framework for heavy-tail estimation using multivariate Student-t distributions. We develop a tailored perturbation kernel and derive the denoising posterior based on the conditional Student-t distribution for the backward process. Inspired by $\gamma$-divergence for heavy-tailed distributions, we derive a training objective for heavy-tailed denoisers. The resulting framework introduces controllable tail generation using only a single scalar hyperparameter, making it easily tunable for diverse real-world distributions. As specific instantiations of our framework, we introduce t-EDM and t-Flow, extensions of existing diffusion and flow models that employ a Student-t prior. Remarkably, our approach is readily compatible with standard Gaussian diffusion models and requires only minimal code changes. Empirically, we show that our t-EDM and t-Flow outperform standard diffusion models in heavy-tail estimation on high-resolution weather datasets in which generating rare and extreme events is crucial.</li>
</ul>

<h3>Title: MultiChartQA: Benchmarking Vision-Language Models on Multi-Chart Problems</h3>
<ul>
<li><strong>Authors: </strong>Zifeng Zhu, Mengzhao Jia, Zhihan Zhang, Lang Li, Meng Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14179">https://arxiv.org/abs/2410.14179</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14179">https://arxiv.org/pdf/2410.14179</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14179]] MultiChartQA: Benchmarking Vision-Language Models on Multi-Chart Problems(https://arxiv.org/abs/2410.14179)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have demonstrated impressive abilities across various tasks, including visual question answering and chart comprehension, yet existing benchmarks for chart-related tasks fall short in capturing the complexity of real-world multi-chart scenarios. Current benchmarks primarily focus on single-chart tasks, neglecting the multi-hop reasoning required to extract and integrate information from multiple charts, which is essential in practical applications. To fill this gap, we introduce MultiChartQA, a benchmark that evaluates MLLMs' capabilities in four key areas: direct question answering, parallel question answering, comparative reasoning, and sequential reasoning. Our evaluation of a wide range of MLLMs reveals significant performance gaps compared to humans. These results highlight the challenges in multi-chart comprehension and the potential of MultiChartQA to drive advancements in this field. Our code and data are available at this https URL</li>
</ul>

<h3>Title: XForecast: Evaluating Natural Language Explanations for Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Taha Aksu, Chenghao Liu, Amrita Saha, Sarah Tan, Caiming Xiong, Doyen Sahoo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14180">https://arxiv.org/abs/2410.14180</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14180">https://arxiv.org/pdf/2410.14180</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14180]] XForecast: Evaluating Natural Language Explanations for Time Series Forecasting(https://arxiv.org/abs/2410.14180)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Time series forecasting aids decision-making, especially for stakeholders who rely on accurate predictions, making it very important to understand and explain these models to ensure informed decisions. Traditional explainable AI (XAI) methods, which underline feature or temporal importance, often require expert knowledge. In contrast, natural language explanations (NLEs) are more accessible to laypeople. However, evaluating forecast NLEs is difficult due to the complex causal relationships in time series data. To address this, we introduce two new performance metrics based on simulatability, assessing how well a human surrogate can predict model forecasts using the explanations. Experiments show these metrics differentiate good from poor explanations and align with human judgments. Utilizing these metrics, we further evaluate the ability of state-of-the-art large language models (LLMs) to generate explanations for time series data, finding that numerical reasoning, rather than model size, is the main factor influencing explanation quality.</li>
</ul>

<h3>Title: LabSafety Bench: Benchmarking LLMs on Safety Issues in Scientific Labs</h3>
<ul>
<li><strong>Authors: </strong>Yujun Zhou, Jingdong Yang, Kehan Guo, Pin-Yu Chen, Tian Gao, Werner Geyer, Nuno Moniz, Nitesh V Chawla, Xiangliang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14182">https://arxiv.org/abs/2410.14182</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14182">https://arxiv.org/pdf/2410.14182</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14182]] LabSafety Bench: Benchmarking LLMs on Safety Issues in Scientific Labs(https://arxiv.org/abs/2410.14182)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, large language model</a></li>
<li><strong>Abstract: </strong>Laboratory accidents pose significant risks to human life and property, underscoring the importance of robust safety protocols. Despite advancements in safety training, laboratory personnel may still unknowingly engage in unsafe practices. With the increasing reliance on large language models (LLMs) for guidance in various fields, including laboratory settings, there is a growing concern about their reliability in critical safety-related decision-making. Unlike trained human researchers, LLMs lack formal lab safety education, raising questions about their ability to provide safe and accurate guidance. Existing research on LLM trustworthiness primarily focuses on issues such as ethical compliance, truthfulness, and fairness but fails to fully cover safety-critical real-world applications, like lab safety. To address this gap, we propose the Laboratory Safety Benchmark (LabSafety Bench), a comprehensive evaluation framework based on a new taxonomy aligned with Occupational Safety and Health Administration (OSHA) protocols. This benchmark includes 765 multiple-choice questions verified by human experts, assessing LLMs and vision language models (VLMs) performance in lab safety contexts. Our evaluations demonstrate that while GPT-4o outperforms human participants, it is still prone to critical errors, highlighting the risks of relying on LLMs in safety-critical environments. Our findings emphasize the need for specialized benchmarks to accurately assess the trustworthiness of LLMs in real-world safety applications.</li>
</ul>

<h3>Title: MetaAlign: Align Large Language Models with Diverse Preferences during Inference Time</h3>
<ul>
<li><strong>Authors: </strong>Mozhi Zhang, Pengyu Wang, Chenkun Tan, Mianqiu Huang, Dong Zhang, Yaqian Zhou, Xipeng Qiu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14184">https://arxiv.org/abs/2410.14184</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14184">https://arxiv.org/pdf/2410.14184</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14184]] MetaAlign: Align Large Language Models with Diverse Preferences during Inference Time(https://arxiv.org/abs/2410.14184)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) acquire extensive knowledge and remarkable abilities from extensive text corpora, making them powerful tools for various applications. To make LLMs more usable, aligning them with human preferences is essential. Existing alignment techniques, such as Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO), typically embed predefined preferences directly within the model's parameters. These methods, however, often result in a static alignment that can not account for the diversity of human preferences in practical applications. In response to this challenge, we propose an effective method, \textbf{MetaAlign}, which aims to help LLMs dynamically align with various explicit or implicit preferences specified at inference time. Experimental results show that LLMs optimized on our meticulously constructed MetaAlign Dataset can effectively align with any preferences specified at the inference stage, validating the feasibility of MetaAlign. We hope that our work can provide some insights into the alignment of language models.</li>
</ul>

<h3>Title: Combining Hough Transform and Deep Learning Approaches to Reconstruct ECG Signals From Printouts</h3>
<ul>
<li><strong>Authors: </strong>Felix Krones, Ben Walker, Terry Lyons, Adam Mahdi</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14185">https://arxiv.org/abs/2410.14185</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14185">https://arxiv.org/pdf/2410.14185</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14185]] Combining Hough Transform and Deep Learning Approaches to Reconstruct ECG Signals From Printouts(https://arxiv.org/abs/2410.14185)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>This work presents our team's (SignalSavants) winning contribution to the 2024 George B. Moody PhysioNet Challenge. The Challenge had two goals: reconstruct ECG signals from printouts and classify them for cardiac diseases. Our focus was the first task. Despite many ECGs being digitally recorded today, paper ECGs remain common throughout the world. Digitising them could help build more diverse datasets and enable automated analyses. However, the presence of varying recording standards and poor image quality requires a data-centric approach for developing robust models that can generalise effectively. Our approach combines the creation of a diverse training set, Hough transform to rotate images, a U-Net based segmentation model to identify individual signals, and mask vectorisation to reconstruct the signals. We assessed the performance of our models using the 10-fold stratified cross-validation (CV) split of 21,799 recordings proposed by the PTB-XL dataset. On the digitisation task, our model achieved an average CV signal-to-noise ratio of 17.02 and an official Challenge score of 12.15 on the hidden set, securing first place in the competition. Our study shows the challenges of building robust, generalisable, digitisation approaches. Such models require large amounts of resources (data, time, and computational power) but have great potential in diversifying the data available.</li>
</ul>

<h3>Title: xPerT: Extended Persistence Transformer</h3>
<ul>
<li><strong>Authors: </strong>Sehun Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, math.AT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14193">https://arxiv.org/abs/2410.14193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14193">https://arxiv.org/pdf/2410.14193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14193]] xPerT: Extended Persistence Transformer(https://arxiv.org/abs/2410.14193)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>A persistence diagram provides a compact summary of persistent homology, which captures the topological features of a space at different scales. However, due to its nature as a set, incorporating it as a feature into a machine learning framework is challenging. Several methods have been proposed to use persistence diagrams as input for machine learning models, but they often require complex preprocessing steps and extensive hyperparameter tuning. In this paper, we propose a novel transformer architecture called the \textit{Extended Persistence Transformer (xPerT)}, which is highly scalable than the compared to Persformer, an existing transformer for persistence diagrams. xPerT reduces GPU memory usage by over 90\% and improves accuracy on multiple datasets. Additionally, xPerT does not require complex preprocessing steps or extensive hyperparameter tuning, making it easy to use in practice. Our code is available at this https URL.</li>
</ul>

<h3>Title: Rethinking Transformer for Long Contextual Histopathology Whole Slide Image Analysis</h3>
<ul>
<li><strong>Authors: </strong>Honglin Li, Yunlong Zhang, Pingyi Chen, Zhongyi Shui, Chenglu Zhu, Lin Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14195">https://arxiv.org/abs/2410.14195</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14195">https://arxiv.org/pdf/2410.14195</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14195]] Rethinking Transformer for Long Contextual Histopathology Whole Slide Image Analysis(https://arxiv.org/abs/2410.14195)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Histopathology Whole Slide Image (WSI) analysis serves as the gold standard for clinical cancer diagnosis in the daily routines of doctors. To develop computer-aided diagnosis model for WSIs, previous methods typically employ Multi-Instance Learning to enable slide-level prediction given only slide-level labels. Among these models, vanilla attention mechanisms without pairwise interactions have traditionally been employed but are unable to model contextual information. More recently, self-attention models have been utilized to address this issue. To alleviate the computational complexity of long sequences in large WSIs, methods like HIPT use region-slicing, and TransMIL employs approximation of full self-attention. Both approaches suffer from suboptimal performance due to the loss of key information. Moreover, their use of absolute positional embedding struggles to effectively handle long contextual dependencies in shape-varying WSIs. In this paper, we first analyze how the low-rank nature of the long-sequence attention matrix constrains the representation ability of WSI modelling. Then, we demonstrate that the rank of attention matrix can be improved by focusing on local interactions via a local attention mask. Our analysis shows that the local mask aligns with the attention patterns in the lower layers of the Transformer. Furthermore, the local attention mask can be implemented during chunked attention calculation, reducing the quadratic computational complexity to linear with a small local bandwidth. Building on this, we propose a local-global hybrid Transformer for both computational acceleration and local-global information interactions modelling. Our method, Long-contextual MIL (LongMIL), is evaluated through extensive experiments on various WSI tasks to validate its superiority. Our code will be available at this http URL.</li>
</ul>

<h3>Title: Supervised Chain of Thought</h3>
<ul>
<li><strong>Authors: </strong>Xiang Zhang, Dujian Ding</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14198">https://arxiv.org/abs/2410.14198</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14198">https://arxiv.org/pdf/2410.14198</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14198]] Supervised Chain of Thought(https://arxiv.org/abs/2410.14198)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have revolutionized natural language processing and hold immense potential for advancing Artificial Intelligence. However, the core architecture of most mainstream LLMs -- the Transformer -- has inherent limitations in computational depth, rendering them theoretically incapable of solving many reasoning tasks that demand increasingly deep computations. Chain of Thought (CoT) prompting has emerged as a technique to address these architectural limitations, as evidenced by several theoretical studies. It offers a promising approach to solving complex reasoning tasks that were previously beyond the capabilities of these models. Despite its successes, CoT and its variants (such as Tree of Thought, Graph of Thought, etc.) rely on a "one-prompt-for-all" approach, using a single prompt structure (e.g., "think step by step") for a wide range of tasks -- from counting and sorting to solving mathematical and algorithmic problems. This approach poses significant challenges for models to generate the correct reasoning steps, as the model must navigate through a vast prompt template space to find the appropriate template for each task. In this work, we build upon previous theoretical analyses of CoT to demonstrate how the one-prompt-for-all approach can negatively affect the computability of LLMs. We partition the solution search space into two: the prompt space and the answer space. Our findings show that task-specific supervision is essential for navigating the prompt space accurately and achieving optimal performance. Through experiments with state-of-the-art LLMs, we reveal a gap in reasoning performance when supervision is applied versus when it is not.</li>
</ul>

<h3>Title: Rationale Behind Essay Scores: Enhancing S-LLM's Multi-Trait Essay Scoring with Rationale Generated by LLMs</h3>
<ul>
<li><strong>Authors: </strong>SeongYeub Chu, JongWoo Kim, Bryan Wong, MunYong Yi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14202">https://arxiv.org/abs/2410.14202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14202">https://arxiv.org/pdf/2410.14202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14202]] Rationale Behind Essay Scores: Enhancing S-LLM's Multi-Trait Essay Scoring with Rationale Generated by LLMs(https://arxiv.org/abs/2410.14202)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Existing automated essay scoring (AES) has solely relied on essay text without using explanatory rationales for the scores, thereby forgoing an opportunity to capture the specific aspects evaluated by rubric indicators in a fine-grained manner. This paper introduces Rationale-based Multiple Trait Scoring (RMTS), a novel approach for multi-trait essay scoring that integrates prompt-engineering-based large language models (LLMs) with a fine-tuning-based essay scoring model using a smaller large language model (S-LLM). RMTS uses an LLM-based trait-wise rationale generation system where a separate LLM agent generates trait-specific rationales based on rubric guidelines, which the scoring model uses to accurately predict multi-trait scores. Extensive experiments on benchmark datasets, including ASAP, ASAP++, and Feedback Prize, show that RMTS significantly outperforms state-of-the-art models and vanilla S-LLMs in trait-specific scoring. By assisting quantitative assessment with fine-grained qualitative rationales, RMTS enhances the trait-wise reliability, providing partial explanations about essays.</li>
</ul>

<h3>Title: MediTOD: An English Dialogue Dataset for Medical History Taking with Comprehensive Annotations</h3>
<ul>
<li><strong>Authors: </strong>Vishal Vivek Saley, Goonjan Saha, Rocktim Jyoti Das, Dinesh Raghu, Mausam</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14204">https://arxiv.org/abs/2410.14204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14204">https://arxiv.org/pdf/2410.14204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14204]] MediTOD: An English Dialogue Dataset for Medical History Taking with Comprehensive Annotations(https://arxiv.org/abs/2410.14204)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Medical task-oriented dialogue systems can assist doctors by collecting patient medical history, aiding in diagnosis, or guiding treatment selection, thereby reducing doctor burnout and expanding access to medical services. However, doctor-patient dialogue datasets are not readily available, primarily due to privacy regulations. Moreover, existing datasets lack comprehensive annotations involving medical slots and their different attributes, such as symptoms and their onset, progression, and severity. These comprehensive annotations are crucial for accurate diagnosis. Finally, most existing datasets are non-English, limiting their utility for the larger research community. In response, we introduce MediTOD, a new dataset of doctor-patient dialogues in English for the medical history-taking task. Collaborating with doctors, we devise a questionnaire-based labeling scheme tailored to the medical domain. Then, medical professionals create the dataset with high-quality comprehensive annotations, capturing medical slots and their attributes. We establish benchmarks in supervised and few-shot settings on MediTOD for natural language understanding, policy learning, and natural language generation subtasks, evaluating models from both TOD and biomedical domains. We make MediTOD publicly available for future research.</li>
</ul>

<h3>Title: Modelling 1/f Noise in TRNGs via Fractional Brownian Motion</h3>
<ul>
<li><strong>Authors: </strong>Maciej Skorski</a></li>
<li><strong>Subjects: </strong>cs.CR, math.ST</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14205">https://arxiv.org/abs/2410.14205</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14205">https://arxiv.org/pdf/2410.14205</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14205]] Modelling 1/f Noise in TRNGs via Fractional Brownian Motion(https://arxiv.org/abs/2410.14205)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>Building upon the foundational work of atomic clock physicists Barnes and Allan, this paper presents a highly scalable and numerically exact framework for modeling \(1/f\) noise in oscillatory True Random Number Generators (TRNGs) and assessing their cryptographic security. By employing Fractional Brownian Motion, the framework constructs Gaussian non-stationary processes that represent these noise spectra accurately and in a mathematically sound way. Furthermore, it establishes several critical properties, including optimal bounds on the achievable generation rate of cryptographically secure bits.</li>
</ul>

<h3>Title: Flexi-Fuzz least squares SVM for Alzheimer's diagnosis: Tackling noise, outliers, and class imbalance</h3>
<ul>
<li><strong>Authors: </strong>Mushir Akhtar, A. Quadir, M. Tanveer, Mohd. Arshad (for the Alzheimer's Disease Neuroimaging)</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14207">https://arxiv.org/abs/2410.14207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14207">https://arxiv.org/pdf/2410.14207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14207]] Flexi-Fuzz least squares SVM for Alzheimer's diagnosis: Tackling noise, outliers, and class imbalance(https://arxiv.org/abs/2410.14207)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Alzheimer's disease (AD) is a leading neurodegenerative condition and the primary cause of dementia, characterized by progressive cognitive decline and memory loss. Its progression, marked by shrinkage in the cerebral cortex, is irreversible. Numerous machine learning algorithms have been proposed for the early diagnosis of AD. However, they often struggle with the issues of noise, outliers, and class imbalance. To tackle the aforementioned limitations, in this article, we introduce a novel, robust, and flexible membership scheme called Flexi-Fuzz. This scheme integrates a novel flexible weighting mechanism, class probability, and imbalance ratio. The proposed flexible weighting mechanism assigns the maximum weight to samples within a specific proximity to the center, with a gradual decrease in weight beyond a certain threshold. This approach ensures that samples near the class boundary still receive significant weight, maintaining their influence in the classification process. Class probability is used to mitigate the impact of noisy samples, while the imbalance ratio addresses class imbalance. Leveraging this, we incorporate the proposed Flexi-Fuzz membership scheme into the least squares support vector machines (LSSVM) framework, resulting in a robust and flexible model termed Flexi-Fuzz-LSSVM. We determine the class-center using two methods: the conventional mean approach and an innovative median approach, leading to two model variants, Flexi-Fuzz-LSSVM-I and Flexi-Fuzz-LSSVM-II. To validate the effectiveness of the proposed Flexi-Fuzz-LSSVM models, we evaluated them on benchmark UCI and KEEL datasets, both with and without label noise. Additionally, we tested the models on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset for AD diagnosis. Experimental results demonstrate the superiority of the Flexi-Fuzz-LSSVM models over baseline models.</li>
</ul>

<h3>Title: Montessori-Instruct: Generate Influential Training Data Tailored for Student Learning</h3>
<ul>
<li><strong>Authors: </strong>Xiaochuan Li, Zichun Yu, Chenyan Xiong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14208">https://arxiv.org/abs/2410.14208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14208">https://arxiv.org/pdf/2410.14208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14208]] Montessori-Instruct: Generate Influential Training Data Tailored for Student Learning(https://arxiv.org/abs/2410.14208)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Synthetic data has been widely used to train large language models, but their generative nature inevitably introduces noisy, non-informative, and misleading learning signals. In this paper, we propose Montessori-Instruct, a novel data synthesis framework that tailors the data synthesis ability of the teacher language model toward the student language model's learning process. Specifically, we utilize local data influence of synthetic training data points on students to characterize students' learning preferences. Then, we train the teacher model with Direct Preference Optimization (DPO) to generate synthetic data tailored toward student learning preferences. Experiments with Llama3-8B-Instruct (teacher) and Llama3-8B (student) on Alpaca Eval and MT-Bench demonstrate that Montessori-Instruct significantly outperforms standard synthesis methods by 18.35\% and 46.24\% relatively. Our method also beats data synthesized by a stronger teacher model, GPT-4o. Further analysis confirms the benefits of teacher's learning to generate more influential training data in the student's improved learning, the advantages of local data influence in accurately measuring student preferences, and the robustness of Montessori-Instruct across different student models. Our code and data are open-sourced at this https URL.</li>
</ul>

<h3>Title: Shape Transformation Driven by Active Contour for Class-Imbalanced Semi-Supervised Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Yuliang Gu, Yepeng Liu, Zhichao Sun, Jinchi Zhu, Yongchao Xu, Laurent Najman (LIGM)</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14210">https://arxiv.org/abs/2410.14210</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14210">https://arxiv.org/pdf/2410.14210</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14210]] Shape Transformation Driven by Active Contour for Class-Imbalanced Semi-Supervised Medical Image Segmentation(https://arxiv.org/abs/2410.14210)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Annotating 3D medical images demands expert knowledge and is time-consuming. As a result, semi-supervised learning (SSL) approaches have gained significant interest in 3D medical image segmentation. The significant size differences among various organs in the human body lead to imbalanced class distribution, which is a major challenge in the real-world application of these SSL approaches. To address this issue, we develop a novel Shape Transformation driven by Active Contour (STAC), that enlarges smaller organs to alleviate imbalanced class distribution across different organs. Inspired by curve evolution theory in active contour methods, STAC employs a signed distance function (SDF) as the level set function, to implicitly represent the shape of organs, and deforms voxels in the direction of the steepest descent of SDF (i.e., the normal vector). To ensure that the voxels far from expansion organs remain unchanged, we design an SDF-based weight function to control the degree of deformation for each voxel. We then use STAC as a data-augmentation process during the training stage. Experimental results on two benchmark datasets demonstrate that the proposed method significantly outperforms some state-of-the-art methods. Source code is publicly available at this https URL.</li>
</ul>

<h3>Title: Paths-over-Graph: Knowledge Graph Enpowered Large Language Model Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Xingyu Tan, Xiaoyang Wang, Qing Liu, Xiwei Xu, Xin Yuan, Wenjie Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14211">https://arxiv.org/abs/2410.14211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14211">https://arxiv.org/pdf/2410.14211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14211]] Paths-over-Graph: Knowledge Graph Enpowered Large Language Model Reasoning(https://arxiv.org/abs/2410.14211)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved impressive results in various tasks but struggle with hallucination problems and lack of relevant knowledge, especially in deep complex reasoning and knowledge-intensive tasks. Knowledge Graphs (KGs), which capture vast amounts of facts in a structured format, offer a reliable source of knowledge for reasoning. However, existing KG-based LLM reasoning methods face challenges like handling multi-hop reasoning, multi-entity questions, and effectively utilizing graph structures. To address these issues, we propose Paths-over-Graph (PoG), a novel method that enhances LLM reasoning by integrating knowledge reasoning paths from KGs, improving the interpretability and faithfulness of LLM outputs. PoG tackles multi-hop and multi-entity questions through a three-phase dynamic multi-hop path exploration, which combines the inherent knowledge of LLMs with factual knowledge from KGs. In order to improve the efficiency, PoG prunes irrelevant information from the graph exploration first and introduces efficient three-step pruning techniques that incorporate graph structures, LLM prompting, and a pre-trained language model (e.g., SBERT) to effectively narrow down the explored candidate paths. This ensures all reasoning paths contain highly relevant information captured from KGs, making the reasoning faithful and interpretable in problem-solving. PoG innovatively utilizes graph structure to prune the irrelevant noise and represents the first method to implement multi-entity deep path detection on KGs for LLM reasoning tasks. Comprehensive experiments on five benchmark KGQA datasets demonstrate PoG outperforms the state-of-the-art method ToG across GPT-3.5-Turbo and GPT-4, achieving an average accuracy improvement of 18.9%. Notably, PoG with GPT-3.5-Turbo surpasses ToG with GPT-4 by up to 23.9%.</li>
</ul>

<h3>Title: G-NeuroDAVIS: A Neural Network model for generalized embedding, data visualization and sample generation</h3>
<ul>
<li><strong>Authors: </strong>Chayan Maitra, Rajat K. De</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14223">https://arxiv.org/abs/2410.14223</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14223">https://arxiv.org/pdf/2410.14223</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14223]] G-NeuroDAVIS: A Neural Network model for generalized embedding, data visualization and sample generation(https://arxiv.org/abs/2410.14223)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Visualizing high-dimensional datasets through a generalized embedding has been a challenge for a long time. Several methods have shown up for the same, but still, they have not been able to generate a generalized embedding, which not only can reveal the hidden patterns present in the data but also generate realistic high-dimensional samples from it. Motivated by this aspect, in this study, a novel generative model, called G-NeuroDAVIS, has been developed, which is capable of visualizing high-dimensional data through a generalized embedding, and thereby generating new samples. The model leverages advanced generative techniques to produce high-quality embedding that captures the underlying structure of the data more effectively than existing methods. G-NeuroDAVIS can be trained in both supervised and unsupervised settings. We rigorously evaluated our model through a series of experiments, demonstrating superior performance in classification tasks, which highlights the robustness of the learned representations. Furthermore, the conditional sample generation capability of the model has been described through qualitative assessments, revealing a marked improvement in generating realistic and diverse samples. G-NeuroDAVIS has outperformed the Variational Autoencoder (VAE) significantly in multiple key aspects, including embedding quality, classification performance, and sample generation capability. These results underscore the potential of our generative model to serve as a powerful tool in various applications requiring high-quality data generation and representation learning.</li>
</ul>

<h3>Title: Few-Shot Joint Multimodal Entity-Relation Extraction via Knowledge-Enhanced Cross-modal Prompt Model</h3>
<ul>
<li><strong>Authors: </strong>Li Yuan, Yi Cai, Junsheng Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14225">https://arxiv.org/abs/2410.14225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14225">https://arxiv.org/pdf/2410.14225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14225]] Few-Shot Joint Multimodal Entity-Relation Extraction via Knowledge-Enhanced Cross-modal Prompt Model(https://arxiv.org/abs/2410.14225)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Joint Multimodal Entity-Relation Extraction (JMERE) is a challenging task that aims to extract entities and their relations from text-image pairs in social media posts. Existing methods for JMERE require large amounts of labeled data. However, gathering and annotating fine-grained multimodal data for JMERE poses significant challenges. Initially, we construct diverse and comprehensive multimodal few-shot datasets fitted to the original data distribution. To address the insufficient information in the few-shot setting, we introduce the \textbf{K}nowledge-\textbf{E}nhanced \textbf{C}ross-modal \textbf{P}rompt \textbf{M}odel (KECPM) for JMERE. This method can effectively address the problem of insufficient information in the few-shot setting by guiding a large language model to generate supplementary background knowledge. Our proposed method comprises two stages: (1) a knowledge ingestion stage that dynamically formulates prompts based on semantic similarity guide ChatGPT generating relevant knowledge and employs self-reflection to refine the knowledge; (2) a knowledge-enhanced language model stage that merges the auxiliary knowledge with the original input and utilizes a transformer-based model to align with JMERE's required output format. We extensively evaluate our approach on a few-shot dataset derived from the JMERE dataset, demonstrating its superiority over strong baselines in terms of both micro and macro F$_1$ scores. Additionally, we present qualitative analyses and case studies to elucidate the effectiveness of our model.</li>
</ul>

<h3>Title: Unveiling Large Language Models Generated Texts: A Multi-Level Fine-Grained Detection Framework</h3>
<ul>
<li><strong>Authors: </strong>Zhen Tao, Zhiyu Li, Runyu Chen, Dinghao Xi, Wei Xu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14231">https://arxiv.org/abs/2410.14231</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14231">https://arxiv.org/pdf/2410.14231</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14231]] Unveiling Large Language Models Generated Texts: A Multi-Level Fine-Grained Detection Framework(https://arxiv.org/abs/2410.14231)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have transformed human writing by enhancing grammar correction, content expansion, and stylistic refinement. However, their widespread use raises concerns about authorship, originality, and ethics, even potentially threatening scholarly integrity. Existing detection methods, which mainly rely on single-feature analysis and binary classification, often fail to effectively identify LLM-generated text in academic contexts. To address these challenges, we propose a novel Multi-level Fine-grained Detection (MFD) framework that detects LLM-generated text by integrating low-level structural, high-level semantic, and deep-level linguistic features, while conducting sentence-level evaluations of lexicon, grammar, and syntax for comprehensive analysis. To improve detection of subtle differences in LLM-generated text and enhance robustness against paraphrasing, we apply two mainstream evasion techniques to rewrite the text. These variations, along with original texts, are used to train a text encoder via contrastive learning, extracting high-level semantic features of sentence to boost detection generalization. Furthermore, we leverage advanced LLM to analyze the entire text and extract deep-level linguistic features, enhancing the model's ability to capture complex patterns and nuances while effectively incorporating contextual information. Extensive experiments on public datasets show that the MFD model outperforms existing methods, achieving an MAE of 0.1346 and an accuracy of 88.56%. Our research provides institutions and publishers with an effective mechanism to detect LLM-generated text, mitigating risks of compromised authorship. Educators and editors can use the model's predictions to refine verification and plagiarism prevention protocols, ensuring adherence to standards.</li>
</ul>

<h3>Title: Towards Robust Knowledge Representations in Multilingual LLMs for Equivalence and Inheritance based Consistent Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Gaurav Arora, Srujana Merugu, Shreya Jain, Vaibhav Saxena</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14235">https://arxiv.org/abs/2410.14235</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14235">https://arxiv.org/pdf/2410.14235</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14235]] Towards Robust Knowledge Representations in Multilingual LLMs for Equivalence and Inheritance based Consistent Reasoning(https://arxiv.org/abs/2410.14235)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Reasoning and linguistic skills form the cornerstone of human intelligence, facilitating problem-solving and decision-making. Recent advances in Large Language Models (LLMs) have led to impressive linguistic capabilities and emergent reasoning behaviors, fueling widespread adoption across application domains. However, LLMs still struggle with complex reasoning tasks, highlighting their systemic limitations. In this work, we focus on evaluating whether LLMs have the requisite representations to reason using two foundational relationships: "equivalence" and "inheritance". We introduce novel tasks and benchmarks spanning six languages and observe that current SOTA LLMs often produce conflicting answers to the same questions across languages in 17.3-57.5% of cases and violate inheritance constraints in up to 37.2% cases. To enhance consistency across languages, we propose novel "Compositional Representations" where tokens are represented as composition of equivalent tokens across languages, with resulting conflict reduction (up to -4.7%) indicating benefits of shared LLM representations.</li>
</ul>

<h3>Title: Unified Convergence Analysis for Score-Based Diffusion Models with Deterministic Samplers</h3>
<ul>
<li><strong>Authors: </strong>Runjia Li, Qiwei Di, Quanquan Gu</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14237">https://arxiv.org/abs/2410.14237</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14237">https://arxiv.org/pdf/2410.14237</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14237]] Unified Convergence Analysis for Score-Based Diffusion Models with Deterministic Samplers(https://arxiv.org/abs/2410.14237)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Score-based diffusion models have emerged as powerful techniques for generating samples from high-dimensional data distributions. These models involve a two-phase process: first, injecting noise to transform the data distribution into a known prior distribution, and second, sampling to recover the original data distribution from noise. Among the various sampling methods, deterministic samplers stand out for their enhanced efficiency. However, analyzing these deterministic samplers presents unique challenges, as they preclude the use of established techniques such as Girsanov's theorem, which are only applicable to stochastic samplers. Furthermore, existing analysis for deterministic samplers usually focuses on specific examples, lacking a generalized approach for general forward processes and various deterministic samplers. Our paper addresses these limitations by introducing a unified convergence analysis framework. To demonstrate the power of our framework, we analyze the variance-preserving (VP) forward process with the exponential integrator (EI) scheme, achieving iteration complexity of $\tilde O(d^2/\epsilon)$. Additionally, we provide a detailed analysis of Denoising Diffusion Implicit Models (DDIM)-type samplers, which have been underexplored in previous research, achieving polynomial iteration complexity.</li>
</ul>

<h3>Title: Storyboard guided Alignment for Fine-grained Video Action Recognition</h3>
<ul>
<li><strong>Authors: </strong>Enqi Liu, Liyuan Pan, Yan Yang, Yiran Zhong, Zhijing Wu, Xinxiao Wu, Liu Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14238">https://arxiv.org/abs/2410.14238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14238">https://arxiv.org/pdf/2410.14238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14238]] Storyboard guided Alignment for Fine-grained Video Action Recognition(https://arxiv.org/abs/2410.14238)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Fine-grained video action recognition can be conceptualized as a video-text matching problem. Previous approaches often rely on global video semantics to consolidate video embeddings, which can lead to misalignment in video-text pairs due to a lack of understanding of action semantics at an atomic granularity level. To tackle this challenge, we propose a multi-granularity framework based on two observations: (i) videos with different global semantics may share similar atomic actions or appearances, and (ii) atomic actions within a video can be momentary, slow, or even non-directly related to the global video semantics. Inspired by the concept of storyboarding, which disassembles a script into individual shots, we enhance global video semantics by generating fine-grained descriptions using a pre-trained large language model. These detailed descriptions capture common atomic actions depicted in videos. A filtering metric is proposed to select the descriptions that correspond to the atomic actions present in both the videos and the descriptions. By employing global semantics and fine-grained descriptions, we can identify key frames in videos and utilize them to aggregate embeddings, thereby making the embedding more accurate. Extensive experiments on various video action recognition datasets demonstrate superior performance of our proposed method in supervised, few-shot, and zero-shot settings.</li>
</ul>

<h3>Title: Almost-Linear RNNs Yield Highly Interpretable Symbolic Codes in Dynamical Systems Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Manuel Brenner, Christoph J√ºrgen Hemmer, Zahra Monfared, Daniel Durstewitz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.DS, nlin.CD, physics.data-an</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14240">https://arxiv.org/abs/2410.14240</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14240">https://arxiv.org/pdf/2410.14240</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14240]] Almost-Linear RNNs Yield Highly Interpretable Symbolic Codes in Dynamical Systems Reconstruction(https://arxiv.org/abs/2410.14240)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Dynamical systems (DS) theory is fundamental for many areas of science and engineering. It can provide deep insights into the behavior of systems evolving in time, as typically described by differential or recursive equations. A common approach to facilitate mathematical tractability and interpretability of DS models involves decomposing nonlinear DS into multiple linear DS separated by switching manifolds, i.e. piecewise linear (PWL) systems. PWL models are popular in engineering and a frequent choice in mathematics for analyzing the topological properties of DS. However, hand-crafting such models is tedious and only possible for very low-dimensional scenarios, while inferring them from data usually gives rise to unnecessarily complex representations with very many linear subregions. Here we introduce Almost-Linear Recurrent Neural Networks (AL-RNNs) which automatically and robustly produce most parsimonious PWL representations of DS from time series data, using as few PWL nonlinearities as possible. AL-RNNs can be efficiently trained with any SOTA algorithm for dynamical systems reconstruction (DSR), and naturally give rise to a symbolic encoding of the underlying DS that provably preserves important topological properties. We show that for the Lorenz and R√∂ssler systems, AL-RNNs discover, in a purely data-driven way, the known topologically minimal PWL representations of the corresponding chaotic attractors. We further illustrate on two challenging empirical datasets that interpretable symbolic encodings of the dynamics can be achieved, tremendously facilitating mathematical and computational analysis of the underlying systems.</li>
</ul>

<h3>Title: ERDDCI: Exact Reversible Diffusion via Dual-Chain Inversion for High-Quality Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Jimin Dai, Yingzhen Zhang, Shuo Chen, Jian Yang, Lei Luo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14247">https://arxiv.org/abs/2410.14247</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14247">https://arxiv.org/pdf/2410.14247</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14247]] ERDDCI: Exact Reversible Diffusion via Dual-Chain Inversion for High-Quality Image Editing(https://arxiv.org/abs/2410.14247)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models (DMs) have been successfully applied to real image editing. These models typically invert images into latent noise vectors used to reconstruct the original images (known as inversion), and then edit them during the inference process. However, recent popular DMs often rely on the assumption of local linearization, where the noise injected during the inversion process is expected to approximate the noise removed during the inference process. While DM efficiently generates images under this assumption, it can also accumulate errors during the diffusion process due to the assumption, ultimately negatively impacting the quality of real image reconstruction and editing. To address this issue, we propose a novel method, referred to as ERDDCI (Exact Reversible Diffusion via Dual-Chain Inversion). ERDDCI uses the new Dual-Chain Inversion (DCI) for joint inference to derive an exact reversible diffusion process. By using DCI, our method effectively avoids the cumbersome optimization process in existing inversion approaches and achieves high-quality image editing. Additionally, to accommodate image operations under high guidance scales, we introduce a dynamic control strategy that enables more refined image reconstruction and editing. Our experiments demonstrate that ERDDCI significantly outperforms state-of-the-art methods in a 50-step diffusion process. It achieves rapid and precise image reconstruction with an SSIM of 0.999 and an LPIPS of 0.001, and also delivers competitive results in image editing.</li>
</ul>

<h3>Title: Addressing Blind Guessing: Calibration of Selection Bias in Multiple-Choice Question Answering by Video Language Models</h3>
<ul>
<li><strong>Authors: </strong>Olga Loginova, Oleksandr Bezrukov, Alexey Kravets</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14248">https://arxiv.org/abs/2410.14248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14248">https://arxiv.org/pdf/2410.14248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14248]] Addressing Blind Guessing: Calibration of Selection Bias in Multiple-Choice Question Answering by Video Language Models(https://arxiv.org/abs/2410.14248)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Evaluating Video Language Models (VLMs) is a challenging task. Due to its transparency, Multiple-Choice Question Answering (MCQA) is widely used to measure the performance of these models through accuracy. However, existing MCQA benchmarks fail to capture the full reasoning capabilities of VLMs due to selection bias, when models disproportionately favor certain answer options based on positional patterns observed during training. In this work, we conduct a comprehensive empirical analysis of several VLM architectures across major datasets designed to assess complex video-focused reasoning. We identify where the bias is most pronounced and demonstrate to what extent model responses reflect genuine understanding of video content and related questions, as opposed to reliance on arbitrary patterns or superficial cues, such as answer position. By decomposing the MCQA task and adapting fairness bias metrics to VLMs, we introduce a post-processing calibration technique BOLD to balance this bias. Our results show that reducing selection bias improves not only debiasing metrics but also overall model performance, including Accuracy and F1 Mean score. Our method, by suppressing "blind guessing", offers a more cost- and time-effective approach to mitigating selection bias compared to existing techniques. This study represents the first focused investigation of selection bias in video-to-text LLM-powered models.</li>
</ul>

<h3>Title: RAZOR: Refining Accuracy by Zeroing Out Redundancies</h3>
<ul>
<li><strong>Authors: </strong>Daniel Riccio, Genoveffa Tortora, Mara Sangiovanni</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14254">https://arxiv.org/abs/2410.14254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14254">https://arxiv.org/pdf/2410.14254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14254]] RAZOR: Refining Accuracy by Zeroing Out Redundancies(https://arxiv.org/abs/2410.14254)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In many application domains, the proliferation of sensors and devices is generating vast volumes of data, imposing significant pressure on existing data analysis and data mining techniques. Nevertheless, an increase in data volume does not inherently imply an increase in informational content, as a substantial portion may be redundant or represent noise. This challenge is particularly evident in the deep learning domain, where the utility of additional data is contingent on its informativeness. In the absence of such, larger datasets merely exacerbate the computational cost and complexity of the learning process. To address these challenges, we propose RAZOR, a novel instance selection technique designed to extract a significantly smaller yet sufficiently informative subset from a larger set of instances without compromising the learning process. RAZOR has been specifically engineered to be robust, efficient, and scalable, making it suitable for large-scale datasets. Unlike many techniques in the literature, RAZOR is capable of operating in both supervised and unsupervised settings. Experimental results demonstrate that RAZOR outperforms recent state-of-the-art techniques in terms of both effectiveness and efficiency.</li>
</ul>

<h3>Title: Revisiting SLO and Goodput Metrics in LLM Serving</h3>
<ul>
<li><strong>Authors: </strong>Zhibin Wang, Shipeng Li, Yuhang Zhou, Xue Li, Rong Gu, Nguyen Cam-Tu, Chen Tian, Sheng Zhong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14257">https://arxiv.org/abs/2410.14257</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14257">https://arxiv.org/pdf/2410.14257</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14257]] Revisiting SLO and Goodput Metrics in LLM Serving(https://arxiv.org/abs/2410.14257)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved remarkable performance and are widely deployed in various applications, while the serving of LLM inference has raised concerns about user experience and serving throughput. Accordingly, service level objectives (SLOs) and goodput-the number of requests that meet SLOs per second-are introduced to evaluate the performance of LLM serving. However, existing metrics fail to capture the nature of user experience. We observe two ridiculous phenomena in existing metrics: 1) delaying token delivery can smooth the tail time between tokens (tail TBT) of a request and 2) dropping the request that fails to meet the SLOs midway can improve goodput. In this paper, we revisit SLO and goodput metrics in LLM serving and propose a unified metric framework smooth goodput including SLOs and goodput to reflect the nature of user experience in LLM serving. The framework can adapt to specific goals of different tasks by setting parameters. We re-evaluate the performance of different LLM serving systems under multiple workloads based on this unified framework and provide possible directions for future optimization of existing strategies. We hope that this framework can provide a unified standard for evaluating LLM serving and foster researches in the field of LLM serving optimization to move in a cohesive direction.</li>
</ul>

<h3>Title: Beyond Binary: Towards Fine-Grained LLM-Generated Text Detection via Role Recognition and Involvement Measurement</h3>
<ul>
<li><strong>Authors: </strong>Zihao Cheng, Li Zhou, Feng Jiang, Benyou Wang, Haizhou Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14259">https://arxiv.org/abs/2410.14259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14259">https://arxiv.org/pdf/2410.14259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14259]] Beyond Binary: Towards Fine-Grained LLM-Generated Text Detection via Role Recognition and Involvement Measurement(https://arxiv.org/abs/2410.14259)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, large language model</a></li>
<li><strong>Abstract: </strong>The rapid development of large language models (LLMs), like ChatGPT, has resulted in the widespread presence of LLM-generated content on social media platforms, raising concerns about misinformation, data biases, and privacy violations, which can undermine trust in online discourse. While detecting LLM-generated content is crucial for mitigating these risks, current methods often focus on binary classification, failing to address the complexities of real-world scenarios like human-AI collaboration. To move beyond binary classification and address these challenges, we propose a new paradigm for detecting LLM-generated content. This approach introduces two novel tasks: LLM Role Recognition (LLM-RR), a multi-class classification task that identifies specific roles of LLM in content generation, and LLM Influence Measurement (LLM-IM), a regression task that quantifies the extent of LLM involvement in content creation. To support these tasks, we propose LLMDetect, a benchmark designed to evaluate detectors' performance on these new tasks. LLMDetect includes the Hybrid News Detection Corpus (HNDC) for training detectors, as well as DetectEval, a comprehensive evaluation suite that considers five distinct cross-context variations and multi-intensity variations within the same LLM role. This allows for a thorough assessment of detectors' generalization and robustness across diverse contexts. Our empirical validation of 10 baseline detection methods demonstrates that fine-tuned PLM-based models consistently outperform others on both tasks, while advanced LLMs face challenges in accurately detecting their own generated content. Our experimental results and analysis offer insights for developing more effective detection models for LLM-generated content. This research enhances the understanding of LLM-generated content and establishes a foundation for more nuanced detection methodologies.</li>
</ul>

<h3>Title: Good Parenting is all you need -- Multi-agentic LLM Hallucination Mitigation</h3>
<ul>
<li><strong>Authors: </strong>Edward (Ted)Kwartler, Matthew Berman, Alan Aqrawi</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14262">https://arxiv.org/abs/2410.14262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14262">https://arxiv.org/pdf/2410.14262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14262]] Good Parenting is all you need -- Multi-agentic LLM Hallucination Mitigation(https://arxiv.org/abs/2410.14262)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This study explores the ability of Large Language Model (LLM) agents to detect and correct hallucinations in AI-generated content. A primary agent was tasked with creating a blog about a fictional Danish artist named Flipfloppidy, which was then reviewed by another agent for factual inaccuracies. Most LLMs hallucinated the existence of this artist. Across 4,900 test runs involving various combinations of primary and reviewing agents, advanced AI models such as Llama3-70b and GPT-4 variants demonstrated near-perfect accuracy in identifying hallucinations and successfully revised outputs in 85% to 100% of cases following feedback. These findings underscore the potential of advanced AI models to significantly enhance the accuracy and reliability of generated content, providing a promising approach to improving AI workflow orchestration.</li>
</ul>

<h3>Title: HYPNOS : Highly Precise Foreground-focused Diffusion Finetuning for Inanimate Objects</h3>
<ul>
<li><strong>Authors: </strong>Oliverio Theophilus Nathanael, Jonathan Samuel Lumentut, Nicholas Hans Muliawan, Edbert Valencio Angky, Felix Indra Kurniadi, Alfi Yusrotis Zakiyyah, Jeklin Harefa</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14265">https://arxiv.org/abs/2410.14265</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14265">https://arxiv.org/pdf/2410.14265</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14265]] HYPNOS : Highly Precise Foreground-focused Diffusion Finetuning for Inanimate Objects(https://arxiv.org/abs/2410.14265)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>In recent years, personalized diffusion-based text-to-image generative tasks have been a hot topic in computer vision studies. A robust diffusion model is determined by its ability to perform near-perfect reconstruction of certain product outcomes given few related input samples. Unfortunately, the current prominent diffusion-based finetuning technique falls short in maintaining the foreground object consistency while being constrained to produce diverse backgrounds in the image outcome. In the worst scenario, the overfitting issue may occur, meaning that the foreground object is less controllable due to the condition above, for example, the input prompt information is transferred ambiguously to both foreground and background regions, instead of the supposed background region only. To tackle the issues above, we proposed Hypnos, a highly precise foreground-focused diffusion finetuning technique. On the image level, this strategy works best for inanimate object generation tasks, and to do so, Hypnos implements two main approaches, namely: (i) a content-centric prompting strategy and (ii) the utilization of our additional foreground-focused discriminative module. The utilized module is connected with the diffusion model and finetuned with our proposed set of supervision mechanism. Combining the strategies above yielded to the foreground-background disentanglement capability of the diffusion model. Our experimental results showed that the proposed strategy gave a more robust performance and visually pleasing results compared to the former technique. For better elaborations, we also provided extensive studies to assess the fruitful outcomes above, which reveal how personalization behaves in regard to several training conditions.</li>
</ul>

<h3>Title: MoDification: Mixture of Depths Made Easy</h3>
<ul>
<li><strong>Authors: </strong>Chen Zhang, Meizhi Zhong, Qimeng Wang, Xuantao Lu, Zheyu Ye, Chengqiang Lu, Yan Gao, Yao Hu, Kehai Chen, Min Zhang, Dawei Song</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14268">https://arxiv.org/abs/2410.14268</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14268">https://arxiv.org/pdf/2410.14268</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14268]] MoDification: Mixture of Depths Made Easy(https://arxiv.org/abs/2410.14268)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Long-context efficiency has recently become a trending topic in serving large language models (LLMs). And mixture of depths (MoD) is proposed as a perfect fit to bring down both latency and memory. In this paper, however, we discover that MoD can barely transform existing LLMs without costly training over an extensive number of tokens. To enable the transformations from any LLMs to MoD ones, we showcase top-k operator in MoD should be promoted to threshold-p operator, and refinement to architecture and data should also be crafted along. All these designs form our method termed MoDification. Through a comprehensive set of experiments covering model scales from 3B to 70B, we exhibit MoDification strikes an excellent balance between efficiency and effectiveness. MoDification can achieve up to ~1.2x speedup in latency and ~1.8x reduction in memory compared to original LLMs especially in long-context applications.</li>
</ul>

<h3>Title: REEF: Representation Encoding Fingerprints for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jie Zhang, Dongrui Liu, Chen Qian, Linfeng Zhang, Yong Liu, Yu Qiao, Jing Shao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14273">https://arxiv.org/abs/2410.14273</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14273">https://arxiv.org/pdf/2410.14273</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14273]] REEF: Representation Encoding Fingerprints for Large Language Models(https://arxiv.org/abs/2410.14273)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust, large language model</a></li>
<li><strong>Abstract: </strong>Protecting the intellectual property of open-source Large Language Models (LLMs) is very important, because training LLMs costs extensive computational resources and data. Therefore, model owners and third parties need to identify whether a suspect model is a subsequent development of the victim model. To this end, we propose a training-free REEF to identify the relationship between the suspect and victim models from the perspective of LLMs' feature representations. Specifically, REEF computes and compares the centered kernel alignment similarity between the representations of a suspect model and a victim model on the same samples. This training-free REEF does not impair the model's general capabilities and is robust to sequential fine-tuning, pruning, model merging, and permutations. In this way, REEF provides a simple and effective way for third parties and models' owners to protect LLMs' intellectual property together. The code is available at this https URL.</li>
</ul>

<h3>Title: EcomEdit: An Automated E-commerce Knowledge Editing Framework for Enhanced Product and Purchase Intention Understanding</h3>
<ul>
<li><strong>Authors: </strong>Ching Ming Samuel Lau, Weiqi Wang, Haochen Shi, Baixuan Xu, Jiaxin Bai, Yangqiu Song</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14276">https://arxiv.org/abs/2410.14276</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14276">https://arxiv.org/pdf/2410.14276</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14276]] EcomEdit: An Automated E-commerce Knowledge Editing Framework for Enhanced Product and Purchase Intention Understanding(https://arxiv.org/abs/2410.14276)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Knowledge Editing (KE) aims to correct and update factual information in Large Language Models (LLMs) to ensure accuracy and relevance without computationally expensive fine-tuning. Though it has been proven effective in several domains, limited work has focused on its application within the e-commerce sector. However, there are naturally occurring scenarios that make KE necessary in this domain, such as the timely updating of product features and trending purchase intentions by customers, which necessitate further exploration. In this paper, we pioneer the application of KE in the e-commerce domain by presenting ECOMEDIT, an automated e-commerce knowledge editing framework tailored for e-commerce-related knowledge and tasks. Our framework leverages more powerful LLMs as judges to enable automatic knowledge conflict detection and incorporates conceptualization to enhance the semantic coverage of the knowledge to be edited. Through extensive experiments, we demonstrate the effectiveness of ECOMEDIT in improving LLMs' understanding of product descriptions and purchase intentions. We also show that LLMs, after our editing, can achieve stronger performance on downstream e-commerce tasks.</li>
</ul>

<h3>Title: ClearSR: Latent Low-Resolution Image Embeddings Help Diffusion-Based Real-World Super Resolution Models See Clearer</h3>
<ul>
<li><strong>Authors: </strong>Yuhao Wan, Peng-Tao Jiang, Qibin Hou, Hao Zhang, Jinwei Chen, Ming-Ming Cheng, Bo Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14279">https://arxiv.org/abs/2410.14279</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14279">https://arxiv.org/pdf/2410.14279</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14279]] ClearSR: Latent Low-Resolution Image Embeddings Help Diffusion-Based Real-World Super Resolution Models See Clearer(https://arxiv.org/abs/2410.14279)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We present ClearSR, a new method that can better take advantage of latent low-resolution image (LR) embeddings for diffusion-based real-world image super-resolution (Real-ISR). Previous Real-ISR models mostly focus on how to activate more generative priors of text-to-image diffusion models to make the output high-resolution (HR) images look better. However, since these methods rely too much on the generative priors, the content of the output images is often inconsistent with the input LR ones. To mitigate the above issue, in this work, we explore using latent LR embeddings to constrain the control signals from ControlNet, and extract LR information at both detail and structure levels. We show that the proper use of latent LR embeddings can produce higher-quality control signals, which enables the super-resolution results to be more consistent with the LR image and leads to clearer visual results. In addition, we also show that latent LR embeddings can be used to control the inference stage, allowing for the improvement of fidelity and generation ability simultaneously. Experiments demonstrate that our model can achieve better performance across multiple metrics on several test sets and generate more consistent SR results with LR images than existing methods. Our code will be made publicly available.</li>
</ul>

<h3>Title: SwaQuAD-24: QA Benchmark Dataset in Swahili</h3>
<ul>
<li><strong>Authors: </strong>Alfred Malengo Kondoro</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14289">https://arxiv.org/abs/2410.14289</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14289">https://arxiv.org/pdf/2410.14289</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14289]] SwaQuAD-24: QA Benchmark Dataset in Swahili(https://arxiv.org/abs/2410.14289)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>This paper proposes the creation of a Swahili Question Answering (QA) benchmark dataset, aimed at addressing the underrepresentation of Swahili in natural language processing (NLP). Drawing from established benchmarks like SQuAD, GLUE, KenSwQuAD, and KLUE, the dataset will focus on providing high-quality, annotated question-answer pairs that capture the linguistic diversity and complexity of Swahili. The dataset is designed to support a variety of applications, including machine translation, information retrieval, and social services like healthcare chatbots. Ethical considerations, such as data privacy, bias mitigation, and inclusivity, are central to the dataset development. Additionally, the paper outlines future expansion plans to include domain-specific content, multimodal integration, and broader crowdsourcing efforts. The Swahili QA dataset aims to foster technological innovation in East Africa and provide an essential resource for NLP research and applications in low-resource languages.</li>
</ul>

<h3>Title: LoGU: Long-form Generation with Uncertainty Expressions</h3>
<ul>
<li><strong>Authors: </strong>Ruihan Yang, Caiqi Zhang, Zhisong Zhang, Xinting Huang, Sen Yang, Nigel Collier, Dong Yu, Deqing Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14309">https://arxiv.org/abs/2410.14309</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14309">https://arxiv.org/pdf/2410.14309</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14309]] LoGU: Long-form Generation with Uncertainty Expressions(https://arxiv.org/abs/2410.14309)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While Large Language Models (LLMs) demonstrate impressive capabilities, they still struggle with generating factually incorrect content (i.e., hallucinations). A promising approach to mitigate this issue is enabling models to express uncertainty when unsure. Previous research on uncertainty modeling has primarily focused on short-form QA, but realworld applications often require much longer responses. In this work, we introduce the task of Long-form Generation with Uncertainty(LoGU). We identify two key challenges: Uncertainty Suppression, where models hesitate to express uncertainty, and Uncertainty Misalignment, where models convey uncertainty inaccurately. To tackle these challenges, we propose a refinement-based data collection framework and a two-stage training pipeline. Our framework adopts a divide-and-conquer strategy, refining uncertainty based on atomic claims. The collected data are then used in training through supervised fine-tuning (SFT) and direct preference optimization (DPO) to enhance uncertainty expression. Extensive experiments on three long-form instruction following datasets show that our method significantly improves accuracy, reduces hallucinations, and maintains the comprehensiveness of responses.</li>
</ul>

<h3>Title: Not Sure Your Car Withstands Cyberwarfare</h3>
<ul>
<li><strong>Authors: </strong>Giampaolo Bella, Gianpietro Castiglione, Sergio Esposito, Mario Raciti, Salvatore Riccobene</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14320">https://arxiv.org/abs/2410.14320</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14320">https://arxiv.org/pdf/2410.14320</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14320]] Not Sure Your Car Withstands Cyberwarfare(https://arxiv.org/abs/2410.14320)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect, attack</a></li>
<li><strong>Abstract: </strong>Data and derived information about target victims has always been key for successful attacks, both during historical wars and modern cyber wars. Ours turns out to be an era in which modern cars generate a plethora of data about their drivers, and such data could be extremely attractive for offenders. This paper seeks to assess how well modern cars protect their drivers' data. It pursues its goal at a requirement level by analysing the gaps of the privacy policies of chief automakers such as BMW and Mercedes with respect to the General Data Protection Regulation (GDPR). It is found that both brands are still imprecise about how they comply with a number of GDPR articles, hence compliance often results non-verifiable. Most importantly, while BMW exhibits slightly broader compliance, both brands still fail to comply with a number of relevant articles of the regulation. An interpretation of these findings is a non-negligible likelihood that your car may turn against you should cyberwarfare break out.</li>
</ul>

<h3>Title: From Solitary Directives to Interactive Encouragement! LLM Secure Code Generation by Natural Language Prompting</h3>
<ul>
<li><strong>Authors: </strong>Shigang Liu, Bushra Sabir, Seung Ick Jang, Yuval Kansal, Yansong Gao, Kristen Moore, Alsharif Abuadbba, Surya Nepal</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.PL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14321">https://arxiv.org/abs/2410.14321</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14321">https://arxiv.org/pdf/2410.14321</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14321]] From Solitary Directives to Interactive Encouragement! LLM Secure Code Generation by Natural Language Prompting(https://arxiv.org/abs/2410.14321)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown remarkable potential in code generation, making them increasingly important in the field. However, the security issues of generated code have not been fully addressed, and the usability of LLMs in code generation still requires further exploration. This work introduces SecCode, a framework that leverages an innovative interactive encouragement prompting (EP) technique for secure code generation with \textit{only NL} prompts. This approach ensures that the prompts can be easily shared and understood by general users. SecCode functions through three stages: 1) Code Generation using NL Prompts; 2) Code Vulnerability Detection and Fixing, utilising our proposed encouragement prompting; 3) Vulnerability Cross-Checking and Code Security Refinement. These stages are executed in multiple interactive iterations to progressively enhance security. By using both proprietary LLMs (i.e., GPT-3.5 Turbo, GPT-4 and GPT-4o) and open-source LLMs (i.e., Llama 3.1 8B Instruct, DeepSeek Coder V2 Lite Instruct) evaluated on three benchmark datasets, extensive experimental results show that our proposed SecCode greatly outperforms compared baselines, generating secure code with a high vulnerability correction rate. For example, SecCode exhibits a high fix success rate of over 76\% after running 5 automated EP interactive iterations and over 89\% after running 10 automated EP interactive iterations. To the best of our knowledge, this work is the first to formulate secure code generation with NL prompts only. We have open-sourced our code and encourage the community to focus on secure code generation.</li>
</ul>

<h3>Title: HiCo: Hierarchical Controllable Diffusion Model for Layout-to-image Generation</h3>
<ul>
<li><strong>Authors: </strong>Bo Cheng, Yuhang Ma, Liebucha Wu, Shanyuan Liu, Ao Ma, Xiaoyu Wu, Dawei Leng, Yuhui Yin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14324">https://arxiv.org/abs/2410.14324</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14324">https://arxiv.org/pdf/2410.14324</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14324]] HiCo: Hierarchical Controllable Diffusion Model for Layout-to-image Generation(https://arxiv.org/abs/2410.14324)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The task of layout-to-image generation involves synthesizing images based on the captions of objects and their spatial positions. Existing methods still struggle in complex layout generation, where common bad cases include object missing, inconsistent lighting, conflicting view angles, etc. To effectively address these issues, we propose a \textbf{Hi}erarchical \textbf{Co}ntrollable (HiCo) diffusion model for layout-to-image generation, featuring object seperable conditioning branch structure. Our key insight is to achieve spatial disentanglement through hierarchical modeling of layouts. We use a multi branch structure to represent hierarchy and aggregate them in fusion module. To evaluate the performance of multi-objective controllable layout generation in natural scenes, we introduce the HiCo-7K benchmark, derived from the GRIT-20M dataset and manually cleaned. this https URL.</li>
</ul>

<h3>Title: Croc: Pretraining Large Multimodal Models with Cross-Modal Comprehension</h3>
<ul>
<li><strong>Authors: </strong>Yin Xie, Kaicheng Yang, Ninghua Yang, Weimo Deng, Xiangzi Dai, Tiancheng Gu, Yumeng Wang, Xiang An, Yongle Zhao, Ziyong Feng, Jiankang Deng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14332">https://arxiv.org/abs/2410.14332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14332">https://arxiv.org/pdf/2410.14332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14332]] Croc: Pretraining Large Multimodal Models with Cross-Modal Comprehension(https://arxiv.org/abs/2410.14332)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in Large Language Models (LLMs) have catalyzed the development of Large Multimodal Models (LMMs). However, existing research primarily focuses on tuning language and image instructions, ignoring the critical pretraining phase where models learn to process textual and visual modalities jointly. In this paper, we propose a new pretraining paradigm for LMMs to enhance the visual comprehension capabilities of LLMs by introducing a novel cross-modal comprehension stage. Specifically, we design a dynamically learnable prompt token pool and employ the Hungarian algorithm to replace part of the original visual tokens with the most relevant prompt tokens. Then, we conceptualize visual tokens as analogous to a "foreign language" for the LLMs and propose a mixed attention mechanism with bidirectional visual attention and unidirectional textual attention to comprehensively enhance the understanding of visual tokens. Meanwhile, we integrate a detailed caption generation task, leveraging rich descriptions to further facilitate LLMs in understanding visual semantic information. After pretraining on 1.5 million publicly accessible data, we present a new foundation model called Croc. Experimental results demonstrate that Croc achieves new state-of-the-art performance on massive vision-language benchmarks. To support reproducibility and facilitate further research, we release the training code and pre-trained model weights at this https URL.</li>
</ul>

<h3>Title: Critical Questions Generation: Motivation and Challenges</h3>
<ul>
<li><strong>Authors: </strong>Blanca Calvo Figueras, Rodrigo Agerri</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14335">https://arxiv.org/abs/2410.14335</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14335">https://arxiv.org/pdf/2410.14335</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14335]] Critical Questions Generation: Motivation and Challenges(https://arxiv.org/abs/2410.14335)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The development of Large Language Models (LLMs) has brought impressive performances on mitigation strategies against misinformation, such as counterargument generation. However, LLMs are still seriously hindered by outdated knowledge and by their tendency to generate hallucinated content. In order to circumvent these issues, we propose a new task, namely, Critical Questions Generation, consisting of processing an argumentative text to generate the critical questions (CQs) raised by it. In argumentation theory CQs are tools designed to lay bare the blind spots of an argument by pointing at the information it could be missing. Thus, instead of trying to deploy LLMs to produce knowledgeable and relevant counterarguments, we use them to question arguments, without requiring any external knowledge. Research on CQs Generation using LLMs requires a reference dataset for large scale experimentation. Thus, in this work we investigate two complementary methods to create such a resource: (i) instantiating CQs templates as defined by Walton's argumentation theory and (ii), using LLMs as CQs generators. By doing so, we contribute with a procedure to establish what is a valid CQ and conclude that, while LLMs are reasonable CQ generators, they still have a wide margin for improvement in this task.</li>
</ul>

<h3>Title: Zero-shot Action Localization via the Confidence of Large Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Josiah Aklilu, Xiaohan Wang, Serena Yeung-Levy</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14340">https://arxiv.org/abs/2410.14340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14340">https://arxiv.org/pdf/2410.14340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14340]] Zero-shot Action Localization via the Confidence of Large Vision-Language Models(https://arxiv.org/abs/2410.14340)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Precise action localization in untrimmed video is vital for fields such as professional sports and minimally invasive surgery, where the delineation of particular motions in recordings can dramatically enhance analysis. But in many cases, large scale datasets with video-label pairs for localization are unavailable, limiting the opportunity to fine-tune video-understanding models. Recent developments in large vision-language models (LVLM) address this need with impressive zero-shot capabilities in a variety of video understanding tasks. However, the adaptation of image-based LVLMs, with their powerful visual question answering capabilities, to action localization in long-form video is still relatively unexplored. To this end, we introduce a true ZEro-shot Action Localization method (ZEAL). Specifically, we leverage the built-in action knowledge of a large language model (LLM) to inflate actions into highly-detailed descriptions of the archetypal start and end of the action. These descriptions serve as queries to LVLM for generating frame-level confidence scores which can be aggregated to produce localization outputs. The simplicity and flexibility of our method lends it amenable to more capable LVLMs as they are developed, and we demonstrate remarkable results in zero-shot action localization on a challenging benchmark, without any training.</li>
</ul>

<h3>Title: A Scientific Machine Learning Approach for Predicting and Forecasting Battery Degradation in Electric Vehicles</h3>
<ul>
<li><strong>Authors: </strong>Sharv Murgai, Hrishikesh Bhagwat, Raj Abhijit Dandekar, Rajat Dandekar, Sreedath Panat</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14347">https://arxiv.org/abs/2410.14347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14347">https://arxiv.org/pdf/2410.14347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14347]] A Scientific Machine Learning Approach for Predicting and Forecasting Battery Degradation in Electric Vehicles(https://arxiv.org/abs/2410.14347)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Carbon emissions are rising at an alarming rate, posing a significant threat to global efforts to mitigate climate change. Electric vehicles have emerged as a promising solution, but their reliance on lithium-ion batteries introduces the critical challenge of battery degradation. Accurate prediction and forecasting of battery degradation over both short and long time spans are essential for optimizing performance, extending battery life, and ensuring effective long-term energy management. This directly influences the reliability, safety, and sustainability of EVs, supporting their widespread adoption and aligning with key UN SDGs. In this paper, we present a novel approach to the prediction and long-term forecasting of battery degradation using Scientific Machine Learning framework which integrates domain knowledge with neural networks, offering more interpretable and scientifically grounded solutions for both predicting short-term battery health and forecasting degradation over extended periods. This hybrid approach captures both known and unknown degradation dynamics, improving predictive accuracy while reducing data requirements. We incorporate ground-truth data to inform our models, ensuring that both the predictions and forecasts reflect practical conditions. The model achieved MSE of 9.90 with the UDE and 11.55 with the NeuralODE, in experimental data, a loss of 1.6986 with the UDE, and a MSE of 2.49 in the NeuralODE, demonstrating the enhanced precision of our approach. This integration of data-driven insights with SciML's strengths in interpretability and scalability allows for robust battery management. By enhancing battery longevity and minimizing waste, our approach contributes to the sustainability of energy systems and accelerates the global transition toward cleaner, more responsible energy solutions, aligning with the UN's SDG agenda.</li>
</ul>

<h3>Title: Impact of imperfect annotations on CNN training and performance for instance segmentation and classification in digital pathology</h3>
<ul>
<li><strong>Authors: </strong>Laura G√°lvez Jim√©nez, Christine Decaestecker</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14365">https://arxiv.org/abs/2410.14365</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14365">https://arxiv.org/pdf/2410.14365</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14365]] Impact of imperfect annotations on CNN training and performance for instance segmentation and classification in digital pathology(https://arxiv.org/abs/2410.14365)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Segmentation and classification of large numbers of instances, such as cell nuclei, are crucial tasks in digital pathology for accurate diagnosis. However, the availability of high-quality datasets for deep learning methods is often limited due to the complexity of the annotation process. In this work, we investigate the impact of noisy annotations on the training and performance of a state-of-the-art CNN model for the combined task of detecting, segmenting and classifying nuclei in histopathology images. In this context, we investigate the conditions for determining an appropriate number of training epochs to prevent overfitting to annotation noise during training. Our results indicate that the utilisation of a small, correctly annotated validation set is instrumental in avoiding overfitting and maintaining model performance to a large extent. Additionally, our findings underscore the beneficial role of pre-training.</li>
</ul>

<h3>Title: Fine-Tuning Pre-trained Language Models for Robust Causal Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Jialin Yu, Yuxiang Zhou, Yulan He, Nevin L. Zhang, Ricardo Silva</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14375">https://arxiv.org/abs/2410.14375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14375">https://arxiv.org/pdf/2410.14375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14375]] Fine-Tuning Pre-trained Language Models for Robust Causal Representation Learning(https://arxiv.org/abs/2410.14375)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The fine-tuning of pre-trained language models (PLMs) has been shown to be effective across various domains. By using domain-specific supervised data, the general-purpose representation derived from PLMs can be transformed into a domain-specific representation. However, these methods often fail to generalize to out-of-domain (OOD) data due to their reliance on non-causal representations, often described as spurious features. Existing methods either make use of adjustments with strong assumptions about lack of hidden common causes, or mitigate the effect of spurious features using multi-domain data. In this work, we investigate how fine-tuned pre-trained language models aid generalizability from single-domain scenarios under mild assumptions, targeting more general and practical real-world scenarios. We show that a robust representation can be derived through a so-called causal front-door adjustment, based on a decomposition assumption, using fine-tuned representations as a source of data augmentation. Comprehensive experiments in both synthetic and real-world settings demonstrate the superior generalizability of the proposed method compared to existing approaches. Our work thus sheds light on the domain generalization problem by introducing links between fine-tuning and causal mechanisms into representation learning.</li>
</ul>

<h3>Title: How Do Multilingual Models Remember? Investigating Multilingual Factual Recall Mechanisms</h3>
<ul>
<li><strong>Authors: </strong>Constanza Fierro, Negar Foroutan, Desmond Elliott, Anders S√∏gaard</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14387">https://arxiv.org/abs/2410.14387</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14387">https://arxiv.org/pdf/2410.14387</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14387]] How Do Multilingual Models Remember? Investigating Multilingual Factual Recall Mechanisms(https://arxiv.org/abs/2410.14387)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) store and retrieve vast amounts of factual knowledge acquired during pre-training. Prior research has localized and identified mechanisms behind knowledge recall; however, it has primarily focused on English monolingual models. The question of how these processes generalize to other languages and multilingual LLMs remains unexplored. In this paper, we address this gap by conducting a comprehensive analysis of two highly multilingual LLMs. We assess the extent to which previously identified components and mechanisms of factual recall in English apply to a multilingual context. Then, we examine when language plays a role in the recall process, uncovering evidence of language-independent and language-dependent mechanisms.</li>
</ul>

<h3>Title: Unscrambling disease progression at scale: fast inference of event permutations with optimal transport</h3>
<ul>
<li><strong>Authors: </strong>Peter A. Wijeratne, Daniel C. Alexander</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14388">https://arxiv.org/abs/2410.14388</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14388">https://arxiv.org/pdf/2410.14388</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14388]] Unscrambling disease progression at scale: fast inference of event permutations with optimal transport(https://arxiv.org/abs/2410.14388)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Disease progression models infer group-level temporal trajectories of change in patients' features as a chronic degenerative condition plays out. They provide unique insight into disease biology and staging systems with individual-level clinical utility. Discrete models consider disease progression as a latent permutation of events, where each event corresponds to a feature becoming measurably abnormal. However, permutation inference using traditional maximum likelihood approaches becomes prohibitive due to combinatoric explosion, severely limiting model dimensionality and utility. Here we leverage ideas from optimal transport to model disease progression as a latent permutation matrix of events belonging to the Birkhoff polytope, facilitating fast inference via optimisation of the variational lower bound. This enables a factor of 1000 times faster inference than the current state of the art and, correspondingly, supports models with several orders of magnitude more features than the current state of the art can consider. Experiments demonstrate the increase in speed, accuracy and robustness to noise in simulation. Further experiments with real-world imaging data from two separate datasets, one from Alzheimer's disease patients, the other age-related macular degeneration, showcase, for the first time, pixel-level disease progression events in the brain and eye, respectively. Our method is low compute, interpretable and applicable to any progressive condition and data modality, giving it broad potential clinical utility.</li>
</ul>

<h3>Title: Personalizing Low-Rank Bayesian Neural Networks Via Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Boning Zhang, Dongzhu Liu, Osvaldo Simeone, Guanchu Wang, Dimitrios Pezaros, Guangxu Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14390">https://arxiv.org/abs/2410.14390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14390">https://arxiv.org/pdf/2410.14390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14390]] Personalizing Low-Rank Bayesian Neural Networks Via Federated Learning(https://arxiv.org/abs/2410.14390)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>To support real-world decision-making, it is crucial for models to be well-calibrated, i.e., to assign reliable confidence estimates to their predictions. Uncertainty quantification is particularly important in personalized federated learning (PFL), as participating clients typically have small local datasets, making it difficult to unambiguously determine optimal model parameters. Bayesian PFL (BPFL) methods can potentially enhance calibration, but they often come with considerable computational and memory requirements due to the need to track the variances of all the individual model parameters. Furthermore, different clients may exhibit heterogeneous uncertainty levels owing to varying local dataset sizes and distributions. To address these challenges, we propose LR-BPFL, a novel BPFL method that learns a global deterministic model along with personalized low-rank Bayesian corrections. To tailor the local model to each client's inherent uncertainty level, LR-BPFL incorporates an adaptive rank selection mechanism. We evaluate LR-BPFL across a variety of datasets, demonstrating its advantages in terms of calibration, accuracy, as well as computational and memory requirements.</li>
</ul>

<h3>Title: Analyzing Context Utilization of LLMs in Document-Level Translation</h3>
<ul>
<li><strong>Authors: </strong>Wafaa Mohammed, Vlad Niculae</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14391">https://arxiv.org/abs/2410.14391</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14391">https://arxiv.org/pdf/2410.14391</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14391]] Analyzing Context Utilization of LLMs in Document-Level Translation(https://arxiv.org/abs/2410.14391)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLM) are increasingly strong contenders in machine translation. We study document-level translation, where some words cannot be translated without context from outside the sentence. We investigate the ability of prominent LLMs to utilize context by analyzing models' robustness to perturbed and randomized document context. We find that LLMs' improved document-translation performance is not always reflected in pronoun translation performance. We highlight the need for context-aware finetuning of LLMs with a focus on relevant parts of the context to improve their reliability for document-level translation.</li>
</ul>

<h3>Title: Debug Smarter, Not Harder: AI Agents for Error Resolution in Computational Notebooks</h3>
<ul>
<li><strong>Authors: </strong>Konstantin Grotov, Artem Borzilov, Maksim Krivobok, Timofey Bryksin, Yaroslav Zharov</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14393">https://arxiv.org/abs/2410.14393</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14393">https://arxiv.org/pdf/2410.14393</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14393]] Debug Smarter, Not Harder: AI Agents for Error Resolution in Computational Notebooks(https://arxiv.org/abs/2410.14393)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Computational notebooks became indispensable tools for research-related development, offering unprecedented interactivity and flexibility in the development process. However, these benefits come at the cost of reproducibility and an increased potential for bugs. With the rise of code-fluent Large Language Models empowered with agentic techniques, smart bug-fixing tools with a high level of autonomy have emerged. However, those tools are tuned for classical script programming and still struggle with non-linear computational notebooks. In this paper, we present an AI agent designed specifically for error resolution in a computational notebook. We have developed an agentic system capable of exploring a notebook environment by interacting with it -- similar to how a user would -- and integrated the system into the JetBrains service for collaborative data science called Datalore. We evaluate our approach against the pre-existing single-action solution by comparing costs and conducting a user study. Users rate the error resolution capabilities of the agentic system higher but experience difficulties with UI. We share the results of the study and consider them valuable for further improving user-agent collaboration.</li>
</ul>

<h3>Title: Generative AI, Pragmatics, and Authenticity in Second Language Learning</h3>
<ul>
<li><strong>Authors: </strong>Robert Godwin-Jones`</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14395">https://arxiv.org/abs/2410.14395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14395">https://arxiv.org/pdf/2410.14395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14395]] Generative AI, Pragmatics, and Authenticity in Second Language Learning(https://arxiv.org/abs/2410.14395)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>There are obvious benefits to integrating generative AI (artificial intelligence) into language learning and teaching. Those include using AI as a language tutor, creating learning materials, or assessing learner output. However, due to how AI systems under-stand human language, based on a mathematical model using statistical probability, they lack the lived experience to be able to use language with the same social aware-ness as humans. Additionally, there are built-in linguistic and cultural biases based on their training data which is mostly in English and predominantly from Western sources. Those facts limit AI suitability for some language learning interactions. Stud-ies have clearly shown that systems such as ChatGPT often do not produce language that is pragmatically appropriate. The lack of linguistic and cultural authenticity has important implications for how AI is integrated into second language acquisition as well as in instruction targeting development of intercultural communication compe-tence.</li>
</ul>

<h3>Title: Design and Prototype of a Unified Framework for Error-robust Compression and Encryption in IoT</h3>
<ul>
<li><strong>Authors: </strong>Gajraj Kuldeep, Qi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14396">https://arxiv.org/abs/2410.14396</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14396">https://arxiv.org/pdf/2410.14396</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14396]] Design and Prototype of a Unified Framework for Error-robust Compression and Encryption in IoT(https://arxiv.org/abs/2410.14396)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, robust</a></li>
<li><strong>Abstract: </strong>The Internet of Things (IoT) relies on resource-constrained devices for data acquisition, but the vast amount of data generated and security concerns present challenges for efficient data handling and confidentiality. Conventional techniques for data compression and secrecy often lack energy efficiency for these devices. Compressive sensing has the potential to compress data and maintain secrecy, but many solutions do not address the issue of packet loss or errors caused by unreliable wireless channels. To address these issues, we have developed the ENCRUST scheme, which combines compression, secrecy, and error recovery. In this paper, we present a prototype of ENCRUST that uses energy-efficient operations, as well as a lighter variant called L-ENCRUST. We also perform security analysis and compare the performance of ENCRUST and L-ENCRUST with a state-of-the-art solution in terms of memory, encryption time, and energy consumption on a resource-constrained TelosB mote. Our results show that both ENCRUST and L-ENCRUST outperform the state-of-the-art solution in these metrics.</li>
</ul>

<h3>Title: Dynamic Negative Guidance of Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Felix Koulischer, Johannes Deleu, Gabriel Raya, Thomas Demeester, Luca Ambrogioni</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14398">https://arxiv.org/abs/2410.14398</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14398">https://arxiv.org/pdf/2410.14398</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14398]] Dynamic Negative Guidance of Diffusion Models(https://arxiv.org/abs/2410.14398)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Negative Prompting (NP) is widely utilized in diffusion models, particularly in text-to-image applications, to prevent the generation of undesired features. In this paper, we show that conventional NP is limited by the assumption of a constant guidance scale, which may lead to highly suboptimal results, or even complete failure, due to the non-stationarity and state-dependence of the reverse process. Based on this analysis, we derive a principled technique called Dynamic Negative Guidance, which relies on a near-optimal time and state dependent modulation of the guidance without requiring additional training. Unlike NP, negative guidance requires estimating the posterior class probability during the denoising process, which is achieved with limited additional computational overhead by tracking the discrete Markov Chain during the generative process. We evaluate the performance of DNG class-removal on MNIST and CIFAR10, where we show that DNG leads to higher safety, preservation of class balance and image quality when compared with baseline methods. Furthermore, we show that it is possible to use DNG with Stable Diffusion to obtain more accurate and less invasive guidance than NP.</li>
</ul>

<h3>Title: SylloBio-NLI: Evaluating Large Language Models on Biomedical Syllogistic Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Magdalena Wysocka, Danilo S. Carvalho, Oskar Wysocki, Marco Valentino, Andre Freitas</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14399">https://arxiv.org/abs/2410.14399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14399">https://arxiv.org/pdf/2410.14399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14399]] SylloBio-NLI: Evaluating Large Language Models on Biomedical Syllogistic Reasoning(https://arxiv.org/abs/2410.14399)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Syllogistic reasoning is crucial for Natural Language Inference (NLI). This capability is particularly significant in specialized domains such as biomedicine, where it can support automatic evidence interpretation and scientific discovery. This paper presents SylloBio-NLI, a novel framework that leverages external ontologies to systematically instantiate diverse syllogistic arguments for biomedical NLI. We employ SylloBio-NLI to evaluate Large Language Models (LLMs) on identifying valid conclusions and extracting supporting evidence across 28 syllogistic schemes instantiated with human genome pathways. Extensive experiments reveal that biomedical syllogistic reasoning is particularly challenging for zero-shot LLMs, which achieve an average accuracy between 70% on generalized modus ponens and 23% on disjunctive syllogism. At the same time, we found that few-shot prompting can boost the performance of different LLMs, including Gemma (+14%) and LLama-3 (+43%). However, a deeper analysis shows that both techniques exhibit high sensitivity to superficial lexical variations, highlighting a dependency between reliability, models' architecture, and pre-training regime. Overall, our results indicate that, while in-context examples have the potential to elicit syllogistic reasoning in LLMs, existing models are still far from achieving the robustness and consistency required for safe biomedical NLI applications.</li>
</ul>

<h3>Title: Fact Recall, Heuristics or Pure Guesswork? Precise Interpretations of Language Models for Fact Completion</h3>
<ul>
<li><strong>Authors: </strong>Denitsa Saynova, Lovisa Hagstr√∂m, Moa Johansson, Richard Johansson, Marco Kuhlmann</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14405">https://arxiv.org/abs/2410.14405</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14405">https://arxiv.org/pdf/2410.14405</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14405]] Fact Recall, Heuristics or Pure Guesswork? Precise Interpretations of Language Models for Fact Completion(https://arxiv.org/abs/2410.14405)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Previous interpretations of language models (LMs) miss important distinctions in how these models process factual information. For example, given the query "Astrid Lindgren was born in" with the corresponding completion "Sweden", no difference is made between whether the prediction was based on having the exact knowledge of the birthplace of the Swedish author or assuming that a person with a Swedish-sounding name was born in Sweden. In this paper, we investigate four different prediction scenarios for which the LM can be expected to show distinct behaviors. These scenarios correspond to different levels of model reliability and types of information being processed - some being less desirable for factual predictions. To facilitate precise interpretations of LMs for fact completion, we propose a model-specific recipe called PrISM for constructing datasets with examples of each scenario based on a set of diagnostic criteria. We apply a popular interpretability method, causal tracing (CT), to the four prediction scenarios and find that while CT produces different results for each scenario, aggregations over a set of mixed examples may only represent the results from the scenario with the strongest measured signal. In summary, we contribute tools for a more granular study of fact completion in language models and analyses that provide a more nuanced understanding of how LMs process fact-related queries.</li>
</ul>

<h3>Title: An explainable machine learning approach for energy forecasting at the household level</h3>
<ul>
<li><strong>Authors: </strong>Pauline B√©raud, Margaux Rioux, Michel Babany, Philippe de La Chevasnerie, Damien Theis, Giacomo Teodori, Chlo√© Pinguet, Romane Rigaud, Fran√ßois Leclerc</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14416">https://arxiv.org/abs/2410.14416</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14416">https://arxiv.org/pdf/2410.14416</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14416]] An explainable machine learning approach for energy forecasting at the household level(https://arxiv.org/abs/2410.14416)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, explainability</a></li>
<li><strong>Abstract: </strong>Electricity forecasting has been a recurring research topic, as it is key to finding the right balance between production and consumption. While most papers are focused on the national or regional scale, few are interested in the household level. Desegregated forecast is a common topic in Machine Learning (ML) literature but lacks explainability that household energy forecasts require. This paper specifically targets the challenges of forecasting electricity use at the household level. This paper confronts common Machine Learning algorithms to electricity household forecasts, weighing the pros and cons, including accuracy and explainability with well-known key metrics. Furthermore, we also confront them in this paper with the business challenges specific to this sector such as explainability or outliers resistance. We introduce a custom decision tree, aiming at providing a fair estimate of the energy consumption, while being explainable and consistent with human intuition. We show that this novel method allows greater explainability without sacrificing much accuracy. The custom tree methodology can be used in various business use cases but is subject to limitations, such as a lack of resilience with outliers.</li>
</ul>

<h3>Title: Unlearning Backdoor Attacks for LLMs with Weak-to-Strong Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Shuai Zhao, Xiaobao Wu, Cong-Duy Nguyen, Meihuizi Jia, Yichao Feng, Luu Anh Tuan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14425">https://arxiv.org/abs/2410.14425</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14425">https://arxiv.org/pdf/2410.14425</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14425]] Unlearning Backdoor Attacks for LLMs with Weak-to-Strong Knowledge Distillation(https://arxiv.org/abs/2410.14425)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Parameter-efficient fine-tuning (PEFT) can bridge the gap between large language models (LLMs) and downstream tasks. However, PEFT has been proven vulnerable to malicious attacks. Research indicates that poisoned LLMs, even after PEFT, retain the capability to activate internalized backdoors when input samples contain predefined triggers. In this paper, we introduce a novel weak-to-strong unlearning algorithm to defend against backdoor attacks based on feature alignment knowledge distillation, named W2SDefense. Specifically, we first train a small-scale language model through full-parameter fine-tuning to serve as the clean teacher model. Then, this teacher model guides the large-scale poisoned student model in unlearning the backdoor, leveraging PEFT. Theoretical analysis suggests that W2SDefense has the potential to enhance the student model's ability to unlearn backdoor features, preventing the activation of the backdoor. We conduct experiments on text classification tasks involving three state-of-the-art language models and three different backdoor attack algorithms. Our empirical results demonstrate the outstanding performance of W2SDefense in defending against backdoor attacks without compromising model performance.</li>
</ul>

<h3>Title: FashionR2R: Texture-preserving Rendered-to-Real Image Translation with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Rui Hu, Qian He, Gaofeng He, Jiedong Zhuang, Huang Chen, Huafeng Liu, Huamin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14429">https://arxiv.org/abs/2410.14429</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14429">https://arxiv.org/pdf/2410.14429</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14429]] FashionR2R: Texture-preserving Rendered-to-Real Image Translation with Diffusion Models(https://arxiv.org/abs/2410.14429)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Modeling and producing lifelike clothed human images has attracted researchers' attention from different areas for decades, with the complexity from highly articulated and structured content. Rendering algorithms decompose and simulate the imaging process of a camera, while are limited by the accuracy of modeled variables and the efficiency of computation. Generative models can produce impressively vivid human images, however still lacking in controllability and editability. This paper studies photorealism enhancement of rendered images, leveraging generative power from diffusion models on the controlled basis of rendering. We introduce a novel framework to translate rendered images into their realistic counterparts, which consists of two stages: Domain Knowledge Injection (DKI) and Realistic Image Generation (RIG). In DKI, we adopt positive (real) domain finetuning and negative (rendered) domain embedding to inject knowledge into a pretrained Text-to-image (T2I) diffusion model. In RIG, we generate the realistic image corresponding to the input rendered image, with a Texture-preserving Attention Control (TAC) to preserve fine-grained clothing textures, exploiting the decoupled features encoded in the UNet structure. Additionally, we introduce SynFashion dataset, featuring high-quality digital clothing images with diverse textures. Extensive experimental results demonstrate the superiority and effectiveness of our method in rendered-to-real image translation.</li>
</ul>

<h3>Title: A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference</h3>
<ul>
<li><strong>Authors: </strong>You Wu, Haoyi Wu, Kewei Tu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14442">https://arxiv.org/abs/2410.14442</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14442">https://arxiv.org/pdf/2410.14442</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14442]] A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference(https://arxiv.org/abs/2410.14442)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>Recently, sharing key-value (KV) cache across layers has been found effective in efficient inference of large language models (LLMs). To systematically investigate different techniques of cross-layer KV sharing, we propose a unified framework that covers several recent methods and their novel variants. We conduct comprehensive experiments on all the configurations of the framework, evaluating their generation throughput and performance in language modeling and downstream tasks. We find that when reducing the size of the KV cache by 2x, most configurations can achieve competitive performance to and higher throughput than standard transformers, but when further reducing the size of the KV cache, pairing queries of all layers with KVs of upper layers can better maintain performance, although it also introduces additional training cost and prefilling latency. We hope that this work will help users choose the appropriate approach according to their requirements and facilitate research on the acceleration of LLM inference.</li>
</ul>

<h3>Title: Toward Generalizing Visual Brain Decoding to Unseen Subjects</h3>
<ul>
<li><strong>Authors: </strong>Xiangtao Kong, Kexin Huang, Ping Li, Lei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14445">https://arxiv.org/abs/2410.14445</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14445">https://arxiv.org/pdf/2410.14445</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14445]] Toward Generalizing Visual Brain Decoding to Unseen Subjects(https://arxiv.org/abs/2410.14445)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Visual brain decoding aims to decode visual information from human brain activities. Despite the great progress, one critical limitation of current brain decoding research lies in the lack of generalization capability to unseen subjects. Prior works typically focus on decoding brain activity of individuals based on the observation that different subjects exhibit different brain activities, while it remains unclear whether brain decoding can be generalized to unseen subjects. This study aims to answer this question. We first consolidate an image-fMRI dataset consisting of stimulus-image and fMRI-response pairs, involving 177 subjects in the movie-viewing task of the Human Connectome Project (HCP). This dataset allows us to investigate the brain decoding performance with the increase of participants. We then present a learning paradigm that applies uniform processing across all subjects, instead of employing different network heads or tokenizers for individuals as in previous methods, which can accommodate a large number of subjects to explore the generalization capability across different subjects. A series of experiments are conducted and we have the following findings. First, the network exhibits clear generalization capabilities with the increase of training subjects. Second, the generalization capability is common to popular network architectures (MLP, CNN and Transformer). Third, the generalization performance is affected by the similarity between subjects. Our findings reveal the inherent similarities in brain activities across individuals. With the emerging of larger and more comprehensive datasets, it is possible to train a brain decoding foundation model in the this http URL and models can be found at this https URL.</li>
</ul>

<h3>Title: LUDVIG: Learning-free Uplifting of 2D Visual features to Gaussian Splatting scenes</h3>
<ul>
<li><strong>Authors: </strong>Juliette Marrie, Romain M√©n√©gaux, Michael Arbel, Diane Larlus, Julien Mairal</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14462">https://arxiv.org/abs/2410.14462</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14462">https://arxiv.org/pdf/2410.14462</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14462]] LUDVIG: Learning-free Uplifting of 2D Visual features to Gaussian Splatting scenes(https://arxiv.org/abs/2410.14462)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>We address the task of uplifting visual features or semantic masks from 2D vision models to 3D scenes represented by Gaussian Splatting. Whereas common approaches rely on iterative optimization-based procedures, we show that a simple yet effective aggregation technique yields excellent results. Applied to semantic masks from Segment Anything (SAM), our uplifting approach leads to segmentation quality comparable to the state of the art. We then extend this method to generic DINOv2 features, integrating 3D scene geometry through graph diffusion, and achieve competitive segmentation results despite DINOv2 not being trained on millions of annotated masks like SAM.</li>
</ul>

<h3>Title: Electrocardiogram-Language Model for Few-Shot Question Answering with Meta Learning</h3>
<ul>
<li><strong>Authors: </strong>Jialu Tang, Tong Xia, Yuan Lu, Cecilia Mascolo, Aaqib Saeed</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14464">https://arxiv.org/abs/2410.14464</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14464">https://arxiv.org/pdf/2410.14464</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14464]] Electrocardiogram-Language Model for Few-Shot Question Answering with Meta Learning(https://arxiv.org/abs/2410.14464)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Electrocardiogram (ECG) interpretation requires specialized expertise, often involving synthesizing insights from ECG signals with complex clinical queries posed in natural language. The scarcity of labeled ECG data coupled with the diverse nature of clinical inquiries presents a significant challenge for developing robust and adaptable ECG diagnostic systems. This work introduces a novel multimodal meta-learning method for few-shot ECG question answering, addressing the challenge of limited labeled data while leveraging the rich knowledge encoded within large language models (LLMs). Our LLM-agnostic approach integrates a pre-trained ECG encoder with a frozen LLM (e.g., LLaMA and Gemma) via a trainable fusion module, enabling the language model to reason about ECG data and generate clinically meaningful answers. Extensive experiments demonstrate superior generalization to unseen diagnostic tasks compared to supervised baselines, achieving notable performance even with limited ECG leads. For instance, in a 5-way 5-shot setting, our method using LLaMA-3.1-8B achieves accuracy of 84.6%, 77.3%, and 69.6% on single verify, choose and query question types, respectively. These results highlight the potential of our method to enhance clinical ECG interpretation by combining signal processing with the nuanced language understanding capabilities of LLMs, particularly in data-constrained scenarios.</li>
</ul>

<h3>Title: Enhancing Cryptocurrency Market Forecasting: Advanced Machine Learning Techniques and Industrial Engineering Contributions</h3>
<ul>
<li><strong>Authors: </strong>Jannatun Nayeem Pinky, Ramya Akula</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14475">https://arxiv.org/abs/2410.14475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14475">https://arxiv.org/pdf/2410.14475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14475]] Enhancing Cryptocurrency Market Forecasting: Advanced Machine Learning Techniques and Industrial Engineering Contributions(https://arxiv.org/abs/2410.14475)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Cryptocurrencies, as decentralized digital assets, have experienced rapid growth and adoption, with over 23,000 cryptocurrencies and a market capitalization nearing \$1.1 trillion (about \$3,400 per person in the US) as of 2023. This dynamic market presents significant opportunities and risks, highlighting the need for accurate price prediction models to manage volatility. This chapter comprehensively reviews machine learning (ML) techniques applied to cryptocurrency price prediction from 2014 to 2024. We explore various ML algorithms, including linear models, tree-based approaches, and advanced deep learning architectures such as transformers and large language models. Additionally, we examine the role of sentiment analysis in capturing market sentiment from textual data like social media posts and news articles to anticipate price fluctuations. With expertise in optimizing complex systems and processes, industrial engineers are pivotal in enhancing these models. They contribute by applying principles of process optimization, efficiency, and risk mitigation to improve computational performance and data management. This chapter highlights the evolving landscape of cryptocurrency price prediction, the integration of emerging technologies, and the significant role of industrial engineers in refining predictive models. By addressing current limitations and exploring future research directions, this chapter aims to advance the development of more accurate and robust prediction systems, supporting better-informed investment decisions and more stable market behavior.</li>
</ul>

<h3>Title: Laplace Transform Based Low-Complexity Learning of Continuous Markov Semigroups</h3>
<ul>
<li><strong>Authors: </strong>Vladimir R. Kostic, Karim Lounici, H√©l√®ne Halconruy, Timoth√©e Devergne, Pietro Novelli, Massimiliano Pontil</a></li>
<li><strong>Subjects: </strong>cs.LG, math.ST</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14477">https://arxiv.org/abs/2410.14477</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14477">https://arxiv.org/pdf/2410.14477</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14477]] Laplace Transform Based Low-Complexity Learning of Continuous Markov Semigroups(https://arxiv.org/abs/2410.14477)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Markov processes serve as a universal model for many real-world random processes. This paper presents a data-driven approach for learning these models through the spectral decomposition of the infinitesimal generator (IG) of the Markov semigroup. The unbounded nature of IGs complicates traditional methods such as vector-valued regression and Hilbert-Schmidt operator analysis. Existing techniques, including physics-informed kernel regression, are computationally expensive and limited in scope, with no recovery guarantees for transfer operator methods when the time-lag is small. We propose a novel method that leverages the IG's resolvent, characterized by the Laplace transform of transfer operators. This approach is robust to time-lag variations, ensuring accurate eigenvalue learning even for small time-lags. Our statistical analysis applies to a broader class of Markov processes than current methods while reducing computational complexity from quadratic to linear in the state dimension. Finally, we illustrate the behaviour of our method in two experiments.</li>
</ul>

<h3>Title: Backdoored Retrievers for Prompt Injection Attacks on Retrieval Augmented Generation of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Cody Clop, Yannick Teglia</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14479">https://arxiv.org/abs/2410.14479</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14479">https://arxiv.org/pdf/2410.14479</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14479]] Backdoored Retrievers for Prompt Injection Attacks on Retrieval Augmented Generation of Large Language Models(https://arxiv.org/abs/2410.14479)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capabilities in generating coherent text but remain limited by the static nature of their training data. Retrieval Augmented Generation (RAG) addresses this issue by combining LLMs with up-to-date information retrieval, but also expand the attack surface of the system. This paper investigates prompt injection attacks on RAG, focusing on malicious objectives beyond misinformation, such as inserting harmful links, promoting unauthorized services, and initiating denial-of-service behaviors. We build upon existing corpus poisoning techniques and propose a novel backdoor attack aimed at the fine-tuning process of the dense retriever component. Our experiments reveal that corpus poisoning can achieve significant attack success rates through the injection of a small number of compromised documents into the retriever corpus. In contrast, backdoor attacks demonstrate even higher success rates but necessitate a more complex setup, as the victim must fine-tune the retriever using the attacker poisoned dataset.</li>
</ul>

<h3>Title: Combining Entropy and Matrix Nuclear Norm for Enhanced Evaluation of Language Models</h3>
<ul>
<li><strong>Authors: </strong>James Vo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14480">https://arxiv.org/abs/2410.14480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14480">https://arxiv.org/pdf/2410.14480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14480]] Combining Entropy and Matrix Nuclear Norm for Enhanced Evaluation of Language Models(https://arxiv.org/abs/2410.14480)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) continue to advance, the need for precise and efficient evaluation metrics becomes more pressing. Traditional approaches, while informative, often face limitations in computational demands and interpretability. In this paper, we introduce a novel hybrid evaluation method that integrates two established techniques: entropy derived from covariance matrices and the Matrix Nuclear Norm (MNN). Our method begins by normalizing hidden states from LLMs, then computes the covariance matrix and MNN from these representations. We further calculate the entropy of the covariance matrix to capture uncertainty and redundancy in the model's outputs. By combining these metrics into a composite score, we offer a comprehensive evaluation framework that balances accuracy with computational efficiency. Additionally, our approach allows for flexibility in adjusting the weightings between entropy and MNN, tailoring the evaluation for different objectives. Through a series of experiments on various LLMs, we demonstrate the robustness and efficacy of our method, offering deeper insights into model performance. This work contributes to the ongoing development of LLM evaluation and opens avenues for future innovations in model assessment techniques.</li>
</ul>

<h3>Title: CaTs and DAGs: Integrating Directed Acyclic Graphs with Transformers and Fully-Connected Neural Networks for Causally Constrained Predictions</h3>
<ul>
<li><strong>Authors: </strong>Matthew J. Vowels, Mathieu Rochat, Sina Akbari</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14485">https://arxiv.org/abs/2410.14485</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14485">https://arxiv.org/pdf/2410.14485</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14485]] CaTs and DAGs: Integrating Directed Acyclic Graphs with Transformers and Fully-Connected Neural Networks for Causally Constrained Predictions(https://arxiv.org/abs/2410.14485)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, explainability, transformer</a></li>
<li><strong>Abstract: </strong>Artificial Neural Networks (ANNs), including fully-connected networks and transformers, are highly flexible and powerful function approximators, widely applied in fields like computer vision and natural language processing. However, their inability to inherently respect causal structures can limit their robustness, making them vulnerable to covariate shift and difficult to interpret/explain. This poses significant challenges for their reliability in real-world applications. In this paper, we introduce Causal Fully-Connected Neural Networks (CFCNs) and Causal Transformers (CaTs), two general model families designed to operate under predefined causal constraints, as specified by a Directed Acyclic Graph (DAG). These models retain the powerful function approximation abilities of traditional neural networks while adhering to the underlying structural constraints, improving robustness, reliability, and interpretability at inference time. This approach opens new avenues for deploying neural networks in more demanding, real-world scenarios where robustness and explainability is critical.</li>
</ul>

<h3>Title: ANT: Adaptive Noise Schedule for Time Series Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Seunghan Lee, Kibok Lee, Taeyoung Park</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14488">https://arxiv.org/abs/2410.14488</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14488">https://arxiv.org/pdf/2410.14488</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14488]] ANT: Adaptive Noise Schedule for Time Series Diffusion Models(https://arxiv.org/abs/2410.14488)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Advances in diffusion models for generative artificial intelligence have recently propagated to the time series (TS) domain, demonstrating state-of-the-art performance on various tasks. However, prior works on TS diffusion models often borrow the framework of existing works proposed in other domains without considering the characteristics of TS data, leading to suboptimal performance. In this work, we propose Adaptive Noise schedule for Time series diffusion models (ANT), which automatically predetermines proper noise schedules for given TS datasets based on their statistics representing non-stationarity. Our intuition is that an optimal noise schedule should satisfy the following desiderata: 1) It linearly reduces the non-stationarity of TS data so that all diffusion steps are equally meaningful, 2) the data is corrupted to the random noise at the final step, and 3) the number of steps is sufficiently large. The proposed method is practical for use in that it eliminates the necessity of finding the optimal noise schedule with a small additional cost to compute the statistics for given datasets, which can be done offline before training. We validate the effectiveness of our method across various tasks, including TS forecasting, refinement, and generation, on datasets from diverse domains. Code is available at this repository: this https URL.</li>
</ul>

<h3>Title: Safeguarding Blockchain Ecosystem: Understanding and Detecting Attack Transactions on Cross-chain Bridges</h3>
<ul>
<li><strong>Authors: </strong>Jiajing Wu, Kaixin Lin, Dan Lin, Bozhao Zhang, Zhiying Wu, Jianzhong Su</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14493">https://arxiv.org/abs/2410.14493</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14493">https://arxiv.org/pdf/2410.14493</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14493]] Safeguarding Blockchain Ecosystem: Understanding and Detecting Attack Transactions on Cross-chain Bridges(https://arxiv.org/abs/2410.14493)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Cross-chain bridges are essential decentralized applications (DApps) to facilitate interoperability between different blockchain networks. Unlike regular DApps, the functionality of cross-chain bridges relies on the collaboration of information both on and off the chain, which exposes them to a wider risk of attacks. According to our statistics, attacks on cross-chain bridges have resulted in losses of nearly 4.3 billion dollars since 2021. Therefore, it is particularly necessary to understand and detect attacks on cross-chain bridges. In this paper, we collect the largest number of cross-chain bridge attack incidents to date, including 49 attacks that occurred between June 2021 and September 2024. Our analysis reveal that attacks against cross-chain business logic cause significantly more damage than those that do not. These cross-chain attacks exhibit different patterns compared to normal transactions in terms of call structure, which effectively indicates potential attack behaviors. Given the significant losses in these cases and the scarcity of related research, this paper aims to detect attacks against cross-chain business logic, and propose the BridgeGuard tool. Specifically, BridgeGuard models cross-chain transactions from a graph perspective, and employs a two-stage detection framework comprising global and local graph mining to identify attack patterns in cross-chain transactions. We conduct multiple experiments on the datasets with 203 attack transactions and 40,000 normal cross-chain transactions. The results show that BridgeGuard's reported recall score is 36.32\% higher than that of state-of-the-art tools and can detect unknown attack transactions.</li>
</ul>

<h3>Title: Neural Real-Time Recalibration for Infrared Multi-Camera Systems</h3>
<ul>
<li><strong>Authors: </strong>Benyamin Mehmandar, Reza Talakoob, Charalambos Poullis</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14505">https://arxiv.org/abs/2410.14505</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14505">https://arxiv.org/pdf/2410.14505</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14505]] Neural Real-Time Recalibration for Infrared Multi-Camera Systems(https://arxiv.org/abs/2410.14505)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Currently, there are no learning-free or neural techniques for real-time recalibration of infrared multi-camera systems. In this paper, we address the challenge of real-time, highly-accurate calibration of multi-camera infrared systems, a critical task for time-sensitive applications. Unlike traditional calibration techniques that lack adaptability and struggle with on-the-fly recalibrations, we propose a neural network-based method capable of dynamic real-time calibration. The proposed method integrates a differentiable projection model that directly correlates 3D geometries with their 2D image projections and facilitates the direct optimization of both intrinsic and extrinsic camera parameters. Key to our approach is the dynamic camera pose synthesis with perturbations in camera parameters, emulating realistic operational challenges to enhance model robustness. We introduce two model variants: one designed for multi-camera systems with onboard processing of 2D points, utilizing the direct 2D projections of 3D fiducials, and another for image-based systems, employing color-coded projected points for implicitly establishing correspondence. Through rigorous experimentation, we demonstrate our method is more accurate than traditional calibration techniques with or without perturbations while also being real-time, marking a significant leap in the field of real-time multi-camera system calibration. The source code can be found at this https URL</li>
</ul>

<h3>Title: SignAttention: On the Interpretability of Transformer Models for Sign Language Translation</h3>
<ul>
<li><strong>Authors: </strong>Pedro Alejandro Dal Bianco, Oscar Agust√≠n Stanchi, Facundo Manuel Quiroga, Franco Ronchetti, Enzo Ferrante</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14506">https://arxiv.org/abs/2410.14506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14506">https://arxiv.org/pdf/2410.14506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14506]] SignAttention: On the Interpretability of Transformer Models for Sign Language Translation(https://arxiv.org/abs/2410.14506)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>This paper presents the first comprehensive interpretability analysis of a Transformer-based Sign Language Translation (SLT) model, focusing on the translation from video-based Greek Sign Language to glosses and text. Leveraging the Greek Sign Language Dataset, we examine the attention mechanisms within the model to understand how it processes and aligns visual input with sequential glosses. Our analysis reveals that the model pays attention to clusters of frames rather than individual ones, with a diagonal alignment pattern emerging between poses and glosses, which becomes less distinct as the number of glosses increases. We also explore the relative contributions of cross-attention and self-attention at each decoding step, finding that the model initially relies on video frames but shifts its focus to previously predicted tokens as the translation progresses. This work contributes to a deeper understanding of SLT models, paving the way for the development of more transparent and reliable translation systems essential for real-world applications.</li>
</ul>

<h3>Title: LEAD: Latent Realignment for Human Motion Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Nefeli Andreou, Xi Wang, Victoria Fern√°ndez Abrevaya, Marie-Paule Cani, Yiorgos Chrysanthou, Vicky Kalogeiton</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14508">https://arxiv.org/abs/2410.14508</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14508">https://arxiv.org/pdf/2410.14508</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14508]] LEAD: Latent Realignment for Human Motion Diffusion(https://arxiv.org/abs/2410.14508)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Our goal is to generate realistic human motion from natural language. Modern methods often face a trade-off between model expressiveness and text-to-motion alignment. Some align text and motion latent spaces but sacrifice expressiveness; others rely on diffusion models producing impressive motions, but lacking semantic meaning in their latent space. This may compromise realism, diversity, and applicability. Here, we address this by combining latent diffusion with a realignment mechanism, producing a novel, semantically structured space that encodes the semantics of language. Leveraging this capability, we introduce the task of textual motion inversion to capture novel motion concepts from a few examples. For motion synthesis, we evaluate LEAD on HumanML3D and KIT-ML and show comparable performance to the state-of-the-art in terms of realism, diversity, and text-motion consistency. Our qualitative analysis and user study reveal that our synthesized motions are sharper, more human-like and comply better with the text compared to modern methods. For motion textual inversion, our method demonstrates improved capacity in capturing out-of-distribution characteristics in comparison to traditional VAEs.</li>
</ul>

<h3>Title: Efficient Annotator Reliability Assessment and Sample Weighting for Knowledge-Based Misinformation Detection on Social Media</h3>
<ul>
<li><strong>Authors: </strong>Owen Cook, Charlie Grimshaw, Ben Wu, Sophie Dillon, Jack Hicks, Luke Jones, Thomas Smith, Matyas Szert, Xingyi Song</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14515">https://arxiv.org/abs/2410.14515</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14515">https://arxiv.org/pdf/2410.14515</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14515]] Efficient Annotator Reliability Assessment and Sample Weighting for Knowledge-Based Misinformation Detection on Social Media(https://arxiv.org/abs/2410.14515)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Misinformation spreads rapidly on social media, confusing the truth and targetting potentially vulnerable people. To effectively mitigate the negative impact of misinformation, it must first be accurately detected before applying a mitigation strategy, such as X's community notes, which is currently a manual process. This study takes a knowledge-based approach to misinformation detection, modelling the problem similarly to one of natural language inference. The EffiARA annotation framework is introduced, aiming to utilise inter- and intra-annotator agreement to understand the reliability of each annotator and influence the training of large language models for classification based on annotator reliability. In assessing the EffiARA annotation framework, the Russo-Ukrainian Conflict Knowledge-Based Misinformation Classification Dataset (RUC-MCD) was developed and made publicly available. This study finds that sample weighting using annotator reliability performs the best, utilising both inter- and intra-annotator agreement and soft-label training. The highest classification performance achieved using Llama-3.2-1B was a macro-F1 of 0.757 and 0.740 using TwHIN-BERT-large.</li>
</ul>

<h3>Title: Rethinking Distance Metrics for Counterfactual Explainability</h3>
<ul>
<li><strong>Authors: </strong>Joshua Nathaniel Williams, Anurag Katakkar, Hoda Heidari, J. Zico Kolter</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14522">https://arxiv.org/abs/2410.14522</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14522">https://arxiv.org/pdf/2410.14522</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14522]] Rethinking Distance Metrics for Counterfactual Explainability(https://arxiv.org/abs/2410.14522)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Counterfactual explanations have been a popular method of post-hoc explainability for a variety of settings in Machine Learning. Such methods focus on explaining classifiers by generating new data points that are similar to a given reference, while receiving a more desirable prediction. In this work, we investigate a framing for counterfactual generation methods that considers counterfactuals not as independent draws from a region around the reference, but as jointly sampled with the reference from the underlying data distribution. Through this framing, we derive a distance metric, tailored for counterfactual similarity that can be applied to a broad range of settings. Through both quantitative and qualitative analyses of counterfactual generation methods, we show that this framing allows us to express more nuanced dependencies among the covariates.</li>
</ul>

<h3>Title: Multi-modal Pose Diffuser: A Multimodal Generative Conditional Pose Prior</h3>
<ul>
<li><strong>Authors: </strong>Calvin-Khang Ta, Arindam Dutta, Rohit Kundu, Rohit Lal, Hannah Dela Cruz, Dripta S. Raychaudhuri, Amit Roy-Chowdhury</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14540">https://arxiv.org/abs/2410.14540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14540">https://arxiv.org/pdf/2410.14540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14540]] Multi-modal Pose Diffuser: A Multimodal Generative Conditional Pose Prior(https://arxiv.org/abs/2410.14540)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>The Skinned Multi-Person Linear (SMPL) model plays a crucial role in 3D human pose estimation, providing a streamlined yet effective representation of the human body. However, ensuring the validity of SMPL configurations during tasks such as human mesh regression remains a significant challenge , highlighting the necessity for a robust human pose prior capable of discerning realistic human poses. To address this, we introduce MOPED: \underline{M}ulti-m\underline{O}dal \underline{P}os\underline{E} \underline{D}iffuser. MOPED is the first method to leverage a novel multi-modal conditional diffusion model as a prior for SMPL pose parameters. Our method offers powerful unconditional pose generation with the ability to condition on multi-modal inputs such as images and text. This capability enhances the applicability of our approach by incorporating additional context often overlooked in traditional pose priors. Extensive experiments across three distinct tasks-pose estimation, pose denoising, and pose completion-demonstrate that our multi-modal diffusion model-based prior significantly outperforms existing methods. These results indicate that our model captures a broader spectrum of plausible human poses.</li>
</ul>

<h3>Title: Tell me what I need to know: Exploring LLM-based (Personalized) Abstractive Multi-Source Meeting Summarization</h3>
<ul>
<li><strong>Authors: </strong>Frederic Kirstein, Terry Ruas, Robert Kratel, Bela Gipp</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14545">https://arxiv.org/abs/2410.14545</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14545">https://arxiv.org/pdf/2410.14545</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14545]] Tell me what I need to know: Exploring LLM-based (Personalized) Abstractive Multi-Source Meeting Summarization(https://arxiv.org/abs/2410.14545)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Meeting summarization is crucial in digital communication, but existing solutions struggle with salience identification to generate personalized, workable summaries, and context understanding to fully comprehend the meetings' content. Previous attempts to address these issues by considering related supplementary resources (e.g., presentation slides) alongside transcripts are hindered by models' limited context sizes and handling the additional complexities of the multi-source tasks, such as identifying relevant information in additional files and seamlessly aligning it with the meeting content. This work explores multi-source meeting summarization considering supplementary materials through a three-stage large language model approach: identifying transcript passages needing additional context, inferring relevant details from supplementary materials and inserting them into the transcript, and generating a summary from this enriched transcript. Our multi-source approach enhances model understanding, increasing summary relevance by ~9% and producing more content-rich outputs. We introduce a personalization protocol that extracts participant characteristics and tailors summaries accordingly, improving informativeness by ~10%. This work further provides insights on performance-cost trade-offs across four leading model families, including edge-device capable options. Our approach can be extended to similar complex generative tasks benefitting from additional resources and personalization, such as dialogue systems and action planning.</li>
</ul>

<h3>Title: RAG-ConfusionQA: A Benchmark for Evaluating LLMs on Confusing Questions</h3>
<ul>
<li><strong>Authors: </strong>Zhiyuan Peng, Jinming Nian, Alexandre Evfimievski, Yi Fang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14567">https://arxiv.org/abs/2410.14567</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14567">https://arxiv.org/pdf/2410.14567</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14567]] RAG-ConfusionQA: A Benchmark for Evaluating LLMs on Confusing Questions(https://arxiv.org/abs/2410.14567)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Conversational AI agents use Retrieval Augmented Generation (RAG) to provide verifiable document-grounded responses to user inquiries. However, many natural questions do not have good answers: about 25\% contain false assumptions~\cite{Yu2023:CREPE}, and over 50\% are ambiguous~\cite{Min2020:AmbigQA}. RAG agents need high-quality data to improve their responses to confusing questions. This paper presents a novel synthetic data generation method to efficiently create a diverse set of context-grounded confusing questions from a given document corpus. We conduct an empirical comparative evaluation of several large language models as RAG agents to measure the accuracy of confusion detection and appropriate response generation. We contribute a benchmark dataset to the public domain.</li>
</ul>

<h3>Title: When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs</h3>
<ul>
<li><strong>Authors: </strong>Hanna Kim, Minkyoo Song, Seung Ho Na, Seungwon Shin, Kimin Lee</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14569">https://arxiv.org/abs/2410.14569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14569">https://arxiv.org/pdf/2410.14569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14569]] When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs(https://arxiv.org/abs/2410.14569)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Recent advancements in Large Language Models (LLMs) have established them as agentic systems capable of planning and interacting with various tools. These LLM agents are often paired with web-based tools, enabling access to diverse sources and real-time information. Although these advancements offer significant benefits across various applications, they also increase the risk of malicious use, particularly in cyberattacks involving personal information. In this work, we investigate the risks associated with misuse of LLM agents in cyberattacks involving personal data. Specifically, we aim to understand: 1) how potent LLM agents can be when directed to conduct cyberattacks, 2) how cyberattacks are enhanced by web-based tools, and 3) how affordable and easy it becomes to launch cyberattacks using LLM agents. We examine three attack scenarios: the collection of Personally Identifiable Information (PII), the generation of impersonation posts, and the creation of spear-phishing emails. Our experiments reveal the effectiveness of LLM agents in these attacks: LLM agents achieved a precision of up to 95.9% in collecting PII, up to 93.9% of impersonation posts created by LLM agents were evaluated as authentic, and the click rate for links in spear phishing emails created by LLM agents reached up to 46.67%. Additionally, our findings underscore the limitations of existing safeguards in contemporary commercial LLMs, emphasizing the urgent need for more robust security measures to prevent the misuse of LLM agents.</li>
</ul>

<h3>Title: Understanding the difficulty of low-precision post-training quantization of large language models</h3>
<ul>
<li><strong>Authors: </strong>Zifei Xu, Sayeh Sharify, Wanzin Yazar, Tristan Webb, Xin Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14570">https://arxiv.org/abs/2410.14570</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14570">https://arxiv.org/pdf/2410.14570</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14570]] Understanding the difficulty of low-precision post-training quantization of large language models(https://arxiv.org/abs/2410.14570)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models of high parameter counts are computationally expensive, yet can be made much more efficient by compressing their weights to very low numerical precision. This can be achieved either through post-training quantization by minimizing local, layer-wise quantization errors, or through quantization-aware fine-tuning by minimizing the global loss function. In this study, we discovered that, under the same data constraint, the former approach nearly always fared worse than the latter, a phenomenon particularly prominent when the numerical precision is very low. We further showed that this difficulty of post-training quantization arose from stark misalignment between optimization of the local and global objective functions. Our findings explains limited utility in minimization of local quantization error and the importance of direct quantization-aware fine-tuning, in the regime of large models at very low precision.</li>
</ul>

<h3>Title: Building Trust in Black-box Optimization: A Comprehensive Framework for Explainability</h3>
<ul>
<li><strong>Authors: </strong>Nazanin Nezami, Hadis Anahideh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14573">https://arxiv.org/abs/2410.14573</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14573">https://arxiv.org/pdf/2410.14573</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14573]] Building Trust in Black-box Optimization: A Comprehensive Framework for Explainability(https://arxiv.org/abs/2410.14573)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Optimizing costly black-box functions within a constrained evaluation budget presents significant challenges in many real-world applications. Surrogate Optimization (SO) is a common resolution, yet its proprietary nature introduced by the complexity of surrogate models and the sampling core (e.g., acquisition functions) often leads to a lack of explainability and transparency. While existing literature has primarily concentrated on enhancing convergence to global optima, the practical interpretation of newly proposed strategies remains underexplored, especially in batch evaluation settings. In this paper, we propose \emph{Inclusive} Explainability Metrics for Surrogate Optimization (IEMSO), a comprehensive set of model-agnostic metrics designed to enhance the transparency, trustworthiness, and explainability of the SO approaches. Through these metrics, we provide both intermediate and post-hoc explanations to practitioners before and after performing expensive evaluations to gain trust. We consider four primary categories of metrics, each targeting a specific aspect of the SO process: Sampling Core Metrics, Batch Properties Metrics, Optimization Process Metrics, and Feature Importance. Our experimental evaluations demonstrate the significant potential of the proposed metrics across different benchmarks.</li>
</ul>

<h3>Title: MomentumSMoE: Integrating Momentum into Sparse Mixture of Experts</h3>
<ul>
<li><strong>Authors: </strong>Rachel S.Y. Teo, Tan M. Nguyen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14574">https://arxiv.org/abs/2410.14574</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14574">https://arxiv.org/pdf/2410.14574</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14574]] MomentumSMoE: Integrating Momentum into Sparse Mixture of Experts(https://arxiv.org/abs/2410.14574)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Sparse Mixture of Experts (SMoE) has become the key to unlocking unparalleled scalability in deep learning. SMoE has the potential to exponentially increase parameter count while maintaining the efficiency of the model by only activating a small subset of these parameters for a given sample. However, it has been observed that SMoE suffers from unstable training and has difficulty adapting to new distributions, leading to the model's lack of robustness to data contamination. To overcome these limitations, we first establish a connection between the dynamics of the expert representations in SMoEs and gradient descent on a multi-objective optimization problem. Leveraging our framework, we then integrate momentum into SMoE and propose a new family of SMoEs named MomentumSMoE. We theoretically prove and numerically demonstrate that MomentumSMoE is more stable and robust than SMoE. In particular, we verify the advantages of MomentumSMoE over SMoE on a variety of practical tasks including ImageNet-1K object recognition and WikiText-103 language modeling. We demonstrate the applicability of MomentumSMoE to many types of SMoE models, including those in the Sparse MoE model for vision (V-MoE) and the Generalist Language Model (GLaM). We also show that other advanced momentum-based optimization methods, such as Adam, can be easily incorporated into the MomentumSMoE framework for designing new SMoE models with even better performance, almost negligible additional computation cost, and simple implementations.</li>
</ul>

<h3>Title: Large Language Models Are Overparameterized Text Encoders</h3>
<ul>
<li><strong>Authors: </strong>Thennal D K, Tim Fischer, Chris Biemann</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14578">https://arxiv.org/abs/2410.14578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14578">https://arxiv.org/pdf/2410.14578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14578]] Large Language Models Are Overparameterized Text Encoders(https://arxiv.org/abs/2410.14578)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) demonstrate strong performance as text embedding models when finetuned with supervised contrastive training. However, their large size balloons inference time and memory requirements. In this paper, we show that by pruning the last $p\%$ layers of an LLM before supervised training for only 1000 steps, we can achieve a proportional reduction in memory and inference time. We evaluate four different state-of-the-art LLMs on text embedding tasks and find that our method can prune up to 30\% of layers with negligible impact on performance and up to 80\% with only a modest drop. With only three lines of code, our method is easily implemented in any pipeline for transforming LLMs to text encoders. We also propose $\text{L}^3 \text{Prune}$, a novel layer-pruning strategy based on the model's initial loss that provides two optimal pruning configurations: a large variant with negligible performance loss and a small variant for resource-constrained settings. On average, the large variant prunes 21\% of the parameters with a $-0.3$ performance drop, and the small variant only suffers from a $-5.1$ decrease while pruning 74\% of the model. We consider these results strong evidence that LLMs are overparameterized for text embedding tasks, and can be easily pruned.</li>
</ul>

<h3>Title: Towards Unsupervised Validation of Anomaly-Detection Models</h3>
<ul>
<li><strong>Authors: </strong>Lihi Idan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14579">https://arxiv.org/abs/2410.14579</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14579">https://arxiv.org/pdf/2410.14579</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14579]] Towards Unsupervised Validation of Anomaly-Detection Models(https://arxiv.org/abs/2410.14579)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Unsupervised validation of anomaly-detection models is a highly challenging task. While the common practices for model validation involve a labeled validation set, such validation sets cannot be constructed when the underlying datasets are unlabeled. The lack of robust and efficient unsupervised model-validation techniques presents an acute challenge in the implementation of automated anomaly-detection pipelines, especially when there exists no prior knowledge of the model's performance on similar datasets. This work presents a new paradigm to automated validation of anomaly-detection models, inspired by real-world, collaborative decision-making mechanisms. We focus on two commonly-used, unsupervised model-validation tasks -- model selection and model evaluation -- and provide extensive experimental results that demonstrate the accuracy and robustness of our approach on both tasks.</li>
</ul>

<h3>Title: Neuro-Symbolic Traders: Assessing the Wisdom of AI Crowds in Markets</h3>
<ul>
<li><strong>Authors: </strong>Namid R. Stillman, Rory Baggott</a></li>
<li><strong>Subjects: </strong>cs.LG, q-fin.CP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14587">https://arxiv.org/abs/2410.14587</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14587">https://arxiv.org/pdf/2410.14587</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14587]] Neuro-Symbolic Traders: Assessing the Wisdom of AI Crowds in Markets(https://arxiv.org/abs/2410.14587)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Deep generative models are becoming increasingly used as tools for financial analysis. However, it is unclear how these models will influence financial markets, especially when they infer financial value in a semi-autonomous way. In this work, we explore the interplay between deep generative models and market dynamics. We develop a form of virtual traders that use deep generative models to make buy/sell decisions, which we term neuro-symbolic traders, and expose them to a virtual market. Under our framework, neuro-symbolic traders are agents that use vision-language models to discover a model of the fundamental value of an asset. Agents develop this model as a stochastic differential equation, calibrated to market data using gradient descent. We test our neuro-symbolic traders on both synthetic data and real financial time series, including an equity stock, commodity, and a foreign exchange pair. We then expose several groups of neuro-symbolic traders to a virtual market environment. This market environment allows for feedback between the traders belief of the underlying value to the observed price dynamics. We find that this leads to price suppression compared to the historical data, highlighting a future risk to market stability. Our work is a first step towards quantifying the effect of deep generative agents on markets dynamics and sets out some of the potential risks and benefits of this approach in the future.</li>
</ul>

<h3>Title: Toolshed: Scale Tool-Equipped Agents with Advanced RAG-Tool Fusion and Tool Knowledge Bases</h3>
<ul>
<li><strong>Authors: </strong>Elias Lumer</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14594">https://arxiv.org/abs/2410.14594</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14594">https://arxiv.org/pdf/2410.14594</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14594]] Toolshed: Scale Tool-Equipped Agents with Advanced RAG-Tool Fusion and Tool Knowledge Bases(https://arxiv.org/abs/2410.14594)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>Recent advancements in tool-equipped Agents (LLMs) have enabled complex tasks like secure database interactions and multi-agent code development. However, scaling tool capacity beyond agent reasoning or model limits remains a challenge. In this paper, we address these challenges by introducing Toolshed Knowledge Bases, a tool knowledge base (vector database) designed to store enhanced tool representations and optimize tool selection for large-scale tool-equipped Agents. Additionally, we propose Advanced RAG-Tool Fusion, a novel ensemble of tool-applied advanced retrieval-augmented generation (RAG) techniques across the pre-retrieval, intra-retrieval, and post-retrieval phases, without requiring model fine-tuning. During pre-retrieval, tool documents are enhanced with key information and stored in the Toolshed Knowledge Base. Intra-retrieval focuses on query planning and transformation to increase retrieval accuracy. Post-retrieval refines the retrieved tool documents and enables self-reflection. Furthermore, by varying both the total number of tools (tool-M) an Agent has access to and the tool selection threshold (top-k), we address trade-offs between retrieval accuracy, agent performance, and token cost. Our approach achieves 46%, 56%, and 47% absolute improvements on the ToolE single-tool, ToolE multi-tool and Seal-Tools benchmark datasets, respectively (Recall@5).</li>
</ul>

<h3>Title: Teaching Models to Balance Resisting and Accepting Persuasion</h3>
<ul>
<li><strong>Authors: </strong>Elias Stengel-Eskin, Peter Hase, Mohit Bansal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14596">https://arxiv.org/abs/2410.14596</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14596">https://arxiv.org/pdf/2410.14596</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14596]] Teaching Models to Balance Resisting and Accepting Persuasion(https://arxiv.org/abs/2410.14596)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are susceptible to persuasion, which can pose risks when models are faced with an adversarial interlocutor. We take a first step towards defending models against persuasion while also arguing that defense against adversarial (i.e. negative) persuasion is only half of the equation: models should also be able to accept beneficial (i.e. positive) persuasion to improve their answers. We show that optimizing models for only one side results in poor performance on the other. In order to balance positive and negative persuasion, we introduce Persuasion-Balanced Training (or PBT), which leverages multi-agent recursive dialogue trees to create data and trains models via preference optimization to accept persuasion when appropriate. PBT consistently improves resistance to misinformation and resilience to being challenged while also resulting in the best overall performance on holistic data containing both positive and negative persuasion. Crucially, we show that PBT models are better teammates in multi-agent debates. We find that without PBT, pairs of stronger and weaker models have unstable performance, with the order in which the models present their answers determining whether the team obtains the stronger or weaker model's performance. PBT leads to better and more stable results and less order dependence, with the stronger model consistently pulling the weaker one up.</li>
</ul>

<h3>Title: A dataset for cyber threat intelligence modeling of connected autonomous vehicles</h3>
<ul>
<li><strong>Authors: </strong>Yinghui Wang, Yilong Ren, Hongmao Qin, Zhiyong Cui, Yanan Zhao, Haiyang Yu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14600">https://arxiv.org/abs/2410.14600</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14600">https://arxiv.org/pdf/2410.14600</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14600]] A dataset for cyber threat intelligence modeling of connected autonomous vehicles(https://arxiv.org/abs/2410.14600)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, extraction</a></li>
<li><strong>Abstract: </strong>Cyber attacks have become a vital threat to connected autonomous vehicles in intelligent transportation systems. Cyber threat intelligence, as the collection of cyber threat information, provides an ideal approach for responding to emerging vehicle cyber threats and enabling proactive security defense. Obtaining valuable information from enormous cybersecurity data using knowledge extraction technologies to achieve cyber threat intelligence modeling is an effective means to ensure automotive cybersecurity. Unfortunately, there is no existing cybersecurity dataset available for cyber threat intelligence modeling research in the automotive field. This paper reports the creation of a cyber threat intelligence corpus focusing on vehicle cybersecurity knowledge mining. This dataset, annotated using a joint labeling strategy, comprises 908 real automotive cybersecurity reports, containing 3678 sentences, 8195 security entities and 4852 semantic relations. We further conduct a comprehensive analysis of cyber threat intelligence mining algorithms based on this corpus. The proposed dataset will serve as a valuable resource for evaluating the performance of existing algorithms and advancing research in cyber threat intelligence modeling within the automotive field.</li>
</ul>

<h3>Title: How Does Data Diversity Shape the Weight Landscape of Neural Networks?</h3>
<ul>
<li><strong>Authors: </strong>Yang Ba, Michelle V. Mancenido, Rong Pan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14602">https://arxiv.org/abs/2410.14602</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14602">https://arxiv.org/pdf/2410.14602</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14602]] How Does Data Diversity Shape the Weight Landscape of Neural Networks?(https://arxiv.org/abs/2410.14602)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>To enhance the generalization of machine learning models to unseen data, techniques such as dropout, weight decay ($L_2$ regularization), and noise augmentation are commonly employed. While regularization methods (i.e., dropout and weight decay) are geared toward adjusting model parameters to prevent overfitting, data augmentation increases the diversity of the input training set, a method purported to improve accuracy and calibration error. In this paper, we investigate the impact of each of these techniques on the parameter space of neural networks, with the goal of understanding how they alter the weight landscape in transfer learning scenarios. To accomplish this, we employ Random Matrix Theory to analyze the eigenvalue distributions of pre-trained models, fine-tuned using these techniques but using different levels of data diversity, for the same downstream tasks. We observe that diverse data influences the weight landscape in a similar fashion as dropout. Additionally, we compare commonly used data augmentation methods with synthetic data created by generative models. We conclude that synthetic data can bring more diversity into real input data, resulting in a better performance on out-of-distribution test instances.</li>
</ul>

<h3>Title: Streaming Deep Reinforcement Learning Finally Works</h3>
<ul>
<li><strong>Authors: </strong>Mohamed Elsayed, Gautham Vasan, A. Rupam Mahmood</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14606">https://arxiv.org/abs/2410.14606</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14606">https://arxiv.org/pdf/2410.14606</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14606]] Streaming Deep Reinforcement Learning Finally Works(https://arxiv.org/abs/2410.14606)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Natural intelligence processes experience as a continuous stream, sensing, acting, and learning moment-by-moment in real time. Streaming learning, the modus operandi of classic reinforcement learning (RL) algorithms like Q-learning and TD, mimics natural learning by using the most recent sample without storing it. This approach is also ideal for resource-constrained, communication-limited, and privacy-sensitive applications. However, in deep RL, learners almost always use batch updates and replay buffers, making them computationally expensive and incompatible with streaming learning. Although the prevalence of batch deep RL is often attributed to its sample efficiency, a more critical reason for the absence of streaming deep RL is its frequent instability and failure to learn, which we refer to as stream barrier. This paper introduces the stream-x algorithms, the first class of deep RL algorithms to overcome stream barrier for both prediction and control and match sample efficiency of batch RL. Through experiments in Mujoco Gym, DM Control Suite, and Atari Games, we demonstrate stream barrier in existing algorithms and successful stable learning with our stream-x algorithms: stream Q, stream AC, and stream TD, achieving the best model-free performance in DM Control Dog environments. A set of common techniques underlies the stream-x algorithms, enabling their success with a single set of hyperparameters and allowing for easy extension to other algorithms, thereby reviving streaming RL.</li>
</ul>

<h3>Title: Evaluating Privacy Measures in Healthcare Apps Predominantly Used by Older Adults</h3>
<ul>
<li><strong>Authors: </strong>Saka Suleiman, Sanchari Das</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14607">https://arxiv.org/abs/2410.14607</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14607">https://arxiv.org/pdf/2410.14607</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14607]] Evaluating Privacy Measures in Healthcare Apps Predominantly Used by Older Adults(https://arxiv.org/abs/2410.14607)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, protect</a></li>
<li><strong>Abstract: </strong>The widespread adoption of telehealth systems has led to a significant increase in the use of healthcare apps among older adults, but this rapid growth has also heightened concerns about the privacy of their health information. While HIPAA in the US and GDPR in the EU establish essential privacy protections for health information, limited research exists on the effectiveness of healthcare app privacy policies, particularly those used predominantly by older adults. To address this, we evaluated 28 healthcare apps across multiple dimensions, including regulatory compliance, data handling practices, and privacy-focused usability. To do this, we created a Privacy Risk Assessment Framework (PRAF) and used it to evaluate the privacy risks associated with these healthcare apps designed for older adults. Our analysis revealed significant gaps in compliance with privacy standards to such, only 25% of apps explicitly state compliance with HIPAA, and only 18% mention GDPR. Surprisingly, 79% of these applications lack breach protocols, putting older adults at risk in the event of a data breach.</li>
</ul>

<h3>Title: SIMformer: Single-Layer Vanilla Transformer Can Learn Free-Space Trajectory Similarity</h3>
<ul>
<li><strong>Authors: </strong>Chuang Yang, Renhe Jiang, Xiaohang Xu, Chuan Xiao, Kaoru Sezaki</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DB, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14629">https://arxiv.org/abs/2410.14629</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14629">https://arxiv.org/pdf/2410.14629</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14629]] SIMformer: Single-Layer Vanilla Transformer Can Learn Free-Space Trajectory Similarity(https://arxiv.org/abs/2410.14629)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Free-space trajectory similarity calculation, e.g., DTW, Hausdorff, and Frechet, often incur quadratic time complexity, thus learning-based methods have been proposed to accelerate the computation. The core idea is to train an encoder to transform trajectories into representation vectors and then compute vector similarity to approximate the ground truth. However, existing methods face dual challenges of effectiveness and efficiency: 1) they all utilize Euclidean distance to compute representation similarity, which leads to the severe curse of dimensionality issue -- reducing the distinguishability among representations and significantly affecting the accuracy of subsequent similarity search tasks; 2) most of them are trained in triplets manner and often necessitate additional information which downgrades the efficiency; 3) previous studies, while emphasizing the scalability in terms of efficiency, overlooked the deterioration of effectiveness when the dataset size grows. To cope with these issues, we propose a simple, yet accurate, fast, scalable model that only uses a single-layer vanilla transformer encoder as the feature extractor and employs tailored representation similarity functions to approximate various ground truth similarity measures. Extensive experiments demonstrate our model significantly mitigates the curse of dimensionality issue and outperforms the state-of-the-arts in effectiveness, efficiency, and scalability.</li>
</ul>

<h3>Title: Swiss Army Knife: Synergizing Biases in Knowledge from Vision Foundation Models for Multi-Task Learning</h3>
<ul>
<li><strong>Authors: </strong>Yuxiang Lu, Shengcao Cao, Yu-Xiong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14633">https://arxiv.org/abs/2410.14633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14633">https://arxiv.org/pdf/2410.14633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14633]] Swiss Army Knife: Synergizing Biases in Knowledge from Vision Foundation Models for Multi-Task Learning(https://arxiv.org/abs/2410.14633)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Vision Foundation Models (VFMs) have demonstrated outstanding performance on numerous downstream tasks. However, due to their inherent representation biases originating from different training paradigms, VFMs exhibit advantages and disadvantages across distinct vision tasks. Although amalgamating the strengths of multiple VFMs for downstream tasks is an intuitive strategy, effectively exploiting these biases remains a significant challenge. In this paper, we propose a novel and versatile "Swiss Army Knife" (SAK) solution, which adaptively distills knowledge from a committee of VFMs to enhance multi-task learning. Unlike existing methods that use a single backbone for knowledge transfer, our approach preserves the unique representation bias of each teacher by collaborating the lightweight Teacher-Specific Adapter Path modules with the Teacher-Agnostic Stem. Through dynamic selection and combination of representations with Mixture-of-Representations Routers, our SAK is capable of synergizing the complementary strengths of multiple VFMs. Extensive experiments show that our SAK remarkably outperforms prior state of the arts in multi-task learning by 10% on the NYUD-v2 benchmark, while also providing a flexible and robust framework that can readily accommodate more advanced model designs.</li>
</ul>

<h3>Title: GenEOL: Harnessing the Generative Power of LLMs for Training-Free Sentence Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Raghuveer Thirukovalluru, Bhuwan Dhingra</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14635">https://arxiv.org/abs/2410.14635</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14635">https://arxiv.org/pdf/2410.14635</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14635]] GenEOL: Harnessing the Generative Power of LLMs for Training-Free Sentence Embeddings(https://arxiv.org/abs/2410.14635)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Training-free embedding methods directly leverage pretrained large language models (LLMs) to embed text, bypassing the costly and complex procedure of contrastive learning. Previous training-free embedding methods have mainly focused on optimizing embedding prompts and have overlooked the benefits of utilizing the generative abilities of LLMs. We propose a novel method, GenEOL, which uses LLMs to generate diverse transformations of a sentence that preserve its meaning, and aggregates the resulting embeddings of these transformations to enhance the overall sentence embedding. GenEOL significantly outperforms the existing training-free embedding methods by an average of 2.85 points across several LLMs on the sentence semantic text similarity (STS) benchmark. Our analysis shows that GenEOL stabilizes representation quality across LLM layers and is robust to perturbations of embedding prompts. GenEOL also achieves notable gains on multiple clustering, reranking and pair-classification tasks from the MTEB benchmark.</li>
</ul>

<h3>Title: HR-Bandit: Human-AI Collaborated Linear Recourse Bandit</h3>
<ul>
<li><strong>Authors: </strong>Junyu Cao, Ruijiang Gao, Esmaeil Keyvanshokooh</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14640">https://arxiv.org/abs/2410.14640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14640">https://arxiv.org/pdf/2410.14640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14640]] HR-Bandit: Human-AI Collaborated Linear Recourse Bandit(https://arxiv.org/abs/2410.14640)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Human doctors frequently recommend actionable recourses that allow patients to modify their conditions to access more effective treatments. Inspired by such healthcare scenarios, we propose the Recourse Linear UCB ($\textsf{RLinUCB}$) algorithm, which optimizes both action selection and feature modifications by balancing exploration and exploitation. We further extend this to the Human-AI Linear Recourse Bandit ($\textsf{HR-Bandit}$), which integrates human expertise to enhance performance. $\textsf{HR-Bandit}$ offers three key guarantees: (i) a warm-start guarantee for improved initial performance, (ii) a human-effort guarantee to minimize required human interactions, and (iii) a robustness guarantee that ensures sublinear regret even when human decisions are suboptimal. Empirical results, including a healthcare case study, validate its superior performance against existing benchmarks.</li>
</ul>

<h3>Title: Distance between Relevant Information Pieces Causes Bias in Long-Context LLMs</h3>
<ul>
<li><strong>Authors: </strong>Runchu Tian, Yanghao Li, Yuepeng Fu, Siyang Deng, Qinyu Luo, Cheng Qian, Shuo Wang, Xin Cong, Zhong Zhang, Yesai Wu, Yankai Lin, Huadong Wang, Xiaojiang Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14641">https://arxiv.org/abs/2410.14641</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14641">https://arxiv.org/pdf/2410.14641</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14641]] Distance between Relevant Information Pieces Causes Bias in Long-Context LLMs(https://arxiv.org/abs/2410.14641)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Positional bias in large language models (LLMs) hinders their ability to effectively process long inputs. A prominent example is the "lost in the middle" phenomenon, where LLMs struggle to utilize relevant information situated in the middle of the input. While prior research primarily focuses on single pieces of relevant information, real-world applications often involve multiple relevant information pieces. To bridge this gap, we present LongPiBench, a benchmark designed to assess positional bias involving multiple pieces of relevant information. Thorough experiments are conducted with five commercial and six open-source models. These experiments reveal that while most current models are robust against the "lost in the middle" issue, there exist significant biases related to the spacing of relevant information pieces. These findings highlight the importance of evaluating and reducing positional biases to advance LLM's capabilities.</li>
</ul>

<h3>Title: EvoPress: Towards Optimal Dynamic Model Compression via Evolutionary Search</h3>
<ul>
<li><strong>Authors: </strong>Oliver Sieberling, Denis Kuznedelev, Eldar Kurtic, Dan Alistarh</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14649">https://arxiv.org/abs/2410.14649</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14649">https://arxiv.org/pdf/2410.14649</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14649]] EvoPress: Towards Optimal Dynamic Model Compression via Evolutionary Search(https://arxiv.org/abs/2410.14649)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The high computational costs of large language models (LLMs) have led to a flurry of research on LLM compression, via methods such as quantization, sparsification, or structured pruning. A new frontier in this area is given by \emph{dynamic, non-uniform} compression methods, which adjust the compression levels (e.g., sparsity) per-block or even per-layer in order to minimize accuracy loss, while guaranteeing a global compression threshold. Yet, current methods rely on heuristics for identifying the "importance" of a given layer towards the loss, based on assumptions such as \emph{error monotonicity}, i.e. that the end-to-end model compression error is proportional to the sum of layer-wise errors. In this paper, we revisit this area, and propose a new and general approach for dynamic compression that is provably optimal in a given input range. We begin from the motivating observation that, in general, \emph{error monotonicity does not hold for LLMs}: compressed models with lower sum of per-layer errors can perform \emph{worse} than models with higher error sums. To address this, we propose a new general evolutionary framework for dynamic LLM compression called EvoPress, which has provable convergence, and low sample and evaluation complexity. We show that these theoretical guarantees lead to highly competitive practical performance for dynamic compression of Llama, Mistral and Phi models. Via EvoPress, we set new state-of-the-art results across all compression approaches: structural pruning (block/layer dropping), unstructured sparsity, as well as quantization with dynamic bitwidths. Our code is available at this https URL.</li>
</ul>

<h3>Title: Real-time Fake News from Adversarial Feedback</h3>
<ul>
<li><strong>Authors: </strong>Sanxing Chen, Yukun Huang, Bhuwan Dhingra</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14651">https://arxiv.org/abs/2410.14651</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14651">https://arxiv.org/pdf/2410.14651</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14651]] Real-time Fake News from Adversarial Feedback(https://arxiv.org/abs/2410.14651)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>We show that existing evaluations for fake news detection based on conventional sources, such as claims on fact-checking websites, result in an increasing accuracy over time for LLM-based detectors -- even after their knowledge cutoffs. This suggests that recent popular political claims, which form the majority of fake news on such sources, are easily classified using surface-level shallow patterns. Instead, we argue that a proper fake news detection dataset should test a model's ability to reason factually about the current world by retrieving and reading related evidence. To this end, we develop a novel pipeline that leverages natural language feedback from a RAG-based detector to iteratively modify real-time news into deceptive fake news that challenges LLMs. Our iterative rewrite decreases the binary classification AUC by an absolute 17.5 percent for a strong RAG GPT-4o detector. Our experiments reveal the important role of RAG in both detecting and generating fake news, as retrieval-free LLM detectors are vulnerable to unseen events and adversarial attacks, while feedback from RAG detection helps discover more deceitful patterns in fake news.</li>
</ul>

<h3>Title: A Large Language Model-Driven Reward Design Framework via Dynamic Feedback for Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Shengjie Sun, Runze Liu, Jiafei Lyu, Jing-Wen Yang, Liangpeng Zhang, Xiu Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14660">https://arxiv.org/abs/2410.14660</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14660">https://arxiv.org/pdf/2410.14660</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14660]] A Large Language Model-Driven Reward Design Framework via Dynamic Feedback for Reinforcement Learning(https://arxiv.org/abs/2410.14660)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown significant potential in designing reward functions for Reinforcement Learning (RL) tasks. However, obtaining high-quality reward code often involves human intervention, numerous LLM queries, or repetitive RL training. To address these issues, we propose CARD, a LLM-driven Reward Design framework that iteratively generates and improves reward function code. Specifically, CARD includes a Coder that generates and verifies the code, while a Evaluator provides dynamic feedback to guide the Coder in improving the code, eliminating the need for human feedback. In addition to process feedback and trajectory feedback, we introduce Trajectory Preference Evaluation (TPE), which evaluates the current reward function based on trajectory preferences. If the code fails the TPE, the Evaluator provides preference feedback, avoiding RL training at every iteration and making the reward function better aligned with the task objective. Empirical results on Meta-World and ManiSkill2 demonstrate that our method achieves an effective balance between task performance and token efficiency, outperforming or matching the baselines across all tasks. On 10 out of 12 tasks, CARD shows better or comparable performance to policies trained with expert-designed rewards, and our method even surpasses the oracle on 3 tasks.</li>
</ul>

<h3>Title: DiscoGraMS: Enhancing Movie Screen-Play Summarization using Movie Character-Aware Discourse Graph</h3>
<ul>
<li><strong>Authors: </strong>Maitreya Prafulla Chitale, Uday Bindal, Rajakrishnan Rajkumar, Rahul Mishra</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14666">https://arxiv.org/abs/2410.14666</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14666">https://arxiv.org/pdf/2410.14666</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14666]] DiscoGraMS: Enhancing Movie Screen-Play Summarization using Movie Character-Aware Discourse Graph(https://arxiv.org/abs/2410.14666)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Summarizing movie screenplays presents a unique set of challenges compared to standard document summarization. Screenplays are not only lengthy, but also feature a complex interplay of characters, dialogues, and scenes, with numerous direct and subtle relationships and contextual nuances that are difficult for machine learning models to accurately capture and comprehend. Recent attempts at screenplay summarization focus on fine-tuning transformer-based pre-trained models, but these models often fall short in capturing long-term dependencies and latent relationships, and frequently encounter the "lost in the middle" issue. To address these challenges, we introduce DiscoGraMS, a novel resource that represents movie scripts as a movie character-aware discourse graph (CaD Graph). This approach is well-suited for various downstream tasks, such as summarization, question-answering, and salience detection. The model aims to preserve all salient information, offering a more comprehensive and faithful representation of the screenplay's content. We further explore a baseline method that combines the CaD Graph with the corresponding movie script through a late fusion of graph and text modalities, and we present very initial promising results.</li>
</ul>

<h3>Title: Stochastic Gradient Descent Jittering for Inverse Problems: Alleviating the Accuracy-Robustness Tradeoff</h3>
<ul>
<li><strong>Authors: </strong>Peimeng Guan, Mark A. Davenport</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14667">https://arxiv.org/abs/2410.14667</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14667">https://arxiv.org/pdf/2410.14667</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14667]] Stochastic Gradient Descent Jittering for Inverse Problems: Alleviating the Accuracy-Robustness Tradeoff(https://arxiv.org/abs/2410.14667)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust</a></li>
<li><strong>Abstract: </strong>Inverse problems aim to reconstruct unseen data from corrupted or perturbed measurements. While most work focuses on improving reconstruction quality, generalization accuracy and robustness are equally important, especially for safety-critical applications. Model-based architectures (MBAs), such as loop unrolling methods, are considered more interpretable and achieve better reconstructions. Empirical evidence suggests that MBAs are more robust to perturbations than black-box solvers, but the accuracy-robustness tradeoff in MBAs remains underexplored. In this work, we propose a simple yet effective training scheme for MBAs, called SGD jittering, which injects noise iteration-wise during reconstruction. We theoretically demonstrate that SGD jittering not only generalizes better than the standard mean squared error training but is also more robust to average-case attacks. We validate SGD jittering using denoising toy examples, seismic deconvolution, and single-coil MRI reconstruction. The proposed method achieves cleaner reconstructions for out-of-distribution data and demonstrates enhanced robustness to adversarial attacks.</li>
</ul>

<h3>Title: MiCEval: Unveiling Multimodal Chain of Thought's Quality via Image Description and Reasoning Steps</h3>
<ul>
<li><strong>Authors: </strong>Xiongtao Zhou, Jie He, Lanyu Chen, jingyu li, Haojing Chen, Victor Gutierrez Basulto, Jeff Z. Pan, Hanjie Chen</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14668">https://arxiv.org/abs/2410.14668</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14668">https://arxiv.org/pdf/2410.14668</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14668]] MiCEval: Unveiling Multimodal Chain of Thought's Quality via Image Description and Reasoning Steps(https://arxiv.org/abs/2410.14668)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Chain of Thought (MCoT) is a popular prompting strategy for improving the performance of multimodal large language models (MLLMs) across a range of complex reasoning tasks. Despite its popularity, there is a notable absence of automated methods for evaluating the quality of reasoning steps in MCoT. To address this gap, we propose Multimodal Chain-of-Thought Evaluation (MiCEval), a framework designed to assess the correctness of reasoning chains by evaluating the quality of both the description and each reasoning step. The evaluation of the description component focuses on the accuracy of the image descriptions, while the reasoning step evaluates the quality of each step as it is conditionally generated based on the preceding steps. MiCEval is built upon a fine-grained dataset with annotations that rate each step according to correctness, relevance, and informativeness. Extensive experiments on four state-of-the-art MLLMs show that step-wise evaluations using MiCEval align more closely with human judgments compared to existing methods based on cosine similarity or fine-tuning approaches. MiCEval datasets and code can be found in this https URL.</li>
</ul>

<h3>Title: BiGR: Harnessing Binary Latent Codes for Image Generation and Improved Visual Representation Capabilities</h3>
<ul>
<li><strong>Authors: </strong>Shaozhe Hao, Xuantong Liu, Xianbiao Qi, Shihao Zhao, Bojia Zi, Rong Xiao, Kai Han, Kwan-Yee K. Wong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14672">https://arxiv.org/abs/2410.14672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14672">https://arxiv.org/pdf/2410.14672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14672]] BiGR: Harnessing Binary Latent Codes for Image Generation and Improved Visual Representation Capabilities(https://arxiv.org/abs/2410.14672)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce BiGR, a novel conditional image generation model using compact binary latent codes for generative training, focusing on enhancing both generation and representation capabilities. BiGR is the first conditional generative model that unifies generation and discrimination within the same framework. BiGR features a binary tokenizer, a masked modeling mechanism, and a binary transcoder for binary code prediction. Additionally, we introduce a novel entropy-ordered sampling method to enable efficient image generation. Extensive experiments validate BiGR's superior performance in generation quality, as measured by FID-50k, and representation capabilities, as evidenced by linear-probe accuracy. Moreover, BiGR showcases zero-shot generalization across various vision tasks, enabling applications such as image inpainting, outpainting, editing, interpolation, and enrichment, without the need for structural modifications. Our findings suggest that BiGR unifies generative and discriminative tasks effectively, paving the way for further advancements in the field.</li>
</ul>

<h3>Title: Enhancing Large Language Models' Situated Faithfulness to External Contexts</h3>
<ul>
<li><strong>Authors: </strong>Yukun Huang, Sanxing Chen, Hongyi Cai, Bhuwan Dhingra</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14675">https://arxiv.org/abs/2410.14675</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14675">https://arxiv.org/pdf/2410.14675</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14675]] Enhancing Large Language Models' Situated Faithfulness to External Contexts(https://arxiv.org/abs/2410.14675)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are often augmented with external information as contexts, but this external information can sometimes be inaccurate or even intentionally misleading. We argue that robust LLMs should demonstrate situated faithfulness, dynamically calibrating their trust in external information based on their confidence in the internal knowledge and the external context. To benchmark this capability, we evaluate LLMs across several QA datasets, including a newly created dataset called RedditQA featuring in-the-wild incorrect contexts sourced from Reddit posts. We show that when provided with both correct and incorrect contexts, both open-source and proprietary models tend to overly rely on external information, regardless of its factual accuracy. To enhance situated faithfulness, we propose two approaches: Self-Guided Confidence Reasoning (SCR) and Rule-Based Confidence Reasoning (RCR). SCR enables models to self-access the confidence of external information relative to their own internal knowledge to produce the most accurate answer. RCR, in contrast, extracts explicit confidence signals from the LLM and determines the final answer using predefined rules. Our results show that for LLMs with strong reasoning capabilities, such as GPT-4o and GPT-4o mini, SCR outperforms RCR, achieving improvements of up to 24.2% over a direct input augmentation baseline. Conversely, for a smaller model like Llama-3-8B, RCR outperforms SCR. Fine-tuning SCR with our proposed Confidence Reasoning Direct Preference Optimization (CR-DPO) method improves performance on both seen and unseen datasets, yielding an average improvement of 8.9% on Llama-3-8B. In addition to quantitative results, we offer insights into the relative strengths of SCR and RCR. Our findings highlight promising avenues for improving situated faithfulness in LLMs. The data and code are released.</li>
</ul>

<h3>Title: SudoLM: Learning Access Control of Parametric Knowledge with Authorization Alignment</h3>
<ul>
<li><strong>Authors: </strong>Qin Liu, Fei Wang, Chaowei Xiao, Muhao Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14676">https://arxiv.org/abs/2410.14676</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14676">https://arxiv.org/pdf/2410.14676</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14676]] SudoLM: Learning Access Control of Parametric Knowledge with Authorization Alignment(https://arxiv.org/abs/2410.14676)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Existing preference alignment is a one-size-fits-all alignment mechanism, where the part of the large language model (LLM) parametric knowledge with non-preferred features is uniformly blocked to all the users. However, this part of knowledge can be useful to advanced users whose expertise qualifies them to handle these information. The one-size-fits-all alignment mechanism undermines LLM's utility for these qualified users. To address this problem, we propose SudoLM, a framework that lets LLMs learn access control over specific parametric knowledge for users with different credentials via authorization alignment. SudoLM allows authorized users to unlock their access to all the parametric knowledge with an assigned SUDO key while blocking access to non-qualified users. Experiments on two application scenarios demonstrate that SudoLM effectively controls the user's access to the parametric knowledge and maintains its general utility.</li>
</ul>

<h3>Title: Are AI Detectors Good Enough? A Survey on Quality of Datasets With Machine-Generated Texts</h3>
<ul>
<li><strong>Authors: </strong>German Gritsai, Anastasia Voznyuk, Andrey Grabovoy, Yury Chekhovich</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.14677">https://arxiv.org/abs/2410.14677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.14677">https://arxiv.org/pdf/2410.14677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.14677]] Are AI Detectors Good Enough? A Survey on Quality of Datasets With Machine-Generated Texts(https://arxiv.org/abs/2410.14677)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, robust, large language model</a></li>
<li><strong>Abstract: </strong>The rapid development of autoregressive Large Language Models (LLMs) has significantly improved the quality of generated texts, necessitating reliable machine-generated text detectors. A huge number of detectors and collections with AI fragments have emerged, and several detection methods even showed recognition quality up to 99.9% according to the target metrics in such collections. However, the quality of such detectors tends to drop dramatically in the wild, posing a question: Are detectors actually highly trustworthy or do their high benchmark scores come from the poor quality of evaluation datasets? In this paper, we emphasise the need for robust and qualitative methods for evaluating generated data to be secure against bias and low generalising ability of future model. We present a systematic review of datasets from competitions dedicated to AI-generated content detection and propose methods for evaluating the quality of datasets containing AI-generated fragments. In addition, we discuss the possibility of using high-quality generated data to achieve two goals: improving the training of detection models and improving the training datasets themselves. Our contribution aims to facilitate a better understanding of the dynamics between human and machine text, which will ultimately support the integrity of information in an increasingly automated world.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
