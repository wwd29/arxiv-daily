<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Secure short-term load forecasting for smart grids with transformer-based federated learning. (arXiv:2310.17477v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17477">http://arxiv.org/abs/2310.17477</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17477]] Secure short-term load forecasting for smart grids with transformer-based federated learning(http://arxiv.org/abs/2310.17477)</code></li>
<li>Summary: <p>Electricity load forecasting is an essential task within smart grids to
assist demand and supply balance. While advanced deep learning models require
large amounts of high-resolution data for accurate short-term load predictions,
fine-grained load profiles can expose users' electricity consumption behaviors,
which raises privacy and security concerns. One solution to improve data
privacy is federated learning, where models are trained locally on private
data, and only the trained model parameters are merged and updated on a global
server. Therefore, this paper presents a novel transformer-based deep learning
approach with federated learning for short-term electricity load prediction. To
evaluate our results, we benchmark our federated learning architecture against
central and local learning and compare the performance of our model to long
short-term memory models and convolutional neural networks. Our simulations are
based on a dataset from a German university campus and show that
transformer-based forecasting is a promising alternative to state-of-the-art
models within federated learning.
</p></li>
</ul>

<h2>security</h2>
<h2>privacy</h2>
<h3>Title: Privately Aligning Language Models with Reinforcement Learning. (arXiv:2310.16960v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16960">http://arxiv.org/abs/2310.16960</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16960]] Privately Aligning Language Models with Reinforcement Learning(http://arxiv.org/abs/2310.16960)</code></li>
<li>Summary: <p>Positioned between pre-training and user deployment, aligning large language
models (LLMs) through reinforcement learning (RL) has emerged as a prevailing
strategy for training instruction following-models such as ChatGPT. In this
work, we initiate the study of privacy-preserving alignment of LLMs through
Differential Privacy (DP) in conjunction with RL. Following the influential
work of Ziegler et al. (2020), we study two dominant paradigms: (i) alignment
via RL without human in the loop (e.g., positive review generation) and (ii)
alignment via RL from human feedback (RLHF) (e.g., summarization in a
human-preferred way). We give a new DP framework to achieve alignment via RL,
and prove its correctness. Our experimental results validate the effectiveness
of our approach, offering competitive utility while ensuring strong privacy
protections.
</p></li>
</ul>

<h3>Title: Redactable and Sanitizable Signature Schemes: Applications and Limitations for use in Decentralized Digital Identity Systems. (arXiv:2310.17297v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17297">http://arxiv.org/abs/2310.17297</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17297]] Redactable and Sanitizable Signature Schemes: Applications and Limitations for use in Decentralized Digital Identity Systems(http://arxiv.org/abs/2310.17297)</code></li>
<li>Summary: <p>Redactable signature schemes and sanitizable signature schemes are methods
that permit modification of a given digital message and retain a valid
signature. This can be applied to decentralized identity systems for delegating
identity issuance and redacting sensitive information for privacy-preserving
verification of identity. We propose implementing these protocols on a digital
credential and compare them against other privacy-enhancing techniques to
assess their suitability
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h3>Title: Detection Defenses: An Empty Promise against Adversarial Patch Attacks on Optical Flow. (arXiv:2310.17403v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17403">http://arxiv.org/abs/2310.17403</a></li>
<li>Code URL: https://github.com/cv-stuttgart/detectiondefenses</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17403]] Detection Defenses: An Empty Promise against Adversarial Patch Attacks on Optical Flow(http://arxiv.org/abs/2310.17403)</code></li>
<li>Summary: <p>Adversarial patches undermine the reliability of optical flow predictions
when placed in arbitrary scene locations. Therefore, they pose a realistic
threat to real-world motion detection and its downstream applications.
Potential remedies are defense strategies that detect and remove adversarial
patches, but their influence on the underlying motion prediction has not been
investigated. In this paper, we thoroughly examine the currently available
detect-and-remove defenses ILP and LGS for a wide selection of state-of-the-art
optical flow methods, and illuminate their side effects on the quality and
robustness of the final flow predictions. In particular, we implement
defense-aware attacks to investigate whether current defenses are able to
withstand attacks that take the defense mechanism into account. Our experiments
yield two surprising results: Detect-and-remove defenses do not only lower the
optical flow quality on benign scenes, in doing so, they also harm the
robustness under patch attacks for all tested optical flow methods except
FlowNetC. As currently employed detect-and-remove defenses fail to deliver the
promised adversarial robustness for optical flow, they evoke a false sense of
security. The code is available at
https://github.com/cv-stuttgart/DetectionDefenses.
</p></li>
</ul>

<h2>attack</h2>
<h3>Title: AntifakePrompt: Prompt-Tuned Vision-Language Models are Fake Image Detectors. (arXiv:2310.17419v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17419">http://arxiv.org/abs/2310.17419</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17419]] AntifakePrompt: Prompt-Tuned Vision-Language Models are Fake Image Detectors(http://arxiv.org/abs/2310.17419)</code></li>
<li>Summary: <p>Deep generative models can create remarkably photorealistic fake images while
raising concerns about misinformation and copyright infringement, known as
deepfake threats. Deepfake detection technique is developed to distinguish
between real and fake images, where the existing methods typically learn
classifiers in the image domain or various feature domains. However, the
generalizability of deepfake detection against emerging and more advanced
generative models remains challenging. In this paper, being inspired by the
zero-shot advantages of Vision-Language Models (VLMs), we propose a novel
approach using VLMs (e.g. InstructBLIP) and prompt tuning techniques to improve
the deepfake detection accuracy over unseen data. We formulate deepfake
detection as a visual question answering problem, and tune soft prompts for
InstructBLIP to answer the real/fake information of a query image. We conduct
full-spectrum experiments on datasets from 3 held-in and 13 held-out generative
models, covering modern text-to-image generation, image editing and image
attacks. Results demonstrate that (1) the deepfake detection accuracy can be
significantly and consistently improved (from 58.8% to 91.31%, in average
accuracy over unseen data) using pretrained vision-language models with prompt
tuning; (2) our superior performance is at less cost of trainable parameters,
resulting in an effective and efficient solution for deepfake detection. Code
and models can be found at https://github.com/nctu-eva-lab/AntifakePrompt.
</p></li>
</ul>

<h3>Title: Uncertainty-weighted Loss Functions for Improved Adversarial Attacks on Semantic Segmentation. (arXiv:2310.17436v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17436">http://arxiv.org/abs/2310.17436</a></li>
<li>Code URL: https://github.com/kmaag/uncertainty-weighted-loss</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17436]] Uncertainty-weighted Loss Functions for Improved Adversarial Attacks on Semantic Segmentation(http://arxiv.org/abs/2310.17436)</code></li>
<li>Summary: <p>State-of-the-art deep neural networks have been shown to be extremely
powerful in a variety of perceptual tasks like semantic segmentation. However,
these networks are vulnerable to adversarial perturbations of the input which
are imperceptible for humans but lead to incorrect predictions. Treating image
segmentation as a sum of pixel-wise classifications, adversarial attacks
developed for classification models were shown to be applicable to segmentation
models as well. In this work, we present simple uncertainty-based weighting
schemes for the loss functions of such attacks that (i) put higher weights on
pixel classifications which can more easily perturbed and (ii) zero-out the
pixel-wise losses corresponding to those pixels that are already confidently
misclassified. The weighting schemes can be easily integrated into the loss
function of a range of well-known adversarial attackers with minimal additional
computational overhead, but lead to significant improved perturbation
performance, as we demonstrate in our empirical analysis on several datasets
and models.
</p></li>
</ul>

<h3>Title: SoK: Pitfalls in Evaluating Black-Box Attacks. (arXiv:2310.17534v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17534">http://arxiv.org/abs/2310.17534</a></li>
<li>Code URL: https://github.com/iamgroot42/blackboxsok</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17534]] SoK: Pitfalls in Evaluating Black-Box Attacks(http://arxiv.org/abs/2310.17534)</code></li>
<li>Summary: <p>Numerous works study black-box attacks on image classifiers. However, these
works make different assumptions on the adversary's knowledge and current
literature lacks a cohesive organization centered around the threat model. To
systematize knowledge in this area, we propose a taxonomy over the threat space
spanning the axes of feedback granularity, the access of interactive queries,
and the quality and quantity of the auxiliary data available to the attacker.
Our new taxonomy provides three key insights. 1) Despite extensive literature,
numerous under-explored threat spaces exist, which cannot be trivially solved
by adapting techniques from well-explored settings. We demonstrate this by
establishing a new state-of-the-art in the less-studied setting of access to
top-k confidence scores by adapting techniques from well-explored settings of
accessing the complete confidence vector, but show how it still falls short of
the more restrictive setting that only obtains the prediction label,
highlighting the need for more research. 2) Identification the threat model of
different attacks uncovers stronger baselines that challenge prior
state-of-the-art claims. We demonstrate this by enhancing an initially weaker
baseline (under interactive query access) via surrogate models, effectively
overturning claims in the respective paper. 3) Our taxonomy reveals
interactions between attacker knowledge that connect well to related areas,
such as model inversion and extraction attacks. We discuss how advances in
other areas can enable potentially stronger black-box attacks. Finally, we
emphasize the need for a more realistic assessment of attack success by
factoring in local attack runtime. This approach reveals the potential for
certain attacks to achieve notably higher success rates and the need to
evaluate attacks in diverse and harder settings, highlighting the need for
better selection criteria.
</p></li>
</ul>

<h3>Title: Instability of computer vision models is a necessary result of the task itself. (arXiv:2310.17559v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17559">http://arxiv.org/abs/2310.17559</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17559]] Instability of computer vision models is a necessary result of the task itself(http://arxiv.org/abs/2310.17559)</code></li>
<li>Summary: <p>Adversarial examples resulting from instability of current computer vision
models are an extremely important topic due to their potential to compromise
any application. In this paper we demonstrate that instability is inevitable
due to a) symmetries (translational invariance) of the data, b) the categorical
nature of the classification task, and c) the fundamental discrepancy of
classifying images as objects themselves. The issue is further exacerbated by
non-exhaustive labelling of the training data. Therefore we conclude that
instability is a necessary result of how the problem of computer vision is
currently formulated. While the problem cannot be eliminated, through the
analysis of the causes, we have arrived at ways how it can be partially
alleviated. These include i) increasing the resolution of images, ii) providing
contextual information for the image, iii) exhaustive labelling of training
data, and iv) preventing attackers from frequent access to the computer vision
system.
</p></li>
</ul>

<h3>Title: ''Fifty Shades of Bias'': Normative Ratings of Gender Bias in GPT Generated English Text. (arXiv:2310.17428v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17428">http://arxiv.org/abs/2310.17428</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17428]] ''Fifty Shades of Bias'': Normative Ratings of Gender Bias in GPT Generated English Text(http://arxiv.org/abs/2310.17428)</code></li>
<li>Summary: <p>Language serves as a powerful tool for the manifestation of societal belief
systems. In doing so, it also perpetuates the prevalent biases in our society.
Gender bias is one of the most pervasive biases in our society and is seen in
online and offline discourses. With LLMs increasingly gaining human-like
fluency in text generation, gaining a nuanced understanding of the biases these
systems can generate is imperative. Prior work often treats gender bias as a
binary classification task. However, acknowledging that bias must be perceived
at a relative scale; we investigate the generation and consequent receptivity
of manual annotators to bias of varying degrees. Specifically, we create the
first dataset of GPT-generated English text with normative ratings of gender
bias. Ratings were obtained using Best--Worst Scaling -- an efficient
comparative annotation framework. Next, we systematically analyze the variation
of themes of gender biases in the observed ranking and show that
identity-attack is most closely related to gender bias. Finally, we show the
performance of existing automated models trained on related concepts on our
dataset.
</p></li>
</ul>

<h3>Title: Static Semantics Reconstruction for Enhancing JavaScript-WebAssembly Multilingual Malware Detection. (arXiv:2310.17304v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17304">http://arxiv.org/abs/2310.17304</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17304]] Static Semantics Reconstruction for Enhancing JavaScript-WebAssembly Multilingual Malware Detection(http://arxiv.org/abs/2310.17304)</code></li>
<li>Summary: <p>The emergence of WebAssembly allows attackers to hide the malicious
functionalities of JavaScript malware in cross-language interoperations, termed
JavaScript-WebAssembly multilingual malware (JWMM). However, existing
anti-virus solutions based on static program analysis are still limited to
monolingual code. As a result, their detection effectiveness decreases
significantly against JWMM. The detection of JWMM is challenging due to the
complex interoperations and semantic diversity between JavaScript and
WebAssembly. To bridge this gap, we present JWBinder, the first technique aimed
at enhancing the static detection of JWMM. JWBinder performs a
language-specific data-flow analysis to capture the cross-language
interoperations and then characterizes the functionalities of JWMM through a
unified high-level structure called Inter-language Program Dependency Graph.
The extensive evaluation on one of the most representative real-world
anti-virus platforms, VirusTotal, shows that \system effectively enhances
anti-virus systems from various vendors and increases the overall successful
detection rate against JWMM from 49.1\% to 86.2\%. Additionally, we assess the
side effects and runtime overhead of JWBinder, corroborating its practical
viability in real-world applications.
</p></li>
</ul>

<h3>Title: A near-autonomous and incremental intrusion detection system through active learning of known and unknown attacks. (arXiv:2310.17430v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17430">http://arxiv.org/abs/2310.17430</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17430]] A near-autonomous and incremental intrusion detection system through active learning of known and unknown attacks(http://arxiv.org/abs/2310.17430)</code></li>
<li>Summary: <p>Intrusion detection is a traditional practice of security experts, however,
there are several issues which still need to be tackled. Therefore, in this
paper, after highlighting these issues, we present an architecture for a hybrid
Intrusion Detection System (IDS) for an adaptive and incremental detection of
both known and unknown attacks. The IDS is composed of supervised and
unsupervised modules, namely, a Deep Neural Network (DNN) and the K-Nearest
Neighbors (KNN) algorithm, respectively. The proposed system is near-autonomous
since the intervention of the expert is minimized through the active learning
(AL) approach. A query strategy for the labeling process is presented, it aims
at teaching the supervised module to detect unknown attacks and improve the
detection of the already-known attacks. This teaching is achieved through
sliding windows (SW) in an incremental fashion where the DNN is retrained when
the data is available over time, thus rendering the IDS adaptive to cope with
the evolutionary aspect of the network traffic. A set of experiments was
conducted on the CICIDS2017 dataset in order to evaluate the performance of the
IDS, promising results were obtained.
</p></li>
</ul>

<h3>Title: CBD: A Certified Backdoor Detector Based on Local Dominant Probability. (arXiv:2310.17498v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17498">http://arxiv.org/abs/2310.17498</a></li>
<li>Code URL: https://github.com/zhenxianglance/cbd</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17498]] CBD: A Certified Backdoor Detector Based on Local Dominant Probability(http://arxiv.org/abs/2310.17498)</code></li>
<li>Summary: <p>Backdoor attack is a common threat to deep neural networks. During testing,
samples embedded with a backdoor trigger will be misclassified as an
adversarial target by a backdoored model, while samples without the backdoor
trigger will be correctly classified. In this paper, we present the first
certified backdoor detector (CBD), which is based on a novel, adjustable
conformal prediction scheme based on our proposed statistic local dominant
probability. For any classifier under inspection, CBD provides 1) a detection
inference, 2) the condition under which the attacks are guaranteed to be
detectable for the same classification domain, and 3) a probabilistic upper
bound for the false positive rate. Our theoretical results show that attacks
with triggers that are more resilient to test-time noise and have smaller
perturbation magnitudes are more likely to be detected with guarantees.
Moreover, we conduct extensive experiments on four benchmark datasets
considering various backdoor types, such as BadNet, CB, and Blend. CBD achieves
comparable or even higher detection accuracy than state-of-the-art detectors,
and it in addition provides detection certification. Notably, for backdoor
attacks with random perturbation triggers bounded by $\ell_2\leq0.75$ which
achieves more than 90\% attack success rate, CBD achieves 100\% (98\%), 100\%
(84\%), 98\% (98\%), and 72\% (40\%) empirical (certified) detection true
positive rates on the four benchmark datasets GTSRB, SVHN, CIFAR-10, and
TinyImageNet, respectively, with low false positive rates.
</p></li>
</ul>

<h3>Title: Break it, Imitate it, Fix it: Robustness by Generating Human-Like Attacks. (arXiv:2310.16955v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16955">http://arxiv.org/abs/2310.16955</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16955]] Break it, Imitate it, Fix it: Robustness by Generating Human-Like Attacks(http://arxiv.org/abs/2310.16955)</code></li>
<li>Summary: <p>Real-world natural language processing systems need to be robust to human
adversaries. Collecting examples of human adversaries for training is an
effective but expensive solution. On the other hand, training on synthetic
attacks with small perturbations - such as word-substitution - does not
actually improve robustness to human adversaries. In this paper, we propose an
adversarial training framework that uses limited human adversarial examples to
generate more useful adversarial examples at scale. We demonstrate the
advantages of this system on the ANLI and hate speech detection benchmark
datasets - both collected via an iterative, adversarial
human-and-model-in-the-loop procedure. Compared to training only on observed
human attacks, also training on our synthetic adversarial examples improves
model robustness to future rounds. In ANLI, we see accuracy gains on the
current set of attacks (44.1%$\,\to\,$50.1%) and on two future unseen rounds of
human generated attacks (32.5%$\,\to\,$43.4%, and 29.4%$\,\to\,$40.2%). In hate
speech detection, we see AUC gains on current attacks (0.76 $\to$ 0.84) and a
future round (0.77 $\to$ 0.79). Attacks from methods that do not learn the
distribution of existing human adversaries, meanwhile, degrade robustness.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Wide Flat Minimum Watermarking for Robust Ownership Verification of GANs. (arXiv:2310.16919v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16919">http://arxiv.org/abs/2310.16919</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16919]] Wide Flat Minimum Watermarking for Robust Ownership Verification of GANs(http://arxiv.org/abs/2310.16919)</code></li>
<li>Summary: <p>We propose a novel multi-bit box-free watermarking method for the protection
of Intellectual Property Rights (IPR) of GANs with improved robustness against
white-box attacks like fine-tuning, pruning, quantization, and surrogate model
attacks. The watermark is embedded by adding an extra watermarking loss term
during GAN training, ensuring that the images generated by the GAN contain an
invisible watermark that can be retrieved by a pre-trained watermark decoder.
In order to improve the robustness against white-box model-level attacks, we
make sure that the model converges to a wide flat minimum of the watermarking
loss term, in such a way that any modification of the model parameters does not
erase the watermark. To do so, we add random noise vectors to the parameters of
the generator and require that the watermarking loss term is as invariant as
possible with respect to the presence of noise. This procedure forces the
generator to converge to a wide flat minimum of the watermarking loss. The
proposed method is architectureand dataset-agnostic, thus being applicable to
many different generation tasks and models, as well as to CNN-based image
processing architectures. We present the results of extensive experiments
showing that the presence of the watermark has a negligible impact on the
quality of the generated images, and proving the superior robustness of the
watermark against model modification and surrogate model attacks.
</p></li>
</ul>

<h3>Title: Diagnosing Alzheimer's Disease using Early-Late Multimodal Data Fusion with Jacobian Maps. (arXiv:2310.16936v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16936">http://arxiv.org/abs/2310.16936</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16936]] Diagnosing Alzheimer's Disease using Early-Late Multimodal Data Fusion with Jacobian Maps(http://arxiv.org/abs/2310.16936)</code></li>
<li>Summary: <p>Alzheimer's disease (AD) is a prevalent and debilitating neurodegenerative
disorder impacting a large aging population. Detecting AD in all its
presymptomatic and symptomatic stages is crucial for early intervention and
treatment. An active research direction is to explore machine learning methods
that harness multimodal data fusion to outperform human inspection of medical
scans. However, existing multimodal fusion models have limitations, including
redundant computation, complex architecture, and simplistic handling of missing
data. Moreover, the preprocessing pipelines of medical scans remain
inadequately detailed and are seldom optimized for individual subjects. In this
paper, we propose an efficient early-late fusion (ELF) approach, which
leverages a convolutional neural network for automated feature extraction and
random forests for their competitive performance on small datasets.
Additionally, we introduce a robust preprocessing pipeline that adapts to the
unique characteristics of individual subjects and makes use of whole brain
images rather than slices or patches. Moreover, to tackle the challenge of
detecting subtle changes in brain volume, we transform images into the Jacobian
domain (JD) to enhance both accuracy and robustness in our classification.
Using MRI and CT images from the OASIS-3 dataset, our experiments demonstrate
the effectiveness of the ELF approach in classifying AD into four stages with
an accuracy of 97.19%.
</p></li>
</ul>

<h3>Title: An Efficient Deep Learning-based approach for Recognizing Agricultural Pests in the Wild. (arXiv:2310.16991v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16991">http://arxiv.org/abs/2310.16991</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16991]] An Efficient Deep Learning-based approach for Recognizing Agricultural Pests in the Wild(http://arxiv.org/abs/2310.16991)</code></li>
<li>Summary: <p>One of the biggest challenges that the farmers go through is to fight insect
pests during agricultural product yields. The problem can be solved easily and
avoid economic losses by taking timely preventive measures. This requires
identifying insect pests in an easy and effective manner. Most of the insect
species have similarities between them. Without proper help from the
agriculturist academician it is very challenging for the farmers to identify
the crop pests accurately. To address this issue we have done extensive
experiments considering different methods to find out the best method among
all. This paper presents a detailed overview of the experiments done on mainly
a robust dataset named IP102 including transfer learning with finetuning,
attention mechanism and custom architecture. Some example from another dataset
D0 is also shown to show robustness of our experimented techniques.
</p></li>
</ul>

<h3>Title: Trust, but Verify: Robust Image Segmentation using Deep Learning. (arXiv:2310.16999v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16999">http://arxiv.org/abs/2310.16999</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16999]] Trust, but Verify: Robust Image Segmentation using Deep Learning(http://arxiv.org/abs/2310.16999)</code></li>
<li>Summary: <p>We describe a method for verifying the output of a deep neural network for
medical image segmentation that is robust to several classes of random as well
as worst-case perturbations i.e. adversarial attacks. This method is based on a
general approach recently developed by the authors called ``Trust, but Verify"
wherein an auxiliary verification network produces predictions about certain
masked features in the input image using the segmentation as an input. A
well-designed auxiliary network will produce high-quality predictions when the
input segmentations are accurate, but will produce low-quality predictions when
the segmentations are incorrect. Checking the predictions of such a network
with the original image allows us to detect bad segmentations. However, to
ensure the verification method is truly robust, we need a method for checking
the quality of the predictions that does not itself rely on a black-box neural
network. Indeed, we show that previous methods for segmentation evaluation that
do use deep neural regression networks are vulnerable to false negatives i.e.
can inaccurately label bad segmentations as good. We describe the design of a
verification network that avoids such vulnerability and present results to
demonstrate its robustness compared to previous methods.
</p></li>
</ul>

<h3>Title: StochGradAdam: Accelerating Neural Networks Training with Stochastic Gradient Sampling. (arXiv:2310.17042v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17042">http://arxiv.org/abs/2310.17042</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17042]] StochGradAdam: Accelerating Neural Networks Training with Stochastic Gradient Sampling(http://arxiv.org/abs/2310.17042)</code></li>
<li>Summary: <p>In the rapidly advancing domain of deep learning optimization, this paper
unveils the StochGradAdam optimizer, a novel adaptation of the well-regarded
Adam algorithm. Central to StochGradAdam is its gradient sampling technique.
This method not only ensures stable convergence but also leverages the
advantages of selective gradient consideration, fostering robust training by
potentially mitigating the effects of noisy or outlier data and enhancing the
exploration of the loss landscape for more dependable convergence. In both
image classification and segmentation tasks, StochGradAdam has demonstrated
superior performance compared to the traditional Adam optimizer. By judiciously
sampling a subset of gradients at each iteration, the optimizer is optimized
for managing intricate models. The paper provides a comprehensive exploration
of StochGradAdam's methodology, from its mathematical foundations to bias
correction strategies, heralding a promising advancement in deep learning
training techniques.
</p></li>
</ul>

<h3>Title: Bridging The Gaps Between Token Pruning and Full Pre-training via Masked Fine-tuning. (arXiv:2310.17177v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17177">http://arxiv.org/abs/2310.17177</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17177]] Bridging The Gaps Between Token Pruning and Full Pre-training via Masked Fine-tuning(http://arxiv.org/abs/2310.17177)</code></li>
<li>Summary: <p>Despite the success of transformers on various computer vision tasks, they
suffer from excessive memory and computational cost. Some works present dynamic
vision transformers to accelerate inference by pruning redundant tokens. A key
to improving token pruning is using well-trained models as initialization for
faster convergence and better performance. However, current base models usually
adopt full image training, i.e., using full images as inputs and keeping the
whole feature maps through the forward process, which causes inconsistencies
with dynamic models that gradually reduce tokens, including calculation
pattern, information amount and token selection strategy inconsistencies.
Inspired by MAE which performs masking and reconstruction self-supervised task,
we devise masked fine-tuning to bridge the gaps between pre-trained base models
used for initialization and token pruning based dynamic vision transformers, by
masking image patches and predicting the image class label based on left
unmasked patches. Extensive experiments on ImageNet demonstrate that base
models via masked fine-tuning gain strong occlusion robustness and ability
against information loss. With this better initialization, Dynamic ViT achieves
higher accuracies, especially under large token pruning ratios (e.g., 81.9% vs.
81.3%, and 62.3% vs. 58.9% for DeiT based Dynamic ViT/0.8 and Dynamic ViT/0.3).
Moreover, we apply our method into different token pruning based dynamic vision
transformers, different pre-trained models and randomly initialized models to
demonstrate the generalization ability.
</p></li>
</ul>

<h3>Title: Blind Image Super-resolution with Rich Texture-Aware Codebooks. (arXiv:2310.17188v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17188">http://arxiv.org/abs/2310.17188</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17188]] Blind Image Super-resolution with Rich Texture-Aware Codebooks(http://arxiv.org/abs/2310.17188)</code></li>
<li>Summary: <p>Blind super-resolution (BSR) methods based on high-resolution (HR)
reconstruction codebooks have achieved promising results in recent years.
However, we find that a codebook based on HR reconstruction may not effectively
capture the complex correlations between low-resolution (LR) and HR images. In
detail, multiple HR images may produce similar LR versions due to complex blind
degradations, causing the HR-dependent only codebooks having limited texture
diversity when faced with confusing LR inputs. To alleviate this problem, we
propose the Rich Texture-aware Codebook-based Network (RTCNet), which consists
of the Degradation-robust Texture Prior Module (DTPM) and the Patch-aware
Texture Prior Module (PTPM). DTPM effectively mines the cross-resolution
correlation of textures between LR and HR images by exploiting the
cross-resolution correspondence of textures. PTPM uses patch-wise semantic
pre-training to correct the misperception of texture similarity in the
high-level semantic regularization. By taking advantage of this, RTCNet
effectively gets rid of the misalignment of confusing textures between HR and
LR in the BSR scenarios. Experiments show that RTCNet outperforms
state-of-the-art methods on various benchmarks by up to 0.16 ~ 0.46dB.
</p></li>
</ul>

<h3>Title: Generalizing to Unseen Domains in Diabetic Retinopathy Classification. (arXiv:2310.17255v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17255">http://arxiv.org/abs/2310.17255</a></li>
<li>Code URL: https://github.com/chumsy0725/spsd-vit</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17255]] Generalizing to Unseen Domains in Diabetic Retinopathy Classification(http://arxiv.org/abs/2310.17255)</code></li>
<li>Summary: <p>Diabetic retinopathy (DR). is caused by long-standing diabetes and is among
the fifth leading cause for visual impairments. The process of early diagnosis
and treatments could be helpful in curing the disease, however, the detection
procedure is rather challenging and mostly tedious. Therefore, automated
diabetic retinopathy classification using deep learning techniques has gained
interest in the medical imaging community. Akin to several other real-world
applications of deep learning, the typical assumption of i.i.d data is also
violated in DR classification that relies on deep learning. Therefore,
developing DR classification methods robust to unseen distributions is of great
value. In this paper, we study the problem of generalizing a model to unseen
distributions or domains (a.k.a domain generalization) in DR classification. To
this end, we propose a simple and effective domain generalization (DG) approach
that achieves self-distillation in vision transformers (ViT) via a novel
prediction softening mechanism. This prediction softening is an adaptive convex
combination one-hot labels with the model's own knowledge. We perform extensive
experiments on challenging open-source DR classification datasets under both
multi-source and single-source DG settings with three different ViT backbones
to establish the efficacy and applicability of our approach against competing
methods. For the first time, we report the performance of several
state-of-the-art DG methods on open-source DR classification datasets after
conducting thorough experiments. Finally, our method is also capable of
delivering improved calibration performance than other methods, showing its
suitability for safety-critical applications, including healthcare. We hope
that our contributions would investigate more DG research across the medical
imaging community.
</p></li>
</ul>

<h3>Title: IndustReal: A Dataset for Procedure Step Recognition Handling Execution Errors in Egocentric Videos in an Industrial-Like Setting. (arXiv:2310.17323v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17323">http://arxiv.org/abs/2310.17323</a></li>
<li>Code URL: https://github.com/timschoonbeek/industreal</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17323]] IndustReal: A Dataset for Procedure Step Recognition Handling Execution Errors in Egocentric Videos in an Industrial-Like Setting(http://arxiv.org/abs/2310.17323)</code></li>
<li>Summary: <p>Although action recognition for procedural tasks has received notable
attention, it has a fundamental flaw in that no measure of success for actions
is provided. This limits the applicability of such systems especially within
the industrial domain, since the outcome of procedural actions is often
significantly more important than the mere execution. To address this
limitation, we define the novel task of procedure step recognition (PSR),
focusing on recognizing the correct completion and order of procedural steps.
Alongside the new task, we also present the multi-modal IndustReal dataset.
Unlike currently available datasets, IndustReal contains procedural errors
(such as omissions) as well as execution errors. A significant part of these
errors are exclusively present in the validation and test sets, making
IndustReal suitable to evaluate robustness of algorithms to new, unseen
mistakes. Additionally, to encourage reproducibility and allow for scalable
approaches trained on synthetic data, the 3D models of all parts are publicly
available. Annotations and benchmark performance are provided for action
recognition and assembly state detection, as well as the new PSR task.
IndustReal, along with the code and model weights, is available at:
https://github.com/TimSchoonbeek/IndustReal .
</p></li>
</ul>

<h3>Title: SE(3) Diffusion Model-based Point Cloud Registration for Robust 6D Object Pose Estimation. (arXiv:2310.17359v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17359">http://arxiv.org/abs/2310.17359</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17359]] SE(3) Diffusion Model-based Point Cloud Registration for Robust 6D Object Pose Estimation(http://arxiv.org/abs/2310.17359)</code></li>
<li>Summary: <p>In this paper, we introduce an SE(3) diffusion model-based point cloud
registration framework for 6D object pose estimation in real-world scenarios.
Our approach formulates the 3D registration task as a denoising diffusion
process, which progressively refines the pose of the source point cloud to
obtain a precise alignment with the model point cloud. Training our framework
involves two operations: An SE(3) diffusion process and an SE(3) reverse
process. The SE(3) diffusion process gradually perturbs the optimal rigid
transformation of a pair of point clouds by continuously injecting noise
(perturbation transformation). By contrast, the SE(3) reverse process focuses
on learning a denoising network that refines the noisy transformation
step-by-step, bringing it closer to the optimal transformation for accurate
pose estimation. Unlike standard diffusion models used in linear Euclidean
spaces, our diffusion model operates on the SE(3) manifold. This requires
exploiting the linear Lie algebra $\mathfrak{se}(3)$ associated with SE(3) to
constrain the transformation transitions during the diffusion and reverse
processes. Additionally, to effectively train our denoising network, we derive
a registration-specific variational lower bound as the optimization objective
for model learning. Furthermore, we show that our denoising network can be
constructed with a surrogate registration model, making our approach applicable
to different deep registration networks. Extensive experiments demonstrate that
our diffusion registration framework presents outstanding pose estimation
performance on the real-world TUD-L, LINEMOD, and Occluded-LINEMOD datasets.
</p></li>
</ul>

<h3>Title: Handshape recognition for Argentinian Sign Language using ProbSom. (arXiv:2310.17427v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17427">http://arxiv.org/abs/2310.17427</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17427]] Handshape recognition for Argentinian Sign Language using ProbSom(http://arxiv.org/abs/2310.17427)</code></li>
<li>Summary: <p>Automatic sign language recognition is an important topic within the areas of
human-computer interaction and machine learning. On the one hand, it poses a
complex challenge that requires the intervention of various knowledge areas,
such as video processing, image processing, intelligent systems and
linguistics. On the other hand, robust recognition of sign language could
assist in the translation process and the integration of hearing-impaired
people.
</p>
<p>This paper offers two main contributions: first, the creation of a database
of handshapes for the Argentinian Sign Language (LSA), which is a topic that
has barely been discussed so far. Secondly, a technique for image processing,
descriptor extraction and subsequent handshape classification using a
supervised adaptation of self-organizing maps that is called ProbSom. This
technique is compared to others in the state of the art, such as Support Vector
Machines (SVM), Random Forests, and Neural Networks.
</p>
<p>The database that was built contains 800 images with 16 LSA handshapes, and
is a first step towards building a comprehensive database of Argentinian signs.
The ProbSom-based neural classifier, using the proposed descriptor, achieved an
accuracy rate above 90%.
</p></li>
</ul>

<h3>Title: LSA64: An Argentinian Sign Language Dataset. (arXiv:2310.17429v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17429">http://arxiv.org/abs/2310.17429</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17429]] LSA64: An Argentinian Sign Language Dataset(http://arxiv.org/abs/2310.17429)</code></li>
<li>Summary: <p>Automatic sign language recognition is a research area that encompasses
human-computer interaction, computer vision and machine learning. Robust
automatic recognition of sign language could assist in the translation process
and the integration of hearing-impaired people, as well as the teaching of sign
language to the hearing population. Sign languages differ significantly in
different countries and even regions, and their syntax and semantics are
different as well from those of written languages. While the techniques for
automatic sign language recognition are mostly the same for different
languages, training a recognition system for a new language requires having an
entire dataset for that language. This paper presents a dataset of 64 signs
from the Argentinian Sign Language (LSA). The dataset, called LSA64, contains
3200 videos of 64 different LSA signs recorded by 10 subjects, and is a first
step towards building a comprehensive research-level dataset of Argentinian
signs, specifically tailored to sign language recognition or other machine
learning tasks. The subjects that performed the signs wore colored gloves to
ease the hand tracking and segmentation steps, allowing experiments on the
dataset to focus specifically on the recognition of signs. We also present a
pre-processed version of the dataset, from which we computed statistics of
movement, position and handshape of the signs.
</p></li>
</ul>

<h3>Title: Sign Languague Recognition without frame-sequencing constraints: A proof of concept on the Argentinian Sign Language. (arXiv:2310.17437v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17437">http://arxiv.org/abs/2310.17437</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17437]] Sign Languague Recognition without frame-sequencing constraints: A proof of concept on the Argentinian Sign Language(http://arxiv.org/abs/2310.17437)</code></li>
<li>Summary: <p>Automatic sign language recognition (SLR) is an important topic within the
areas of human-computer interaction and machine learning. On the one hand, it
poses a complex challenge that requires the intervention of various knowledge
areas, such as video processing, image processing, intelligent systems and
linguistics. On the other hand, robust recognition of sign language could
assist in the translation process and the integration of hearing-impaired
people, as well as the teaching of sign language for the hearing population.
</p>
<p>SLR systems usually employ Hidden Markov Models, Dynamic Time Warping or
similar models to recognize signs. Such techniques exploit the sequential
ordering of frames to reduce the number of hypothesis. This paper presents a
general probabilistic model for sign classification that combines
sub-classifiers based on different types of features such as position, movement
and handshape. The model employs a bag-of-words approach in all classification
steps, to explore the hypothesis that ordering is not essential for
recognition. The proposed model achieved an accuracy rate of 97% on an
Argentinian Sign Language dataset containing 64 classes of signs and 3200
samples, providing some evidence that indeed recognition without ordering is
possible.
</p></li>
</ul>

<h3>Title: Cross-modal Active Complementary Learning with Self-refining Correspondence. (arXiv:2310.17468v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17468">http://arxiv.org/abs/2310.17468</a></li>
<li>Code URL: https://github.com/qinyang79/crcl</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17468]] Cross-modal Active Complementary Learning with Self-refining Correspondence(http://arxiv.org/abs/2310.17468)</code></li>
<li>Summary: <p>Recently, image-text matching has attracted more and more attention from
academia and industry, which is fundamental to understanding the latent
correspondence across visual and textual modalities. However, most existing
methods implicitly assume the training pairs are well-aligned while ignoring
the ubiquitous annotation noise, a.k.a noisy correspondence (NC), thereby
inevitably leading to a performance drop. Although some methods attempt to
address such noise, they still face two challenging problems: excessive
memorizing/overfitting and unreliable correction for NC, especially under high
noise. To address the two problems, we propose a generalized Cross-modal Robust
Complementary Learning framework (CRCL), which benefits from a novel Active
Complementary Loss (ACL) and an efficient Self-refining Correspondence
Correction (SCC) to improve the robustness of existing methods. Specifically,
ACL exploits active and complementary learning losses to reduce the risk of
providing erroneous supervision, leading to theoretically and experimentally
demonstrated robustness against NC. SCC utilizes multiple self-refining
processes with momentum correction to enlarge the receptive field for
correcting correspondences, thereby alleviating error accumulation and
achieving accurate and stable corrections. We carry out extensive experiments
on three image-text benchmarks, i.e., Flickr30K, MS-COCO, and CC152K, to verify
the superior robustness of our CRCL against synthetic and real-world noisy
correspondences.
</p></li>
</ul>

<h3>Title: On Surgical Fine-tuning for Language Encoders. (arXiv:2310.17041v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17041">http://arxiv.org/abs/2310.17041</a></li>
<li>Code URL: https://github.com/ymtao5219/surgical_fine_tuning</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17041]] On Surgical Fine-tuning for Language Encoders(http://arxiv.org/abs/2310.17041)</code></li>
<li>Summary: <p>Fine-tuning all the layers of a pre-trained neural language encoder (either
using all the parameters or using parameter-efficient methods) is often the
de-facto way of adapting it to a new task. We show evidence that for different
downstream language tasks, fine-tuning only a subset of layers is sufficient to
obtain performance that is close to and often better than fine-tuning all the
layers in the language encoder. We propose an efficient metric based on the
diagonal of the Fisher information matrix (FIM score), to select the candidate
layers for selective fine-tuning. We show, empirically on GLUE and SuperGLUE
tasks and across distinct language encoders, that this metric can effectively
select layers leading to a strong downstream performance. Our work highlights
that task-specific information corresponding to a given downstream task is
often localized within a few layers, and tuning only those is sufficient for
strong performance. Additionally, we demonstrate the robustness of the FIM
score to rank layers in a manner that remains constant during the optimization
process.
</p></li>
</ul>

<h3>Title: Incorporating Probing Signals into Multimodal Machine Translation via Visual Question-Answering Pairs. (arXiv:2310.17133v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17133">http://arxiv.org/abs/2310.17133</a></li>
<li>Code URL: https://github.com/libeineu/mmt-vqa</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17133]] Incorporating Probing Signals into Multimodal Machine Translation via Visual Question-Answering Pairs(http://arxiv.org/abs/2310.17133)</code></li>
<li>Summary: <p>This paper presents an in-depth study of multimodal machine translation
(MMT), examining the prevailing understanding that MMT systems exhibit
decreased sensitivity to visual information when text inputs are complete.
Instead, we attribute this phenomenon to insufficient cross-modal interaction,
rather than image information redundancy. A novel approach is proposed to
generate parallel Visual Question-Answering (VQA) style pairs from the source
text, fostering more robust cross-modal interaction. Using Large Language
Models (LLMs), we explicitly model the probing signal in MMT to convert it into
VQA-style data to create the Multi30K-VQA dataset. An MMT-VQA multitask
learning framework is introduced to incorporate explicit probing signals from
the dataset into the MMT training process. Experimental results on two
widely-used benchmarks demonstrate the effectiveness of this novel approach.
Our code and data would be available at:
\url{https://github.com/libeineu/MMT-VQA}.
</p></li>
</ul>

<h3>Title: Learning to Abstract with Nonparametric Variational Information Bottleneck. (arXiv:2310.17284v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17284">http://arxiv.org/abs/2310.17284</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17284]] Learning to Abstract with Nonparametric Variational Information Bottleneck(http://arxiv.org/abs/2310.17284)</code></li>
<li>Summary: <p>Learned representations at the level of characters, sub-words, words and
sentences, have each contributed to advances in understanding different NLP
tasks and linguistic phenomena. However, learning textual embeddings is costly
as they are tokenization specific and require different models to be trained
for each level of abstraction. We introduce a novel language representation
model which can learn to compress to different levels of abstraction at
different layers of the same model. We apply Nonparametric Variational
Information Bottleneck (NVIB) to stacked Transformer self-attention layers in
the encoder, which encourages an information-theoretic compression of the
representations through the model. We find that the layers within the model
correspond to increasing levels of abstraction and that their representations
are more linguistically informed. Finally, we show that NVIB compression
results in a model which is more robust to adversarial perturbations.
</p></li>
</ul>

<h3>Title: A Method for Network Intrusion Detection Using Flow Sequence and BERT Framework. (arXiv:2310.17127v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17127">http://arxiv.org/abs/2310.17127</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17127]] A Method for Network Intrusion Detection Using Flow Sequence and BERT Framework(http://arxiv.org/abs/2310.17127)</code></li>
<li>Summary: <p>A Network Intrusion Detection System (NIDS) is a tool that identifies
potential threats to a network. Recently, different flow-based NIDS designs
utilizing Machine Learning (ML) algorithms have been proposed as solutions to
detect intrusions efficiently. However, conventional ML-based classifiers have
not seen widespread adoption in the real world due to their poor domain
adaptation capability. In this research, our goal is to explore the possibility
of using sequences of flows to improve the domain adaptation capability of
network intrusion detection systems. Our proposal employs natural language
processing techniques and Bidirectional Encoder Representations from
Transformers framework, which is an effective technique for modeling data with
respect to its context. Early empirical results show that our approach has
improved domain adaptation capability compared to previous approaches. The
proposed approach provides a new research method for building a robust
intrusion detection system.
</p></li>
</ul>

<h3>Title: Large-Scale Gaussian Processes via Alternating Projection. (arXiv:2310.17137v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17137">http://arxiv.org/abs/2310.17137</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17137]] Large-Scale Gaussian Processes via Alternating Projection(http://arxiv.org/abs/2310.17137)</code></li>
<li>Summary: <p>Gaussian process (GP) hyperparameter optimization requires repeatedly solving
linear systems with $n \times n$ kernel matrices. To address the prohibitive
$\mathcal{O}(n^3)$ time complexity, recent work has employed fast iterative
numerical methods, like conjugate gradients (CG). However, as datasets increase
in magnitude, the corresponding kernel matrices become increasingly
ill-conditioned and still require $\mathcal{O}(n^2)$ space without
partitioning. Thus, while CG increases the size of datasets GPs can be trained
on, modern datasets reach scales beyond its applicability. In this work, we
propose an iterative method which only accesses subblocks of the kernel matrix,
effectively enabling \emph{mini-batching}. Our algorithm, based on alternating
projection, has $\mathcal{O}(n)$ per-iteration time and space complexity,
solving many of the practical challenges of scaling GPs to very large datasets.
Theoretically, we prove our method enjoys linear convergence and empirically we
demonstrate its robustness to ill-conditioning. On large-scale benchmark
datasets up to four million datapoints our approach accelerates training by a
factor of 2$\times$ to 27$\times$ compared to CG.
</p></li>
</ul>

<h3>Title: Understanding and Addressing the Pitfalls of Bisimulation-based Representations in Offline Reinforcement Learning. (arXiv:2310.17139v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17139">http://arxiv.org/abs/2310.17139</a></li>
<li>Code URL: https://github.com/zanghyu/offline_bisimulation</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17139]] Understanding and Addressing the Pitfalls of Bisimulation-based Representations in Offline Reinforcement Learning(http://arxiv.org/abs/2310.17139)</code></li>
<li>Summary: <p>While bisimulation-based approaches hold promise for learning robust state
representations for Reinforcement Learning (RL) tasks, their efficacy in
offline RL tasks has not been up to par. In some instances, their performance
has even significantly underperformed alternative methods. We aim to understand
why bisimulation methods succeed in online settings, but falter in offline
tasks. Our analysis reveals that missing transitions in the dataset are
particularly harmful to the bisimulation principle, leading to ineffective
estimation. We also shed light on the critical role of reward scaling in
bounding the scale of bisimulation measurements and of the value error they
induce. Based on these findings, we propose to apply the expectile operator for
representation learning to our offline RL setting, which helps to prevent
overfitting to incomplete data. Meanwhile, by introducing an appropriate reward
scaling strategy, we avoid the risk of feature collapse in representation
space. We implement these recommendations on two state-of-the-art
bisimulation-based algorithms, MICo and SimSR, and demonstrate performance
gains on two benchmark suites: D4RL and Visual D4RL. Codes are provided at
\url{https://github.com/zanghyu/Offline_Bisimulation}.
</p></li>
</ul>

<h3>Title: Counterfactual-Augmented Importance Sampling for Semi-Offline Policy Evaluation. (arXiv:2310.17146v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17146">http://arxiv.org/abs/2310.17146</a></li>
<li>Code URL: https://github.com/mld3/counterfactualannot-semiope</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17146]] Counterfactual-Augmented Importance Sampling for Semi-Offline Policy Evaluation(http://arxiv.org/abs/2310.17146)</code></li>
<li>Summary: <p>In applying reinforcement learning (RL) to high-stakes domains, quantitative
and qualitative evaluation using observational data can help practitioners
understand the generalization performance of new policies. However, this type
of off-policy evaluation (OPE) is inherently limited since offline data may not
reflect the distribution shifts resulting from the application of new policies.
On the other hand, online evaluation by collecting rollouts according to the
new policy is often infeasible, as deploying new policies in these domains can
be unsafe. In this work, we propose a semi-offline evaluation framework as an
intermediate step between offline and online evaluation, where human users
provide annotations of unobserved counterfactual trajectories. While tempting
to simply augment existing data with such annotations, we show that this naive
approach can lead to biased results. Instead, we design a new family of OPE
estimators based on importance sampling (IS) and a novel weighting scheme that
incorporate counterfactual annotations without introducing additional bias. We
analyze the theoretical properties of our approach, showing its potential to
reduce both bias and variance compared to standard IS estimators. Our analyses
reveal important practical considerations for handling biased, noisy, or
missing annotations. In a series of proof-of-concept experiments involving
bandits and a healthcare-inspired simulator, we demonstrate that our approach
outperforms purely offline IS estimators and is robust to imperfect
annotations. Our framework, combined with principled human-centered design of
annotation solicitation, can enable the application of RL in high-stakes
domains.
</p></li>
</ul>

<h3>Title: DSAC-C: Constrained Maximum Entropy for Robust Discrete Soft-Actor Critic. (arXiv:2310.17173v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17173">http://arxiv.org/abs/2310.17173</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17173]] DSAC-C: Constrained Maximum Entropy for Robust Discrete Soft-Actor Critic(http://arxiv.org/abs/2310.17173)</code></li>
<li>Summary: <p>We present a novel extension to the family of Soft Actor-Critic (SAC)
algorithms. We argue that based on the Maximum Entropy Principle, discrete SAC
can be further improved via additional statistical constraints derived from a
surrogate critic policy. Furthermore, our findings suggests that these
constraints provide an added robustness against potential domain shifts, which
are essential for safe deployment of reinforcement learning agents in the
real-world. We provide theoretical analysis and show empirical results on low
data regimes for both in-distribution and out-of-distribution variants of Atari
2600 games.
</p></li>
</ul>

<h3>Title: Little Exploration is All You Need. (arXiv:2310.17538v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17538">http://arxiv.org/abs/2310.17538</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17538]] Little Exploration is All You Need(http://arxiv.org/abs/2310.17538)</code></li>
<li>Summary: <p>The prevailing principle of "Optimism in the Face of Uncertainty" advocates
for the incorporation of an exploration bonus, generally assumed to be
proportional to the inverse square root of the visit count ($1/\sqrt{n}$),
where $n$ is the number of visits to a particular state-action pair. This
approach, however, exclusively focuses on "uncertainty," neglecting the
inherent "difficulty" of different options. To address this gap, we introduce a
novel modification of standard UCB algorithm in the multi-armed bandit problem,
proposing an adjusted bonus term of $1/n^\tau$, where $\tau &gt; 1/2$, that
accounts for task difficulty. Our proposed algorithm, denoted as UCB$^\tau$, is
substantiated through comprehensive regret and risk analyses, confirming its
theoretical robustness. Comparative evaluations with standard UCB and Thompson
Sampling algorithms on synthetic datasets demonstrate that UCB$^\tau$ not only
outperforms in efficacy but also exhibits lower risk across various
environmental conditions and hyperparameter settings.
</p></li>
</ul>

<h2>biometric</h2>
<h3>Title: On the recognition of the game type based on physiological signals and eye tracking. (arXiv:2310.17383v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17383">http://arxiv.org/abs/2310.17383</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17383]] On the recognition of the game type based on physiological signals and eye tracking(http://arxiv.org/abs/2310.17383)</code></li>
<li>Summary: <p>Automated interpretation of signals yields many impressive applications from
the area of affective computing and human activity recognition (HAR). In this
paper we ask the question about possibility of cognitive activity recognition
on the base of particular set of signals. We use recognition of the game played
by the participant as a playground for exploration of the problem. We build
classifier of three different games (Space Invaders, Tetris, Tower Defence) and
inter-game pause. We validate classifier in the player-independent and
player-dependent scenario. We discuss the improvement in the player-dependent
scenario in the context of biometric person recognition. On the base of the
results obtained in game classification, we consider potential applications in
smart surveillance and quantified self.
</p></li>
</ul>

<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Circuit as Set of Points. (arXiv:2310.17418v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17418">http://arxiv.org/abs/2310.17418</a></li>
<li>Code URL: https://github.com/hustvl/circuitformer</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17418]] Circuit as Set of Points(http://arxiv.org/abs/2310.17418)</code></li>
<li>Summary: <p>As the size of circuit designs continues to grow rapidly, artificial
intelligence technologies are being extensively used in Electronic Design
Automation (EDA) to assist with circuit design. Placement and routing are the
most time-consuming parts of the physical design process, and how to quickly
evaluate the placement has become a hot research topic. Prior works either
transformed circuit designs into images using hand-crafted methods and then
used Convolutional Neural Networks (CNN) to extract features, which are limited
by the quality of the hand-crafted methods and could not achieve end-to-end
training, or treated the circuit design as a graph structure and used Graph
Neural Networks (GNN) to extract features, which require time-consuming
preprocessing. In our work, we propose a novel perspective for circuit design
by treating circuit components as point clouds and using Transformer-based
point cloud perception methods to extract features from the circuit. This
approach enables direct feature extraction from raw data without any
preprocessing, allows for end-to-end training, and results in high performance.
Experimental results show that our method achieves state-of-the-art performance
in congestion prediction tasks on both the CircuitNet and ISPD2015 datasets, as
well as in design rule check (DRC) violation prediction tasks on the CircuitNet
dataset. Our method establishes a bridge between the relatively mature point
cloud perception methods and the fast-developing EDA algorithms, enabling us to
leverage more collective intelligence to solve this task. To facilitate the
research of open EDA design, source codes and pre-trained models are released
at https://github.com/hustvl/circuitformer.
</p></li>
</ul>

<h3>Title: A Hybrid Graph Network for Complex Activity Detection in Video. (arXiv:2310.17493v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17493">http://arxiv.org/abs/2310.17493</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17493]] A Hybrid Graph Network for Complex Activity Detection in Video(http://arxiv.org/abs/2310.17493)</code></li>
<li>Summary: <p>Interpretation and understanding of video presents a challenging computer
vision task in numerous fields - e.g. autonomous driving and sports analytics.
Existing approaches to interpreting the actions taking place within a video
clip are based upon Temporal Action Localisation (TAL), which typically
identifies short-term actions. The emerging field of Complex Activity Detection
(CompAD) extends this analysis to long-term activities, with a deeper
understanding obtained by modelling the internal structure of a complex
activity taking place within the video. We address the CompAD problem using a
hybrid graph neural network which combines attention applied to a graph
encoding the local (short-term) dynamic scene with a temporal graph modelling
the overall long-duration activity. Our approach is as follows: i) Firstly, we
propose a novel feature extraction technique which, for each video snippet,
generates spatiotemporal `tubes' for the active elements (`agents') in the
(local) scene by detecting individual objects, tracking them and then
extracting 3D features from all the agent tubes as well as the overall scene.
ii) Next, we construct a local scene graph where each node (representing either
an agent tube or the scene) is connected to all other nodes. Attention is then
applied to this graph to obtain an overall representation of the local dynamic
scene. iii) Finally, all local scene graph representations are interconnected
via a temporal graph, to estimate the complex activity class together with its
start and end time. The proposed framework outperforms all previous
state-of-the-art methods on all three datasets including ActivityNet-1.3,
Thumos-14, and ROAD.
</p></li>
</ul>

<h3>Title: Joint Entity and Relation Extraction with Span Pruning and Hypergraph Neural Networks. (arXiv:2310.17238v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17238">http://arxiv.org/abs/2310.17238</a></li>
<li>Code URL: https://github.com/yanzhh/hgere</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17238]] Joint Entity and Relation Extraction with Span Pruning and Hypergraph Neural Networks(http://arxiv.org/abs/2310.17238)</code></li>
<li>Summary: <p>Entity and Relation Extraction (ERE) is an important task in information
extraction. Recent marker-based pipeline models achieve state-of-the-art
performance, but still suffer from the error propagation issue. Also, most of
current ERE models do not take into account higher-order interactions between
multiple entities and relations, while higher-order modeling could be
beneficial.In this work, we propose HyperGraph neural network for ERE
($\hgnn{}$), which is built upon the PL-marker (a state-of-the-art marker-based
pipleline model). To alleviate error propagation,we use a high-recall pruner
mechanism to transfer the burden of entity identification and labeling from the
NER module to the joint module of our model. For higher-order modeling, we
build a hypergraph, where nodes are entities (provided by the span pruner) and
relations thereof, and hyperedges encode interactions between two different
relations or between a relation and its associated subject and object entities.
We then run a hypergraph neural network for higher-order inference by applying
message passing over the built hypergraph. Experiments on three widely used
benchmarks (\acef{}, \ace{} and \scierc{}) for ERE task show significant
improvements over the previous state-of-the-art PL-marker.
</p></li>
</ul>

<h3>Title: Exploring the Trie of Rules: a fast data structure for the representation of association rules. (arXiv:2310.17355v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17355">http://arxiv.org/abs/2310.17355</a></li>
<li>Code URL: https://github.com/arm-interpretation/trie-of-rules</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17355]] Exploring the Trie of Rules: a fast data structure for the representation of association rules(http://arxiv.org/abs/2310.17355)</code></li>
<li>Summary: <p>Association rule mining techniques can generate a large volume of sequential
data when implemented on transactional databases. Extracting insights from a
large set of association rules has been found to be a challenging process. When
examining a ruleset, the fundamental question is how to summarise and represent
meaningful mined knowledge efficiently. Many algorithms and strategies have
been developed to address issue of knowledge extraction; however, the
effectiveness of this process can be limited by the data structures. A better
data structure can sufficiently affect the speed of the knowledge extraction
process. This paper proposes a novel data structure, called the Trie of rules,
for storing a ruleset that is generated by association rule mining. The
resulting data structure is a prefix-tree graph structure made of pre-mined
rules. This graph stores the rules as paths within the prefix-tree in a way
that similar rules overlay each other. Each node in the tree represents a rule
where a consequent is this node, and an antecedent is a path from this node to
the root of the tree. The evaluation showed that the proposed representation
technique is promising. It compresses a ruleset with almost no data loss and
benefits in terms of time for basic operations such as searching for a specific
rule and sorting, which is the base for many knowledge discovery methods.
Moreover, our method demonstrated a significant improvement in traversing time,
achieving an 8-fold increase compared to traditional data structures.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Navigating Data Heterogeneity in Federated Learning: A Semi-Supervised Approach for Object Detection. (arXiv:2310.17097v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17097">http://arxiv.org/abs/2310.17097</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17097]] Navigating Data Heterogeneity in Federated Learning: A Semi-Supervised Approach for Object Detection(http://arxiv.org/abs/2310.17097)</code></li>
<li>Summary: <p>Federated Learning (FL) has emerged as a potent framework for training models
across distributed data sources while maintaining data privacy. Nevertheless,
it faces challenges with limited high-quality labels and non-IID client data,
particularly in applications like autonomous driving. To address these hurdles,
we navigate the uncharted waters of Semi-Supervised Federated Object Detection
(SSFOD). We present a pioneering SSFOD framework, designed for scenarios where
labeled data reside only at the server while clients possess unlabeled data.
Notably, our method represents the inaugural implementation of SSFOD for
clients with 0% labeled non-IID data, a stark contrast to previous studies that
maintain some subset of labels at each client. We propose FedSTO, a two-stage
strategy encompassing Selective Training followed by Orthogonally enhanced
full-parameter training, to effectively address data shift (e.g. weather
conditions) between server and clients. Our contributions include selectively
refining the backbone of the detector to avert overfitting, orthogonality
regularization to boost representation divergence, and local EMA-driven pseudo
label assignment to yield high-quality pseudo labels. Extensive validation on
prominent autonomous driving datasets (BDD100K, Cityscapes, and SODA10M)
attests to the efficacy of our approach, demonstrating state-of-the-art
results. Remarkably, FedSTO, using just 20-30% of labels, performs nearly as
well as fully-supervised centralized training methods.
</p></li>
</ul>

<h3>Title: Taming Gradient Variance in Federated Learning with Networked Control Variates. (arXiv:2310.17200v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17200">http://arxiv.org/abs/2310.17200</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17200]] Taming Gradient Variance in Federated Learning with Networked Control Variates(http://arxiv.org/abs/2310.17200)</code></li>
<li>Summary: <p>Federated learning, a decentralized approach to machine learning, faces
significant challenges such as extensive communication overheads, slow
convergence, and unstable improvements. These challenges primarily stem from
the gradient variance due to heterogeneous client data distributions. To
address this, we introduce a novel Networked Control Variates (FedNCV)
framework for Federated Learning. We adopt the REINFORCE Leave-One-Out (RLOO)
as a fundamental control variate unit in the FedNCV framework, implemented at
both client and server levels. At the client level, the RLOO control variate is
employed to optimize local gradient updates, mitigating the variance introduced
by data samples. Once relayed to the server, the RLOO-based estimator further
provides an unbiased and low-variance aggregated gradient, leading to robust
global updates. This dual-side application is formalized as a linear
combination of composite control variates. We provide a mathematical expression
capturing this integration of double control variates within FedNCV and present
three theoretical results with corresponding proofs. This unique dual structure
equips FedNCV to address data heterogeneity and scalability issues, thus
potentially paving the way for large-scale applications. Moreover, we tested
FedNCV on six diverse datasets under a Dirichlet distribution with {\alpha} =
0.1, and benchmarked its performance against six SOTA methods, demonstrating
its superiority.
</p></li>
</ul>

<h3>Title: FedPEAT: Convergence of Federated Learning, Parameter-Efficient Fine Tuning, and Emulator Assisted Tuning for Artificial Intelligence Foundation Models with Mobile Edge Computing. (arXiv:2310.17491v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17491">http://arxiv.org/abs/2310.17491</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17491]] FedPEAT: Convergence of Federated Learning, Parameter-Efficient Fine Tuning, and Emulator Assisted Tuning for Artificial Intelligence Foundation Models with Mobile Edge Computing(http://arxiv.org/abs/2310.17491)</code></li>
<li>Summary: <p>The emergence of foundation models, including language and vision models, has
reshaped AI's landscape, offering capabilities across various applications.
Deploying and fine-tuning these large models, like GPT-3 and BERT, presents
challenges, especially in the current foundation model era. We introduce
Emulator-Assisted Tuning (EAT) combined with Parameter-Efficient Fine-Tuning
(PEFT) to form Parameter-Efficient Emulator-Assisted Tuning (PEAT). Further, we
expand this into federated learning as Federated PEAT (FedPEAT). FedPEAT uses
adapters, emulators, and PEFT for federated model tuning, enhancing model
privacy and memory efficiency. Adapters adjust pre-trained models, while
emulators give a compact representation of original models, addressing both
privacy and efficiency. Adaptable to various neural networks, our approach also
uses deep reinforcement learning for hyper-parameter optimization. We tested
FedPEAT in a unique scenario with a server participating in collaborative
federated tuning, showcasing its potential in tackling foundation model
challenges.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: Evaluating Bias and Fairness in Gender-Neutral Pretrained Vision-and-Language Models. (arXiv:2310.17530v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17530">http://arxiv.org/abs/2310.17530</a></li>
<li>Code URL: https://github.com/coastalcph/gender-neutral-vl</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17530]] Evaluating Bias and Fairness in Gender-Neutral Pretrained Vision-and-Language Models(http://arxiv.org/abs/2310.17530)</code></li>
<li>Summary: <p>Pretrained machine learning models are known to perpetuate and even amplify
existing biases in data, which can result in unfair outcomes that ultimately
impact user experience. Therefore, it is crucial to understand the mechanisms
behind those prejudicial biases to ensure that model performance does not
result in discriminatory behaviour toward certain groups or populations. In
this work, we define gender bias as our case study. We quantify bias
amplification in pretraining and after fine-tuning on three families of
vision-and-language models. We investigate the connection, if any, between the
two learning stages, and evaluate how bias amplification reflects on model
performance. Overall, we find that bias amplification in pretraining and after
fine-tuning are independent. We then examine the effect of continued
pretraining on gender-neutral data, finding that this reduces group
disparities, i.e., promotes fairness, on VQAv2 and retrieval tasks without
significantly compromising task performance.
</p></li>
</ul>

<h3>Title: fairret: a Framework for Differentiable Fairness Regularization Terms. (arXiv:2310.17256v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17256">http://arxiv.org/abs/2310.17256</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17256]] fairret: a Framework for Differentiable Fairness Regularization Terms(http://arxiv.org/abs/2310.17256)</code></li>
<li>Summary: <p>Current tools for machine learning fairness only admit a limited range of
fairness definitions and have seen little integration with automatic
differentiation libraries, despite the central role these libraries play in
modern machine learning pipelines.
</p>
<p>We introduce a framework of fairness regularization terms (fairrets) which
quantify bias as modular objectives that are easily integrated in automatic
differentiation pipelines. By employing a general definition of fairness in
terms of linear-fractional statistics, a wide class of fairrets can be computed
efficiently. Experiments show the behavior of their gradients and their utility
in enforcing fairness with minimal loss of predictive power compared to
baselines. Our contribution includes a PyTorch implementation of the fairret
framework.
</p></li>
</ul>

<h3>Title: Fair collaborative vehicle routing: A deep multi-agent reinforcement learning approach. (arXiv:2310.17485v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17485">http://arxiv.org/abs/2310.17485</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17485]] Fair collaborative vehicle routing: A deep multi-agent reinforcement learning approach(http://arxiv.org/abs/2310.17485)</code></li>
<li>Summary: <p>Collaborative vehicle routing occurs when carriers collaborate through
sharing their transportation requests and performing transportation requests on
behalf of each other. This achieves economies of scale, thus reducing cost,
greenhouse gas emissions and road congestion. But which carrier should partner
with whom, and how much should each carrier be compensated? Traditional game
theoretic solution concepts are expensive to calculate as the characteristic
function scales exponentially with the number of agents. This would require
solving the vehicle routing problem (NP-hard) an exponential number of times.
We therefore propose to model this problem as a coalitional bargaining game
solved using deep multi-agent reinforcement learning, where - crucially -
agents are not given access to the characteristic function. Instead, we
implicitly reason about the characteristic function; thus, when deployed in
production, we only need to evaluate the expensive post-collaboration vehicle
routing problem once. Our contribution is that we are the first to consider
both the route allocation problem and gain sharing problem simultaneously -
without access to the expensive characteristic function. Through decentralised
machine learning, our agents bargain with each other and agree to outcomes that
correlate well with the Shapley value - a fair profit allocation mechanism.
Importantly, we are able to achieve a reduction in run-time of 88%.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Codebook Features: Sparse and Discrete Interpretability for Neural Networks. (arXiv:2310.17230v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17230">http://arxiv.org/abs/2310.17230</a></li>
<li>Code URL: https://github.com/taufeeque9/codebook-features</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17230]] Codebook Features: Sparse and Discrete Interpretability for Neural Networks(http://arxiv.org/abs/2310.17230)</code></li>
<li>Summary: <p>Understanding neural networks is challenging in part because of the dense,
continuous nature of their hidden states. We explore whether we can train
neural networks to have hidden states that are sparse, discrete, and more
interpretable by quantizing their continuous features into what we call
codebook features. Codebook features are produced by finetuning neural networks
with vector quantization bottlenecks at each layer, producing a network whose
hidden features are the sum of a small number of discrete vector codes chosen
from a larger codebook. Surprisingly, we find that neural networks can operate
under this extreme bottleneck with only modest degradation in performance. This
sparse, discrete bottleneck also provides an intuitive way of controlling
neural network behavior: first, find codes that activate when the desired
behavior is present, then activate those same codes during generation to elicit
that behavior. We validate our approach by training codebook Transformers on
several different datasets. First, we explore a finite state machine dataset
with far more hidden states than neurons. In this setting, our approach
overcomes the superposition problem by assigning states to distinct codes, and
we find that we can make the neural network behave as if it is in a different
state by activating the code for that state. Second, we train Transformer
language models with up to 410M parameters on two natural language datasets. We
identify codes in these models representing diverse, disentangled concepts
(ranging from negative emotions to months of the year) and find that we can
guide the model to generate different topics by activating the appropriate
codes during inference. Overall, codebook features appear to be a promising
unit of analysis and control for neural networks and interpretability. Our
codebase and models are open-sourced at
https://github.com/taufeeque9/codebook-features.
</p></li>
</ul>

<h3>Title: Explainable Spatio-Temporal Graph Neural Networks. (arXiv:2310.17149v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17149">http://arxiv.org/abs/2310.17149</a></li>
<li>Code URL: https://github.com/hkuds/stexplainer</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17149]] Explainable Spatio-Temporal Graph Neural Networks(http://arxiv.org/abs/2310.17149)</code></li>
<li>Summary: <p>Spatio-temporal graph neural networks (STGNNs) have gained popularity as a
powerful tool for effectively modeling spatio-temporal dependencies in diverse
real-world urban applications, including intelligent transportation and public
safety. However, the black-box nature of STGNNs limits their interpretability,
hindering their application in scenarios related to urban resource allocation
and policy formulation. To bridge this gap, we propose an Explainable
Spatio-Temporal Graph Neural Networks (STExplainer) framework that enhances
STGNNs with inherent explainability, enabling them to provide accurate
predictions and faithful explanations simultaneously. Our framework integrates
a unified spatio-temporal graph attention network with a positional information
fusion layer as the STG encoder and decoder, respectively. Furthermore, we
propose a structure distillation approach based on the Graph Information
Bottleneck (GIB) principle with an explainable objective, which is instantiated
by the STG encoder and decoder. Through extensive experiments, we demonstrate
that our STExplainer outperforms state-of-the-art baselines in terms of
predictive accuracy and explainability metrics (i.e., sparsity and fidelity) on
traffic and crime prediction tasks. Furthermore, our model exhibits superior
representation ability in alleviating data missing and sparsity issues. The
implementation code is available at: https://github.com/HKUDS/STExplainer.
</p></li>
</ul>

<h3>Title: Invariance Measures for Neural Networks. (arXiv:2310.17404v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17404">http://arxiv.org/abs/2310.17404</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17404]] Invariance Measures for Neural Networks(http://arxiv.org/abs/2310.17404)</code></li>
<li>Summary: <p>Invariances in neural networks are useful and necessary for many tasks.
However, the representation of the invariance of most neural network models has
not been characterized. We propose measures to quantify the invariance of
neural networks in terms of their internal representation. The measures are
efficient and interpretable, and can be applied to any neural network model.
They are also more sensitive to invariance than previously defined measures. We
validate the measures and their properties in the domain of affine
transformations and the CIFAR10 and MNIST datasets, including their stability
and interpretability. Using the measures, we perform a first analysis of CNN
models and show that their internal invariance is remarkably stable to random
weight initializations, but not to changes in dataset or transformation. We
believe the measures will enable new avenues of research in invariance
representation.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: This Reads Like That: Deep Learning for Interpretable Natural Language Processing. (arXiv:2310.17010v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17010">http://arxiv.org/abs/2310.17010</a></li>
<li>Code URL: https://github.com/fanconic/this_reads_like_that</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17010]] This Reads Like That: Deep Learning for Interpretable Natural Language Processing(http://arxiv.org/abs/2310.17010)</code></li>
<li>Summary: <p>Prototype learning, a popular machine learning method designed for inherently
interpretable decisions, leverages similarities to learned prototypes for
classifying new data. While it is mainly applied in computer vision, in this
work, we build upon prior research and further explore the extension of
prototypical networks to natural language processing. We introduce a learned
weighted similarity measure that enhances the similarity computation by
focusing on informative dimensions of pre-trained sentence embeddings.
Additionally, we propose a post-hoc explainability mechanism that extracts
prediction-relevant words from both the prototype and input sentences. Finally,
we empirically demonstrate that our proposed method not only improves
predictive performance on the AG News and RT Polarity datasets over a previous
prototype-based approach, but also improves the faithfulness of explanations
compared to rationale-based recurrent convolutions.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Improving Denoising Diffusion Models via Simultaneous Estimation of Image and Noise. (arXiv:2310.17167v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17167">http://arxiv.org/abs/2310.17167</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17167]] Improving Denoising Diffusion Models via Simultaneous Estimation of Image and Noise(http://arxiv.org/abs/2310.17167)</code></li>
<li>Summary: <p>This paper introduces two key contributions aimed at improving the speed and
quality of images generated through inverse diffusion processes. The first
contribution involves reparameterizing the diffusion process in terms of the
angle on a quarter-circular arc between the image and noise, specifically
setting the conventional $\displaystyle \sqrt{\bar{\alpha}}=\cos(\eta)$. This
reparameterization eliminates two singularities and allows for the expression
of diffusion evolution as a well-behaved ordinary differential equation (ODE).
In turn, this allows higher order ODE solvers such as Runge-Kutta methods to be
used effectively. The second contribution is to directly estimate both the
image ($\mathbf{x}_0$) and noise ($\mathbf{\epsilon}$) using our network, which
enables more stable calculations of the update step in the inverse diffusion
steps, as accurate estimation of both the image and noise are crucial at
different stages of the process. Together with these changes, our model
achieves faster generation, with the ability to converge on high-quality images
more quickly, and higher quality of the generated images, as measured by
metrics such as Frechet Inception Distance (FID), spatial Frechet Inception
Distance (sFID), precision, and recall.
</p></li>
</ul>

<h3>Title: Exploring Iterative Refinement with Diffusion Models for Video Grounding. (arXiv:2310.17189v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17189">http://arxiv.org/abs/2310.17189</a></li>
<li>Code URL: https://github.com/mastervito/diffusionvg</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17189]] Exploring Iterative Refinement with Diffusion Models for Video Grounding(http://arxiv.org/abs/2310.17189)</code></li>
<li>Summary: <p>Video grounding aims to localize the target moment in an untrimmed video
corresponding to a given sentence query. Existing methods typically select the
best prediction from a set of predefined proposals or directly regress the
target span in a single-shot manner, resulting in the absence of a systematical
prediction refinement process. In this paper, we propose DiffusionVG, a novel
framework with diffusion models that formulates video grounding as a
conditional generation task, where the target span is generated from Gaussian
noise inputs and interatively refined in the reverse diffusion process. During
training, DiffusionVG progressively adds noise to the target span with a fixed
forward diffusion process and learns to recover the target span in the reverse
diffusion process. In inference, DiffusionVG can generate the target span from
Gaussian noise inputs by the learned reverse diffusion process conditioned on
the video-sentence representations. Our DiffusionVG follows the encoder-decoder
architecture, which firstly encodes the video-sentence features and iteratively
denoises the predicted spans in its specialized span refining decoder. Without
bells and whistles, our DiffusionVG demonstrates competitive or even superior
performance compared to existing well-crafted models on mainstream Charades-STA
and ActivityNet Captions benchmarks.
</p></li>
</ul>

<h3>Title: Defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics. (arXiv:2310.17316v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17316">http://arxiv.org/abs/2310.17316</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17316]] Defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics(http://arxiv.org/abs/2310.17316)</code></li>
<li>Summary: <p>Defect inspection is paramount within the closed-loop manufacturing system.
However, existing datasets for defect inspection often lack precision and
semantic granularity required for practical applications. In this paper, we
introduce the Defect Spectrum, a comprehensive benchmark that offers precise,
semantic-abundant, and large-scale annotations for a wide range of industrial
defects. Building on four key industrial benchmarks, our dataset refines
existing annotations and introduces rich semantic details, distinguishing
multiple defect types within a single image. Furthermore, we introduce
Defect-Gen, a two-stage diffusion-based generator designed to create
high-quality and diverse defective images, even when working with limited
datasets. The synthetic images generated by Defect-Gen significantly enhance
the efficacy of defect inspection models. Overall, The Defect Spectrum dataset
demonstrates its potential in defect inspection research, offering a solid
platform for testing and refining advanced models.
</p></li>
</ul>

<h3>Title: CADS: Unleashing the Diversity of Diffusion Models through Condition-Annealed Sampling. (arXiv:2310.17347v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17347">http://arxiv.org/abs/2310.17347</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17347]] CADS: Unleashing the Diversity of Diffusion Models through Condition-Annealed Sampling(http://arxiv.org/abs/2310.17347)</code></li>
<li>Summary: <p>While conditional diffusion models are known to have good coverage of the
data distribution, they still face limitations in output diversity,
particularly when sampled with a high classifier-free guidance scale for
optimal image quality or when trained on small datasets. We attribute this
problem to the role of the conditioning signal in inference and offer an
improved sampling strategy for diffusion models that can increase generation
diversity, especially at high guidance scales, with minimal loss of sample
quality. Our sampling strategy anneals the conditioning signal by adding
scheduled, monotonically decreasing Gaussian noise to the conditioning vector
during inference to balance diversity and condition alignment. Our
Condition-Annealed Diffusion Sampler (CADS) can be used with any pretrained
model and sampling algorithm, and we show that it boosts the diversity of
diffusion models in various conditional generation tasks. Further, using an
existing pretrained diffusion model, CADS achieves a new state-of-the-art FID
of 1.70 and 2.31 for class-conditional ImageNet generation at 256$\times$256
and 512$\times$512 respectively.
</p></li>
</ul>

<h3>Title: The Expressive Power of Low-Rank Adaptation. (arXiv:2310.17513v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17513">http://arxiv.org/abs/2310.17513</a></li>
<li>Code URL: https://github.com/uw-madison-lee-lab/expressive_power_of_lora</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17513]] The Expressive Power of Low-Rank Adaptation(http://arxiv.org/abs/2310.17513)</code></li>
<li>Summary: <p>Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method that
leverages low-rank adaptation of weight matrices, has emerged as a prevalent
technique for fine-tuning pre-trained models such as large language models and
diffusion models. Despite its huge success in practice, the theoretical
underpinnings of LoRA have largely remained unexplored. This paper takes the
first step to bridge this gap by theoretically analyzing the expressive power
of LoRA. We prove that, for fully connected neural networks, LoRA can adapt any
model $f$ to accurately represent any smaller target model $\overline{f}$ if
LoRA-rank $\geq(\text{width of }f) \times \frac{\text{depth of
}\overline{f}}{\text{depth of }f}$. We also quantify the approximation error
when LoRA-rank is lower than the threshold. For Transformer networks, we show
any model can be adapted to a target model of the same size with
rank-$(\frac{\text{embedding size}}{2})$ LoRA adapters.
</p></li>
</ul>

<h3>Title: Hierarchical Semi-Implicit Variational Inference with Application to Diffusion Model Acceleration. (arXiv:2310.17153v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17153">http://arxiv.org/abs/2310.17153</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17153]] Hierarchical Semi-Implicit Variational Inference with Application to Diffusion Model Acceleration(http://arxiv.org/abs/2310.17153)</code></li>
<li>Summary: <p>Semi-implicit variational inference (SIVI) has been introduced to expand the
analytical variational families by defining expressive semi-implicit
distributions in a hierarchical manner. However, the single-layer architecture
commonly used in current SIVI methods can be insufficient when the target
posterior has complicated structures. In this paper, we propose hierarchical
semi-implicit variational inference, called HSIVI, which generalizes SIVI to
allow more expressive multi-layer construction of semi-implicit distributions.
By introducing auxiliary distributions that interpolate between a simple base
distribution and the target distribution, the conditional layers can be trained
by progressively matching these auxiliary distributions one layer after
another. Moreover, given pre-trained score networks, HSIVI can be used to
accelerate the sampling process of diffusion models with the score matching
objective. We show that HSIVI significantly enhances the expressiveness of SIVI
on several Bayesian inference problems with complicated target distributions.
When used for diffusion model acceleration, we show that HSIVI can produce high
quality samples comparable to or better than the existing fast diffusion model
based samplers with a small number of function evaluations on various datasets.
</p></li>
</ul>

<h3>Title: Towards Unifying Diffusion Models for Probabilistic Spatio-Temporal Graph Learning. (arXiv:2310.17360v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17360">http://arxiv.org/abs/2310.17360</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17360]] Towards Unifying Diffusion Models for Probabilistic Spatio-Temporal Graph Learning(http://arxiv.org/abs/2310.17360)</code></li>
<li>Summary: <p>Spatio-temporal graph learning is a fundamental problem in the Web of Things
era, which enables a plethora of Web applications such as smart cities, human
mobility and climate analysis. Existing approaches tackle different learning
tasks independently, tailoring their models to unique task characteristics.
These methods, however, fall short of modeling intrinsic uncertainties in the
spatio-temporal data. Meanwhile, their specialized designs limit their
universality as general spatio-temporal learning solutions. In this paper, we
propose to model the learning tasks in a unified perspective, viewing them as
predictions based on conditional information with shared spatio-temporal
patterns. Based on this proposal, we introduce Unified Spatio-Temporal
Diffusion Models (USTD) to address the tasks uniformly within the
uncertainty-aware diffusion framework. USTD is holistically designed,
comprising a shared spatio-temporal encoder and attention-based denoising
networks that are task-specific. The shared encoder, optimized by a
pre-training strategy, effectively captures conditional spatio-temporal
patterns. The denoising networks, utilizing both cross- and self-attention,
integrate conditional dependencies and generate predictions. Opting for
forecasting and kriging as downstream tasks, we design Gated Attention (SGA)
and Temporal Gated Attention (TGA) for each task, with different emphases on
the spatial and temporal dimensions, respectively. By combining the advantages
of deterministic encoders and probabilistic diffusion models, USTD achieves
state-of-the-art performances compared to deterministic and probabilistic
baselines in both tasks, while also providing valuable uncertainty estimates.
</p></li>
</ul>

<h3>Title: Causal Modeling with Stationary Diffusions. (arXiv:2310.17405v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17405">http://arxiv.org/abs/2310.17405</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17405]] Causal Modeling with Stationary Diffusions(http://arxiv.org/abs/2310.17405)</code></li>
<li>Summary: <p>We develop a novel approach towards causal inference. Rather than structural
equations over a causal graph, we learn stochastic differential equations
(SDEs) whose stationary densities model a system's behavior under
interventions. These stationary diffusion models do not require the formalism
of causal graphs, let alone the common assumption of acyclicity. We show that
in several cases, they generalize to unseen interventions on their variables,
often better than classical approaches. Our inference method is based on a new
theoretical result that expresses a stationarity condition on the diffusion's
generator in a reproducing kernel Hilbert space. The resulting kernel deviation
from stationarity (KDS) is an objective function of independent interest.
</p></li>
</ul>

<h3>Title: Likelihood-based Out-of-Distribution Detection with Denoising Diffusion Probabilistic Models. (arXiv:2310.17432v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17432">http://arxiv.org/abs/2310.17432</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17432]] Likelihood-based Out-of-Distribution Detection with Denoising Diffusion Probabilistic Models(http://arxiv.org/abs/2310.17432)</code></li>
<li>Summary: <p>Out-of-Distribution detection between dataset pairs has been extensively
explored with generative models. We show that likelihood-based
Out-of-Distribution detection can be extended to diffusion models by leveraging
the fact that they, like other likelihood-based generative models, are
dramatically affected by the input sample complexity. Currently, all
Out-of-Distribution detection methods with Diffusion Models are
reconstruction-based. We propose a new likelihood ratio for Out-of-Distribution
detection with Deep Denoising Diffusion Models, which we call the Complexity
Corrected Likelihood Ratio. Our likelihood ratio is constructed using Evidence
Lower-Bound evaluations from an individual model at various noising levels. We
present results that are comparable to state-of-the-art Out-of-Distribution
detection methods with generative models.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: GraFT: Gradual Fusion Transformer for Multimodal Re-Identification. (arXiv:2310.16856v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16856">http://arxiv.org/abs/2310.16856</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16856]] GraFT: Gradual Fusion Transformer for Multimodal Re-Identification(http://arxiv.org/abs/2310.16856)</code></li>
<li>Summary: <p>Object Re-Identification (ReID) is pivotal in computer vision, witnessing an
escalating demand for adept multimodal representation learning. Current models,
although promising, reveal scalability limitations with increasing modalities
as they rely heavily on late fusion, which postpones the integration of
specific modality insights. Addressing this, we introduce the \textbf{Gradual
Fusion Transformer (GraFT)} for multimodal ReID. At its core, GraFT employs
learnable fusion tokens that guide self-attention across encoders, adeptly
capturing both modality-specific and object-specific features. Further
bolstering its efficacy, we introduce a novel training paradigm combined with
an augmented triplet loss, optimizing the ReID feature embedding space. We
demonstrate these enhancements through extensive ablation studies and show that
GraFT consistently surpasses established multimodal ReID benchmarks.
Additionally, aiming for deployment versatility, we've integrated neural
network pruning into GraFT, offering a balance between model size and
performance.
</p></li>
</ul>

<h3>Title: General Point Model with Autoencoding and Autoregressive. (arXiv:2310.16861v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16861">http://arxiv.org/abs/2310.16861</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16861]] General Point Model with Autoencoding and Autoregressive(http://arxiv.org/abs/2310.16861)</code></li>
<li>Summary: <p>The pre-training architectures of large language models encompass various
types, including autoencoding models, autoregressive models, and
encoder-decoder models. We posit that any modality can potentially benefit from
a large language model, as long as it undergoes vector quantization to become
discrete tokens. Inspired by GLM, we propose a General Point Model (GPM) which
seamlessly integrates autoencoding and autoregressive tasks in point cloud
transformer. This model is versatile, allowing fine-tuning for downstream point
cloud representation tasks, as well as unconditional and conditional generation
tasks. GPM enhances masked prediction in autoencoding through various forms of
mask padding tasks, leading to improved performance in point cloud
understanding. Additionally, GPM demonstrates highly competitive results in
unconditional point cloud generation tasks, even exhibiting the potential for
conditional generation tasks by modifying the input's conditional information.
Compared to models like Point-BERT, MaskPoint and PointMAE, our GPM achieves
superior performance in point cloud understanding tasks. Furthermore, the
integration of autoregressive and autoencoding within the same transformer
underscores its versatility across different downstream tasks.
</p></li>
</ul>

<h3>Title: MCUFormer: Deploying Vision Tranformers on Microcontrollers with Limited Memory. (arXiv:2310.16898v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16898">http://arxiv.org/abs/2310.16898</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16898]] MCUFormer: Deploying Vision Tranformers on Microcontrollers with Limited Memory(http://arxiv.org/abs/2310.16898)</code></li>
<li>Summary: <p>Due to the high price and heavy energy consumption of GPUs, deploying deep
models on IoT devices such as microcontrollers makes significant contributions
for ecological AI. Conventional methods successfully enable convolutional
neural network inference of high resolution images on microcontrollers, while
the framework for vision transformers that achieve the state-of-the-art
performance in many vision applications still remains unexplored. In this
paper, we propose a hardware-algorithm co-optimizations method called MCUFormer
to deploy vision transformers on microcontrollers with extremely limited
memory, where we jointly design transformer architecture and construct the
inference operator library to fit the memory resource constraint. More
specifically, we generalize the one-shot network architecture search (NAS) to
discover the optimal architecture with highest task performance given the
memory budget from the microcontrollers, where we enlarge the existing search
space of vision transformers by considering the low-rank decomposition
dimensions and patch resolution for memory reduction. For the construction of
the inference operator library of vision transformers, we schedule the memory
buffer during inference through operator integration, patch embedding
decomposition, and token overwriting, allowing the memory buffer to be fully
utilized to adapt to the forward pass of the vision transformer. Experimental
results demonstrate that our MCUFormer achieves 73.62\% top-1 accuracy on
ImageNet for image classification with 320KB memory on STM32F746
microcontroller. Code is available at https://github.com/liangyn22/MCUFormer.
</p></li>
</ul>

<h3>Title: HCT: Hybrid Convnet-Transformer for Parkinson's disease detection and severity prediction from gait. (arXiv:2310.17078v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17078">http://arxiv.org/abs/2310.17078</a></li>
<li>Code URL: https://github.com/safwennaimi/hct-hybrid-convnet-transformer-for-parkinson-s-disease-detection-and-severity-prediction-from-gait</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17078]] HCT: Hybrid Convnet-Transformer for Parkinson's disease detection and severity prediction from gait(http://arxiv.org/abs/2310.17078)</code></li>
<li>Summary: <p>In this paper, we propose a novel deep learning method based on a new Hybrid
ConvNet-Transformer architecture to detect and stage Parkinson's disease (PD)
from gait data. We adopt a two-step approach by dividing the problem into two
sub-problems. Our Hybrid ConvNet-Transformer model first distinguishes healthy
versus parkinsonian patients. If the patient is parkinsonian, a multi-class
Hybrid ConvNet-Transformer model determines the Hoehn and Yahr (H&amp;Y) score to
assess the PD severity stage. Our hybrid architecture exploits the strengths of
both Convolutional Neural Networks (ConvNets) and Transformers to accurately
detect PD and determine the severity stage. In particular, we take advantage of
ConvNets to capture local patterns and correlations in the data, while we
exploit Transformers for handling long-term dependencies in the input signal.
We show that our hybrid method achieves superior performance when compared to
other state-of-the-art methods, with a PD detection accuracy of 97% and a
severity staging accuracy of 87%. Our source code is available at:
https://github.com/SafwenNaimi
</p></li>
</ul>

<h3>Title: MO-YOLO: End-to-End Multiple-Object Tracking Method with YOLO and MOTR. (arXiv:2310.17170v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17170">http://arxiv.org/abs/2310.17170</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17170]] MO-YOLO: End-to-End Multiple-Object Tracking Method with YOLO and MOTR(http://arxiv.org/abs/2310.17170)</code></li>
<li>Summary: <p>This paper aims to address critical issues in the field of Multi-Object
Tracking (MOT) by proposing an efficient and computationally resource-efficient
end-to-end multi-object tracking model, named MO-YOLO. Traditional MOT methods
typically involve two separate steps: object detection and object tracking,
leading to computational complexity and error propagation issues. Recent
research has demonstrated outstanding performance in end-to-end MOT models
based on Transformer architectures, but they require substantial hardware
support. MO-YOLO combines the strengths of YOLO and RT-DETR models to construct
a high-efficiency, lightweight, and resource-efficient end-to-end multi-object
tracking network, offering new opportunities in the multi-object tracking
domain. On the MOT17 dataset, MOTR\cite{zeng2022motr} requires training with 8
GeForce 2080 Ti GPUs for 4 days to achieve satisfactory results, while MO-YOLO
only requires 1 GeForce 2080 Ti GPU and 12 hours of training to achieve
comparable performance.
</p></li>
</ul>

<h3>Title: Divide et Impera: Multi-Transformer Architectures for Complex NLP-Tasks. (arXiv:2310.16897v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16897">http://arxiv.org/abs/2310.16897</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16897]] Divide et Impera: Multi-Transformer Architectures for Complex NLP-Tasks(http://arxiv.org/abs/2310.16897)</code></li>
<li>Summary: <p>The growing capabilities of transformer models pave the way for solving
increasingly complex NLP tasks. A key to supporting application-specific
requirements is the ability to fine-tune. However, compiling a fine-tuning
dataset tailored to complex tasks is tedious and results in large datasets,
limiting the ability to control transformer output. We present an approach in
which complex tasks are divided into simpler subtasks. Multiple transformer
models are fine-tuned to one subtask each, and lined up to accomplish the
complex task. This simplifies the compilation of fine-tuning datasets and
increases overall controllability. Using the example of reducing gender bias as
a complex task, we demonstrate our approach and show that it performs better
than using a single model.
</p></li>
</ul>

<h3>Title: Learning Transfers over Several Programming Languages. (arXiv:2310.16937v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16937">http://arxiv.org/abs/2310.16937</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16937]] Learning Transfers over Several Programming Languages(http://arxiv.org/abs/2310.16937)</code></li>
<li>Summary: <p>Large language models (LLMs) have recently become remarkably good at
improving developer productivity for high-resource programming languages. These
models use two kinds of data: large amounts of unlabeled code samples for
pretraining and relatively smaller amounts of labeled code samples for
fine-tuning or in-context learning. Unfortunately, many programming languages
are low-resource, lacking labeled samples for most tasks and often even lacking
unlabeled samples. Therefore, users of low-resource languages (e.g., legacy or
new languages) miss out on the benefits of LLMs. Cross-lingual transfer
learning uses data from a source language to improve model performance on a
target language. It has been well-studied for natural languages, but has
received little attention for programming languages. This paper reports
extensive experiments on four tasks using a transformer-based LLM and 11 to 41
programming languages to explore the following questions. First, how well
cross-lingual transfer works for a given task across different language pairs.
Second, given a task and target language, how to best choose a source language.
Third, the characteristics of a language pair that are predictive of transfer
performance, and fourth, how that depends on the given task.
</p></li>
</ul>

<h3>Title: How well can machine-generated texts be identified and can language models be trained to avoid identification?. (arXiv:2310.16992v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16992">http://arxiv.org/abs/2310.16992</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16992]] How well can machine-generated texts be identified and can language models be trained to avoid identification?(http://arxiv.org/abs/2310.16992)</code></li>
<li>Summary: <p>With the rise of generative pre-trained transformer models such as GPT-3,
GPT-NeoX, or OPT, distinguishing human-generated texts from machine-generated
ones has become important. We refined five separate language models to generate
synthetic tweets, uncovering that shallow learning classification algorithms,
like Naive Bayes, achieve detection accuracy between 0.6 and 0.8.
</p>
<p>Shallow learning classifiers differ from human-based detection, especially
when using higher temperature values during text generation, resulting in a
lower detection rate. Humans prioritize linguistic acceptability, which tends
to be higher at lower temperature values. In contrast, transformer-based
classifiers have an accuracy of 0.9 and above. We found that using a
reinforcement learning approach to refine our generative models can
successfully evade BERT-based classifiers with a detection accuracy of 0.15 or
less.
</p></li>
</ul>

<h3>Title: Follow-on Question Suggestion via Voice Hints for Voice Assistants. (arXiv:2310.17034v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17034">http://arxiv.org/abs/2310.17034</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17034]] Follow-on Question Suggestion via Voice Hints for Voice Assistants(http://arxiv.org/abs/2310.17034)</code></li>
<li>Summary: <p>The adoption of voice assistants like Alexa or Siri has grown rapidly,
allowing users to instantly access information via voice search. Query
suggestion is a standard feature of screen-based search experiences, allowing
users to explore additional topics. However, this is not trivial to implement
in voice-based settings. To enable this, we tackle the novel task of suggesting
questions with compact and natural voice hints to allow users to ask follow-up
questions.
</p>
<p>We define the task, ground it in syntactic theory and outline linguistic
desiderata for spoken hints. We propose baselines and an approach using
sequence-to-sequence Transformers to generate spoken hints from a list of
questions. Using a new dataset of 6681 input questions and human written hints,
we evaluated the models with automatic metrics and human evaluation. Results
show that a naive approach of concatenating suggested questions creates poor
voice hints. Our approach, which applies a linguistically-motivated pretraining
task was strongly preferred by humans for producing the most natural hints.
</p></li>
</ul>

<h3>Title: Transformers Learn Higher-Order Optimization Methods for In-Context Learning: A Study with Linear Models. (arXiv:2310.17086v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17086">http://arxiv.org/abs/2310.17086</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17086]] Transformers Learn Higher-Order Optimization Methods for In-Context Learning: A Study with Linear Models(http://arxiv.org/abs/2310.17086)</code></li>
<li>Summary: <p>Transformers are remarkably good at in-context learning (ICL) -- learning
from demonstrations without parameter updates -- but how they perform ICL
remains a mystery. Recent work suggests that Transformers may learn in-context
by internally running Gradient Descent, a first-order optimization method. In
this paper, we instead demonstrate that Transformers learn to implement
higher-order optimization methods to perform ICL. Focusing on in-context linear
regression, we show that Transformers learn to implement an algorithm very
similar to Iterative Newton's Method, a higher-order optimization method,
rather than Gradient Descent. Empirically, we show that predictions from
successive Transformer layers closely match different iterations of Newton's
Method linearly, with each middle layer roughly computing 3 iterations. In
contrast, exponentially more Gradient Descent steps are needed to match an
additional Transformers layer; this suggests that Transformers have an
comparable rate of convergence with high-order methods such as Iterative
Newton, which are exponentially faster than Gradient Descent. We also show that
Transformers can learn in-context on ill-conditioned data, a setting where
Gradient Descent struggles but Iterative Newton succeeds. Finally, we show
theoretical results which support our empirical findings and have a close
correspondence with them: we prove that Transformers can implement $k$
iterations of Newton's method with $\mathcal{O}(k)$ layers.
</p></li>
</ul>

<h3>Title: An Ensemble Method Based on the Combination of Transformers with Convolutional Neural Networks to Detect Artificially Generated Text. (arXiv:2310.17312v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17312">http://arxiv.org/abs/2310.17312</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17312]] An Ensemble Method Based on the Combination of Transformers with Convolutional Neural Networks to Detect Artificially Generated Text(http://arxiv.org/abs/2310.17312)</code></li>
<li>Summary: <p>Thanks to the state-of-the-art Large Language Models (LLMs), language
generation has reached outstanding levels. These models are capable of
generating high quality content, thus making it a challenging task to detect
generated text from human-written content. Despite the advantages provided by
Natural Language Generation, the inability to distinguish automatically
generated text can raise ethical concerns in terms of authenticity.
Consequently, it is important to design and develop methodologies to detect
artificial content. In our work, we present some classification models
constructed by ensembling transformer models such as Sci-BERT, DeBERTa and
XLNet, with Convolutional Neural Networks (CNNs). Our experiments demonstrate
that the considered ensemble architectures surpass the performance of the
individual transformer models for classification. Furthermore, the proposed
SciBERT-CNN ensemble model produced an F1-score of 98.36% on the ALTA shared
task 2023 data.
</p></li>
</ul>

<h3>Title: Harnessing GPT-3.5-turbo for Rhetorical Role Prediction in Legal Cases. (arXiv:2310.17413v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17413">http://arxiv.org/abs/2310.17413</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17413]] Harnessing GPT-3(http://arxiv.org/abs/2310.17413)</code></li>
<li>Summary: <p>We propose a comprehensive study of one-stage elicitation techniques for
querying a large pre-trained generative transformer (GPT-3.5-turbo) in the
rhetorical role prediction task of legal cases. This task is known as requiring
textual context to be addressed. Our study explores strategies such as zero-few
shots, task specification with definitions and clarification of annotation
ambiguities, textual context and reasoning with general prompts and specific
questions. We show that the number of examples, the definition of labels, the
presentation of the (labelled) textual context and specific questions about
this context have a positive influence on the performance of the model. Given
non-equivalent test set configurations, we observed that prompting with a few
labelled examples from direct context can lead the model to a better
performance than a supervised fined-tuned multi-class classifier based on the
BERT encoder (weighted F1 score of = 72%). But there is still a gap to reach
the performance of the best systems = 86%) in the LegalEval 2023 task which, on
the other hand, require dedicated resources, architectures and training.
</p></li>
</ul>

<h3>Title: Transferring a molecular foundation model for polymer property predictions. (arXiv:2310.16958v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16958">http://arxiv.org/abs/2310.16958</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16958]] Transferring a molecular foundation model for polymer property predictions(http://arxiv.org/abs/2310.16958)</code></li>
<li>Summary: <p>Transformer-based large language models have remarkable potential to
accelerate design optimization for applications such as drug development and
materials discovery. Self-supervised pretraining of transformer models requires
large-scale datasets, which are often sparsely populated in topical areas such
as polymer science. State-of-the-art approaches for polymers conduct data
augmentation to generate additional samples but unavoidably incurs extra
computational costs. In contrast, large-scale open-source datasets are
available for small molecules and provide a potential solution to data scarcity
through transfer learning. In this work, we show that using transformers
pretrained on small molecules and fine-tuned on polymer properties achieve
comparable accuracy to those trained on augmented polymer datasets for a series
of benchmark prediction tasks.
</p></li>
</ul>

<h3>Title: Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time. (arXiv:2310.17157v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17157">http://arxiv.org/abs/2310.17157</a></li>
<li>Code URL: https://github.com/fminference/dejavu</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17157]] Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time(http://arxiv.org/abs/2310.17157)</code></li>
<li>Summary: <p>Large language models (LLMs) with hundreds of billions of parameters have
sparked a new wave of exciting AI applications. However, they are
computationally expensive at inference time. Sparsity is a natural approach to
reduce this cost, but existing methods either require costly retraining, have
to forgo LLM's in-context learning ability, or do not yield wall-clock time
speedup on modern hardware. We hypothesize that contextual sparsity, which are
small, input-dependent sets of attention heads and MLP parameters that yield
approximately the same output as the dense model for a given input, can address
these issues. We show that contextual sparsity exists, that it can be
accurately predicted, and that we can exploit it to speed up LLM inference in
wall-clock time without compromising LLM's quality or in-context learning
ability. Based on these insights, we propose DejaVu, a system that uses a
low-cost algorithm to predict contextual sparsity on the fly given inputs to
each layer, along with an asynchronous and hardware-aware implementation that
speeds up LLM inference. We validate that DejaVu can reduce the inference
latency of OPT-175B by over 2X compared to the state-of-the-art
FasterTransformer, and over 6X compared to the widely used Hugging Face
implementation, without compromising model quality. The code is available at
https://github.com/FMInference/DejaVu.
</p></li>
</ul>

<h3>Title: miditok: A Python package for MIDI file tokenization. (arXiv:2310.17202v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17202">http://arxiv.org/abs/2310.17202</a></li>
<li>Code URL: https://github.com/Natooz/MidiTok</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17202]] miditok: A Python package for MIDI file tokenization(http://arxiv.org/abs/2310.17202)</code></li>
<li>Summary: <p>Recent progress in natural language processing has been adapted to the
symbolic music modality. Language models, such as Transformers, have been used
with symbolic music for a variety of tasks among which music generation,
modeling or transcription, with state-of-the-art performances. These models are
beginning to be used in production products. To encode and decode music for the
backbone model, they need to rely on tokenizers, whose role is to serialize
music into sequences of distinct elements called tokens. MidiTok is an
open-source library allowing to tokenize symbolic music with great flexibility
and extended features. It features the most popular music tokenizations, under
a unified API. It is made to be easily used and extensible for everyone.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Attribute Based Interpretable Evaluation Metrics for Generative Models. (arXiv:2310.17261v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17261">http://arxiv.org/abs/2310.17261</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17261]] Attribute Based Interpretable Evaluation Metrics for Generative Models(http://arxiv.org/abs/2310.17261)</code></li>
<li>Summary: <p>When the training dataset comprises a 1:1 proportion of dogs to cats, a
generative model that produces 1:1 dogs and cats better resembles the training
species distribution than another model with 3:1 dogs and cats. Can we capture
this phenomenon using existing metrics? Unfortunately, we cannot, because these
metrics do not provide any interpretability beyond "diversity". In this
context, we propose a new evaluation protocol that measures the divergence of a
set of generated images from the training set regarding the distribution of
attribute strengths as follows. Single-attribute Divergence (SaD) measures the
divergence regarding PDFs of a single attribute. Paired-attribute Divergence
(PaD) measures the divergence regarding joint PDFs of a pair of attributes.
They provide which attributes the models struggle. For measuring the attribute
strengths of an image, we propose Heterogeneous CLIPScore (HCS) which measures
the cosine similarity between image and text vectors with heterogeneous initial
points. With SaD and PaD, we reveal the following about existing generative
models. ProjectedGAN generates implausible attribute relationships such as a
baby with a beard even though it has competitive scores of existing metrics.
Diffusion models struggle to capture diverse colors in the datasets. The larger
sampling timesteps of latent diffusion model generate the more minor objects
including earrings and necklaces. Stable Diffusion v1.5 better captures the
attributes than v2.1. Our metrics lay a foundation for explainable evaluations
of generative models.
</p></li>
</ul>

<h3>Title: C-Disentanglement: Discovering Causally-Independent Generative Factors under an Inductive Bias of Confounder. (arXiv:2310.17325v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17325">http://arxiv.org/abs/2310.17325</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17325]] C-Disentanglement: Discovering Causally-Independent Generative Factors under an Inductive Bias of Confounder(http://arxiv.org/abs/2310.17325)</code></li>
<li>Summary: <p>Representation learning assumes that real-world data is generated by a few
semantically meaningful generative factors (i.e., sources of variation) and
aims to discover them in the latent space. These factors are expected to be
causally disentangled, meaning that distinct factors are encoded into separate
latent variables, and changes in one factor will not affect the values of the
others. Compared to statistical independence, causal disentanglement allows
more controllable data generation, improved robustness, and better
generalization. However, most existing work assumes unconfoundedness in the
discovery process, that there are no common causes to the generative factors
and thus obtain only statistical independence. In this paper, we recognize the
importance of modeling confounders in discovering causal generative factors.
Unfortunately, such factors are not identifiable without proper inductive bias.
We fill the gap by introducing a framework entitled Confounded-Disentanglement
(C-Disentanglement), the first framework that explicitly introduces the
inductive bias of confounder via labels from domain expertise. In addition, we
accordingly propose an approach to sufficiently identify the causally
disentangled factors under any inductive bias of the confounder. We conduct
extensive experiments on both synthetic and real-world datasets. Our method
demonstrates competitive results compared to various SOTA baselines in
obtaining causally disentangled features and downstream tasks under domain
shifts.
</p></li>
</ul>

<h3>Title: Beyond MLE: Convex Learning for Text Generation. (arXiv:2310.17217v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17217">http://arxiv.org/abs/2310.17217</a></li>
<li>Code URL: https://github.com/ictnlp/convex-learning</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17217]] Beyond MLE: Convex Learning for Text Generation(http://arxiv.org/abs/2310.17217)</code></li>
<li>Summary: <p>Maximum likelihood estimation (MLE) is a statistical method used to estimate
the parameters of a probability distribution that best explain the observed
data. In the context of text generation, MLE is often used to train generative
language models, which can then be used to generate new text. However, we argue
that MLE is not always necessary and optimal, especially for closed-ended text
generation tasks like machine translation. In these tasks, the goal of model is
to generate the most appropriate response, which does not necessarily require
it to estimate the entire data distribution with MLE. To this end, we propose a
novel class of training objectives based on convex functions, which enables
text generation models to focus on highly probable outputs without having to
estimate the entire data distribution. We investigate the theoretical
properties of the optimal predicted distribution when applying convex functions
to the loss, demonstrating that convex functions can sharpen the optimal
distribution, thereby enabling the model to better capture outputs with high
probabilities. Experiments on various text generation tasks and models show the
effectiveness of our approach. It enables autoregressive models to bridge the
gap between greedy and beam search, and facilitates the learning of
non-autoregressive models with a maximum improvement of 9+ BLEU points.
Moreover, our approach also exhibits significant impact on large language
models (LLMs), substantially enhancing their generative capability on various
tasks. Source code is available at
\url{https://github.com/ictnlp/Convex-Learning}.
</p></li>
</ul>

<h3>Title: An Explainable Deep Learning-Based Method For Schizophrenia Diagnosis Using Generative Data-Augmentation. (arXiv:2310.16867v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16867">http://arxiv.org/abs/2310.16867</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16867]] An Explainable Deep Learning-Based Method For Schizophrenia Diagnosis Using Generative Data-Augmentation(http://arxiv.org/abs/2310.16867)</code></li>
<li>Summary: <p>In this study, we leverage a deep learning-based method for the automatic
diagnosis of schizophrenia using EEG brain recordings. This approach utilizes
generative data augmentation, a powerful technique that enhances the accuracy
of the diagnosis. To enable the utilization of time-frequency features,
spectrograms were extracted from the raw signals. After exploring several
neural network architectural setups, a proper convolutional neural network
(CNN) was used for the initial diagnosis. Subsequently, using Wasserstein GAN
with Gradient Penalty (WGAN-GP) and Variational Autoencoder (VAE), two
different synthetic datasets were generated in order to augment the initial
dataset and address the over-fitting issue. The augmented dataset using VAE
achieved a 3.0\% improvement in accuracy reaching up to 99.0\% and yielded a
lower loss value as well as a faster convergence. Finally, we addressed the
lack of trust in black-box models using the Local Interpretable Model-agnostic
Explanations (LIME) algorithm to determine the most important superpixels
(frequencies) in the diagnosis process.
</p></li>
</ul>

<h3>Title: Probabilistic Integral Circuits. (arXiv:2310.16986v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16986">http://arxiv.org/abs/2310.16986</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16986]] Probabilistic Integral Circuits(http://arxiv.org/abs/2310.16986)</code></li>
<li>Summary: <p>Continuous latent variables (LVs) are a key ingredient of many generative
models, as they allow modelling expressive mixtures with an uncountable number
of components. In contrast, probabilistic circuits (PCs) are hierarchical
discrete mixtures represented as computational graphs composed of input, sum
and product units. Unlike continuous LV models, PCs provide tractable inference
but are limited to discrete LVs with categorical (i.e. unordered) states. We
bridge these model classes by introducing probabilistic integral circuits
(PICs), a new language of computational graphs that extends PCs with integral
units representing continuous LVs. In the first place, PICs are symbolic
computational graphs and are fully tractable in simple cases where analytical
integration is possible. In practice, we parameterise PICs with light-weight
neural nets delivering an intractable hierarchical continuous mixture that can
be approximated arbitrarily well with large PCs using numerical quadrature. On
several distribution estimation benchmarks, we show that such PIC-approximating
PCs systematically outperform PCs commonly learned via expectation-maximization
or SGD.
</p></li>
</ul>

<h3>Title: Learning an Inventory Control Policy with General Inventory Arrival Dynamics. (arXiv:2310.17168v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17168">http://arxiv.org/abs/2310.17168</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17168]] Learning an Inventory Control Policy with General Inventory Arrival Dynamics(http://arxiv.org/abs/2310.17168)</code></li>
<li>Summary: <p>In this paper we address the problem of learning and backtesting inventory
control policies in the presence of general arrival dynamics -- which we term
as a quantity-over-time arrivals model (QOT). We also allow for order
quantities to be modified as a post-processing step to meet vendor constraints
such as order minimum and batch size constraints -- a common practice in real
supply chains. To the best of our knowledge this is the first work to handle
either arbitrary arrival dynamics or an arbitrary downstream post-processing of
order quantities. Building upon recent work (Madeka et al., 2022) we similarly
formulate the periodic review inventory control problem as an exogenous
decision process, where most of the state is outside the control of the agent.
Madeka et al. (2022) show how to construct a simulator that replays historic
data to solve this class of problem. In our case, we incorporate a deep
generative model for the arrivals process as part of the history replay. By
formulating the problem as an exogenous decision process, we can apply results
from Madeka et al. (2022) to obtain a reduction to supervised learning.
Finally, we show via simulation studies that this approach yields statistically
significant improvements in profitability over production baselines. Using data
from an ongoing real-world A/B test, we show that Gen-QOT generalizes well to
off-policy data.
</p></li>
</ul>

<h3>Title: Adaptive important sampling for Deep Ritz. (arXiv:2310.17185v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17185">http://arxiv.org/abs/2310.17185</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17185]] Adaptive important sampling for Deep Ritz(http://arxiv.org/abs/2310.17185)</code></li>
<li>Summary: <p>We introduce an adaptive sampling method for the Deep Ritz method aimed at
solving partial differential equations (PDEs). Two deep neural networks are
used. One network is employed to approximate the solution of PDEs, while the
other one is a deep generative model used to generate new collocation points to
refine the training set. The adaptive sampling procedure consists of two main
steps. The first step is solving the PDEs using the Deep Ritz method by
minimizing an associated variational loss discretized by the collocation points
in the training set. The second step involves generating a new training set,
which is then used in subsequent computations to further improve the accuracy
of the current approximate solution. We treat the integrand in the variational
loss as an unnormalized probability density function (PDF) and approximate it
using a deep generative model called bounded KRnet. The new samples and their
associated PDF values are obtained from the bounded KRnet. With these new
samples and their associated PDF values, the variational loss can be
approximated more accurately by importance sampling. Compared to the original
Deep Ritz method, the proposed adaptive method improves accuracy, especially
for problems characterized by low regularity and high dimensionality. We
demonstrate the effectiveness of our new method through a series of numerical
experiments.
</p></li>
</ul>

<h3>Title: De-novo Chemical Reaction Generation by Means of Temporarily Convolutional Neural Networks. (arXiv:2310.17341v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17341">http://arxiv.org/abs/2310.17341</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17341]] De-novo Chemical Reaction Generation by Means of Temporarily Convolutional Neural Networks(http://arxiv.org/abs/2310.17341)</code></li>
<li>Summary: <p>We present here a combination of two networks, Recurrent Neural Networks
(RNN) and Temporarily Convolutional Neural Networks (TCN) in de novo reaction
generation using the novel Reaction Smiles-like representation of reactions
(CGRSmiles) with atom mapping directly incorporated. Recurrent Neural Networks
are known for their autoregressive properties and are frequently used in
language modelling with direct application to SMILES generation. The relatively
novel TCNs possess similar properties with wide receptive field while obeying
the causality required for natural language processing (NLP). The combination
of both latent representations expressed through TCN and RNN results in an
overall better performance compared to RNN alone. Additionally, it is shown
that different fine-tuning protocols have a profound impact on generative scope
of the model when applied on a dataset of interest via transfer learning.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: Conditionally Combining Robot Skills using Large Language Models. (arXiv:2310.17019v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17019">http://arxiv.org/abs/2310.17019</a></li>
<li>Code URL: https://github.com/krzentner/language-world</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17019]] Conditionally Combining Robot Skills using Large Language Models(http://arxiv.org/abs/2310.17019)</code></li>
<li>Summary: <p>This paper combines two contributions. First, we introduce an extension of
the Meta-World benchmark, which we call "Language-World," which allows a large
language model to operate in a simulated robotic environment using
semi-structured natural language queries and scripted skills described using
natural language. By using the same set of tasks as Meta-World, Language-World
results can be easily compared to Meta-World results, allowing for a point of
comparison between recent methods using Large Language Models (LLMs) and those
using Deep Reinforcement Learning. Second, we introduce a method we call Plan
Conditioned Behavioral Cloning (PCBC), that allows finetuning the behavior of
high-level plans using end-to-end demonstrations. Using Language-World, we show
that PCBC is able to achieve strong performance in a variety of few-shot
regimes, often achieving task generalization with as little as a single
demonstration. We have made Language-World available as open-source software at
https://github.com/krzentner/language-world/.
</p></li>
</ul>

<h3>Title: BOOST: Harnessing Black-Box Control to Boost Commonsense in LMs' Generation. (arXiv:2310.17054v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17054">http://arxiv.org/abs/2310.17054</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17054]] BOOST: Harnessing Black-Box Control to Boost Commonsense in LMs' Generation(http://arxiv.org/abs/2310.17054)</code></li>
<li>Summary: <p>Large language models (LLMs) such as GPT-3 have demonstrated a strong
capability to generate coherent and contextually relevant text. However, amidst
their successes, a crucial issue persists: their generated outputs still lack
commonsense at times. Moreover, fine-tuning the entire LLM towards more
commonsensical outputs is computationally expensive if not infeasible. In this
paper, we present a computation-efficient framework that steers a frozen
Pre-Trained Language Model (PTLM) towards more commonsensical generation (i.e.,
producing a plausible output that incorporates a list of concepts in a
meaningful way). Specifically, we first construct a reference-free evaluator
that assigns a sentence with a commonsensical score by grounding the sentence
to a dynamic commonsense knowledge base from four different relational aspects.
We then use the scorer as the oracle for commonsense knowledge, and extend the
controllable generation method called NADO to train an auxiliary head that
guides a fixed PTLM to better satisfy the oracle. We test our framework on a
series of GPT-2-, Flan-T5-, and Alpaca-based language models (LMs) on two
constrained concept-to-sentence benchmarks. Human evaluation results
demonstrate that our method consistently leads to the most commonsensical
outputs.
</p></li>
</ul>

<h3>Title: M2C: Towards Automatic Multimodal Manga Complement. (arXiv:2310.17130v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17130">http://arxiv.org/abs/2310.17130</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17130]] M2C: Towards Automatic Multimodal Manga Complement(http://arxiv.org/abs/2310.17130)</code></li>
<li>Summary: <p>Multimodal manga analysis focuses on enhancing manga understanding with
visual and textual features, which has attracted considerable attention from
both natural language processing and computer vision communities. Currently,
most comics are hand-drawn and prone to problems such as missing pages, text
contamination, and aging, resulting in missing comic text content and seriously
hindering human comprehension. In other words, the Multimodal Manga Complement
(M2C) task has not been investigated, which aims to handle the aforementioned
issues by providing a shared semantic space for vision and language
understanding. To this end, we first propose the Multimodal Manga Complement
task by establishing a new M2C benchmark dataset covering two languages. First,
we design a manga argumentation method called MCoT to mine event knowledge in
comics with large language models. Then, an effective baseline FVP-M$^{2}$
using fine-grained visual prompts is proposed to support manga complement.
Extensive experimental results show the effectiveness of FVP-M$^{2}$ method for
Multimodal Mange Complement.
</p></li>
</ul>

<h3>Title: Symbolic Planning and Code Generation for Grounded Dialogue. (arXiv:2310.17140v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17140">http://arxiv.org/abs/2310.17140</a></li>
<li>Code URL: https://github.com/justinchiu/onecommon-gpt</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17140]] Symbolic Planning and Code Generation for Grounded Dialogue(http://arxiv.org/abs/2310.17140)</code></li>
<li>Summary: <p>Large language models (LLMs) excel at processing and generating both text and
code. However, LLMs have had limited applicability in grounded task-oriented
dialogue as they are difficult to steer toward task objectives and fail to
handle novel grounding. We present a modular and interpretable grounded
dialogue system that addresses these shortcomings by composing LLMs with a
symbolic planner and grounded code execution. Our system consists of a reader
and planner: the reader leverages an LLM to convert partner utterances into
executable code, calling functions that perform grounding. The translated
code's output is stored to track dialogue state, while a symbolic planner
determines the next appropriate response. We evaluate our system's performance
on the demanding OneCommon dialogue task, involving collaborative reference
resolution on abstract images of scattered dots. Our system substantially
outperforms the previous state-of-the-art, including improving task success in
human evaluations from 56% to 69% in the most challenging setting.
</p></li>
</ul>

<h3>Title: ACT-SQL: In-Context Learning for Text-to-SQL with Automatically-Generated Chain-of-Thought. (arXiv:2310.17342v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17342">http://arxiv.org/abs/2310.17342</a></li>
<li>Code URL: https://github.com/x-lance/text2sql-gpt</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17342]] ACT-SQL: In-Context Learning for Text-to-SQL with Automatically-Generated Chain-of-Thought(http://arxiv.org/abs/2310.17342)</code></li>
<li>Summary: <p>Recently Large Language Models (LLMs) have been proven to have strong
abilities in various domains and tasks. We study the problem of prompt
designing in the text-to-SQL task and attempt to improve the LLMs' reasoning
ability when generating SQL queries. Besides the trivial few-shot in-context
learning setting, we design our chain-of-thought (CoT) prompt with a similar
method to schema linking. We provide a method named ACT-SQL to automatically
generate auto-CoT exemplars and thus the whole process doesn't need manual
labeling. Our approach is cost-saving since we only use the LLMs' API call once
when generating one SQL query. Furthermore, we extend our in-context learning
method to the multi-turn text-to-SQL task. The experiment results show that the
LLMs' performance can benefit from our ACT-SQL approach. Our approach achieves
SOTA performance on the Spider dev set among existing in-context learning
approaches.
</p></li>
</ul>

<h3>Title: Cultural Adaptation of Recipes. (arXiv:2310.17353v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17353">http://arxiv.org/abs/2310.17353</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17353]] Cultural Adaptation of Recipes(http://arxiv.org/abs/2310.17353)</code></li>
<li>Summary: <p>Building upon the considerable advances in Large Language Models (LLMs), we
are now equipped to address more sophisticated tasks demanding a nuanced
understanding of cross-cultural contexts. A key example is recipe adaptation,
which goes beyond simple translation to include a grasp of ingredients,
culinary techniques, and dietary preferences specific to a given culture. We
introduce a new task involving the translation and cultural adaptation of
recipes between Chinese and English-speaking cuisines. To support this
investigation, we present CulturalRecipes, a unique dataset comprised of
automatically paired recipes written in Mandarin Chinese and English. This
dataset is further enriched with a human-written and curated test set. In this
intricate task of cross-cultural recipe adaptation, we evaluate the performance
of various methods, including GPT-4 and other LLMs, traditional machine
translation, and information retrieval techniques. Our comprehensive analysis
includes both automatic and human evaluation metrics. While GPT-4 exhibits
impressive abilities in adapting Chinese recipes into English, it still lags
behind human expertise when translating English recipes into Chinese. This
underscores the multifaceted nature of cultural adaptations. We anticipate that
these insights will significantly contribute to future research on
culturally-aware language models and their practical application in culturally
diverse contexts.
</p></li>
</ul>

<h3>Title: ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in Real-World User-AI Conversation. (arXiv:2310.17389v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17389">http://arxiv.org/abs/2310.17389</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17389]] ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in Real-World User-AI Conversation(http://arxiv.org/abs/2310.17389)</code></li>
<li>Summary: <p>Despite remarkable advances that large language models have achieved in
chatbots, maintaining a non-toxic user-AI interactive environment has become
increasingly critical nowadays. However, previous efforts in toxicity detection
have been mostly based on benchmarks derived from social media content, leaving
the unique challenges inherent to real-world user-AI interactions
insufficiently explored. In this work, we introduce ToxicChat, a novel
benchmark based on real user queries from an open-source chatbot. This
benchmark contains the rich, nuanced phenomena that can be tricky for current
toxicity detection models to identify, revealing a significant domain
difference compared to social media content. Our systematic evaluation of
models trained on existing toxicity datasets has shown their shortcomings when
applied to this unique domain of ToxicChat. Our work illuminates the
potentially overlooked challenges of toxicity detection in real-world user-AI
conversations. In the future, ToxicChat can be a valuable resource to drive
further advancements toward building a safe and healthy environment for user-AI
interactions.
</p></li>
</ul>

<h3>Title: Meaning and understanding in large language models. (arXiv:2310.17407v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17407">http://arxiv.org/abs/2310.17407</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17407]] Meaning and understanding in large language models(http://arxiv.org/abs/2310.17407)</code></li>
<li>Summary: <p>Can a machine understand the meanings of natural language? Recent
developments in the generative large language models (LLMs) of artificial
intelligence have led to the belief that traditional philosophical assumptions
about machine understanding of language need to be revised. This article
critically evaluates the prevailing tendency to regard machine language
performance as mere syntactic manipulation and the simulation of understanding,
which is only partial and very shallow, without sufficient referential
grounding in the world. The aim is to highlight the conditions crucial to
attributing natural language understanding to state-of-the-art LLMs, where it
can be legitimately argued that LLMs not only use syntax but also semantics,
their understanding not being simulated but duplicated; and determine how they
ground the meanings of linguistic expressions.
</p></li>
</ul>

<h3>Title: Improving Zero-shot Reader by Reducing Distractions from Irrelevant Documents in Open-Domain Question Answering. (arXiv:2310.17490v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17490">http://arxiv.org/abs/2310.17490</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17490]] Improving Zero-shot Reader by Reducing Distractions from Irrelevant Documents in Open-Domain Question Answering(http://arxiv.org/abs/2310.17490)</code></li>
<li>Summary: <p>Large language models (LLMs) enable zero-shot approaches in open-domain
question answering (ODQA), yet with limited advancements as the reader is
compared to the retriever. This study aims at the feasibility of a zero-shot
reader that addresses the challenges of computational cost and the need for
labeled data. We find that LLMs are distracted due to irrelevant documents in
the retrieved set and the overconfidence of the generated answers when they are
exploited as zero-shot readers. To tackle these problems, we mitigate the
impact of such documents via Distraction-aware Answer Selection (DAS) with a
negation-based instruction and score adjustment for proper answer selection.
Experimental results show that our approach successfully handles distraction
across diverse scenarios, enhancing the performance of zero-shot readers.
Furthermore, unlike supervised readers struggling with unseen data, zero-shot
readers demonstrate outstanding transferability without any training.
</p></li>
</ul>

<h3>Title: Can large language models replace humans in the systematic review process? Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages. (arXiv:2310.17526v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17526">http://arxiv.org/abs/2310.17526</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17526]] Can large language models replace humans in the systematic review process? Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages(http://arxiv.org/abs/2310.17526)</code></li>
<li>Summary: <p>Systematic reviews are vital for guiding practice, research, and policy, yet
they are often slow and labour-intensive. Large language models (LLMs) could
offer a way to speed up and automate systematic reviews, but their performance
in such tasks has not been comprehensively evaluated against humans, and no
study has tested GPT-4, the biggest LLM so far. This pre-registered study
evaluates GPT-4's capability in title/abstract screening, full-text review, and
data extraction across various literature types and languages using a
'human-out-of-the-loop' approach. Although GPT-4 had accuracy on par with human
performance in most tasks, results were skewed by chance agreement and dataset
imbalance. After adjusting for these, there was a moderate level of performance
for data extraction, and - barring studies that used highly reliable prompts -
screening performance levelled at none to moderate for different stages and
languages. When screening full-text literature using highly reliable prompts,
GPT-4's performance was 'almost perfect.' Penalising GPT-4 for missing key
studies using highly reliable prompts improved its performance even more. Our
findings indicate that, currently, substantial caution should be used if LLMs
are being used to conduct systematic reviews, but suggest that, for certain
systematic review tasks delivered under reliable prompts, LLMs can rival human
performance.
</p></li>
</ul>

<h3>Title: Improving Few-shot Generalization of Safety Classifiers via Data Augmented Parameter-Efficient Fine-Tuning. (arXiv:2310.16959v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16959">http://arxiv.org/abs/2310.16959</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16959]] Improving Few-shot Generalization of Safety Classifiers via Data Augmented Parameter-Efficient Fine-Tuning(http://arxiv.org/abs/2310.16959)</code></li>
<li>Summary: <p>As large language models (LLMs) are widely adopted, new safety issues and
policies emerge, to which existing safety classifiers do not generalize well.
If we have only observed a few examples of violations of a new safety rule, how
can we build a classifier to detect violations? In this paper, we study the
novel setting of domain-generalized few-shot learning for LLM-based text safety
classifiers. Unlike prior few-shot work, these new safety issues can be hard to
uncover and we do not get to choose the few examples. We demonstrate that
existing few-shot techniques do not perform well in this setting, and rather we
propose to do parameter-efficient fine-tuning (PEFT) combined with augmenting
training data based on similar examples in prior existing rules. We empirically
show that our approach of similarity-based data-augmentation + prompt-tuning
(DAPT) consistently outperforms baselines that either do not rely on data
augmentation or on PEFT by 7-17% F1 score in the Social Chemistry moral
judgement and 9-13% AUC in the Toxicity detection tasks, even when the new rule
is loosely correlated with existing ones.
</p></li>
</ul>

<h3>Title: LLM4DyG: Can Large Language Models Solve Problems on Dynamic Graphs?. (arXiv:2310.17110v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17110">http://arxiv.org/abs/2310.17110</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17110]] LLM4DyG: Can Large Language Models Solve Problems on Dynamic Graphs?(http://arxiv.org/abs/2310.17110)</code></li>
<li>Summary: <p>In an era marked by the increasing adoption of Large Language Models (LLMs)
for various tasks, there is a growing focus on exploring LLMs' capabilities in
handling web data, particularly graph data. Dynamic graphs, which capture
temporal network evolution patterns, are ubiquitous in real-world web data.
Evaluating LLMs' competence in understanding spatial-temporal information on
dynamic graphs is essential for their adoption in web applications, which
remains unexplored in the literature. In this paper, we bridge the gap via
proposing to evaluate LLMs' spatial-temporal understanding abilities on dynamic
graphs, to the best of our knowledge, for the first time. Specifically, we
propose the LLM4DyG benchmark, which includes nine specially designed tasks
considering the capability evaluation of LLMs from both temporal and spatial
dimensions. Then, we conduct extensive experiments to analyze the impacts of
different data generators, data statistics, prompting techniques, and LLMs on
the model performance. Finally, we propose Disentangled Spatial-Temporal
Thoughts (DST2) for LLMs on dynamic graphs to enhance LLMs' spatial-temporal
understanding abilities. Our main observations are: 1) LLMs have preliminary
spatial-temporal understanding abilities on dynamic graphs, 2) Dynamic graph
tasks show increasing difficulties for LLMs as the graph size and density
increase, while not sensitive to the time span and data generation mechanism,
3) the proposed DST2 prompting method can help to improve LLMs'
spatial-temporal understanding abilities on dynamic graphs for most tasks. The
data and codes will be open-sourced at publication time.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: 4D-Editor: Interactive Object-level Editing in Dynamic Neural Radiance Fields via 4D Semantic Segmentation. (arXiv:2310.16858v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16858">http://arxiv.org/abs/2310.16858</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16858]] 4D-Editor: Interactive Object-level Editing in Dynamic Neural Radiance Fields via 4D Semantic Segmentation(http://arxiv.org/abs/2310.16858)</code></li>
<li>Summary: <p>This paper targets interactive object-level editing(e.g., deletion,
recoloring, transformation, composition) in dynamic scenes. Recently, some
methods aiming for flexible editing static scenes represented by neural
radiance field (NeRF) have shown impressive synthesis quality, while similar
capabilities in time-variant dynamic scenes remain limited. To solve this
problem, we propose 4D-Editor, an interactive semantic-driven editing
framework, allowing editing multiple objects in dynamic NeRF based on user
strokes on a single frame. Our dynamic scene representation is built upon
hybrid semantic feature fields so that the spatial-temporal consistency can be
maintained after editing. In addition, we design recursive selection refinement
that significantly boosts segmentation accuracy in a dynamic NeRF to aid the
editing process. Moreover, we develop multi-view reprojection inpainting to
fill holes caused by incomplete scene capture after editing. Extensive
experiments and editing examples on real-world demonstrate that 4D-Editor
achieves photo-realistic dynamic NeRF editing. Project page:
https://patrickddj.github.io/4D-Editor
</p></li>
</ul>

<h3>Title: Unsupervised Domain Adaptation for Semantic Segmentation with Pseudo Label Self-Refinement. (arXiv:2310.16979v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16979">http://arxiv.org/abs/2310.16979</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16979]] Unsupervised Domain Adaptation for Semantic Segmentation with Pseudo Label Self-Refinement(http://arxiv.org/abs/2310.16979)</code></li>
<li>Summary: <p>Deep learning-based solutions for semantic segmentation suffer from
significant performance degradation when tested on data with different
characteristics than what was used during the training. Adapting the models
using annotated data from the new domain is not always practical. Unsupervised
Domain Adaptation (UDA) approaches are crucial in deploying these models in the
actual operating conditions. Recent state-of-the-art (SOTA) UDA methods employ
a teacher-student self-training approach, where a teacher model is used to
generate pseudo-labels for the new data which in turn guide the training
process of the student model. Though this approach has seen a lot of success,
it suffers from the issue of noisy pseudo-labels being propagated in the
training process. To address this issue, we propose an auxiliary pseudo-label
refinement network (PRN) for online refining of the pseudo labels and also
localizing the pixels whose predicted labels are likely to be noisy. Being able
to improve the quality of pseudo labels and select highly reliable ones, PRN
helps self-training of segmentation models to be robust against pseudo label
noise propagation during different stages of adaptation. We evaluate our
approach on benchmark datasets with three different domain shifts, and our
approach consistently performs significantly better than the previous
state-of-the-art methods.
</p></li>
</ul>

<h3>Title: Automating lichen monitoring in ecological studies using instance segmentation of time-lapse images. (arXiv:2310.17080v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17080">http://arxiv.org/abs/2310.17080</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17080]] Automating lichen monitoring in ecological studies using instance segmentation of time-lapse images(http://arxiv.org/abs/2310.17080)</code></li>
<li>Summary: <p>Lichens are symbiotic organisms composed of fungi, algae, and/or
cyanobacteria that thrive in a variety of environments. They play important
roles in carbon and nitrogen cycling, and contribute directly and indirectly to
biodiversity. Ecologists typically monitor lichens by using them as indicators
to assess air quality and habitat conditions. In particular, epiphytic lichens,
which live on trees, are key markers of air quality and environmental health. A
new method of monitoring epiphytic lichens involves using time-lapse cameras to
gather images of lichen populations. These cameras are used by ecologists in
Newfoundland and Labrador to subsequently analyze and manually segment the
images to determine lichen thalli condition and change. These methods are
time-consuming and susceptible to observer bias. In this work, we aim to
automate the monitoring of lichens over extended periods and to estimate their
biomass and condition to facilitate the task of ecologists. To accomplish this,
our proposed framework uses semantic segmentation with an effective training
approach to automate monitoring and biomass estimation of epiphytic lichens on
time-lapse images. We show that our method has the potential to significantly
improve the accuracy and efficiency of lichen population monitoring, making it
a valuable tool for forest ecologists and environmental scientists to evaluate
the impact of climate change on Canada's forests. To the best of our knowledge,
this is the first time that such an approach has been used to assist ecologists
in monitoring and analyzing epiphytic lichens.
</p></li>
</ul>

<h3>Title: Task-driven Prompt Evolution for Foundation Models. (arXiv:2310.17128v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17128">http://arxiv.org/abs/2310.17128</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17128]] Task-driven Prompt Evolution for Foundation Models(http://arxiv.org/abs/2310.17128)</code></li>
<li>Summary: <p>Promptable foundation models, particularly Segment Anything Model (SAM), have
emerged as a promising alternative to the traditional task-specific supervised
learning for image segmentation. However, many evaluation studies have found
that their performance on medical imaging modalities to be underwhelming
compared to conventional deep learning methods. In the world of large
pre-trained language and vision-language models, learning prompt from
downstream tasks has achieved considerable success in improving performance. In
this work, we propose a plug-and-play Prompt Optimization Technique for
foundation models like SAM (SAMPOT) that utilizes the downstream segmentation
task to optimize the human-provided prompt to obtain improved performance. We
demonstrate the utility of SAMPOT on lung segmentation in chest X-ray images
and obtain an improvement on a significant number of cases ($\sim75\%$) over
human-provided initial prompts. We hope this work will lead to further
investigations in the nascent field of automatic visual prompt-tuning.
</p></li>
</ul>

<h3>Title: Virtual Accessory Try-On via Keypoint Hallucination. (arXiv:2310.17131v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17131">http://arxiv.org/abs/2310.17131</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17131]] Virtual Accessory Try-On via Keypoint Hallucination(http://arxiv.org/abs/2310.17131)</code></li>
<li>Summary: <p>The virtual try-on task refers to fitting the clothes from one image onto
another portrait image. In this paper, we focus on virtual accessory try-on,
which fits accessory (e.g., glasses, ties) onto a face or portrait image.
Unlike clothing try-on, which relies on human silhouette as guidance, accessory
try-on warps the accessory into an appropriate location and shape to generate a
plausible composite image. In contrast to previous try-on methods that treat
foreground (i.e., accessories) and background (i.e., human faces or bodies)
equally, we propose a background-oriented network to utilize the prior
knowledge of human bodies and accessories. Specifically, our approach learns
the human body priors and hallucinates the target locations of specified
foreground keypoints in the background. Then our approach will inject
foreground information with accessory priors into the background UNet. Based on
the hallucinated target locations, the warping parameters are calculated to
warp the foreground. Moreover, this background-oriented network can also easily
incorporate auxiliary human face/body semantic segmentation supervision to
further boost performance. Experiments conducted on STRAT dataset validate the
effectiveness of our proposed method.
</p></li>
</ul>

<h3>Title: Comparison of Cross-Entropy, Dice, and Focal Loss for Sea Ice Type Segmentation. (arXiv:2310.17135v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17135">http://arxiv.org/abs/2310.17135</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17135]] Comparison of Cross-Entropy, Dice, and Focal Loss for Sea Ice Type Segmentation(http://arxiv.org/abs/2310.17135)</code></li>
<li>Summary: <p>Up-to-date sea ice charts are crucial for safer navigation in ice-infested
waters. Recently, Convolutional Neural Network (CNN) models show the potential
to accelerate the generation of ice maps for large regions. However, results
from CNN models still need to undergo scrutiny as higher metrics performance
not always translate to adequate outputs. Sea ice type classes are imbalanced,
requiring special treatment during training. We evaluate how three different
loss functions, some developed for imbalanced class problems, affect the
performance of CNN models trained to predict the dominant ice type in
Sentinel-1 images. Despite the fact that Dice and Focal loss produce higher
metrics, results from cross-entropy seem generally more physically consistent.
</p></li>
</ul>

<h3>Title: Technical Note: Feasibility of translating 3.0T-trained Deep-Learning Segmentation Models Out-of-the-Box on Low-Field MRI 0.55T Knee-MRI of Healthy Controls. (arXiv:2310.17152v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17152">http://arxiv.org/abs/2310.17152</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17152]] Technical Note: Feasibility of translating 3(http://arxiv.org/abs/2310.17152)</code></li>
<li>Summary: <p>In the current study, our purpose is to evaluate the feasibility of applying
deep learning (DL) enabled algorithms to quantify bilateral knee biomarkers in
healthy controls scanned at 0.55T and compared with 3.0T. The current study
assesses the performance of standard in-practice bone, and cartilage
segmentation algorithms at 0.55T, both qualitatively and quantitatively, in
terms of comparing segmentation performance, areas of improvement, and
compartment-wise cartilage thickness values between 0.55T vs. 3.0T. Initial
results demonstrate a usable to good technical feasibility of translating
existing quantitative deep-learning-based image segmentation techniques,
trained on 3.0T, out of 0.55T for knee MRI, in a multi-vendor acquisition
environment. Especially in terms of segmenting cartilage compartments, the
models perform almost equivalent to 3.0T in terms of Likert ranking. The 0.55T
low-field sustainable and easy-to-install MRI, as demonstrated, thus, can be
utilized for evaluating knee cartilage thickness and bone segmentations aided
by established DL algorithms trained at higher-field strengths out-of-the-box
initially. This could be utilized at the far-spread point-of-care locations
with a lack of radiologists available to manually segment low-field images, at
least till a decent base of low-field data pool is collated. With further
fine-tuning with manual labeling of low-field data or utilizing synthesized
higher SNR images from low-field images, OA biomarker quantification
performance is potentially guaranteed to be further improved.
</p></li>
</ul>

<h3>Title: A Deep Learning Approach to Teeth Segmentation and Orientation from Panoramic X-rays. (arXiv:2310.17176v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17176">http://arxiv.org/abs/2310.17176</a></li>
<li>Code URL: https://github.com/mrinal054/instance_teeth_segmentation</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17176]] A Deep Learning Approach to Teeth Segmentation and Orientation from Panoramic X-rays(http://arxiv.org/abs/2310.17176)</code></li>
<li>Summary: <p>Accurate teeth segmentation and orientation are fundamental in modern oral
healthcare, enabling precise diagnosis, treatment planning, and dental implant
design. In this study, we present a comprehensive approach to teeth
segmentation and orientation from panoramic X-ray images, leveraging deep
learning techniques. We build our model based on FUSegNet, a popular model
originally developed for wound segmentation, and introduce modifications by
incorporating grid-based attention gates into the skip connections. We
introduce oriented bounding box (OBB) generation through principal component
analysis (PCA) for precise tooth orientation estimation. Evaluating our
approach on the publicly available DNS dataset, comprising 543 panoramic X-ray
images, we achieve the highest Intersection-over-Union (IoU) score of 82.43%
and Dice Similarity Coefficient (DSC) score of 90.37% among compared models in
teeth instance segmentation. In OBB analysis, we obtain the Rotated IoU (RIoU)
score of 82.82%. We also conduct detailed analyses of individual tooth labels
and categorical performance, shedding light on strengths and weaknesses. The
proposed model's accuracy and versatility offer promising prospects for
improving dental diagnoses, treatment planning, and personalized healthcare in
the oral domain. Our generated OBB coordinates and codes are available at
https://github.com/mrinal054/Instance_teeth_segmentation.
</p></li>
</ul>

<h3>Title: Weakly-Supervised Surgical Phase Recognition. (arXiv:2310.17209v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17209">http://arxiv.org/abs/2310.17209</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17209]] Weakly-Supervised Surgical Phase Recognition(http://arxiv.org/abs/2310.17209)</code></li>
<li>Summary: <p>A key element of computer-assisted surgery systems is phase recognition of
surgical videos. Existing phase recognition algorithms require frame-wise
annotation of a large number of videos, which is time and money consuming. In
this work we join concepts of graph segmentation with self-supervised learning
to derive a random-walk solution for per-frame phase prediction. Furthermore,
we utilize within our method two forms of weak supervision: sparse timestamps
or few-shot learning. The proposed algorithm enjoys low complexity and can
operate in lowdata regimes. We validate our method by running experiments with
the public Cholec80 dataset of laparoscopic cholecystectomy videos,
demonstrating promising performance in multiple setups.
</p></li>
</ul>

<h3>Title: BEVContrast: Self-Supervision in BEV Space for Automotive Lidar Point Clouds. (arXiv:2310.17281v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17281">http://arxiv.org/abs/2310.17281</a></li>
<li>Code URL: https://github.com/valeoai/bevcontrast</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17281]] BEVContrast: Self-Supervision in BEV Space for Automotive Lidar Point Clouds(http://arxiv.org/abs/2310.17281)</code></li>
<li>Summary: <p>We present a surprisingly simple and efficient method for self-supervision of
3D backbone on automotive Lidar point clouds. We design a contrastive loss
between features of Lidar scans captured in the same scene. Several such
approaches have been proposed in the literature from PointConstrast, which uses
a contrast at the level of points, to the state-of-the-art TARL, which uses a
contrast at the level of segments, roughly corresponding to objects. While the
former enjoys a great simplicity of implementation, it is surpassed by the
latter, which however requires a costly pre-processing. In BEVContrast, we
define our contrast at the level of 2D cells in the Bird's Eye View plane.
Resulting cell-level representations offer a good trade-off between the
point-level representations exploited in PointContrast and segment-level
representations exploited in TARL: we retain the simplicity of PointContrast
(cell representations are cheap to compute) while surpassing the performance of
TARL in downstream semantic segmentation.
</p></li>
</ul>

<h3>Title: Revisiting the Distillation of Image Representations into Point Clouds for Autonomous Driving. (arXiv:2310.17504v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17504">http://arxiv.org/abs/2310.17504</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17504]] Revisiting the Distillation of Image Representations into Point Clouds for Autonomous Driving(http://arxiv.org/abs/2310.17504)</code></li>
<li>Summary: <p>Self-supervised image networks can be used to address complex 2D tasks (e.g.,
semantic segmentation, object discovery) very efficiently and with little or no
downstream supervision. However, self-supervised 3D networks on lidar data do
not perform as well for now. A few methods therefore propose to distill
high-quality self-supervised 2D features into 3D networks. The most recent ones
doing so on autonomous driving data show promising results. Yet, a performance
gap persists between these distilled features and fully-supervised ones. In
this work, we revisit 2D-to-3D distillation. First, we propose, for semantic
segmentation, a simple approach that leads to a significant improvement
compared to prior 3D distillation methods. Second, we show that distillation in
high capacity 3D networks is key to reach high quality 3D features. This
actually allows us to significantly close the gap between unsupervised
distilled 3D features and fully-supervised ones. Last, we show that our
high-quality distilled representations can also be used for open-vocabulary
segmentation and background/foreground discovery.
</p></li>
</ul>

<h3>Title: Topic Segmentation of Semi-Structured and Unstructured Conversational Datasets using Language Models. (arXiv:2310.17120v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.17120">http://arxiv.org/abs/2310.17120</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.17120]] Topic Segmentation of Semi-Structured and Unstructured Conversational Datasets using Language Models(http://arxiv.org/abs/2310.17120)</code></li>
<li>Summary: <p>Breaking down a document or a conversation into multiple contiguous segments
based on its semantic structure is an important and challenging problem in NLP,
which can assist many downstream tasks. However, current works on topic
segmentation often focus on segmentation of structured texts. In this paper, we
comprehensively analyze the generalization capabilities of state-of-the-art
topic segmentation models on unstructured texts. We find that: (a) Current
strategies of pre-training on a large corpus of structured text such as
Wiki-727K do not help in transferability to unstructured conversational data.
(b) Training from scratch with only a relatively small-sized dataset of the
target unstructured domain improves the segmentation results by a significant
margin. We stress-test our proposed Topic Segmentation approach by
experimenting with multiple loss functions, in order to mitigate effects of
imbalance in unstructured conversational datasets. Our empirical evaluation
indicates that Focal Loss function is a robust alternative to Cross-Entropy and
re-weighted Cross-Entropy loss function when segmenting unstructured and
semi-structured chats.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
