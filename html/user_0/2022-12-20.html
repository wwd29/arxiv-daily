<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: Study on Domain Name System (DNS) Abuse: Technical Report. (arXiv:2212.08879v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.08879">http://arxiv.org/abs/2212.08879</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.08879] Study on Domain Name System (DNS) Abuse: Technical Report](http://arxiv.org/abs/2212.08879) #secure</code></li>
<li>Summary: <p>A safe and secure Domain Name System (DNS) is of paramount importance for the
digital economy and society. Malicious activities on the DNS, generally
referred to as "DNS abuse" are frequent and severe problems affecting online
security and undermining users' trust in the Internet. The proposed definition
of DNS abuse is as follows: Domain Name System (DNS) abuse is any activity that
makes use of domain names or the DNS protocol to carry out harmful or illegal
activity. DNS abuse exploits the domain name registration process, the domain
name resolution process, or other services associated with the domain name
(e.g., shared web hosting service). Notably, we distinguish between:
maliciously registered domain names: domain name registered with the malicious
intent to carry out harmful or illegal activity compromised domain names:
domain name registered by bona fide third-party for legitimate purposes,
compromised by malicious actors to carry out harmful and illegal activity. DNS
abuse disrupts, damages, or otherwise adversely impacts the DNS and the
Internet infrastructure, their users or other persons.
</p></li>
</ul>

<h3>Title: PlexiChain: A Secure Blockchain-based Flexibility Aggregator Framework. (arXiv:2212.09064v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.09064">http://arxiv.org/abs/2212.09064</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.09064] PlexiChain: A Secure Blockchain-based Flexibility Aggregator Framework](http://arxiv.org/abs/2212.09064) #secure</code></li>
<li>Summary: <p>Flexible resources in built environments are seen as a low-cost opportunity
for delivering grid management services. Consequently, the centralised
aggregator model, where the aggregator is used to bundle demand flexibility
from flexible resources and deliver it to flexibility customers such as
Distributed/Transmission System Operator (DSO/TSO) in flexibility markets, has
been adopted. However, the aggregator role introduces various security and
trust challenges. In this work, we propose a blockchain-based flexibility
trading framework dubbed PlexiChain to address the security and trust
challenges the aggregator poses in the centralised aggregator model. The
security evaluations performed using a real-world dataset show that PlexiChain
is robust against known security attacks, such as MadIoT and False Data
Injection attacks. Additionally, the performance evaluations show that
PlexiChain has lower computation and communication costs than other
blockchain-based applications in resource-constrained environments.
</p></li>
</ul>

<h3>Title: Blockchain Interoperability Landscape. (arXiv:2212.09227v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.09227">http://arxiv.org/abs/2212.09227</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.09227] Blockchain Interoperability Landscape](http://arxiv.org/abs/2212.09227) #secure</code></li>
<li>Summary: <p>Blockchain has become a popular emergent technology in many industries. It is
suitable for a broad range of applications, from its base role as an immutable
distributed ledger to the deployment of distributed applications. Many
organizations are adopting the technology, but choosing a specific blockchain
implementation in an emerging field exposes them to significant technology
risk. Selecting the wrong implementation could expose an organization to
security vulnerabilities, reduce access to its target audience, or cause issues
in the future when switching to a more mature protocol. Blockchain
interoperability aims to solve this adaptability problem by increasing the
extensibility of blockchain, enabling the addition of new use cases and
features without sacrificing the performance of the original blockchain.
However, most existing blockchain platforms need to be designed for
interoperability, and simple operations like sending assets across platforms
create problems. Cryptographic protocols that are secure in isolation may
become insecure when several different (individually secure) protocols are
composed. Similarly, utilizing trusted custodians may undercut most of the
benefits of decentralization offered by blockchain-based systems. Even though
there is some research and development in the field of blockchain
interoperability, a characterization of the interoperability solutions for
various infrastructure options is lacking. This paper presents a methodology
for characterizing blockchain interoperability solutions that will help focus
on new developments and evaluate existing and future solutions in this space.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: AI Security for Geoscience and Remote Sensing: Challenges and Future Trends. (arXiv:2212.09360v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.09360">http://arxiv.org/abs/2212.09360</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.09360] AI Security for Geoscience and Remote Sensing: Challenges and Future Trends](http://arxiv.org/abs/2212.09360) #security</code></li>
<li>Summary: <p>Recent advances in artificial intelligence (AI) have significantly
intensified research in the geoscience and remote sensing (RS) field. AI
algorithms, especially deep learning-based ones, have been developed and
applied widely to RS data analysis. The successful application of AI covers
almost all aspects of Earth observation (EO) missions, from low-level vision
tasks like super-resolution, denoising, and inpainting, to high-level vision
tasks like scene classification, object detection, and semantic segmentation.
While AI techniques enable researchers to observe and understand the Earth more
accurately, the vulnerability and uncertainty of AI models deserve further
attention, considering that many geoscience and RS tasks are highly
safety-critical. This paper reviews the current development of AI security in
the geoscience and RS field, covering the following five important aspects:
adversarial attack, backdoor attack, federated learning, uncertainty, and
explainability. Moreover, the potential opportunities and trends are discussed
to provide insights for future research. To the best of the authors' knowledge,
this paper is the first attempt to provide a systematic review of AI
security-related research in the geoscience and RS community. Available code
and datasets are also listed in the paper to move this vibrant field of
research forward.
</p></li>
</ul>

<h3>Title: Rainproof: An Umbrella To Shield Text Generators From Out-Of-Distribution Data. (arXiv:2212.09171v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.09171">http://arxiv.org/abs/2212.09171</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.09171] Rainproof: An Umbrella To Shield Text Generators From Out-Of-Distribution Data](http://arxiv.org/abs/2212.09171) #security</code></li>
<li>Summary: <p>As more and more conversational and translation systems are deployed in
production, it is essential to implement and to develop effective control
mechanisms guaranteeing their proper functioning and security. An essential
component to ensure safe system behavior is out-of-distribution (OOD)
detection, which aims at detecting whether an input sample is statistically far
from the training distribution. Although OOD detection is a widely covered
topic in classification tasks, it has received much less attention in text
generation. This paper addresses the problem of OOD detection for machine
translation and dialog generation from an operational perspective. Our
contributions include: (i) RAINPROOF a Relative informAItioN Projection ODD
detection framework; and (ii) a more operational evaluation setting for OOD
detection. Surprisingly, we find that OOD detection is not necessarily aligned
with task-specific measures. The OOD detector may filter out samples that are
well processed by the model and keep samples that are not, leading to weaker
performance. Our results show that RAINPROOF breaks this curse and achieve good
results in OOD detection while increasing performance.
</p></li>
</ul>

<h3>Title: A systematic literature review on Internet of Vehicles Security. (arXiv:2212.08754v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.08754">http://arxiv.org/abs/2212.08754</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.08754] A systematic literature review on Internet of Vehicles Security](http://arxiv.org/abs/2212.08754) #security</code></li>
<li>Summary: <p>The Internet of Vehicles IoV commonly referred to as connected automobiles is
a vast network that connects various entities including users sensors and
vehicles They will connect across a network to lessen traffic accidents and
improve both the security and safety of smart vehicles The Internet of Vehicles
is subject to a wide variety of threats including spoofing attacks recognition
attacks privacy attacks and verification attacks Our the primary concern when
creating any new smart gadget is the users safety which will be improved by
identifying solutions to the various cyber threats Therefore we will cover the
security of smart automobiles in this literature review including their attacks
and solutions.
</p></li>
</ul>

<h3>Title: A Survey on Password Guessing. (arXiv:2212.08796v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.08796">http://arxiv.org/abs/2212.08796</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.08796] A Survey on Password Guessing](http://arxiv.org/abs/2212.08796) #security</code></li>
<li>Summary: <p>Text password has served as the most popular method for user authentication
so far, and is not likely to be totally replaced in foreseeable future.
Password authentication offers several desirable properties (e.g., low-cost,
highly available, easy-to-implement, reusable). However, it suffers from a
critical security issue mainly caused by the inability to memorize complicated
strings of human. Users tend to choose easy-to-remember passwords which are not
uniformly distributed in the key space, and are susceptible to guessing attack.
In order to encourage and support users to use strong passwords, it is
necessary to simulate automate password guessing methods to determine the
passwords' strength and identify weak passwords. A large number of password
guessing models have been proposed in the literature. However, little attention
was paid on the task of providing a systematic survey which is necessary to
review the state-of-the-art approaches, identify gaps, and avoid duplicate
study. Motivated from that, we conduct a comprehensive survey on all password
guessing studies presented in the literature from 1979 to 2022. We propose a
generic methodology map of existing models to present an overview of this
field, then, subsequently explain each approach in detail. The experimental
procedures and available datasets used for evaluating password guessing models
are summarized, along with the reported performances of representative studies.
Finally, the current limitations and the open problems as future research
directions are discussed. We believe that this survey is helpful to both the
experts and newcomers who are interested in password security.
</p></li>
</ul>

<h3>Title: Increasing Physical Layer Security through Hyperchaos in VLC Systems. (arXiv:2212.08927v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.08927">http://arxiv.org/abs/2212.08927</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.08927] Increasing Physical Layer Security through Hyperchaos in VLC Systems](http://arxiv.org/abs/2212.08927) #security</code></li>
<li>Summary: <p>Visible Light Communication (VLC) systems have relatively higher security
compared with traditional Radio Frequency (RF) channels due to line-of-sight
(LOS) propagation. However, they still are susceptible to eavesdropping. The
proposed solution of the papers have been built on existing work on
hyperchaos-based security measure to increase physical layer security from
eavesdroppers. A fourth-order Henon map is used to scramble the constellation
diagrams of the transmitted signals. The scramblers change the constellation
symbol of the system using a key. That key on the receiver side de-scrambles
the received data. The presented modulation scheme takes advantage of a higher
degree of the map to isolate the data transmission to a single dimension,
allowing for better scrambling and synchronization. A sliding mode controller
is used at the receiver in a master-slave configuration for projective
synchronization of the two Henon maps, which helps de-scramble the received
data. The data is only isolated for the users aware of the key for
synchronization, providing security against eavesdroppers. The proposed VLC
system is compared against various existing approaches based on various
metrics. An improved Bit Error Rate and a lower information leakage are
achieved for a variety of modulation schemes at an acceptable Signal-to-Noise
Ratio.
</p></li>
</ul>

<h3>Title: Determining Distributions of Security Means for Wireless Sensor Networks based on the Model of a Neighbourhood Watch. (arXiv:2212.09050v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.09050">http://arxiv.org/abs/2212.09050</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.09050] Determining Distributions of Security Means for Wireless Sensor Networks based on the Model of a Neighbourhood Watch](http://arxiv.org/abs/2212.09050) #security</code></li>
<li>Summary: <p>Neighbourhood watch is a concept that allows a community to distribute a
complex security task in between all members. Members of the community carry
out individual security tasks to contribute to the overall security of it. It
reduces the workload of a particular individual while securing all members and
allowing them to carry out a multitude of security tasks. Wireless sensor
networks (WSNs) are composed of resource-constraint independent battery driven
computers as nodes communicating wirelessly. Security in WSNs is essential.
Without sufficient security, an attacker is able to eavesdrop the
communication, tamper monitoring results or deny critical nodes providing their
service in a way to cut off larger network parts. The resource-constraint
nature of sensor nodes prevents them from running full-fledged security
protocols. Instead, it is necessary to assess the most significant security
threats and implement specialised protocols. A neighbourhood-watch inspired
distributed security scheme for WSNs has been introduced by Langend\"orfer. Its
goal is to increase the variety of attacks a WSN can fend off. A framework of
such complexity has to be designed in multiple steps. Here, we introduce an
approach to determine distributions of security means on large-scale static
homogeneous WSNs. Therefore, we model WSNs as undirected graphs in which two
nodes connected iff they are in transmission range. The framework aims to
partition the graph into $n$ distinct security means resulting in the targeted
distribution. The underlying problems turn out to be NP hard and we attempt to
solve them using linear programs (LPs). To evaluate the computability of the
LPs, we generate large numbers of random {\lambda}-precision unit disk graphs
(UDGs) as representation of WSNs. For this purpose, we introduce a novel
{\lambda}-precision UDG generator to model WSNs with a minimal distance in
between nodes.
</p></li>
</ul>

<h3>Title: From NEA and NIA to NESAS and SCAS: Demystifying the 5G Security Ecosystem. (arXiv:2212.09149v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.09149">http://arxiv.org/abs/2212.09149</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.09149] From NEA and NIA to NESAS and SCAS: Demystifying the 5G Security Ecosystem](http://arxiv.org/abs/2212.09149) #security</code></li>
<li>Summary: <p>Despite the numerous pompous statements regarding 5G, it is indisputable that
5G creates a radical shift in telecommunications. The main reason is that 5G is
an enabler of numerous applications we have long envisioned and either
simulated or implemented in test environments, partially or on a smaller scale.
5G will soon unlock the potential of smart cities, industry 4.0, and IoT, to
name a few. However, a crucial question is how much we can trust this
technology. Since this technology will soon become the core infrastructure for
all of the above, it is critical to understand the fundamental security
mechanisms that comprise this technology and the guarantees they provide to
assess the potential risks we are exposed to. This work follows a non-technical
yet bottom-up approach to introduce the reader to the core security mechanisms
and establish a baseline for the security of 5G, to demystify the principal
notions and processes. Based on the above, we streamline future directions and
highlight possible threats.
</p></li>
</ul>

<h3>Title: UAVCAN Dataset Description. (arXiv:2212.09268v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.09268">http://arxiv.org/abs/2212.09268</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.09268] UAVCAN Dataset Description](http://arxiv.org/abs/2212.09268) #security</code></li>
<li>Summary: <p>We collected attack data from unmanned vehicles using the UAVCAN protocol,
and public and described technical documents. A testbed was built with a drone
using PX4, and a total of three attacks, Flooding, Fuzzy, and Replay, were
performed. The attack was carried out in a total of 10 scenarios. We expect
that the attack data will help develop technologies such as anomaly detection
to solve the security threat problem of drones.
</p></li>
</ul>

<h3>Title: Review of security techniques for memristor computing systems. (arXiv:2212.09347v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.09347">http://arxiv.org/abs/2212.09347</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.09347] Review of security techniques for memristor computing systems](http://arxiv.org/abs/2212.09347) #security</code></li>
<li>Summary: <p>Neural network (NN) algorithms have become the dominant tool in visual object
recognition, natural language processing, and robotics. To enhance the
computational efficiency of these algorithms, in comparison to the traditional
von Neuman computing architectures, researchers have been focusing on memristor
computing systems. A major drawback when using memristor computing systems
today is that, in the artificial intelligence (AI) era, well-trained NN models
are intellectual property and, when loaded in the memristor computing systems,
face theft threats, especially when running in edge devices. An adversary may
steal the well-trained NN models through advanced attacks such as learning
attacks and side-channel analysis. In this paper, we review different security
techniques for protecting memristor computing systems. Two threat models are
described based on their assumptions regarding the adversary's capabilities: a
black-box (BB) model and a white-box (WB) model. We categorize the existing
security techniques into five classes in the context of these threat models:
thwarting learning attacks (BB), thwarting side-channel attacks (BB), NN model
encryption (WB), NN weight transformation (WB), and fingerprint embedding (WB).
We also present a cross-comparison of the limitations of the security
techniques. This paper could serve as an aid when designing secure memristor
computing systems.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Plankton-FL: Exploration of Federated Learning for Privacy-Preserving Training of Deep Neural Networks for Phytoplankton Classification. (arXiv:2212.08990v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.08990">http://arxiv.org/abs/2212.08990</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.08990] Plankton-FL: Exploration of Federated Learning for Privacy-Preserving Training of Deep Neural Networks for Phytoplankton Classification](http://arxiv.org/abs/2212.08990) #privacy</code></li>
<li>Summary: <p>Creating high-performance generalizable deep neural networks for
phytoplankton monitoring requires utilizing large-scale data coming from
diverse global water sources. A major challenge to training such networks lies
in data privacy, where data collected at different facilities are often
restricted from being transferred to a centralized location. A promising
approach to overcome this challenge is federated learning, where training is
done at site level on local data, and only the model parameters are exchanged
over the network to generate a global model. In this study, we explore the
feasibility of leveraging federated learning for privacy-preserving training of
deep neural networks for phytoplankton classification. More specifically, we
simulate two different federated learning frameworks, federated learning (FL)
and mutually exclusive FL (ME-FL), and compare their performance to a
traditional centralized learning (CL) framework. Experimental results from this
study demonstrate the feasibility and potential of federated learning for
phytoplankton monitoring.
</p></li>
</ul>

<h3>Title: Stateful Switch: Optimized Time Series Release with Local Differential Privacy. (arXiv:2212.08792v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.08792">http://arxiv.org/abs/2212.08792</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.08792] Stateful Switch: Optimized Time Series Release with Local Differential Privacy](http://arxiv.org/abs/2212.08792) #privacy</code></li>
<li>Summary: <p>Time series data have numerous applications in big data analytics. However,
they often cause privacy issues when collected from individuals. To address
this problem, most existing works perturb the values in the time series while
retaining their temporal order, which may lead to significant distortion of the
values. Recently, we propose TLDP model that perturbs temporal perturbation to
ensure privacy guarantee while retaining original values. It has shown great
promise to achieve significantly higher utility than value perturbation
mechanisms in many time series analysis. However, its practicability is still
undermined by two factors, namely, utility cost of extra missing or empty
values, and inflexibility of privacy budget settings. To address them, in this
paper we propose {\it switch} as a new two-way operation for temporal
perturbation, as opposed to the one-way {\it dispatch} operation. The former
inherently eliminates the cost of missing, empty or repeated values. Optimizing
switch operation in a {\it stateful} manner, we then propose $StaSwitch$
mechanism for time series release under TLDP. Through both analytical and
empirical studies, we show that $StaSwitch$ has significantly higher utility
for the published time series than any state-of-the-art temporal- or
value-perturbation mechanism, while allowing any combination of privacy budget
settings.
</p></li>
</ul>

<h3>Title: Addressing Data Heterogeneity in Decentralized Learning via Topological Pre-processing. (arXiv:2212.08743v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.08743">http://arxiv.org/abs/2212.08743</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.08743] Addressing Data Heterogeneity in Decentralized Learning via Topological Pre-processing](http://arxiv.org/abs/2212.08743) #privacy</code></li>
<li>Summary: <p>Recently, local peer topology has been shown to influence the overall
convergence of decentralized learning (DL) graphs in the presence of data
heterogeneity. In this paper, we demonstrate the advantages of constructing a
proxy-based locally heterogeneous DL topology to enhance convergence and
maintain data privacy. In particular, we propose a novel peer clumping strategy
to efficiently cluster peers before arranging them in a final training graph.
By showing how locally heterogeneous graphs outperform locally homogeneous
graphs of similar size and from the same global data distribution, we present a
strong case for topological pre-processing. Moreover, we demonstrate the
scalability of our approach by showing how the proposed topological
pre-processing overhead remains small in large graphs while the performance
gains get even more pronounced. Furthermore, we show the robustness of our
approach in the presence of network partitions.
</p></li>
</ul>

<h2>protect</h2>
<h2>defense</h2>
<h2>attack</h2>
<h3>Title: Minimizing Maximum Model Discrepancy for Transferable Black-box Targeted Attacks. (arXiv:2212.09035v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.09035">http://arxiv.org/abs/2212.09035</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.09035] Minimizing Maximum Model Discrepancy for Transferable Black-box Targeted Attacks](http://arxiv.org/abs/2212.09035) #attack</code></li>
<li>Summary: <p>In this work, we study the black-box targeted attack problem from the model
discrepancy perspective. On the theoretical side, we present a generalization
error bound for black-box targeted attacks, which gives a rigorous theoretical
analysis for guaranteeing the success of the attack. We reveal that the attack
error on a target model mainly depends on empirical attack error on the
substitute model and the maximum model discrepancy among substitute models. On
the algorithmic side, we derive a new algorithm for black-box targeted attacks
based on our theoretical analysis, in which we additionally minimize the
maximum model discrepancy(M3D) of the substitute models when training the
generator to generate adversarial examples. In this way, our model is capable
of crafting highly transferable adversarial examples that are robust to the
model variation, thus improving the success rate for attacking the black-box
model. We conduct extensive experiments on the ImageNet dataset with different
classification models, and our proposed approach outperforms existing
state-of-the-art methods by a significant margin. Our codes will be released.
</p></li>
</ul>

<h3>Title: Fine-Tuning Is All You Need to Mitigate Backdoor Attacks. (arXiv:2212.09067v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.09067">http://arxiv.org/abs/2212.09067</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.09067] Fine-Tuning Is All You Need to Mitigate Backdoor Attacks](http://arxiv.org/abs/2212.09067) #attack</code></li>
<li>Summary: <p>Backdoor attacks represent one of the major threats to machine learning
models. Various efforts have been made to mitigate backdoors. However, existing
defenses have become increasingly complex and often require high computational
resources or may also jeopardize models' utility. In this work, we show that
fine-tuning, one of the most common and easy-to-adopt machine learning training
operations, can effectively remove backdoors from machine learning models while
maintaining high model utility. Extensive experiments over three machine
learning paradigms show that fine-tuning and our newly proposed
super-fine-tuning achieve strong defense performance. Furthermore, we coin a
new term, namely backdoor sequela, to measure the changes in model
vulnerabilities to other attacks before and after the backdoor has been
removed. Empirical evaluation shows that, compared to other defense methods,
super-fine-tuning leaves limited backdoor sequela. We hope our results can help
machine learning model owners better protect their models from backdoor
threats. Also, it calls for the design of more advanced attacks in order to
comprehensively assess machine learning models' backdoor vulnerabilities.
</p></li>
</ul>

<h3>Title: SkillFence: A Systems Approach to Practically Mitigating Voice-Based Confusion Attacks. (arXiv:2212.08738v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.08738">http://arxiv.org/abs/2212.08738</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.08738] SkillFence: A Systems Approach to Practically Mitigating Voice-Based Confusion Attacks](http://arxiv.org/abs/2212.08738) #attack</code></li>
<li>Summary: <p>Voice assistants are deployed widely and provide useful functionality.
However, recent work has shown that commercial systems like Amazon Alexa and
Google Home are vulnerable to voice-based confusion attacks that exploit design
issues. We propose a systems-oriented defense against this class of attacks and
demonstrate its functionality for Amazon Alexa. We ensure that only the skills
a user intends execute in response to voice commands. Our key insight is that
we can interpret a user's intentions by analyzing their activity on counterpart
systems of the web and smartphones. For example, the Lyft ride-sharing Alexa
skill has an Android app and a website. Our work shows how information from
counterpart apps can help reduce dis-ambiguities in the skill invocation
process. We build SkilIFence, a browser extension that existing voice assistant
users can install to ensure that only legitimate skills run in response to
their commands. Using real user data from MTurk (N = 116) and experimental
trials involving synthetic and organic speech, we show that SkillFence provides
a balance between usability and security by securing 90.83% of skills that a
user will need with a False acceptance rate of 19.83%.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: Multi-person 3D pose estimation from unlabelled data. (arXiv:2212.08731v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.08731">http://arxiv.org/abs/2212.08731</a></li>
<li>Code URL: <a href="https://github.com/gnns4hri/3d_multi_pose_estimator">https://github.com/gnns4hri/3d_multi_pose_estimator</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2212.08731] Multi-person 3D pose estimation from unlabelled data](http://arxiv.org/abs/2212.08731) #robust</code></li>
<li>Summary: <p>Its numerous applications make multi-human 3D pose estimation a remarkably
impactful area of research. Nevertheless, assuming a multiple-view system
composed of several regular RGB cameras, 3D multi-pose estimation presents
several challenges. First of all, each person must be uniquely identified in
the different views to separate the 2D information provided by the cameras.
Secondly, the 3D pose estimation process from the multi-view 2D information of
each person must be robust against noise and potential occlusions in the
scenario. In this work, we address these two challenges with the help of deep
learning. Specifically, we present a model based on Graph Neural Networks
capable of predicting the cross-view correspondence of the people in the
scenario along with a Multilayer Perceptron that takes the 2D points to yield
the 3D poses of each person. These two models are trained in a self-supervised
manner, thus avoiding the need for large datasets with 3D annotations.
</p></li>
</ul>

<h3>Title: Towards Robust Handwritten Text Recognition with On-the-fly User Participation. (arXiv:2212.08834v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.08834">http://arxiv.org/abs/2212.08834</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.08834] Towards Robust Handwritten Text Recognition with On-the-fly User Participation](http://arxiv.org/abs/2212.08834) #robust</code></li>
<li>Summary: <p>Long-term OCR services aim to provide high-quality output to their users at
competitive costs. It is essential to upgrade the models because of the complex
data loaded by the users. The service providers encourage the users who provide
data where the OCR model fails by rewarding them based on data complexity,
readability, and available budget. Hitherto, the OCR works include preparing
the models on standard datasets without considering the end-users. We propose a
strategy of consistently upgrading an existing Handwritten Hindi OCR model
three times on the dataset of 15 users. We fix the budget of 4 users for each
iteration. For the first iteration, the model directly trains on the dataset
from the first four users. For the rest iteration, all remaining users write a
page each, which service providers later analyze to select the 4 (new) best
users based on the quality of predictions on the human-readable words. Selected
users write 23 more pages for upgrading the model. We upgrade the model with
Curriculum Learning (CL) on the data available in the current iteration and
compare the subset from previous iterations. The upgraded model is tested on a
held-out set of one page each from all 23 users. We provide insights into our
investigations on the effect of CL, user selection, and especially the data
from unseen writing styles. Our work can be used for long-term OCR services in
crowd-sourcing scenarios for the service providers and end users.
</p></li>
</ul>

<h3>Title: Hyperbolic Hierarchical Contrastive Hashing. (arXiv:2212.08904v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.08904">http://arxiv.org/abs/2212.08904</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.08904] Hyperbolic Hierarchical Contrastive Hashing](http://arxiv.org/abs/2212.08904) #robust</code></li>
<li>Summary: <p>Hierarchical semantic structures, naturally existing in real-world datasets,
can assist in capturing the latent distribution of data to learn robust hash
codes for retrieval systems. Although hierarchical semantic structures can be
simply expressed by integrating semantically relevant data into a high-level
taxon with coarser-grained semantics, the construction, embedding, and
exploitation of the structures remain tricky for unsupervised hash learning. To
tackle these problems, we propose a novel unsupervised hashing method named
Hyperbolic Hierarchical Contrastive Hashing (HHCH). We propose to embed
continuous hash codes into hyperbolic space for accurate semantic expression
since embedding hierarchies in hyperbolic space generates less distortion than
in hyper-sphere space and Euclidean space. In addition, we extend the K-Means
algorithm to hyperbolic space and perform the proposed hierarchical hyperbolic
K-Means algorithm to construct hierarchical semantic structures adaptively. To
exploit the hierarchical semantic structures in hyperbolic space, we designed
the hierarchical contrastive learning algorithm, including hierarchical
instance-wise and hierarchical prototype-wise contrastive learning. Extensive
experiments on four benchmark datasets demonstrate that the proposed method
outperforms the state-of-the-art unsupervised hashing methods. Codes will be
released.
</p></li>
</ul>

<h3>Title: Mask-FPAN: Semi-Supervised Face Parsing in the Wild With De-Occlusion and UV GAN. (arXiv:2212.09098v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.09098">http://arxiv.org/abs/2212.09098</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.09098] Mask-FPAN: Semi-Supervised Face Parsing in the Wild With De-Occlusion and UV GAN](http://arxiv.org/abs/2212.09098) #robust</code></li>
<li>Summary: <p>Fine-grained semantic segmentation of a person's face and head, including
facial parts and head components, has progressed a great deal in recent years.
However, it remains a challenging task, whereby considering ambiguous
occlusions and large pose variations are particularly difficult. To overcome
these difficulties, we propose a novel framework termed Mask-FPAN. It uses a
de-occlusion module that learns to parse occluded faces in a semi-supervised
way. In particular, face landmark localization, face occlusionstimations, and
detected head poses are taken into account. A 3D morphable face model combined
with the UV GAN improves the robustness of 2D face parsing. In addition, we
introduce two new datasets named FaceOccMask-HQ and CelebAMaskOcc-HQ for face
paring work. The proposed Mask-FPAN framework addresses the face parsing
problem in the wild and shows significant performance improvements with MIOU
from 0.7353 to 0.9013 compared to the state-of-the-art on challenging face
datasets.
</p></li>
</ul>

<h3>Title: Robust Anomaly Map Assisted Multiple Defect Detection with Supervised Classification Techniques. (arXiv:2212.09352v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.09352">http://arxiv.org/abs/2212.09352</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.09352] Robust Anomaly Map Assisted Multiple Defect Detection with Supervised Classification Techniques](http://arxiv.org/abs/2212.09352) #robust</code></li>
<li>Summary: <p>Industry 4.0 aims to optimize the manufacturing environment by leveraging new
technological advances, such as new sensing capabilities and artificial
intelligence. The DRAEM technique has shown state-of-the-art performance for
unsupervised classification. The ability to create anomaly maps highlighting
areas where defects probably lie can be leveraged to provide cues to supervised
classification models and enhance their performance. Our research shows that
the best performance is achieved when training a defect detection model by
providing an image and the corresponding anomaly map as input. Furthermore,
such a setting provides consistent performance when framing the defect
detection as a binary or multiclass classification problem and is not affected
by class balancing policies. We performed the experiments on three datasets
with real-world data provided by Philips Consumer Lifestyle BV.
</p></li>
</ul>

<h3>Title: HyPe: Better Pre-trained Language Model Fine-tuning with Hidden Representation Perturbation. (arXiv:2212.08853v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.08853">http://arxiv.org/abs/2212.08853</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.08853] HyPe: Better Pre-trained Language Model Fine-tuning with Hidden Representation Perturbation](http://arxiv.org/abs/2212.08853) #robust</code></li>
<li>Summary: <p>Language models with the Transformers structure have shown great performance
in natural language processing. However, there still poses problems when
fine-tuning pre-trained language models on downstream tasks, such as
over-fitting or representation collapse. In this work, we propose HyPe, a
simple yet effective fine-tuning technique to alleviate such problems by
perturbing hidden representations of Transformers layers. Unlike previous works
that only add noise to inputs or parameters, we argue that the hidden
representations of Transformers layers convey more diverse and meaningful
language information. Therefore, making the Transformers layers more robust to
hidden representation perturbations can further benefit the fine-tuning of PLMs
en bloc. We conduct extensive experiments and analyses on GLUE and other
natural language inference datasets. Results demonstrate that HyPe outperforms
vanilla fine-tuning and enhances generalization of hidden representations from
different layers. In addition, HyPe acquires negligible computational
overheads, and is better than and compatible with previous state-of-the-art
fine-tuning techniques.
</p></li>
</ul>

<h3>Title: Language model acceptability judgements are not always robust to context. (arXiv:2212.08979v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.08979">http://arxiv.org/abs/2212.08979</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.08979] Language model acceptability judgements are not always robust to context](http://arxiv.org/abs/2212.08979) #robust</code></li>
<li>Summary: <p>Targeted syntactic evaluations of language models ask whether models show
stable preferences for syntactically acceptable content over minimal-pair
unacceptable inputs. Most targeted syntactic evaluation datasets ask models to
make these judgements with just a single context-free sentence as input. This
does not match language models' training regime, in which input sentences are
always highly contextualized by the surrounding corpus. This mismatch raises an
important question: how robust are models' syntactic judgements in different
contexts? In this paper, we investigate the stability of language models'
performance on targeted syntactic evaluations as we vary properties of the
input context: the length of the context, the types of syntactic phenomena it
contains, and whether or not there are violations of grammaticality. We find
that model judgements are generally robust when placed in randomly sampled
linguistic contexts. However, they are substantially unstable for contexts
containing syntactic structures matching those in the critical test content.
Among all tested models (GPT-2 and five variants of OPT), we significantly
improve models' judgements by providing contexts with matching syntactic
structures, and conversely significantly worsen them using unacceptable
contexts with matching but violated syntactic structures. This effect is
amplified by the length of the context, except for unrelated inputs. We show
that these changes in model performance are not explainable by simple features
matching the context and the test inputs, such as lexical overlap and
dependency overlap. This sensitivity to highly specific syntactic features of
the context can only be explained by the models' implicit in-context learning
abilities.
</p></li>
</ul>

<h3>Title: A Robust Semantic Frame Parsing Pipeline on a New Complex Twitter Dataset. (arXiv:2212.08987v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.08987">http://arxiv.org/abs/2212.08987</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.08987] A Robust Semantic Frame Parsing Pipeline on a New Complex Twitter Dataset](http://arxiv.org/abs/2212.08987) #robust</code></li>
<li>Summary: <p>Most recent semantic frame parsing systems for spoken language understanding
(SLU) are designed based on recurrent neural networks. These systems display
decent performance on benchmark SLU datasets such as ATIS or SNIPS, which
contain short utterances with relatively simple patterns. However, the current
semantic frame parsing models lack a mechanism to handle out-of-distribution
(\emph{OOD}) patterns and out-of-vocabulary (\emph{OOV}) tokens. In this paper,
we introduce a robust semantic frame parsing pipeline that can handle both
\emph{OOD} patterns and \emph{OOV} tokens in conjunction with a new complex
Twitter dataset that contains long tweets with more \emph{OOD} patterns and
\emph{OOV} tokens. The new pipeline demonstrates much better results in
comparison to state-of-the-art baseline SLU models on both the SNIPS dataset
and the new Twitter dataset (Our new Twitter dataset can be downloaded from
https://1drv.ms/u/s!AroHb-W6_OAlavK4begsDsMALfE?e=c8f2XX ). Finally, we also
build an E2E application to demo the feasibility of our algorithm and show why
it is useful in real application.
</p></li>
</ul>

<h3>Title: I2D2: Inductive Knowledge Distillation with NeuroLogic and Self-Imitation. (arXiv:2212.09246v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.09246">http://arxiv.org/abs/2212.09246</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.09246] I2D2: Inductive Knowledge Distillation with NeuroLogic and Self-Imitation](http://arxiv.org/abs/2212.09246) #robust</code></li>
<li>Summary: <p>Pre-trained language models, despite their rapid advancements powered by
scale, still fall short of robust commonsense capabilities. And yet, scale
appears to be the winning recipe; after all, the largest models seem to have
acquired the largest amount of commonsense capabilities. Or is it?
</p></li>
</ul>

<p>In this paper, we investigate the possibility of a seemingly impossible
match: can smaller language models with dismal commonsense capabilities (i.e.,
GPT-2), ever win over models that are orders of magnitude larger and better
(i.e., GPT-3), if the smaller models are powered with novel commonsense
distillation algorithms? The key intellectual question we ask here is whether
it is possible, if at all, to design a learning algorithm that does not benefit
from scale, yet leads to a competitive level of commonsense acquisition. In
this work, we study the generative models of commonsense knowledge, focusing on
the task of generating generics, statements of commonsense facts about everyday
concepts, e.g., birds can fly.
</p>
<p>We introduce a novel commonsense distillation framework, I2D2, that loosely
follows the Symbolic Knowledge Distillation of West et al. but breaks the
dependence on the extreme-scale models as the teacher model by two innovations:
(1) the novel adaptation of NeuroLogic Decoding to enhance the generation
quality of the weak, off-the-shelf language models, and (2) self-imitation
learning to iteratively learn from the model's own enhanced commonsense
acquisition capabilities. Empirical results suggest that scale is not the only
way, as novel algorithms can be a promising alternative. Moreover, our study
leads to a new corpus of generics, Gen-A-Tomic, that is of the largest and
highest quality available to date.
</p>

<h3>Title: TextGrad: Advancing Robustness Evaluation in NLP by Gradient-Driven Optimization. (arXiv:2212.09254v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.09254">http://arxiv.org/abs/2212.09254</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.09254] TextGrad: Advancing Robustness Evaluation in NLP by Gradient-Driven Optimization](http://arxiv.org/abs/2212.09254) #robust</code></li>
<li>Summary: <p>Robustness evaluation against adversarial examples has become increasingly
important to unveil the trustworthiness of the prevailing deep models in
natural language processing (NLP). However, in contrast to the computer vision
domain where the first-order projected gradient descent (PGD) is used as the
benchmark approach to generate adversarial examples for robustness evaluation,
there lacks a principled first-order gradient-based robustness evaluation
framework in NLP. The emerging optimization challenges lie in 1) the discrete
nature of textual inputs together with the strong coupling between the
perturbation location and the actual content, and 2) the additional constraint
that the perturbed text should be fluent and achieve a low perplexity under a
language model. These challenges make the development of PGD-like NLP attacks
difficult. To bridge the gap, we propose TextGrad, a new attack generator using
gradient-driven optimization, supporting high-accuracy and high-quality
assessment of adversarial robustness in NLP. Specifically, we address the
aforementioned challenges in a unified optimization framework. And we develop
an effective convex relaxation method to co-optimize the continuously-relaxed
site selection and perturbation variables and leverage an effective sampling
method to establish an accurate mapping from the continuous optimization
variables to the discrete textual perturbations. Moreover, as a first-order
attack generation method, TextGrad can be baked into adversarial training to
further improve the robustness of NLP models. Extensive experiments are
provided to demonstrate the effectiveness of TextGrad not only in attack
generation for robustness evaluation but also in adversarial defense.
</p></li>
</ul>

<h3>Title: Pre-Trained Image Encoder for Generalizable Visual Reinforcement Learning. (arXiv:2212.08860v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.08860">http://arxiv.org/abs/2212.08860</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.08860] Pre-Trained Image Encoder for Generalizable Visual Reinforcement Learning](http://arxiv.org/abs/2212.08860) #robust</code></li>
<li>Summary: <p>Learning generalizable policies that can adapt to unseen environments remains
challenging in visual Reinforcement Learning (RL). Existing approaches try to
acquire a robust representation via diversifying the appearances of in-domain
observations for better generalization. Limited by the specific observations of
the environment, these methods ignore the possibility of exploring diverse
real-world image datasets. In this paper, we investigate how a visual RL agent
would benefit from the off-the-shelf visual representations. Surprisingly, we
find that the early layers in an ImageNet pre-trained ResNet model could
provide rather generalizable representations for visual RL. Hence, we propose
Pre-trained Image Encoder for Generalizable visual reinforcement learning
(PIE-G), a simple yet effective framework that can generalize to the unseen
visual scenarios in a zero-shot manner. Extensive experiments are conducted on
DMControl Generalization Benchmark, DMControl Manipulation Tasks, Drawer World,
and CARLA to verify the effectiveness of PIE-G. Empirical evidence suggests
PIE-G improves sample efficiency and significantly outperforms previous
state-of-the-art methods in terms of generalization performance. In particular,
PIE-G boasts a 55% generalization performance gain on average in the
challenging video background setting. Project Page:
https://sites.google.com/view/pie-g/home.
</p></li>
</ul>

<h3>Title: Confidence-aware Training of Smoothed Classifiers for Certified Robustness. (arXiv:2212.09000v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.09000">http://arxiv.org/abs/2212.09000</a></li>
<li>Code URL: <a href="https://github.com/alinlab/smoothing-catrs">https://github.com/alinlab/smoothing-catrs</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2212.09000] Confidence-aware Training of Smoothed Classifiers for Certified Robustness](http://arxiv.org/abs/2212.09000) #robust</code></li>
<li>Summary: <p>Any classifier can be "smoothed out" under Gaussian noise to build a new
classifier that is provably robust to $\ell_2$-adversarial perturbations, viz.,
by averaging its predictions over the noise via randomized smoothing. Under the
smoothed classifiers, the fundamental trade-off between accuracy and
(adversarial) robustness has been well evidenced in the literature: i.e.,
increasing the robustness of a classifier for an input can be at the expense of
decreased accuracy for some other inputs. In this paper, we propose a simple
training method leveraging this trade-off to obtain robust smoothed
classifiers, in particular, through a sample-wise control of robustness over
the training samples. We make this control feasible by using "accuracy under
Gaussian noise" as an easy-to-compute proxy of adversarial robustness for an
input. Specifically, we differentiate the training objective depending on this
proxy to filter out samples that are unlikely to benefit from the worst-case
(adversarial) objective. Our experiments show that the proposed method, despite
its simplicity, consistently exhibits improved certified robustness upon
state-of-the-art training methods. Somewhat surprisingly, we find these
improvements persist even for other notions of robustness, e.g., to various
types of common corruptions.
</p></li>
</ul>

<h3>Title: Estimating the Adversarial Robustness of Attributions in Text with Transformers. (arXiv:2212.09155v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.09155">http://arxiv.org/abs/2212.09155</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.09155] Estimating the Adversarial Robustness of Attributions in Text with Transformers](http://arxiv.org/abs/2212.09155) #robust</code></li>
<li>Summary: <p>Explanations are crucial parts of deep neural network (DNN) classifiers. In
high stakes applications, faithful and robust explanations are important to
understand and gain trust in DNN classifiers. However, recent work has shown
that state-of-the-art attribution methods in text classifiers are susceptible
to imperceptible adversarial perturbations that alter explanations
significantly while maintaining the correct prediction outcome. If undetected,
this can critically mislead the users of DNNs. Thus, it is crucial to
understand the influence of such adversarial perturbations on the networks'
explanations and their perceptibility. In this work, we establish a novel
definition of attribution robustness (AR) in text classification, based on
Lipschitz continuity. Crucially, it reflects both attribution change induced by
adversarial input alterations and perceptibility of such alterations. Moreover,
we introduce a wide set of text similarity measures to effectively capture
locality between two text samples and imperceptibility of adversarial
perturbations in text. We then propose our novel TransformerExplanationAttack
(TEA), a strong adversary that provides a tight estimation for attribution
robustness in text classification. TEA uses state-of-the-art language models to
extract word substitutions that result in fluent, contextual adversarial
samples. Finally, with experiments on several text classification
architectures, we show that TEA consistently outperforms current
state-of-the-art AR estimators, yielding perturbations that alter explanations
to a greater extent while being more fluent and less perceptible.
</p></li>
</ul>

<h2>biometric</h2>
<h3>Title: Gait Recognition Using 3-D Human Body Shape Inference. (arXiv:2212.09042v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.09042">http://arxiv.org/abs/2212.09042</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.09042] Gait Recognition Using 3-D Human Body Shape Inference](http://arxiv.org/abs/2212.09042) #biometric</code></li>
<li>Summary: <p>Gait recognition, which identifies individuals based on their walking
patterns, is an important biometric technique since it can be observed from a
distance and does not require the subject's cooperation. Recognizing a person's
gait is difficult because of the appearance variants in human silhouette
sequences produced by varying viewing angles, carrying objects, and clothing.
Recent research has produced a number of ways for coping with these variants.
In this paper, we present the usage of inferring 3-D body shapes distilled from
limited images, which are, in principle, invariant to the specified variants.
Inference of 3-D shape is a difficult task, especially when only silhouettes
are provided in a dataset. We provide a method for learning 3-D body inference
from silhouettes by transferring knowledge from 3-D shape prior from RGB
photos. We use our method on multiple existing state-of-the-art gait baselines
and obtain consistent improvements for gait identification on two public
datasets, CASIA-B and OUMVLP, on several variants and settings, including a new
setting of novel views not seen during training.
</p></li>
</ul>

<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Flattening-Net: Deep Regular 2D Representation for 3D Point Cloud Analysis. (arXiv:2212.08892v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.08892">http://arxiv.org/abs/2212.08892</a></li>
<li>Code URL: <a href="https://github.com/keeganhk/flattening-net">https://github.com/keeganhk/flattening-net</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2212.08892] Flattening-Net: Deep Regular 2D Representation for 3D Point Cloud Analysis](http://arxiv.org/abs/2212.08892) #extraction</code></li>
<li>Summary: <p>Point clouds are characterized by irregularity and unstructuredness, which
pose challenges in efficient data exploitation and discriminative feature
extraction. In this paper, we present an unsupervised deep neural architecture
called Flattening-Net to represent irregular 3D point clouds of arbitrary
geometry and topology as a completely regular 2D point geometry image (PGI)
structure, in which coordinates of spatial points are captured in colors of
image pixels. \mr{Intuitively, Flattening-Net implicitly approximates a locally
smooth 3D-to-2D surface flattening process while effectively preserving
neighborhood consistency.} \mr{As a generic representation modality, PGI
inherently encodes the intrinsic property of the underlying manifold structure
and facilitates surface-style point feature aggregation.} To demonstrate its
potential, we construct a unified learning framework directly operating on PGIs
to achieve \mr{diverse types of high-level and low-level} downstream
applications driven by specific task networks, including classification,
segmentation, reconstruction, and upsampling. Extensive experiments demonstrate
that our methods perform favorably against the current state-of-the-art
competitors. We will make the code and data publicly available at
https://github.com/keeganhk/Flattening-Net.
</p></li>
</ul>

<h3>Title: Graph Neural Network based Child Activity Recognition. (arXiv:2212.09013v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.09013">http://arxiv.org/abs/2212.09013</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.09013] Graph Neural Network based Child Activity Recognition](http://arxiv.org/abs/2212.09013) #extraction</code></li>
<li>Summary: <p>This paper presents an implementation on child activity recognition (CAR)
with a graph convolution network (GCN) based deep learning model since prior
implementations in this domain have been dominated by CNN, LSTM and other
methods despite the superior performance of GCN. To the best of our knowledge,
we are the first to use a GCN model in child activity recognition domain. In
overcoming the challenges of having small size publicly available child action
datasets, several learning methods such as feature extraction, fine-tuning and
curriculum learning were implemented to improve the model performance. Inspired
by the contradicting claims made on the use of transfer learning in CAR, we
conducted a detailed implementation and analysis on transfer learning together
with a study on negative transfer learning effect on CAR as it hasn't been
addressed previously. As the principal contribution, we were able to develop a
ST-GCN based CAR model which, despite the small size of the dataset, obtained
around 50% accuracy on vanilla implementations. With feature extraction and
fine-tuning methods, accuracy was improved by 20%-30% with the highest accuracy
being 82.24%. Furthermore, the results provided on activity datasets
empirically demonstrate that with careful selection of pre-train model datasets
through methods such as curriculum learning could enhance the accuracy levels.
Finally, we provide preliminary evidence on possible frame rate effect on the
accuracy of CAR models, a direction future research can explore.
</p></li>
</ul>

<h3>Title: Building Height Prediction with Instance Segmentation. (arXiv:2212.09277v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.09277">http://arxiv.org/abs/2212.09277</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.09277] Building Height Prediction with Instance Segmentation](http://arxiv.org/abs/2212.09277) #extraction</code></li>
<li>Summary: <p>Extracting building heights from satellite images is an active research area
used in many fields such as telecommunications, city planning, etc. Many
studies utilize DSM (Digital Surface Models) generated with lidars or stereo
images for this purpose. Predicting the height of the buildings using only RGB
images is challenging due to the insufficient amount of data, low data quality,
variations of building types, different angles of light and shadow, etc. In
this study, we present an instance segmentation-based building height
extraction method to predict building masks with their respective heights from
a single RGB satellite image. We used satellite images with building height
annotations of certain cities along with an open-source satellite dataset with
the transfer learning approach. We reached, the bounding box mAP 59, the mask
mAP 52.6, and the average accuracy value of 70% for buildings belonging to each
height class in our test set.
</p></li>
</ul>

<h3>Title: Unsupervised Dense Retrieval Deserves Better Positive Pairs: Scalable Augmentation with Query Extraction and Generation. (arXiv:2212.08841v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.08841">http://arxiv.org/abs/2212.08841</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.08841] Unsupervised Dense Retrieval Deserves Better Positive Pairs: Scalable Augmentation with Query Extraction and Generation](http://arxiv.org/abs/2212.08841) #extraction</code></li>
<li>Summary: <p>Dense retrievers have made significant strides in obtaining state-of-the-art
results on text retrieval and open-domain question answering (ODQA). Yet most
of these achievements were made possible with the help of large annotated
datasets, unsupervised learning for dense retrieval models remains an open
problem. In this work, we explore two categories of methods for creating pseudo
query-document pairs, named query extraction (QExt) and transferred query
generation (TQGen), to augment the retriever training in an annotation-free and
scalable manner. Specifically, QExt extracts pseudo queries by document
structures or selecting salient random spans, and TQGen utilizes generation
models trained for other NLP tasks (e.g., summarization) to produce pseudo
queries. Extensive experiments show that dense retrievers trained with
individual augmentation methods can perform comparably well with multiple
strong baselines, and combining them leads to further improvements, achieving
state-of-the-art performance of unsupervised dense retrieval on both BEIR and
ODQA datasets.
</p></li>
</ul>

<h3>Title: 'If you build they will come': Automatic Identification of News-Stakeholders to detect Party Preference in News Coverage. (arXiv:2212.08864v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.08864">http://arxiv.org/abs/2212.08864</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.08864] 'If you build they will come': Automatic Identification of News-Stakeholders to detect Party Preference in News Coverage](http://arxiv.org/abs/2212.08864) #extraction</code></li>
<li>Summary: <p>The coverage of different stakeholders mentioned in the news articles
significantly impacts the slant or polarity detection of the concerned news
publishers. For instance, the pro-government media outlets would give more
coverage to the government stakeholders to increase their accessibility to the
news audiences. In contrast, the anti-government news agencies would focus more
on the views of the opponent stakeholders to inform the readers about the
shortcomings of government policies. In this paper, we address the problem of
stakeholder extraction from news articles and thereby determine the inherent
bias present in news reporting. Identifying potential stakeholders in
multi-topic news scenarios is challenging because each news topic has different
stakeholders. The research presented in this paper utilizes both contextual
information and external knowledge to identify the topic-specific stakeholders
from news articles. We also apply a sequential incremental clustering algorithm
to group the entities with similar stakeholder types. We carried out all our
experiments on news articles on four Indian government policies published by
numerous national and international news agencies. We also further generalize
our system, and the experimental results show that the proposed model can be
extended to other news topics.
</p></li>
</ul>

<h3>Title: Joint Information Extraction with Cross-Task and Cross-Instance High-Order Modeling. (arXiv:2212.08929v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.08929">http://arxiv.org/abs/2212.08929</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.08929] Joint Information Extraction with Cross-Task and Cross-Instance High-Order Modeling](http://arxiv.org/abs/2212.08929) #extraction</code></li>
<li>Summary: <p>Prior works on Information Extraction (IE) typically predict different tasks
and instances (e.g., event triggers, entities, roles, relations) independently,
while neglecting their interactions and leading to model inefficiency. In this
work, we introduce a joint IE framework, HighIE, that learns and predicts
multiple IE tasks by integrating high-order cross-task and cross-instance
dependencies. Specifically, we design two categories of high-order factors:
homogeneous factors and heterogeneous factors. Then, these factors are utilized
to jointly predict labels of all instances. To address the intractability
problem of exact high-order inference, we incorporate a high-order neural
decoder that is unfolded from a mean-field variational inference method. The
experimental results show that our approach achieves consistent improvements on
three IE tasks compared with our baseline and prior work.
</p></li>
</ul>

<h3>Title: A Better Choice: Entire-space Datasets for Aspect Sentiment Triplet Extraction. (arXiv:2212.09052v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.09052">http://arxiv.org/abs/2212.09052</a></li>
<li>Code URL: <a href="https://github.com/l294265421/entire-space-aste">https://github.com/l294265421/entire-space-aste</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2212.09052] A Better Choice: Entire-space Datasets for Aspect Sentiment Triplet Extraction](http://arxiv.org/abs/2212.09052) #extraction</code></li>
<li>Summary: <p>Aspect sentiment triplet extraction (ASTE) aims to extract aspect term,
sentiment and opinion term triplets from sentences. Since the initial datasets
used to evaluate models on ASTE had flaws, several studies later corrected the
initial datasets and released new versions of the datasets independently. As a
result, different studies select different versions of datasets to evaluate
their methods, which makes ASTE-related works hard to follow. In this paper, we
analyze the relation between different versions of datasets and suggest that
the entire-space version should be used for ASTE. Besides the sentences
containing triplets and the triplets in the sentences, the entire-space version
additionally includes the sentences without triplets and the aspect terms which
do not belong to any triplets. Hence, the entire-space version is consistent
with real-world scenarios and evaluating models on the entire-space version can
better reflect the models' performance in real-world scenarios. In addition,
experimental results show that evaluating models on non-entire-space datasets
inflates the performance of existing models and models trained on the
entire-space version can obtain better performance.
</p></li>
</ul>

<h3>Title: Bridging The Gap: Entailment Fused-T5 for Open-retrieval Conversational Machine Reading Comprehension. (arXiv:2212.09353v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.09353">http://arxiv.org/abs/2212.09353</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.09353] Bridging The Gap: Entailment Fused-T5 for Open-retrieval Conversational Machine Reading Comprehension](http://arxiv.org/abs/2212.09353) #extraction</code></li>
<li>Summary: <p>Open-retrieval conversational machine reading comprehension (OCMRC) simulates
real-life conversational interaction scenes. Machines are required to make a
decision of "Yes/No/Inquire" or generate a follow-up question when the decision
is "Inquire" based on retrieved rule texts, user scenario, user question, and
dialogue history. Recent studies explored the methods to reduce the information
gap between decision-making and question generation and thus improve the
performance of generation. However, the information gap still exists because
these pipeline structures are still limited in decision-making, span
extraction, and question rephrasing three stages. Decision-making and
generation are reasoning separately, and the entailment reasoning utilized in
decision-making is hard to share through all stages. To tackle the above
problem, we proposed a novel one-stage end-to-end framework, called Entailment
Fused-T5 (EFT), to bridge the information gap between decision-making and
generation in a global understanding manner. The extensive experimental results
demonstrate that our proposed framework achieves new state-of-the-art
performance on the OR-ShARC benchmark.
</p></li>
</ul>

<h3>Title: Enriching Relation Extraction with OpenIE. (arXiv:2212.09376v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.09376">http://arxiv.org/abs/2212.09376</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.09376] Enriching Relation Extraction with OpenIE](http://arxiv.org/abs/2212.09376) #extraction</code></li>
<li>Summary: <p>Relation extraction (RE) is a sub-discipline of information extraction (IE)
which focuses on the prediction of a relational predicate from a
natural-language input unit (such as a sentence, a clause, or even a short
paragraph consisting of multiple sentences and/or clauses). Together with
named-entity recognition (NER) and disambiguation (NED), RE forms the basis for
many advanced IE tasks such as knowledge-base (KB) population and verification.
In this work, we explore how recent approaches for open information extraction
(OpenIE) may help to improve the task of RE by encoding structured information
about the sentences' principal units, such as subjects, objects, verbal
phrases, and adverbials, into various forms of vectorized (and hence
unstructured) representations of the sentences. Our main conjecture is that the
decomposition of long and possibly convoluted sentences into multiple smaller
clauses via OpenIE even helps to fine-tune context-sensitive language models
such as BERT (and its plethora of variants) for RE. Our experiments over two
annotated corpora, KnowledgeNet and FewRel, demonstrate the improved accuracy
of our enriched models compared to existing RE approaches. Our best results
reach 92% and 71% of F1 score for KnowledgeNet and FewRel, respectively,
proving the effectiveness of our approach on competitive benchmarks.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Modeling Global Distribution for Federated Learning with Label Distribution Skew. (arXiv:2212.08883v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.08883">http://arxiv.org/abs/2212.08883</a></li>
<li>Code URL: <a href="https://github.com/sheng-t/fedmgd">https://github.com/sheng-t/fedmgd</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2212.08883] Modeling Global Distribution for Federated Learning with Label Distribution Skew](http://arxiv.org/abs/2212.08883) #federate</code></li>
<li>Summary: <p>Federated learning achieves joint training of deep models by connecting
decentralized data sources, which can significantly mitigate the risk of
privacy leakage. However, in a more general case, the distributions of labels
among clients are different, called ``label distribution skew''. Directly
applying conventional federated learning without consideration of label
distribution skew issue significantly hurts the performance of the global
model. To this end, we propose a novel federated learning method, named FedMGD,
to alleviate the performance degradation caused by the label distribution skew
issue. It introduces a global Generative Adversarial Network to model the
global data distribution without access to local datasets, so the global model
can be trained using the global information of data distribution without
privacy leakage. The experimental results demonstrate that our proposed method
significantly outperforms the state-of-the-art on several public benchmarks.
Code is available at \url{https://github.com/Sheng-T/FedMGD}.
</p></li>
</ul>

<h3>Title: On Noisy Evaluation in Federated Hyperparameter Tuning. (arXiv:2212.08930v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.08930">http://arxiv.org/abs/2212.08930</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.08930] On Noisy Evaluation in Federated Hyperparameter Tuning](http://arxiv.org/abs/2212.08930) #federate</code></li>
<li>Summary: <p>Hyperparameter tuning is critical to the success of federated learning
applications. Unfortunately, appropriately selecting hyperparameters is
challenging in federated networks. Issues of scale, privacy, and heterogeneity
introduce noise in the tuning process and make it difficult to evaluate the
performance of various hyperparameters. In this work, we perform the first
systematic study on the effect of noisy evaluation in federated hyperparameter
tuning. We first identify and rigorously explore key sources of noise,
including client subsampling, data and systems heterogeneity, and data privacy.
Surprisingly, our results indicate that even small amounts of noise can
significantly impact tuning methods-reducing the performance of
state-of-the-art approaches to that of naive baselines. To address noisy
evaluation in such scenarios, we propose a simple and effective approach that
leverages public proxy data to boost the evaluation signal. Our work
establishes general challenges, baselines, and best practices for future work
in federated hyperparameter tuning.
</p></li>
</ul>

<h3>Title: Toward Data Heterogeneity of Federated Learning. (arXiv:2212.08944v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.08944">http://arxiv.org/abs/2212.08944</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.08944] Toward Data Heterogeneity of Federated Learning](http://arxiv.org/abs/2212.08944) #federate</code></li>
<li>Summary: <p>Federated learning is a popular paradigm for machine learning. Ideally,
federated learning works best when all clients share a similar data
distribution. However, it is not always the case in the real world. Therefore,
the topic of federated learning on heterogeneous data has gained more and more
effort from both academia and industry. In this project, we first do extensive
experiments to show how data skew and quantity skew will affect the performance
of state-of-art federated learning algorithms. Then we propose a new algorithm
FedMix which adjusts existing federated learning algorithms and we show its
performance. We find that existing state-of-art algorithms such as FedProx and
FedNova do not have a significant improvement in all testing cases. But by
testing the existing and new algorithms, it seems that tweaking the client side
is more effective than tweaking the server side.
</p></li>
</ul>

<h3>Title: Enhancing Cyber Resilience of Networked Microgrids using Vertical Federated Reinforcement Learning. (arXiv:2212.08973v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.08973">http://arxiv.org/abs/2212.08973</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.08973] Enhancing Cyber Resilience of Networked Microgrids using Vertical Federated Reinforcement Learning](http://arxiv.org/abs/2212.08973) #federate</code></li>
<li>Summary: <p>This paper presents a novel federated reinforcement learning (Fed-RL)
methodology to enhance the cyber resiliency of networked microgrids. We
formulate a resilient reinforcement learning (RL) training setup which (a)
generates episodic trajectories injecting adversarial actions at primary
control reference signals of the grid forming (GFM) inverters and (b) trains
the RL agents (or controllers) to alleviate the impact of the injected
adversaries. To circumvent data-sharing issues and concerns for proprietary
privacy in multi-party-owned networked grids, we bring in the aspects of
federated machine learning and propose a novel Fed-RL algorithm to train the RL
agents. To this end, the conventional horizontal Fed-RL approaches using
decoupled independent environments fail to capture the coupled dynamics in a
networked microgrid, which leads us to propose a multi-agent vertically
federated variation of actor-critic algorithms, namely federated soft
actor-critic (FedSAC) algorithm. We created a customized simulation setup
encapsulating microgrid dynamics in the GridLAB-D/HELICS co-simulation platform
compatible with the OpenAI Gym interface for training RL agents. Finally, the
proposed methodology is validated with numerical examples of modified IEEE
123-bus benchmark test systems consisting of three coupled microgrids.
</p></li>
</ul>

<h2>fair</h2>
<h3>Title: AutoSlicer: Scalable Automated Data Slicing for ML Model Analysis. (arXiv:2212.09032v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.09032">http://arxiv.org/abs/2212.09032</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.09032] AutoSlicer: Scalable Automated Data Slicing for ML Model Analysis](http://arxiv.org/abs/2212.09032) #fair</code></li>
<li>Summary: <p>Automated slicing aims to identify subsets of evaluation data where a trained
model performs anomalously. This is an important problem for machine learning
pipelines in production since it plays a key role in model debugging and
comparison, as well as the diagnosis of fairness issues. Scalability has become
a critical requirement for any automated slicing system due to the large search
space of possible slices and the growing scale of data. We present Autoslicer,
a scalable system that searches for problematic slices through distributed
metric computation and hypothesis testing. We develop an efficient strategy
that reduces the search space through pruning and prioritization. In the
experiments, we show that our search strategy finds most of the anomalous
slices by inspecting a small portion of the search space.
</p></li>
</ul>

<h2>interpretability</h2>
<h3>Title: Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale. (arXiv:2212.09095v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.09095">http://arxiv.org/abs/2212.09095</a></li>
<li>Code URL: <a href="https://github.com/amazon-science/llm-interpret">https://github.com/amazon-science/llm-interpret</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2212.09095] Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale](http://arxiv.org/abs/2212.09095) #interpretability</code></li>
<li>Summary: <p>Language models have been shown to perform better with an increase in scale
on a wide variety of tasks via the in-context learning paradigm. In this paper,
we investigate the hypothesis that the ability of a large language model to
in-context learn-perform a task is not uniformly spread across all of its
underlying components. Using a 66 billion parameter language model (OPT-66B)
across a diverse set of 14 downstream tasks, we find this is indeed the case:
$\sim$70% of attention heads and $\sim$20% of feed forward networks can be
removed with minimal decline in task performance. We find substantial overlap
in the set of attention heads (un)important for in-context learning across
tasks and number of in-context examples. We also address our hypothesis through
a task-agnostic lens, finding that a small set of attention heads in OPT-66B
score highly on their ability to perform primitive induction operations
associated with in-context learning, namely, prefix matching and copying. These
induction heads overlap with task-specific important heads, suggesting that
induction heads are among the heads capable of more sophisticated behaviors
associated with in-context learning. Overall, our study provides several
insights that indicate large language models may be under-trained to perform
in-context learning and opens up questions on how to pre-train language models
to more effectively perform in-context learning.
</p></li>
</ul>

<h2>explainability</h2>
<h3>Title: Bort: Towards Explainable Neural Networks with Bounded Orthogonal Constraint. (arXiv:2212.09062v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.09062">http://arxiv.org/abs/2212.09062</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.09062] Bort: Towards Explainable Neural Networks with Bounded Orthogonal Constraint](http://arxiv.org/abs/2212.09062) #explainability</code></li>
<li>Summary: <p>Deep learning has revolutionized human society, yet the black-box nature of
deep neural networks hinders further application to reliability-demanded
industries. In the attempt to unpack them, many works observe or impact
internal variables to improve the model's comprehensibility and transparency.
However, existing methods rely on intuitive assumptions and lack mathematical
guarantees. To bridge this gap, we introduce Bort, an optimizer for improving
model explainability with boundedness and orthogonality constraints on model
parameters, derived from the sufficient conditions of model comprehensibility
and transparency. We perform reconstruction and backtracking on the model
representations optimized by Bort and observe an evident improvement in model
explainability. Based on Bort, we are able to synthesize explainable
adversarial samples without additional parameters and training. Surprisingly,
we find Bort constantly improves the classification accuracy of various
architectures including ResNet and DeiT on MNIST, CIFAR-10, and ImageNet.
</p></li>
</ul>

<h3>Title: Natural Language to Code Generation in Interactive Data Science Notebooks. (arXiv:2212.09248v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.09248">http://arxiv.org/abs/2212.09248</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.09248] Natural Language to Code Generation in Interactive Data Science Notebooks](http://arxiv.org/abs/2212.09248) #explainability</code></li>
<li>Summary: <p>Computational notebooks, such as Jupyter notebooks, are interactive computing
environments that are ubiquitous among data scientists to perform data
wrangling and analytic tasks. To measure the performance of AI pair programmers
that automatically synthesize programs for those tasks given natural language
(NL) intents from users, we build ARCADE, a benchmark of 1082 code generation
problems using the pandas data analysis framework in data science notebooks.
ARCADE features multiple rounds of NL-to-code problems from the same notebook.
It requires a model to understand rich multi-modal contexts, such as existing
notebook cells and their execution states as well as previous turns of
interaction. To establish a strong baseline on this challenging task, we
develop PaChiNCo, a 62B code language model (LM) for Python computational
notebooks, which significantly outperforms public code LMs. Finally, we explore
few-shot prompting strategies to elicit better code with step-by-step
decomposition and NL explanation, showing the potential to improve the
diversity and explainability of model predictions.
</p></li>
</ul>

<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Uncovering the Disentanglement Capability in Text-to-Image Diffusion Models. (arXiv:2212.08698v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.08698">http://arxiv.org/abs/2212.08698</a></li>
<li>Code URL: <a href="https://github.com/ucsb-nlp-chang/diffusiondisentanglement">https://github.com/ucsb-nlp-chang/diffusiondisentanglement</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2212.08698] Uncovering the Disentanglement Capability in Text-to-Image Diffusion Models](http://arxiv.org/abs/2212.08698) #diffusion</code></li>
<li>Summary: <p>Generative models have been widely studied in computer vision. Recently,
diffusion models have drawn substantial attention due to the high quality of
their generated images. A key desired property of image generative models is
the ability to disentangle different attributes, which should enable
modification towards a style without changing the semantic content, and the
modification parameters should generalize to different images. Previous studies
have found that generative adversarial networks (GANs) are inherently endowed
with such disentanglement capability, so they can perform disentangled image
editing without re-training or fine-tuning the network. In this work, we
explore whether diffusion models are also inherently equipped with such a
capability. Our finding is that for stable diffusion models, by partially
changing the input text embedding from a neutral description (e.g., "a photo of
person") to one with style (e.g., "a photo of person with smile") while fixing
all the Gaussian random noises introduced during the denoising process, the
generated images can be modified towards the target style without changing the
semantic content. Based on this finding, we further propose a simple,
light-weight image editing algorithm where the mixing weights of the two text
embeddings are optimized for style matching and content preservation. This
entire process only involves optimizing over around 50 parameters and does not
fine-tune the diffusion model itself. Experiments show that the proposed method
can modify a wide range of attributes, with the performance outperforming
diffusion-model-based image-editing algorithms that require fine-tuning. The
optimized weights generalize well to different images. Our code is publicly
available at https://github.com/UCSB-NLP-Chang/DiffusionDisentanglement.
</p></li>
</ul>

<h3>Title: Point-E: A System for Generating 3D Point Clouds from Complex Prompts. (arXiv:2212.08751v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.08751">http://arxiv.org/abs/2212.08751</a></li>
<li>Code URL: <a href="https://github.com/openai/point-e">https://github.com/openai/point-e</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2212.08751] Point-E: A System for Generating 3D Point Clouds from Complex Prompts](http://arxiv.org/abs/2212.08751) #diffusion</code></li>
<li>Summary: <p>While recent work on text-conditional 3D object generation has shown
promising results, the state-of-the-art methods typically require multiple
GPU-hours to produce a single sample. This is in stark contrast to
state-of-the-art generative image models, which produce samples in a number of
seconds or minutes. In this paper, we explore an alternative method for 3D
object generation which produces 3D models in only 1-2 minutes on a single GPU.
Our method first generates a single synthetic view using a text-to-image
diffusion model, and then produces a 3D point cloud using a second diffusion
model which conditions on the generated image. While our method still falls
short of the state-of-the-art in terms of sample quality, it is one to two
orders of magnitude faster to sample from, offering a practical trade-off for
some use cases. We release our pre-trained point cloud diffusion models, as
well as evaluation code and models, at https://github.com/openai/point-e.
</p></li>
</ul>

<h3>Title: DAG: Depth-Aware Guidance with Denoising Diffusion Probabilistic Models. (arXiv:2212.08861v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.08861">http://arxiv.org/abs/2212.08861</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2212.08861] DAG: Depth-Aware Guidance with Denoising Diffusion Probabilistic Models](http://arxiv.org/abs/2212.08861) #diffusion</code></li>
<li>Summary: <p>In recent years, generative models have undergone significant advancement due
to the success of diffusion models. The success of these models is often
attributed to their use of guidance techniques, such as classifier and
classifier-free methods, which provides effective mechanisms to trade-off
between fidelity and diversity. However, these methods are not capable of
guiding a generated image to be aware of its geometric configuration, e.g.,
depth, which hinders the application of diffusion models to areas that require
a certain level of depth awareness. To address this limitation, we propose a
novel guidance approach for diffusion models that uses estimated depth
information derived from the rich intermediate representations of diffusion
models. To do this, we first present a label-efficient depth estimation
framework using the internal representations of diffusion models. At the
sampling phase, we utilize two guidance techniques to self-condition the
generated image using the estimated depth map, the first of which uses
pseudo-labeling, and the subsequent one uses a depth-domain diffusion prior.
Experiments and extensive ablation studies demonstrate the effectiveness of our
method in guiding the diffusion models toward geometrically plausible image
generation. Project page is available at https://ku-cvlab.github.io/DAG/.
</p></li>
</ul>

<h3>Title: Leveraging Wastewater Monitoring for COVID-19 Forecasting in the US: a Deep Learning study. (arXiv:2212.08798v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2212.08798">http://arxiv.org/abs/2212.08798</a></li>
<li>Code URL: <a href="https://github.com/mehrdadfazli/deeplearning-covid19-wastewater">https://github.com/mehrdadfazli/deeplearning-covid19-wastewater</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2212.08798] Leveraging Wastewater Monitoring for COVID-19 Forecasting in the US: a Deep Learning study](http://arxiv.org/abs/2212.08798) #diffusion</code></li>
<li>Summary: <p>The outburst of COVID-19 in late 2019 was the start of a health crisis that
shook the world and took millions of lives in the ensuing years. Many
governments and health officials failed to arrest the rapid circulation of
infection in their communities. The long incubation period and the large
proportion of asymptomatic cases made COVID-19 particularly elusive to track.
However, wastewater monitoring soon became a promising data source in addition
to conventional indicators such as confirmed daily cases, hospitalizations, and
deaths. Despite the consensus on the effectiveness of wastewater viral load
data, there is a lack of methodological approaches that leverage viral load to
improve COVID-19 forecasting. This paper proposes using deep learning to
automatically discover the relationship between daily confirmed cases and viral
load data. We trained one Deep Temporal Convolutional Networks (DeepTCN) and
one Temporal Fusion Transformer (TFT) model to build a global forecasting
model. We supplement the daily confirmed cases with viral loads and other
socio-economic factors as covariates to the models. Our results suggest that
TFT outperforms DeepTCN and learns a better association between viral load and
daily cases. We demonstrated that equipping the models with the viral load
improves their forecasting performance significantly. Moreover, viral load is
shown to be the second most predictive input, following the containment and
health index. Our results reveal the feasibility of training a
location-agnostic deep-learning model to capture the dynamics of infection
diffusion when wastewater viral load data is provided.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
