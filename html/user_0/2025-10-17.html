<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-10-17</h1>
<h3>Title: Large Language Models for Real-World IoT Device Identification</h3>
<ul>
<li><strong>Authors: </strong>Rameen Mahmood, Tousif Ahmed, Sai Teja Peddinti, Danny Yuxing Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13817">https://arxiv.org/abs/2510.13817</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13817">https://arxiv.org/pdf/2510.13817</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13817]] Large Language Models for Real-World IoT Device Identification(https://arxiv.org/abs/2510.13817)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, large language model</a></li>
<li><strong>Abstract: </strong>The rapid expansion of IoT devices has outpaced current identification methods, creating significant risks for security, privacy, and network accountability. These challenges are heightened in open-world environments, where traffic metadata is often incomplete, noisy, or intentionally obfuscated. We introduce a semantic inference pipeline that reframes device identification as a language modeling task over heterogeneous network metadata. To construct reliable supervision, we generate high-fidelity vendor labels for the IoT Inspector dataset, the largest real-world IoT traffic corpus, using an ensemble of large language models guided by mutual-information and entropy-based stability scores. We then instruction-tune a quantized LLaMA3.18B model with curriculum learning to support generalization under sparsity and long-tail vendor distributions. Our model achieves 98.25% top-1 accuracy and 90.73% macro accuracy across 2,015 vendors while maintaining resilience to missing fields, protocol drift, and adversarial manipulation. Evaluation on an independent IoT testbed, coupled with explanation quality and adversarial stress tests, demonstrates that instruction-tuned LLMs provide a scalable and interpretable foundation for real-world device identification at scale.</li>
</ul>

<h3>Title: Noisy Networks, Nosy Neighbors: Inferring Privacy Invasive Information from Encrypted Wireless Traffic</h3>
<ul>
<li><strong>Authors: </strong>Bartosz Burgiel</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13822">https://arxiv.org/abs/2510.13822</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13822">https://arxiv.org/pdf/2510.13822</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13822]] Noisy Networks, Nosy Neighbors: Inferring Privacy Invasive Information from Encrypted Wireless Traffic(https://arxiv.org/abs/2510.13822)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>This thesis explores the extent to which passive observation of wireless traffic in a smart home environment can be used to infer privacy-invasive information about its inhabitants. Using a setup that mimics the capabilities of a nosy neighbor in an adjacent flat, we analyze raw 802.11 packets and Bluetooth Low Energy advertisemets. From this data, we identify devices, infer their activity states and approximate their location using RSSI-based trilateration. Despite the encrypted nature of the data, we demonstrate that it is possible to detect active periods of multimedia devices, infer common activities such as sleeping, working and consuming media, and even approximate the layout of the neighbor's apartment. Our results show that privacy risks in smart homes extend beyond traditional data breaches: a nosy neighbor behind the wall can gain privacy-invasive insights into the lives of their neighbors purely from encrypted network traffic.</li>
</ul>

<h3>Title: Multi-Layer Secret Sharing for Cross-Layer Attack Defense in 5G Networks: a COTS UE Demonstration</h3>
<ul>
<li><strong>Authors: </strong>Wai Ming Chan, Remi Chou, Taejoon Kim</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13824">https://arxiv.org/abs/2510.13824</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13824">https://arxiv.org/pdf/2510.13824</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13824]] Multi-Layer Secret Sharing for Cross-Layer Attack Defense in 5G Networks: a COTS UE Demonstration(https://arxiv.org/abs/2510.13824)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>This demo presents the first implementation of multi-layer secret sharing on commercial-off-the-shelf (COTS) 5G user equipment (UE), operating without infrastructure modifications or pre-shared keys. Our XOR-based approach distributes secret shares across network operators and distributed relays, ensuring perfect recovery and data confidentiality even if one network operator and one relay are simultaneously lost (e.g., under denial of service (DoS) or unanticipated attacks).</li>
</ul>

<h3>Title: A2AS: Agentic AI Runtime Security and Self-Defense</h3>
<ul>
<li><strong>Authors: </strong>Eugene Neelou, Ivan Novikov, Max Moroz, Om Narayan, Tiffany Saade, Mika Ayenson, Ilya Kabanov, Jen Ozmen, Edward Lee, Vineeth Sai Narajala, Emmanuel Guilherme Junior, Ken Huang, Huseyin Gulsin, Jason Ross, Marat Vyshegorodtsev, Adelin Travers, Idan Habler, Rahul Jadav</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13825">https://arxiv.org/abs/2510.13825</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13825">https://arxiv.org/pdf/2510.13825</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13825]] A2AS: Agentic AI Runtime Security and Self-Defense(https://arxiv.org/abs/2510.13825)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense</a></li>
<li><strong>Abstract: </strong>The A2AS framework is introduced as a security layer for AI agents and LLM-powered applications, similar to how HTTPS secures HTTP. A2AS enforces certified behavior, activates model self-defense, and ensures context window integrity. It defines security boundaries, authenticates prompts, applies security rules and custom policies, and controls agentic behavior, enabling a defense-in-depth strategy. The A2AS framework avoids latency overhead, external dependencies, architectural changes, model retraining, and operational complexity. The BASIC security model is introduced as the A2AS foundation: (B) Behavior certificates enable behavior enforcement, (A) Authenticated prompts enable context window integrity, (S) Security boundaries enable untrusted input isolation, (I) In-context defenses enable secure model reasoning, (C) Codified policies enable application-specific rules. This first paper in the series introduces the BASIC security model and the A2AS framework, exploring their potential toward establishing the A2AS industry standard.</li>
</ul>

<h3>Title: From Explainability to Action: A Generative Operational Framework for Integrating XAI in Clinical Mental Health Screening</h3>
<ul>
<li><strong>Authors: </strong>Ratna Kandala, Akshata Kishore Moharir, Divya Arvinda Nayak</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13828">https://arxiv.org/abs/2510.13828</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13828">https://arxiv.org/pdf/2510.13828</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13828]] From Explainability to Action: A Generative Operational Framework for Integrating XAI in Clinical Mental Health Screening(https://arxiv.org/abs/2510.13828)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, generative, large language model</a></li>
<li><strong>Abstract: </strong>Explainable Artificial Intelligence (XAI) has been presented as the critical component for unlocking the potential of machine learning in mental health screening (MHS). However, a persistent lab-to-clinic gap remains. Current XAI techniques, such as SHAP and LIME, excel at producing technically faithful outputs such as feature importance scores, but fail to deliver clinically relevant, actionable insights that can be used by clinicians or understood by patients. This disconnect between technical transparency and human utility is the primary barrier to real-world adoption. This paper argues that this gap is a translation problem and proposes the Generative Operational Framework, a novel system architecture that leverages Large Language Models (LLMs) as a central translation engine. This framework is designed to ingest the raw, technical outputs from diverse XAI tools and synthesize them with clinical guidelines (via RAG) to automatically generate human-readable, evidence-backed clinical narratives. To justify our solution, we provide a systematic analysis of the components it integrates, tracing the evolution from intrinsic models to generative XAI. We demonstrate how this framework directly addresses key operational barriers, including workflow integration, bias mitigation, and stakeholder-specific communication. This paper also provides a strategic roadmap for moving the field beyond the generation of isolated data points toward the delivery of integrated, actionable, and trustworthy AI in clinical practice.</li>
</ul>

<h3>Title: A Linguistics-Aware LLM Watermarking via Syntactic Predictability</h3>
<ul>
<li><strong>Authors: </strong>Shinwoo Park, Hyejin Park, Hyeseon Ahn, Yo-Sub Han</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13829">https://arxiv.org/abs/2510.13829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13829">https://arxiv.org/pdf/2510.13829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13829]] A Linguistics-Aware LLM Watermarking via Syntactic Predictability(https://arxiv.org/abs/2510.13829)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, watermark, large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) continue to advance rapidly, reliable governance tools have become critical. Publicly verifiable watermarking is particularly essential for fostering a trustworthy AI ecosystem. A central challenge persists: balancing text quality against detection robustness. Recent studies have sought to navigate this trade-off by leveraging signals from model output distributions (e.g., token-level entropy); however, their reliance on these model-specific signals presents a significant barrier to public verification, as the detection process requires access to the logits of the underlying model. We introduce STELA, a novel framework that aligns watermark strength with the linguistic degrees of freedom inherent in language. STELA dynamically modulates the signal using part-of-speech (POS) n-gram-modeled linguistic indeterminacy, weakening it in grammatically constrained contexts to preserve quality and strengthen it in contexts with greater linguistic flexibility to enhance detectability. Our detector operates without access to any model logits, thus facilitating publicly verifiable detection. Through extensive experiments on typologically diverse languages-analytic English, isolating Chinese, and agglutinative Korean-we show that STELA surpasses prior methods in detection robustness. Our code is available at this https URL.</li>
</ul>

<h3>Title: Users as Annotators: LLM Preference Learning from Comparison Mode</h3>
<ul>
<li><strong>Authors: </strong>Zhongze Cai, Xiaocheng Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13830">https://arxiv.org/abs/2510.13830</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13830">https://arxiv.org/pdf/2510.13830</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13830]] Users as Annotators: LLM Preference Learning from Comparison Mode(https://arxiv.org/abs/2510.13830)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Pairwise preference data have played an important role in the alignment of large language models (LLMs). Each sample of such data consists of a prompt, two different responses to the prompt, and a binary label indicating which of the two responses is better. The labels are usually annotated by professional human annotators. In this paper, we consider an alternative approach to collect pairwise preference data -- user annotation from comparison mode. With the increasingly wider adoption of LLMs among the population, users are contributing more and more of their preference labels through their daily interactions with the LLMs. The upside of such labels is that users are the best experts in judging the responses to their own queries/prompts, but the downside is the lack of quality control in these labels. In this paper, we consider a new idea of generating two responses from two different models or two different versions of the same model. The asymmetry allows us to make an inference of the user's data quality through our proposed user behavior model. We develop an expectation-maximization algorithm to estimate a latent quality factor of the user, and filter users' annotation data accordingly. The downstream task shows the effectiveness of our approach in both capturing the user behavior and data filtering for LLM alignment.</li>
</ul>

<h3>Title: Informed Routing in LLMs: Smarter Token-Level Computation for Faster Inference</h3>
<ul>
<li><strong>Authors: </strong>Chao Han, Yijuan Liang, Zihao Xuan, Daokuan Wu, Wei Zhang, Xiaoyu Shen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13831">https://arxiv.org/abs/2510.13831</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13831">https://arxiv.org/pdf/2510.13831</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13831]] Informed Routing in LLMs: Smarter Token-Level Computation for Faster Inference(https://arxiv.org/abs/2510.13831)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The deployment of large language models (LLMs) in real-world applications is increasingly limited by their high inference cost. While recent advances in dynamic token-level computation allocation attempt to improve efficiency by selectively activating model components per token, existing methods rely on greedy routing--a myopic execute-or-skip mechanism that often leads to irreversible information loss and suboptimal token selection. This paper introduces informed routing, a new paradigm that proactively addresses these issues. The key insight is to assess not only a token's immediate importance but also its recoverability, i.e., how well its transformation can be approximated. To this end, we propose the Lightweight Feature Forecaster (LFF), a small predictive module that estimates a unit's output before routing decisions are made. This enables a flexible execute-or-approximate policy that preserves model fidelity while drastically reducing computation. Extensive experiments on both language modeling and reasoning tasks show that informed routing achieves state-of-the-art efficiency-performance trade-offs across multiple sparsity levels. Notably, even without final LoRA fine-tuning, our method matches or surpasses strong baselines that require full fine-tuning, all while reducing training time by over 50%. The code is available at: this https URL</li>
</ul>

<h3>Title: Entropy Meets Importance: A Unified Head Importance-Entropy Score for Stable and Efficient Transformer Pruning</h3>
<ul>
<li><strong>Authors: </strong>Minsik Choi, Hyegang Son, Changhoon Kim, Young Geun Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13832">https://arxiv.org/abs/2510.13832</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13832">https://arxiv.org/pdf/2510.13832</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13832]] Entropy Meets Importance: A Unified Head Importance-Entropy Score for Stable and Efficient Transformer Pruning(https://arxiv.org/abs/2510.13832)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Transformer-based models have achieved remarkable performance in NLP tasks. However, their structural characteristics-multiple layers and attention heads-introduce efficiency challenges in inference and deployment. To address these challenges, various pruning methods have recently been proposed. Notably, gradient-based methods using Head Importance Scores (HIS) have gained traction for interpretability, efficiency, and ability to identify redundant heads. However, HIS alone has limitations as it captures only the gradient-driven contribution, overlooking the diversity of attention patterns. To overcome these limitations, we introduce a novel pruning criterion, HIES (Head Importance-Entropy Score), which integrates head importance scores with attention entropy, providing complementary evidence on per-head contribution. Empirically, HIES-based pruning yields up to 15.2% improvement in model quality and 2.04x improvement in stability over HIS-only methods, enabling substantial model compression without sacrificing either accuracy or stability. Code will be released upon publication.</li>
</ul>

<h3>Title: SIMBA UQ: Similarity-Based Aggregation for Uncertainty Quantification in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Debarun Bhattacharjya, Balaji Ganesan, Junkyu Lee, Radu Marinescu, Katsiaryna Mirylenka, Michael Glass, Xiao Shou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13836">https://arxiv.org/abs/2510.13836</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13836">https://arxiv.org/pdf/2510.13836</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13836]] SIMBA UQ: Similarity-Based Aggregation for Uncertainty Quantification in Large Language Models(https://arxiv.org/abs/2510.13836)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>When does a large language model (LLM) know what it does not know? Uncertainty quantification (UQ) provides measures of uncertainty, such as an estimate of the confidence in an LLM's generated output, and is therefore increasingly recognized as a crucial component of trusted AI systems. Black-box UQ methods do not require access to internal model information from the generating LLM and therefore have numerous real-world advantages, such as robustness to system changes, adaptability to choice of LLM, reduced costs, and computational tractability. In this paper, we investigate the effectiveness of UQ techniques that are primarily but not necessarily entirely black-box, where the consistency between a generated output and other sampled generations is used as a proxy for confidence in its correctness. We propose a high-level non-verbalized similarity-based aggregation framework that subsumes a broad swath of UQ approaches suitable for complex generative tasks, as well as introduce specific novel techniques from the framework that train confidence estimation models using small training sets. Through an empirical study with datasets spanning the diverse tasks of question answering, summarization, and text-to-SQL, we demonstrate that our proposed similarity-based methods can yield better calibrated confidences than baselines.</li>
</ul>

<h3>Title: Meronymic Ontology Extraction via Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Dekai Zhang, Simone Conia, Antonio Rago</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13839">https://arxiv.org/abs/2510.13839</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13839">https://arxiv.org/pdf/2510.13839</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13839]] Meronymic Ontology Extraction via Large Language Models(https://arxiv.org/abs/2510.13839)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Ontologies have become essential in today's digital age as a way of organising the vast amount of readily available unstructured text. In providing formal structure to this information, ontologies have immense value and application across various domains, e.g., e-commerce, where countless product listings necessitate proper product organisation. However, the manual construction of these ontologies is a time-consuming, expensive and laborious process. In this paper, we harness the recent advancements in large language models (LLMs) to develop a fully-automated method of extracting product ontologies, in the form of meronymies, from raw review texts. We demonstrate that the ontologies produced by our method surpass an existing, BERT-based baseline when evaluating using an LLM-as-a-judge. Our investigation provides the groundwork for LLMs to be used more generally in (product or otherwise) ontology extraction.</li>
</ul>

<h3>Title: ADMIT: Few-shot Knowledge Poisoning Attacks on RAG-based Fact Checking</h3>
<ul>
<li><strong>Authors: </strong>Yutao Wu, Xiao Liu, Yinghui Li, Yifeng Gao, Yifan Ding, Jiale Ding, Xiang Zheng, Xingjun Ma</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13842">https://arxiv.org/abs/2510.13842</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13842">https://arxiv.org/pdf/2510.13842</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13842]] ADMIT: Few-shot Knowledge Poisoning Attacks on RAG-based Fact Checking(https://arxiv.org/abs/2510.13842)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>Knowledge poisoning poses a critical threat to Retrieval-Augmented Generation (RAG) systems by injecting adversarial content into knowledge bases, tricking Large Language Models (LLMs) into producing attacker-controlled outputs grounded in manipulated context. Prior work highlights LLMs' susceptibility to misleading or malicious retrieved content. However, real-world fact-checking scenarios are more challenging, as credible evidence typically dominates the retrieval pool. To investigate this problem, we extend knowledge poisoning to the fact-checking setting, where retrieved context includes authentic supporting or refuting evidence. We propose \textbf{ADMIT} (\textbf{AD}versarial \textbf{M}ulti-\textbf{I}njection \textbf{T}echnique), a few-shot, semantically aligned poisoning attack that flips fact-checking decisions and induces deceptive justifications, all without access to the target LLMs, retrievers, or token-level control. Extensive experiments show that ADMIT transfers effectively across 4 retrievers, 11 LLMs, and 4 cross-domain benchmarks, achieving an average attack success rate (ASR) of 86\% at an extremely low poisoning rate of $0.93 \times 10^{-6}$, and remaining robust even in the presence of strong counter-evidence. Compared with prior state-of-the-art attacks, ADMIT improves ASR by 11.2\% across all settings, exposing significant vulnerabilities in real-world RAG-based fact-checking systems.</li>
</ul>

<h3>Title: DynaSpec: Context-aware Dynamic Speculative Sampling for Large-Vocabulary Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jinbin Zhang, Nasib Ullah, Erik Schultheis, Rohit Babbar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13847">https://arxiv.org/abs/2510.13847</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13847">https://arxiv.org/pdf/2510.13847</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13847]] DynaSpec: Context-aware Dynamic Speculative Sampling for Large-Vocabulary Language Models(https://arxiv.org/abs/2510.13847)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Speculative decoding (a.k.a. speculative sampling) has become a standard way to accelerate LLM inference: a small drafter proposes multiple tokens and a large target model verifies them once per speculation length. Recently, scaling of the LLM vocabulary has pushed the number of tokens to grow substantially. While verification over the full vocabulary leaves the target model largely unaffected, the O(|V|d) parameters in the drafter's output head become a latency bottleneck, slowing the entire pipeline. Contemporary methods (e.g., FR-Spec, VocabTrim) restrict the drafter's vocabulary to a fixed subset of the target model's vocabulary, ranked in descending order of token frequency. Although this reduces draft-time compute, it is brittle, since: (i) frequency lists are corpus-dependent and require retuning to generalize, and (ii) static shortlists suppress rare or domain-specific tokens, lowering the expected number of tokens per verification step. We propose DynaSpec, a context-dependent dynamic shortlisting mechanism that is robust, speeds up drafting, and generalizes across diverse tasks. Concretely, we introduce lightweight, coarse-grained meta-classifiers that route contexts to a small number of token clusters; the union of the top-k selected clusters forms the drafter's shortlist, while verification retains the full vocabulary and exactness. The meta-classifier finishes its computation earlier than the drafter's hidden state generation by exploiting parallel execution of draft encoding and meta shortlisting on separate streams. On standard speculative-decoding benchmarks, we observe consistent gains in mean accepted length over fixed-shortlist baselines, while context-dependent selection enables smaller shortlists without degrading acceptance.</li>
</ul>

<h3>Title: On-device System of Compositional Multi-tasking in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Ondrej Bohdal, Konstantinos Theodosiadis, Asterios Mpatziakas, Dimitris Filippidis, Iro Spyrou, Christos Zonios, Anastasios Drosou, Dimosthenis Ioannidis, Kyeng-Hun Lee, Jijoong Moon, Hyeonmok Ko, Mete Ozay, Umberto Michieli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13848">https://arxiv.org/abs/2510.13848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13848">https://arxiv.org/pdf/2510.13848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13848]] On-device System of Compositional Multi-tasking in Large Language Models(https://arxiv.org/abs/2510.13848)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are commonly adapted for diverse downstream tasks via parameter-efficient fine-tuning techniques such as Low-Rank Adapters (LoRA). While adapters can be combined to handle multiple tasks separately, standard approaches struggle when targeting the simultaneous execution of complex tasks, such as generating a translated summary from a long conversation. To address this challenge, we propose a novel approach tailored specifically for compositional multi-tasking scenarios involving summarization and translation. Our technique involves adding a learnable projection layer on top of the combined summarization and translation adapters. This design enables effective integration while maintaining efficiency through reduced computational overhead compared to alternative strategies requiring extensive retraining or sequential processing. We demonstrate the practical viability of our method within an on-device environment by developing an Android app capable of executing compositional tasks seamlessly. Experimental results indicate our solution performs well and is fast in both cloud-based and on-device implementations, highlighting the potential benefits of adopting our framework in real-world applications demanding high-speed operation alongside resource constraints.</li>
</ul>

<h3>Title: Language steering in latent space to mitigate unintended code-switching</h3>
<ul>
<li><strong>Authors: </strong>Andrey Goncharov, Nikolai Kondusov, Alexey Zaytsev</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13849">https://arxiv.org/abs/2510.13849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13849">https://arxiv.org/pdf/2510.13849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13849]] Language steering in latent space to mitigate unintended code-switching(https://arxiv.org/abs/2510.13849)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multilingual Large Language Models (LLMs) often exhibit unintended code-switching, reducing reliability in downstream tasks. We propose latent-space language steering, a lightweight inference-time method that identifies language directions via PCA on parallel translations and steers token embeddings along these axes to control language identity. Our approach mitigates code-switching while preserving semantics with negligible computational overhead and requires only minimal parallel data for calibration. Empirically, we achieve 95-99\% language classification accuracy using a single principal component and reduce next-token distributional divergence by up to 42% across multiple language pairs on Qwen2.5 and Llama-3.2 models. We further analyze the layer-wise evolution of language representations, revealing that language identity concentrates in final layers with near-perfect linear separability.</li>
</ul>

<h3>Title: Revisiting the UID Hypothesis in LLM Reasoning Traces</h3>
<ul>
<li><strong>Authors: </strong>Minju Gwak, Guijin Son, Jaehyung Kim</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13850">https://arxiv.org/abs/2510.13850</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13850">https://arxiv.org/pdf/2510.13850</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13850]] Revisiting the UID Hypothesis in LLM Reasoning Traces(https://arxiv.org/abs/2510.13850)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) often solve problems using step-by-step Chain-of-Thought (CoT) reasoning, yet these intermediate steps are frequently unfaithful or hard to interpret. Inspired by the Uniform Information Density (UID) hypothesis in psycholinguistics -- which posits that humans communicate by maintaining a stable flow of information -- we introduce entropy-based metrics to analyze the information flow within reasoning traces. Surprisingly, across three challenging mathematical benchmarks, we find that successful reasoning in LLMs is globally non-uniform: correct solutions are characterized by uneven swings in information density, in stark contrast to human communication patterns. This result challenges assumptions about machine reasoning and suggests new directions for designing interpretable and adaptive reasoning models.</li>
</ul>

<h3>Title: EvoEdit: Evolving Null-space Alignment for Robust and Efficient Knowledge Editing</h3>
<ul>
<li><strong>Authors: </strong>Sicheng Lyu, Yu Gu, Xinyu Wang, Jerry Huang, Sitao Luan, Yufei Cui, Xiao-Wen Chang, Peng Lu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13851">https://arxiv.org/abs/2510.13851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13851">https://arxiv.org/pdf/2510.13851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13851]] EvoEdit: Evolving Null-space Alignment for Robust and Efficient Knowledge Editing(https://arxiv.org/abs/2510.13851)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) require continual updates to rectify outdated or erroneous knowledge. Model editing has emerged as a compelling paradigm for introducing targeted modifications without the computational burden of full retraining. Existing approaches are mainly based on a locate-then-edit framework. However, in sequential editing contexts, where multiple updates are applied over time, they exhibit significant limitations and suffer from catastrophic interference, i.e., new edits compromise previously integrated updates and degrade preserved knowledge. To address these challenges, we introduce EvoEdit, a novel editing strategy that mitigates catastrophic interference through sequential null-space alignment, enabling stable and efficient model editing. By performing sequential null-space alignment for each incoming edit, EvoEdit preserves both original and previously modified knowledge representations and maintains output invariance on preserved knowledge even across long edit sequences, effectively mitigating interference. Evaluations on real-world sequential knowledge-editing benchmarks show that EvoEdit achieves better or comparable performance than prior state-of-the-art locate-then-edit techniques, with up to 3.53 times speedup. Overall, these results underscore the necessity of developing more principled approaches for designing LLMs in dynamically evolving information settings, while providing a simple yet effective solution with strong theoretical guarantees.</li>
</ul>

<h3>Title: ConsistencyAI: A Benchmark to Assess LLMs' Factual Consistency When Responding to Different Demographic Groups</h3>
<ul>
<li><strong>Authors: </strong>Peter Banyas, Shristi Sharma, Alistair Simmons, Atharva Vispute</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13852">https://arxiv.org/abs/2510.13852</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13852">https://arxiv.org/pdf/2510.13852</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13852]] ConsistencyAI: A Benchmark to Assess LLMs' Factual Consistency When Responding to Different Demographic Groups(https://arxiv.org/abs/2510.13852)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Is an LLM telling you different facts than it's telling me? This paper introduces ConsistencyAI, an independent benchmark for measuring the factual consistency of large language models (LLMs) for different personas. ConsistencyAI tests whether, when users of different demographics ask identical questions, the model responds with factually inconsistent answers. Designed without involvement from LLM providers, this benchmark offers impartial evaluation and accountability. In our experiment, we queried 19 LLMs with prompts that requested 5 facts for each of 15 topics. We repeated this query 100 times for each LLM, each time adding prompt context from a different persona selected from a subset of personas modeling the general population. We processed the responses into sentence embeddings, computed cross-persona cosine similarity, and computed the weighted average of cross-persona cosine similarity to calculate factual consistency scores. In 100-persona experiments, scores ranged from 0.9065 to 0.7896, and the mean was 0.8656, which we adopt as a benchmark threshold. xAI's Grok-3 is most consistent, while several lightweight models rank lowest. Consistency varies by topic: the job market is least consistent, G7 world leaders most consistent, and issues like vaccines or the Israeli-Palestinian conflict diverge by provider. These results show that both the provider and the topic shape the factual consistency. We release our code and interactive demo to support reproducible evaluation and encourage persona-invariant prompting strategies.</li>
</ul>

<h3>Title: BenchPress: A Human-in-the-Loop Annotation System for Rapid Text-to-SQL Benchmark Curation</h3>
<ul>
<li><strong>Authors: </strong>Fabian Wenz, Omar Bouattour, Devin Yang, Justin Choi, Cecil Gregg, Nesime Tatbul, Çağatay Demiralp</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DB, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13853">https://arxiv.org/abs/2510.13853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13853">https://arxiv.org/pdf/2510.13853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13853]] BenchPress: A Human-in-the-Loop Annotation System for Rapid Text-to-SQL Benchmark Curation(https://arxiv.org/abs/2510.13853)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have been successfully applied to many tasks, including text-to-SQL generation. However, much of this work has focused on publicly available datasets, such as Fiben, Spider, and Bird. Our earlier work showed that LLMs are much less effective in querying large private enterprise data warehouses and released Beaver, the first private enterprise text-to-SQL benchmark. To create Beaver, we leveraged SQL logs, which are often readily available. However, manually annotating these logs to identify which natural language questions they answer is a daunting task. Asking database administrators, who are highly trained experts, to take on additional work to construct and validate corresponding natural language utterances is not only challenging but also quite costly. To address this challenge, we introduce BenchPress, a human-in-the-loop system designed to accelerate the creation of domain-specific text-to-SQL benchmarks. Given a SQL query, BenchPress uses retrieval-augmented generation (RAG) and LLMs to propose multiple natural language descriptions. Human experts then select, rank, or edit these drafts to ensure accuracy and domain alignment. We evaluated BenchPress on annotated enterprise SQL logs, demonstrating that LLM-assisted annotation drastically reduces the time and effort required to create high-quality benchmarks. Our results show that combining human verification with LLM-generated suggestions enhances annotation accuracy, benchmark reliability, and model evaluation robustness. By streamlining the creation of custom benchmarks, BenchPress offers researchers and practitioners a mechanism for assessing text-to-SQL models on a given domain-specific workload. BenchPress is freely available via our public GitHub repository at this https URL and is also accessible on our website at this http URL.</li>
</ul>

<h3>Title: Harnessing Consistency for Robust Test-Time LLM Ensemble</h3>
<ul>
<li><strong>Authors: </strong>Zhichen Zeng, Qi Yu, Xiao Lin, Ruizhong Qiu, Xuying Ning, Tianxin Wei, Yuchen Yan, Jingrui He, Hanghang Tong</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13855">https://arxiv.org/abs/2510.13855</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13855">https://arxiv.org/pdf/2510.13855</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13855]] Harnessing Consistency for Robust Test-Time LLM Ensemble(https://arxiv.org/abs/2510.13855)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Different large language models (LLMs) exhibit diverse strengths and weaknesses, and LLM ensemble serves as a promising approach to integrate their complementary capabilities. Despite substantial progress in improving ensemble quality, limited attention has been paid to the robustness of ensembles against potential erroneous signals, which often arise from heterogeneous tokenization schemes and varying model expertise. Our analysis shows that ensemble failures typically arise from both the token level and the model level: the former reflects severe disagreement in token predictions, while the latter involves low confidence and pronounced disparities among models. In light of this, we propose CoRE, a plug-and-play technique that harnesses model consistency for robust LLM ensemble, which can be seamlessly integrated with diverse ensemble methods. Token-level consistency captures fine-grained disagreements by applying a low-pass filter to downweight uncertain tokens with high inconsistency, often due to token misalignment, thereby improving robustness at a granular level. Model-level consistency models global agreement by promoting model outputs with high self-confidence and minimal divergence from others, enhancing robustness at a coarser level. Extensive experiments across diverse benchmarks, model combinations, and ensemble strategies demonstrate that CoRE consistently improves ensemble performance and robustness.</li>
</ul>

<h3>Title: Multimodal Retrieval-Augmented Generation with Large Language Models for Medical VQA</h3>
<ul>
<li><strong>Authors: </strong>A H M Rezaul Karim, Ozlem Uzuner</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13856">https://arxiv.org/abs/2510.13856</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13856">https://arxiv.org/pdf/2510.13856</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13856]] Multimodal Retrieval-Augmented Generation with Large Language Models for Medical VQA(https://arxiv.org/abs/2510.13856)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Medical Visual Question Answering (MedVQA) enables natural language queries over medical images to support clinical decision-making and patient care. The MEDIQA-WV 2025 shared task addressed wound-care VQA, requiring systems to generate free-text responses and structured wound attributes from images and patient queries. We present the MasonNLP system, which employs a general-domain, instruction-tuned large language model with a retrieval-augmented generation (RAG) framework that incorporates textual and visual examples from in-domain data. This approach grounds outputs in clinically relevant exemplars, improving reasoning, schema adherence, and response quality across dBLEU, ROUGE, BERTScore, and LLM-based metrics. Our best-performing system ranked 3rd among 19 teams and 51 submissions with an average score of 41.37%, demonstrating that lightweight RAG with general-purpose LLMs -- a minimal inference-time layer that adds a few relevant exemplars via simple indexing and fusion, with no extra training or complex re-ranking -- provides a simple and effective baseline for multimodal clinical NLP tasks.</li>
</ul>

<h3>Title: ShishuLM: Lightweight Language Model with Hybrid Decoder-MLP Architecture and Paired Weight Sharing</h3>
<ul>
<li><strong>Authors: </strong>Shivanshu Kumar, Gopalakrishnan Srinivasan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13860">https://arxiv.org/abs/2510.13860</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13860">https://arxiv.org/pdf/2510.13860</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13860]] ShishuLM: Lightweight Language Model with Hybrid Decoder-MLP Architecture and Paired Weight Sharing(https://arxiv.org/abs/2510.13860)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>While the transformer architecture has achieved state-of-the-art performance on natural language processing tasks, these models impose substantial memory and computational overhead. Recent research has identified significant architectural redundancies within these models, presenting opportunities for optimization without compromising performance. Taking insights from research in AI interpretability and inference-time layer pruning, we introduce an efficient language model architecture, referred to as ShishuLM, which reduces both the parameter count and Key-Value (KV) cache requirements. Given the increasing importance of Small Language Models (SLMs) in agentic AI systems, we evaluate our approach on two SLMs of different scales. Our analysis reveals that for moderate-context scenarios, normalization coupled with attention computation is roughly linear with the input, enabling entire transformer blocks to be approximated through Multi-Layer Perceptrons (MLPs). Our results show that ShishuLM provides up to 25% reduction in memory requirements and up to 40% improvement in latency during both training and inference, compared to parent models. Our experimental and analytical findings provide insights towards building more efficient SLM architectures from a pre-training standpoint.</li>
</ul>

<h3>Title: Ensembling Large Language Models to Characterize Affective Dynamics in Student-AI Tutor Dialogues</h3>
<ul>
<li><strong>Authors: </strong>Chenyu Zhang, Sharifa Alghowinem, Cynthia Breazeal</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13862">https://arxiv.org/abs/2510.13862</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13862">https://arxiv.org/pdf/2510.13862</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13862]] Ensembling Large Language Models to Characterize Affective Dynamics in Student-AI Tutor Dialogues(https://arxiv.org/abs/2510.13862)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>While recent studies have examined the leaning impact of large language model (LLM) in educational contexts, the affective dynamics of LLM-mediated tutoring remain insufficiently understood. This work introduces the first ensemble-LLM framework for large-scale affect sensing in tutoring dialogues, advancing the conversation on responsible pathways for integrating generative AI into education by attending to learners' evolving affective states. To achieve this, we analyzed two semesters' worth of 16,986 conversational turns exchanged between PyTutor, an LLM-powered AI tutor, and 261 undergraduate learners across three U.S. institutions. To investigate learners' emotional experiences, we generate zero-shot affect annotations from three frontier LLMs (Gemini, GPT-4o, Claude), including scalar ratings of valence, arousal, and learning-helpfulness, along with free-text emotion labels. These estimates are fused through rank-weighted intra-model pooling and plurality consensus across models to produce robust emotion profiles. Our analysis shows that during interaction with the AI tutor, students typically report mildly positive affect and moderate arousal. Yet learning is not uniformly smooth: confusion and curiosity are frequent companions to problem solving, and frustration, while less common, still surfaces in ways that can derail progress. Emotional states are short-lived--positive moments last slightly longer than neutral or negative ones, but they are fragile and easily disrupted. Encouragingly, negative emotions often resolve quickly, sometimes rebounding directly into positive states. Neutral moments frequently act as turning points, more often steering students upward than downward, suggesting opportunities for tutors to intervene at precisely these junctures.</li>
</ul>

<h3>Title: Self-Training with Dynamic Weighting for Robust Gradual Domain Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Zixi Wang, Yushe Cao, Yubo Huang, Jinzhu Wei, Jingzehua Xu, Shuai Zhang, Xin Lai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13864">https://arxiv.org/abs/2510.13864</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13864">https://arxiv.org/pdf/2510.13864</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13864]] Self-Training with Dynamic Weighting for Robust Gradual Domain Adaptation(https://arxiv.org/abs/2510.13864)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>In this paper, we propose a new method called Self-Training with Dynamic Weighting (STDW), which aims to enhance robustness in Gradual Domain Adaptation (GDA) by addressing the challenge of smooth knowledge migration from the source to the target domain. Traditional GDA methods mitigate domain shift through intermediate domains and self-training but often suffer from inefficient knowledge migration or incomplete intermediate data. Our approach introduces a dynamic weighting mechanism that adaptively balances the loss contributions of the source and target domains during training. Specifically, we design an optimization framework governed by a time-varying hyperparameter $\varrho$ (progressing from 0 to 1), which controls the strength of domain-specific learning and ensures stable adaptation. The method leverages self-training to generate pseudo-labels and optimizes a weighted objective function for iterative model updates, maintaining robustness across intermediate domains. Experiments on rotated MNIST, color-shifted MNIST, portrait datasets, and the Cover Type dataset demonstrate that STDW outperforms existing baselines. Ablation studies further validate the critical role of $\varrho$'s dynamic scheduling in achieving progressive adaptation, confirming its effectiveness in reducing domain bias and improving generalization. This work provides both theoretical insights and a practical framework for robust gradual domain adaptation, with potential applications in dynamic real-world scenarios. The code is available at this https URL.</li>
</ul>

<h3>Title: CoLoR-GAN: Continual Few-Shot Learning with Low-Rank Adaptation in Generative Adversarial Networks</h3>
<ul>
<li><strong>Authors: </strong>Munsif Ali, Leonardo Rossi, Massimo Bertozzi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13869">https://arxiv.org/abs/2510.13869</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13869">https://arxiv.org/pdf/2510.13869</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13869]] CoLoR-GAN: Continual Few-Shot Learning with Low-Rank Adaptation in Generative Adversarial Networks(https://arxiv.org/abs/2510.13869)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Continual learning (CL) in the context of Generative Adversarial Networks (GANs) remains a challenging problem, particularly when it comes to learn from a few-shot (FS) samples without catastrophic forgetting. Current most effective state-of-the-art (SOTA) methods, like LFS-GAN, introduce a non-negligible quantity of new weights at each training iteration, which would become significant when considering the long term. For this reason, this paper introduces \textcolor{red}{\textbf{\underline{c}}}ontinual few-sh\textcolor{red}{\textbf{\underline{o}}}t learning with \textcolor{red}{\textbf{\underline{lo}}}w-\textcolor{red}{\textbf{\underline{r}}}ank adaptation in GANs named CoLoR-GAN, a framework designed to handle both FS and CL together, leveraging low-rank tensors to efficiently adapt the model to target tasks while reducing even more the number of parameters required. Applying a vanilla LoRA implementation already permitted us to obtain pretty good results. In order to optimize even further the size of the adapters, we challenged LoRA limits introducing a LoRA in LoRA (LLoRA) technique for convolutional layers. Finally, aware of the criticality linked to the choice of the hyperparameters of LoRA, we provide an empirical study to easily find the best ones. We demonstrate the effectiveness of CoLoR-GAN through experiments on several benchmark CL and FS tasks and show that our model is efficient, reaching SOTA performance but with a number of resources enormously reduced. Source code is available on \href{this https URL}{Github.</li>
</ul>

<h3>Title: Unlocking the Potential of Diffusion Language Models through Template Infilling</h3>
<ul>
<li><strong>Authors: </strong>Junhoo Lee, Seungyeon Kim, Nojun Kwak</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13870">https://arxiv.org/abs/2510.13870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13870">https://arxiv.org/pdf/2510.13870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13870]] Unlocking the Potential of Diffusion Language Models through Template Infilling(https://arxiv.org/abs/2510.13870)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion Language Models (DLMs) have emerged as a promising alternative to Autoregressive Language Models, yet their inference strategies remain limited to prefix-based prompting inherited from the autoregressive paradigm. In this paper, we propose Template Infilling (TI), a tailored conditioning methodology for DLMs' generation process. Unlike conventional prefix prompting, TI first generates a structural template for the target response, then fills in the masked segments. To enhance the flexibility of this structural control, we introduce Dynamic Segment Allocation (DSA), which adaptively adjusts segment lengths based on generation confidence. We demonstrate the effectiveness of our approach on mathematical reasoning and code generation benchmarks, achieving consistent improvements of 17.01$\%$p over baseline. Furthermore, we show that TI provides additional advantages in multi-token generation settings, enabling effective speedup while maintaining generation quality.</li>
</ul>

<h3>Title: Joint Discriminative-Generative Modeling via Dual Adversarial Training</h3>
<ul>
<li><strong>Authors: </strong>Xuwang Yin, Claire Zhang, Julie Steele, Nir Shavit, Tony T. Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13872">https://arxiv.org/abs/2510.13872</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13872">https://arxiv.org/pdf/2510.13872</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13872]] Joint Discriminative-Generative Modeling via Dual Adversarial Training(https://arxiv.org/abs/2510.13872)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Simultaneously achieving robust classification and high-fidelity generative modeling within a single framework presents a significant challenge. Hybrid approaches, such as Joint Energy-Based Models (JEM), interpret classifiers as EBMs but are often limited by the instability and poor sample quality inherent in SGLD-based training. We address these limitations by proposing a novel training framework that integrates adversarial training (AT) principles for both discriminative robustness and stable generative learning. The proposed method introduces three key innovations: (1) the replacement of SGLD-based JEM learning with a stable, AT-based approach that optimizes the energy function by discriminating between real data and PGD-generated contrastive samples using the BCE loss; (2) synergistic adversarial training for the discriminative component that enhances classification robustness while eliminating the need for explicit gradient penalties; and (3) a two-stage training procedure to resolve the incompatibility between batch normalization and EBM training. Experiments on CIFAR-10, CIFAR-100, and ImageNet demonstrate that our method substantially improves adversarial robustness over existing hybrid models while maintaining competitive generative performance. On ImageNet, when optimized for generative modeling, our model's generative fidelity surpasses that of BigGAN and approaches diffusion models, representing the first MCMC-based EBM approach to achieve high-quality generation on complex, high-resolution datasets. Our approach addresses key stability issues that have limited JEM scaling and demonstrates that adversarial training can serve as an effective foundation for unified frameworks capable of generating and robustly classifying visual data.</li>
</ul>

<h3>Title: What Layers When: Learning to Skip Compute in LLMs with Residual Gates</h3>
<ul>
<li><strong>Authors: </strong>Filipe Laitenberger, Dawid Kopiczko, Cees G.M. Snoek, Yuki M. Asano</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13876">https://arxiv.org/abs/2510.13876</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13876">https://arxiv.org/pdf/2510.13876</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13876]] What Layers When: Learning to Skip Compute in LLMs with Residual Gates(https://arxiv.org/abs/2510.13876)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We introduce GateSkip, a simple residual-stream gating mechanism that enables token-wise layer skipping in decoder-only LMs. Each Attention/MLP branch is equipped with a sigmoid-linear gate that condenses the branch's output before it re-enters the residual stream. During inference we rank tokens by the gate values and skip low-importance ones using a per-layer budget. While early-exit or router-based Mixture-of-Depths models are known to be unstable and need extensive retraining, our smooth, differentiable gates fine-tune stably on top of pretrained models. On long-form reasoning, we save up to 15\% compute while retaining over 90\% of baseline accuracy. On instruction-tuned models we see accuracy gains at full compute and match baseline quality near 50\% savings. The learned gates give insight into transformer information flow (e.g., BOS tokens act as anchors), and the method combines easily with quantization, pruning, and self-speculative decoding.</li>
</ul>

<h3>Title: TextBandit: Evaluating Probabilistic Reasoning in LLMs Through Language-Only Decision Tasks</h3>
<ul>
<li><strong>Authors: </strong>Jimin Lim, Arjun Damerla, Arthur Jiang, Nam Le</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13878">https://arxiv.org/abs/2510.13878</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13878">https://arxiv.org/pdf/2510.13878</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13878]] TextBandit: Evaluating Probabilistic Reasoning in LLMs Through Language-Only Decision Tasks(https://arxiv.org/abs/2510.13878)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have shown to be increasingly capable of performing reasoning tasks, but their ability to make sequential decisions under uncertainty only using natural language remains underexplored. We introduce a novel benchmark in which LLMs interact with multi-armed bandit environments using purely textual feedback, "you earned a token", without access to numerical cues or explicit probabilities, resulting in the model to infer latent reward structures purely off linguistic cues and to adapt accordingly. We evaluated the performance of four open-source LLMs and compare their performance to standard decision-making algorithms such as Thompson Sampling, Epsilon Greedy, Upper Confidence Bound (UCB), and random choice. While most of the LLMs underperformed compared to the baselines, Qwen3-4B, achieved the best-arm selection rate of 89.2% , which significantly outperformed both the larger LLMs and traditional methods. Our findings suggest that probabilistic reasoning is able to emerge from language alone, and we present this benchmark as a step towards evaluating decision-making capabilities in naturalistic, non-numeric contexts.</li>
</ul>

<h3>Title: PAGE: Prompt Augmentation for text Generation Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Mauro Jose Pacchiotti, Luciana Ballejos, Mariel Ale</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13880">https://arxiv.org/abs/2510.13880</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13880">https://arxiv.org/pdf/2510.13880</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13880]] PAGE: Prompt Augmentation for text Generation Enhancement(https://arxiv.org/abs/2510.13880)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In recent years, natural language generative models have shown outstanding performance in text generation tasks. However, when facing specific tasks or particular requirements, they may exhibit poor performance or require adjustments that demand large amounts of additional data. This work introduces PAGE (Prompt Augmentation for text Generation Enhancement), a framework designed to assist these models through the use of simple auxiliary modules. These modules, lightweight models such as classifiers or extractors, provide inferences from the input text. The output of these auxiliaries is then used to construct an enriched input that improves the quality and controllability of the generation. Unlike other generation-assistance approaches, PAGE does not require auxiliary generative models; instead, it proposes a simpler, modular architecture that is easy to adapt to different tasks. This paper presents the proposal, its components and architecture, and reports a proof of concept in the domain of requirements engineering, where an auxiliary module with a classifier is used to improve the quality of software requirements generation.</li>
</ul>

<h3>Title: Too Open for Opinion? Embracing Open-Endedness in Large Language Models for Social Simulation</h3>
<ul>
<li><strong>Authors: </strong>Bolei Ma, Yong Cao, Indira Sen, Anna-Carolina Haensch, Frauke Kreuter, Barbara Plank, Daniel Hershcovich</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13884">https://arxiv.org/abs/2510.13884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13884">https://arxiv.org/pdf/2510.13884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13884]] Too Open for Opinion? Embracing Open-Endedness in Large Language Models for Social Simulation(https://arxiv.org/abs/2510.13884)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly used to simulate public opinion and other social phenomena. Most current studies constrain these simulations to multiple-choice or short-answer formats for ease of scoring and comparison, but such closed designs overlook the inherently generative nature of LLMs. In this position paper, we argue that open-endedness, using free-form text that captures topics, viewpoints, and reasoning processes "in" LLMs, is essential for realistic social simulation. Drawing on decades of survey-methodology research and recent advances in NLP, we argue why this open-endedness is valuable in LLM social simulations, showing how it can improve measurement and design, support exploration of unanticipated views, and reduce researcher-imposed directive bias. It also captures expressiveness and individuality, aids in pretesting, and ultimately enhances methodological utility. We call for novel practices and evaluation frameworks that leverage rather than constrain the open-ended generative diversity of LLMs, creating synergies between NLP and social science.</li>
</ul>

<h3>Title: Order from Chaos: Comparative Study of Ten Leading LLMs on Unstructured Data Categorization</h3>
<ul>
<li><strong>Authors: </strong>Ariel Kamen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13885">https://arxiv.org/abs/2510.13885</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13885">https://arxiv.org/pdf/2510.13885</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13885]] Order from Chaos: Comparative Study of Ten Leading LLMs on Unstructured Data Categorization(https://arxiv.org/abs/2510.13885)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This study presents a comparative evaluation of ten state-of-the-art large language models (LLMs) applied to unstructured text categorization using the Interactive Advertising Bureau (IAB) 2.2 hierarchical taxonomy. The analysis employed a uniform dataset of 8,660 human-annotated samples and identical zero-shot prompts to ensure methodological consistency across all models. Evaluation metrics included four classic measures - accuracy, precision, recall, and F1-score - and three LLM-specific indicators: hallucination ratio, inflation ratio, and categorization cost. Results show that, despite their rapid advancement, contemporary LLMs achieve only moderate classic performance, with average scores of 34% accuracy, 42% precision, 45% recall, and 41% F1-score. Hallucination and inflation ratios reveal that models frequently overproduce categories relative to human annotators. Among the evaluated systems, Gemini 1.5/2.0 Flash and GPT 20B/120B offered the most favorable cost-to-performance balance, while GPT 120B demonstrated the lowest hallucination ratio. The findings suggest that scaling and architectural improvements alone do not ensure better categorization accuracy, as the task requires compressing rich unstructured text into a limited taxonomy - a process that challenges current model architectures. To address these limitations, a separate ensemble-based approach was developed and tested. The ensemble method, in which multiple LLMs act as independent experts, substantially improved accuracy, reduced inflation, and completely eliminated hallucinations. These results indicate that coordinated orchestration of models - rather than sheer scale - may represent the most effective path toward achieving or surpassing human-expert performance in large-scale text categorization.</li>
</ul>

<h3>Title: Reliable Fine-Grained Evaluation of Natural Language Math Proofs</h3>
<ul>
<li><strong>Authors: </strong>Wenjie Ma, Andrei Cojocaru, Neel Kolhe, Bradley Louie, Robin Said Sharif, Haihan Zhang, Vincent Zhuang, Matei Zaharia, Sewon Min</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13888">https://arxiv.org/abs/2510.13888</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13888">https://arxiv.org/pdf/2510.13888</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13888]] Reliable Fine-Grained Evaluation of Natural Language Math Proofs(https://arxiv.org/abs/2510.13888)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) for mathematical reasoning have largely focused on tasks with easily verifiable final answers; however, generating and verifying natural language math proofs remains an open challenge. We identify the absence of a reliable, fine-grained evaluator for LLM-generated math proofs as a critical gap. To address this, we propose a systematic methodology for developing and validating evaluators that assign fine-grained scores on a 0-7 scale to model-generated math proofs. To enable this study, we introduce ProofBench, the first expert-annotated dataset of fine-grained proof ratings, spanning 145 problems from six major math competitions (USAMO, IMO, Putnam, etc) and 435 LLM-generated solutions from Gemini-2.5-pro, o3, and DeepSeek-R1. %with expert gradings. Using ProofBench as a testbed, we systematically explore the evaluator design space across key axes: the backbone model, input context, instructions and evaluation workflow. Our analysis delivers ProofGrader, an evaluator that combines a strong reasoning backbone LM, rich context from reference solutions and marking schemes, and a simple ensembling method; it achieves a low Mean Absolute Error (MAE) of 0.926 against expert scores, significantly outperforming naive baselines. Finally, we demonstrate its practical utility in a best-of-$n$ selection task: at $n=16$, ProofGrader achieves an average score of 4.14 (out of 7), closing 78% of the gap between a naive binary evaluator (2.48) and the human oracle (4.62), highlighting its potential to advance downstream proof generation.</li>
</ul>

<h3>Title: MultiFoodhat: A potential new paradigm for intelligent food quality inspection</h3>
<ul>
<li><strong>Authors: </strong>Yue Hu, Guohang Zhuang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13889">https://arxiv.org/abs/2510.13889</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13889">https://arxiv.org/pdf/2510.13889</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13889]] MultiFoodhat: A potential new paradigm for intelligent food quality inspection(https://arxiv.org/abs/2510.13889)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Food image classification plays a vital role in intelligent food quality inspection, dietary assessment, and automated monitoring. However, most existing supervised models rely heavily on large labeled datasets and exhibit limited generalization to unseen food categories. To overcome these challenges, this study introduces MultiFoodChat, a dialogue-driven multi-agent reasoning framework for zero-shot food recognition. The framework integrates vision-language models (VLMs) and large language models (LLMs) to enable collaborative reasoning through multi-round visual-textual dialogues. An Object Perception Token (OPT) captures fine-grained visual attributes, while an Interactive Reasoning Agent (IRA) dynamically interprets contextual cues to refine predictions. This multi-agent design allows flexible and human-like understanding of complex food scenes without additional training or manual annotations. Experiments on multiple public food datasets demonstrate that MultiFoodChat achieves superior recognition accuracy and interpretability compared with existing unsupervised and few-shot methods, highlighting its potential as a new paradigm for intelligent food quality inspection and analysis.</li>
</ul>

<h3>Title: A Survey on Collaborating Small and Large Language Models for Performance, Cost-effectiveness, Cloud-edge Privacy, and Trustworthiness</h3>
<ul>
<li><strong>Authors: </strong>Fali Wang, Jihai Chen, Shuhua Yang, Ali Al-Lawati, Linli Tang, Hui Liu, Suhang Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13890">https://arxiv.org/abs/2510.13890</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13890">https://arxiv.org/pdf/2510.13890</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13890]] A Survey on Collaborating Small and Large Language Models for Performance, Cost-effectiveness, Cloud-edge Privacy, and Trustworthiness(https://arxiv.org/abs/2510.13890)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have advanced many domains and applications but face high fine-tuning costs, inference latency, limited edge deployability, and reliability concerns. Small language models (SLMs), compact, efficient, and adaptable, offer complementary remedies. Recent work explores collaborative frameworks that fuse SLMs' specialization and efficiency with LLMs' generalization and reasoning to meet diverse objectives across tasks and deployment scenarios. Motivated by these developments, this paper presents a systematic survey of SLM-LLM collaboration organized by collaboration objectives. We propose a taxonomy with four goals: performance enhancement, cost-effectiveness, cloud-edge privacy, and trustworthiness. Within this framework, we review representative methods, summarize design paradigms, and outline open challenges and future directions toward efficient, secure, and scalable SLM-LLM collaboration.</li>
</ul>

<h3>Title: K-frames: Scene-Driven Any-k Keyframe Selection for long video understanding</h3>
<ul>
<li><strong>Authors: </strong>Yifeng Yao, Yike Yun, Jing Wang, Huishuai Zhang, Dongyan Zhao, Ke Tian, Zhihao Wang, Minghui Qiu, Tao Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13891">https://arxiv.org/abs/2510.13891</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13891">https://arxiv.org/pdf/2510.13891</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13891]] K-frames: Scene-Driven Any-k Keyframe Selection for long video understanding(https://arxiv.org/abs/2510.13891)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have demonstrated significant capabilities in image understanding, but long-video are constrained by context windows and computational cost. Uniform frame sampling often leads to substantial information loss. Meanwhile existing keyframe selection methods such as text-frame retrieval or RL-based frame optimization typically yield sparse and temporally disjointed frames, overlooking scene continuity and lacking flexibility for multi-scale frame selection. To address these limitations, we introduce K-frames, a novel paradigm for scene-driven keyframe selection that preserves temporal continuity. Instead of selecting individual frames, K-frames predicts semantically coherent, query-relevant clips, which enables any-k keyframes selection to meet diverse user budgets. To achieve this approach, we first introduce PeakClips, a dataset of 200K video highlights conditioned by query. Building on this dataset, K-frames learns clip2frame selection using a three-stage progressive curriculum. It involves two Supervised Fine-Tuning stages for temporal grounding and key-clip perception, followed by a Reinforcement Learning stage that directly optimizes the scene-driven prediction policy for downstream task without further annotations. Extensive experiments on major long-video understanding benchmarks demonstrate that K-frames provides an effective, interpretable, and plug-and-play solution for keyframe selection at various scales. Our dataset and model will be available.</li>
</ul>

<h3>Title: The Harder The Better: Maintaining Supervised Fine-tuning Generalization with Less but Harder Data</h3>
<ul>
<li><strong>Authors: </strong>Zhaoyang Shang, Sibo Wei, Jianbin Guo, Rui Zhou, Lifeng Dong, Yin Luo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13892">https://arxiv.org/abs/2510.13892</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13892">https://arxiv.org/pdf/2510.13892</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13892]] The Harder The Better: Maintaining Supervised Fine-tuning Generalization with Less but Harder Data(https://arxiv.org/abs/2510.13892)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) excel in general tasks, but adapting them to specialized domains relies on high-quality supervised fine-tuning (SFT) data. Although existing methods can identify subsets of high-quality data and reduce training cost to some extent, their selection process still suffers from over-reliance on LLMs' internal knowledge, weak interpretability, and limited generalization. To address these limitations, we propose THTB (The Harder The Better), a cognitive science-inspired framework for instruction data selection and annotation guidance. THTB prioritizes higher-level cognitive instructions by combining quality filtering with intrinsic and extrinsic hardness scoring, offering interpretable and quantifiable criteria for efficient SFT, both in data selection and annotation guidance. Experiments show that THTB enables models trained on only 5% of the data to outperform full-dataset training, while achieving superior generalization compared with LLM-only selection. In addition, THTB provides effective annotation guidance in vertical domains, enabling a model trained on just 2% of the data to surpass models trained on much larger datasets, demonstrating strong potential for domain adaptation. Our code, datasets, and models are available on this https URL.</li>
</ul>

<h3>Title: Guarding the Guardrails: A Taxonomy-Driven Approach to Jailbreak Detection</h3>
<ul>
<li><strong>Authors: </strong>Olga E. Sorokoletova, Francesco Giarrusso, Vincenzo Suriani, Daniele Nardi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13893">https://arxiv.org/abs/2510.13893</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13893">https://arxiv.org/pdf/2510.13893</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13893]] Guarding the Guardrails: A Taxonomy-Driven Approach to Jailbreak Detection(https://arxiv.org/abs/2510.13893)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Jailbreaking techniques pose a significant threat to the safety of Large Language Models (LLMs). Existing defenses typically focus on single-turn attacks, lack coverage across languages, and rely on limited taxonomies that either fail to capture the full diversity of attack strategies or emphasize risk categories rather than the jailbreaking techniques. To advance the understanding of the effectiveness of jailbreaking techniques, we conducted a structured red-teaming challenge. The outcome of our experiments are manifold. First, we developed a comprehensive hierarchical taxonomy of 50 jailbreak strategies, consolidating and extending prior classifications into seven broad families, including impersonation, persuasion, privilege escalation, cognitive overload, obfuscation, goal conflict, and data poisoning. Second, we analyzed the data collected from the challenge to examine the prevalence and success rates of different attack types, providing insights into how specific jailbreak strategies exploit model vulnerabilities and induce misalignment. Third, we benchmark a popular LLM for jailbreak detection, evaluating the benefits of taxonomy-guided prompting for improving automatic detection. Finally, we compiled a new Italian dataset of 1364 multi-turn adversarial dialogues, annotated with our taxonomy, enabling the study of interactions where adversarial intent emerges gradually and succeeds in bypassing traditional safeguards.</li>
</ul>

<h3>Title: Attribution Quality in AI-Generated Content:Benchmarking Style Embeddings and LLM Judges</h3>
<ul>
<li><strong>Authors: </strong>Misam Abbas</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13898">https://arxiv.org/abs/2510.13898</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13898">https://arxiv.org/pdf/2510.13898</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13898]] Attribution Quality in AI-Generated Content:Benchmarking Style Embeddings and LLM Judges(https://arxiv.org/abs/2510.13898)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Attributing authorship in the era of large language models (LLMs) is increasingly challenging as machine-generated prose rivals human writing. We benchmark two complementary attribution mechanisms , fixed Style Embeddings and an instruction-tuned LLM judge (GPT-4o) on the Human AI Parallel Corpus, an open dataset of 600 balanced instances spanning six domains (academic, news, fiction, blogs, spoken transcripts, and TV/movie scripts). Each instance contains a human prompt with both a gold continuation and an LLM-generated continuation from either GPT-4o or LLaMA-70B-Instruct. The Style Embedding baseline achieves stronger aggregate accuracy on GPT continuations (82 pct vs. 68 pct). The LLM Judge is slightly better than the Style embeddings on LLaMA continuations (85 pct vs. 81 pct) but the results are not statistically significant. Crucially, the LLM judge significantly outperforms in fiction and academic prose, indicating semantic sensitivity, whereas embeddings dominate in spoken and scripted dialogue, reflecting structural strengths. These complementary patterns highlight attribution as a multidimensional problem requiring hybrid strategies. To support reproducibility we provide code on GitHub and derived data on Hugging Face under the MIT license. This open framework provides a reproducible benchmark for attribution quality assessment in AI-generated content, along with a review of related literature influencing this work.</li>
</ul>

<h3>Title: Post-surgical Endometriosis Segmentation in Laparoscopic Videos</h3>
<ul>
<li><strong>Authors: </strong>Andreas Leibetseder, Klaus Schoeffmann, Jörg Keckstein, Simon Keckstein</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13899">https://arxiv.org/abs/2510.13899</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13899">https://arxiv.org/pdf/2510.13899</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13899]] Post-surgical Endometriosis Segmentation in Laparoscopic Videos(https://arxiv.org/abs/2510.13899)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Endometriosis is a common women's condition exhibiting a manifold visual appearance in various body-internal locations. Having such properties makes its identification very difficult and error-prone, at least for laymen and non-specialized medical practitioners. In an attempt to provide assistance to gynecologic physicians treating endometriosis, this demo paper describes a system that is trained to segment one frequently occurring visual appearance of endometriosis, namely dark endometrial implants. The system is capable of analyzing laparoscopic surgery videos, annotating identified implant regions with multi-colored overlays and displaying a detection summary for improved video browsing.</li>
</ul>

<h3>Title: Narrow Finetuning Leaves Clearly Readable Traces in Activation Differences</h3>
<ul>
<li><strong>Authors: </strong>Julian Minder, Clément Dumas, Stewart Slocum, Helena Casademunt, Cameron Holmes, Robert West, Neel Nanda</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13900">https://arxiv.org/abs/2510.13900</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13900">https://arxiv.org/pdf/2510.13900</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13900]] Narrow Finetuning Leaves Clearly Readable Traces in Activation Differences(https://arxiv.org/abs/2510.13900)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Finetuning on narrow domains has become an essential tool to adapt Large Language Models (LLMs) to specific tasks and to create models with known unusual properties that are useful for research. We show that narrow finetuning creates strong biases in LLM activations that can be interpreted to understand the finetuning domain. These biases can be discovered using simple tools from model diffing - the study of differences between models before and after finetuning. In particular, analyzing activation differences on the first few tokens of random text and steering by adding this difference to the model activations produces text similar to the format and general content of the finetuning data. We demonstrate that these analyses contain crucial information by creating an LLM-based interpretability agent to understand the finetuning domain. With access to the bias, the agent performs significantly better compared to baseline agents using simple prompting. Our analysis spans synthetic document finetuning for false facts, emergent misalignment, subliminal learning, and taboo word guessing game models across different architectures (Gemma, LLaMA, Qwen) and scales (1B to 32B parameters). We suspect these biases reflect overfitting and find that mixing pretraining data into the finetuning corpus largely removes them, though residual risks may remain. Our work (1) demonstrates that narrowly finetuned models have salient traces of their training objective in their activations and suggests ways to improve how they are trained, (2) warns AI safety and interpretability researchers that the common practice of using such models as a proxy for studying broader finetuning (e.g., chat-tuning) might not be realistic, and (3) highlights the need for deeper investigation into the effects of narrow finetuning and development of truly realistic case studies for model-diffing, safety and interpretability research.</li>
</ul>

<h3>Title: RAID: Refusal-Aware and Integrated Decoding for Jailbreaking LLMs</h3>
<ul>
<li><strong>Authors: </strong>Tuan T. Nguyen, John Le, Thai T. Vu, Willy Susilo, Heath Cooper</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13901">https://arxiv.org/abs/2510.13901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13901">https://arxiv.org/pdf/2510.13901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13901]] RAID: Refusal-Aware and Integrated Decoding for Jailbreaking LLMs(https://arxiv.org/abs/2510.13901)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) achieve impressive performance across diverse tasks yet remain vulnerable to jailbreak attacks that bypass safety mechanisms. We present RAID (Refusal-Aware and Integrated Decoding), a framework that systematically probes these weaknesses by crafting adversarial suffixes that induce restricted content while preserving fluency. RAID relaxes discrete tokens into continuous embeddings and optimizes them with a joint objective that (i) encourages restricted responses, (ii) incorporates a refusal-aware regularizer to steer activations away from refusal directions in embedding space, and (iii) applies a coherence term to maintain semantic plausibility and non-redundancy. After optimization, a critic-guided decoding procedure maps embeddings back to tokens by balancing embedding affinity with language-model likelihood. This integration yields suffixes that are both effective in bypassing defenses and natural in form. Experiments on multiple open-source LLMs show that RAID achieves higher attack success rates with fewer queries and lower computational cost than recent white-box and black-box baselines. These findings highlight the importance of embedding-space regularization for understanding and mitigating LLM jailbreak vulnerabilities.</li>
</ul>

<h3>Title: Investigating Political and Demographic Associations in Large Language Models Through Moral Foundations Theory</h3>
<ul>
<li><strong>Authors: </strong>Nicole Smith-Vaniz, Harper Lyon, Lorraine Steigner, Ben Armstrong, Nicholas Mattei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13902">https://arxiv.org/abs/2510.13902</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13902">https://arxiv.org/pdf/2510.13902</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13902]] Investigating Political and Demographic Associations in Large Language Models Through Moral Foundations Theory(https://arxiv.org/abs/2510.13902)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have become increasingly incorporated into everyday life for many internet users, taking on significant roles as advice givers in the domains of medicine, personal relationships, and even legal matters. The importance of these roles raise questions about how and what responses LLMs make in difficult political and moral domains, especially questions about possible biases. To quantify the nature of potential biases in LLMs, various works have applied Moral Foundations Theory (MFT), a framework that categorizes human moral reasoning into five dimensions: Harm, Fairness, Ingroup Loyalty, Authority, and Purity. Previous research has used the MFT to measure differences in human participants along political, national, and cultural lines. While there has been some analysis of the responses of LLM with respect to political stance in role-playing scenarios, no work so far has directly assessed the moral leanings in the LLM responses, nor have they connected LLM outputs with robust human data. In this paper we analyze the distinctions between LLM MFT responses and existing human research directly, investigating whether commonly available LLM responses demonstrate ideological leanings: either through their inherent responses, straightforward representations of political ideologies, or when responding from the perspectives of constructed human personas. We assess whether LLMs inherently generate responses that align more closely with one political ideology over another, and additionally examine how accurately LLMs can represent ideological perspectives through both explicit prompting and demographic-based role-playing. By systematically analyzing LLM behavior across these conditions and experiments, our study provides insight into the extent of political and demographic dependency in AI-generated responses.</li>
</ul>

<h3>Title: Schema for In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Pan Chen, Shaohong Chen, Mark Wang, Shi Xuan Leong, Priscilla Fung, Varinia Bernales, Alan Aspuru-Guzik</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13905">https://arxiv.org/abs/2510.13905</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13905">https://arxiv.org/pdf/2510.13905</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13905]] Schema for In-Context Learning(https://arxiv.org/abs/2510.13905)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer, large language model</a></li>
<li><strong>Abstract: </strong>In-Context Learning (ICL) enables transformer-based language models to adapt to new tasks by conditioning on demonstration examples. However, traditional example-driven in-context learning lacks explicit modules for knowledge retrieval and transfer at the abstraction level. Inspired by cognitive science, specifically schema theory, which holds that humans interpret new information by activating pre-existing mental frameworks (schemas) to structure understanding, we introduce SCHEMA ACTIVATED IN CONTEXT LEARNING (SA-ICL). This framework extracts the representation of the building blocks of cognition for the reasoning process instilled from prior examples, creating an abstracted schema, a lightweight, structured template of key inferential steps and their relationships, which is then used to augment a model's reasoning process when presented with a novel question. We demonstrate that a broad range of large language models (LLMs) lack the capacity to form and utilize internal schema-based learning representations implicitly, but instead benefit significantly from explicit schema-based scaffolding. Across chemistry and physics questions from the GPQA dataset, our experiments show that SA-ICL consistently boosts performance, up to 36.19 percent, when the single demonstration example is of high quality, which simultaneously reduces reliance on the number of demonstrations and enhances interpretability. SCHEMA ACTIVATED IN CONTEXT LEARNING not only bridges disparate ICL strategies ranging from pattern priming to Chain-of-Thought prompting, but also paves a new path for enhancing human-like reasoning in LLMs.</li>
</ul>

<h3>Title: LLM Prompt Duel Optimizer: Efficient Label-Free Prompt Optimization</h3>
<ul>
<li><strong>Authors: </strong>Yuanchen Wu, Saurabh Verma, Justin Lee, Fangzhou Xiong, Poppy Zhang, Amel Awadelkarim, Xu Chen, Yubai Yuan, Shawndra Hill</a></li>
<li><strong>Subjects: </strong>cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13907">https://arxiv.org/abs/2510.13907</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13907">https://arxiv.org/pdf/2510.13907</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13907]] LLM Prompt Duel Optimizer: Efficient Label-Free Prompt Optimization(https://arxiv.org/abs/2510.13907)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are highly sensitive to their input prompts, making prompt design a central challenge. While automatic prompt optimization (APO) reduces manual engineering, most approaches assume access to ground-truth references such as labeled validation data. In practice, however, collecting high-quality labels is costly and slow. We propose the Prompt Duel Optimizer (PDO), a sample-efficient framework for label-free prompt optimization. PDO formulates the problem as a dueling-bandit setting, where supervision signal comes from pairwise preference feedback provided by an LLM judge. The framework combines Double Thompson Sampling (D-TS), which prioritizes informative prompt comparisons, with Top-Performer Guided Mutation, which expands the candidate pool by mutating high-performing prompts. PDO naturally operates in label-free settings and can also incorporate partial labels to mitigate judge noise. Experiments on BIG-bench Hard (BBH) and MS MARCO show that PDO consistently outperforms baseline methods. Ablation studies further demonstrate the effectiveness of both D-TS and prompt mutation.</li>
</ul>

<h3>Title: Interpreting the Latent Structure of Operator Precedence in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Dharunish Yugeswardeenoo, Harshil Nukala, Cole Blondin, Sean O Brien, Vasu Sharma, Kevin Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13908">https://arxiv.org/abs/2510.13908</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13908">https://arxiv.org/pdf/2510.13908</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13908]] Interpreting the Latent Structure of Operator Precedence in Language Models(https://arxiv.org/abs/2510.13908)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated impressive reasoning capabilities but continue to struggle with arithmetic tasks. Prior works largely focus on outputs or prompting strategies, leaving the open question of the internal structure through which models do arithmetic computation. In this work, we investigate whether LLMs encode operator precedence in their internal representations via the open-source instruction-tuned LLaMA 3.2-3B model. We constructed a dataset of arithmetic expressions with three operands and two operators, varying the order and placement of parentheses. Using this dataset, we trace whether intermediate results appear in the residual stream of the instruction-tuned LLaMA 3.2-3B model. We apply interpretability techniques such as logit lens, linear classification probes, and UMAP geometric visualization. Our results show that intermediate computations are present in the residual stream, particularly after MLP blocks. We also find that the model linearly encodes precedence in each operator's embeddings post attention layer. We introduce partial embedding swap, a technique that modifies operator precedence by exchanging high-impact embedding dimensions between operators.</li>
</ul>

<h3>Title: Knowledge Reasoning Language Model: Unifying Knowledge and Language for Inductive Knowledge Graph Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Xingrui Zhuo, Jiapu Wang, Gongqing Wu, Zhongyuan Wang, Jichen Zhang, Shirui Pan, Xindong Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13909">https://arxiv.org/abs/2510.13909</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13909">https://arxiv.org/pdf/2510.13909</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13909]] Knowledge Reasoning Language Model: Unifying Knowledge and Language for Inductive Knowledge Graph Reasoning(https://arxiv.org/abs/2510.13909)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Inductive Knowledge Graph Reasoning (KGR) aims to discover facts in open-domain KGs containing unknown entities and relations, which poses a challenge for KGR models in comprehending uncertain KG components. Existing studies have proposed Knowledge Graph Foundation Models (KGFMs) that learn structural invariances across KGs to handle this uncertainty. Recently, Large Language Models (LLMs) have demonstrated strong capabilities for open-domain knowledge reasoning. As a result, the latest research has focused on LLM-based KGFMs that integrate LLM knowledge with KG context for inductive KGR. However, the intrinsic knowledge of LLMs may be overshadowed by sparse KG context, leading to LLM knowledge distortion, which can cause irreversible damage to model reasoning. Moreover, existing LLM-based KGR methods still struggle to fully constrain generative hallucinations in LLMs, severely limiting the credibility of reasoning results. To address these limitations, we propose a Knowledge Reasoning Language Model (KRLM) that achieves unified coordination between LLM knowledge and KG context throughout the KGR process. Specifically, we design a Knowledge Reasoning Language (KRL) instruction format and a KRL tokenizer to align LLM knowledge with KG representations. Then, we propose a KRL attention layer that coordinates intrinsic LLM knowledge with additional KG context through a dynamic knowledge memory mechanism. Finally, a structure-aware next-entity predictor is proposed, which strictly constrains the reasoning results within a trustworthy knowledge domain. Extensive experimental results on 25 real-world inductive KGR datasets demonstrate the significant superiority of the proposed KRLM\footnote{Our source codes are available at this https URL in both zero-shot reasoning and fine-tuning scenarios.</li>
</ul>

<h3>Title: RAGCap-Bench: Benchmarking Capabilities of LLMs in Agentic Retrieval Augmented Generation Systems</h3>
<ul>
<li><strong>Authors: </strong>Jingru Lin, Chen Zhang, Stephen Y. Liu, Haizhou Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13910">https://arxiv.org/abs/2510.13910</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13910">https://arxiv.org/pdf/2510.13910</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13910]] RAGCap-Bench: Benchmarking Capabilities of LLMs in Agentic Retrieval Augmented Generation Systems(https://arxiv.org/abs/2510.13910)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) mitigates key limitations of Large Language Models (LLMs)-such as factual errors, outdated knowledge, and hallucinations-by dynamically retrieving external information. Recent work extends this paradigm through agentic RAG systems, where LLMs act as agents to iteratively plan, retrieve, and reason over complex queries. However, these systems still struggle with challenging multi-hop questions, and their intermediate reasoning capabilities remain underexplored. To address this, we propose RAGCap-Bench, a capability-oriented benchmark for fine-grained evaluation of intermediate tasks in agentic RAG workflows. We analyze outputs from state-of-the-art systems to identify common tasks and the core capabilities required for their execution, then construct a taxonomy of typical LLM errors to design targeted evaluation questions. Experiments show that "slow-thinking" models with stronger RAGCap performance achieve better end-to-end results, underscoring the benchmark's validity and the importance of enhancing these intermediate capabilities.</li>
</ul>

<h3>Title: AI Debaters are More Persuasive when Arguing in Alignment with Their Own Beliefs</h3>
<ul>
<li><strong>Authors: </strong>María Victoria Carro, Denise Alejandra Mester, Facundo Nieto, Oscar Agustín Stanchi, Guido Ernesto Bergman, Mario Alejandro Leiva, Eitan Sprejer, Luca Nicolás Forziati Gangi, Francisca Gauna Selasco, Juan Gustavo Corvalán, Gerardo I. Simari, María Vanina Martinez</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13912">https://arxiv.org/abs/2510.13912</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13912">https://arxiv.org/pdf/2510.13912</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13912]] AI Debaters are More Persuasive when Arguing in Alignment with Their Own Beliefs(https://arxiv.org/abs/2510.13912)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The core premise of AI debate as a scalable oversight technique is that it is harder to lie convincingly than to refute a lie, enabling the judge to identify the correct position. Yet, existing debate experiments have relied on datasets with ground truth, where lying is reduced to defending an incorrect proposition. This overlooks a subjective dimension: lying also requires the belief that the claim defended is false. In this work, we apply debate to subjective questions and explicitly measure large language models' prior beliefs before experiments. Debaters were asked to select their preferred position, then presented with a judge persona deliberately designed to conflict with their identified priors. This setup tested whether models would adopt sycophantic strategies, aligning with the judge's presumed perspective to maximize persuasiveness, or remain faithful to their prior beliefs. We implemented and compared two debate protocols, sequential and simultaneous, to evaluate potential systematic biases. Finally, we assessed whether models were more persuasive and produced higher-quality arguments when defending positions consistent with their prior beliefs versus when arguing against them. Our main findings show that models tend to prefer defending stances aligned with the judge persona rather than their prior beliefs, sequential debate introduces significant bias favoring the second debater, models are more persuasive when defending positions aligned with their prior beliefs, and paradoxically, arguments misaligned with prior beliefs are rated as higher quality in pairwise comparison. These results can inform human judges to provide higher-quality training signals and contribute to more aligned AI systems, while revealing important aspects of human-AI interaction regarding persuasion dynamics in language models.</li>
</ul>

<h3>Title: Element2Vec: Build Chemical Element Representation from Text for Property Prediction</h3>
<ul>
<li><strong>Authors: </strong>Yuanhao Li, Keyuan Lai, Tianqi Wang, Qihao Liu, Jiawei Ma, Yuan-Chao Hu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13916">https://arxiv.org/abs/2510.13916</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13916">https://arxiv.org/pdf/2510.13916</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13916]] Element2Vec: Build Chemical Element Representation from Text for Property Prediction(https://arxiv.org/abs/2510.13916)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Accurate property data for chemical elements is crucial for materials design and manufacturing, but many of them are difficult to measure directly due to equipment constraints. While traditional methods use the properties of other elements or related properties for prediction via numerical analyses, they often fail to model complex relationships. After all, not all characteristics can be represented as scalars. Recent efforts have been made to explore advanced AI tools such as language models for property estimation, but they still suffer from hallucinations and a lack of interpretability. In this paper, we investigate Element2Vecto effectively represent chemical elements from natural languages to support research in the natural sciences. Given the text parsed from Wikipedia pages, we use language models to generate both a single general-purpose embedding (Global) and a set of attribute-highlighted vectors (Local). Despite the complicated relationship across elements, the computational challenges also exist because of 1) the discrepancy in text distribution between common descriptions and specialized scientific texts, and 2) the extremely limited data, i.e., with only 118 known elements, data for specific properties is often highly sparse and incomplete. Thus, we also design a test-time training method based on self-attention to mitigate the prediction error caused by Vanilla regression clearly. We hope this work could pave the way for advancing AI-driven discovery in materials science.</li>
</ul>

<h3>Title: Optimal Aggregation of LLM and PRM Signals for Efficient Test-Time Scaling</h3>
<ul>
<li><strong>Authors: </strong>Peng Kuang, Yanli Wang, Xiaoyu Han, Yaowenqi Liu, Kaidi Xu, Haohan Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13918">https://arxiv.org/abs/2510.13918</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13918">https://arxiv.org/pdf/2510.13918</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13918]] Optimal Aggregation of LLM and PRM Signals for Efficient Test-Time Scaling(https://arxiv.org/abs/2510.13918)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Process reward models (PRMs) are a cornerstone of test-time scaling (TTS), designed to verify and select the best responses from large language models (LLMs). However, this promise is challenged by recent benchmarks where simple majority voting, which ignores PRM signals, occasionally outperforms standard PRM-based selection. This raises a critical question: How can we effectively utilize verification signals from PRMs for TTS? To address this, we start by developing a theoretical framework for optimally combining signals from both the LLM and the PRM. Our framework reveals that the optimal strategy is a weighted aggregation of responses, a strategy whose effectiveness hinges on estimating weights that capture the complex interplay between the models. Based on our theoretical results, we empirically show that these optimal weighting functions differ significantly across LLM-PRM pairs and, notably, often assign substantial negative weights. Motivated by these insights, we propose efficient pre-computation methods to calibrate these weighting functions. Extensive experiments across 5 LLMs and 7 PRMs demonstrate that our calibration method significantly boosts the TTS efficiency, surpassing the performance of vanilla weighted majority voting while using only $21.3\%$ of the computation. Ultimately, our work demonstrates that investing in a more intelligent aggregation strategy can be a more convincing path to performance gains than simply scaling test-time computation.</li>
</ul>

<h3>Title: FACTS: Table Summarization via Offline Template Generation with Agentic Workflows</h3>
<ul>
<li><strong>Authors: </strong>Ye Yuan, Mohammad Amin Shabani, Siqi Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13920">https://arxiv.org/abs/2510.13920</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13920">https://arxiv.org/pdf/2510.13920</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13920]] FACTS: Table Summarization via Offline Template Generation with Agentic Workflows(https://arxiv.org/abs/2510.13920)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust</a></li>
<li><strong>Abstract: </strong>Query-focused table summarization requires generating natural language summaries of tabular data conditioned on a user query, enabling users to access insights beyond fact retrieval. Existing approaches face key limitations: table-to-text models require costly fine-tuning and struggle with complex reasoning, prompt-based LLM methods suffer from token-limit and efficiency issues while exposing sensitive data, and prior agentic pipelines often rely on decomposition, planning, or manual templates that lack robustness and scalability. To mitigate these issues, we introduce an agentic workflow, FACTS, a Fast, Accurate, and Privacy-Compliant Table Summarization approach via Offline Template Generation. FACTS produces offline templates, consisting of SQL queries and Jinja2 templates, which can be rendered into natural language summaries and are reusable across multiple tables sharing the same schema. It enables fast summarization through reusable offline templates, accurate outputs with executable SQL queries, and privacy compliance by sending only table schemas to LLMs. Evaluations on widely-used benchmarks show that FACTS consistently outperforms baseline methods, establishing it as a practical solution for real-world query-focused table summarization.</li>
</ul>

<h3>Title: Weight Weaving: Parameter Pooling for Data-Free Model Merging</h3>
<ul>
<li><strong>Authors: </strong>Levy Chaves, Eduardo Valle, Sandra Avila</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13921">https://arxiv.org/abs/2510.13921</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13921">https://arxiv.org/pdf/2510.13921</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13921]] Weight Weaving: Parameter Pooling for Data-Free Model Merging(https://arxiv.org/abs/2510.13921)</code><input type="text"></li>
<li><strong>Keywords: </strong>data-free</a></li>
<li><strong>Abstract: </strong>Model merging provides a cost-effective and data-efficient combination of specialized deep neural networks through parameter integration. This technique leverages expert models across downstream tasks without requiring retraining. Most model merging approaches critically depend on scaling hyper-parameters $\lambda$, which weight each model's contribution globally or individually. Principled approaches for setting scaling factors without accessing any data (data-free) are scarce, often leading researchers to tune $\lambda$ using privileged data from the evaluation set, which is obviously unfeasible in practice. To address this limitation, we introduce Weight Weaving, a plug-and-play technique that pools model weights across $\lambda$ values search space using user-defined pooling functions, such as averaging, random selection, or even existing model merging methods. Our method demonstrates high modularity, imposing minimal constraints on the search space. It operates orthogonally to existing model merging methods and eliminates evaluation data requirements. We validate Weight Weaving across three ViT variants in three experimental setups: vision multi-task learning, vision continual learning, and domain generalization. Our method consistently improves the performance of several model merging methods, achieving average accuracy gains of up to 15.9 percentage points in a data-free setting.</li>
</ul>

<h3>Title: An LLM-Powered AI Agent Framework for Holistic IoT Traffic Interpretation</h3>
<ul>
<li><strong>Authors: </strong>Daniel Adu Worae, Spyridon Mastorakis</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13925">https://arxiv.org/abs/2510.13925</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13925">https://arxiv.org/pdf/2510.13925</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13925]] An LLM-Powered AI Agent Framework for Holistic IoT Traffic Interpretation(https://arxiv.org/abs/2510.13925)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Internet of Things (IoT) networks generate diverse and high-volume traffic that reflects both normal activity and potential threats. Deriving meaningful insight from such telemetry requires cross-layer interpretation of behaviors, protocols, and context rather than isolated detection. This work presents an LLM-powered AI agent framework that converts raw packet captures into structured and semantically enriched representations for interactive analysis. The framework integrates feature extraction, transformer-based anomaly detection, packet and flow summarization, threat intelligence enrichment, and retrieval-augmented question answering. An AI agent guided by a large language model performs reasoning over the indexed traffic artifacts, assembling evidence to produce accurate and human-readable interpretations. Experimental evaluation on multiple IoT captures and six open models shows that hybrid retrieval, which combines lexical and semantic search with reranking, substantially improves BLEU, ROUGE, METEOR, and BERTScore results compared with dense-only retrieval. System profiling further indicates low CPU, GPU, and memory overhead, demonstrating that the framework achieves holistic and efficient interpretation of IoT network traffic.</li>
</ul>

<h3>Title: BioMedSearch: A Multi-Source Biomedical Retrieval Framework Based on LLMs</h3>
<ul>
<li><strong>Authors: </strong>Congying Liu, Xingyuan Wei, Peipei Liu, Yiqing Shen, Yanxu Mao, Tiehan Cui</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13926">https://arxiv.org/abs/2510.13926</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13926">https://arxiv.org/pdf/2510.13926</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13926]] BioMedSearch: A Multi-Source Biomedical Retrieval Framework Based on LLMs(https://arxiv.org/abs/2510.13926)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Biomedical queries often rely on a deep understanding of specialized knowledge such as gene regulatory mechanisms and pathological processes of diseases. They require detailed analysis of complex physiological processes and effective integration of information from multiple data sources to support accurate retrieval and reasoning. Although large language models (LLMs) perform well in general reasoning tasks, their generated biomedical content often lacks scientific rigor due to the inability to access authoritative biomedical databases and frequently fabricates protein functions, interactions, and structural details that deviate from authentic information. Therefore, we present BioMedSearch, a multi-source biomedical information retrieval framework based on LLMs. The method integrates literature retrieval, protein database and web search access to support accurate and efficient handling of complex biomedical queries. Through sub-queries decomposition, keywords extraction, task graph construction, and multi-source information filtering, BioMedSearch generates high-quality question-answering results. To evaluate the accuracy of question answering, we constructed a multi-level dataset, BioMedMCQs, consisting of 3,000 questions. The dataset covers three levels of reasoning: mechanistic identification, non-adjacent semantic integration, and temporal causal reasoning, and is used to assess the performance of BioMedSearch and other methods on complex QA tasks. Experimental results demonstrate that BioMedSearch consistently improves accuracy over all baseline models across all levels. Specifically, at Level 1, the average accuracy increases from 59.1% to 91.9%; at Level 2, it rises from 47.0% to 81.0%; and at the most challenging Level 3, the average accuracy improves from 36.3% to 73.4%. The code and BioMedMCQs are available at: this https URL</li>
</ul>

<h3>Title: LLMs Can Get "Brain Rot"!</h3>
<ul>
<li><strong>Authors: </strong>Shuo Xing, Junyuan Hong, Yifan Wang, Runjin Chen, Zhenyu Zhang, Ananth Grama, Zhengzhong Tu, Zhangyang Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13928">https://arxiv.org/abs/2510.13928</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13928">https://arxiv.org/pdf/2510.13928</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13928]] LLMs Can Get "Brain Rot"!(https://arxiv.org/abs/2510.13928)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We propose and test the LLM Brain Rot Hypothesis: continual exposure to junk web text induces lasting cognitive decline in large language models (LLMs). To causally isolate data quality, we run controlled experiments on real Twitter/X corpora, constructing junk and reversely controlled datasets via two orthogonal operationalizations: M1 (engagement degree) and M2 (semantic quality), with matched token scale and training operations across conditions. Contrary to the control group, continual pre-training of 4 LLMs on the junk dataset causes non-trivial declines (Hedges' $g>0.3$) on reasoning, long-context understanding, safety, and inflating "dark traits" (e.g., psychopathy, narcissism). The gradual mixtures of junk and control datasets also yield dose-response cognition decay: for example, under M1, ARC-Challenge with Chain Of Thoughts drops $74.9 \rightarrow 57.2$ and RULER-CWE $84.4 \rightarrow 52.3$ as junk ratio rises from $0\%$ to $100\%$. Error forensics reveal several key insights. First, we identify thought-skipping as the primary lesion: models increasingly truncate or skip reasoning chains, explaining most of the error growth. Second, partial but incomplete healing is observed: scaling instruction tuning and clean data pre-training improve the declined cognition yet cannot restore baseline capability, suggesting persistent representational drift rather than format mismatch. Finally, we discover that the popularity, a non-semantic metric, of a tweet is a better indicator of the Brain Rot effect than the length in M1. Together, the results provide significant, multi-perspective evidence that data quality is a causal driver of LLM capability decay, reframing curation for continual pretraining as a \textit{training-time safety} problem and motivating routine "cognitive health checks" for deployed LLMs.</li>
</ul>

<h3>Title: Robust or Suggestible? Exploring Non-Clinical Induction in LLM Drug-Safety Decisions</h3>
<ul>
<li><strong>Authors: </strong>Siying Liu, Shisheng Zhang, Indu Bala</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13931">https://arxiv.org/abs/2510.13931</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13931">https://arxiv.org/pdf/2510.13931</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13931]] Robust or Suggestible? Exploring Non-Clinical Induction in LLM Drug-Safety Decisions(https://arxiv.org/abs/2510.13931)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly applied in biomedical domains, yet their reliability in drug-safety prediction remains underexplored. In this work, we investigate whether LLMs incorporate socio-demographic information into adverse event (AE) predictions, despite such attributes being clinically irrelevant. Using structured data from the United States Food and Drug Administration Adverse Event Reporting System (FAERS) and a persona-based evaluation framework, we assess two state-of-the-art models, ChatGPT-4o and Bio-Medical-Llama-3.8B, across diverse personas defined by education, marital status, employment, insurance, language, housing stability, and religion. We further evaluate performance across three user roles (general practitioner, specialist, patient) to reflect real-world deployment scenarios where commercial systems often differentiate access by user type. Our results reveal systematic disparities in AE prediction accuracy. Disadvantaged groups (e.g., low education, unstable housing) were frequently assigned higher predicted AE likelihoods than more privileged groups (e.g., postgraduate-educated, privately insured). Beyond outcome disparities, we identify two distinct modes of bias: explicit bias, where incorrect predictions directly reference persona attributes in reasoning traces, and implicit bias, where predictions are inconsistent, yet personas are not explicitly mentioned. These findings expose critical risks in applying LLMs to pharmacovigilance and highlight the urgent need for fairness-aware evaluation protocols and mitigation strategies before clinical deployment.</li>
</ul>

<h3>Title: Big Reasoning with Small Models: Instruction Retrieval at Inference Time</h3>
<ul>
<li><strong>Authors: </strong>Kenan Alkiek, David Jurgens, Vinod Vydiswaran</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13935">https://arxiv.org/abs/2510.13935</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13935">https://arxiv.org/pdf/2510.13935</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13935]] Big Reasoning with Small Models: Instruction Retrieval at Inference Time(https://arxiv.org/abs/2510.13935)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Can we bring large-scale reasoning to local-scale compute? Small language models (SLMs) are increasingly attractive because they run efficiently on local hardware, offering strong privacy, low cost, and reduced environmental impact. Yet they often struggle with tasks that require multi-step reasoning or domain-specific knowledge. We address this limitation through instruction intervention at inference time, where an SLM retrieves structured reasoning procedures rather than generating them from scratch. Our method builds an Instruction Corpus by grouping similar training questions and creating instructions via GPT-5. During inference, the SLM retrieves the most relevant instructions and follows their steps. Unlike retrieval-augmented generation, which retrieves text passages, instruction retrieval gives the model structured guidance for reasoning. We evaluate this framework on MedQA (medical board exams), MMLU Professional Law, and MathQA using models from 3B to 14B parameters without any additional fine-tuning. Instruction retrieval yields consistent gains: 9.4% on MedQA, 7.9% on MMLU Law, and 5.1% on MathQA. Concise instructions outperform longer ones, and the magnitude of improvement depends strongly on model family and intrinsic reasoning ability.</li>
</ul>

<h3>Title: FinDeepResearch: Evaluating Deep Research Agents in Rigorous Financial Analysis</h3>
<ul>
<li><strong>Authors: </strong>Fengbin Zhu, Xiang Yao Ng, Ziyang Liu, Chang Liu, Xianwei Zeng, Chao Wang, Tianhui Tan, Xuan Yao, Pengyang Shao, Min Xu, Zixuan Wang, Jing Wang, Xin Lin, Junfeng Li, Jingxian Zhu, Yang Zhang, Wenjie Wang, Fuli Feng, Richang Hong, Huanbo Luan, Ke-Wei Huang, Tat-Seng Chua</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13936">https://arxiv.org/abs/2510.13936</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13936">https://arxiv.org/pdf/2510.13936</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13936]] FinDeepResearch: Evaluating Deep Research Agents in Rigorous Financial Analysis(https://arxiv.org/abs/2510.13936)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Deep Research (DR) agents, powered by advanced Large Language Models (LLMs), have recently garnered increasing attention for their capability in conducting complex research tasks. However, existing literature lacks a rigorous and systematic evaluation of DR Agent's capabilities in critical research analysis. To address this gap, we first propose HisRubric, a novel evaluation framework with a hierarchical analytical structure and a fine-grained grading rubric for rigorously assessing DR agents' capabilities in corporate financial analysis. This framework mirrors the professional analyst's workflow, progressing from data recognition to metric calculation, and finally to strategic summarization and interpretation. Built on this framework, we construct a FinDeepResearch benchmark that comprises 64 listed companies from 8 financial markets across 4 languages, encompassing a total of 15,808 grading items. We further conduct extensive experiments on the FinDeepResearch using 16 representative methods, including 6 DR agents, 5 LLMs equipped with both deep reasoning and search capabilities, and 5 LLMs with deep reasoning capabilities only. The results reveal the strengths and limitations of these approaches across diverse capabilities, financial markets, and languages, offering valuable insights for future research and development. The benchmark and evaluation code will be made publicly available.</li>
</ul>

<h3>Title: Readers Prefer Outputs of AI Trained on Copyrighted Books over Expert Human Writers</h3>
<ul>
<li><strong>Authors: </strong>Tuhin Chakrabarty, Jane C. Ginsburg, Paramveer Dhillon</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13939">https://arxiv.org/abs/2510.13939</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13939">https://arxiv.org/pdf/2510.13939</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13939]] Readers Prefer Outputs of AI Trained on Copyrighted Books over Expert Human Writers(https://arxiv.org/abs/2510.13939)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>The use of copyrighted books for training AI models has led to numerous lawsuits from authors concerned about AI's ability to generate derivative this http URL it's unclear whether these models can generate high quality literary text while emulating authors' styles. To answer this we conducted a preregistered study comparing MFA-trained expert writers with three frontier AI models: ChatGPT, Claude & Gemini in writing up to 450 word excerpts emulating 50 award-winning authors' diverse styles. In blind pairwise evaluations by 159 representative expert & lay readers, AI-generated text from in-context prompting was strongly disfavored by experts for both stylistic fidelity (OR=0.16, p<10^8) & writing quality (OR=0.13, p<10^7) but showed mixed results with lay readers. However, fine-tuning ChatGPT on individual authors' complete works completely reversed these findings: experts now favored AI-generated text for stylistic fidelity (OR=8.16, p<10^13) & writing quality (OR=1.87, p=0.010), with lay readers showing similar shifts. These effects generalize across authors & styles. The fine-tuned outputs were rarely flagged as AI-generated (3% rate v. 97% for in-context prompting) by best AI detectors. Mediation analysis shows this reversal occurs because fine-tuning eliminates detectable AI stylistic quirks (e.g., cliche density) that penalize in-context outputs. While we do not account for additional costs of human effort required to transform raw AI output into cohesive, publishable prose, the median fine-tuning & inference cost of $81 per author represents a dramatic 99.7% reduction compared to typical professional writer compensation. Author-specific fine-tuning thus enables non-verbatim AI writing that readers prefer to expert human writing, providing empirical evidence directly relevant to copyright's fourth fair-use factor, the "effect upon the potential market or value" of the source works.</li>
</ul>

<h3>Title: Less is More: Improving LLM Reasoning with Minimal Test-Time Intervention</h3>
<ul>
<li><strong>Authors: </strong>Zhen Yang, Mingyang Zhang, Feng Chen, Ganggui Ding, Liang Hou, Xin Tao, Pengfei Wan, Ying-Cong Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13940">https://arxiv.org/abs/2510.13940</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13940">https://arxiv.org/pdf/2510.13940</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13940]] Less is More: Improving LLM Reasoning with Minimal Test-Time Intervention(https://arxiv.org/abs/2510.13940)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent progress in large language models (LLMs) has focused on test-time scaling to improve reasoning via increased inference computation, but often at the cost of efficiency. We revisit test-time behavior and uncover a simple yet underexplored phenomenon: reasoning uncertainty is highly localized-only a small subset of high-entropy tokens dominantly affects output correctness. Motivated by this, we propose Minimal Test-Time Intervention (MTI), a training-free framework that enhances reasoning accuracy and stability with minimal overhead. MTI includes: (i) Selective CFG intervention, applying classifier-free guidance only at uncertain positions; and (ii) Lightweight negative-prompt guidance, reusing the main model's KV cache to approximate unconditional decoding efficiently. MTI yields consistent gains across general, coding, and STEM tasks-e.g., +1.35% average improvement on eight benchmarks for Qwen3-8B-Base and +5% on AIME2024 using Qwen3-32B-Reasoning-while remaining highly efficient.</li>
</ul>

<h3>Title: Classifying and Addressing the Diversity of Errors in Retrieval-Augmented Generation Systems</h3>
<ul>
<li><strong>Authors: </strong>Kin Kwan Leung, Mouloud Belbahri, Yi Sui, Alex Labach, Xueying Zhang, Stephen Rose, Jesse C. Cresswell</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13975">https://arxiv.org/abs/2510.13975</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13975">https://arxiv.org/pdf/2510.13975</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13975]] Classifying and Addressing the Diversity of Errors in Retrieval-Augmented Generation Systems(https://arxiv.org/abs/2510.13975)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Retrieval-augmented generation (RAG) is a prevalent approach for building LLM-based question-answering systems that can take advantage of external knowledge databases. Due to the complexity of real-world RAG systems, there are many potential causes for erroneous outputs. Understanding the range of errors that can occur in practice is crucial for robust deployment. We present a new taxonomy of the error types that can occur in realistic RAG systems, examples of each, and practical advice for addressing them. Additionally, we curate a dataset of erroneous RAG responses annotated by error types. We then propose an auto-evaluation method aligned with our taxonomy that can be used in practice to track and address errors during development. Code and data are available at this https URL.</li>
</ul>

<h3>Title: Finding Holes: Pathologist Level Performance Using AI for Cribriform Morphology Detection in Prostate Cancer</h3>
<ul>
<li><strong>Authors: </strong>Kelvin Szolnoky, Anders Blilie, Nita Mulliqi, Toyonori Tsuzuki, Hemamali Samaratunga, Matteo Titus, Xiaoyi Ji, Sol Erika Boman, Einar Gudlaugsson, Svein Reidar Kjosavik, José Asenjo, Marcello Gambacorta, Paolo Libretti, Marcin Braun, Radisław Kordek, Roman Łowicki, Brett Delahunt, Kenneth A. Iczkowski, Theo van der Kwast, Geert J. L. H. van Leenders, Katia R. M. Leite, Chin-Chen Pan, Emiel Adrianus Maria Janssen, Martin Eklund, Lars Egevad, Kimmo Kartasalo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13995">https://arxiv.org/abs/2510.13995</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13995">https://arxiv.org/pdf/2510.13995</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13995]] Finding Holes: Pathologist Level Performance Using AI for Cribriform Morphology Detection in Prostate Cancer(https://arxiv.org/abs/2510.13995)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Background: Cribriform morphology in prostate cancer is a histological feature that indicates poor prognosis and contraindicates active surveillance. However, it remains underreported and subject to significant interobserver variability amongst pathologists. We aimed to develop and validate an AI-based system to improve cribriform pattern detection. Methods: We created a deep learning model using an EfficientNetV2-S encoder with multiple instance learning for end-to-end whole-slide classification. The model was trained on 640 digitised prostate core needle biopsies from 430 patients, collected across three cohorts. It was validated internally (261 slides from 171 patients) and externally (266 slides, 104 patients from three independent cohorts). Internal validation cohorts included laboratories or scanners from the development set, while external cohorts used completely independent instruments and laboratories. Annotations were provided by three expert uropathologists with known high concordance. Additionally, we conducted an inter-rater analysis and compared the model's performance against nine expert uropathologists on 88 slides from the internal validation cohort. Results: The model showed strong internal validation performance (AUC: 0.97, 95% CI: 0.95-0.99; Cohen's kappa: 0.81, 95% CI: 0.72-0.89) and robust external validation (AUC: 0.90, 95% CI: 0.86-0.93; Cohen's kappa: 0.55, 95% CI: 0.45-0.64). In our inter-rater analysis, the model achieved the highest average agreement (Cohen's kappa: 0.66, 95% CI: 0.57-0.74), outperforming all nine pathologists whose Cohen's kappas ranged from 0.35 to 0.62. Conclusion: Our AI model demonstrates pathologist-level performance for cribriform morphology detection in prostate cancer. This approach could enhance diagnostic reliability, standardise reporting, and improve treatment decisions for prostate cancer patients.</li>
</ul>

<h3>Title: The German Commons - 154 Billion Tokens of Openly Licensed Text for German Language Models</h3>
<ul>
<li><strong>Authors: </strong>Lukas Gienapp, Christopher Schröder, Stefan Schweter, Christopher Akiki, Ferdinand Schlatt, Arden Zimmermann, Phillipe Genêt, Martin Potthast</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13996">https://arxiv.org/abs/2510.13996</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13996">https://arxiv.org/pdf/2510.13996</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13996]] The German Commons - 154 Billion Tokens of Openly Licensed Text for German Language Models(https://arxiv.org/abs/2510.13996)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language model development relies on large-scale training corpora, yet most contain data of unclear licensing status, limiting the development of truly open models. This problem is exacerbated for non-English languages, where openly licensed text remains critically scarce. We introduce the German Commons, the largest collection of openly licensed German text to date. It compiles data from 41 sources across seven domains, encompassing legal, scientific, cultural, political, news, economic, and web text. Through systematic sourcing from established data providers with verifiable licensing, it yields 154.56 billion tokens of high-quality text for language model training. Our processing pipeline implements comprehensive quality filtering, deduplication, and text formatting fixes, ensuring consistent quality across heterogeneous text sources. All domain subsets feature licenses of at least CC-BY-SA 4.0 or equivalent, ensuring legal compliance for model training and redistribution. The German Commons therefore addresses the critical gap in openly licensed German pretraining data, and enables the development of truly open German language models. We also release code for corpus construction and data filtering tailored to German language text, rendering the German Commons fully reproducible and extensible.</li>
</ul>

<h3>Title: REAP the Experts: Why Pruning Prevails for One-Shot MoE compression</h3>
<ul>
<li><strong>Authors: </strong>Mike Lasby, Ivan Lazarevich, Nish Sinnadurai, Sean Lie, Yani Ioannou, Vithursan Thangarasa</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.13999">https://arxiv.org/abs/2510.13999</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.13999">https://arxiv.org/pdf/2510.13999</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.13999]] REAP the Experts: Why Pruning Prevails for One-Shot MoE compression(https://arxiv.org/abs/2510.13999)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Sparsely-activated Mixture-of-Experts (SMoE) models offer efficient pre-training and low latency but their large parameter counts create significant memory overhead, motivating research into expert compression. Contrary to recent findings favouring expert merging on discriminative benchmarks, we demonstrate that expert pruning is a superior strategy for generative tasks. We prove that merging introduces an irreducible error by causing a "functional subspace collapse", due to the loss of the router's independent, input-dependent control over experts. Leveraging this insight, we propose Router-weighted Expert Activation Pruning (REAP), a novel pruning criterion that considers both router gate-values and expert activation norms. Across a diverse set of SMoE models ranging from 20B to 1T parameters, REAP consistently outperforms merging and other pruning methods on generative benchmarks, especially at 50% compression. Notably, our method achieves near-lossless compression on code generation and tool-calling tasks with Qwen3-Coder-480B and Kimi-K2, even after pruning 50% of experts.</li>
</ul>

<h3>Title: PIShield: Detecting Prompt Injection Attacks via Intrinsic LLM Features</h3>
<ul>
<li><strong>Authors: </strong>Wei Zou, Yupei Liu, Yanting Wang, Ying Chen, Neil Gong, Jinyuan Jia</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14005">https://arxiv.org/abs/2510.14005</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14005">https://arxiv.org/pdf/2510.14005</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14005]] PIShield: Detecting Prompt Injection Attacks via Intrinsic LLM Features(https://arxiv.org/abs/2510.14005)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>LLM-integrated applications are vulnerable to prompt injection attacks, where an attacker contaminates the input to inject malicious prompts, causing the LLM to follow the attacker's intent instead of the original user's. Existing prompt injection detection methods often have sub-optimal performance and/or high computational overhead. In this work, we propose PIShield, a detection method that is both effective and efficient. Our key observation is that the internal representation of the final token in a prompt-extracted from a specific layer of the LLM, which we term the injection-critical layer-captures distinguishing features between clean and contaminated prompts. Leveraging this insight, we train a simple linear classifier on these internal representations using a labeled set of clean and contaminated prompts. We compare PIShield against 11 baselines across 5 diverse benchmark datasets and 8 prompt injection attacks. The results demonstrate that PIShield is both highly effective and efficient, substantially outperforming existing methods. Additionally, we show that PIShield resists strong adaptive attacks.</li>
</ul>

<h3>Title: Noise-Adaptive Layerwise Learning Rates: Accelerating Geometry-Aware Optimization for Deep Neural Network Training</h3>
<ul>
<li><strong>Authors: </strong>Jie Hao, Xiaochuan Gong, Jie Xu, Zhengdao Wang, Mingrui Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14009">https://arxiv.org/abs/2510.14009</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14009">https://arxiv.org/pdf/2510.14009</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14009]] Noise-Adaptive Layerwise Learning Rates: Accelerating Geometry-Aware Optimization for Deep Neural Network Training(https://arxiv.org/abs/2510.14009)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Geometry-aware optimization algorithms, such as Muon, have achieved remarkable success in training deep neural networks (DNNs). These methods leverage the underlying geometry of DNNs by selecting appropriate norms for different layers and updating parameters via norm-constrained linear minimization oracles (LMOs). However, even within a group of layers associated with the same norm, the local curvature can be heterogeneous across layers and vary dynamically over the course of training. For example, recent work shows that sharpness varies substantially across transformer layers and throughout training, yet standard geometry-aware optimizers impose fixed learning rates to layers within the same group, which may be inefficient for DNN training. In this paper, we introduce a noise-adaptive layerwise learning rate scheme on top of geometry-aware optimization algorithms and substantially accelerate DNN training compared to methods that use fixed learning rates within each group. Our method estimates gradient variance in the dual norm induced by the chosen LMO on the fly, and uses it to assign time-varying noise-adaptive layerwise learning rates within each group. We provide a theoretical analysis showing that our algorithm achieves a sharp convergence rate. Empirical results on transformer architectures such as LLaMA and GPT demonstrate that our approach achieves faster convergence than state-of-the-art optimizers.</li>
</ul>

<h3>Title: CRaFT: An Explanation-Based Framework for Evaluating Cultural Reasoning in Multilingual Language Models</h3>
<ul>
<li><strong>Authors: </strong>Shehenaz Hossain, Haithem Afli</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14014">https://arxiv.org/abs/2510.14014</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14014">https://arxiv.org/pdf/2510.14014</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14014]] CRaFT: An Explanation-Based Framework for Evaluating Cultural Reasoning in Multilingual Language Models(https://arxiv.org/abs/2510.14014)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Correct answers do not necessarily reflect cultural understanding. We introduce CRaFT, an explanation-based multilingual evaluation framework designed to assess how large language models (LLMs) reason across cultural contexts. Rather than scoring outputs solely based on accuracy, CRaFT evaluates model explanations using four interpretable metrics: Cultural Fluency, Deviation, Consistency, and Linguistic Adaptation. We apply the framework to 50 culturally grounded questions from the World Values Survey, translated into Arabic, Bengali, and Spanish, and evaluate three models (GPT, DeepSeek, and FANAR) across over 2,100 answer-explanation pairs. Results reveal significant cross-lingual variation in reasoning: Arabic reduces fluency, Bengali enhances it, and Spanish remains largely stable. While GPT adapts more effectively across languages, it exhibits lower consistency; FANAR shows stable but rigid reasoning. These findings suggest that cultural awareness in LLMs is not intrinsic but emerges through linguistic framing. CRaFT offers a new lens for evaluating cross-cultural reasoning in multilingual settings, providing actionable insights for building culturally adaptive language models.</li>
</ul>

<h3>Title: NAPPure: Adversarial Purification for Robust Image Classification under Non-Additive Perturbations</h3>
<ul>
<li><strong>Authors: </strong>Junjie Nan, Jianing Li, Wei Chen, Mingkun Zhang, Xueqi Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14025">https://arxiv.org/abs/2510.14025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14025">https://arxiv.org/pdf/2510.14025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14025]] NAPPure: Adversarial Purification for Robust Image Classification under Non-Additive Perturbations(https://arxiv.org/abs/2510.14025)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Adversarial purification has achieved great success in combating adversarial image perturbations, which are usually assumed to be additive. However, non-additive adversarial perturbations such as blur, occlusion, and distortion are also common in the real world. Under such perturbations, existing adversarial purification methods are much less effective since they are designed to fit the additive nature. In this paper, we propose an extended adversarial purification framework named NAPPure, which can further handle non-additive perturbations. Specifically, we first establish the generation process of an adversarial image, and then disentangle the underlying clean image and perturbation parameters through likelihood maximization. Experiments on GTSRB and CIFAR-10 datasets show that NAPPure significantly boosts the robustness of image classification models against non-additive perturbations.</li>
</ul>

<h3>Title: Context-Selective State Space Models: Feedback is All You Need</h3>
<ul>
<li><strong>Authors: </strong>Riccardo Zattra, Giacomo Baggio, Umberto Casti, Augusto Ferrante, Francesco Ticozzi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14027">https://arxiv.org/abs/2510.14027</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14027">https://arxiv.org/pdf/2510.14027</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14027]] Context-Selective State Space Models: Feedback is All You Need(https://arxiv.org/abs/2510.14027)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Transformers, powered by the attention mechanism, are the backbone of most foundation models, yet they suffer from quadratic complexity and difficulties in dealing with long-range dependencies in the input sequence. Recent work has shown that state space models (SSMs) provide an efficient alternative, with the S6 module at the core of the Mamba architecture achieving state-of-the-art results on long-sequence benchmarks. In this paper, we introduce the COFFEE (COntext From FEEdback) model, a novel time-varying SSM that incorporates state feedback to enable context-dependent selectivity, while still allowing for parallel implementation. Whereas the selectivity mechanism of S6 only depends on the current input, COFFEE computes it from the internal state, which serves as a compact representation of the sequence history. This shift allows the model to regulate its dynamics based on accumulated context, improving its ability to capture long-range dependencies. In addition to state feedback, we employ an efficient model parametrization that removes redundancies present in S6 and leads to a more compact and trainable formulation. On the induction head task, COFFEE achieves near-perfect accuracy with two orders of magnitude fewer parameters and training sequences compared to S6. On MNIST, COFFEE largely outperforms S6 within the same architecture, reaching 97% accuracy with only 3585 parameters. These results showcase the role of state feedback as a key mechanism for building scalable and efficient sequence models.</li>
</ul>

<h3>Title: Think Globally, Group Locally: Evaluating LLMs Using Multi-Lingual Word Grouping Games</h3>
<ul>
<li><strong>Authors: </strong>César Guerra-Solano, Zhuochun Li, Xiang Lorraine Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14030">https://arxiv.org/abs/2510.14030</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14030">https://arxiv.org/pdf/2510.14030</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14030]] Think Globally, Group Locally: Evaluating LLMs Using Multi-Lingual Word Grouping Games(https://arxiv.org/abs/2510.14030)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) can exhibit biases in reasoning capabilities due to linguistic modality, performing better on tasks in one language versus another, even with similar content. Most previous works evaluate this through reasoning tasks where reliance on strategies or knowledge can ensure success, such as in commonsense or math tasks. However, abstract reasoning is vital to reasoning for everyday life, where people apply "out-of-the-box thinking" to identify and use patterns for solutions, without a reliance on formulaic approaches. Comparatively, little work has evaluated linguistic biases in this task type. In this paper, we propose a task inspired by the New York Times Connections: GlobalGroup, that evaluates models in an abstract reasoning task across several languages. We constructed a game benchmark with five linguistic backgrounds -- English, Spanish, Chinese, Hindi, and Arabic -- in both the native language and an English translation for comparison. We also proposed game difficulty measurements to evaluate models on games with similar difficulty, enabling a more controlled comparison, which is particularly important in reasoning evaluations. Through experimentation, we find English modalities largely lead to better performance in this abstract reasoning task, and performance disparities between open- and closed-source models.</li>
</ul>

<h3>Title: Vgent: Graph-based Retrieval-Reasoning-Augmented Generation For Long Video Understanding</h3>
<ul>
<li><strong>Authors: </strong>Xiaoqian Shen, Wenxuan Zhang, Jun Chen, Mohamed Elhoseiny</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14032">https://arxiv.org/abs/2510.14032</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14032">https://arxiv.org/pdf/2510.14032</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14032]] Vgent: Graph-based Retrieval-Reasoning-Augmented Generation For Long Video Understanding(https://arxiv.org/abs/2510.14032)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Understanding and reasoning over long videos pose significant challenges for large video language models (LVLMs) due to the difficulty in processing intensive video tokens beyond context window and retaining long-term sequential information. Retrieval-Augmented Generation (RAG) has demonstrated effectiveness in processing long context for Large Language Models (LLMs); however, applying RAG to long video faces challenges such as disrupted temporal dependencies and inclusion of irrelevant information that can hinder accurate reasoning. To address these limitations, we propose Vgent, a novel graph-based retrieval-reasoning-augmented generation framework to enhance LVLMs for long video understanding. Our approach introduces two key innovations: (i) It represents videos by structured graphs with semantic relationships across video clips preserved to improve retrieval effectiveness. (ii) It introduces an intermediate reasoning step to mitigate the reasoning limitation of LVLMs, which leverages structured verification to reduce retrieval noise and facilitate the explicit aggregation of relevant information across clips, resulting in more accurate and context-aware responses. We comprehensively evaluate our framework with various open-source LVLMs on three long-video understanding benchmarks. Our approach yielded an overall performance improvement of $3.0\%\sim 5.4\%$ over base models on MLVU, and outperformed state-of-the-art video RAG methods by $8.6\%$. Our code is publicly available at this https URL.</li>
</ul>

<h3>Title: Synchronization of Multiple Videos</h3>
<ul>
<li><strong>Authors: </strong>Avihai Naaman, Ron Shapira Weber, Oren Freifeld</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14051">https://arxiv.org/abs/2510.14051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14051">https://arxiv.org/pdf/2510.14051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14051]] Synchronization of Multiple Videos(https://arxiv.org/abs/2510.14051)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>Synchronizing videos captured simultaneously from multiple cameras in the same scene is often easy and typically requires only simple time shifts. However, synchronizing videos from different scenes or, more recently, generative AI videos, poses a far more complex challenge due to diverse subjects, backgrounds, and nonlinear temporal misalignment. We propose Temporal Prototype Learning (TPL), a prototype-based framework that constructs a shared, compact 1D representation from high-dimensional embeddings extracted by any of various pretrained models. TPL robustly aligns videos by learning a unified prototype sequence that anchors key action phases, thereby avoiding exhaustive pairwise matching. Our experiments show that TPL improves synchronization accuracy, efficiency, and robustness across diverse datasets, including fine-grained frame retrieval and phase classification tasks. Importantly, TPL is the first approach to mitigate synchronization issues in multiple generative AI videos depicting the same action. Our code and a new multiple video synchronization dataset are available at this https URL</li>
</ul>

<h3>Title: FedHFT: Efficient Federated Finetuning with Heterogeneous Edge Clients</h3>
<ul>
<li><strong>Authors: </strong>Fatih Ilhan, Selim Furkan Tekin, Tiansheng Huang, Gaowen Liu, Ramana Kompella, Greg Eisenhauer, Yingyan Celine Lin, Calton Pu, Ling Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14054">https://arxiv.org/abs/2510.14054</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14054">https://arxiv.org/pdf/2510.14054</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14054]] FedHFT: Efficient Federated Finetuning with Heterogeneous Edge Clients(https://arxiv.org/abs/2510.14054)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate, large language model</a></li>
<li><strong>Abstract: </strong>Fine-tuning pre-trained large language models (LLMs) has become a common practice for personalized natural language understanding (NLU) applications on downstream tasks and domain-specific datasets. However, there are two main challenges: (i) limited and/or heterogeneous data for fine-tuning due to proprietary data confidentiality or privacy requirements, and (ii) varying computation resources available across participating clients such as edge devices. This paper presents FedHFT - an efficient and personalized federated fine-tuning framework to address both challenges. First, we introduce a mixture of masked adapters to handle resource heterogeneity across participating clients, enabling high-performance collaborative fine-tuning of pre-trained language model(s) across multiple clients in a distributed setting, while keeping proprietary data local. Second, we introduce a bi-level optimization approach to handle non-iid data distribution based on masked personalization and client clustering. Extensive experiments demonstrate significant performance and efficiency improvements over various natural language understanding tasks under data and resource heterogeneity compared to representative heterogeneous federated learning methods.</li>
</ul>

<h3>Title: Quantitative Analysis of UAV Intrusion Mitigation for Border Security in 5G with LEO Backhaul Impairments</h3>
<ul>
<li><strong>Authors: </strong>Rajendra Upadhyay, Al Nahian Bin Emran, Rajendra Paudyal, Lisa Donnan, Duminda Wijesekera</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14066">https://arxiv.org/abs/2510.14066</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14066">https://arxiv.org/pdf/2510.14066</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14066]] Quantitative Analysis of UAV Intrusion Mitigation for Border Security in 5G with LEO Backhaul Impairments(https://arxiv.org/abs/2510.14066)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, robust</a></li>
<li><strong>Abstract: </strong>Uncooperative unmanned aerial vehicles (UAVs) pose emerging threats to critical infrastructure and border protection by operating as rogue user equipment (UE) within cellular networks, consuming resources, creating interference, and potentially violating restricted airspaces. This paper presents minimal features of the operating space, yet an end-to-end simulation framework to analyze detect-to-mitigate latency of such intrusions in a hybrid terrestrial-non-terrestrial (LEO satellite) 5G system. The system model includes terrestrial gNBs, satellite backhaul (with stochastic outages), and a detection logic (triggered by handover instability and signal quality variance). A lockdown mechanism is invoked upon detection, with optional local fallback to cap mitigation delays. Monte Carlo sweeps across UAV altitudes, speeds, and satellite outage rates yield several insights. First, satellite backhaul outages can cause arbitrarily long mitigation delays, yet, to meet fallback deadlines, they need to be effectively bounded. Second, while handover instability was hypothesized, our results show that extra handovers have a negligible effect within the range of parameters we considered. The main benefit of resilience from fallback comes from the delay in limiting mitigation. Third, patrol UEs experience negligible collateral impact, with handover rates close to terrestrial baselines. Stress scenarios further highlight that fallback is indispensable in preventing extreme control-plane and physical security vulnerabilities: Without fallback, prolonged outages in the satellite backhaul delay lockdown commands, allowing rogue UAVs to linger inside restricted corridors for several seconds longer. These results underscore the importance of complementing non-terrestrial links with local control to ensure robust and timely response against uncooperative UAV intrusions.</li>
</ul>

<h3>Title: Exploratory Causal Inference in SAEnce</h3>
<ul>
<li><strong>Authors: </strong>Tommaso Mencattini, Riccardo Cadei, Francesco Locatello</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14073">https://arxiv.org/abs/2510.14073</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14073">https://arxiv.org/pdf/2510.14073</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14073]] Exploratory Causal Inference in SAEnce(https://arxiv.org/abs/2510.14073)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Randomized Controlled Trials are one of the pillars of science; nevertheless, they rely on hand-crafted hypotheses and expensive analysis. Such constraints prevent causal effect estimation at scale, potentially anchoring on popular yet incomplete hypotheses. We propose to discover the unknown effects of a treatment directly from data. For this, we turn unstructured data from a trial into meaningful representations via pretrained foundation models and interpret them via a sparse autoencoder. However, discovering significant causal effects at the neural level is not trivial due to multiple-testing issues and effects entanglement. To address these challenges, we introduce Neural Effect Search, a novel recursive procedure solving both issues by progressive stratification. After assessing the robustness of our algorithm on semi-synthetic experiments, we showcase, in the context of experimental ecology, the first successful unsupervised causal effect identification on a real-world scientific trial.</li>
</ul>

<h3>Title: ERGO: Entropy-guided Resetting for Generation Optimization in Multi-turn Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haziq Mohammad Khalid, Athikash Jeyaganthan, Timothy Do, Yicheng Fu, Sean O'Brien, Vasu Sharma, Kevin Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14077">https://arxiv.org/abs/2510.14077</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14077">https://arxiv.org/pdf/2510.14077</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14077]] ERGO: Entropy-guided Resetting for Generation Optimization in Multi-turn Language Models(https://arxiv.org/abs/2510.14077)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) suffer significant performance degradation in multi-turn conversations when information is presented incrementally. Given that multi-turn conversations characterize everyday interactions with LLMs, this degradation poses a severe challenge to real world usability. We hypothesize that abrupt increases in model uncertainty signal misalignment in multi-turn LLM interactions, and we exploit this insight to dynamically realign conversational context. We introduce ERGO (Entropy-guided Resetting for Generation Optimization), which continuously quantifies internal uncertainty via Shannon entropy over next token distributions and triggers adaptive prompt consolidation when a sharp spike in entropy is detected. By treating uncertainty as a first class signal rather than a nuisance to eliminate, ERGO embraces variability in language and modeling, representing and responding to uncertainty. In multi-turn tasks with incrementally revealed instructions, ERGO yields a 56.6% average performance gain over standard baselines, increases aptitude (peak performance capability) by 24.7%, and decreases unreliability (variability in performance) by 35.3%, demonstrating that uncertainty aware interventions can improve both accuracy and reliability in conversational AI.</li>
</ul>

<h3>Title: Capture, Canonicalize, Splat: Zero-Shot 3D Gaussian Avatars from Unstructured Phone Images</h3>
<ul>
<li><strong>Authors: </strong>Emanuel Garbin, Guy Adam, Oded Krams, Zohar Barzelay, Eran Guendelman, Michael Schwarz, Moran Vatelmacher, Yigal Shenkman, Eli Peker, Itai Druker, Uri Patish, Yoav Blum, Max Bluvstein, Junxuan Li, Rawal Khirodkar, Shunsuke Saito</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14081">https://arxiv.org/abs/2510.14081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14081">https://arxiv.org/pdf/2510.14081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14081]] Capture, Canonicalize, Splat: Zero-Shot 3D Gaussian Avatars from Unstructured Phone Images(https://arxiv.org/abs/2510.14081)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, generative</a></li>
<li><strong>Abstract: </strong>We present a novel, zero-shot pipeline for creating hyperrealistic, identity-preserving 3D avatars from a few unstructured phone images. Existing methods face several challenges: single-view approaches suffer from geometric inconsistencies and hallucinations, degrading identity preservation, while models trained on synthetic data fail to capture high-frequency details like skin wrinkles and fine hair, limiting realism. Our method introduces two key contributions: (1) a generative canonicalization module that processes multiple unstructured views into a standardized, consistent representation, and (2) a transformer-based model trained on a new, large-scale dataset of high-fidelity Gaussian splatting avatars derived from dome captures of real people. This "Capture, Canonicalize, Splat" pipeline produces static quarter-body avatars with compelling realism and robust identity preservation from unstructured photos.</li>
</ul>

<h3>Title: Neural Network approximation power on homogeneous and heterogeneous reaction-diffusion equations</h3>
<ul>
<li><strong>Authors: </strong>Haotian Feng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14094">https://arxiv.org/abs/2510.14094</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14094">https://arxiv.org/pdf/2510.14094</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14094]] Neural Network approximation power on homogeneous and heterogeneous reaction-diffusion equations(https://arxiv.org/abs/2510.14094)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Reaction-diffusion systems represent one of the most fundamental formulations used to describe a wide range of physical, chemical, and biological processes. With the increasing adoption of neural networks, recent research has focused on solving differential equations using machine learning techniques. However, the theoretical foundation explaining why neural networks can effectively approximate such solutions remains insufficiently explored. This paper provides a theoretical analysis of the approximation power of neural networks for one- and two-dimensional reaction-diffusion equations in both homogeneous and heterogeneous media. Building upon the universal approximation theorem, we demonstrate that a two-layer neural network can approximate the one-dimensional reaction-diffusion equation, while a three-layer neural network can approximate its two-dimensional counterpart. The theoretical framework presented here can be further extended to elliptic and parabolic equations. Overall, this work highlights the expressive power of neural networks in approximating solutions to reaction-diffusion equations and related PDEs, providing a theoretical foundation for neural network-based differential equation solvers.</li>
</ul>

<h3>Title: Unlocking Out-of-Distribution Generalization in Transformers via Recursive Latent Space Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Awni Altabaa, Siyu Chen, John Lafferty, Zhuoran Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14095">https://arxiv.org/abs/2510.14095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14095">https://arxiv.org/pdf/2510.14095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14095]] Unlocking Out-of-Distribution Generalization in Transformers via Recursive Latent Space Reasoning(https://arxiv.org/abs/2510.14095)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Systematic, compositional generalization beyond the training distribution remains a core challenge in machine learning -- and a critical bottleneck for the emergent reasoning abilities of modern language models. This work investigates out-of-distribution (OOD) generalization in Transformer networks using a GSM8K-style modular arithmetic on computational graphs task as a testbed. We introduce and explore a set of four architectural mechanisms aimed at enhancing OOD generalization: (i) input-adaptive recurrence; (ii) algorithmic supervision; (iii) anchored latent representations via a discrete bottleneck; and (iv) an explicit error-correction mechanism. Collectively, these mechanisms yield an architectural approach for native and scalable latent space reasoning in Transformer networks with robust algorithmic generalization capabilities. We complement these empirical results with a detailed mechanistic interpretability analysis that reveals how these mechanisms give rise to robust OOD generalization abilities.</li>
</ul>

<h3>Title: TENDE: Transfer Entropy Neural Diffusion Estimation</h3>
<ul>
<li><strong>Authors: </strong>Simon Pedro Galeano Munoz, Mustapha Bounoua, Giulio Franzese, Pietro Michiardi, Maurizio Filippone</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14096">https://arxiv.org/abs/2510.14096</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14096">https://arxiv.org/pdf/2510.14096</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14096]] TENDE: Transfer Entropy Neural Diffusion Estimation(https://arxiv.org/abs/2510.14096)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Transfer entropy measures directed information flow in time series, and it has become a fundamental quantity in applications spanning neuroscience, finance, and complex systems analysis. However, existing estimation methods suffer from the curse of dimensionality, require restrictive distributional assumptions, or need exponentially large datasets for reliable convergence. We address these limitations in the literature by proposing TENDE (Transfer Entropy Neural Diffusion Estimation), a novel approach that leverages score-based diffusion models to estimate transfer entropy through conditional mutual information. By learning score functions of the relevant conditional distributions, TENDE provides flexible, scalable estimation while making minimal assumptions about the underlying data-generating process. We demonstrate superior accuracy and robustness compared to existing neural estimators and other state-of-the-art approaches across synthetic benchmarks and real data.</li>
</ul>

<h3>Title: DROID: Dual Representation for Out-of-Scope Intent Detection</h3>
<ul>
<li><strong>Authors: </strong>Wael Rashwan, Hossam M. Zawbaa, Sourav Dutta, Haytham Assem</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14110">https://arxiv.org/abs/2510.14110</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14110">https://arxiv.org/pdf/2510.14110</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14110]] DROID: Dual Representation for Out-of-Scope Intent Detection(https://arxiv.org/abs/2510.14110)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Detecting out-of-scope (OOS) user utterances remains a key challenge in task-oriented dialogue systems and, more broadly, in open-set intent recognition. Existing approaches often depend on strong distributional assumptions or auxiliary calibration modules. We present DROID (Dual Representation for Out-of-Scope Intent Detection), a compact end-to-end framework that combines two complementary encoders -- the Universal Sentence Encoder (USE) for broad semantic generalization and a domain-adapted Transformer-based Denoising Autoencoder (TSDAE) for domain-specific contextual distinctions. Their fused representations are processed by a lightweight branched classifier with a single calibrated threshold that separates in-domain and OOS intents without post-hoc scoring. To enhance boundary learning under limited supervision, DROID incorporates both synthetic and open-domain outlier augmentation. Despite using only 1.5M trainable parameters, DROID consistently outperforms recent state-of-the-art baselines across multiple intent benchmarks, achieving macro-F1 improvements of 6--15% for known and 8--20% for OOS intents, with the most significant gains in low-resource settings. These results demonstrate that dual-encoder representations with simple calibration can yield robust, scalable, and reliable OOS detection for neural dialogue systems.</li>
</ul>

<h3>Title: Toward Cybersecurity-Expert Small Language Models</h3>
<ul>
<li><strong>Authors: </strong>Matan Levi, Daniel Ohayon, Ariel Blobstein, Ravid Sagi, Ian Molloy, Yair Allouche</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14113">https://arxiv.org/abs/2510.14113</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14113">https://arxiv.org/pdf/2510.14113</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14113]] Toward Cybersecurity-Expert Small Language Models(https://arxiv.org/abs/2510.14113)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are transforming everyday applications, yet deployment in cybersecurity lags due to a lack of high-quality, domain-specific models and training datasets. To address this gap, we present CyberPal 2.0, a family of cybersecurity-expert small language models (SLMs) ranging from 4B-20B parameters. To train CyberPal 2.0, we generate an enriched chain-of-thought cybersecurity instruction dataset built with our data enrichment and formatting pipeline, SecKnowledge 2.0, which integrates expert-in-the-loop steering of reasoning formats alongside LLM-driven multi-step grounding, yielding higher-fidelity, task-grounded reasoning traces for security tasks. Across diverse cybersecurity benchmarks, CyberPal 2.0 consistently outperforms its baselines and matches or surpasses various open and closed-source frontier models, while remaining a fraction of their size. On core cyber threat intelligence knowledge tasks, our models outperform almost all tested frontier models, ranking second only to Sec-Gemini v1. On core threat-investigation tasks, such as correlating vulnerabilities and bug tickets with weaknesses, our best 20B-parameter model outperforms GPT-4o, o1, o3-mini, and Sec-Gemini v1, ranking first, while our smallest 4B-parameter model ranks second.</li>
</ul>

<h3>Title: Briding Diffusion Posterior Sampling and Monte Carlo methods: a survey</h3>
<ul>
<li><strong>Authors: </strong>Yazid Janati, Alain Durmus, Jimmy Olsson, Eric Moulines</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14114">https://arxiv.org/abs/2510.14114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14114">https://arxiv.org/pdf/2510.14114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14114]] Briding Diffusion Posterior Sampling and Monte Carlo methods: a survey(https://arxiv.org/abs/2510.14114)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models enable the synthesis of highly accurate samples from complex distributions and have become foundational in generative modeling. Recently, they have demonstrated significant potential for solving Bayesian inverse problems by serving as priors. This review offers a comprehensive overview of current methods that leverage \emph{pre-trained} diffusion models alongside Monte Carlo methods to address Bayesian inverse problems without requiring additional training. We show that these methods primarily employ a \emph{twisting} mechanism for the intermediate distributions within the diffusion process, guiding the simulations toward the posterior distribution. We describe how various Monte Carlo methods are then used to aid in sampling from these twisted distributions.</li>
</ul>

<h3>Title: Neural Network-enabled Domain-consistent Robust Optimisation for Global CO$_2$ Reduction Potential of Gas Power Plants</h3>
<ul>
<li><strong>Authors: </strong>Waqar Muhammad Ashraf, Talha Ansar, Abdulelah S. Alshehri, Peipei Chen, Ramit Debnath, Vivek Dua</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14125">https://arxiv.org/abs/2510.14125</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14125">https://arxiv.org/pdf/2510.14125</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14125]] Neural Network-enabled Domain-consistent Robust Optimisation for Global CO$_2$ Reduction Potential of Gas Power Plants(https://arxiv.org/abs/2510.14125)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We introduce a neural network-driven robust optimisation framework that integrates data-driven domain as a constraint into the nonlinear programming technique, addressing the overlooked issue of domain-inconsistent solutions arising from the interaction of parametrised neural network models with optimisation solvers. Applied to a 1180 MW capacity combined cycle gas power plant, our framework delivers domain-consistent robust optimal solutions that achieve a verified 0.76 percentage point mean improvement in energy efficiency. For the first time, scaling this efficiency gain to the global fleet of gas power plants, we estimate an annual 26 Mt reduction potential in CO$_2$ (with 10.6 Mt in Asia, 9.0 Mt in the Americas, and 4.5 Mt in Europe). These results underscore the synergetic role of machine learning in delivering near-term, scalable decarbonisation pathways for global climate action.</li>
</ul>

<h3>Title: Inferred global dense residue transition graphs from primary structure sequences enable protein interaction prediction via directed graph convolutional neural networks</h3>
<ul>
<li><strong>Authors: </strong>Islam Akef Ebeid, Haoteng Tang, Pengfei Gu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14139">https://arxiv.org/abs/2510.14139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14139">https://arxiv.org/pdf/2510.14139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14139]] Inferred global dense residue transition graphs from primary structure sequences enable protein interaction prediction via directed graph convolutional neural networks(https://arxiv.org/abs/2510.14139)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Introduction Accurate prediction of protein-protein interactions (PPIs) is crucial for understanding cellular functions and advancing drug development. Existing in-silico methods use direct sequence embeddings from Protein Language Models (PLMs). Others use Graph Neural Networks (GNNs) for 3D protein structures. This study explores less computationally intensive alternatives. We introduce a novel framework for downstream PPI prediction through link prediction. Methods We introduce a two-stage graph representation learning framework, ProtGram-DirectGCN. First, we developed ProtGram. This approach models a protein's primary structure as a hierarchy of globally inferred n-gram graphs. In these graphs, residue transition probabilities define edge weights. Each edge connects a pair of residues in a directed graph. The probabilities are aggregated from a large corpus of sequences. Second, we propose DirectGCN, a custom directed graph convolutional neural network. This model features a unique convolutional layer. It processes information through separate path-specific transformations: incoming, outgoing, and undirected. A shared transformation is also applied. These paths are combined via a learnable gating mechanism. We apply DirectGCN to ProtGram graphs to learn residue-level embeddings. These embeddings are pooled via attention to generate protein-level embeddings for prediction. Results We first established the efficacy of DirectGCN on standard node classification benchmarks. Its performance matches established methods on general datasets. The model excels at complex, directed graphs with dense, heterophilic structures. When applied to PPI prediction, the full ProtGram-DirectGCN framework delivers robust predictive power. This strong performance holds even with limited training data.</li>
</ul>

<h3>Title: cubic: CUDA-accelerated 3D Bioimage Computing</h3>
<ul>
<li><strong>Authors: </strong>Alexandr A. Kalinin, Anne E. Carpenter, Shantanu Singh, Matthew J. O'Meara</a></li>
<li><strong>Subjects: </strong>cs.CV, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14143">https://arxiv.org/abs/2510.14143</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14143">https://arxiv.org/pdf/2510.14143</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14143]] cubic: CUDA-accelerated 3D Bioimage Computing(https://arxiv.org/abs/2510.14143)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, segmentation</a></li>
<li><strong>Abstract: </strong>Quantitative analysis of multidimensional biological images is useful for understanding complex cellular phenotypes and accelerating advances in biomedical research. As modern microscopy generates ever-larger 2D and 3D datasets, existing computational approaches are increasingly limited by their scalability, efficiency, and integration with modern scientific computing workflows. Existing bioimage analysis tools often lack application programmable interfaces (APIs), do not support graphics processing unit (GPU) acceleration, lack broad 3D image processing capabilities, and/or have poor interoperability for compute-heavy workflows. Here, we introduce cubic, an open-source Python library that addresses these challenges by augmenting widely used SciPy and scikit-image APIs with GPU-accelerated alternatives from CuPy and RAPIDS cuCIM. cubic's API is device-agnostic and dispatches operations to GPU when data reside on the device and otherwise executes on CPU, seamlessly accelerating a broad range of image processing routines. This approach enables GPU acceleration of existing bioimage analysis workflows, from preprocessing to segmentation and feature extraction for 2D and 3D data. We evaluate cubic both by benchmarking individual operations and by reproducing existing deconvolution and segmentation pipelines, achieving substantial speedups while maintaining algorithmic fidelity. These advances establish a robust foundation for scalable, reproducible bioimage analysis that integrates with the broader Python scientific computing ecosystem, including other GPU-accelerated methods, enabling both interactive exploration and automated high-throughput analysis workflows. cubic is openly available at https://github$.$com/alxndrkalinin/cubic</li>
</ul>

<h3>Title: On Evaluating Loss Functions for Stock Ranking: An Empirical Analysis With Transformer Model</h3>
<ul>
<li><strong>Authors: </strong>Jan Kwiatkowski, Jarosław A. Chudziak</a></li>
<li><strong>Subjects: </strong>cs.LG, q-fin.PM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14156">https://arxiv.org/abs/2510.14156</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14156">https://arxiv.org/pdf/2510.14156</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14156]] On Evaluating Loss Functions for Stock Ranking: An Empirical Analysis With Transformer Model(https://arxiv.org/abs/2510.14156)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Quantitative trading strategies rely on accurately ranking stocks to identify profitable investments. Effective portfolio management requires models that can reliably order future stock returns. Transformer models are promising for understanding financial time series, but how different training loss functions affect their ability to rank stocks well is not yet fully understood. Financial markets are challenging due to their changing nature and complex relationships between stocks. Standard loss functions, which aim for simple prediction accuracy, often aren't enough. They don't directly teach models to learn the correct order of stock returns. While many advanced ranking losses exist from fields such as information retrieval, there hasn't been a thorough comparison to see how well they work for ranking financial returns, especially when used with modern Transformer models for stock selection. This paper addresses this gap by systematically evaluating a diverse set of advanced loss functions including pointwise, pairwise, listwise for daily stock return forecasting to facilitate rank-based portfolio selection on S&P 500 data. We focus on assessing how each loss function influences the model's ability to discern profitable relative orderings among assets. Our research contributes a comprehensive benchmark revealing how different loss functions impact a model's ability to learn cross-sectional and temporal patterns crucial for portfolio selection, thereby offering practical guidance for optimizing ranking-based trading strategies.</li>
</ul>

<h3>Title: Data Understanding Survey: Pursuing Improved Dataset Characterization Via Tensor-based Methods</h3>
<ul>
<li><strong>Authors: </strong>Matthew D. Merris, Tim Andersen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14161">https://arxiv.org/abs/2510.14161</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14161">https://arxiv.org/pdf/2510.14161</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14161]] Data Understanding Survey: Pursuing Improved Dataset Characterization Via Tensor-based Methods(https://arxiv.org/abs/2510.14161)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability, explainability</a></li>
<li><strong>Abstract: </strong>In the evolving domains of Machine Learning and Data Analytics, existing dataset characterization methods such as statistical, structural, and model-based analyses often fail to deliver the deep understanding and insights essential for innovation and explainability. This work surveys the current state-of-the-art conventional data analytic techniques and examines their limitations, and discusses a variety of tensor-based methods and how these may provide a more robust alternative to traditional statistical, structural, and model-based dataset characterization techniques. Through examples, we illustrate how tensor methods unveil nuanced data characteristics, offering enhanced interpretability and actionable intelligence. We advocate for the adoption of tensor-based characterization, promising a leap forward in understanding complex datasets and paving the way for intelligent, explainable data-driven discoveries.</li>
</ul>

<h3>Title: Towards Reversible Model Merging For Low-rank Weights</h3>
<ul>
<li><strong>Authors: </strong>Mohammadsajad Alipour, Mohammad Mohammadi Amiri</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14163">https://arxiv.org/abs/2510.14163</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14163">https://arxiv.org/pdf/2510.14163</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14163]] Towards Reversible Model Merging For Low-rank Weights(https://arxiv.org/abs/2510.14163)</code><input type="text"></li>
<li><strong>Keywords: </strong>data-free</a></li>
<li><strong>Abstract: </strong>Model merging aims to combine multiple fine-tuned models into a single set of weights that performs well across all source tasks. While prior work has shown that merging can approximate the performance of individual fine-tuned models for each task, it largely overlooks scenarios where models are compressed into low-rank representations, either through low-rank adaptation (LoRA) or post-training singular value decomposition (SVD). We first demonstrate that applying conventional merging methods to low-rank weights leads to severe performance degradation in the merged model. Motivated by this phenomenon, we propose a fundamentally different approach: instead of collapsing all adapters into one set of weights, we construct a compact basis (e.g., an equivalent of holding two or more models) from which original task-specific models can be recovered via linear combination. This reframes merging as generating a reconstruction-capable model space rather than producing a single merged model. Crucially, this allows us to ``revert'' to each individual model when needed, recognizing that no merged model can consistently outperform one specialized for its task. Building on this insight, we introduce our method, Reversible Model Merging (RMM), an efficient, data-free, and flexible method that provides a closed-form solution for selecting the optimal basis of model weights and task-specific coefficients for linear combination. Extensive experiments across diverse datasets and model scales demonstrate that RMM consistently outperforms existing merging approaches, preserving the performance of low-rank compressed models by a significant margin.</li>
</ul>

<h3>Title: Optimal Control Theoretic Neural Optimizer: From Backpropagation to Dynamic Programming</h3>
<ul>
<li><strong>Authors: </strong>Guan-Horng Liu, Tianrong Chen, Evangelos A. Theodorou</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14168">https://arxiv.org/abs/2510.14168</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14168">https://arxiv.org/pdf/2510.14168</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14168]] Optimal Control Theoretic Neural Optimizer: From Backpropagation to Dynamic Programming(https://arxiv.org/abs/2510.14168)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Optimization of deep neural networks (DNNs) has been a driving force in the advancement of modern machine learning and artificial intelligence. With DNNs characterized by a prolonged sequence of nonlinear propagation, determining their optimal parameters given an objective naturally fits within the framework of Optimal Control Programming. Such an interpretation of DNNs as dynamical systems has proven crucial in offering a theoretical foundation for principled analysis from numerical equations to physics. In parallel to these theoretical pursuits, this paper focuses on an algorithmic perspective. Our motivated observation is the striking algorithmic resemblance between the Backpropagation algorithm for computing gradients in DNNs and the optimality conditions for dynamical systems, expressed through another backward process known as dynamic programming. Consolidating this connection, where Backpropagation admits a variational structure, solving an approximate dynamic programming up to the first-order expansion leads to a new class of optimization methods exploring higher-order expansions of the Bellman equation. The resulting optimizer, termed Optimal Control Theoretic Neural Optimizer (OCNOpt), enables rich algorithmic opportunities, including layer-wise feedback policies, game-theoretic applications, and higher-order training of continuous-time models such as Neural ODEs. Extensive experiments demonstrate that OCNOpt improves upon existing methods in robustness and efficiency while maintaining manageable computational complexity, paving new avenues for principled algorithmic design grounded in dynamical systems and optimal control theory.</li>
</ul>

<h3>Title: Power Grid Cybersecurity: Policy Analysis White Paper</h3>
<ul>
<li><strong>Authors: </strong>Jack Vanlyssel</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14171">https://arxiv.org/abs/2510.14171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14171">https://arxiv.org/pdf/2510.14171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14171]] Power Grid Cybersecurity: Policy Analysis White Paper(https://arxiv.org/abs/2510.14171)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>The U.S. power grid underpins national security, public safety, and economic stability, but faces growing cyber risks from vulnerabilities in industrial control systems, remote access, and poor cyber hygiene. Despite its critical importance, current policy remains fragmented and reactive. This paper proposes a dual policy approach to strengthen grid cybersecurity: enhanced information sharing between government and private utilities to improve threat detection and response, and standardized cyber hygiene practices to reduce common attack vectors. For long-term resilience, a Unified National Cybersecurity Framework is recommended to align existing NERC, IEC, IEEE, and NIST standards, eliminate regulatory overlap, and adapt to evolving threats. Together, these policies offer both immediate and sustainable improvements in safeguarding the nation's most vital infrastructure.</li>
</ul>

<h3>Title: Virtually Being: Customizing Camera-Controllable Video Diffusion Models with Multi-View Performance Captures</h3>
<ul>
<li><strong>Authors: </strong>Yuancheng Xu, Wenqi Xian, Li Ma, Julien Philip, Ahmet Levent Taşel, Yiwei Zhao, Ryan Burgert, Mingming He, Oliver Hermann, Oliver Pilarski, Rahul Garg, Paul Debevec, Ning Yu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14179">https://arxiv.org/abs/2510.14179</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14179">https://arxiv.org/pdf/2510.14179</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14179]] Virtually Being: Customizing Camera-Controllable Video Diffusion Models with Multi-View Performance Captures(https://arxiv.org/abs/2510.14179)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce a framework that enables both multi-view character consistency and 3D camera control in video diffusion models through a novel customization data pipeline. We train the character consistency component with recorded volumetric capture performances re-rendered with diverse camera trajectories via 4D Gaussian Splatting (4DGS), lighting variability obtained with a video relighting model. We fine-tune state-of-the-art open-source video diffusion models on this data to provide strong multi-view identity preservation, precise camera control, and lighting adaptability. Our framework also supports core capabilities for virtual production, including multi-subject generation using two approaches: joint training and noise blending, the latter enabling efficient composition of independently customized models at inference time; it also achieves scene and real-life video customization as well as control over motion and spatial layout during customization. Extensive experiments show improved video quality, higher personalization accuracy, and enhanced camera control and lighting adaptability, advancing the integration of video generation into virtual production. Our project page is available at: this https URL.</li>
</ul>

<h3>Title: Securing U.S. Critical Infrastructure: Lessons from Stuxnet and the Ukraine Power Grid Attacks</h3>
<ul>
<li><strong>Authors: </strong>Jack Vanlyssel</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14185">https://arxiv.org/abs/2510.14185</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14185">https://arxiv.org/pdf/2510.14185</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14185]] Securing U.S. Critical Infrastructure: Lessons from Stuxnet and the Ukraine Power Grid Attacks(https://arxiv.org/abs/2510.14185)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, segmentation</a></li>
<li><strong>Abstract: </strong>Industrial Control Systems (ICS) underpin the United States' critical infrastructure, managing essential services such as power, water, and transportation that are vital to national security and public safety. However, increasing digital integration has exposed these systems to escalating cyber threats. Historical attacks like Stuxnet and the Ukraine power grid incident revealed exploitable weaknesses-poor network segmentation, outdated software, weak authentication, and inadequate monitoring-that persist in many U.S. ICS environments today. This paper analyzes these landmark attacks to identify recurring vulnerabilities and assess their relevance to current U.S. infrastructure. It argues that without immediate reforms, similar exploits could lead to catastrophic disruptions and national security crises. To address these risks, the paper proposes policy measures focused on implementing zero-trust architecture and improved network segmentation to enhance system resilience. These recommendations aim to guide policymakers and industry leaders in securing the nation's most critical operational technologies against future cyber threats.</li>
</ul>

<h3>Title: Contrastive Diffusion Alignment: Learning Structured Latents for Controllable Generation</h3>
<ul>
<li><strong>Authors: </strong>Ruchi Sandilya, Sumaira Perez, Charles Lynch, Lindsay Victoria, Benjamin Zebley, Derrick Matthew Buchanan, Mahendra T. Bhati, Nolan Williams, Timothy J. Spellman, Faith M. Gunning, Conor Liston, Logan Grosenick</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14190">https://arxiv.org/abs/2510.14190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14190">https://arxiv.org/pdf/2510.14190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14190]] Contrastive Diffusion Alignment: Learning Structured Latents for Controllable Generation(https://arxiv.org/abs/2510.14190)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models excel at generation, but their latent spaces are not explicitly organized for interpretable control. We introduce ConDA (Contrastive Diffusion Alignment), a framework that applies contrastive learning within diffusion embeddings to align latent geometry with system dynamics. Motivated by recent advances showing that contrastive objectives can recover more disentangled and structured representations, ConDA organizes diffusion latents such that traversal directions reflect underlying dynamical factors. Within this contrastively structured space, ConDA enables nonlinear trajectory traversal that supports faithful interpolation, extrapolation, and controllable generation. Across benchmarks in fluid dynamics, neural calcium imaging, therapeutic neurostimulation, and facial expression, ConDA produces interpretable latent representations with improved controllability compared to linear traversals and conditioning-based baselines. These results suggest that diffusion latents encode dynamics-relevant structure, but exploiting this structure requires latent organization and traversal along the latent manifold.</li>
</ul>

<h3>Title: Infrastructure Patterns in Toll Scam Domains: A Comprehensive Analysis of Cybercriminal Registration and Hosting Strategies</h3>
<ul>
<li><strong>Authors: </strong>Morium Akter Munny, Mahbub Alam, Sonjoy Kumar Paul, Daniel Timko, Muhammad Lutfor Rahman, Nitesh Saxena</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14198">https://arxiv.org/abs/2510.14198</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14198">https://arxiv.org/pdf/2510.14198</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14198]] Infrastructure Patterns in Toll Scam Domains: A Comprehensive Analysis of Cybercriminal Registration and Hosting Strategies(https://arxiv.org/abs/2510.14198)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Toll scams involve criminals registering fake domains that pretend to be legitimate transportation agencies to trick users into making fraudulent payments. Although these scams are rapidly increasing and causing significant harm, they have not been extensively studied. We present the first large-scale analysis of toll scam domains, using a newly created dataset of 67,907 confirmed scam domains mostly registered in 2025. Our study reveals that attackers exploit permissive registrars and less common top-level domains, with 86.9% of domains concentrated in just five non-mainstream TLDs and 72.9% registered via a single provider. We also discover specific registration patterns, including short bursts of activity that suggest automated, coordinated attacks, with over half of domains registered in the first quarter of 2025. This extreme temporal clustering reflects highly synchronized campaign launches. Additionally, we build a simple predictive model using only domain registration data to predict which scam domains are likely to be suspended -- a proxy for confirmed abuse -- achieving 80.4% accuracy, and 92.3% sensitivity. Our analysis reveals attacker strategies for evading detection -- such as exploiting obscure TLDs, permissive registrars, and coordinated registration bursts -- which can inform more targeted interventions by registrars, hosting providers, and security platforms. However, our results suggest that registration metadata alone may be insufficient, and incorporating features from domain URLs and webpage content could further improve detection.</li>
</ul>

<h3>Title: DPRF: A Generalizable Dynamic Persona Refinement Framework for Optimizing Behavior Alignment Between Personalized LLM Role-Playing Agents and Humans</h3>
<ul>
<li><strong>Authors: </strong>Bingsheng Yao, Bo Sun, Yuanzhe Dong, Yuxuan Lu, Dakuo Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14205">https://arxiv.org/abs/2510.14205</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14205">https://arxiv.org/pdf/2510.14205</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14205]] DPRF: A Generalizable Dynamic Persona Refinement Framework for Optimizing Behavior Alignment Between Personalized LLM Role-Playing Agents and Humans(https://arxiv.org/abs/2510.14205)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>The emerging large language model role-playing agents (LLM RPAs) aim to simulate individual human behaviors, but the persona fidelity is often undermined by manually-created profiles (e.g., cherry-picked information and personality characteristics) without validating the alignment with the target individuals. To address this limitation, our work introduces the Dynamic Persona Refinement Framework (DPRF).DPRF aims to optimize the alignment of LLM RPAs' behaviors with those of target individuals by iteratively identifying the cognitive divergence, either through free-form or theory-grounded, structured analysis, between generated behaviors and human ground truth, and refining the persona profile to mitigate these this http URL evaluate DPRF with five LLMs on four diverse behavior-prediction scenarios: formal debates, social media posts with mental health issues, public interviews, and movie this http URL can consistently improve behavioral alignment considerably over baseline personas and generalizes across models and this http URL work provides a robust methodology for creating high-fidelity persona profiles and enhancing the validity of downstream applications, such as user simulation, social studies, and personalized AI.</li>
</ul>

<h3>Title: Incentive-Based Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Chanuka A.S. Hewa Kaluannakkage, Rajkumar Buyya</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14208">https://arxiv.org/abs/2510.14208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14208">https://arxiv.org/pdf/2510.14208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14208]] Incentive-Based Federated Learning(https://arxiv.org/abs/2510.14208)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate, fair</a></li>
<li><strong>Abstract: </strong>Federated learning promises to revolutionize machine learning by enabling collaborative model training without compromising data privacy. However, practical adaptability can be limited by critical factors, such as the participation dilemma. Participating entities are often unwilling to contribute to a learning system unless they receive some benefits, or they may pretend to participate and free-ride on others. This chapter identifies the fundamental challenges in designing incentive mechanisms for federated learning systems. It examines how foundational concepts from economics and game theory can be applied to federated learning, alongside technology-driven solutions such as blockchain and deep reinforcement learning. This work presents a comprehensive taxonomy that thoroughly covers both centralized and decentralized architectures based on the aforementioned theoretical concepts. Furthermore, the concepts described are presented from an application perspective, covering emerging industrial applications, including healthcare, smart infrastructure, vehicular networks, and blockchain-based decentralized systems. Through this exploration, this chapter demonstrates that well-designed incentive mechanisms are not merely optional features but essential components for the practical success of federated learning. This analysis reveals both the promising solutions that have emerged and the significant challenges that remain in building truly sustainable, fair, and robust federated learning ecosystems.</li>
</ul>

<h3>Title: Spectral Analysis of Molecular Kernels: When Richer Features Do Not Guarantee Better Generalization</h3>
<ul>
<li><strong>Authors: </strong>Asma Jamali, Tin Sum Cheng, Rodrigo A. Vargas-Hernández</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.chem-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14217">https://arxiv.org/abs/2510.14217</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14217">https://arxiv.org/pdf/2510.14217</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14217]] Spectral Analysis of Molecular Kernels: When Richer Features Do Not Guarantee Better Generalization(https://arxiv.org/abs/2510.14217)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Understanding the spectral properties of kernels offers a principled perspective on generalization and representation quality. While deep models achieve state-of-the-art accuracy in molecular property prediction, kernel methods remain widely used for their robustness in low-data regimes and transparent theoretical grounding. Despite extensive studies of kernel spectra in machine learning, systematic spectral analyses of molecular kernels are scarce. In this work, we provide the first comprehensive spectral analysis of kernel ridge regression on the QM9 dataset, molecular fingerprint, pretrained transformer-based, global and local 3D representations across seven molecular properties. Surprisingly, richer spectral features, measured by four different spectral metrics, do not consistently improve accuracy. Pearson correlation tests further reveal that for transformer-based and local 3D representations, spectral richness can even have a negative correlation with performance. We also implement truncated kernels to probe the relationship between spectrum and predictive performance: in many kernels, retaining only the top 2% of eigenvalues recovers nearly all performance, indicating that the leading eigenvalues capture the most informative features. Our results challenge the common heuristic that "richer spectra yield better generalization" and highlight nuanced relationships between representation, kernel features, and predictive performance. Beyond molecular property prediction, these findings inform how kernel and self-supervised learning methods are evaluated in data-limited scientific and real-world tasks.</li>
</ul>

<h3>Title: An Information Asymmetry Game for Trigger-based DNN Model Watermarking</h3>
<ul>
<li><strong>Authors: </strong>Chaoyue Huang, Gejian Zhao, Hanzhou Wu, Zhihua Xia, Asad Malik</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14218">https://arxiv.org/abs/2510.14218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14218">https://arxiv.org/pdf/2510.14218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14218]] An Information Asymmetry Game for Trigger-based DNN Model Watermarking(https://arxiv.org/abs/2510.14218)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, attack, robust, watermark</a></li>
<li><strong>Abstract: </strong>As a valuable digital product, deep neural networks (DNNs) face increasingly severe threats to the intellectual property, making it necessary to develop effective technical measures to protect them. Trigger-based watermarking methods achieve copyright protection by embedding triggers into the host DNNs. However, the attacker may remove the watermark by pruning or fine-tuning. We model this interaction as a game under conditions of information asymmetry, namely, the defender embeds a secret watermark with private knowledge, while the attacker can only access the watermarked model and seek removal. We define strategies, costs, and utilities for both players, derive the attacker's optimal pruning budget, and establish an exponential lower bound on the accuracy of watermark detection after attack. Experimental results demonstrate the feasibility of the watermarked model, and indicate that sparse watermarking can resist removal with negligible accuracy loss. This study highlights the effectiveness of game-theoretic analysis in guiding the design of robust watermarking schemes for model copyright protection.</li>
</ul>

<h3>Title: LOTA: Bit-Planes Guided AI-Generated Image Detection</h3>
<ul>
<li><strong>Authors: </strong>Hongsong Wang, Renxi Cheng, Yang Zhang, Chaolei Han, Jie Gui</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14230">https://arxiv.org/abs/2510.14230</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14230">https://arxiv.org/pdf/2510.14230</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14230]] LOTA: Bit-Planes Guided AI-Generated Image Detection(https://arxiv.org/abs/2510.14230)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, diffusion</a></li>
<li><strong>Abstract: </strong>The rapid advancement of GAN and Diffusion models makes it more difficult to distinguish AI-generated images from real ones. Recent studies often use image-based reconstruction errors as an important feature for determining whether an image is AI-generated. However, these approaches typically incur high computational costs and also fail to capture intrinsic noisy features present in the raw images. To solve these problems, we innovatively refine error extraction by using bit-plane-based image processing, as lower bit planes indeed represent noise patterns in images. We introduce an effective bit-planes guided noisy image generation and exploit various image normalization strategies, including scaling and thresholding. Then, to amplify the noise signal for easier AI-generated image detection, we design a maximum gradient patch selection that applies multi-directional gradients to compute the noise score and selects the region with the highest score. Finally, we propose a lightweight and effective classification head and explore two different structures: noise-based classifier and noise-guided classifier. Extensive experiments on the GenImage benchmark demonstrate the outstanding performance of our method, which achieves an average accuracy of \textbf{98.9\%} (\textbf{11.9}\%~$\uparrow$) and shows excellent cross-generator generalization capability. Particularly, our method achieves an accuracy of over 98.2\% from GAN to Diffusion and over 99.2\% from Diffusion to GAN. Moreover, it performs error extraction at the millisecond level, nearly a hundred times faster than existing methods. The code is at this https URL.</li>
</ul>

<h3>Title: When Flatness Does (Not) Guarantee Adversarial Robustness</h3>
<ul>
<li><strong>Authors: </strong>Nils Philipp Walter, Linara Adilova, Jilles Vreeken, Michael Kamp</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14231">https://arxiv.org/abs/2510.14231</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14231">https://arxiv.org/pdf/2510.14231</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14231]] When Flatness Does (Not) Guarantee Adversarial Robustness(https://arxiv.org/abs/2510.14231)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Despite their empirical success, neural networks remain vulnerable to small, adversarial perturbations. A longstanding hypothesis suggests that flat minima, regions of low curvature in the loss landscape, offer increased robustness. While intuitive, this connection has remained largely informal and incomplete. By rigorously formalizing the relationship, we show this intuition is only partially correct: flatness implies local but not global adversarial robustness. To arrive at this result, we first derive a closed-form expression for relative flatness in the penultimate layer, and then show we can use this to constrain the variation of the loss in input space. This allows us to formally analyze the adversarial robustness of the entire network. We then show that to maintain robustness beyond a local neighborhood, the loss needs to curve sharply away from the data manifold. We validate our theoretical predictions empirically across architectures and datasets, uncovering the geometric structure that governs adversarial vulnerability, and linking flatness to model confidence: adversarial examples often lie in large, flat regions where the model is confidently wrong. Our results challenge simplified views of flatness and provide a nuanced understanding of its role in robustness.</li>
</ul>

<h3>Title: Scaling Test-Time Compute to Achieve IOI Gold Medal with Open-Weight Models</h3>
<ul>
<li><strong>Authors: </strong>Mehrzad Samadi, Aleksander Ficek, Sean Narenthiran, Siddhartha Jain, Wasi Uddin Ahmad, Somshubra Majumdar, Vahid Noroozi, Boris Ginsburg</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14232">https://arxiv.org/abs/2510.14232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14232">https://arxiv.org/pdf/2510.14232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14232]] Scaling Test-Time Compute to Achieve IOI Gold Medal with Open-Weight Models(https://arxiv.org/abs/2510.14232)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Competitive programming has become a rigorous benchmark for evaluating the reasoning and problem-solving capabilities of large language models (LLMs). The International Olympiad in Informatics (IOI) stands out as one of the most prestigious annual competitions in competitive programming and has become a key benchmark for comparing human and AI-level programming ability. While several proprietary models have been claimed to achieve gold medal-level performance at the IOI, often with undisclosed methods, achieving comparable results with open-weight models remains a significant challenge. In this paper, we present \gencluster, a scalable and reproducible test-time compute framework that attains IOI gold-level performance using open-weight models. It combines large-scale generation, behavioral clustering, ranking, and a round-robin submission strategy to efficiently explore diverse solution spaces under limited validation budgets. Our experiments show that the performance of our proposed approach scales consistently with available compute, narrowing the gap between open and closed systems. Notably, we will show that GenCluster can achieve a gold medal at IOI 2025 for the first time with an open-weight model gpt-oss-120b, setting a new benchmark for transparent and reproducible evaluation of reasoning in LLMs.</li>
</ul>

<h3>Title: RHINO: Guided Reasoning for Mapping Network Logs to Adversarial Tactics and Techniques with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Fanchao Meng, Jiaping Gui, Yunbo Li, Yue Wu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14233">https://arxiv.org/abs/2510.14233</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14233">https://arxiv.org/pdf/2510.14233</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14233]] RHINO: Guided Reasoning for Mapping Network Logs to Adversarial Tactics and Techniques with Large Language Models(https://arxiv.org/abs/2510.14233)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Modern Network Intrusion Detection Systems generate vast volumes of low-level alerts, yet these outputs remain semantically fragmented, requiring labor-intensive manual correlation with high-level adversarial behaviors. Existing solutions for automating this mapping-rule-based systems and machine learning classifiers-suffer from critical limitations: rule-based approaches fail to adapt to novel attack variations, while machine learning methods lack contextual awareness and treat tactic-technique mapping as a syntactic matching problem rather than a reasoning task. Although Large Language Models have shown promise in cybersecurity tasks, preliminary experiments reveal that existing LLM-based methods frequently hallucinate technique names or produce decontextualized mappings due to their single-step classification approach. To address these challenges, we introduce RHINO, a novel framework that decomposes LLM-based attack analysis into three interpretable phases mirroring human reasoning: (1) behavioral abstraction, where raw logs are translated into contextualized narratives; (2) multi-role collaborative inference, generating candidate techniques by evaluating behavioral evidence against MITRE ATT&CK knowledge; and (3) validation, cross-referencing predictions with official MITRE definitions to rectify hallucinations. RHINO bridges the semantic gap between low-level observations and adversarial intent while improving output reliability through structured reasoning. We evaluate RHINO on three benchmarks across four backbone models. RHINO achieved high accuracy, with model performance ranging from 86.38% to 88.45%, resulting in relative gains from 24.25% to 76.50% across different models. Our results demonstrate that RHINO significantly enhances the interpretability and scalability of threat analysis, offering a blueprint for deploying LLMs in operational security settings.</li>
</ul>

<h3>Title: PIA: Deepfake Detection Using Phoneme-Temporal and Identity-Dynamic Analysis</h3>
<ul>
<li><strong>Authors: </strong>Soumyya Kanti Datta, Tanvi Ranga, Chengzhe Sun, Siwei Lyu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14241">https://arxiv.org/abs/2510.14241</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14241">https://arxiv.org/pdf/2510.14241</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14241]] PIA: Deepfake Detection Using Phoneme-Temporal and Identity-Dynamic Analysis(https://arxiv.org/abs/2510.14241)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The rise of manipulated media has made deepfakes a particularly insidious threat, involving various generative manipulations such as lip-sync modifications, face-swaps, and avatar-driven facial synthesis. Conventional detection methods, which predominantly depend on manually designed phoneme-viseme alignment thresholds, fundamental frame-level consistency checks, or a unimodal detection strategy, inadequately identify modern-day deepfakes generated by advanced generative models such as GANs, diffusion models, and neural rendering techniques. These advanced techniques generate nearly perfect individual frames yet inadvertently create minor temporal discrepancies frequently overlooked by traditional detectors. We present a novel multimodal audio-visual framework, Phoneme-Temporal and Identity-Dynamic Analysis(PIA), incorporating language, dynamic face motion, and facial identification cues to address these limitations. We utilize phoneme sequences, lip geometry data, and advanced facial identity embeddings. This integrated method significantly improves the detection of subtle deepfake alterations by identifying inconsistencies across multiple complementary modalities. Code is available at this https URL</li>
</ul>

<h3>Title: Flip-Flop Consistency: Unsupervised Training for Robustness to Prompt Perturbations in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Parsa Hejabi, Elnaz Rahmati, Alireza S. Ziabari, Morteza Dehghani</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14242">https://arxiv.org/abs/2510.14242</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14242">https://arxiv.org/pdf/2510.14242</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14242]] Flip-Flop Consistency: Unsupervised Training for Robustness to Prompt Perturbations in LLMs(https://arxiv.org/abs/2510.14242)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) often produce inconsistent answers when faced with different phrasings of the same prompt. In this paper, we propose Flip-Flop Consistency ($F^2C$), an unsupervised training method that improves robustness to such perturbations. $F^2C$ is composed of two key components. The first, Consensus Cross-Entropy (CCE), uses a majority vote across prompt variations to create a hard pseudo-label. The second is a representation alignment loss that pulls lower-confidence and non-majority predictors toward the consensus established by high-confidence, majority-voting variations. We evaluate our method on 11 datasets spanning four NLP tasks, with 4-15 prompt variations per dataset. On average, $F^2C$ raises observed agreement by 11.62%, improves mean $F_1$ by 8.94%, and reduces performance variance across formats by 3.29%. In out-of-domain evaluations, $F^2C$ generalizes effectively, increasing $\overline{F_1}$ and agreement while decreasing variance across most source-target pairs. Finally, when trained on only a subset of prompt perturbations and evaluated on held-out formats, $F^2C$ consistently improves both performance and agreement while reducing variance. These findings highlight $F^2C$ as an effective unsupervised method for enhancing LLM consistency, performance, and generalization under prompt perturbations. Code is available at this https URL.</li>
</ul>

<h3>Title: Event Interval Modulation: A Novel Scheme for Event-based Optical Camera Communication</h3>
<ul>
<li><strong>Authors: </strong>Miu Sumino, Mayu Ishii, Shun Kaizu, Daisuke Hisano, Yu Nakayama</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14245">https://arxiv.org/abs/2510.14245</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14245">https://arxiv.org/pdf/2510.14245</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14245]] Event Interval Modulation: A Novel Scheme for Event-based Optical Camera Communication(https://arxiv.org/abs/2510.14245)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Optical camera communication (OCC) represents a promising visible light communication technology. Nonetheless, typical OCC systems utilizing frame-based cameras are encumbered by limitations, including low bit rate and high processing load. To address these issues, OCC system utilizing an event-based vision sensor (EVS) as receivers have been proposed. The EVS enables high-speed, low-latency, and robust communication due to its asynchronous operation and high dynamic range. In existing event-based OCC systems, conventional modulation schemes such as on-off keying (OOK) and pulse position modulation have been applied, however, to the best of our knowledge, no modulation method has been proposed that fully exploits the unique characteristics of the EVS. This paper proposes a novel modulation scheme, called the event interval modulation (EIM) scheme, specifically designed for event-based OCC. EIM enables improvement in transmission speed by modulating information using the intervals between events. This paper proposes a theoretical model of EIM and conducts a proof-of-concept experiment. First, the parameters of the EVS are tuned and customized to optimize the frequency response specifically for EIM. Then, the maximum modulation order usable in EIM is determined experimentally. We conduct transmission experiments based on the obtained parameters. Finally, we report successful transmission at 28 kbps over 10 meters and 8.4 kbps over 50 meters in an indoor environment. This sets a new benchmark for bit rate in event-based OCC systems.</li>
</ul>

<h3>Title: Policy Regularized Distributionally Robust Markov Decision Processes with Linear Function Approximation</h3>
<ul>
<li><strong>Authors: </strong>Jingwen Gu, Yiting He, Zhishuai Liu, Pan Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14246">https://arxiv.org/abs/2510.14246</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14246">https://arxiv.org/pdf/2510.14246</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14246]] Policy Regularized Distributionally Robust Markov Decision Processes with Linear Function Approximation(https://arxiv.org/abs/2510.14246)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Decision-making under distribution shift is a central challenge in reinforcement learning (RL), where training and deployment environments differ. We study this problem through the lens of robust Markov decision processes (RMDPs), which optimize performance against adversarial transition dynamics. Our focus is the online setting, where the agent has only limited interaction with the environment, making sample efficiency and exploration especially critical. Policy optimization, despite its success in standard RL, remains theoretically and empirically underexplored in robust RL. To bridge this gap, we propose \textbf{D}istributionally \textbf{R}obust \textbf{R}egularized \textbf{P}olicy \textbf{O}ptimization algorithm (DR-RPO), a model-free online policy optimization method that learns robust policies with sublinear regret. To enable tractable optimization within the softmax policy class, DR-RPO incorporates reference-policy regularization, yielding RMDP variants that are doubly constrained in both transitions and policies. To scale to large state-action spaces, we adopt the $d$-rectangular linear MDP formulation and combine linear function approximation with an upper confidence bonus for optimistic exploration. We provide theoretical guarantees showing that policy optimization can achieve polynomial suboptimality bounds and sample efficiency in robust RL, matching the performance of value-based approaches. Finally, empirical results across diverse domains corroborate our theory and demonstrate the robustness of DR-RPO.</li>
</ul>

<h3>Title: A Physics Prior-Guided Dual-Stream Attention Network for Motion Prediction of Elastic Bragg Breakwaters</h3>
<ul>
<li><strong>Authors: </strong>Lianzi Jiang, Jianxin Zhang, Xinyu Han, Huanhe Dong, Xiangrong Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14250">https://arxiv.org/abs/2510.14250</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14250">https://arxiv.org/pdf/2510.14250</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14250]] A Physics Prior-Guided Dual-Stream Attention Network for Motion Prediction of Elastic Bragg Breakwaters(https://arxiv.org/abs/2510.14250)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate motion response prediction for elastic Bragg breakwaters is critical for their structural safety and operational integrity in marine environments. However, conventional deep learning models often exhibit limited generalization capabilities when presented with unseen sea states. These deficiencies stem from the neglect of natural decay observed in marine systems and inadequate modeling of wave-structure interaction (WSI). To overcome these challenges, this study proposes a novel Physics Prior-Guided Dual-Stream Attention Network (PhysAttnNet). First, the decay bidirectional self-attention (DBSA) module incorporates a learnable temporal decay to assign higher weights to recent states, aiming to emulate the natural decay phenomenon. Meanwhile, the phase differences guided bidirectional cross-attention (PDG-BCA) module explicitly captures the bidirectional interaction and phase relationship between waves and the structure using a cosine-based bias within a bidirectional cross-computation paradigm. These streams are synergistically integrated through a global context fusion (GCF) module. Finally, PhysAttnNet is trained with a hybrid time-frequency loss that jointly minimizes time-domain prediction errors and frequency-domain spectral discrepancies. Comprehensive experiments on wave flume datasets demonstrate that PhysAttnNet significantly outperforms mainstream models. Furthermore,cross-scenario generalization tests validate the model's robustness and adaptability to unseen environments, highlighting its potential as a framework to develop predictive models for complex systems in ocean engineering.</li>
</ul>

<h3>Title: MoM: Mixtures of Scenario-Aware Document Memories for Retrieval-Augmented Generation Systems</h3>
<ul>
<li><strong>Authors: </strong>Jihao Zhao, Zhiyuan Ji, Simin Niu, Hanyu Wang, Feiyu Xiong, Zhiyu Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14252">https://arxiv.org/abs/2510.14252</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14252">https://arxiv.org/pdf/2510.14252</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14252]] MoM: Mixtures of Scenario-Aware Document Memories for Retrieval-Augmented Generation Systems(https://arxiv.org/abs/2510.14252)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>The traditional RAG paradigm, which typically engages in the comprehension of relevant text chunks in response to received queries, inherently restricts both the depth of knowledge internalization and reasoning capabilities. To address this limitation, our research transforms the text processing in RAG from passive chunking to proactive understanding, defining this process as document memory extraction with the objective of simulating human cognitive processes during reading. Building upon this, we propose the Mixtures of scenario-aware document Memories (MoM) framework, engineered to efficiently handle documents from multiple domains and train small language models (SLMs) to acquire the ability to proactively explore and construct document memories. The MoM initially instructs large language models (LLMs) to simulate domain experts in generating document logical outlines, thereby directing structured chunking and core content extraction. It employs a multi-path sampling and multi-perspective evaluation mechanism, specifically designing comprehensive metrics that represent chunk clarity and extraction completeness to select the optimal document memories. Additionally, to infuse deeper human-like reading abilities during the training of SLMs, we incorporate a reverse reasoning strategy, which deduces refined expert thinking paths from high-quality outcomes. Finally, leveraging diverse forms of content generated by MoM, we develop a three-layer document memory retrieval mechanism, which is grounded in our theoretical proof from the perspective of probabilistic modeling. Extensive experimental results across three distinct domains demonstrate that the MoM framework not only resolves text chunking challenges in existing RAG systems, providing LLMs with semantically complete document memories, but also paves the way for SLMs to achieve human-centric intelligent text processing.</li>
</ul>

<h3>Title: Generalist vs Specialist Time Series Foundation Models: Investigating Potential Emergent Behaviors in Assessing Human Health Using PPG Signals</h3>
<ul>
<li><strong>Authors: </strong>Saurabh Kataria, Yi Wu, Zhaoliang Chen, Hyunjung Gloria Kwak, Yuhao Xu, Lovely Yeswanth Panchumarthi, Ran Xiao, Jiaying Lu, Ayca Ermis, Anni Zhao, Runze Yan, Alex Federov, Zewen Liu, Xu Wu, Wei Jin, Carl Yang, Jocelyn Grunwell, Stephanie R. Brown, Amit Shah, Craig Jabaley, Tim Buchman, Sivasubramanium V Bhavani, Randall J. Lee, Xiao Hu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14254">https://arxiv.org/abs/2510.14254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14254">https://arxiv.org/pdf/2510.14254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14254]] Generalist vs Specialist Time Series Foundation Models: Investigating Potential Emergent Behaviors in Assessing Human Health Using PPG Signals(https://arxiv.org/abs/2510.14254)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair</a></li>
<li><strong>Abstract: </strong>Foundation models are large-scale machine learning models that are pre-trained on massive amounts of data and can be adapted for various downstream tasks. They have been extensively applied to tasks in Natural Language Processing and Computer Vision with models such as GPT, BERT, and CLIP. They are now also increasingly gaining attention in time-series analysis, particularly for physiological sensing. However, most time series foundation models are specialist models - with data in pre-training and testing of the same type, such as Electrocardiogram, Electroencephalogram, and Photoplethysmogram (PPG). Recent works, such as MOMENT, train a generalist time series foundation model with data from multiple domains, such as weather, traffic, and electricity. This paper aims to conduct a comprehensive benchmarking study to compare the performance of generalist and specialist models, with a focus on PPG signals. Through an extensive suite of total 51 tasks covering cardiac state assessment, laboratory value estimation, and cross-modal inference, we comprehensively evaluate both models across seven dimensions, including win score, average performance, feature quality, tuning gain, performance variance, transferability, and scalability. These metrics jointly capture not only the models' capability but also their adaptability, robustness, and efficiency under different fine-tuning strategies, providing a holistic understanding of their strengths and limitations for diverse downstream scenarios. In a full-tuning scenario, we demonstrate that the specialist model achieves a 27% higher win score. Finally, we provide further analysis on generalization, fairness, attention visualizations, and the importance of training data choice.</li>
</ul>

<h3>Title: Identity-Preserving Image-to-Video Generation via Reward-Guided Optimization</h3>
<ul>
<li><strong>Authors: </strong>Liao Shen, Wentao Jiang, Yiran Zhu, Tiezheng Ge, Zhiguo Cao, Bo Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14255">https://arxiv.org/abs/2510.14255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14255">https://arxiv.org/pdf/2510.14255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14255]] Identity-Preserving Image-to-Video Generation via Reward-Guided Optimization(https://arxiv.org/abs/2510.14255)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in image-to-video (I2V) generation have achieved remarkable progress in synthesizing high-quality, temporally coherent videos from static images. Among all the applications of I2V, human-centric video generation includes a large portion. However, existing I2V models encounter difficulties in maintaining identity consistency between the input human image and the generated video, especially when the person in the video exhibits significant expression changes and movements. This issue becomes critical when the human face occupies merely a small fraction of the image. Since humans are highly sensitive to identity variations, this poses a critical yet under-explored challenge in I2V generation. In this paper, we propose Identity-Preserving Reward-guided Optimization (IPRO), a novel video diffusion framework based on reinforcement learning to enhance identity preservation. Instead of introducing auxiliary modules or altering model architectures, our approach introduces a direct and effective tuning algorithm that optimizes diffusion models using a face identity scorer. To improve performance and accelerate convergence, our method backpropagates the reward signal through the last steps of the sampling chain, enabling richer gradient feedback. We also propose a novel facial scoring mechanism that treats faces in ground-truth videos as facial feature pools, providing multi-angle facial information to enhance generalization. A KL-divergence regularization is further incorporated to stabilize training and prevent overfitting to the reward signal. Extensive experiments on Wan 2.2 I2V model and our in-house I2V model demonstrate the effectiveness of our method. Our project and code are available at \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: CAST: Compositional Analysis via Spectral Tracking for Understanding Transformer Layer Functions</h3>
<ul>
<li><strong>Authors: </strong>Zihao Fu, Ming Liao, Chris Russell, Zhenguang G. Cai</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14262">https://arxiv.org/abs/2510.14262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14262">https://arxiv.org/pdf/2510.14262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14262]] CAST: Compositional Analysis via Spectral Tracking for Understanding Transformer Layer Functions(https://arxiv.org/abs/2510.14262)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, interpretability, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Large language models have achieved remarkable success but remain largely black boxes with poorly understood internal mechanisms. To address this limitation, many researchers have proposed various interpretability methods including mechanistic analysis, probing classifiers, and activation visualization, each providing valuable insights from different perspectives. Building upon this rich landscape of complementary approaches, we introduce CAST (Compositional Analysis via Spectral Tracking), a probe-free framework that contributes a novel perspective by analyzing transformer layer functions through direct transformation matrix estimation and comprehensive spectral analysis. CAST offers complementary insights to existing methods by estimating the realized transformation matrices for each layer using Moore-Penrose pseudoinverse and applying spectral analysis with six interpretable metrics characterizing layer behavior. Our analysis reveals distinct behaviors between encoder-only and decoder-only models, with decoder models exhibiting compression-expansion cycles while encoder models maintain consistent high-rank processing. Kernel analysis further demonstrates functional relationship patterns between layers, with CKA similarity matrices clearly partitioning layers into three phases: feature extraction, compression, and specialization.</li>
</ul>

<h3>Title: Experimental Demonstration of Event-based Optical Camera Communication in Long-Range Outdoor Environment</h3>
<ul>
<li><strong>Authors: </strong>Miu Sumino, Mayu Ishii, Shun Kaizu, Daisuke Hisano, Yu Nakayama</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14266">https://arxiv.org/abs/2510.14266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14266">https://arxiv.org/pdf/2510.14266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14266]] Experimental Demonstration of Event-based Optical Camera Communication in Long-Range Outdoor Environment(https://arxiv.org/abs/2510.14266)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We propose a robust demodulation scheme for optical camera communication systems using an event-based vision sensor, combining OOK with toggle demodulation and a digital phase-locked loop. This is the first report to achieve a $\mathrm{BER} < 10^{-3}$ at 200m-60kbps and 400m-30kbps in outdoor experiments.</li>
</ul>

<h3>Title: Nonparametric Data Attribution for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Yutian Zhao, Chao Du, Xiaosen Zheng, Tianyu Pang, Min Lin</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14269">https://arxiv.org/abs/2510.14269</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14269">https://arxiv.org/pdf/2510.14269</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14269]] Nonparametric Data Attribution for Diffusion Models(https://arxiv.org/abs/2510.14269)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Data attribution for generative models seeks to quantify the influence of individual training examples on model outputs. Existing methods for diffusion models typically require access to model gradients or retraining, limiting their applicability in proprietary or large-scale settings. We propose a nonparametric attribution method that operates entirely on data, measuring influence via patch-level similarity between generated and training images. Our approach is grounded in the analytical form of the optimal score function and naturally extends to multiscale representations, while remaining computationally efficient through convolution-based acceleration. In addition to producing spatially interpretable attributions, our framework uncovers patterns that reflect intrinsic relationships between training data and outputs, independent of any specific model. Experiments demonstrate that our method achieves strong attribution performance, closely matching gradient-based approaches and substantially outperforming existing nonparametric baselines. Code is available at this https URL.</li>
</ul>

<h3>Title: GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and Geometric Filtering</h3>
<ul>
<li><strong>Authors: </strong>Alexander Valverde, Brian Xu, Yuyin Zhou, Meng Xu, Hongyun Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14270">https://arxiv.org/abs/2510.14270</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14270">https://arxiv.org/pdf/2510.14270</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14270]] GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and Geometric Filtering(https://arxiv.org/abs/2510.14270)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Scene reconstruction has emerged as a central challenge in computer vision, with approaches such as Neural Radiance Fields (NeRF) and Gaussian Splatting achieving remarkable progress. While Gaussian Splatting demonstrates strong performance on large-scale datasets, it often struggles to capture fine details or maintain realism in regions with sparse coverage, largely due to the inherent limitations of sparse 3D training data. In this work, we propose GauSSmart, a hybrid method that effectively bridges 2D foundational models and 3D Gaussian Splatting reconstruction. Our approach integrates established 2D computer vision techniques, including convex filtering and semantic feature supervision from foundational models such as DINO, to enhance Gaussian-based scene reconstruction. By leveraging 2D segmentation priors and high-dimensional feature embeddings, our method guides the densification and refinement of Gaussian splats, improving coverage in underrepresented areas and preserving intricate structural details. We validate our approach across three datasets, where GauSSmart consistently outperforms existing Gaussian Splatting in the majority of evaluated scenes. Our results demonstrate the significant potential of hybrid 2D-3D approaches, highlighting how the thoughtful combination of 2D foundational models with 3D reconstruction pipelines can overcome the limitations inherent in either approach alone.</li>
</ul>

<h3>Title: Less is More: Denoising Knowledge Graphs For Retrieval Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Yilun Zheng, Dan Yang, Jie Li, Lin Shang, Lihui Chen, Jiahao Xu, Sitao Luan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14271">https://arxiv.org/abs/2510.14271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14271">https://arxiv.org/pdf/2510.14271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14271]] Less is More: Denoising Knowledge Graphs For Retrieval Augmented Generation(https://arxiv.org/abs/2510.14271)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) systems enable large language models (LLMs) instant access to relevant information for the generative process, demonstrating their superior performance in addressing common LLM challenges such as hallucination, factual inaccuracy, and the knowledge cutoff. Graph-based RAG further extends this paradigm by incorporating knowledge graphs (KGs) to leverage rich, structured connections for more precise and inferential responses. A critical challenge, however, is that most Graph-based RAG systems rely on LLMs for automated KG construction, often yielding noisy KGs with redundant entities and unreliable relationships. This noise degrades retrieval and generation performance while also increasing computational cost. Crucially, current research does not comprehensively address the denoising problem for LLM-generated KGs. In this paper, we introduce DEnoised knowledge Graphs for Retrieval Augmented Generation (DEG-RAG), a framework that addresses these challenges through: (1) entity resolution, which eliminates redundant entities, and (2) triple reflection, which removes erroneous relations. Together, these techniques yield more compact, higher-quality KGs that significantly outperform their unprocessed counterparts. Beyond the methods, we conduct a systematic evaluation of entity resolution for LLM-generated KGs, examining different blocking strategies, embedding choices, similarity metrics, and entity merging techniques. To the best of our knowledge, this is the first comprehensive exploration of entity resolution in LLM-generated KGs. Our experiments demonstrate that this straightforward approach not only drastically reduces graph size but also consistently improves question answering performance across diverse popular Graph-based RAG variants.</li>
</ul>

<h3>Title: CLEAR: Causal Learning Framework For Robust Histopathology Tumor Detection Under Out-Of-Distribution Shifts</h3>
<ul>
<li><strong>Authors: </strong>Kieu-Anh Truong Thi, Huy-Hieu Pham, Duc-Trong Le</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14273">https://arxiv.org/abs/2510.14273</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14273">https://arxiv.org/pdf/2510.14273</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14273]] CLEAR: Causal Learning Framework For Robust Histopathology Tumor Detection Under Out-Of-Distribution Shifts(https://arxiv.org/abs/2510.14273)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Domain shift in histopathology, often caused by differences in acquisition processes or data sources, poses a major challenge to the generalization ability of deep learning models. Existing methods primarily rely on modeling statistical correlations by aligning feature distributions or introducing statistical variation, yet they often overlook causal relationships. In this work, we propose a novel causal-inference-based framework that leverages semantic features while mitigating the impact of confounders. Our method implements the front-door principle by designing transformation strategies that explicitly incorporate mediators and observed tissue slides. We validate our method on the CAMELYON17 dataset and a private histopathology dataset, demonstrating consistent performance gains across unseen domains. As a result, our approach achieved up to a 7% improvement in both the CAMELYON17 dataset and the private histopathology dataset, outperforming existing baselines. These results highlight the potential of causal inference as a powerful tool for addressing domain shift in histopathology image analysis.</li>
</ul>

<h3>Title: Qwen3Guard Technical Report</h3>
<ul>
<li><strong>Authors: </strong>Haiquan Zhao, Chenhan Yuan, Fei Huang, Xiaomeng Hu, Yichang Zhang, An Yang, Bowen Yu, Dayiheng Liu, Jingren Zhou, Junyang Lin, Baosong Yang, Chen Cheng, Jialong Tang, Jiandong Jiang, Jianwei Zhang, Jijie Xu, Ming Yan, Minmin Sun, Pei Zhang, Pengjun Xie, Qiaoyu Tang, Qin Zhu, Rong Zhang, Shibin Wu, Shuo Zhang, Tao He, Tianyi Tang, Tingyu Xia, Wei Liao, Weizhou Shen, Wenbiao Yin, Wenmeng Zhou, Wenyuan Yu, Xiaobin Wang, Xiaodong Deng, Xiaodong Xu, Xinyu Zhang, Yang Liu, Yeqiu Li, Yi Zhang, Yong Jiang, Yu Wan, Yuxin Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14276">https://arxiv.org/abs/2510.14276</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14276">https://arxiv.org/pdf/2510.14276</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14276]] Qwen3Guard Technical Report(https://arxiv.org/abs/2510.14276)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) become more capable and widely used, ensuring the safety of their outputs is increasingly critical. Existing guardrail models, though useful in static evaluation settings, face two major limitations in real-world applications: (1) they typically output only binary "safe/unsafe" labels, which can be interpreted inconsistently across diverse safety policies, rendering them incapable of accommodating varying safety tolerances across domains; and (2) they require complete model outputs before performing safety checks, making them fundamentally incompatible with streaming LLM inference, thereby preventing timely intervention during generation and increasing exposure to harmful partial outputs. To address these challenges, we present Qwen3Guard, a series of multilingual safety guardrail models with two specialized variants: Generative Qwen3Guard, which casts safety classification as an instruction-following task to enable fine-grained tri-class judgments (safe, controversial, unsafe); and Stream Qwen3Guard, which introduces a token-level classification head for real-time safety monitoring during incremental text generation. Both variants are available in three sizes (0.6B, 4B, and 8B parameters) and support up to 119 languages and dialects, providing comprehensive, scalable, and low-latency safety moderation for global LLM deployments. Evaluated across English, Chinese, and multilingual benchmarks, Qwen3Guard achieves state-of-the-art performance in both prompt and response safety classification. All models are released under the Apache 2.0 license for public use.</li>
</ul>

<h3>Title: PRISM: Agentic Retrieval with LLMs for Multi-Hop Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Md Mahadi Hasan Nahid, Davood Rafiei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14278">https://arxiv.org/abs/2510.14278</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14278">https://arxiv.org/pdf/2510.14278</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14278]] PRISM: Agentic Retrieval with LLMs for Multi-Hop Question Answering(https://arxiv.org/abs/2510.14278)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval plays a central role in multi-hop question answering (QA), where answering complex questions requires gathering multiple pieces of evidence. We introduce an Agentic Retrieval System that leverages large language models (LLMs) in a structured loop to retrieve relevant evidence with high precision and recall. Our framework consists of three specialized agents: a Question Analyzer that decomposes a multi-hop question into sub-questions, a Selector that identifies the most relevant context for each sub-question (focusing on precision), and an Adder that brings in any missing evidence (focusing on recall). The iterative interaction between Selector and Adder yields a compact yet comprehensive set of supporting passages. In particular, it achieves higher retrieval accuracy while filtering out distracting content, enabling downstream QA models to surpass full-context answer accuracy while relying on significantly less irrelevant information. Experiments on four multi-hop QA benchmarks -- HotpotQA, 2WikiMultiHopQA, MuSiQue, and MultiHopRAG -- demonstrates that our approach consistently outperforms strong baselines.</li>
</ul>

<h3>Title: Beyond a Single Perspective: Towards a Realistic Evaluation of Website Fingerprinting Attacks</h3>
<ul>
<li><strong>Authors: </strong>Xinhao Deng, Jingyou Chen, Linxiao Yu, Yixiang Zhang, Zhongyi Gu, Changhao Qiu, Xiyuan Zhao, Ke Xu, Qi Li</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14283">https://arxiv.org/abs/2510.14283</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14283">https://arxiv.org/pdf/2510.14283</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14283]] Beyond a Single Perspective: Towards a Realistic Evaluation of Website Fingerprinting Attacks(https://arxiv.org/abs/2510.14283)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Website Fingerprinting (WF) attacks exploit patterns in encrypted traffic to infer the websites visited by users, posing a serious threat to anonymous communication systems. Although recent WF techniques achieve over 90% accuracy in controlled experimental settings, most studies remain confined to single scenarios, overlooking the complexity of real-world environments. This paper presents the first systematic and comprehensive evaluation of existing WF attacks under diverse realistic conditions, including defense mechanisms, traffic drift, multi-tab browsing, early-stage detection, open-world settings, and few-shot scenarios. Experimental results show that many WF techniques with strong performance in isolated settings degrade significantly when facing other conditions. Since real-world environments often combine multiple challenges, current WF attacks are difficult to apply directly in practice. This study highlights the limitations of WF attacks and introduces a multidimensional evaluation framework, offering critical insights for developing more robust and practical WF attacks.</li>
</ul>

<h3>Title: Rethinking Schema Linking: A Context-Aware Bidirectional Retrieval Approach for Text-to-SQL</h3>
<ul>
<li><strong>Authors: </strong>Md Mahadi Hasan Nahid, Davood Rafiei, Weiwei Zhang, Yong Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14296">https://arxiv.org/abs/2510.14296</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14296">https://arxiv.org/pdf/2510.14296</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14296]] Rethinking Schema Linking: A Context-Aware Bidirectional Retrieval Approach for Text-to-SQL(https://arxiv.org/abs/2510.14296)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>Schema linking -- the process of aligning natural language questions with database schema elements -- is a critical yet underexplored component of Text-to-SQL systems. While recent methods have focused primarily on improving SQL generation, they often neglect the retrieval of relevant schema elements, which can lead to hallucinations and execution failures. In this work, we propose a context-aware bidirectional schema retrieval framework that treats schema linking as a standalone problem. Our approach combines two complementary strategies: table-first retrieval followed by column selection, and column-first retrieval followed by table selection. It is further augmented with techniques such as question decomposition, keyword extraction, and keyphrase extraction. Through comprehensive evaluations on challenging benchmarks such as BIRD and Spider, we demonstrate that our method significantly improves schema recall while reducing false positives. Moreover, SQL generation using our retrieved schema consistently outperforms full-schema baselines and closely approaches oracle performance, all without requiring query refinement. Notably, our method narrows the performance gap between full and perfect schema settings by 50\%. Our findings highlight schema linking as a powerful lever for enhancing Text-to-SQL accuracy and efficiency.</li>
</ul>

<h3>Title: TED++: Submanifold-Aware Backdoor Detection via Layerwise Tubular-Neighbourhood Screening</h3>
<ul>
<li><strong>Authors: </strong>Nam Le, Leo Yu Zhang, Kewen Liao, Shirui Pan, Wei Luo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14299">https://arxiv.org/abs/2510.14299</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14299">https://arxiv.org/pdf/2510.14299</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14299]] TED++: Submanifold-Aware Backdoor Detection via Layerwise Tubular-Neighbourhood Screening(https://arxiv.org/abs/2510.14299)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, steal</a></li>
<li><strong>Abstract: </strong>As deep neural networks power increasingly critical applications, stealthy backdoor attacks, where poisoned training inputs trigger malicious model behaviour while appearing benign, pose a severe security risk. Many existing defences are vulnerable when attackers exploit subtle distance-based anomalies or when clean examples are scarce. To meet this challenge, we introduce TED++, a submanifold-aware framework that effectively detects subtle backdoors that evade existing defences. TED++ begins by constructing a tubular neighbourhood around each class's hidden-feature manifold, estimating its local ``thickness'' from a handful of clean activations. It then applies Locally Adaptive Ranking (LAR) to detect any activation that drifts outside the admissible tube. By aggregating these LAR-adjusted ranks across all layers, TED++ captures how faithfully an input remains on the evolving class submanifolds. Based on such characteristic ``tube-constrained'' behaviour, TED++ flags inputs whose LAR-based ranking sequences deviate significantly. Extensive experiments are conducted on benchmark datasets and tasks, demonstrating that TED++ achieves state-of-the-art detection performance under both adaptive-attack and limited-data scenarios. Remarkably, even with only five held-out examples per class, TED++ still delivers near-perfect detection, achieving gains of up to 14\% in AUROC over the next-best method. The code is publicly available at this https URL.</li>
</ul>

<h3>Title: Constraint-Driven Small Language Models Based on Agent and OpenAlex Knowledge Graph: Mining Conceptual Pathways and Discovering Innovation Points in Academic Papers</h3>
<ul>
<li><strong>Authors: </strong>Ziye Xia, Sergei S. Ospichev</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14303">https://arxiv.org/abs/2510.14303</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14303">https://arxiv.org/pdf/2510.14303</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14303]] Constraint-Driven Small Language Models Based on Agent and OpenAlex Knowledge Graph: Mining Conceptual Pathways and Discovering Innovation Points in Academic Papers(https://arxiv.org/abs/2510.14303)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction</a></li>
<li><strong>Abstract: </strong>In recent years, the rapid increase in academic publications across various fields has posed severe challenges for academic paper analysis: scientists struggle to timely and comprehensively track the latest research findings and methodologies. Key concept extraction has proven to be an effective analytical paradigm, and its automation has been achieved with the widespread application of language models in industrial and scientific domains. However, existing paper databases are mostly limited to similarity matching and basic classification of key concepts, failing to deeply explore the relational networks between concepts. This paper is based on the OpenAlex opensource knowledge graph. By analyzing nearly 8,000 open-source paper data from Novosibirsk State University, we discovered a strong correlation between the distribution patterns of paper key concept paths and both innovation points and rare paths. We propose a prompt engineering-based key concept path analysis method. This method leverages small language models to achieve precise key concept extraction and innovation point identification, and constructs an agent based on a knowledge graph constraint mechanism to enhance analysis accuracy. Through fine-tuning of the Qwen and DeepSeek models, we achieved significant improvements in accuracy, with the models publicly available on the Hugging Face platform.</li>
</ul>

<h3>Title: Watermarking for Factuality: Guiding Vision-Language Models Toward Truth via Tri-layer Contrastive Decoding</h3>
<ul>
<li><strong>Authors: </strong>Kyungryul Back, Seongbeom Park, Milim Kim, Mincheol Kwon, SangHyeok Lee, Hyunyoung Lee, Junhee Cho, Seunghyun Park, Jinkyu Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14304">https://arxiv.org/abs/2510.14304</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14304">https://arxiv.org/pdf/2510.14304</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14304]] Watermarking for Factuality: Guiding Vision-Language Models Toward Truth via Tri-layer Contrastive Decoding(https://arxiv.org/abs/2510.14304)</code><input type="text"></li>
<li><strong>Keywords: </strong>watermark</a></li>
<li><strong>Abstract: </strong>Large Vision-Language Models (LVLMs) have recently shown promising results on various multimodal tasks, even achieving human-comparable performance in certain cases. Nevertheless, LVLMs remain prone to hallucinations -- they often rely heavily on a single modality or memorize training data without properly grounding their outputs. To address this, we propose a training-free, tri-layer contrastive decoding with watermarking, which proceeds in three steps: (1) select a mature layer and an amateur layer among the decoding layers, (2) identify a pivot layer using a watermark-related question to assess whether the layer is visually well-grounded, and (3) apply tri-layer contrastive decoding to generate the final output. Experiments on public benchmarks such as POPE, MME and AMBER demonstrate that our method achieves state-of-the-art performance in reducing hallucinations in LVLMs and generates more visually grounded responses.</li>
</ul>

<h3>Title: MathMist: A Parallel Multilingual Benchmark Dataset for Mathematical Problem Solving and Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Mahbub E Sobhani, Md. Faiyaz Abdullah Sayeedi, Tasnim Mohiuddin, Md Mofijul Islam, Swakkhar Shatabda</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14305">https://arxiv.org/abs/2510.14305</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14305">https://arxiv.org/pdf/2510.14305</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14305]] MathMist: A Parallel Multilingual Benchmark Dataset for Mathematical Problem Solving and Reasoning(https://arxiv.org/abs/2510.14305)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Mathematical reasoning remains one of the most challenging domains for large language models (LLMs), requiring not only linguistic understanding but also structured logical deduction and numerical precision. While recent LLMs demonstrate strong general-purpose reasoning abilities, their mathematical competence across diverse languages remains underexplored. Existing benchmarks primarily focus on English or a narrow subset of high-resource languages, leaving significant gaps in assessing multilingual and cross-lingual mathematical reasoning. To address this, we introduce MathMist, a parallel multilingual benchmark for mathematical problem solving and reasoning. MathMist encompasses over 21K aligned question-answer pairs across seven languages, representing a balanced coverage of high-, medium-, and low-resource linguistic settings. The dataset captures linguistic variety, multiple types of problem settings, and solution synthesizing capabilities. We systematically evaluate a diverse suite of models, including open-source small and medium LLMs, proprietary systems, and multilingual-reasoning-focused models, under zero-shot, chain-of-thought (CoT), and code-switched reasoning paradigms. Our results reveal persistent deficiencies in LLMs' ability to perform consistent and interpretable mathematical reasoning across languages, with pronounced degradation in low-resource settings. All the codes and data are available at GitHub: this https URL</li>
</ul>

<h3>Title: A Multi-domain Image Translative Diffusion StyleGAN for Iris Presentation Attack Detection</h3>
<ul>
<li><strong>Authors: </strong>Shivangi Yadav, Arun Ross</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14314">https://arxiv.org/abs/2510.14314</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14314">https://arxiv.org/pdf/2510.14314</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14314]] A Multi-domain Image Translative Diffusion StyleGAN for Iris Presentation Attack Detection(https://arxiv.org/abs/2510.14314)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, biometric, diffusion, generative</a></li>
<li><strong>Abstract: </strong>An iris biometric system can be compromised by presentation attacks (PAs) where artifacts such as artificial eyes, printed eye images, or cosmetic contact lenses are presented to the system. To counteract this, several presentation attack detection (PAD) methods have been developed. However, there is a scarcity of datasets for training and evaluating iris PAD techniques due to the implicit difficulties in constructing and imaging PAs. To address this, we introduce the Multi-domain Image Translative Diffusion StyleGAN (MID-StyleGAN), a new framework for generating synthetic ocular images that captures the PA and bonafide characteristics in multiple domains such as bonafide, printed eyes and cosmetic contact lens. MID-StyleGAN combines the strengths of diffusion models and generative adversarial networks (GANs) to produce realistic and diverse synthetic data. Our approach utilizes a multi-domain architecture that enables the translation between bonafide ocular images and different PA domains. The model employs an adaptive loss function tailored for ocular data to maintain domain consistency. Extensive experiments demonstrate that MID-StyleGAN outperforms existing methods in generating high-quality synthetic ocular images. The generated data was used to significantly enhance the performance of PAD systems, providing a scalable solution to the data scarcity problem in iris and ocular biometrics. For example, on the LivDet2020 dataset, the true detect rate at 1% false detect rate improved from 93.41% to 98.72%, showcasing the impact of the proposed method.</li>
</ul>

<h3>Title: Evaluating & Reducing Deceptive Dialogue From Language Models with Multi-turn RL</h3>
<ul>
<li><strong>Authors: </strong>Marwa Abdulhai, Ryan Cheng, Aryansh Shrivastava, Natasha Jaques, Yarin Gal, Sergey Levine</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14318">https://arxiv.org/abs/2510.14318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14318">https://arxiv.org/pdf/2510.14318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14318]] Evaluating & Reducing Deceptive Dialogue From Language Models with Multi-turn RL(https://arxiv.org/abs/2510.14318)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) interact with millions of people worldwide in applications such as customer support, education and healthcare. However, their ability to produce deceptive outputs, whether intentionally or inadvertently, poses significant safety concerns. The unpredictable nature of LLM behavior, combined with insufficient safeguards against hallucination, misinformation, and user manipulation, makes their misuse a serious, real-world risk. In this paper, we investigate the extent to which LLMs engage in deception within dialogue, and propose the belief misalignment metric to quantify deception. We evaluate deception across four distinct dialogue scenarios, using five established deception detection metrics and our proposed metric. Our findings reveal this novel deception measure correlates more closely with human judgments than any existing metrics we test. Additionally, our benchmarking of eight state-of-the-art models indicates that LLMs naturally exhibit deceptive behavior in approximately 26% of dialogue turns, even when prompted with seemingly benign objectives. When prompted to deceive, LLMs are capable of increasing deceptiveness by as much as 31% relative to baselines. Unexpectedly, models trained with RLHF, the predominant approach for ensuring the safety of widely-deployed LLMs, still exhibit deception at a rate of 43% on average. Given that deception in dialogue is a behavior that develops over an interaction history, its effective evaluation and mitigation necessitates moving beyond single-utterance analyses. We introduce a multi-turn reinforcement learning methodology to fine-tune LLMs to reduce deceptive behaviors, leading to a 77.6% reduction compared to other instruction-tuned models.</li>
</ul>

<h3>Title: LLM-ERM: Sample-Efficient Program Learning via LLM-Guided Search</h3>
<ul>
<li><strong>Authors: </strong>Shivam Singhal, Eran Malach, Tomaso Poggio, Tomer Galanti</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14331">https://arxiv.org/abs/2510.14331</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14331">https://arxiv.org/pdf/2510.14331</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14331]] LLM-ERM: Sample-Efficient Program Learning via LLM-Guided Search(https://arxiv.org/abs/2510.14331)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We seek algorithms for program learning that are both sample-efficient and computationally feasible. Classical results show that targets admitting short program descriptions (e.g., with short ``python code'') can be learned with a ``small'' number of examples (scaling with the size of the code) via length-first program enumeration, but the search is exponential in description length. Consequently, Gradient-based training avoids this cost yet can require exponentially many samples on certain short-program families. To address this gap, we introduce LLM-ERM, a propose-and-verify framework that replaces exhaustive enumeration with an LLM-guided search over candidate programs while retaining ERM-style selection on held-out data. Specifically, we draw $k$ candidates with a pretrained reasoning-augmented LLM, compile and check each on the data, and return the best verified hypothesis, with no feedback, adaptivity, or gradients. Theoretically, we show that coordinate-wise online mini-batch SGD requires many samples to learn certain short programs. {\em Empirically, LLM-ERM solves tasks such as parity variants, pattern matching, and primality testing with as few as 200 samples, while SGD-trained transformers overfit even with 100,000 samples}. These results indicate that language-guided program synthesis recovers much of the statistical efficiency of finite-class ERM while remaining computationally tractable, offering a practical route to learning succinct hypotheses beyond the reach of gradient-based training.</li>
</ul>

<h3>Title: A Robust Classification Method using Hybrid Word Embedding for Early Diagnosis of Alzheimer's Disease</h3>
<ul>
<li><strong>Authors: </strong>Yangyang Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14332">https://arxiv.org/abs/2510.14332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14332">https://arxiv.org/pdf/2510.14332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14332]] A Robust Classification Method using Hybrid Word Embedding for Early Diagnosis of Alzheimer's Disease(https://arxiv.org/abs/2510.14332)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Early detection of Alzheimer's Disease (AD) is greatly beneficial to AD patients, leading to early treatments that lessen symptoms and alleviating financial burden of health care. As one of the leading signs of AD, language capability changes can be used for early diagnosis of AD. In this paper, I develop a robust classification method using hybrid word embedding and fine-tuned hyperparameters to achieve state-of-the-art accuracy in the early detection of AD. Specifically, we create a hybrid word embedding based on word vectors from Doc2Vec and ELMo to obtain perplexity scores of the sentences. The scores identify whether a sentence is fluent or not and capture semantic context of the sentences. I enrich the word embedding by adding linguistic features to analyze syntax and semantics. Further, we input an embedded feature vector into logistic regression and fine tune hyperparameters throughout the pipeline. By tuning hyperparameters of the machine learning pipeline (e.g., model regularization parameter, learning rate and vector size of Doc2Vec, and vector size of ELMo), I achieve 91% classification accuracy and an Area Under the Curve (AUC) of 97% in distinguishing early AD from healthy subjects. Based on my knowledge, my model with 91% accuracy and 97% AUC outperforms the best existing NLP model for AD diagnosis with an accuracy of 88% [32]. I study the model stability through repeated experiments and find that the model is stable even though the training data is split randomly (standard deviation of accuracy = 0.0403; standard deviation of AUC = 0.0174). This affirms our proposed method is accurate and stable. This model can be used as a large-scale screening method for AD, as well as a complementary examination for doctors to detect AD.</li>
</ul>

<h3>Title: DARTS-GT: Differentiable Architecture Search for Graph Transformers with Quantifiable Instance-Specific Interpretability Analysis</h3>
<ul>
<li><strong>Authors: </strong>Shruti Sarika Chakraborty, Peter Minary</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14336">https://arxiv.org/abs/2510.14336</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14336">https://arxiv.org/pdf/2510.14336</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14336]] DARTS-GT: Differentiable Architecture Search for Graph Transformers with Quantifiable Instance-Specific Interpretability Analysis(https://arxiv.org/abs/2510.14336)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Graph Transformers (GTs) have emerged as powerful architectures for graph-structured data, yet remain constrained by rigid designs and lack quantifiable interpretability. Current state-of-the-art GTs commit to fixed GNN types across all layers, missing potential benefits of depth-specific component selection, while their complex architectures become opaque where performance gains cannot be distinguished between meaningful patterns and spurious correlations. We redesign GT attention through asymmetry, decoupling structural encoding from feature representation: queries derive from node features while keys and values come from GNN transformations. Within this framework, we use Differentiable ARchiTecture Search (DARTS) to select optimal GNN operators at each layer, enabling depth-wise heterogeneity inside transformer attention itself (DARTS-GT). To understand discovered architectures, we develop the first quantitative interpretability framework for GTs through causal ablation. Our metrics (Head-deviation, Specialization, and Focus), identify which heads and nodes drive predictions while enabling model comparison. Experiments across eight benchmarks show DARTS-GT achieves state-of-the-art on four datasets while remaining competitive on others, with discovered architectures revealing dataset-specific patterns. Our interpretability analysis reveals that visual attention salience and causal importance do not always correlate, indicating widely used visualization approaches may miss components that actually matter. Crucially, heterogeneous architectures found by DARTS-GT consistently produced more interpretable models than baselines, establishing that Graph Transformers need not choose between performance and interpretability.</li>
</ul>

<h3>Title: Stop-RAG: Value-Based Retrieval Control for Iterative RAG</h3>
<ul>
<li><strong>Authors: </strong>Jaewan Park, Solbee Cho, Jay-Yoon Lee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14337">https://arxiv.org/abs/2510.14337</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14337">https://arxiv.org/pdf/2510.14337</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14337]] Stop-RAG: Value-Based Retrieval Control for Iterative RAG(https://arxiv.org/abs/2510.14337)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Iterative retrieval-augmented generation (RAG) enables large language models to answer complex multi-hop questions, but each additional loop increases latency, costs, and the risk of introducing distracting evidence, motivating the need for an efficient stopping strategy. Existing methods either use a predetermined number of iterations or rely on confidence proxies that poorly reflect whether more retrieval will actually help. We cast iterative RAG as a finite-horizon Markov decision process and introduce Stop-RAG, a value-based controller that adaptively decides when to stop retrieving. Trained with full-width forward-view Q($\lambda$) targets from complete trajectories, Stop-RAG learns effective stopping policies while remaining compatible with black-box APIs and existing pipelines. On multi-hop question-answering benchmarks, Stop-RAG consistently outperforms both fixed-iteration baselines and prompting-based stopping with LLMs. These results highlight adaptive stopping as a key missing component in current agentic systems, and demonstrate that value-based control can improve the accuracy of RAG systems.</li>
</ul>

<h3>Title: BinCtx: Multi-Modal Representation Learning for Robust Android App Behavior Detection</h3>
<ul>
<li><strong>Authors: </strong>Zichen Liu, Shao Yang, Xusheng Xiao</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14344">https://arxiv.org/abs/2510.14344</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14344">https://arxiv.org/pdf/2510.14344</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14344]] BinCtx: Multi-Modal Representation Learning for Robust Android App Behavior Detection(https://arxiv.org/abs/2510.14344)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust</a></li>
<li><strong>Abstract: </strong>Mobile app markets host millions of apps, yet undesired behaviors (e.g., disruptive ads, illegal redirection, payment deception) remain hard to catch because they often do not rely on permission-protected APIs and can be easily camouflaged via UI or metadata edits. We present BINCTX, a learning approach that builds multi-modal representations of an app from (i) a global bytecode-as-image view that captures code-level semantics and family-style patterns, (ii) a contextual view (manifested actions, components, declared permissions, URL/IP constants) indicating how behaviors are triggered, and (iii) a third-party-library usage view summarizing invocation frequencies along inter-component call paths. The three views are embedded and fused to train a contextual-aware classifier. On real-world malware and benign apps, BINCTX attains a macro F1 of 94.73%, outperforming strong baselines by at least 14.92%. It remains robust under commercial obfuscation (F1 84% post-obfuscation) and is more resistant to adversarial samples than state-of-the-art bytecode-only systems.</li>
</ul>

<h3>Title: Vision-Centric Activation and Coordination for Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yunnan Wang, Fan Lu, Kecheng Zheng, Ziyuan Huang, Ziqiang Li, Wenjun Zeng, Xin Jin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14349">https://arxiv.org/abs/2510.14349</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14349">https://arxiv.org/pdf/2510.14349</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14349]] Vision-Centric Activation and Coordination for Multimodal Large Language Models(https://arxiv.org/abs/2510.14349)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) integrate image features from visual encoders with LLMs, demonstrating advanced comprehension capabilities. However, mainstream MLLMs are solely supervised by the next-token prediction of textual tokens, neglecting critical vision-centric information essential for analytical abilities. To track this dilemma, we introduce VaCo, which optimizes MLLM representations through Vision-Centric activation and Coordination from multiple vision foundation models (VFMs). VaCo introduces visual discriminative alignment to integrate task-aware perceptual features extracted from VFMs, thereby unifying the optimization of both textual and visual outputs in MLLMs. Specifically, we incorporate the learnable Modular Task Queries (MTQs) and Visual Alignment Layers (VALs) into MLLMs, activating specific visual signals under the supervision of diverse VFMs. To coordinate representation conflicts across VFMs, the crafted Token Gateway Mask (TGM) restricts the information flow among multiple groups of MTQs. Extensive experiments demonstrate that VaCo significantly improves the performance of different MLLMs on various benchmarks, showcasing its superior capabilities in visual comprehension.</li>
</ul>

<h3>Title: Beyond One World: Benchmarking Super Heros in Role-Playing Across Multiversal Contexts</h3>
<ul>
<li><strong>Authors: </strong>Perapard Ngokpol, Kun Kerdthaisong, Pasin Buakhaw, Pitikorn Khlaisamniang, Supasate Vorathammathorn, Piyalitt Ittichaiwong, Nutchanon Yongsatianchot</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14351">https://arxiv.org/abs/2510.14351</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14351">https://arxiv.org/pdf/2510.14351</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14351]] Beyond One World: Benchmarking Super Heros in Role-Playing Across Multiversal Contexts(https://arxiv.org/abs/2510.14351)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are increasingly used as role-playing agents, yet their capacity to faithfully and consistently portray version-specific characters -- for example, superheroes across comic and cinematic universes -- remains underexplored. Superhero canons such as Marvel and DC provide a rich testbed: decades of storytelling yield multiple incarnations of the same character with distinct histories, values, and moral codes. To study this problem, we introduce Beyond One World, a benchmark for character-grounded roleplay spanning 30 iconic heroes and 90 canon-specific versions. The benchmark comprises two tasks: (i) Canon Events, which probes factual recall of pivotal life stages, and (ii) Moral Dilemmas, which confronts models with ethically charged scenarios. We score responses for canonical accuracy and reasoning fidelity under a framework that separates internal deliberation ("thinking") from outward decisions ("acting"). We further propose Think-Act Matching, a metric that quantifies alignment between reasons and actions and serves as a proxy for model trustworthiness. Experiments across reasoning- and non-reasoning-oriented models yield three findings: (1) chain-of-thought prompting improves narrative coherence in weaker models but can reduce canonical accuracy in stronger ones; (2) cross-version generalization within a character remains a major obstacle; and (3) models often excel at either thinking or acting, but rarely both. Beyond One World exposes critical gaps in multiversal consistency and reasoning alignment, offering a challenging evaluation for role-playing LLMs.</li>
</ul>

<h3>Title: CURE: Confidence-driven Unified Reasoning Ensemble Framework for Medical Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Ziad Elshaer, Essam A. Rashed</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, physics.med-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14353">https://arxiv.org/abs/2510.14353</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14353">https://arxiv.org/pdf/2510.14353</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14353]] CURE: Confidence-driven Unified Reasoning Ensemble Framework for Medical Question Answering(https://arxiv.org/abs/2510.14353)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>High-performing medical Large Language Models (LLMs) typically require extensive fine-tuning with substantial computational resources, limiting accessibility for resource-constrained healthcare institutions. This study introduces a confidence-driven multi-model framework that leverages model diversity to enhance medical question answering without fine-tuning. Our framework employs a two-stage architecture: a confidence detection module assesses the primary model's certainty, and an adaptive routing mechanism directs low-confidence queries to Helper models with complementary knowledge for collaborative reasoning. We evaluate our approach using Qwen3-30B-A3B-Instruct, Phi-4 14B, and Gemma 2 12B across three medical benchmarks; MedQA, MedMCQA, and PubMedQA. Result demonstrate that our framework achieves competitive performance, with particularly strong results in PubMedQA (95.0\%) and MedMCQA (78.0\%). Ablation studies confirm that confidence-aware routing combined with multi-model collaboration substantially outperforms single-model approaches and uniform reasoning strategies. This work establishes that strategic model collaboration offers a practical, computationally efficient pathway to improve medical AI systems, with significant implications for democratizing access to advanced medical AI in resource-limited settings.</li>
</ul>

<h3>Title: On the Ability of LLMs to Handle Character-Level Perturbations: How Well and How?</h3>
<ul>
<li><strong>Authors: </strong>Anyun Zhuo, Xuefei Ning, Ningyuan Li, Yu Wang, Pinyan Lu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14365">https://arxiv.org/abs/2510.14365</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14365">https://arxiv.org/pdf/2510.14365</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14365]] On the Ability of LLMs to Handle Character-Level Perturbations: How Well and How?(https://arxiv.org/abs/2510.14365)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This work investigates the resilience of contemporary LLMs against frequent and structured character-level perturbations, specifically through the insertion of noisy characters after each input character. We introduce \nameshort{}, a practical method that inserts invisible Unicode control characters into text to discourage LLM misuse in scenarios such as online exam systems. Surprisingly, despite strong obfuscation that fragments tokenization and reduces the signal-to-noise ratio significantly, many LLMs still maintain notable performance. Through comprehensive evaluation across model-, problem-, and noise-related configurations, we examine the extent and mechanisms of this robustness, exploring both the handling of character-level tokenization and \textit{implicit} versus \textit{explicit} denoising mechanism hypotheses of character-level noises. We hope our findings on the low-level robustness of LLMs will shed light on the risks of their misuse and on the reliability of deploying LLMs across diverse applications.</li>
</ul>

<h3>Title: From Binary to Bilingual: How the National Weather Service is Using Artificial Intelligence to Develop a Comprehensive Translation Program</h3>
<ul>
<li><strong>Authors: </strong>Joseph E. Trujillo-Falcon, Monica L. Bozeman, Liam E. Llewellyn, Samuel T. Halvorson, Meryl Mizell, Stuti Deshpande, Bob Manning, Todd Fagin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14369">https://arxiv.org/abs/2510.14369</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14369">https://arxiv.org/pdf/2510.14369</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14369]] From Binary to Bilingual: How the National Weather Service is Using Artificial Intelligence to Develop a Comprehensive Translation Program(https://arxiv.org/abs/2510.14369)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>To advance a Weather-Ready Nation, the National Weather Service (NWS) is developing a systematic translation program to better serve the 68.8 million people in the U.S. who do not speak English at home. This article outlines the foundation of an automated translation tool for NWS products, powered by artificial intelligence. The NWS has partnered with LILT, whose patented training process enables large language models (LLMs) to adapt neural machine translation (NMT) tools for weather terminology and messaging. Designed for scalability across Weather Forecast Offices (WFOs) and National Centers, the system is currently being developed in Spanish, Simplified Chinese, Vietnamese, and other widely spoken non-English languages. Rooted in best practices for multilingual risk communication, the system provides accurate, timely, and culturally relevant translations, significantly reducing manual translation time and easing operational workloads across the NWS. To guide the distribution of these products, GIS mapping was used to identify language needs across different NWS regions, helping prioritize resources for the communities that need them most. We also integrated ethical AI practices throughout the program's design, ensuring that transparency, fairness, and human oversight guide how automated translations are created, evaluated, and shared with the public. This work has culminated into a website featuring experimental multilingual NWS products, including translated warnings, 7-day forecasts, and educational campaigns, bringing the country one step closer to a national warning system that reaches all Americans.</li>
</ul>

<h3>Title: Spatial Preference Rewarding for MLLMs Spatial Understanding</h3>
<ul>
<li><strong>Authors: </strong>Han Qiu, Peng Gao, Lewei Lu, Xiaoqin Zhang, Ling Shao, Shijian Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14374">https://arxiv.org/abs/2510.14374</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14374">https://arxiv.org/pdf/2510.14374</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14374]] Spatial Preference Rewarding for MLLMs Spatial Understanding(https://arxiv.org/abs/2510.14374)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models~(MLLMs) have demonstrated promising spatial understanding capabilities, such as referencing and grounding object descriptions. Despite their successes, MLLMs still fall short in fine-grained spatial perception abilities, such as generating detailed region descriptions or accurately localizing objects. Additionally, they often fail to respond to the user's requirements for desired fine-grained spatial understanding. This issue might arise because existing approaches primarily focus on tuning MLLMs to model pre-annotated instruction data to inject spatial knowledge, without direct supervision of MLLMs' actual responses. We address this issue by SPR, a Spatial Preference Rewarding~(SPR) approach that enhances MLLMs' spatial capabilities by rewarding MLLMs' detailed responses with precise object localization over vague or inaccurate responses. With randomly selected image regions and region descriptions from MLLMs, SPR introduces semantic and localization scores to comprehensively evaluate the text quality and localization quality in MLLM-generated descriptions. We also refine the MLLM descriptions with better localization accuracy and pair the best-scored refinement with the initial descriptions of the lowest score for direct preference optimization, thereby enhancing fine-grained alignment with visual input. Extensive experiments over standard referring and grounding benchmarks show that SPR improves MLLM spatial understanding capabilities effectively with minimal overhead in training. Data and code will be released at this https URL</li>
</ul>

<h3>Title: DOS: Directional Object Separation in Text Embeddings for Multi-Object Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Dongnam Byun, Jungwon Park, Jumgmin Ko, Changin Choi, Wonjong Rhee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14376">https://arxiv.org/abs/2510.14376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14376">https://arxiv.org/pdf/2510.14376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14376]] DOS: Directional Object Separation in Text Embeddings for Multi-Object Image Generation(https://arxiv.org/abs/2510.14376)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent progress in text-to-image (T2I) generative models has led to significant improvements in generating high-quality images aligned with text prompts. However, these models still struggle with prompts involving multiple objects, often resulting in object neglect or object mixing. Through extensive studies, we identify four problematic scenarios, Similar Shapes, Similar Textures, Dissimilar Background Biases, and Many Objects, where inter-object relationships frequently lead to such failures. Motivated by two key observations about CLIP embeddings, we propose DOS (Directional Object Separation), a method that modifies three types of CLIP text embeddings before passing them into text-to-image models. Experimental results show that DOS consistently improves the success rate of multi-object image generation and reduces object mixing. In human evaluations, DOS significantly outperforms four competing methods, receiving 26.24%-43.04% more votes across four benchmarks. These results highlight DOS as a practical and effective solution for improving multi-object image generation.</li>
</ul>

<h3>Title: PluriHop: Exhaustive, Recall-Sensitive QA over Distractor-Rich Corpora</h3>
<ul>
<li><strong>Authors: </strong>Mykolas Sveistrys, Richard Kunert</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14377">https://arxiv.org/abs/2510.14377</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14377">https://arxiv.org/pdf/2510.14377</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14377]] PluriHop: Exhaustive, Recall-Sensitive QA over Distractor-Rich Corpora(https://arxiv.org/abs/2510.14377)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models (LLMs) and retrieval-augmented generation (RAG) have enabled progress on question answering (QA) when relevant evidence is in one (single-hop) or multiple (multi-hop) passages. Yet many realistic questions about recurring report data - medical records, compliance filings, maintenance logs - require aggregation across all documents, with no clear stopping point for retrieval and high sensitivity to even one missed passage. We term these pluri-hop questions and formalize them by three criteria: recall sensitivity, exhaustiveness, and exactness. To study this setting, we introduce PluriHopWIND, a diagnostic multilingual dataset of 48 pluri-hop questions built from 191 real-world wind industry reports in German and English. We show that PluriHopWIND is 8-40% more repetitive than other common datasets and thus has higher density of distractor documents, better reflecting practical challenges of recurring report corpora. We test a traditional RAG pipeline as well as graph-based and multimodal variants, and find that none of the tested approaches exceed 40% in statement-wise F1 score. Motivated by this, we propose PluriHopRAG, a RAG architecture that follows a "check all documents individually, filter cheaply" approach: it (i) decomposes queries into document-level subquestions and (ii) uses a cross-encoder filter to discard irrelevant documents before costly LLM reasoning. We find that PluriHopRAG achieves relative F1 score improvements of 18-52% depending on base LLM. Despite its modest size, PluriHopWIND exposes the limitations of current QA systems on repetitive, distractor-rich corpora. PluriHopRAG's performance highlights the value of exhaustive retrieval and early filtering as a powerful alternative to top-k methods.</li>
</ul>

<h3>Title: Are My Optimized Prompts Compromised? Exploring Vulnerabilities of LLM-based Optimizers</h3>
<ul>
<li><strong>Authors: </strong>Andrew Zhao, Reshmi Ghosh, Vitor Carvalho, Emily Lawton, Keegan Hines, Gao Huang, Jack W. Stokes</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14381">https://arxiv.org/abs/2510.14381</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14381">https://arxiv.org/pdf/2510.14381</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14381]] Are My Optimized Prompts Compromised? Exploring Vulnerabilities of LLM-based Optimizers(https://arxiv.org/abs/2510.14381)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Large language model (LLM) systems now underpin everyday AI applications such as chatbots, computer-use assistants, and autonomous robots, where performance often depends on carefully designed prompts. LLM-based prompt optimizers reduce that effort by iteratively refining prompts from scored feedback, yet the security of this optimization stage remains underexamined. We present the first systematic analysis of poisoning risks in LLM-based prompt optimization. Using HarmBench, we find systems are substantially more vulnerable to manipulated feedback than to injected queries: feedback-based attacks raise attack success rate (ASR) by up to $\Delta$ASR = 0.48. We introduce a simple fake-reward attack that requires no access to the reward model and significantly increases vulnerability, and we propose a lightweight highlighting defense that reduces the fake-reward $\Delta$ASR from 0.23 to 0.07 without degrading utility. These results establish prompt optimization pipelines as a first-class attack surface and motivate stronger safeguards for feedback channels and optimization frameworks.</li>
</ul>

<h3>Title: DRBD-Mamba for Robust and Efficient Brain Tumor Segmentation with Analytical Insights</h3>
<ul>
<li><strong>Authors: </strong>Danish Ali, Ajmal Mian, Naveed Akhtar, Ghulam Mubashar Hassan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14383">https://arxiv.org/abs/2510.14383</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14383">https://arxiv.org/pdf/2510.14383</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14383]] DRBD-Mamba for Robust and Efficient Brain Tumor Segmentation with Analytical Insights(https://arxiv.org/abs/2510.14383)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Accurate brain tumor segmentation is significant for clinical diagnosis and treatment. It is challenging due to the heterogeneity of tumor subregions. Mamba-based State Space Models have demonstrated promising performance. However, they incur significant computational overhead due to sequential feature computation across multiple spatial axes. Moreover, their robustness across diverse BraTS data partitions remains largely unexplored, leaving a critical gap in reliable evaluation. To address these limitations, we propose dual-resolution bi-directional Mamba (DRBD-Mamba), an efficient 3D segmentation model that captures multi-scale long-range dependencies with minimal computational overhead. We leverage a space-filling curve to preserve spatial locality during 3D-to-1D feature mapping, thereby reducing reliance on computationally expensive multi-axial feature scans. To enrich feature representation, we propose a gated fusion module that adaptively integrates forward and reverse contexts, along with a quantization block that discretizes features to improve robustness. In addition, we propose five systematic folds on BraTS2023 for rigorous evaluation of segmentation techniques under diverse conditions and present detailed analysis of common failure scenarios. On the 20\% test set used by recent methods, our model achieves Dice improvements of 0.10\% for whole tumor, 1.75\% for tumor core, and 0.93\% for enhancing tumor. Evaluations on the proposed systematic five folds demonstrate that our model maintains competitive whole tumor accuracy while achieving clear average Dice gains of 0.86\% for tumor core and 1.45\% for enhancing tumor over existing state-of-the-art. Furthermore, our model attains 15 times improvement in efficiency while maintaining high segmentation accuracy, highlighting its robustness and computational advantage over existing approaches.</li>
</ul>

<h3>Title: Match & Mend: Minimally Invasive Local Reassembly for Patching N-day Vulnerabilities in ARM Binaries</h3>
<ul>
<li><strong>Authors: </strong>Sebastian Jänich, Merlin Sievers, Johannes Kinder</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14384">https://arxiv.org/abs/2510.14384</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14384">https://arxiv.org/pdf/2510.14384</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14384]] Match & Mend: Minimally Invasive Local Reassembly for Patching N-day Vulnerabilities in ARM Binaries(https://arxiv.org/abs/2510.14384)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure</a></li>
<li><strong>Abstract: </strong>Low-cost Internet of Things (IoT) devices are increasingly popular but often insecure due to poor update regimes. As a result, many devices run outdated and known-vulnerable versions of open-source software. We address this problem by proposing to patch IoT firmware at the binary level, without requiring vendor support. In particular, we introduce minimally invasive local reassembly, a new technique for automatically patching known (n-day) vulnerabilities in IoT firmware. Our approach is designed to minimize side effects and reduce the risk of introducing breaking changes. We systematically evaluate our approach both on 108 binaries within the controlled environment of the MAGMA benchmarks, as well as on 30 real-world Linux-based IoT firmware images from the KARONTE dataset. Our prototype successfully patches 83% of targeted vulnerabilities in MAGMA and 96% in the firmware dataset.</li>
</ul>

<h3>Title: SHaRe-SSM: An Oscillatory Spiking Neural Network for Target Variable Modeling in Long Sequences</h3>
<ul>
<li><strong>Authors: </strong>Kartikay Agrawal, Abhijeet Vikram, Vedant Sharma, Vaishnavi N., Ayon Borthakur</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14386">https://arxiv.org/abs/2510.14386</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14386">https://arxiv.org/pdf/2510.14386</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14386]] SHaRe-SSM: An Oscillatory Spiking Neural Network for Target Variable Modeling in Long Sequences(https://arxiv.org/abs/2510.14386)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>In recent years, with the emergence of large models, there has been a significant interest in spiking neural networks (SNNs) primarily due to their energy efficiency, multiplication-free, and sparse event-based deep learning. Similarly, state space models (SSMs) in varying designs have evolved as a powerful alternative to transformers for target modeling in long sequences, thereby overcoming the quadratic dependence on sequence length of a transformer. Inspired by this progress, we here design SHaRe-SSM (Spiking Harmonic Resonate and Fire State Space Model), for target variable modeling (including both classification and regression) for very-long-range sequences. Our second-order spiking SSM, on average, performs better than transformers or first-order SSMs while circumventing multiplication operations, making it ideal for resource-constrained applications. The proposed block consumes $73 \times$ less energy than second-order ANN-based SSMs for an 18k sequence, while retaining performance. To ensure learnability over the long-range sequences, we propose exploiting the stable and efficient implementation of the dynamical system using parallel scans. Moreover, for the first time, we propose a kernel-based spiking regressor using resonate and fire neurons for very long-range sequences. Our network shows superior performance on even a 50k sequence while being significantly energy-efficient. In addition, we conducted a systematic analysis of the impact of heterogeneity, dissipation, and conservation in resonate-and-fire SSMs.</li>
</ul>

<h3>Title: BoardVision: Deployment-ready and Robust Motherboard Defect Detection with YOLO+Faster-RCNN Ensemble</h3>
<ul>
<li><strong>Authors: </strong>Brandon Hill, Kma Solaiman</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14389">https://arxiv.org/abs/2510.14389</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14389">https://arxiv.org/pdf/2510.14389</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14389]] BoardVision: Deployment-ready and Robust Motherboard Defect Detection with YOLO+Faster-RCNN Ensemble(https://arxiv.org/abs/2510.14389)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Motherboard defect detection is critical for ensuring reliability in high-volume electronics manufacturing. While prior research in PCB inspection has largely targeted bare-board or trace-level defects, assembly-level inspection of full motherboards inspection remains underexplored. In this work, we present BoardVision, a reproducible framework for detecting assembly-level defects such as missing screws, loose fan wiring, and surface scratches. We benchmark two representative detectors - YOLOv7 and Faster R-CNN, under controlled conditions on the MiracleFactory motherboard dataset, providing the first systematic comparison in this domain. To mitigate the limitations of single models, where YOLO excels in precision but underperforms in recall and Faster R-CNN shows the reverse, we propose a lightweight ensemble, Confidence-Temporal Voting (CTV Voter), that balances precision and recall through interpretable rules. We further evaluate robustness under realistic perturbations including sharpness, brightness, and orientation changes, highlighting stability challenges often overlooked in motherboard defect detection. Finally, we release a deployable GUI-driven inspection tool that bridges research evaluation with operator usability. Together, these contributions demonstrate how computer vision techniques can transition from benchmark results to practical quality assurance for assembly-level motherboard manufacturing.</li>
</ul>

<h3>Title: Suicidal Comment Tree Dataset: Enhancing Risk Assessment and Prediction Through Contextual Analysis</h3>
<ul>
<li><strong>Authors: </strong>Jun Li, Qun Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14395">https://arxiv.org/abs/2510.14395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14395">https://arxiv.org/pdf/2510.14395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14395]] Suicidal Comment Tree Dataset: Enhancing Risk Assessment and Prediction Through Contextual Analysis(https://arxiv.org/abs/2510.14395)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Suicide remains a critical global public health issue. While previous studies have provided valuable insights into detecting suicidal expressions in individual social media posts, limited attention has been paid to the analysis of longitudinal, sequential comment trees for predicting a user's evolving suicidal risk. Users, however, often reveal their intentions through historical posts and interactive comments over time. This study addresses this gap by investigating how the information in comment trees affects both the discrimination and prediction of users' suicidal risk levels. We constructed a high-quality annotated dataset, sourced from Reddit, which incorporates users' posting history and comments, using a refined four-label annotation framework based on the Columbia Suicide Severity Rating Scale (C-SSRS). Statistical analysis of the dataset, along with experimental results from Large Language Models (LLMs) experiments, demonstrates that incorporating comment trees data significantly enhances the discrimination and prediction of user suicidal risk levels. This research offers a novel insight to enhancing the detection accuracy of at-risk individuals, thereby providing a valuable foundation for early suicide intervention strategies.</li>
</ul>

<h3>Title: Your Next Token Prediction: A Multilingual Benchmark for Personalized Response Generation</h3>
<ul>
<li><strong>Authors: </strong>Shiyao Ding, Takayuki Ito</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14398">https://arxiv.org/abs/2510.14398</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14398">https://arxiv.org/pdf/2510.14398</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14398]] Your Next Token Prediction: A Multilingual Benchmark for Personalized Response Generation(https://arxiv.org/abs/2510.14398)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) excel at general next-token prediction but still struggle to generate responses that reflect how individuals truly communicate, such as replying to emails or social messages in their own style. However, real SNS or email histories are difficult to collect due to privacy concerns. To address this, we propose the task of "Your Next Token Prediction (YNTP)", which models a user's precise word choices through controlled human-agent conversations. We build a multilingual benchmark of 100 dialogue sessions across English, Japanese, and Chinese, where users interact for five days with psychologically grounded NPCs based on MBTI dimensions. This setup captures natural, daily-life communication patterns and enables analysis of users' internal models. We evaluate prompt-based and fine-tuning-based personalization methods, establishing the first benchmark for YNTP and a foundation for user-aligned language modeling. The dataset is available at: this https URL</li>
</ul>

<h3>Title: MedTrust-RAG: Evidence Verification and Trust Alignment for Biomedical Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Yingpeng Ning, Yuanyuan Sun, Ling Luo, Yanhua Wang, Yuchen Pan, Hongfei Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14400">https://arxiv.org/abs/2510.14400</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14400">https://arxiv.org/pdf/2510.14400</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14400]] MedTrust-RAG: Evidence Verification and Trust Alignment for Biomedical Question Answering(https://arxiv.org/abs/2510.14400)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Biomedical question answering (QA) requires accurate interpretation of complex medical knowledge. Large language models (LLMs) have shown promising capabilities in this domain, with retrieval-augmented generation (RAG) systems enhancing performance by incorporating external medical literature. However, RAG-based approaches in biomedical QA suffer from hallucinations due to post-retrieval noise and insufficient verification of retrieved evidence, undermining response reliability. We propose MedTrust-Guided Iterative RAG, a framework designed to enhance factual consistency and mitigate hallucinations in medical QA. Our method introduces three key innovations. First, it enforces citation-aware reasoning by requiring all generated content to be explicitly grounded in retrieved medical documents, with structured Negative Knowledge Assertions used when evidence is insufficient. Second, it employs an iterative retrieval-verification process, where a verification agent assesses evidence adequacy and refines queries through Medical Gap Analysis until reliable information is obtained. Third, it integrates the MedTrust-Align Module (MTAM) that combines verified positive examples with hallucination-aware negative samples, leveraging Direct Preference Optimization to reinforce citation-grounded reasoning while penalizing hallucination-prone response patterns. Experiments on MedMCQA, MedQA, and MMLU-Med demonstrate that our approach consistently outperforms competitive baselines across multiple model architectures, achieving the best average accuracy with gains of 2.7% for LLaMA3.1-8B-Instruct and 2.4% for Qwen3-8B.</li>
</ul>

<h3>Title: DCMIL: A Progressive Representation Learning Model of Whole Slide Images for Cancer Prognosis Analysis</h3>
<ul>
<li><strong>Authors: </strong>Chao Tu, Kun Huang, Jie Zhang, Qianjin Feng, Yu Zhang, Zhenyuan Ning</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14403">https://arxiv.org/abs/2510.14403</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14403">https://arxiv.org/pdf/2510.14403</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14403]] DCMIL: A Progressive Representation Learning Model of Whole Slide Images for Cancer Prognosis Analysis(https://arxiv.org/abs/2510.14403)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The burgeoning discipline of computational pathology shows promise in harnessing whole slide images (WSIs) to quantify morphological heterogeneity and develop objective prognostic modes for human cancers. However, progress is impeded by the computational bottleneck of gigapixel-size inputs and the scarcity of dense manual annotations. Current methods often overlook fine-grained information across multi-magnification WSIs and variations in tumor microenvironments. Here, we propose an easy-to-hard progressive representation learning model, termed dual-curriculum contrastive multi-instance learning (DCMIL), to efficiently process WSIs for cancer prognosis. The model does not rely on dense annotations and enables the direct transformation of gigapixel-size WSIs into outcome predictions. Extensive experiments on twelve cancer types (5,954 patients, 12.54 million tiles) demonstrate that DCMIL outperforms standard WSI-based prognostic models. Additionally, DCMIL identifies fine-grained prognosis-salient regions, provides robust instance uncertainty estimation, and captures morphological differences between normal and tumor tissues, with the potential to generate new biological insights. All codes have been made publicly accessible at this https URL.</li>
</ul>

<h3>Title: A Free Lunch in LLM Compression: Revisiting Retraining after Pruning</h3>
<ul>
<li><strong>Authors: </strong>Moritz Wagner, Christophe Roux, Max Zimmer, Sebastian Pokutta</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14444">https://arxiv.org/abs/2510.14444</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14444">https://arxiv.org/pdf/2510.14444</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14444]] A Free Lunch in LLM Compression: Revisiting Retraining after Pruning(https://arxiv.org/abs/2510.14444)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>While Neural Network pruning typically requires retraining the model to recover pruning-induced performance degradation, state-of-the-art Large Language Models (LLMs) pruning methods instead solve a layer-wise mask selection and reconstruction problem on a small set of calibration data to avoid full retraining, as it is considered computationally infeasible for LLMs. Reconstructing single matrices in isolation has favorable properties, such as convexity of the objective and significantly reduced memory requirements compared to full retraining. In practice, however, reconstruction is often implemented at coarser granularities, e.g., reconstructing a whole transformer block against its dense activations instead of a single matrix. In this work, we study the key design choices when reconstructing or retraining the remaining weights after pruning. We conduct an extensive computational study on state-of-the-art GPT architectures, and report several surprising findings that challenge common intuitions about retraining after pruning. In particular, we observe a free lunch scenario: reconstructing attention and MLP components separately within each transformer block is nearly the most resource-efficient yet achieves the best perplexity. Most importantly, this Pareto-optimal setup achieves better performance than full retraining, despite requiring only a fraction of the memory. Furthermore, we demonstrate that simple and efficient pruning criteria such as Wanda can outperform much more complex approaches when the reconstruction step is properly executed, highlighting its importance. Our findings challenge the narrative that retraining should be avoided at all costs and provide important insights into post-pruning performance recovery for LLMs.</li>
</ul>

<h3>Title: Towards geological inference with process-based and deep generative modeling, part 1: training on fluvial deposits</h3>
<ul>
<li><strong>Authors: </strong>Guillaume Rongier, Luk Peeters</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.geo-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14445">https://arxiv.org/abs/2510.14445</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14445">https://arxiv.org/pdf/2510.14445</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14445]] Towards geological inference with process-based and deep generative modeling, part 1: training on fluvial deposits(https://arxiv.org/abs/2510.14445)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative</a></li>
<li><strong>Abstract: </strong>The distribution of resources in the subsurface is deeply linked to the variations of its physical properties. Generative modeling has long been used to predict those physical properties while quantifying the associated uncertainty. But current approaches struggle to properly reproduce geological structures, and fluvial deposits in particular, because of their continuity. This study explores whether a generative adversarial network (GAN) - a type of deep-learning algorithm for generative modeling - can be trained to reproduce fluvial deposits simulated by a process-based model - a more expensive model that mimics geological processes. An ablation study shows that developments from the deep-learning community to generate large 2D images are directly transferable to 3D images of fluvial deposits. Training remains stable, and the generated samples reproduce the non-stationarity and details of the deposits without mode collapse or pure memorization of the training data. Using a process-based model to generate those training data allows us to include valuable properties other than the usual physical properties. We show how the deposition time let us monitor and validate the performance of a GAN by checking that its samples honor the law of superposition. Our work joins a series of previous studies suggesting that GANs are more robust that given credit for, at least for training datasets targeting specific geological structures. Whether this robustness transfers to larger 3D images and multimodal datasets remains to be seen. Exploring how deep generative models can leverage geological principles like the law of superposition shows a lot of promise.</li>
</ul>

<h3>Title: Feature Selection and Regularization in Multi-Class Classification: An Empirical Study of One-vs-Rest Logistic Regression with Gradient Descent Optimization and L1 Sparsity Constraints</h3>
<ul>
<li><strong>Authors: </strong>Jahidul Arafat, Fariha Tasmin, Md Kaosar Uddin, Sanjaya Poudel, Eftakhar Ahmed Arnob</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14449">https://arxiv.org/abs/2510.14449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14449">https://arxiv.org/pdf/2510.14449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14449]] Feature Selection and Regularization in Multi-Class Classification: An Empirical Study of One-vs-Rest Logistic Regression with Gradient Descent Optimization and L1 Sparsity Constraints(https://arxiv.org/abs/2510.14449)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Multi-class wine classification presents fundamental trade-offs between model accuracy, feature dimensionality, and interpretability - critical factors for production deployment in analytical chemistry. This paper presents a comprehensive empirical study of One-vs-Rest logistic regression on the UCI Wine dataset (178 samples, 3 cultivars, 13 chemical features), comparing from-scratch gradient descent implementation against scikit-learn's optimized solvers and quantifying L1 regularization effects on feature sparsity. Manual gradient descent achieves 92.59 percent mean test accuracy with smooth convergence, validating theoretical foundations, though scikit-learn provides 24x training speedup and 98.15 percent accuracy. Class-specific analysis reveals distinct chemical signatures with heterogeneous patterns where color intensity varies dramatically (0.31 to 16.50) across cultivars. L1 regularization produces 54-69 percent feature reduction with only 4.63 percent accuracy decrease, demonstrating favorable interpretability-performance trade-offs. We propose an optimal 5-feature subset achieving 62 percent complexity reduction with estimated 92-94 percent accuracy, enabling cost-effective deployment with 80 dollars savings per sample and 56 percent time reduction. Statistical validation confirms robust generalization with sub-2ms prediction latency suitable for real-time quality control. Our findings provide actionable guidelines for practitioners balancing comprehensive chemical analysis against targeted feature measurement in resource-constrained environments.</li>
</ul>

<h3>Title: Natural Language Tools: A Natural Language Approach to Tool Calling In Large Language Agents</h3>
<ul>
<li><strong>Authors: </strong>Reid T. Johnson, Michelle D. Pain, Jordan D. West</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14453">https://arxiv.org/abs/2510.14453</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14453">https://arxiv.org/pdf/2510.14453</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14453]] Natural Language Tools: A Natural Language Approach to Tool Calling In Large Language Agents(https://arxiv.org/abs/2510.14453)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We present Natural Language Tools (NLT), a framework that replaces programmatic JSON tool calling in large language models (LLMs) with natural language outputs. By decoupling tool selection from response generation, NLT eliminates task interference and format constraints that degrade tool call performance. When evaluated across 10 models and 6,400 trials spanning customer service and mental health domains, NLT improves tool calling accuracy by 18.4 percentage points while reducing output variance by 70%. Open-weight models see the largest gains, surpassing flagship closed-weight alternatives, with implications for model training in both reinforcement learning and supervised fine-tuning stages. These improvements persist under prompt perturbations and extend tool-calling capabilities to models lacking native support.</li>
</ul>

<h3>Title: Coder as Editor: Code-driven Interpretable Molecular Optimization</h3>
<ul>
<li><strong>Authors: </strong>Wenyu Zhu, Chengzhu Li, Xiaohe Tian, Yifan Wang, Yinjun Jia, Jianhui Wang, Bowen Gao, Ya-Qin Zhang, Wei-Ying Ma, Yanyan Lan</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14455">https://arxiv.org/abs/2510.14455</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14455">https://arxiv.org/pdf/2510.14455</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14455]] Coder as Editor: Code-driven Interpretable Molecular Optimization(https://arxiv.org/abs/2510.14455)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Molecular optimization is a central task in drug discovery that requires precise structural reasoning and domain knowledge. While large language models (LLMs) have shown promise in generating high-level editing intentions in natural language, they often struggle to faithfully execute these modifications-particularly when operating on non-intuitive representations like SMILES. We introduce MECo, a framework that bridges reasoning and execution by translating editing actions into executable code. MECo reformulates molecular optimization for LLMs as a cascaded framework: generating human-interpretable editing intentions from a molecule and property goal, followed by translating those intentions into executable structural edits via code generation. Our approach achieves over 98% accuracy in reproducing held-out realistic edits derived from chemical reactions and target-specific compound pairs. On downstream optimization benchmarks spanning physicochemical properties and target activities, MECo substantially improves consistency by 38-86 percentage points to 90%+ and achieves higher success rates over SMILES-based baselines while preserving structural similarity. By aligning intention with execution, MECo enables consistent, controllable and interpretable molecular design, laying the foundation for high-fidelity feedback loops and collaborative human-AI workflows in drug discovery.</li>
</ul>

<h3>Title: Structured Universal Adversarial Attacks on Object Detection for Video Sequences</h3>
<ul>
<li><strong>Authors: </strong>Sven Jacob, Weijia Shao, Gjergji Kasneci</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14460">https://arxiv.org/abs/2510.14460</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14460">https://arxiv.org/pdf/2510.14460</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14460]] Structured Universal Adversarial Attacks on Object Detection for Video Sequences(https://arxiv.org/abs/2510.14460)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, steal</a></li>
<li><strong>Abstract: </strong>Video-based object detection plays a vital role in safety-critical applications. While deep learning-based object detectors have achieved impressive performance, they remain vulnerable to adversarial attacks, particularly those involving universal perturbations. In this work, we propose a minimally distorted universal adversarial attack tailored for video object detection, which leverages nuclear norm regularization to promote structured perturbations concentrated in the background. To optimize this formulation efficiently, we employ an adaptive, optimistic exponentiated gradient method that enhances both scalability and convergence. Our results demonstrate that the proposed attack outperforms both low-rank projected gradient descent and Frank-Wolfe based attacks in effectiveness while maintaining high stealthiness. All code and data are publicly available at this https URL.</li>
</ul>

<h3>Title: Unsupervised Deep Generative Models for Anomaly Detection in Neuroimaging: A Systematic Scoping Review</h3>
<ul>
<li><strong>Authors: </strong>Youwan Mahé, Elise Bannier, Stéphanie Leplaideur, Elisa Fromont, Francesca Galassi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14462">https://arxiv.org/abs/2510.14462</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14462">https://arxiv.org/pdf/2510.14462</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14462]] Unsupervised Deep Generative Models for Anomaly Detection in Neuroimaging: A Systematic Scoping Review(https://arxiv.org/abs/2510.14462)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Unsupervised deep generative models are emerging as a promising alternative to supervised methods for detecting and segmenting anomalies in brain imaging. Unlike fully supervised approaches, which require large voxel-level annotated datasets and are limited to well-characterised pathologies, these models can be trained exclusively on healthy data and identify anomalies as deviations from learned normative brain structures. This PRISMA-guided scoping review synthesises recent work on unsupervised deep generative models for anomaly detection in neuroimaging, including autoencoders, variational autoencoders, generative adversarial networks, and denoising diffusion models. A total of 49 studies published between 2018 - 2025 were identified, covering applications to brain MRI and, less frequently, CT across diverse pathologies such as tumours, stroke, multiple sclerosis, and small vessel disease. Reported performance metrics are compared alongside architectural design choices. Across the included studies, generative models achieved encouraging performance for large focal lesions and demonstrated progress in addressing more subtle abnormalities. A key strength of generative models is their ability to produce interpretable pseudo-healthy (also referred to as counterfactual) reconstructions, which is particularly valuable when annotated data are scarce, as in rare or heterogeneous diseases. Looking ahead, these models offer a compelling direction for anomaly detection, enabling semi-supervised learning, supporting the discovery of novel imaging biomarkers, and facilitating within- and cross-disease deviation mapping in unified end-to-end frameworks. To realise clinical impact, future work should prioritise anatomy-aware modelling, development of foundation models, task-appropriate evaluation metrics, and rigorous clinical validation.</li>
</ul>

<h3>Title: LiRA: Linguistic Robust Anchoring for Cross-lingual Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Haolin Li, Haipeng Zhang, Mang Li, Yaohua Wang, Lijie Wen, Yu Zhang, Biqing Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14466">https://arxiv.org/abs/2510.14466</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14466">https://arxiv.org/pdf/2510.14466</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14466]] LiRA: Linguistic Robust Anchoring for Cross-lingual Large Language Models(https://arxiv.org/abs/2510.14466)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) rapidly advance, performance on high-resource languages (e.g., English, Chinese) is nearing saturation, yet remains substantially lower for low-resource languages (e.g., Urdu, Thai) due to limited training data, machine-translation noise, and unstable cross-lingual alignment. We introduce LiRA (Linguistic Robust Anchoring for Large Language Models), a training framework that robustly improves cross-lingual representations under low-resource conditions while jointly strengthening retrieval and reasoning. LiRA comprises two modules: (i) Arca (Anchored Representation Composition Architecture), which anchors low-resource languages to an English semantic space via anchor-based alignment and multi-agent collaborative encoding, preserving geometric stability in a shared embedding space; and (ii) LaSR (Language-coupled Semantic Reasoner), which adds a language-aware lightweight reasoning head with consistency regularization on top of Arca's multilingual representations, unifying the training objective to enhance cross-lingual understanding, retrieval, and reasoning robustness. We further construct and release a multilingual product retrieval dataset covering five Southeast Asian and two South Asian languages. Experiments across low-resource benchmarks (cross-lingual retrieval, semantic similarity, and reasoning) show consistent gains and robustness under few-shot and noise-amplified settings; ablations validate the contribution of both Arca and LaSR. Code will be released on GitHub and the dataset on Hugging Face.</li>
</ul>

<h3>Title: Stealthy Dual-Trigger Backdoors: Attacking Prompt Tuning in LM-Empowered Graph Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyu Xue, Yuni Lai, Chenxi Huang, Yulin Zhu, Gaolei Li, Xiaoge Zhang, Kai Zhou</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14470">https://arxiv.org/abs/2510.14470</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14470">https://arxiv.org/pdf/2510.14470</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14470]] Stealthy Dual-Trigger Backdoors: Attacking Prompt Tuning in LM-Empowered Graph Foundation Models(https://arxiv.org/abs/2510.14470)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, robust, steal</a></li>
<li><strong>Abstract: </strong>The emergence of graph foundation models (GFMs), particularly those incorporating language models (LMs), has revolutionized graph learning and demonstrated remarkable performance on text-attributed graphs (TAGs). However, compared to traditional GNNs, these LM-empowered GFMs introduce unique security vulnerabilities during the unsecured prompt tuning phase that remain understudied in current research. Through empirical investigation, we reveal a significant performance degradation in traditional graph backdoor attacks when operating in attribute-inaccessible constrained TAG systems without explicit trigger node attribute optimization. To address this, we propose a novel dual-trigger backdoor attack framework that operates at both text-level and struct-level, enabling effective attacks without explicit optimization of trigger node text attributes through the strategic utilization of a pre-established text pool. Extensive experimental evaluations demonstrate that our attack maintains superior clean accuracy while achieving outstanding attack success rates, including scenarios with highly concealed single-trigger nodes. Our work highlights critical backdoor risks in web-deployed LM-empowered GFMs and contributes to the development of more robust supervision mechanisms for open-source platforms in the era of foundation models.</li>
</ul>

<h3>Title: Certifying optimal MEV strategies with Lean</h3>
<ul>
<li><strong>Authors: </strong>Massimo Bartoletti, Riccardo Marchesin, Roberto Zunino</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14480">https://arxiv.org/abs/2510.14480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14480">https://arxiv.org/pdf/2510.14480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14480]] Certifying optimal MEV strategies with Lean(https://arxiv.org/abs/2510.14480)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Maximal Extractable Value (MEV) refers to a class of attacks to decentralized applications where the adversary profits by manipulating the ordering, inclusion, or exclusion of transactions in a blockchain. Decentralized Finance (DeFi) protocols are a primary target of these attacks, as their logic depends critically on transaction sequencing. To date, MEV attacks have already extracted billions of dollars in value, underscoring their systemic impact on blockchain security. Verifying the absence of MEV attacks requires determining suitable upper bounds, i.e. proving that no adversarial strategy can extract more value (if any) than expected by protocol designers. This problem is notoriously difficult: the space of adversarial strategies is extremely vast, making empirical studies and pen-and-paper reasoning insufficiently rigorous. In this paper, we present the first mechanized formalization of MEV in the Lean theorem prover. We introduce a methodology to construct machine-checked proofs of MEV bounds, providing correctness guarantees beyond what is possible with existing techniques. To demonstrate the generality of our approach, we model and analyse the MEV of two paradigmatic DeFi protocols. Notably, we develop the first machine-checked proof of the optimality of sandwich attacks in Automated Market Makers, a fundamental DeFi primitive.</li>
</ul>

<h3>Title: Learning to Undo: Rollback-Augmented Reinforcement Learning with Reversibility Signals</h3>
<ul>
<li><strong>Authors: </strong>Andrejs Sorstkins, Omer Tariq, Muhammad Bilal</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14503">https://arxiv.org/abs/2510.14503</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14503">https://arxiv.org/pdf/2510.14503</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14503]] Learning to Undo: Rollback-Augmented Reinforcement Learning with Reversibility Signals(https://arxiv.org/abs/2510.14503)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This paper proposes a reversible learning framework to improve the robustness and efficiency of value based Reinforcement Learning agents, addressing vulnerability to value overestimation and instability in partially irreversible environments. The framework has two complementary core mechanisms: an empirically derived transition reversibility measure called Phi of s and a, and a selective state rollback operation. We introduce an online per state action estimator called Phi that quantifies the likelihood of returning to a prior state within a fixed horizon K. This measure is used to adjust the penalty term during temporal difference updates dynamically, integrating reversibility awareness directly into the value function. The system also includes a selective rollback operator. When an action yields an expected return markedly lower than its instantaneous estimated value and violates a predefined threshold, the agent is penalized and returns to the preceding state rather than progressing. This interrupts sub optimal high risk trajectories and avoids catastrophic steps. By combining reversibility aware evaluation with targeted rollback, the method improves safety, performance, and stability. In the CliffWalking v0 domain, the framework reduced catastrophic falls by over 99.8 percent and yielded a 55 percent increase in mean episode return. In the Taxi v3 domain, it suppressed illegal actions by greater than or equal to 99.9 percent and achieved a 65.7 percent improvement in cumulative reward, while also sharply reducing reward variance in both environments. Ablation studies confirm that the rollback mechanism is the critical component underlying these safety and performance gains, marking a robust step toward safe and reliable sequential decision making.</li>
</ul>

<h3>Title: Vision Mamba for Permeability Prediction of Porous Media</h3>
<ul>
<li><strong>Authors: </strong>Ali Kashefi, Tapan Mukerji</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14516">https://arxiv.org/abs/2510.14516</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14516">https://arxiv.org/pdf/2510.14516</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14516]] Vision Mamba for Permeability Prediction of Porous Media(https://arxiv.org/abs/2510.14516)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Vision Mamba has recently received attention as an alternative to Vision Transformers (ViTs) for image classification. The network size of Vision Mamba scales linearly with input image resolution, whereas ViTs scale quadratically, a feature that improves computational and memory efficiency. Moreover, Vision Mamba requires a significantly smaller number of trainable parameters than traditional convolutional neural networks (CNNs), and thus, they can be more memory efficient. Because of these features, we introduce, for the first time, a neural network that uses Vision Mamba as its backbone for predicting the permeability of three-dimensional porous media. We compare the performance of Vision Mamba with ViT and CNN models across multiple aspects of permeability prediction and perform an ablation study to assess the effects of its components on accuracy. We demonstrate in practice the aforementioned advantages of Vision Mamba over ViTs and CNNs in the permeability prediction of three-dimensional porous media. We make the source code publicly available to facilitate reproducibility and to enable other researchers to build on and extend this work. We believe the proposed framework has the potential to be integrated into large vision models in which Vision Mamba is used instead of ViTs.</li>
</ul>

<h3>Title: Lexo: Eliminating Stealthy Supply-Chain Attacks via LLM-Assisted Program Regeneration</h3>
<ul>
<li><strong>Authors: </strong>Evangelos Lamprou, Julian Dai, Grigoris Ntousakis, Martin C. Rinard, Nikos Vasilakis</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14522">https://arxiv.org/abs/2510.14522</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14522">https://arxiv.org/pdf/2510.14522</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14522]] Lexo: Eliminating Stealthy Supply-Chain Attacks via LLM-Assisted Program Regeneration(https://arxiv.org/abs/2510.14522)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, steal, large language model</a></li>
<li><strong>Abstract: </strong>Software supply-chain attacks are an important and ongoing concern in the open source software ecosystem. These attacks maintain the standard functionality that a component implements, but additionally hide malicious functionality activated only when the component reaches its target environment. Lexo addresses such stealthy attacks by automatically learning and regenerating vulnerability-free versions of potentially malicious components. Lexo first generates a set of input-output pairs to model a component's full observable behavior, which it then uses to synthesize a new version of the original component. The new component implements the original functionality but avoids stealthy malicious behavior. Throughout this regeneration process, Lexo consults several distinct instances of Large Language Models (LLMs), uses correctness and coverage metrics to shepherd these instances, and guardrails their results. Our evaluation on 100+ real-world packages, including high profile stealthy supply-chain attacks, indicates that Lexo scales across multiple domains, regenerates code efficiently (<100s on average), maintains compatibility, and succeeds in eliminating malicious code in several real-world supply-chain-attacks, even in cases when a state-of-the-art LLM fails to eliminate malicious code when prompted to do so.</li>
</ul>

<h3>Title: On the Identifiability of Tensor Ranks via Prior Predictive Matching</h3>
<ul>
<li><strong>Authors: </strong>Eliezer da Silva, Arto Klami, Diego Mesquita, Iñigo Urteaga</a></li>
<li><strong>Subjects: </strong>cs.LG, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14523">https://arxiv.org/abs/2510.14523</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14523">https://arxiv.org/pdf/2510.14523</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14523]] On the Identifiability of Tensor Ranks via Prior Predictive Matching(https://arxiv.org/abs/2510.14523)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Selecting the latent dimensions (ranks) in tensor factorization is a central challenge that often relies on heuristic methods. This paper introduces a rigorous approach to determine rank identifiability in probabilistic tensor models, based on prior predictive moment matching. We transform a set of moment matching conditions into a log-linear system of equations in terms of marginal moments, prior hyperparameters, and ranks; establishing an equivalence between rank identifiability and the solvability of such system. We apply this framework to four foundational tensor-models, demonstrating that the linear structure of the PARAFAC/CP model, the chain structure of the Tensor Train model, and the closed-loop structure of the Tensor Ring model yield solvable systems, making their ranks identifiable. In contrast, we prove that the symmetric topology of the Tucker model leads to an underdetermined system, rendering the ranks unidentifiable by this method. For the identifiable models, we derive explicit closed-form rank estimators based on the moments of observed data only. We empirically validate these estimators and evaluate the robustness of the proposal.</li>
</ul>

<h3>Title: Noise Projection: Closing the Prompt-Agnostic Gap Behind Text-to-Image Misalignment in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Yunze Tong, Didi Zhu, Zijing Hu, Jinluan Yang, Ziyu Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14526">https://arxiv.org/abs/2510.14526</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14526">https://arxiv.org/pdf/2510.14526</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14526]] Noise Projection: Closing the Prompt-Agnostic Gap Behind Text-to-Image Misalignment in Diffusion Models(https://arxiv.org/abs/2510.14526)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In text-to-image generation, different initial noises induce distinct denoising paths with a pretrained Stable Diffusion (SD) model. While this pattern could output diverse images, some of them may fail to align well with the prompt. Existing methods alleviate this issue either by altering the denoising dynamics or by drawing multiple noises and conducting post-selection. In this paper, we attribute the misalignment to a training-inference mismatch: during training, prompt-conditioned noises lie in a prompt-specific subset of the latent space, whereas at inference the noise is drawn from a prompt-agnostic Gaussian prior. To close this gap, we propose a noise projector that applies text-conditioned refinement to the initial noise before denoising. Conditioned on the prompt embedding, it maps the noise to a prompt-aware counterpart that better matches the distribution observed during SD training, without modifying the SD model. Our framework consists of these steps: we first sample some noises and obtain token-level feedback for their corresponding images from a vision-language model (VLM), then distill these signals into a reward model, and finally optimize the noise projector via a quasi-direct preference optimization. Our design has two benefits: (i) it requires no reference images or handcrafted priors, and (ii) it incurs small inference cost, replacing multi-sample selection with a single forward pass. Extensive experiments further show that our prompt-aware noise projection improves text-image alignment across diverse prompts.</li>
</ul>

<h3>Title: Towards Generalist Intelligence in Dentistry: Vision Foundation Models for Oral and Maxillofacial Radiology</h3>
<ul>
<li><strong>Authors: </strong>Xinrui Huang, Fan Xiao, Dongming He, Anqi Gao, Dandan Li, Xiaofan Zhang, Shaoting Zhang, Xudong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14532">https://arxiv.org/abs/2510.14532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14532">https://arxiv.org/pdf/2510.14532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14532]] Towards Generalist Intelligence in Dentistry: Vision Foundation Models for Oral and Maxillofacial Radiology(https://arxiv.org/abs/2510.14532)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Oral and maxillofacial radiology plays a vital role in dental healthcare, but radiographic image interpretation is limited by a shortage of trained professionals. While AI approaches have shown promise, existing dental AI systems are restricted by their single-modality focus, task-specific design, and reliance on costly labeled data, hindering their generalization across diverse clinical scenarios. To address these challenges, we introduce DentVFM, the first family of vision foundation models (VFMs) designed for dentistry. DentVFM generates task-agnostic visual representations for a wide range of dental applications and uses self-supervised learning on DentVista, a large curated dental imaging dataset with approximately 1.6 million multi-modal radiographic images from various medical centers. DentVFM includes 2D and 3D variants based on the Vision Transformer (ViT) architecture. To address gaps in dental intelligence assessment and benchmarks, we introduce DentBench, a comprehensive benchmark covering eight dental subspecialties, more diseases, imaging modalities, and a wide geographical distribution. DentVFM shows impressive generalist intelligence, demonstrating robust generalization to diverse dental tasks, such as disease diagnosis, treatment analysis, biomarker identification, and anatomical landmark detection and segmentation. Experimental results indicate DentVFM significantly outperforms supervised, self-supervised, and weakly supervised baselines, offering superior generalization, label efficiency, and scalability. Additionally, DentVFM enables cross-modality diagnostics, providing more reliable results than experienced dentists in situations where conventional imaging is unavailable. DentVFM sets a new paradigm for dental AI, offering a scalable, adaptable, and label-efficient model to improve intelligent dental healthcare and address critical gaps in global oral healthcare.</li>
</ul>

<h3>Title: Acquisition of interpretable domain information during brain MR image harmonization for content-based image retrieval</h3>
<ul>
<li><strong>Authors: </strong>Keima Abe, Hayato Muraki, Shuhei Tomoshige, Kenichi Oishi, Hitoshi Iyatomi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14535">https://arxiv.org/abs/2510.14535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14535">https://arxiv.org/pdf/2510.14535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14535]] Acquisition of interpretable domain information during brain MR image harmonization for content-based image retrieval(https://arxiv.org/abs/2510.14535)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>Medical images like MR scans often show domain shifts across imaging sites due to scanner and protocol differences, which degrade machine learning performance in tasks such as disease classification. Domain harmonization is thus a critical research focus. Recent approaches encode brain images $\boldsymbol{x}$ into a low-dimensional latent space $\boldsymbol{z}$, then disentangle it into $\boldsymbol{z_u}$ (domain-invariant) and $\boldsymbol{z_d}$ (domain-specific), achieving strong results. However, these methods often lack interpretability$-$an essential requirement in medical applications$-$leaving practical issues unresolved. We propose Pseudo-Linear-Style Encoder Adversarial Domain Adaptation (PL-SE-ADA), a general framework for domain harmonization and interpretable representation learning that preserves disease-relevant information in brain MR images. PL-SE-ADA includes two encoders $f_E$ and $f_{SE}$ to extract $\boldsymbol{z_u}$ and $\boldsymbol{z_d}$, a decoder to reconstruct the image $f_D$, and a domain predictor $g_D$. Beyond adversarial training between the encoder and domain predictor, the model learns to reconstruct the input image $\boldsymbol{x}$ by summing reconstructions from $\boldsymbol{z_u}$ and $\boldsymbol{z_d}$, ensuring both harmonization and informativeness. Compared to prior methods, PL-SE-ADA achieves equal or better performance in image reconstruction, disease classification, and domain recognition. It also enables visualization of both domain-independent brain features and domain-specific components, offering high interpretability across the entire framework.</li>
</ul>

<h3>Title: Exploring Image Representation with Decoupled Classical Visual Descriptors</h3>
<ul>
<li><strong>Authors: </strong>Chenyuan Qu, Hao Chen, Jianbo Jiao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14536">https://arxiv.org/abs/2510.14536</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14536">https://arxiv.org/pdf/2510.14536</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14536]] Exploring Image Representation with Decoupled Classical Visual Descriptors(https://arxiv.org/abs/2510.14536)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, segmentation</a></li>
<li><strong>Abstract: </strong>Exploring and understanding efficient image representations is a long-standing challenge in computer vision. While deep learning has achieved remarkable progress across image understanding tasks, its internal representations are often opaque, making it difficult to interpret how visual information is processed. In contrast, classical visual descriptors (e.g. edge, colour, and intensity distribution) have long been fundamental to image analysis and remain intuitively understandable to humans. Motivated by this gap, we ask a central question: Can modern learning benefit from these classical cues? In this paper, we answer it with VisualSplit, a framework that explicitly decomposes images into decoupled classical descriptors, treating each as an independent but complementary component of visual knowledge. Through a reconstruction-driven pre-training scheme, VisualSplit learns to capture the essence of each visual descriptor while preserving their interpretability. By explicitly decomposing visual attributes, our method inherently facilitates effective attribute control in various advanced visual tasks, including image generation and editing, extending beyond conventional classification and segmentation, suggesting the effectiveness of this new learning approach for visual understanding. Project page: this https URL.</li>
</ul>

<h3>Title: Exploring Cross-Modal Flows for Few-Shot Learning</h3>
<ul>
<li><strong>Authors: </strong>Ziqi Jiang, Yanghao Wang, Long Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14543">https://arxiv.org/abs/2510.14543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14543">https://arxiv.org/pdf/2510.14543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14543]] Exploring Cross-Modal Flows for Few-Shot Learning(https://arxiv.org/abs/2510.14543)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Aligning features from different modalities, is one of the most fundamental challenges for cross-modal tasks. Although pre-trained vision-language models can achieve a general alignment between image and text, they often require parameter-efficient fine-tuning (PEFT) for further adjustment. Today's PEFT methods (e.g., prompt tuning, LoRA-based, or adapter-based) always selectively fine-tune a subset of parameters, which can slightly adjust either visual or textual features, and avoid overfitting. In this paper, we are the first to highlight that all existing PEFT methods perform one-step adjustment. It is insufficient for complex (or difficult) datasets, where features of different modalities are highly entangled. To this end, we propose the first model-agnostic multi-step adjustment approach by learning a cross-modal velocity field: Flow Matching Alignment (FMA). Specifically, to ensure the correspondence between categories during training, we first utilize a fixed coupling strategy. Then, we propose a noise augmentation strategy to alleviate the data scarcity issue. Finally, we design an early-stopping solver, which terminates the transformation process earlier, improving both efficiency and accuracy. Compared with one-step PEFT methods, FMA has the multi-step rectification ability to achieve more precise and robust alignment. Extensive results have demonstrated that FMA can consistently yield significant performance gains across various benchmarks and backbones, particularly on challenging datasets.</li>
</ul>

<h3>Title: MX+: Pushing the Limits of Microscaling Formats for Efficient Large Language Model Serving</h3>
<ul>
<li><strong>Authors: </strong>Jungi Lee, Junyong Park, Soohyun Cha, Jaehoon Cho, Jaewoong Sim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14557">https://arxiv.org/abs/2510.14557</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14557">https://arxiv.org/pdf/2510.14557</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14557]] MX+: Pushing the Limits of Microscaling Formats for Efficient Large Language Model Serving(https://arxiv.org/abs/2510.14557)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reduced-precision data formats are crucial for cost-effective serving of large language models (LLMs). While numerous reduced-precision formats have been introduced thus far, they often require intrusive modifications to the software frameworks or are rather unconventional for widespread adoption across hardware vendors. In this paper, we instead focus on recent industry-driven variants of block floating-point (BFP) formats and conduct a comprehensive analysis to push their limits for efficient LLM serving. Our analysis shows that existing ultra low-bit BFP variants struggle to provide reasonable language model performance due to outlier values in blocks. To address the outliers with BFPs, we propose MX+, a cost-effective and non-intrusive extension designed for seamless integration into the microscaling (MX) formats. MX+ builds on the key insight that the outlier does not need to use its exponent field in the element data type, which allows us to repurpose the exponent field as an extended mantissa to increase the precision of the outlier element. Our evaluation shows that MX+ achieves significantly higher model performance compared to the 4-bit MX format (MXFP4) with negligible storage overhead and slowdown, thus offering a compelling alternative to MXFP4 or MXFP6 for efficient LLM inference.</li>
</ul>

<h3>Title: Assessing Socio-Cultural Alignment and Technical Safety of Sovereign LLMs</h3>
<ul>
<li><strong>Authors: </strong>Kyubyung Chae, Gihoon Kim, Gyuseong Lee, Taesup Kim, Jaejin Lee, Heejin Kim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14565">https://arxiv.org/abs/2510.14565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14565">https://arxiv.org/pdf/2510.14565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14565]] Assessing Socio-Cultural Alignment and Technical Safety of Sovereign LLMs(https://arxiv.org/abs/2510.14565)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent trends in LLMs development clearly show growing interest in the use and application of sovereign LLMs. The global debate over sovereign LLMs highlights the need for governments to develop their LLMs, tailored to their unique socio-cultural and historical contexts. However, there remains a shortage of frameworks and datasets to verify two critical questions: (1) how well these models align with users' socio-cultural backgrounds, and (2) whether they maintain safety and technical robustness without exposing users to potential harms and risks. To address this gap, we construct a new dataset and introduce an analytic framework for extracting and evaluating the socio-cultural elements of sovereign LLMs, alongside assessments of their technical robustness. Our experimental results demonstrate that while sovereign LLMs play a meaningful role in supporting low-resource languages, they do not always meet the popular claim that these models serve their target users well. We also show that pursuing this untested claim may lead to underestimating critical quality attributes such as safety. Our study suggests that advancing sovereign LLMs requires a more extensive evaluation that incorporates a broader range of well-grounded and practical criteria.</li>
</ul>

<h3>Title: State-Space Models for Tabular Prior-Data Fitted Networks</h3>
<ul>
<li><strong>Authors: </strong>Felix Koch, Marcel Wever, Fabian Raisch, Benjamin Tischler</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14573">https://arxiv.org/abs/2510.14573</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14573">https://arxiv.org/pdf/2510.14573</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14573]] State-Space Models for Tabular Prior-Data Fitted Networks(https://arxiv.org/abs/2510.14573)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Recent advancements in foundation models for tabular data, such as TabPFN, demonstrated that pretrained Transformer architectures can approximate Bayesian inference with high predictive performance. However, Transformers suffer from quadratic complexity with respect to sequence length, motivating the exploration of more efficient sequence models. In this work, we investigate the potential of using Hydra, a bidirectional linear-time structured state space model (SSM), as an alternative to Transformers in TabPFN. A key challenge lies in SSM's inherent sensitivity to the order of input tokens - an undesirable property for tabular datasets where the row order is semantically meaningless. We investigate to what extent a bidirectional approach can preserve efficiency and enable symmetric context aggregation. Our experiments show that this approach reduces the order-dependence, achieving predictive performance competitive to the original TabPFN model.</li>
</ul>

<h3>Title: STANCE: Motion Coherent Video Generation Via Sparse-to-Dense Anchored Encoding</h3>
<ul>
<li><strong>Authors: </strong>Zhifei Chen, Tianshuo Xu, Leyi Wu, Luozhou Wang, Dongyu Yan, Zihan You, Wenting Luo, Guo Zhang, Yingcong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14588">https://arxiv.org/abs/2510.14588</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14588">https://arxiv.org/pdf/2510.14588</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14588]] STANCE: Motion Coherent Video Generation Via Sparse-to-Dense Anchored Encoding(https://arxiv.org/abs/2510.14588)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Video generation has recently made striking visual progress, but maintaining coherent object motion and interactions remains difficult. We trace two practical bottlenecks: (i) human-provided motion hints (e.g., small 2D maps) often collapse to too few effective tokens after encoding, weakening guidance; and (ii) optimizing for appearance and motion in a single head can favor texture over temporal consistency. We present STANCE, an image-to-video framework that addresses both issues with two simple components. First, we introduce Instance Cues -- a pixel-aligned control signal that turns sparse, user-editable hints into a dense 2.5D (camera-relative) motion field by averaging per-instance flow and augmenting with monocular depth over the instance mask. This reduces depth ambiguity compared to 2D arrow inputs while remaining easy to use. Second, we preserve the salience of these cues in token space with Dense RoPE, which tags a small set of motion tokens (anchored on the first frame) with spatial-addressable rotary embeddings. Paired with joint RGB \(+\) auxiliary-map prediction (segmentation or depth), our model anchors structure while RGB handles appearance, stabilizing optimization and improving temporal coherence without requiring per-frame trajectory scripts.</li>
</ul>

<h3>Title: Symbolic verification of Apple's Find My location-tracking protocol</h3>
<ul>
<li><strong>Authors: </strong>Vaishnavi Sundararajan, Rithwik</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14589">https://arxiv.org/abs/2510.14589</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14589">https://arxiv.org/pdf/2510.14589</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14589]] Symbolic verification of Apple's Find My location-tracking protocol(https://arxiv.org/abs/2510.14589)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, attack</a></li>
<li><strong>Abstract: </strong>Tracking devices, while designed to help users find their belongings in case of loss/theft, bring in new questions about privacy and surveillance of not just their own users, but in the case of crowd-sourced location tracking, even that of others even orthogonally associated with these platforms. Apple's Find My is perhaps the most ubiquitous such system which can even locate devices which do not possess any cellular support or GPS, running on millions of devices worldwide. Apple claims that this system is private and secure, but the code is proprietary, and such claims have to be taken on faith. It is well known that even with perfect cryptographic guarantees, logical flaws might creep into protocols, and allow undesirable attacks. In this paper, we present a symbolic model of the Find My protocol, as well as a precise formal specification of desirable properties, and provide automated, machine-checkable proofs of these properties in the Tamarin prover.</li>
</ul>

<h3>Title: Hierarchical Re-Classification: Combining Animal Classification Models with Vision Transformers</h3>
<ul>
<li><strong>Authors: </strong>Hugo Markoff, Jevgenijs Galaktionovs</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14594">https://arxiv.org/abs/2510.14594</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14594">https://arxiv.org/pdf/2510.14594</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14594]] Hierarchical Re-Classification: Combining Animal Classification Models with Vision Transformers(https://arxiv.org/abs/2510.14594)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>State-of-the-art animal classification models like SpeciesNet provide predictions across thousands of species but use conservative rollup strategies, resulting in many animals labeled at high taxonomic levels rather than species. We present a hierarchical re-classification system for the Animal Detect platform that combines SpeciesNet EfficientNetV2-M predictions with CLIP embeddings and metric learning to refine high-level taxonomic labels toward species-level identification. Our five-stage pipeline (high-confidence acceptance, bird override, centroid building, triplet-loss metric learning, and adaptive cosine-distance scoring) is evaluated on a segment of the LILA BC Desert Lion Conservation dataset (4,018 images, 15,031 detections). After recovering 761 bird detections from "blank" and "animal" labels, we re-classify 456 detections labeled animal, mammal, or blank with 96.5% accuracy, achieving species-level identification for 64.9 percent</li>
</ul>

<h3>Title: Zero-Shot Wildlife Sorting Using Vision Transformers: Evaluating Clustering and Continuous Similarity Ordering</h3>
<ul>
<li><strong>Authors: </strong>Hugo Markoff, Jevgenijs Galaktionovs</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14596">https://arxiv.org/abs/2510.14596</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14596">https://arxiv.org/pdf/2510.14596</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14596]] Zero-Shot Wildlife Sorting Using Vision Transformers: Evaluating Clustering and Continuous Similarity Ordering(https://arxiv.org/abs/2510.14596)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Camera traps generate millions of wildlife images, yet many datasets contain species that are absent from existing classifiers. This work evaluates zero-shot approaches for organizing unlabeled wildlife imagery using self-supervised vision transformers, developed and tested within the Animal Detect platform for camera trap analysis. We compare unsupervised clustering methods (DBSCAN, GMM) across three architectures (CLIP, DINOv2, MegaDescriptor) combined with dimensionality reduction techniques (PCA, UMAP), and we demonstrate continuous 1D similarity ordering via t-SNE projection. On a 5-species test set with ground truth labels used only for evaluation, DINOv2 with UMAP and GMM achieves 88.6 percent accuracy (macro-F1 = 0.874), while 1D sorting reaches 88.2 percent coherence for mammals and birds and 95.2 percent for fish across 1,500 images. Based on these findings, we deployed continuous similarity ordering in production, enabling rapid exploratory analysis and accelerating manual annotation workflows for biodiversity monitoring.</li>
</ul>

<h3>Title: First Attentions Last: Better Exploiting First Attentions for Efficient Transformer Training</h3>
<ul>
<li><strong>Authors: </strong>Gyudong Kim, Hyukju Na, Jin Hyeon Kim, Hyunsung Jang, Jaemin Park, Jaegi Hwang, Namkoo Ha, Seungryong Kim, Young Geun Kim</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14614">https://arxiv.org/abs/2510.14614</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14614">https://arxiv.org/pdf/2510.14614</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14614]] First Attentions Last: Better Exploiting First Attentions for Efficient Transformer Training(https://arxiv.org/abs/2510.14614)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>As training billion-scale transformers becomes increasingly common, employing multiple distributed GPUs along with parallel training methods has become a standard practice. However, existing transformer designs suffer from significant communication overhead, especially in Tensor Parallelism (TP), where each block's MHA-MLP connection requires an all-reduce communication. Through our investigation, we show that the MHA-MLP connections can be bypassed for efficiency, while the attention output of the first layer can serve as an alternative signal for the bypassed connection. Motivated by the observations, we propose FAL (First Attentions Last), an efficient transformer architecture that redirects the first MHA output to the MLP inputs of the following layers, eliminating the per-block MHA-MLP connections. This removes the all-reduce communication and enables parallel execution of MHA and MLP on a single GPU. We also introduce FAL+, which adds the normalized first attention output to the MHA outputs of the following layers to augment the MLP input for the model quality. Our evaluation shows that FAL reduces multi-GPU training time by up to 44%, improves single-GPU throughput by up to 1.18x, and achieves better perplexity compared to the baseline GPT. FAL+ achieves even lower perplexity without increasing the training time than the baseline.</li>
</ul>

<h3>Title: Beyond Correctness: Evaluating Subjective Writing Preferences Across Cultures</h3>
<ul>
<li><strong>Authors: </strong>Shuangshuang Ying, Yunwen Li, Xingwei Qu, Xin Li, Sheng Jin, Minghao Liu, Zhoufutu Wen, Xeron Du, Tianyu Zheng, Yichi Zhang, Letian Ni, Yuyang Cheng, Qiguang Chen, Jingzhe Ding, Shengda Long, Wangchunshu Zhou, Jiazhan Feng, Wanjun Zhong, Libo Qin, Ge Zhang, Wenhao Huang, Wanxiang Che, Chenghua Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14616">https://arxiv.org/abs/2510.14616</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14616">https://arxiv.org/pdf/2510.14616</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14616]] Beyond Correctness: Evaluating Subjective Writing Preferences Across Cultures(https://arxiv.org/abs/2510.14616)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Current preference learning methods achieve high accuracy on standard benchmarks but exhibit significant performance degradation when objective quality signals are removed. We introduce WritingPreferenceBench, a dataset of 1,800 human-annotated preference pairs (1,200 English, 600 Chinese) across 8 creative writing genres, where responses are matched for objective correctness, factual accuracy, and length. On this benchmark, sequence-based reward models--the standard architecture for RLHF--achieve only 52.7% mean accuracy, while zero-shot language model judges perform at 53.9%. In contrast, generative reward models that produce explicit reasoning chains achieve 81.8% accuracy. We observe high within-model variance across genres: individual models range from 18.2% to 81.8% accuracy across different writing categories, with standard deviations averaging 10.1%. This variance persists regardless of model scale, with 27B parameter models showing no consistent improvement over 8B variants. Our results suggest that current RLHF methods primarily learn to detect objective errors rather than capture subjective quality preferences (e.g., creativity, stylistic flair, and emotional resonance), and that successful preference modeling may require intermediate reasoning representations rather than direct classification.</li>
</ul>

<h3>Title: Shot2Tactic-Caption: Multi-Scale Captioning of Badminton Videos for Tactical Understanding</h3>
<ul>
<li><strong>Authors: </strong>Ning Ding, Keisuke Fujii, Toru Tamaki</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14617">https://arxiv.org/abs/2510.14617</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14617">https://arxiv.org/pdf/2510.14617</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14617]] Shot2Tactic-Caption: Multi-Scale Captioning of Badminton Videos for Tactical Understanding(https://arxiv.org/abs/2510.14617)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Tactical understanding in badminton involves interpreting not only individual actions but also how tactics are dynamically executed over time. In this paper, we propose \textbf{Shot2Tactic-Caption}, a novel framework for semantic and temporal multi-scale video captioning in badminton, capable of generating shot-level captions that describe individual actions and tactic-level captions that capture how these actions unfold over time within a tactical execution. We also introduce the Shot2Tactic-Caption Dataset, the first badminton captioning dataset containing 5,494 shot captions and 544 tactic captions. Shot2Tactic-Caption adopts a dual-branch design, with both branches including a visual encoder, a spatio-temporal Transformer encoder, and a Transformer-based decoder to generate shot and tactic captions. To support tactic captioning, we additionally introduce a Tactic Unit Detector that identifies valid tactic units, tactic types, and tactic states (e.g., Interrupt, Resume). For tactic captioning, we further incorporate a shot-wise prompt-guided mechanism, where the predicted tactic type and state are embedded as prompts and injected into the decoder via cross-attention. The shot-wise prompt-guided mechanism enables our system not only to describe successfully executed tactics but also to capture tactical executions that are temporarily interrupted and later resumed. Experimental results demonstrate the effectiveness of our framework in generating both shot and tactic captions. Ablation studies show that the ResNet50-based spatio-temporal encoder outperforms other variants, and that shot-wise prompt structuring leads to more coherent and accurate tactic captioning.</li>
</ul>

<h3>Title: Code-driven Number Sequence Calculation: Enhancing the inductive Reasoning Abilities of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kedi Chen, Zhikai Lei, Xu Guo, Xuecheng Wu, Siyuan Zeng, Jianghao Yin, Yinqi Zhang, Qin Chen, Jie Zhou, Liang He, Qipeng Guo, Kai Chen, Wei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14620">https://arxiv.org/abs/2510.14620</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14620">https://arxiv.org/pdf/2510.14620</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14620]] Code-driven Number Sequence Calculation: Enhancing the inductive Reasoning Abilities of Large Language Models(https://arxiv.org/abs/2510.14620)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) make remarkable progress in reasoning tasks. Among different reasoning modes, inductive reasoning, due to its better alignment with human learning, attracts increasing interest. However, research on inductive reasoning faces certain challenges. First, existing inductive data mostly focuses on superficial regularities while lacking more complex internal patterns. Second, current works merely prompt LLMs or finetune on simple prompt-response pairs, but do not provide precise thinking processes nor implement difficulty control. Unlike previous work, we address these challenges by introducing \textit{CodeSeq}, a synthetic post-training dataset built from number sequences. We package number sequences into algorithmic problems to discover their general terms, defining a general term generation (GTG) task correspondingly. Our pipeline generates supervised finetuning data by reflecting on failed test cases and incorporating iterative corrections, thereby teaching LLMs to learn autonomous case generation and self-checking. Additionally, it leverages reinforcement learning with a novel Case-Synergy Solvability Scaling Reward based on both solvability, estimated from the problem pass rate, and the success rate of self-directed case generation, enabling models to learn more effectively from both successes and failures. Experimental results show that the models trained with \textit{CodeSeq} improve on various reasoning tasks and can preserve the models' OOD performance.</li>
</ul>

<h3>Title: LeapFactual: Reliable Visual Counterfactual Explanation Using Conditional Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Zhuo Cao, Xuan Zhao, Lena Krieger, Hanno Scharr, Ira Assent</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14623">https://arxiv.org/abs/2510.14623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14623">https://arxiv.org/pdf/2510.14623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14623]] LeapFactual: Reliable Visual Counterfactual Explanation Using Conditional Flow Matching(https://arxiv.org/abs/2510.14623)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>The growing integration of machine learning (ML) and artificial intelligence (AI) models into high-stakes domains such as healthcare and scientific research calls for models that are not only accurate but also interpretable. Among the existing explainable methods, counterfactual explanations offer interpretability by identifying minimal changes to inputs that would alter a model's prediction, thus providing deeper insights. However, current counterfactual generation methods suffer from critical limitations, including gradient vanishing, discontinuous latent spaces, and an overreliance on the alignment between learned and true decision boundaries. To overcome these limitations, we propose LeapFactual, a novel counterfactual explanation algorithm based on conditional flow matching. LeapFactual generates reliable and informative counterfactuals, even when true and learned decision boundaries diverge. Following a model-agnostic approach, LeapFactual is not limited to models with differentiable loss functions. It can even handle human-in-the-loop systems, expanding the scope of counterfactual explanations to domains that require the participation of human annotators, such as citizen science. We provide extensive experiments on benchmark and real-world datasets showing that LeapFactual generates accurate and in-distribution counterfactual explanations that offer actionable insights. We observe, for instance, that our reliable counterfactual samples with labels aligning to ground truth can be beneficially used as new training data to enhance the model. The proposed method is broadly applicable and enhances both scientific knowledge discovery and non-expert interpretability.</li>
</ul>

<h3>Title: Efficient Video Sampling: Pruning Temporally Redundant Tokens for Faster VLM Inference</h3>
<ul>
<li><strong>Authors: </strong>Natan Bagrov, Eugene Khvedchenia, Borys Tymchenko, Shay Aharon, Lior Kadoch, Tomer Keren, Ofri Masad, Yonatan Geifman, Ran Zilberstein, Tuomas Rintamaki, Matthieu Le, Andrew Tao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14624">https://arxiv.org/abs/2510.14624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14624">https://arxiv.org/pdf/2510.14624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14624]] Efficient Video Sampling: Pruning Temporally Redundant Tokens for Faster VLM Inference(https://arxiv.org/abs/2510.14624)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Vision-language models (VLMs) have recently expanded from static image understanding to video reasoning, but their scalability is fundamentally limited by the quadratic cost of processing dense frame sequences. Long videos often exceed the token budget of modern language models, leading to severe context limitations and latency issues. We introduce Efficient Video Sampling (EVS), a simple, plug-and-play method for reducing token redundancy in videos by identifying and pruning temporally static patches -- spatial regions that remain unchanged across consecutive frames. EVS preserves positional identity, requires no architectural changes or retraining. We show that EVS substantially reduces token count while maintaining semantic fidelity, enabling faster inference and longer input sequences. Applied at inference time, EVS reduces large language model (LLM) time-to-first-token (TTFT) by up to 4x with minimal accuracy loss. When combined with an uptraining phase using stochastic pruning rates, EVS yields models that are robust to varying compression levels and retain full performance under aggressive pruning. Extensive experiments demonstrate that EVS consistently improves efficiency-accuracy trade-offs, unlocking scalable video-language understanding without sacrificing quality.</li>
</ul>

<h3>Title: RLAIF-SPA: Optimizing LLM-based Emotional Speech Synthesis via RLAIF</h3>
<ul>
<li><strong>Authors: </strong>Qing Yang, Zhenghao Liu, Junxin Wang, Yangfan Du, Pengcheng Huang, Tong Xiao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14628">https://arxiv.org/abs/2510.14628</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14628">https://arxiv.org/pdf/2510.14628</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14628]] RLAIF-SPA: Optimizing LLM-based Emotional Speech Synthesis via RLAIF(https://arxiv.org/abs/2510.14628)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Text-To-Speech synthesis has achieved near-human quality in neutral speech, but emotional expressiveness remains a challenge. Existing methods often rely on costly emotion annotations or optimize indirect objectives that fail to capture the emotional expressiveness and perceptual naturalness of speech, leading to generated speech that is accurate but emotionally flat. To address these challenges, we propose the RLAIF-SPA framework, incorporating a Reinforcement Learning from AI Feedback (RLAIF) mechanism to employ Automatic Speech Recognition (ASR) and Large Language Model (LLM) techniques to respectively judge semantic accuracy and prosodic-emotional label alignment as a direct reward for emotional expressiveness and intelligibility optimization. Specifically, it leverages Prosodic Label Alignment to enhance expressive quality by jointly considering semantic accuracy and prosodic-emotional alignment along four fine-grained dimensions: Structure, Emotion, Speed, and Tone. In addition, it incorporates Semantic Accuracy Feedback to ensure the generation of clear and accurate speech. Experiments on the Libri Speech dataset show that RLAIF-SPA outperforms Chat-TTS, with a 26.1% reduction in WER, a 9.1% increase in SIM-O, and over 10% improvement in human evaluation.</li>
</ul>

<h3>Title: Adapting Self-Supervised Representations as a Latent Space for Efficient Generation</h3>
<ul>
<li><strong>Authors: </strong>Ming Gui, Johannes Schusterbauer, Timy Phan, Felix Krause, Josh Susskind, Miguel Angel Bautista, Björn Ommer</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14630">https://arxiv.org/abs/2510.14630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14630">https://arxiv.org/pdf/2510.14630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14630]] Adapting Self-Supervised Representations as a Latent Space for Efficient Generation(https://arxiv.org/abs/2510.14630)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>We introduce Representation Tokenizer (RepTok), a generative modeling framework that represents an image using a single continuous latent token obtained from self-supervised vision transformers. Building on a pre-trained SSL encoder, we fine-tune only the semantic token embedding and pair it with a generative decoder trained jointly using a standard flow matching objective. This adaptation enriches the token with low-level, reconstruction-relevant details, enabling faithful image reconstruction. To preserve the favorable geometry of the original SSL space, we add a cosine-similarity loss that regularizes the adapted token, ensuring the latent space remains smooth and suitable for generation. Our single-token formulation resolves spatial redundancies of 2D latent spaces and significantly reduces training costs. Despite its simplicity and efficiency, RepTok achieves competitive results on class-conditional ImageNet generation and naturally extends to text-to-image synthesis, reaching competitive zero-shot performance on MS-COCO under extremely limited training budgets. Our findings highlight the potential of fine-tuned SSL representations as compact and effective latent spaces for efficient generative modeling.</li>
</ul>

<h3>Title: SteeringTTA: Guiding Diffusion Trajectories for Robust Test-Time-Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Jihyun Yu, Yoojin Oh, Wonho Bae, Mingyu Kim, Junhyug Noh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14634">https://arxiv.org/abs/2510.14634</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14634">https://arxiv.org/pdf/2510.14634</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14634]] SteeringTTA: Guiding Diffusion Trajectories for Robust Test-Time-Adaptation(https://arxiv.org/abs/2510.14634)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Test-time adaptation (TTA) aims to correct performance degradation of deep models under distribution shifts by updating models or inputs using unlabeled test data. Input-only diffusion-based TTA methods improve robustness for classification to corruptions but rely on gradient guidance, limiting exploration and generalization across distortion types. We propose SteeringTTA, an inference-only framework that adapts Feynman-Kac steering to guide diffusion-based input adaptation for classification with rewards driven by pseudo-label. SteeringTTA maintains multiple particle trajectories, steered by a combination of cumulative top-K probabilities and an entropy schedule, to balance exploration and confidence. On ImageNet-C, SteeringTTA consistently outperforms the baseline without any model updates or source data.</li>
</ul>

<h3>Title: Improving Cybercrime Detection and Digital Forensics Investigations with Artificial Intelligence</h3>
<ul>
<li><strong>Authors: </strong>Silvia Lucia Sanna, Leonardo Regano, Davide Maiorca, Giorgio Giacinto</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14638">https://arxiv.org/abs/2510.14638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14638">https://arxiv.org/pdf/2510.14638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14638]] Improving Cybercrime Detection and Digital Forensics Investigations with Artificial Intelligence(https://arxiv.org/abs/2510.14638)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>According to a recent EUROPOL report, cybercrime is still recurrent in Europe, and different activities and countermeasures must be taken to limit, prevent, detect, analyze, and fight it. Cybercrime must be prevented with specific measures, tools, and techniques, for example through automated network and malware analysis. Countermeasures against cybercrime can also be improved with proper \df analysis in order to extract data from digital devices trying to retrieve information on the cybercriminals. Indeed, results obtained through a proper \df analysis can be leveraged to train cybercrime detection systems to prevent the success of similar crimes. Nowadays, some systems have started to adopt Artificial Intelligence (AI) algorithms for cyberattack detection and \df analysis improvement. However, AI can be better applied as an additional instrument in these systems to improve the detection and in the \df analysis. For this reason, we highlight how cybercrime analysis and \df procedures can take advantage of AI. On the other hand, cybercriminals can use these systems to improve their skills, bypass automatic detection, and develop advanced attack techniques. The case study we presented highlights how it is possible to integrate the use of the three popular chatbots {\tt Gemini}, {\tt Copilot} and {\tt chatGPT} to develop a Python code to encode and decoded images with steganographic technique, even though their presence is not an indicator of crime, attack or maliciousness but used by a cybercriminal as anti-forensics technique.</li>
</ul>

<h3>Title: Decorrelation Speeds Up Vision Transformers</h3>
<ul>
<li><strong>Authors: </strong>Kieran Carrigg, Rob van Gastel, Melda Yeghaian, Sander Dalm, Faysal Boughorbel, Marcel van Gerven</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14657">https://arxiv.org/abs/2510.14657</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14657">https://arxiv.org/pdf/2510.14657</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14657]] Decorrelation Speeds Up Vision Transformers(https://arxiv.org/abs/2510.14657)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Masked Autoencoder (MAE) pre-training of vision transformers (ViTs) yields strong performance in low-label regimes but comes with substantial computational costs, making it impractical in time- and resource-constrained industrial settings. We address this by integrating Decorrelated Backpropagation (DBP) into MAE pre-training, an optimization method that iteratively reduces input correlations at each layer to accelerate convergence. Applied selectively to the encoder, DBP achieves faster pre-training without loss of stability. On ImageNet-1K pre-training with ADE20K fine-tuning, DBP-MAE reduces wall-clock time to baseline performance by 21.1%, lowers carbon emissions by 21.4% and improves segmentation mIoU by 1.1 points. We observe similar gains when pre-training and fine-tuning on proprietary industrial data, confirming the method's applicability in real-world scenarios. These results demonstrate that DBP can reduce training time and energy use while improving downstream performance for large-scale ViT pre-training.</li>
</ul>

<h3>Title: An Efficient Rubric-based Generative Verifier for Search-Augmented LLMs</h3>
<ul>
<li><strong>Authors: </strong>Linyue Ma, Yilong Xu, Xiang Long, Zhi Zheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14660">https://arxiv.org/abs/2510.14660</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14660">https://arxiv.org/pdf/2510.14660</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14660]] An Efficient Rubric-based Generative Verifier for Search-Augmented LLMs(https://arxiv.org/abs/2510.14660)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, generative, large language model</a></li>
<li><strong>Abstract: </strong>Search augmentation empowers Large Language Models with retrieval capabilities to overcome the limitations imposed by static parameters. Recently, Reinforcement Learning leverages tailored reward signals as a viable technique to enhance LLMs performing tasks involving search. However, existing reward modeling for search-augmented LLMs faces several limitations. Rule-based rewards, such as Exact Match, are verifiable but fragile to variations in expression and cannot be applied to long-form workloads. In contrast, generative rewards improve robustness, but designing verifiable and stable rewards for long-form workloads in dynamic corpora remains challenging and also incurs high computational costs. In this paper, we propose a unified and verifiable paradigm, "nugget-as-rubric", which treats atomic information points as structured evaluation criteria for different search-augmentation workloads. Short-form tasks correspond to a single rubric, whereas long-form tasks expand to multiple rubrics aligned with the question's information needs. To support long-form settings, we design an automatic rubric construction pipeline based on query rewriting, which can automatically retrieve passages relevant to each question and extract rubrics from them, both from static corpora and from dynamic online web content. Furthermore, we introduce \textbf{Search-Gen-V}, a 4B-parameter efficient generative verifier under our proposed verifiable paradigm, which is trained via the idea of distillation and a two-stage strategy. Experimental results show that Search-Gen-V achieves strong verification accuracy across different workloads, making it a scalable, robust, and efficient verifiable reward constructor for search-augmented LLMs.</li>
</ul>

<h3>Title: EuroMineNet: A Multitemporal Sentinel-2 Benchmark for Spatiotemporal Mining Footprint Analysis in the European Union (2015-2024)</h3>
<ul>
<li><strong>Authors: </strong>Weikang Yu, Vincent Nwazelibe, Xianping Ma, Xiaokang Zhang, Richard Gloaguen, Xiao Xiang Zhu, Pedram Ghamisi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14661">https://arxiv.org/abs/2510.14661</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14661">https://arxiv.org/pdf/2510.14661</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14661]] EuroMineNet: A Multitemporal Sentinel-2 Benchmark for Spatiotemporal Mining Footprint Analysis in the European Union (2015-2024)(https://arxiv.org/abs/2510.14661)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Mining activities are essential for industrial and economic development, but remain a leading source of environmental degradation, contributing to deforestation, soil erosion, and water contamination. Sustainable resource management and environmental governance require consistent, long-term monitoring of mining-induced land surface changes, yet existing datasets are often limited in temporal depth or geographic scope. To address this gap, we present EuroMineNet, the first comprehensive multitemporal benchmark for mining footprint mapping and monitoring based on Sentinel-2 multispectral imagery. Spanning 133 mining sites across the European Union, EuroMineNet provides annual observations and expert-verified annotations from 2015 to 2024, enabling GeoAI-based models to analyze environmental dynamics at a continental scale. It supports two sustainability-driven tasks: (1) multitemporal mining footprint mapping for consistent annual land-use delineation, evaluated with a novel Change-Aware Temporal IoU (CA-TIoU) metric, and (2) cross-temporal change detection to capture both gradual and abrupt surface transformations. Benchmarking 20 state-of-the-art deep learning models reveals that while GeoAI methods effectively identify long-term environmental changes, challenges remain in detecting short-term dynamics critical for timely mitigation. By advancing temporally consistent and explainable mining monitoring, EuroMineNet contributes to sustainable land-use management, environmental resilience, and the broader goal of applying GeoAI for social and environmental good. We release the codes and datasets by aligning with FAIR and the open science paradigm at this https URL.</li>
</ul>

<h3>Title: VTimeCoT: Thinking by Drawing for Video Temporal Grounding and Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Jinglei Zhang, Yuanfan Guo, Rolandos Alexandros Potamias, Jiankang Deng, Hang Xu, Chao Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14672">https://arxiv.org/abs/2510.14672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14672">https://arxiv.org/pdf/2510.14672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14672]] VTimeCoT: Thinking by Drawing for Video Temporal Grounding and Reasoning(https://arxiv.org/abs/2510.14672)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In recent years, video question answering based on multimodal large language models (MLLM) has garnered considerable attention, due to the benefits from the substantial advancements in LLMs. However, these models have a notable deficiency in the domains of video temporal grounding and reasoning, posing challenges to the development of effective real-world video understanding systems. Inspired by how humans use video players to interact with the progress bar for video comprehension, we introduce VTimeCoT, a simple yet effective training-free framework, designed for high-performance video grounding and reasoning. The proposed framework incorporates two novel visual tools of the progress bar: a plug-and-play progress bar integration tool and a high-efficiency highlighting tool. In addition, to address the limitations of conventional text-based chain-of-thought (CoT) approaches, we introduce a visuotemporal CoT process that integrates cross-modality reasoning across both video and text. Our approach demonstrates significant performance improvements on both Qwen2VL-7B and GPT4o baselines in tasks of video temporal grounding and reasoning-based question answering. Finally, we showcase that the proposed framework achieves a compositional and interpretable reasoning process. Project page: this https URL</li>
</ul>

<h3>Title: AEX-NStep: Probabilistic Interrupt Counting Attacks on Intel SGX</h3>
<ul>
<li><strong>Authors: </strong>Nicolas Dutly, Friederike Groschupp, Ivan Puddu, Kari Kostiainen, Srdjan Capkun</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14675">https://arxiv.org/abs/2510.14675</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14675">https://arxiv.org/pdf/2510.14675</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14675]] AEX-NStep: Probabilistic Interrupt Counting Attacks on Intel SGX(https://arxiv.org/abs/2510.14675)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>To mitigate interrupt-based stepping attacks (notably using SGX-Step), Intel introduced AEX-Notify, an ISA extension to Intel SGX that aims to prevent deterministic single-stepping. In this work, we introduce AEX-NStep, the first interrupt counting attack on AEX-Notify-enabled Enclaves. We show that deterministic single-stepping is not required for interrupt counting attacks to be practical and that, therefore, AEX-Notify does not entirely prevent such attacks. We specifically show that one of AEX-Notify's security guarantees, obfuscated forward progress, does not hold, and we introduce two new probabilistic interrupt counting attacks. We use these attacks to construct a practical ECDSA key leakage attack on an AEX-Notify-enabled SGX enclave. Our results extend the original security analysis of AEX-Notify and inform the design of future mitigations.</li>
</ul>

<h3>Title: FibRace: a large-scale benchmark of client-side proving on mobile devices</h3>
<ul>
<li><strong>Authors: </strong>Simon Malatrait, Alex Sirac</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14693">https://arxiv.org/abs/2510.14693</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14693">https://arxiv.org/pdf/2510.14693</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14693]] FibRace: a large-scale benchmark of client-side proving on mobile devices(https://arxiv.org/abs/2510.14693)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>FibRace, jointly developed by KKRT Labs and Hyli, was the first large-scale experiment to test client-side proof generation on smartphones using Cairo M. Presented as a mobile game in which players proved Fibonacci numbers and climbed a leaderboard, FibRace served a dual purpose: to engage the public and to provide empirical benchmarking. Over a three-week campaign (September 11-30, 2025), 6,047 players across 99 countries generated 2,195,488 proofs on 1,420 unique device models. The results show that most modern smartphones can complete a proof in under 5 seconds, confirming that *mobile devices are now capable of producing zero-knowledge proofs reliably*, without the need for remote provers or specialized hardware. Performance was correlated primarily with RAM capacity and SoC (System on Chip) performance: devices with at least 3 GB of RAM proved stably, when Apple's A19 Pro and M-series chips achieved the fastest proving times. Hyli's blockchain natively verified every proof onchain without congestion. FibRace provides the most comprehensive dataset to date on mobile proving performance, establishing a practical baseline for future research in lightweight provers, proof-powered infrastructure, and privacy-preserving mobile applications.</li>
</ul>

<h3>Title: FedPPA: Progressive Parameter Alignment for Personalized Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Maulidi Adi Prasetia, Muhamad Risqi U. Saputra, Guntur Dharma Putra</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14698">https://arxiv.org/abs/2510.14698</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14698">https://arxiv.org/pdf/2510.14698</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14698]] FedPPA: Progressive Parameter Alignment for Personalized Federated Learning(https://arxiv.org/abs/2510.14698)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) is designed as a decentralized, privacy-preserving machine learning paradigm that enables multiple clients to collaboratively train a model without sharing their data. In real-world scenarios, however, clients often have heterogeneous computational resources and hold non-independent and identically distributed data (non-IID), which poses significant challenges during training. Personalized Federated Learning (PFL) has emerged to address these issues by customizing models for each client based on their unique data distribution. Despite its potential, existing PFL approaches typically overlook the coexistence of model and data heterogeneity arising from clients with diverse computational capabilities. To overcome this limitation, we propose a novel method, called Progressive Parameter Alignment (FedPPA), which progressively aligns the weights of common layers across clients with the global model's weights. Our approach not only mitigates inconsistencies between global and local models during client updates, but also preserves client's local knowledge, thereby enhancing personalization robustness in non-IID settings. To further enhance the global model performance while retaining strong personalization, we also integrate entropy-based weighted averaging into the FedPPA framework. Experiments on three image classification datasets, including MNIST, FMNIST, and CIFAR-10, demonstrate that FedPPA consistently outperforms existing FL algorithms, achieving superior performance in personalized adaptation.</li>
</ul>

<h3>Title: SLIE: A Secure and Lightweight Cryptosystem for Data Sharing in IoT Healthcare Services</h3>
<ul>
<li><strong>Authors: </strong>Ha Xuan Son, Nguyen Quoc Anh, Phat T. Tran-Truong, Le Thanh Tuan, Pham Thanh Nghiem</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14708">https://arxiv.org/abs/2510.14708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14708">https://arxiv.org/pdf/2510.14708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14708]] SLIE: A Secure and Lightweight Cryptosystem for Data Sharing in IoT Healthcare Services(https://arxiv.org/abs/2510.14708)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack</a></li>
<li><strong>Abstract: </strong>The Internet of Medical Things (IoMT) has revolutionized healthcare by transforming medical operations into standardized, interoperable services. However, this service-oriented model introduces significant security vulnerabilities in device management and communication, which are especially critical given the sensitivity of medical data. To address these risks, this paper proposes SLIE (Secure and Lightweight Identity Encryption), a novel cryptosystem based on Wildcard Key Derivation Identity-Based Encryption (WKD-IBE). SLIE ensures scalable trust and secure omnidirectional communication through end-to-end encryption, hierarchical access control, and a lightweight key management system designed for resource-constrained devices. It incorporates constant-time operations, memory obfuscation, and expiry-based key revocation to counter side-channel, man-in-the-middle, and unauthorized access attacks, thereby ensuring compliance with standards like HIPAA and GDPR. Evaluations show that SLIE significantly outperforms RSA, with encryption and decryption times of 0.936ms and 0.217ms for 1KB of data, an 84.54% improvement in encryption speed, a 99.70% improvement in decryption speed, and an energy efficiency of 0.014 J/KB.</li>
</ul>

<h3>Title: Where are the Whales: A Human-in-the-loop Detection Method for Identifying Whales in High-resolution Satellite Imagery</h3>
<ul>
<li><strong>Authors: </strong>Caleb Robinson, Kimberly T. Goetz, Christin B. Khan, Meredith Sackett, Kathleen Leonard, Rahul Dodhia, Juan M. Lavista Ferres</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14709">https://arxiv.org/abs/2510.14709</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14709">https://arxiv.org/pdf/2510.14709</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14709]] Where are the Whales: A Human-in-the-loop Detection Method for Identifying Whales in High-resolution Satellite Imagery(https://arxiv.org/abs/2510.14709)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Effective monitoring of whale populations is critical for conservation, but traditional survey methods are expensive and difficult to scale. While prior work has shown that whales can be identified in very high-resolution (VHR) satellite imagery, large-scale automated detection remains challenging due to a lack of annotated imagery, variability in image quality and environmental conditions, and the cost of building robust machine learning pipelines over massive remote sensing archives. We present a semi-automated approach for surfacing possible whale detections in VHR imagery using a statistical anomaly detection method that flags spatial outliers, i.e. "interesting points". We pair this detector with a web-based labeling interface designed to enable experts to quickly annotate the interesting points. We evaluate our system on three benchmark scenes with known whale annotations and achieve recalls of 90.3% to 96.4%, while reducing the area requiring expert inspection by up to 99.8% -- from over 1,000 sq km to less than 2 sq km in some cases. Our method does not rely on labeled training data and offers a scalable first step toward future machine-assisted marine mammal monitoring from space. We have open sourced this pipeline at this https URL.</li>
</ul>

<h3>Title: Camera Movement Classification in Historical Footage: A Comparative Study of Deep Video Models</h3>
<ul>
<li><strong>Authors: </strong>Tingyu Lin, Armin Dadras, Florian Kleber, Robert Sablatnig</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14713">https://arxiv.org/abs/2510.14713</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14713">https://arxiv.org/pdf/2510.14713</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14713]] Camera Movement Classification in Historical Footage: A Comparative Study of Deep Video Models(https://arxiv.org/abs/2510.14713)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Camera movement conveys spatial and narrative information essential for understanding video content. While recent camera movement classification (CMC) methods perform well on modern datasets, their generalization to historical footage remains unexplored. This paper presents the first systematic evaluation of deep video CMC models on archival film material. We summarize representative methods and datasets, highlighting differences in model design and label definitions. Five standard video classification models are assessed on the HISTORIAN dataset, which includes expert-annotated World War II footage. The best-performing model, Video Swin Transformer, achieves 80.25% accuracy, showing strong convergence despite limited training data. Our findings highlight the challenges and potential of adapting existing models to low-quality video and motivate future work combining diverse input modalities and temporal architectures.</li>
</ul>

<h3>Title: Seesaw: Accelerating Training by Balancing Learning Rate and Batch Size Scheduling</h3>
<ul>
<li><strong>Authors: </strong>Alexandru Meterez, Depen Morwani, Jingfeng Wu, Costin-Andrei Oncescu, Cengiz Pehlevan, Sham Kakade</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, math.OC, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14717">https://arxiv.org/abs/2510.14717</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14717">https://arxiv.org/pdf/2510.14717</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14717]] Seesaw: Accelerating Training by Balancing Learning Rate and Batch Size Scheduling(https://arxiv.org/abs/2510.14717)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Increasing the batch size during training -- a ''batch ramp'' -- is a promising strategy to accelerate large language model pretraining. While for SGD, doubling the batch size can be equivalent to halving the learning rate, the optimal strategy for adaptive optimizers like Adam is less clear. As a result, any batch-ramp scheduling, if used at all, is typically tuned heuristically. This work develops a principled framework for batch-size scheduling and introduces Seesaw: whenever a standard scheduler would halve the learning rate, Seesaw instead multiplies it by $1/\sqrt{2}$ and doubles the batch size, preserving loss dynamics while reducing serial steps. Theoretically, we provide, to our knowledge, the first finite-sample proof of equivalence between learning-rate decay and batch-size ramp-up for SGD on noisy linear regression, and we extend this equivalence to normalized SGD, a tractable proxy for Adam, under a variance-dominated regime observed in practice. Empirically, on 150M/300M/600M-parameter models trained at Chinchilla scale using a constant (critical) batch size, Seesaw matches cosine decay at equal FLOPs while reducing wall-clock time by $\approx 36\%$, approaching the theoretical limit implied by our analysis.</li>
</ul>

<h3>Title: Speculative Model Risk in Healthcare AI: Using Storytelling to Surface Unintended Harms</h3>
<ul>
<li><strong>Authors: </strong>Xingmeng Zhao, Dan Schumacher, Veronica Rammouz, Anthony Rios</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14718">https://arxiv.org/abs/2510.14718</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14718">https://arxiv.org/pdf/2510.14718</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14718]] Speculative Model Risk in Healthcare AI: Using Storytelling to Surface Unintended Harms(https://arxiv.org/abs/2510.14718)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Artificial intelligence (AI) is rapidly transforming healthcare, enabling fast development of tools like stress monitors, wellness trackers, and mental health chatbots. However, rapid and low-barrier development can introduce risks of bias, privacy violations, and unequal access, especially when systems ignore real-world contexts and diverse user needs. Many recent methods use AI to detect risks automatically, but this can reduce human engagement in understanding how harms arise and who they affect. We present a human-centered framework that generates user stories and supports multi-agent discussions to help people think creatively about potential benefits and harms before deployment. In a user study, participants who read stories recognized a broader range of harms, distributing their responses more evenly across all 13 harm types. In contrast, those who did not read stories focused primarily on privacy and well-being (58.3%). Our findings show that storytelling helped participants speculate about a broader range of harms and benefits and think more creatively about AI's impact on users.</li>
</ul>

<h3>Title: Cross-Layer Feature Self-Attention Module for Multi-Scale Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Dingzhou Xie, Rushi Lan, Cheng Pang, Enhao Ning, Jiahao Zeng, Wei Zheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14726">https://arxiv.org/abs/2510.14726</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14726">https://arxiv.org/pdf/2510.14726</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14726]] Cross-Layer Feature Self-Attention Module for Multi-Scale Object Detection(https://arxiv.org/abs/2510.14726)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Recent object detection methods have made remarkable progress by leveraging attention mechanisms to improve feature discriminability. However, most existing approaches are confined to refining single-layer or fusing dual-layer features, overlooking the rich inter-layer dependencies across multi-scale representations. This limits their ability to capture comprehensive contextual information essential for detecting objects with large scale variations. In this paper, we propose a novel Cross-Layer Feature Self-Attention Module (CFSAM), which holistically models both local and global dependencies within multi-scale feature maps. CFSAM consists of three key components: a convolutional local feature extractor, a Transformer-based global modeling unit that efficiently captures cross-layer interactions, and a feature fusion mechanism to restore and enhance the original representations. When integrated into the SSD300 framework, CFSAM significantly boosts detection performance, achieving 78.6% mAP on PASCAL VOC (vs. 75.5% baseline) and 52.1% mAP on COCO (vs. 43.1% baseline), outperforming existing attention modules. Moreover, the module accelerates convergence during training without introducing substantial computational overhead. Our work highlights the importance of explicit cross-layer attention modeling in advancing multi-scale object detection.</li>
</ul>

<h3>Title: AutoRubric-R1V: Rubric-Based Generative Rewards for Faithful Multimodal Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Mengzhao Jia, Zhihan Zhang, Ignacio Cases, Zheyuan Liu, Meng Jiang, Peng Qi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14738">https://arxiv.org/abs/2510.14738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14738">https://arxiv.org/pdf/2510.14738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14738]] AutoRubric-R1V: Rubric-Based Generative Rewards for Faithful Multimodal Reasoning(https://arxiv.org/abs/2510.14738)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) have rapidly advanced from perception tasks to complex multi-step reasoning, yet reinforcement learning with verifiable rewards (RLVR) often leads to spurious reasoning since only the final-answer correctness is rewarded. To address this limitation, we propose AutoRubric-R1V, a framework that integrates RLVR with process-level supervision through automatically collected rubric-based generative rewards. Our key innovation lies in a scalable self-aggregation method that distills consistent reasoning checkpoints from successful trajectories, enabling problem-specific rubric construction without human annotation or stronger teacher models. By jointly leveraging rubric-based and outcome rewards, AutoRubric-R1V achieves state-of-the-art performance on six multimodal reasoning benchmarks and substantially improves reasoning faithfulness in dedicated evaluations.</li>
</ul>

<h3>Title: DEXTER: Diffusion-Guided EXplanations with TExtual Reasoning for Vision Models</h3>
<ul>
<li><strong>Authors: </strong>Simone Carnemolla, Matteo Pennisi, Sarinda Samarasinghe, Giovanni Bellitto, Simone Palazzo, Daniela Giordano, Mubarak Shah, Concetto Spampinato</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14741">https://arxiv.org/abs/2510.14741</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14741">https://arxiv.org/pdf/2510.14741</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14741]] DEXTER: Diffusion-Guided EXplanations with TExtual Reasoning for Vision Models(https://arxiv.org/abs/2510.14741)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, diffusion, data-free, large language model</a></li>
<li><strong>Abstract: </strong>Understanding and explaining the behavior of machine learning models is essential for building transparent and trustworthy AI systems. We introduce DEXTER, a data-free framework that employs diffusion models and large language models to generate global, textual explanations of visual classifiers. DEXTER operates by optimizing text prompts to synthesize class-conditional images that strongly activate a target classifier. These synthetic samples are then used to elicit detailed natural language reports that describe class-specific decision patterns and biases. Unlike prior work, DEXTER enables natural language explanation about a classifier's decision process without access to training data or ground-truth labels. We demonstrate DEXTER's flexibility across three tasks-activation maximization, slice discovery and debiasing, and bias explanation-each illustrating its ability to uncover the internal mechanisms of visual classifiers. Quantitative and qualitative evaluations, including a user study, show that DEXTER produces accurate, interpretable outputs. Experiments on ImageNet, Waterbirds, CelebA, and FairFaces confirm that DEXTER outperforms existing approaches in global model explanation and class-level bias reporting. Code is available at this https URL.</li>
</ul>

<h3>Title: Beyond Multi-Token Prediction: Pretraining LLMs with Future Summaries</h3>
<ul>
<li><strong>Authors: </strong>Divyat Mahajan, Sachin Goyal, Badr Youbi Idrissi, Mohammad Pezeshki, Ioannis Mitliagkas, David Lopez-Paz, Kartik Ahuja</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14751">https://arxiv.org/abs/2510.14751</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14751">https://arxiv.org/pdf/2510.14751</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14751]] Beyond Multi-Token Prediction: Pretraining LLMs with Future Summaries(https://arxiv.org/abs/2510.14751)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Next-token prediction (NTP) has driven the success of large language models (LLMs), but it struggles with long-horizon reasoning, planning, and creative writing, with these limitations largely attributed to teacher-forced training. Multi-token prediction (MTP) partially mitigates these issues by predicting several future tokens at once, but it mostly captures short-range dependencies and offers limited improvement. We propose future summary prediction (FSP), which trains an auxiliary head to predict a compact representation of the long-term future, preserving information relevant for long-form generations. We explore two variants of FSP: handcrafted summaries, for example, a bag of words summary of the future of the sequence, and learned summaries, which use embeddings produced by a reverse language model trained from right to left. Large-scale pretraining experiments (3B and 8B-parameter models) demonstrate that FSP provides improvements over both NTP and MTP across math, reasoning, and coding benchmarks.</li>
</ul>

<h3>Title: LightQANet: Quantized and Adaptive Feature Learning for Low-Light Image Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Xu Wu, Zhihui Lai, Xianxu Hou, Jie Zhou, Ya-nan Zhang, Linlin Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14753">https://arxiv.org/abs/2510.14753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14753">https://arxiv.org/pdf/2510.14753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14753]] LightQANet: Quantized and Adaptive Feature Learning for Low-Light Image Enhancement(https://arxiv.org/abs/2510.14753)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Low-light image enhancement (LLIE) aims to improve illumination while preserving high-quality color and texture. However, existing methods often fail to extract reliable feature representations due to severely degraded pixel-level information under low-light conditions, resulting in poor texture restoration, color inconsistency, and artifact. To address these challenges, we propose LightQANet, a novel framework that introduces quantized and adaptive feature learning for low-light enhancement, aiming to achieve consistent and robust image quality across diverse lighting conditions. From the static modeling perspective, we design a Light Quantization Module (LQM) to explicitly extract and quantify illumination-related factors from image features. By enforcing structured light factor learning, LQM enhances the extraction of light-invariant representations and mitigates feature inconsistency across varying illumination levels. From the dynamic adaptation perspective, we introduce a Light-Aware Prompt Module (LAPM), which encodes illumination priors into learnable prompts to dynamically guide the feature learning process. LAPM enables the model to flexibly adapt to complex and continuously changing lighting conditions, further improving image enhancement. Extensive experiments on multiple low-light datasets demonstrate that our method achieves state-of-the-art performance, delivering superior qualitative and quantitative results across various challenging lighting scenarios.</li>
</ul>

<h3>Title: Pluto: A Benchmark for Evaluating Efficiency of LLM-generated Hardware Code</h3>
<ul>
<li><strong>Authors: </strong>Manar Abdelatty, Maryam Nouh, Jacob K. Rosenstein, Sherief Reda</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14756">https://arxiv.org/abs/2510.14756</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14756">https://arxiv.org/pdf/2510.14756</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14756]] Pluto: A Benchmark for Evaluating Efficiency of LLM-generated Hardware Code(https://arxiv.org/abs/2510.14756)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly used to automate hardware design tasks, including the generation of Verilog code. While early benchmarks focus primarily on functional correctness, efficient hardware design demands additional optimization for synthesis metrics such as area, delay, and power. Existing benchmarks fall short in evaluating these aspects comprehensively: they often lack optimized baselines or testbenches for verification. To address these gaps, we present Pluto, a benchmark and evaluation framework designed to assess the efficiency of LLM-generated Verilog designs. Pluto presents a comprehensive evaluation set of 114 problems with self-checking testbenches and multiple Pareto-optimal reference implementations. Experimental results show that state-of-the-art LLMs can achieve high functional correctness, reaching 78.3\% at pass@1, but their synthesis efficiency still lags behind expert-crafted implementations, with area efficiency of 63.8\%, delay efficiency of 65.9\%, and power efficiency of 64.0\% at eff@1. This highlights the need for efficiency-aware evaluation frameworks such as Pluto to drive progress in hardware-focused LLM research.</li>
</ul>

<h3>Title: COIG-Writer: A High-Quality Dataset for Chinese Creative Writing with Thought Processes</h3>
<ul>
<li><strong>Authors: </strong>Yunwen Li, Shuangshuang Ying, Xingwei Qu, Xin Li, Sheng Jin, Minghao Liu, Zhoufutu Wen, Tianyu Zheng, Xeron Du, Qiguang Chen, Jiajun Shi, Wangchunshu Zhou, Jiazhan Feng, Wanjun Zhong, Libo Qin, Stephen Huang, Wanxiang Che, Chenghua Lin, Eli Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14763">https://arxiv.org/abs/2510.14763</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14763">https://arxiv.org/pdf/2510.14763</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14763]] COIG-Writer: A High-Quality Dataset for Chinese Creative Writing with Thought Processes(https://arxiv.org/abs/2510.14763)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models exhibit systematic deficiencies in creative writing, particularly in non-English contexts where training data is scarce and lacks process-level supervision. We present COIG-Writer, a novel Chinese creative writing dataset that captures both diverse outputs and their underlying thought processes through systematic reverse-engineering of high-quality texts. Unlike existing datasets that provide only input-output pairs, COIG-Writer comprises 1,665 meticulously curated triplets spanning 51 genres, each containing: (1) a reverse-engineered prompt, (2) detailed creative reasoning documenting decision-making processes, and (3) the final text. Through comprehensive experiments, we identify a two-component model of creative writing: narrative logic (provided by process supervision) and linguistic expression (maintained by general-purpose data). Our findings reveal three critical insights: (1) Process supervision is highly effective but requires stabilization with general data. A ratio of at least one creative sample to twelve general samples is needed to achieve optimal performance; below this threshold, the win rate progressively degrades (from 62.75% down to 35.78%)., (2) creative capabilities are culturally-bound with no cross-lingual transfer (89.26pp gap between Chinese and English performance), and (3) lexical diversity inversely correlates with creative quality (TTR paradox), suggesting high diversity signals compensatory behavior for logical deficiencies. These findings establish that creative excellence emerges from the interaction between logical scaffolding and linguistic grounding, analogous to how mathematical reasoning enhances but cannot replace linguistic competence in foundation models.</li>
</ul>

<h3>Title: Inpainting the Red Planet: Diffusion Models for the Reconstruction of Martian Environments in Virtual Reality</h3>
<ul>
<li><strong>Authors: </strong>Giuseppe Lorenzo Catalano, Agata Marta Soccini</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14765">https://arxiv.org/abs/2510.14765</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14765">https://arxiv.org/pdf/2510.14765</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14765]] Inpainting the Red Planet: Diffusion Models for the Reconstruction of Martian Environments in Virtual Reality(https://arxiv.org/abs/2510.14765)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Space exploration increasingly relies on Virtual Reality for several tasks, such as mission planning, multidisciplinary scientific analysis, and astronaut training. A key factor for the reliability of the simulations is having accurate 3D representations of planetary terrains. Extraterrestrial heightmaps derived from satellite imagery often contain missing values due to acquisition and transmission constraints. Mars is among the most studied planets beyond Earth, and its extensive terrain datasets make the Martian surface reconstruction a valuable task, although many areas remain unmapped. Deep learning algorithms can support void-filling tasks; however, whereas Earth's comprehensive datasets enables the use of conditional methods, such approaches cannot be applied to Mars. Current approaches rely on simpler interpolation techniques which, however, often fail to preserve geometric coherence. In this work, we propose a method for reconstructing the surface of Mars based on an unconditional diffusion model. Training was conducted on an augmented dataset of 12000 Martian heightmaps derived from NASA's HiRISE survey. A non-homogeneous rescaling strategy captures terrain features across multiple scales before resizing to a fixed 128x128 model resolution. We compared our method against established void-filling and inpainting techniques, including Inverse Distance Weighting, kriging, and Navier-Stokes algorithm, on an evaluation set of 1000 samples. Results show that our approach consistently outperforms these methods in terms of reconstruction accuracy (4-15% on RMSE) and perceptual similarity (29-81% on LPIPS) with the original data.</li>
</ul>

<h3>Title: MoCom: Motion-based Inter-MAV Visual Communication Using Event Vision and Spiking Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Zhang Nengbo, Hann Woei Ho, Ye Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14770">https://arxiv.org/abs/2510.14770</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14770">https://arxiv.org/pdf/2510.14770</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14770]] MoCom: Motion-based Inter-MAV Visual Communication Using Event Vision and Spiking Neural Networks(https://arxiv.org/abs/2510.14770)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Reliable communication in Micro Air Vehicle (MAV) swarms is challenging in environments, where conventional radio-based methods suffer from spectrum congestion, jamming, and high power consumption. Inspired by the waggle dance of honeybees, which efficiently communicate the location of food sources without sound or contact, we propose a novel visual communication framework for MAV swarms using motion-based signaling. In this framework, MAVs convey information, such as heading and distance, through deliberate flight patterns, which are passively captured by event cameras and interpreted using a predefined visual codebook of four motion primitives: vertical (up/down), horizontal (left/right), left-to-up-to-right, and left-to-down-to-right, representing control symbols (``start'', ``end'', ``1'', ``0''). To decode these signals, we design an event frame-based segmentation model and a lightweight Spiking Neural Network (SNN) for action recognition. An integrated decoding algorithm then combines segmentation and classification to robustly interpret MAV motion sequences. Experimental results validate the framework's effectiveness, which demonstrates accurate decoding and low power consumption, and highlights its potential as an energy-efficient alternative for MAV communication in constrained environments.</li>
</ul>

<h3>Title: Finding Answers in Thought Matters: Revisiting Evaluation on Large Language Models with Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Hwiyeol Jo, Joosung Lee, Jaehone Lee, Sang-Woo Lee, Joonsuk Park, Kang Min Yoo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14773">https://arxiv.org/abs/2510.14773</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14773">https://arxiv.org/pdf/2510.14773</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14773]] Finding Answers in Thought Matters: Revisiting Evaluation on Large Language Models with Reasoning(https://arxiv.org/abs/2510.14773)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, generative, large language model</a></li>
<li><strong>Abstract: </strong>Evaluating generative models, such as large language models (LLMs), commonly involves question-answering tasks where the final answer is selected based on probability of answer choices. On the other hand, for models requiring reasoning, the method of answer extraction plays a critical role. Our research reveals that the performance of reasoning models and their final answer distributions are highly sensitive to the answer extraction algorithm employed. In order to mitigate this, we propose a basic framework: Answer Regeneration. The method uses an additional model inference, providing the prior input and output prefaced by the prompt "Answer:". The final answer is then selected or extracted from the regenerated output. We show that this extraction-rule-agnostic approach exhibits improved performance and enhanced robustness. Furthermore, we have applied this framework to general math problems and open-ended question answering tasks. Our analysis and this framework could offer a more reliable results for model evaluation.</li>
</ul>

<h3>Title: CoT-PL: Visual Chain-of-Thought Reasoning Meets Pseudo-Labeling for Open-Vocabulary Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Hojun Choi, Youngsun Lim, Jaeyo Shin, Hyunjung Shim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14792">https://arxiv.org/abs/2510.14792</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14792">https://arxiv.org/pdf/2510.14792</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14792]] CoT-PL: Visual Chain-of-Thought Reasoning Meets Pseudo-Labeling for Open-Vocabulary Object Detection(https://arxiv.org/abs/2510.14792)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Open-vocabulary object detection (OVD) seeks to recognize and localize object categories beyond those seen during training. Recent approaches typically leverage vision-language models (VLMs) to generate pseudo-labels using image-text alignment, allowing detectors to generalize to unseen classes without explicit supervision. However, these methods depend heavily on direct image-text matching, neglecting the intermediate reasoning steps essential for interpreting semantically complex scenes. This results in limited robustness when confronted with crowded or occluded visual contexts. In this paper, we introduce CoT-PL, a new framework that employs structured visual chain-of-thought (CoT) reasoning into the pseudo-labeling process. CoT-PL decomposes object understanding into three interpretable steps: (1) region perception even for unseen objects, (2) category recognition via zero-shot reasoning, and (3) background grounding to separate semantically complex objects. Crucially, the third step naturally motivates our contrastive background learning (CBL) that uses the pre-computed background cues as negatives to promote feature disentanglement between objects and background. In this way, CoT reasoning and CBL form an integrated pipeline tailored to robust pseudo-labeling in crowded or occluded scenes. Notably, in these two settings, our novel-class pseudo-label quality achieves relative improvements of 103.4% and 168.4% over the best prior, respectively. Our extensive experiments demonstrate that CoT-PL achieves +7.7 AP50 on open-vocabulary COCO and +2.9 mask AP on LVIS for novel classes, setting a new state of the art.</li>
</ul>

<h3>Title: Morphology-Aware Prognostic model for Five-Year Survival Prediction in Colorectal Cancer from H&E Whole Slide Images</h3>
<ul>
<li><strong>Authors: </strong>Usama Sajjad, Abdul Rehman Akbar, Ziyu Su, Deborah Knight, Wendy L. Frankel, Metin N. Gurcan, Wei Chen, Muhammad Khalid Khan Niazi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14800">https://arxiv.org/abs/2510.14800</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14800">https://arxiv.org/pdf/2510.14800</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14800]] Morphology-Aware Prognostic model for Five-Year Survival Prediction in Colorectal Cancer from H&E Whole Slide Images(https://arxiv.org/abs/2510.14800)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Colorectal cancer (CRC) remains the third most prevalent malignancy globally, with approximately 154,000 new cases and 54,000 projected deaths anticipated for 2025. The recent advancement of foundation models in computational pathology has been largely propelled by task agnostic methodologies that can overlook organ-specific crucial morphological patterns that represent distinct biological processes that can fundamentally influence tumor behavior, therapeutic response, and patient outcomes. The aim of this study is to develop a novel, interpretable AI model, PRISM (Prognostic Representation of Integrated Spatial Morphology), that incorporates a continuous variability spectrum within each distinct morphology to characterize phenotypic diversity and reflecting the principle that malignant transformation occurs through incremental evolutionary processes rather than abrupt phenotypic shifts. PRISM is trained on 8.74 million histological images extracted from surgical resection specimens of 424 patients with stage III CRC. PRISM achieved superior prognostic performance for five-year OS (AUC = 0.70 +- 0.04; accuracy = 68.37% +- 4.75%; HR = 3.34, 95% CI = 2.28-4.90; p < 0.0001), outperforming existing CRC-specific methods by 15% and AI foundation models by ~23% accuracy. It showed sex-agnostic robustness (AUC delta = 0.02; accuracy delta = 0.15%) and stable performance across clinicopathological subgroups, with minimal accuracy fluctuation (delta = 1.44%) between 5FU/LV and CPT-11/5FU/LV regimens, replicating the Alliance cohort finding of no survival difference between treatments.</li>
</ul>

<h3>Title: Scaling Artificial Intelligence for Multi-Tumor Early Detection with More Reports, Fewer Masks</h3>
<ul>
<li><strong>Authors: </strong>Pedro R. A. S. Bassi, Xinze Zhou, Wenxuan Li, Szymon Płotka, Jieneng Chen, Qi Chen, Zheren Zhu, Jakub Prządo, Ibrahim E. Hamacı, Sezgin Er, Yuhan Wang, Ashwin Kumar, Bjoern Menze, Jarosław B. Ćwikła, Yuyin Zhou, Akshay S. Chaudhari, Curtis P. Langlotz, Sergio Decherchi, Andrea Cavalli, Kang Wang, Yang Yang, Alan L. Yuille, Zongwei Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14803">https://arxiv.org/abs/2510.14803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14803">https://arxiv.org/pdf/2510.14803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14803]] Scaling Artificial Intelligence for Multi-Tumor Early Detection with More Reports, Fewer Masks(https://arxiv.org/abs/2510.14803)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Early tumor detection save lives. Each year, more than 300 million computed tomography (CT) scans are performed worldwide, offering a vast opportunity for effective cancer screening. However, detecting small or early-stage tumors on these CT scans remains challenging, even for experts. Artificial intelligence (AI) models can assist by highlighting suspicious regions, but training such models typically requires extensive tumor masks--detailed, voxel-wise outlines of tumors manually drawn by radiologists. Drawing these masks is costly, requiring years of effort and millions of dollars. In contrast, nearly every CT scan in clinical practice is already accompanied by medical reports describing the tumor's size, number, appearance, and sometimes, pathology results--information that is rich, abundant, and often underutilized for AI training. We introduce R-Super, which trains AI to segment tumors that match their descriptions in medical reports. This approach scales AI training with large collections of readily available medical reports, substantially reducing the need for manually drawn tumor masks. When trained on 101,654 reports, AI models achieved performance comparable to those trained on 723 masks. Combining reports and masks further improved sensitivity by +13% and specificity by +8%, surpassing radiologists in detecting five of the seven tumor types. Notably, R-Super enabled segmentation of tumors in the spleen, gallbladder, prostate, bladder, uterus, and esophagus, for which no public masks or AI models previously existed. This study challenges the long-held belief that large-scale, labor-intensive tumor mask creation is indispensable, establishing a scalable and accessible path toward early detection across diverse tumor types. We plan to release our trained models, code, and dataset at this https URL</li>
</ul>

<h3>Title: Rethinking Hebbian Principle: Low-Dimensional Structural Projection for Unsupervised Learning</h3>
<ul>
<li><strong>Authors: </strong>Shikuang Deng, Jiayuan Zhang, Yuhang Wu, Ting Chen, Shi Gu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14810">https://arxiv.org/abs/2510.14810</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14810">https://arxiv.org/pdf/2510.14810</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14810]] Rethinking Hebbian Principle: Low-Dimensional Structural Projection for Unsupervised Learning(https://arxiv.org/abs/2510.14810)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Hebbian learning is a biological principle that intuitively describes how neurons adapt their connections through repeated stimuli. However, when applied to machine learning, it suffers serious issues due to the unconstrained updates of the connections and the lack of accounting for feedback mediation. Such shortcomings limit its effective scaling to complex network architectures and tasks. To this end, here we introduce the Structural Projection Hebbian Representation (SPHeRe), a novel unsupervised learning method that integrates orthogonality and structural information preservation through a local auxiliary nonlinear block. The loss for structural information preservation backpropagates to the input through an auxiliary lightweight projection that conceptually serves as feedback mediation while the orthogonality constraints account for the boundedness of updating magnitude. Extensive experimental results show that SPHeRe achieves SOTA performance among unsupervised synaptic plasticity approaches on standard image classification benchmarks, including CIFAR-10, CIFAR-100, and Tiny-ImageNet. Furthermore, the method exhibits strong effectiveness in continual learning and transfer learning scenarios, and image reconstruction tasks show the robustness and generalizability of the extracted features. This work demonstrates the competitiveness and potential of Hebbian unsupervised learning rules within modern deep learning frameworks, demonstrating the possibility of efficient and biologically inspired learning algorithms without the strong dependence on strict backpropagation. Our code is available at this https URL.</li>
</ul>

<h3>Title: Unifying Environment Perception and Route Choice Modeling for Trajectory Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Ji Cao, Yu Wang, Tongya Zheng, Zujie Ren, Canghong Jin, Gang Chen, Mingli Song</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14819">https://arxiv.org/abs/2510.14819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14819">https://arxiv.org/pdf/2510.14819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14819]] Unifying Environment Perception and Route Choice Modeling for Trajectory Representation Learning(https://arxiv.org/abs/2510.14819)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Trajectory Representation Learning (TRL) aims to encode raw trajectories into low-dimensional vectors, which can then be leveraged in various downstream tasks, including travel time estimation, location prediction, and trajectory similarity analysis. However, existing TRL methods suffer from a key oversight: treating trajectories as isolated spatio-temporal sequences, without considering the external environment and internal route choice behavior that govern their formation. To bridge this gap, we propose a novel framework that unifies comprehensive environment \textbf{P}erception and explicit \textbf{R}oute choice modeling for effective \textbf{Traj}ectory representation learning, dubbed \textbf{PRTraj}. Specifically, PRTraj first introduces an Environment Perception Module to enhance the road network by capturing multi-granularity environmental semantics from surrounding POI distributions. Building on this environment-aware backbone, a Route Choice Encoder then captures the route choice behavior inherent in each trajectory by modeling its constituent road segment transitions as a sequence of decisions. These route-choice-aware representations are finally aggregated to form the global trajectory embedding. Extensive experiments on 3 real-world datasets across 5 downstream tasks validate the effectiveness and generalizability of PRTraj. Moreover, PRTraj demonstrates strong data efficiency, maintaining robust performance under few-shot scenarios. Our code is available at: this https URL.</li>
</ul>

<h3>Title: FraQAT: Quantization Aware Training with Fractional bits</h3>
<ul>
<li><strong>Authors: </strong>Luca Morreale, Alberto Gil C. P. Ramos, Malcolm Chadwick, Mehid Noroozi, Ruchika Chavhan, Abhinav Mehrotra, Sourav Bhattacharya</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14823">https://arxiv.org/abs/2510.14823</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14823">https://arxiv.org/pdf/2510.14823</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14823]] FraQAT: Quantization Aware Training with Fractional bits(https://arxiv.org/abs/2510.14823)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>State-of-the-art (SOTA) generative models have demonstrated impressive capabilities in image synthesis or text generation, often with a large capacity model. However, these large models cannot be deployed on smartphones due to the limited availability of on-board memory and computations. Quantization methods lower the precision of the model parameters, allowing for efficient computations, \eg, in \INT{8}. Although aggressive quantization addresses efficiency and memory constraints, preserving the quality of the model remains a challenge. To retain quality in previous aggressive quantization, we propose a new fractional bits quantization (\short) approach. The novelty is a simple yet effective idea: we progressively reduce the model's precision from 32 to 4 bits per parameter, and exploit the fractional bits during optimization to maintain high generation quality. We show that the \short{} yields improved quality on a variety of diffusion models, including SD3.5-Medium, Sana, \pixart, and FLUX.1-schnell, while achieving $4-7\%$ lower FiD than standard QAT. Finally, we deploy and run Sana on a Samsung S25U, which runs on the Qualcomm SM8750-AB Snapdragon 8 Elite Hexagon Tensor Processor (HTP).</li>
</ul>

<h3>Title: Supervised Fine-Tuning or Contrastive Learning? Towards Better Multimodal LLM Reranking</h3>
<ul>
<li><strong>Authors: </strong>Ziqi Dai, Xin Zhang, Mingxin Li, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, Wenjie Li, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14824">https://arxiv.org/abs/2510.14824</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14824">https://arxiv.org/pdf/2510.14824</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14824]] Supervised Fine-Tuning or Contrastive Learning? Towards Better Multimodal LLM Reranking(https://arxiv.org/abs/2510.14824)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>In information retrieval, training reranking models mainly focuses on two types of objectives: metric learning (e.g. contrastive loss to increase the predicted scores on relevant query-document pairs) and classification (binary label prediction of relevance vs. irrelevance). For BERT-style encoders, various studies have shown that contrastive learning (CL) can be more effective than discriminative (classification) learning. However, for large language models (LLMs), classification via supervised fine-tuning (SFT), which predicts ''yes'' (resp. ''no'') token for relevant (resp. irrelevant) pairs, appears more promising as it aligns well with the generative nature of LLMs. This divergence raises a central question: which objective is intrinsically better suited to LLM-based reranking, and what mechanism underlies the difference? In this work, we conduct a comprehensive comparison and analysis between CL and SFT for reranking, taking the universal multimodal retrieval (UMR) as the experimental playground. We first decompose the objectives into two components: weight, which controls the magnitude of those updates, and direction, which guides the model updates, then present a unified framework for understanding their interactions. Through probing experiments, we find that SFT provides a substantially stronger weighting scheme than CL, whereas the preferred scoring direction shows no clear winner. Taken together, these results point to a consistent advantage of SFT over CL for LLM reranking. To further validate our findings, we conduct large-scale training with SFT and present new state-of-the-art rerankers on the MRB benchmark. We also provide ablations on SFT settings and expect our findings to benefit future research and applications in this area.</li>
</ul>

<h3>Title: Programmatic Representation Learning with Language Models</h3>
<ul>
<li><strong>Authors: </strong>Gabriel Poesia, Georgia Gabriela Sampaio</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14825">https://arxiv.org/abs/2510.14825</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14825">https://arxiv.org/pdf/2510.14825</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14825]] Programmatic Representation Learning with Language Models(https://arxiv.org/abs/2510.14825)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Classical models for supervised machine learning, such as decision trees, are efficient and interpretable predictors, but their quality is highly dependent on the particular choice of input features. Although neural networks can learn useful representations directly from raw data (e.g., images or text), this comes at the expense of interpretability and the need for specialized hardware to run them efficiently. In this paper, we explore a hypothesis class we call Learned Programmatic Representations (LeaPR) models, which stack arbitrary features represented as code (functions from data points to scalars) and decision tree predictors. We synthesize feature functions using Large Language Models (LLMs), which have rich prior knowledge in a wide range of domains and a remarkable ability to write code using existing domain-specific libraries. We propose two algorithms to learn LeaPR models from supervised data. First, we design an adaptation of FunSearch to learn features rather than directly generate predictors. Then, we develop a novel variant of the classical ID3 algorithm for decision tree learning, where new features are generated on demand when splitting leaf nodes. In experiments from chess position evaluation to image and text classification, our methods learn high-quality, neural network-free predictors often competitive with neural networks. Our work suggests a flexible paradigm for learning interpretable representations end-to-end where features and predictions can be readily inspected and understood.</li>
</ul>

<h3>Title: To Infinity and Beyond: Tool-Use Unlocks Length Generalization in State Space Models</h3>
<ul>
<li><strong>Authors: </strong>Eran Malach, Omid Saremi, Sinead Williamson, Arwen Bradley, Aryo Lotfi, Emmanuel Abbe, Josh Susskind, Etai Littwin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14826">https://arxiv.org/abs/2510.14826</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14826">https://arxiv.org/pdf/2510.14826</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14826]] To Infinity and Beyond: Tool-Use Unlocks Length Generalization in State Space Models(https://arxiv.org/abs/2510.14826)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>State Space Models (SSMs) have become the leading alternative to Transformers for sequence modeling. Their primary advantage is efficiency in long-context and long-form generation, enabled by fixed-size memory and linear scaling of computational complexity. We begin this work by showing a simple theoretical result stating that SSMs cannot accurately solve any ``truly long-form'' generation problem (in a sense we formally define), undermining their main competitive advantage. However, we show that this limitation can be mitigated by allowing SSMs interactive access to external tools. In fact, we show that given the right choice of tool access and problem-dependent training data, SSMs can learn to solve any tractable problem and generalize to arbitrary problem length/complexity (i.e., achieve length generalization). Following our theoretical finding, we demonstrate that tool-augmented SSMs achieve remarkable length generalization on a variety of arithmetic, reasoning, and coding tasks. These findings highlight SSMs as a potential efficient alternative to Transformers in interactive tool-based and agentic settings.</li>
</ul>

<h3>Title: Scaling Tumor Segmentation: Best Lessons from Real and Synthetic Data</h3>
<ul>
<li><strong>Authors: </strong>Qi Chen, Xinze Zhou, Chen Liu, Hao Chen, Wenxuan Li, Zekun Jiang, Ziyan Huang, Yuxuan Zhao, Dexin Yu, Junjun He, Yefeng Zheng, Ling Shao, Alan Yuille, Zongwei Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14831">https://arxiv.org/abs/2510.14831</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14831">https://arxiv.org/pdf/2510.14831</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14831]] Scaling Tumor Segmentation: Best Lessons from Real and Synthetic Data(https://arxiv.org/abs/2510.14831)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>AI for tumor segmentation is limited by the lack of large, voxel-wise annotated datasets, which are hard to create and require medical experts. In our proprietary JHH dataset of 3,000 annotated pancreatic tumor scans, we found that AI performance stopped improving after 1,500 scans. With synthetic data, we reached the same performance using only 500 real scans. This finding suggests that synthetic data can steepen data scaling laws, enabling more efficient model training than real data alone. Motivated by these lessons, we created AbdomenAtlas 2.0--a dataset of 10,135 CT scans with a total of 15,130 tumor instances per-voxel manually annotated in six organs (pancreas, liver, kidney, colon, esophagus, and uterus) and 5,893 control scans. Annotated by 23 expert radiologists, it is several orders of magnitude larger than existing public tumor datasets. While we continue expanding the dataset, the current version of AbdomenAtlas 2.0 already provides a strong foundation--based on lessons from the JHH dataset--for training AI to segment tumors in six organs. It achieves notable improvements over public datasets, with a +7% DSC gain on in-distribution tests and +16% on out-of-distribution tests.</li>
</ul>

<h3>Title: Provable Unlearning with Gradient Ascent on Two-Layer ReLU Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Odelia Melamed, Gilad Yehudai, Gal Vardi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.NE, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14844">https://arxiv.org/abs/2510.14844</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14844">https://arxiv.org/pdf/2510.14844</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14844]] Provable Unlearning with Gradient Ascent on Two-Layer ReLU Neural Networks(https://arxiv.org/abs/2510.14844)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Machine Unlearning aims to remove specific data from trained models, addressing growing privacy and ethical concerns. We provide a theoretical analysis of a simple and widely used method - gradient ascent - used to reverse the influence of a specific data point without retraining from scratch. Leveraging the implicit bias of gradient descent towards solutions that satisfy the Karush-Kuhn-Tucker (KKT) conditions of a margin maximization problem, we quantify the quality of the unlearned model by evaluating how well it satisfies these conditions w.r.t. the retained data. To formalize this idea, we propose a new success criterion, termed \textbf{$(\epsilon, \delta, \tau)$-successful} unlearning, and show that, for both linear models and two-layer neural networks with high dimensional data, a properly scaled gradient-ascent step satisfies this criterion and yields a model that closely approximates the retrained solution on the retained data. We also show that gradient ascent performs successful unlearning while still preserving generalization in a synthetic Gaussian-mixture setting.</li>
</ul>

<h3>Title: Backdoor Unlearning by Linear Task Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Amel Abdelraheem, Alessandro Favero, Gerome Bovet, Pascal Frossard</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14845">https://arxiv.org/abs/2510.14845</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14845">https://arxiv.org/pdf/2510.14845</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14845]] Backdoor Unlearning by Linear Task Decomposition(https://arxiv.org/abs/2510.14845)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>Foundation models have revolutionized computer vision by enabling broad generalization across diverse tasks. Yet, they remain highly susceptible to adversarial perturbations and targeted backdoor attacks. Mitigating such vulnerabilities remains an open challenge, especially given that the large-scale nature of the models prohibits retraining to ensure safety. Existing backdoor removal approaches rely on costly fine-tuning to override the harmful behavior, and can often degrade performance on other unrelated tasks. This raises the question of whether backdoors can be removed without compromising the general capabilities of the models. In this work, we address this question and study how backdoors are encoded in the model weight space, finding that they are disentangled from other benign tasks. Specifically, this separation enables the isolation and erasure of the backdoor's influence on the model with minimal impact on clean performance. Building on this insight, we introduce a simple unlearning method that leverages such disentanglement. Through extensive experiments with CLIP-based models and common adversarial triggers, we show that, given the knowledge of the attack, our method achieves approximately perfect unlearning, while retaining, on average, 96% of clean accuracy. Additionally, we demonstrate that even when the attack and its presence are unknown, our method successfully unlearns backdoors by proper estimation using reverse-engineered triggers. Overall, our method consistently yields better unlearning and clean accuracy tradeoffs when compared to present state-of-the-art defenses.</li>
</ul>

<h3>Title: Rewiring Experts on the Fly:Continuous Rerouting for Better Online Adaptation in Mixture-of-Expert models</h3>
<ul>
<li><strong>Authors: </strong>Guinan Su, Yanwu Yang, Li Shen, Lu Yin, Shiwei Liu, Jonas Geiping</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14853">https://arxiv.org/abs/2510.14853</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14853">https://arxiv.org/pdf/2510.14853</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14853]] Rewiring Experts on the Fly:Continuous Rerouting for Better Online Adaptation in Mixture-of-Expert models(https://arxiv.org/abs/2510.14853)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, data-free</a></li>
<li><strong>Abstract: </strong>Mixture-of-Experts (MoE) models achieve efficient scaling through sparse expert activation, but often suffer from suboptimal routing decisions due to distribution shifts in deployment. While existing test-time adaptation methods could potentially address these issues, they primarily focus on dense models and require access to external data, limiting their practical applicability to MoE architectures. However, we find that, instead of relying on reference data, we can optimize MoE expert selection on-the-fly based only on input context. As such, we propose \textit{a data-free, online test-time framework} that continuously adapts MoE routing decisions during text generation without external supervision or data. Our method cycles between two phases: During the prefill stage, and later in regular intervals, we optimize the routing decisions of the model using self-supervision based on the already generated sequence. Then, we generate text as normal, maintaining the modified router until the next adaption. We implement this through lightweight additive vectors that only update router logits in selected layers, maintaining computational efficiency while preventing over-adaptation. The experimental results show consistent performance gains on challenging reasoning tasks while maintaining robustness to context shifts. For example, our method achieves a 5.5\% improvement on HumanEval with OLMoE. Furthermore, owing to its plug-and-play property, our method naturally complements existing test-time scaling techniques, e.g., achieving 6\% average gains when incorporated with self-consistency on DeepSeek-V2-Lite.</li>
</ul>

<h3>Title: Multi-modal video data-pipelines for machine learning with minimal human supervision</h3>
<ul>
<li><strong>Authors: </strong>Mihai-Cristian Pîrvu, Marius Leordeanu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14862">https://arxiv.org/abs/2510.14862</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14862">https://arxiv.org/pdf/2510.14862</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14862]] Multi-modal video data-pipelines for machine learning with minimal human supervision(https://arxiv.org/abs/2510.14862)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The real-world is inherently multi-modal at its core. Our tools observe and take snapshots of it, in digital form, such as videos or sounds, however much of it is lost. Similarly for actions and information passing between humans, languages are used as a written form of communication. Traditionally, Machine Learning models have been unimodal (i.e. rgb -> semantic or text -> sentiment_class). Recent trends go towards bi-modality, where images and text are learned together, however, in order to truly understand the world, we need to integrate all these independent modalities. In this work we try to combine as many visual modalities as we can using little to no human supervision. In order to do this, we use pre-trained experts and procedural combinations between them on top of raw videos using a fully autonomous data-pipeline, which we also open-source. We then make use of PHG-MAE, a model specifically designed to leverage multi-modal data. We show that this model which was efficiently distilled into a low-parameter (<1M) can have competitive results compared to models of ~300M parameters. We deploy this model and analyze the use-case of real-time semantic segmentation from handheld devices or webcams on commodity hardware. Finally, we deploy other off-the-shelf models using the same framework, such as DPT for near real-time depth estimation.</li>
</ul>

<h3>Title: Benchmarking Multimodal Large Language Models for Face Recognition</h3>
<ul>
<li><strong>Authors: </strong>Hatef Otroshi Shahreza, Sébastien Marcel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14866">https://arxiv.org/abs/2510.14866</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14866">https://arxiv.org/pdf/2510.14866</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14866]] Benchmarking Multimodal Large Language Models for Face Recognition(https://arxiv.org/abs/2510.14866)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) have achieved remarkable performance across diverse vision-and-language tasks. However, their potential in face recognition remains underexplored. In particular, the performance of open-source MLLMs needs to be evaluated and compared with existing face recognition models on standard benchmarks with similar protocol. In this work, we present a systematic benchmark of state-of-the-art MLLMs for face recognition on several face recognition datasets, including LFW, CALFW, CPLFW, CFP, AgeDB and RFW. Experimental results reveal that while MLLMs capture rich semantic cues useful for face-related tasks, they lag behind specialized models in high-precision recognition scenarios in zero-shot applications. This benchmark provides a foundation for advancing MLLM-based face recognition, offering insights for the design of next-generation models with higher accuracy and generalization. The source code of our benchmark is publicly available in the project page.</li>
</ul>

<h3>Title: TOUCH: Text-guided Controllable Generation of Free-Form Hand-Object Interactions</h3>
<ul>
<li><strong>Authors: </strong>Guangyi Han, Wei Zhai, Yuhang Yang, Yang Cao, Zheng-Jun Zha</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14874">https://arxiv.org/abs/2510.14874</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14874">https://arxiv.org/pdf/2510.14874</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14874]] TOUCH: Text-guided Controllable Generation of Free-Form Hand-Object Interactions(https://arxiv.org/abs/2510.14874)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Hand-object interaction (HOI) is fundamental for humans to express intent. Existing HOI generation research is predominantly confined to fixed grasping patterns, where control is tied to physical priors such as force closure or generic intent instructions, even when expressed through elaborate language. Such an overly general conditioning imposes a strong inductive bias for stable grasps, thus failing to capture the diversity of daily HOI. To address these limitations, we introduce Free-Form HOI Generation, which aims to generate controllable, diverse, and physically plausible HOI conditioned on fine-grained intent, extending HOI from grasping to free-form interactions, like pushing, poking, and rotating. To support this task, we construct WildO2, an in-the-wild diverse 3D HOI dataset, which includes diverse HOI derived from internet videos. Specifically, it contains 4.4k unique interactions across 92 intents and 610 object categories, each with detailed semantic annotations. Building on this dataset, we propose TOUCH, a three-stage framework centered on a multi-level diffusion model that facilitates fine-grained semantic control to generate versatile hand poses beyond grasping priors. This process leverages explicit contact modeling for conditioning and is subsequently refined with contact consistency and physical constraints to ensure realism. Comprehensive experiments demonstrate our method's ability to generate controllable, diverse, and physically plausible hand interactions representative of daily activities. The project page is $\href{this https URL}{here}$.</li>
</ul>

<h3>Title: BADAS: Context Aware Collision Prediction Using Real-World Dashcam Data</h3>
<ul>
<li><strong>Authors: </strong>Roni Goldshmidt, Hamish Scott, Lorenzo Niccolini, Shizhan Zhu, Daniel Moura, Orly Zvitia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14876">https://arxiv.org/abs/2510.14876</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14876">https://arxiv.org/pdf/2510.14876</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14876]] BADAS: Context Aware Collision Prediction Using Real-World Dashcam Data(https://arxiv.org/abs/2510.14876)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Existing collision prediction methods often fail to distinguish between ego-vehicle threats and random accidents not involving the ego vehicle, leading to excessive false alerts in real-world deployment. We present BADAS, a family of collision prediction models trained on Nexar's real-world dashcam collision dataset -- the first benchmark designed explicitly for ego-centric evaluation. We re-annotate major benchmarks to identify ego involvement, add consensus alert-time labels, and synthesize negatives where needed, enabling fair AP/AUC and temporal evaluation. BADAS uses a V-JEPA2 backbone trained end-to-end and comes in two variants: BADAS-Open (trained on our 1.5k public videos) and BADAS1.0 (trained on 40k proprietary videos). Across DAD, DADA-2000, DoTA, and Nexar, BADAS achieves state-of-the-art AP/AUC and outperforms a forward-collision ADAS baseline while producing more realistic time-to-accident estimates. We release our BADAS-Open model weights and code, along with re-annotations of all evaluation datasets to promote ego-centric collision prediction research.</li>
</ul>

<h3>Title: ScaleWeaver: Weaving Efficient Controllable T2I Generation with Multi-Scale Reference Attention</h3>
<ul>
<li><strong>Authors: </strong>Keli Liu, Zhendong Wang, Wengang Zhou, Shaodong Xu, Ruixiao Dong, Houqiang Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14882">https://arxiv.org/abs/2510.14882</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14882">https://arxiv.org/pdf/2510.14882</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14882]] ScaleWeaver: Weaving Efficient Controllable T2I Generation with Multi-Scale Reference Attention(https://arxiv.org/abs/2510.14882)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Text-to-image generation with visual autoregressive~(VAR) models has recently achieved impressive advances in generation fidelity and inference efficiency. While control mechanisms have been explored for diffusion models, enabling precise and flexible control within VAR paradigm remains underexplored. To bridge this critical gap, in this paper, we introduce ScaleWeaver, a novel framework designed to achieve high-fidelity, controllable generation upon advanced VAR models through parameter-efficient fine-tuning. The core module in ScaleWeaver is the improved MMDiT block with the proposed Reference Attention module, which efficiently and effectively incorporates conditional information. Different from MM Attention, the proposed Reference Attention module discards the unnecessary attention from image$\rightarrow$condition, reducing computational cost while stabilizing control injection. Besides, it strategically emphasizes parameter reuse, leveraging the capability of the VAR backbone itself with a few introduced parameters to process control information, and equipping a zero-initialized linear projection to ensure that control signals are incorporated effectively without disrupting the generative capability of the base model. Extensive experiments show that ScaleWeaver delivers high-quality generation and precise control while attaining superior efficiency over diffusion-based methods, making ScaleWeaver a practical and effective solution for controllable text-to-image generation within the visual autoregressive paradigm. Code and models will be released.</li>
</ul>

<h3>Title: You May Speak Freely: Improving the Fine-Grained Visual Recognition Capabilities of Multimodal Large Language Models with Answer Extraction</h3>
<ul>
<li><strong>Authors: </strong>Logan Lawrence, Oindrila Saha, Megan Wei, Chen Sun, Subhransu Maji, Grant Van Horn</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14885">https://arxiv.org/abs/2510.14885</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14885">https://arxiv.org/pdf/2510.14885</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14885]] You May Speak Freely: Improving the Fine-Grained Visual Recognition Capabilities of Multimodal Large Language Models with Answer Extraction(https://arxiv.org/abs/2510.14885)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, large language model</a></li>
<li><strong>Abstract: </strong>Despite the renewed interest in zero-shot visual classification due to the rise of Multimodal Large Language Models (MLLMs), the problem of evaluating free-form responses of auto-regressive models remains a persistent challenge. Most existing works focus on language-only tasks or don't consider Multiple Choice Questions (MCQs) beyond 5-way options, both of which are critical capabilities to solve tasks in Fine-Grained Visual Classification (FGVC) where choice counts are in the hundreds to thousands and the choices are highly related. Furthermore, in this highly multi-way MCQ setting it is not clear how to extend LLM choice extraction to retrieval-based problems, where computing probabilities over the choice set is computationally costly. In this work we investigate nlg2choice, a simple two-stage method which first asks the MLLM an open-ended question for the task with minimal constraints, then uses text-only constrained decoding to predict the most likely choice. In retrieval settings, we compute the probability of the constrained response taking that choice with an early stopping method to significantly improve throughput. Our results show improvement over a suite of seven fine-grained visual datasets when evaluating in terms of classification and retrieval, and show that this performance holds over the various ways that users of LLMs can implement tasks in natural language.</li>
</ul>

<h3>Title: Secure Sparse Matrix Multiplications and their Applications to Privacy-Preserving Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Marc Damie, Florian Hahn, Andreas Peter, Jan Ramon</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14894">https://arxiv.org/abs/2510.14894</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14894">https://arxiv.org/pdf/2510.14894</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14894]] Secure Sparse Matrix Multiplications and their Applications to Privacy-Preserving Machine Learning(https://arxiv.org/abs/2510.14894)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy</a></li>
<li><strong>Abstract: </strong>To preserve privacy, multi-party computation (MPC) enables executing Machine Learning (ML) algorithms on secret-shared or encrypted data. However, existing MPC frameworks are not optimized for sparse data. This makes them unsuitable for ML applications involving sparse data, e.g., recommender systems or genomics. Even in plaintext, such applications involve high-dimensional sparse data, that cannot be processed without sparsity-related optimizations due to prohibitively large memory requirements. Since matrix multiplication is central in ML algorithms, we propose MPC algorithms to multiply secret sparse matrices. On the one hand, our algorithms avoid the memory issues of the "dense" data representation of classic secure matrix multiplication algorithms. On the other hand, our algorithms can significantly reduce communication costs (some experiments show a factor 1000) for realistic problem sizes. We validate our algorithms in two ML applications in which existing protocols are impractical. An important question when developing MPC algorithms is what assumptions can be made. In our case, if the number of non-zeros in a row is a sensitive piece of information then a short runtime may reveal that the number of non-zeros is small. Existing approaches make relatively simple assumptions, e.g., that there is a universal upper bound to the number of non-zeros in a row. This often doesn't align with statistical reality, in a lot of sparse datasets the amount of data per instance satisfies a power law. We propose an approach which allows adopting a safe upper bound on the distribution of non-zeros in rows/columns of sparse matrices.</li>
</ul>

<h3>Title: Leveraging Multimodal LLM Descriptions of Activity for Explainable Semi-Supervised Video Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Furkan Mumcu, Michael J. Jones, Anoop Cherian, Yasin Yilmaz</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14896">https://arxiv.org/abs/2510.14896</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14896">https://arxiv.org/pdf/2510.14896</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14896]] Leveraging Multimodal LLM Descriptions of Activity for Explainable Semi-Supervised Video Anomaly Detection(https://arxiv.org/abs/2510.14896)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, explainability, large language model</a></li>
<li><strong>Abstract: </strong>Existing semi-supervised video anomaly detection (VAD) methods often struggle with detecting complex anomalies involving object interactions and generally lack explainability. To overcome these limitations, we propose a novel VAD framework leveraging Multimodal Large Language Models (MLLMs). Unlike previous MLLM-based approaches that make direct anomaly judgments at the frame level, our method focuses on extracting and interpreting object activity and interactions over time. By querying an MLLM with visual inputs of object pairs at different moments, we generate textual descriptions of the activity and interactions from nominal videos. These textual descriptions serve as a high-level representation of the activity and interactions of objects in a video. They are used to detect anomalies during test time by comparing them to textual descriptions found in nominal training videos. Our approach inherently provides explainability and can be combined with many traditional VAD methods to further enhance their interpretability. Extensive experiments on benchmark datasets demonstrate that our method not only detects complex interaction-based anomalies effectively but also achieves state-of-the-art performance on datasets without interaction anomalies.</li>
</ul>

<h3>Title: Reasoning with Sampling: Your Base Model is Smarter Than You Think</h3>
<ul>
<li><strong>Authors: </strong>Aayush Karan, Yilun Du</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14901">https://arxiv.org/abs/2510.14901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14901">https://arxiv.org/pdf/2510.14901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14901]] Reasoning with Sampling: Your Base Model is Smarter Than You Think(https://arxiv.org/abs/2510.14901)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Frontier reasoning models have exhibited incredible capabilities across a wide array of disciplines, driven by posttraining large language models (LLMs) with reinforcement learning (RL). However, despite the widespread success of this paradigm, much of the literature has been devoted to disentangling truly novel behaviors that emerge during RL but are not present in the base models. In our work, we approach this question from a different angle, instead asking whether comparable reasoning capabilites can be elicited from base models at inference time by pure sampling, without any additional training. Inspired by Markov chain Monte Carlo (MCMC) techniques for sampling from sharpened distributions, we propose a simple iterative sampling algorithm leveraging the base models' own likelihoods. Over different base models, we show that our algorithm offers substantial boosts in reasoning that nearly match and even outperform those from RL on a wide variety of single-shot tasks, including MATH500, HumanEval, and GPQA. Moreover, our sampler avoids the collapse in diversity over multiple samples that is characteristic of RL-posttraining. Crucially, our method does not require training, curated datasets, or a verifier, suggesting broad applicability beyond easily verifiable domains.</li>
</ul>

<h3>Title: A Hard-Label Black-Box Evasion Attack against ML-based Malicious Traffic Detection Systems</h3>
<ul>
<li><strong>Authors: </strong>Zixuan Liu, Yi Zhao, Zhuotao Liu, Qi Li, Chuanpu Fu, Guangmeng Zhou, Ke Xu</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14906">https://arxiv.org/abs/2510.14906</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14906">https://arxiv.org/pdf/2510.14906</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14906]] A Hard-Label Black-Box Evasion Attack against ML-based Malicious Traffic Detection Systems(https://arxiv.org/abs/2510.14906)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, steal</a></li>
<li><strong>Abstract: </strong>Machine Learning (ML)-based malicious traffic detection is a promising security paradigm. It outperforms rule-based traditional detection by identifying various advanced attacks. However, the robustness of these ML models is largely unexplored, thereby allowing attackers to craft adversarial traffic examples that evade detection. Existing evasion attacks typically rely on overly restrictive conditions (e.g., encrypted protocols, Tor, or specialized setups), or require detailed prior knowledge of the target (e.g., training data and model parameters), which is impractical in realistic black-box scenarios. The feasibility of a hard-label black-box evasion attack (i.e., applicable across diverse tasks and protocols without internal target insights) thus remains an open challenge. To this end, we develop NetMasquerade, which leverages reinforcement learning (RL) to manipulate attack flows to mimic benign traffic and evade detection. Specifically, we establish a tailored pre-trained model called Traffic-BERT, utilizing a network-specialized tokenizer and an attention mechanism to extract diverse benign traffic patterns. Subsequently, we integrate Traffic-BERT into the RL framework, allowing NetMasquerade to effectively manipulate malicious packet sequences based on benign traffic patterns with minimal modifications. Experimental results demonstrate that NetMasquerade enables both brute-force and stealthy attacks to evade 6 existing detection methods under 80 attack scenarios, achieving over 96.65% attack success rate. Notably, it can evade the methods that are either empirically or certifiably robust against existing evasion attacks. Finally, NetMasquerade achieves low-latency adversarial traffic generation, demonstrating its practicality in real-world scenarios.</li>
</ul>

<h3>Title: Harmonizing Diverse Models: A Layer-wise Merging Strategy for Consistent Generation</h3>
<ul>
<li><strong>Authors: </strong>Xujun Peng, Anoop Kumar, Jingyu Wu, Parker Glenn, Daben Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14915">https://arxiv.org/abs/2510.14915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14915">https://arxiv.org/pdf/2510.14915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14915]] Harmonizing Diverse Models: A Layer-wise Merging Strategy for Consistent Generation(https://arxiv.org/abs/2510.14915)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) systems leverage Large Language Models (LLMs) to generate accurate and reliable responses that are grounded in retrieved context. However, LLMs often generate inconsistent outputs for semantically equivalent inputs, a problem compounded by the scarcity of consistency-focused training data and the limitations of current fine-tuning techniques in enhancing output consistency. We propose a new approach combining systematic synthetic data generation, triplet loss for better embeddings, and a novel layer-wise model merging approach. Using consistency-aware weights derived from intermediate layer activations, our method effectively integrates knowledge from specialized models. Experimental results how that our merged model significantly enhances output consistency, achieving a ~47.5\% improvement in response similarity over the baseline, thus offering a practical solution for increasing the reliability of an industrial RAG system.</li>
</ul>

<h3>Title: Predicting Task Performance with Context-aware Scaling Laws</h3>
<ul>
<li><strong>Authors: </strong>Kyle Montgomery, David Park, Jianhong Tu, Michael Bendersky, Beliz Gunel, Dawn Song, Chenguang Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14919">https://arxiv.org/abs/2510.14919</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14919">https://arxiv.org/pdf/2510.14919</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14919]] Predicting Task Performance with Context-aware Scaling Laws(https://arxiv.org/abs/2510.14919)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Scaling laws have transformed our understanding of large language models by linking upstream metrics like cross-entropy loss to design factors such as model size, training data, and compute. However, these conventional laws fail to capture downstream task performance, where context plays a critical role. In this work, we propose a straightforward, interpretable framework that jointly models downstream performance as a function of the training compute and the provided context. We empirically validate our framework by fitting it on the observed downstream performance of extended-context variants of Llama-2-7B and Llama-2-13B across 65,500 unique instances spanning three tasks: arithmetic reasoning, common sense reasoning, and machine translation. Our results demonstrate that our framework accurately models in-distribution downstream performance, generalizes across three orders of magnitude in training compute, and reliably extrapolates performance as the amount of context increases. These findings offer valuable insights into the interplay between training compute and context utilization, providing guidance for designing more efficient long-context LLMs for diverse downstream tasks. Our code is available at this https URL.</li>
</ul>

<h3>Title: Circuit Insights: Towards Interpretability Beyond Activations</h3>
<ul>
<li><strong>Authors: </strong>Elena Golimblevskaia, Aakriti Jain, Bruno Puri, Ammar Ibrahim, Wojciech Samek, Sebastian Lapuschkin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14936">https://arxiv.org/abs/2510.14936</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14936">https://arxiv.org/pdf/2510.14936</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14936]] Circuit Insights: Towards Interpretability Beyond Activations(https://arxiv.org/abs/2510.14936)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>The fields of explainable AI and mechanistic interpretability aim to uncover the internal structure of neural networks, with circuit discovery as a central tool for understanding model computations. Existing approaches, however, rely on manual inspection and remain limited to toy tasks. Automated interpretability offers scalability by analyzing isolated features and their activations, but it often misses interactions between features and depends strongly on external LLMs and dataset quality. Transcoders have recently made it possible to separate feature attributions into input-dependent and input-invariant components, providing a foundation for more systematic circuit analysis. Building on this, we propose WeightLens and CircuitLens, two complementary methods that go beyond activation-based analysis. WeightLens interprets features directly from their learned weights, removing the need for explainer models or datasets while matching or exceeding the performance of existing methods on context-independent features. CircuitLens captures how feature activations arise from interactions between components, revealing circuit-level dynamics that activation-only approaches cannot identify. Together, these methods increase interpretability robustness and enhance scalable mechanistic analysis of circuits while maintaining efficiency and quality.</li>
</ul>

<h3>Title: LaSeR: Reinforcement Learning with Last-Token Self-Rewarding</h3>
<ul>
<li><strong>Authors: </strong>Wenkai Yang, Weijie Liu, Ruobing Xie, Yiju Guo, Lulu Wu, Saiyong Yang, Yankai Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14943">https://arxiv.org/abs/2510.14943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14943">https://arxiv.org/pdf/2510.14943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14943]] LaSeR: Reinforcement Learning with Last-Token Self-Rewarding(https://arxiv.org/abs/2510.14943)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a core paradigm for enhancing the reasoning capabilities of Large Language Models (LLMs). To address the lack of verification signals at test time, prior studies incorporate the training of model's self-verification capability into the standard RLVR process, thereby unifying reasoning and verification capabilities within a single LLM. However, previous practice requires the LLM to sequentially generate solutions and self-verifications using two separate prompt templates, which significantly reduces efficiency. In this work, we theoretically reveal that the closed-form solution to the RL objective of self-verification can be reduced to a remarkably simple form: the true reasoning reward of a solution is equal to its last-token self-rewarding score, which is computed as the difference between the policy model's next-token log-probability assigned to any pre-specified token at the solution's last token and a pre-calculated constant, scaled by the KL coefficient. Based on this insight, we propose LaSeR (Reinforcement Learning with Last-Token Self-Rewarding), an algorithm that simply augments the original RLVR loss with a MSE loss that aligns the last-token self-rewarding scores with verifier-based reasoning rewards, jointly optimizing the reasoning and self-rewarding capabilities of LLMs. The optimized self-rewarding scores can be utilized in both training and testing to enhance model performance. Notably, our algorithm derives these scores from the predicted next-token probability distribution of the last token immediately after generation, incurring only the minimal extra cost of one additional token inference. Experiments show that our method not only improves the model's reasoning performance but also equips it with remarkable self-rewarding capability, thereby boosting its inference-time scaling performance.</li>
</ul>

<h3>Title: MetaBench: A Multi-task Benchmark for Assessing LLMs in Metabolomics</h3>
<ul>
<li><strong>Authors: </strong>Yuxing Lu, Xukai Zhao, J. Ben Tamo, Micky C. Nnamdi, Rui Peng, Shuang Zeng, Xingyu Hu, Jinzhuo Wang, May D. Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14944">https://arxiv.org/abs/2510.14944</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14944">https://arxiv.org/pdf/2510.14944</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14944]] MetaBench: A Multi-task Benchmark for Assessing LLMs in Metabolomics(https://arxiv.org/abs/2510.14944)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capabilities on general text; however, their proficiency in specialized scientific domains that require deep, interconnected knowledge remains largely uncharacterized. Metabolomics presents unique challenges with its complex biochemical pathways, heterogeneous identifier systems, and fragmented databases. To systematically evaluate LLM capabilities in this domain, we introduce MetaBench, the first benchmark for metabolomics assessment. Curated from authoritative public resources, MetaBench evaluates five capabilities essential for metabolomics research: knowledge, understanding, grounding, reasoning, and research. Our evaluation of 25 open- and closed-source LLMs reveals distinct performance patterns across metabolomics tasks: while models perform well on text generation tasks, cross-database identifier grounding remains challenging even with retrieval augmentation. Model performance also decreases on long-tail metabolites with sparse annotations. With MetaBench, we provide essential infrastructure for developing and evaluating metabolomics AI systems, enabling systematic progress toward reliable computational tools for metabolomics research.</li>
</ul>

<h3>Title: DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal Generation</h3>
<ul>
<li><strong>Authors: </strong>Yu Zhou, Sohyun An, Haikang Deng, Da Yin, Clark Peng, Cho-Jui Hsieh, Kai-Wei Chang, Nanyun Peng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14949">https://arxiv.org/abs/2510.14949</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14949">https://arxiv.org/pdf/2510.14949</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14949]] DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal Generation(https://arxiv.org/abs/2510.14949)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, generative</a></li>
<li><strong>Abstract: </strong>Contact languages like English exhibit rich regional variations in the form of dialects, which are often used by dialect speakers interacting with generative models. However, can multimodal generative models effectively produce content given dialectal textual input? In this work, we study this question by constructing a new large-scale benchmark spanning six common English dialects. We work with dialect speakers to collect and verify over 4200 unique prompts and evaluate on 17 image and video generative models. Our automatic and human evaluation results show that current state-of-the-art multimodal generative models exhibit 32.26% to 48.17% performance degradation when a single dialect word is used in the prompt. Common mitigation methods such as fine-tuning and prompt rewriting can only improve dialect performance by small margins (< 7%), while potentially incurring significant performance degradation in Standard American English (SAE). To this end, we design a general encoder-based mitigation strategy for multimodal generative models. Our method teaches the model to recognize new dialect features while preserving SAE performance. Experiments on models such as Stable Diffusion 1.5 show that our method is able to simultaneously raise performance on five dialects to be on par with SAE (+34.4%), while incurring near zero cost to SAE performance.</li>
</ul>

<h3>Title: OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression</h3>
<ul>
<li><strong>Authors: </strong>Zhe Li, Weihao Yuan, Weichao Shen, Siyu Zhu, Zilong Dong, Chang Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14954">https://arxiv.org/abs/2510.14954</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14954">https://arxiv.org/pdf/2510.14954</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14954]] OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression(https://arxiv.org/abs/2510.14954)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Whole-body multi-modal human motion generation poses two primary challenges: creating an effective motion generation mechanism and integrating various modalities, such as text, speech, and music, into a cohesive framework. Unlike previous methods that usually employ discrete masked modeling or autoregressive modeling, we develop a continuous masked autoregressive motion transformer, where a causal attention is performed considering the sequential nature within the human motion. Within this transformer, we introduce a gated linear attention and an RMSNorm module, which drive the transformer to pay attention to the key actions and suppress the instability caused by either the abnormal movements or the heterogeneous distributions within multi-modalities. To further enhance both the motion generation and the multimodal generalization, we employ the DiT structure to diffuse the conditions from the transformer towards the targets. To fuse different modalities, AdaLN and cross-attention are leveraged to inject the text, speech, and music signals. Experimental results demonstrate that our framework outperforms previous methods across all modalities, including text-to-motion, speech-to-gesture, and music-to-dance. The code of our method will be made public.</li>
</ul>

<h3>Title: RealDPO: Real or Not Real, that is the Preference</h3>
<ul>
<li><strong>Authors: </strong>Guo Cheng, Danni Yang, Ziqi Huang, Jianlou Si, Chenyang Si, Ziwei Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14955">https://arxiv.org/abs/2510.14955</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14955">https://arxiv.org/pdf/2510.14955</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14955]] RealDPO: Real or Not Real, that is the Preference(https://arxiv.org/abs/2510.14955)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Video generative models have recently achieved notable advancements in synthesis quality. However, generating complex motions remains a critical challenge, as existing models often struggle to produce natural, smooth, and contextually consistent movements. This gap between generated and real-world motions limits their practical applicability. To address this issue, we introduce RealDPO, a novel alignment paradigm that leverages real-world data as positive samples for preference learning, enabling more accurate motion synthesis. Unlike traditional supervised fine-tuning (SFT), which offers limited corrective feedback, RealDPO employs Direct Preference Optimization (DPO) with a tailored loss function to enhance motion realism. By contrasting real-world videos with erroneous model outputs, RealDPO enables iterative self-correction, progressively refining motion quality. To support post-training in complex motion synthesis, we propose RealAction-5K, a curated dataset of high-quality videos capturing human daily activities with rich and precise motion details. Extensive experiments demonstrate that RealDPO significantly improves video quality, text alignment, and motion realism compared to state-of-the-art models and existing preference optimization techniques.</li>
</ul>

<h3>Title: MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Weikang Shi, Aldrich Yu, Rongyao Fang, Houxing Ren, Ke Wang, Aojun Zhou, Changyao Tian, Xinyu Fu, Yuxuan Hu, Zimu Lu, Linjiang Huang, Si Liu, Rui Liu, Hongsheng Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14958">https://arxiv.org/abs/2510.14958</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14958">https://arxiv.org/pdf/2510.14958</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14958]] MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning(https://arxiv.org/abs/2510.14958)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>While Large Language Models (LLMs) have excelled in textual reasoning, they struggle with mathematical domains like geometry that intrinsically rely on visual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often limited by rigid external tools or fail to generate the high-fidelity, strategically-timed diagrams necessary for complex problem-solving. To bridge this gap, we introduce MathCanvas, a comprehensive framework designed to endow unified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for mathematics. Our approach consists of two phases. First, a Visual Manipulation stage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M caption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing trajectories (MathCanvas-Edit), to master diagram generation and editing. Second, a Strategic Visual-Aided Reasoning stage fine-tunes the model on MathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual reasoning paths, teaching it when and how to leverage visual aids. To facilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging benchmark with 3K problems that require models to produce interleaved visual-textual solutions. Our model, BAGEL-Canvas, trained under this framework, achieves an 86% relative improvement over strong LMM baselines on MathCanvas-Bench, demonstrating excellent generalization to other public math benchmarks. Our work provides a complete toolkit-framework, datasets, and benchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project Page: this https URL</li>
</ul>

<h3>Title: Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jonas Geiping, Xinyu Yang, Guinan Su</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14961">https://arxiv.org/abs/2510.14961</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14961">https://arxiv.org/pdf/2510.14961</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14961]] Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models(https://arxiv.org/abs/2510.14961)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Language models with recurrent depth, also referred to as universal or looped when considering transformers, are defined by the capacity to increase their computation through the repetition of layers. Recent efforts in pretraining have demonstrated that these architectures can scale to modern language modeling tasks while exhibiting advantages in reasoning tasks. In this work, we examine the relationship between recurrent-depth models and diffusion language models. Building on their similarities, we develop a new diffusion forcing sampler for these models to accelerate generation. The sampler advances by decoding new tokens at every forward pass of the model, while the latent states of these tokens can be further refined in parallel through recurrence. Theoretically, generation with our sampler is strictly more expressive than the baseline autoregressive generation using the same time budget on modern hardware. Moreover, this sampler, based on principles from diffusion literature, can be directly applied to existing 3.5B recurrent-depth transformers without any tuning, leading to up to a 5x speedup. Consequently, our findings not only provide an efficient mechanism for parallelizing the extra computation in recurrent-depth models at inference, but also suggest that such models can be naturally viewed as strong continuous, though causal, diffusion language models.</li>
</ul>

<h3>Title: RainDiff: End-to-end Precipitation Nowcasting Via Token-wise Attention Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Thao Nguyen, Jiaqi Ma, Fahad Shahbaz Khan, Souhaib Ben Taieb, Salman Khan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14962">https://arxiv.org/abs/2510.14962</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14962">https://arxiv.org/pdf/2510.14962</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14962]] RainDiff: End-to-end Precipitation Nowcasting Via Token-wise Attention Diffusion(https://arxiv.org/abs/2510.14962)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Precipitation nowcasting, predicting future radar echo sequences from current observations, is a critical yet challenging task due to the inherently chaotic and tightly coupled spatio-temporal dynamics of the atmosphere. While recent advances in diffusion-based models attempt to capture both large-scale motion and fine-grained stochastic variability, they often suffer from scalability issues: latent-space approaches require a separately trained autoencoder, adding complexity and limiting generalization, while pixel-space approaches are computationally intensive and often omit attention mechanisms, reducing their ability to model long-range spatio-temporal dependencies. To address these limitations, we propose a Token-wise Attention integrated into not only the U-Net diffusion model but also the spatio-temporal encoder that dynamically captures multi-scale spatial interactions and temporal evolution. Unlike prior approaches, our method natively integrates attention into the architecture without incurring the high resource cost typical of pixel-space diffusion, thereby eliminating the need for separate latent modules. Our extensive experiments and visual evaluations across diverse datasets demonstrate that the proposed method significantly outperforms state-of-the-art approaches, yielding superior local fidelity, generalization, and robustness in complex precipitation forecasting scenarios.</li>
</ul>

<h3>Title: Identity-Link IRT for Label-Free LLM Evaluation: Preserving Additivity in TVD-MI Scores</h3>
<ul>
<li><strong>Authors: </strong>Zachary Robertson</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14966">https://arxiv.org/abs/2510.14966</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14966">https://arxiv.org/pdf/2510.14966</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14966]] Identity-Link IRT for Label-Free LLM Evaluation: Preserving Additivity in TVD-MI Scores(https://arxiv.org/abs/2510.14966)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Pairwise comparisons of large language models using total variation distance mutual information (TVD-MI) produce binary critic decisions per pair. We show that averaging TVD-MI's binary trials yields centered-probability scores with additive structure suitable for item-response theory (IRT) without nonlinear link functions. Maximum-likelihood approaches to IRT use logistic links, but we find empirically that these transformations introduce curvature that breaks additivity: across three domains, the identity link yields median curl on raw data of 0.080-0.150 (P95 = [0.474, 0.580]), whereas probit/logit introduce substantially higher violations (median [0.245, 0.588], P95 [0.825, 2.252]). We derive this clipped-linear model from Gini entropy maximization, yielding a box-constrained least-squares formulation that handles boundary saturation. At 33% coverage, we achieve holdout RMSE $0.117 \pm 0.008$ while preserving agent rankings (Spearman $\rho = 0.972 \pm 0.015$), three times fewer evaluations than full dense. Judge robustness analysis (GPT-4o-mini vs. Llama3-70b) shows strong agreement in agent rankings ($\rho = 0.872$) and consistent identity-link advantage. TVD-MI's geometry is best preserved by identity mapping for efficient LLM evaluation, applicable to other bounded-response domains.</li>
</ul>

<h3>Title: Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents</h3>
<ul>
<li><strong>Authors: </strong>Guoqing Wang, Sunhao Dai, Guangze Ye, Zeyu Gan, Wei Yao, Yong Deng, Xiaofeng Wu, Zhenzhe Ying</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14967">https://arxiv.org/abs/2510.14967</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14967">https://arxiv.org/pdf/2510.14967</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14967]] Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents(https://arxiv.org/abs/2510.14967)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language model (LLM)-based agents are increasingly trained with reinforcement learning (RL) to enhance their ability to interact with external environments through tool use, particularly in search-based settings that require multi-turn reasoning and knowledge acquisition. However, existing approaches typically rely on outcome-based rewards that are only provided at the final answer. This reward sparsity becomes particularly problematic in multi-turn settings, where long trajectories exacerbate two critical issues: (i) advantage collapse, where all rollouts receive identical rewards and provide no useful learning signals, and (ii) lack of fine-grained credit assignment, where dependencies between turns are obscured, especially in long-horizon tasks. In this paper, we propose Information Gain-based Policy Optimization (IGPO), a simple yet effective RL framework that provides dense and intrinsic supervision for multi-turn agent training. IGPO models each interaction turn as an incremental process of acquiring information about the ground truth, and defines turn-level rewards as the marginal increase in the policy's probability of producing the correct answer. Unlike prior process-level reward approaches that depend on external reward models or costly Monte Carlo estimation, IGPO derives intrinsic rewards directly from the model's own belief updates. These intrinsic turn-level rewards are combined with outcome-level supervision to form dense reward trajectories. Extensive experiments on both in-domain and out-of-domain benchmarks demonstrate that IGPO consistently outperforms strong baselines in multi-turn scenarios, achieving higher accuracy and improved sample efficiency.</li>
</ul>

<h3>Title: LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent Training</h3>
<ul>
<li><strong>Authors: </strong>Yiming Wang, Da Yin, Yuedong Cui, Ruichen Zheng, Zhiqian Li, Zongyu Lin, Di Wu, Xueqing Wu, Chenchen Ye, Yu Zhou, Kai-Wei Chang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14969">https://arxiv.org/abs/2510.14969</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14969">https://arxiv.org/pdf/2510.14969</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14969]] LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent Training(https://arxiv.org/abs/2510.14969)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Digital agents require diverse, large-scale UI trajectories to generalize across real-world tasks, yet collecting such data is prohibitively expensive in both human annotation, infra and engineering perspectives. To this end, we introduce $\textbf{UI-Simulator}$, a scalable paradigm that generates structured UI states and transitions to synthesize training trajectories at scale. Our paradigm integrates a digital world simulator for diverse UI states, a guided rollout process for coherent exploration, and a trajectory wrapper that produces high-quality and diverse trajectories for agent training. We further propose $\textbf{UI-Simulator-Grow}$, a targeted scaling strategy that enables more rapid and data-efficient scaling by prioritizing high-impact tasks and synthesizes informative trajectory variants. Experiments on WebArena and AndroidWorld show that UI-Simulator rivals or surpasses open-source agents trained on real UIs with significantly better robustness, despite using weaker teacher models. Moreover, UI-Simulator-Grow matches the performance of Llama-3-70B-Instruct using only Llama-3-8B-Instruct as the base model, highlighting the potential of targeted synthesis scaling paradigm to continuously and efficiently enhance the digital agents.</li>
</ul>

<h3>Title: Biology-informed neural networks learn nonlinear representations from omics data to improve genomic prediction and interpretability</h3>
<ul>
<li><strong>Authors: </strong>Katiana Kontolati, Rini Jasmine Gladstone, Ian Davis, Ethan Pickering</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14970">https://arxiv.org/abs/2510.14970</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14970">https://arxiv.org/pdf/2510.14970</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14970]] Biology-informed neural networks learn nonlinear representations from omics data to improve genomic prediction and interpretability(https://arxiv.org/abs/2510.14970)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability</a></li>
<li><strong>Abstract: </strong>We extend biologically-informed neural networks (BINNs) for genomic prediction (GP) and selection (GS) in crops by integrating thousands of single-nucleotide polymorphisms (SNPs) with multi-omics measurements and prior biological knowledge. Traditional genotype-to-phenotype (G2P) models depend heavily on direct mappings that achieve only modest accuracy, forcing breeders to conduct large, costly field trials to maintain or marginally improve genetic gain. Models that incorporate intermediate molecular phenotypes such as gene expression can achieve higher predictive fit, but they remain impractical for GS since such data are unavailable at deployment or design time. BINNs overcome this limitation by encoding pathway-level inductive biases and leveraging multi-omics data only during training, while using genotype data alone during inference. Applied to maize gene-expression and multi-environment field-trial data, BINN improves rank-correlation accuracy by up to 56% within and across subpopulations under sparse-data conditions and nonlinearly identifies genes that GWAS/TWAS fail to uncover. With complete domain knowledge for a synthetic metabolomics benchmark, BINN reduces prediction error by 75% relative to conventional neural nets and correctly identifies the most important nonlinear pathway. Importantly, both cases show highly sensitive BINN latent variables correlate with the experimental quantities they represent, despite not being trained on them. This suggests BINNs learn biologically-relevant representations, nonlinear or linear, from genotype to phenotype. Together, BINNs establish a framework that leverages intermediate domain information to improve genomic prediction accuracy and reveal nonlinear biological relationships that can guide genomic selection, candidate gene selection, pathway enrichment, and gene-editing prioritization.</li>
</ul>

<h3>Title: TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar</h3>
<ul>
<li><strong>Authors: </strong>Yinxi Li, Yuntian Deng, Pengyu Nie</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.PL, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14972">https://arxiv.org/abs/2510.14972</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14972">https://arxiv.org/pdf/2510.14972</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14972]] TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar(https://arxiv.org/abs/2510.14972)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model, segmentation</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) for code rely on subword tokenizers, such as byte-pair encoding (BPE), learned from mixed natural language text and programming language code but driven by statistics rather than grammar. As a result, semantically identical code snippets can be tokenized differently depending on superficial factors such as whitespace or identifier naming. To measure the impact of this misalignment, we introduce TokDrift, a framework that applies semantic-preserving rewrite rules to create code variants differing only in tokenization. Across nine code LLMs, including large ones with over 30B parameters, even minor formatting changes can cause substantial shifts in model behavior. Layer-wise analysis shows that the issue originates in early embeddings, where subword segmentation fails to capture grammar token boundaries. Our findings identify misaligned tokenization as a hidden obstacle to reliable code understanding and generation, highlighting the need for grammar-aware tokenization for future code LLMs.</li>
</ul>

<h3>Title: Attention Is All You Need for KV Cache in Diffusion LLMs</h3>
<ul>
<li><strong>Authors: </strong>Quan Nguyen-Tri, Mukul Ranjan, Zhiqiang Shen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14973">https://arxiv.org/abs/2510.14973</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14973">https://arxiv.org/pdf/2510.14973</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14973]] Attention Is All You Need for KV Cache in Diffusion LLMs(https://arxiv.org/abs/2510.14973)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, large language model</a></li>
<li><strong>Abstract: </strong>This work studies how to adaptively recompute key-value (KV) caches for diffusion large language models (DLMs) to maximize prediction accuracy while minimizing decoding latency. Prior methods' decoders recompute QKV for all tokens at every denoising step and layer, despite KV states changing little across most steps, especially in shallow layers, leading to substantial redundancy. We make three observations: (1) distant ${\bf MASK}$ tokens primarily act as a length-bias and can be cached block-wise beyond the active prediction window; (2) KV dynamics increase with depth, suggesting that selective refresh starting from deeper layers is sufficient; and (3) the most-attended token exhibits the smallest KV drift, providing a conservative lower bound on cache change for other tokens. Building on these, we propose ${\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that jointly decides ${when}$ to refresh (via an attention-aware drift test on the most-attended token) and ${where}$ to refresh (via a depth-aware schedule that recomputes from a chosen layer onward while reusing shallow-layer caches and off-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs adaptive, layer-aware cache updates for diffusion LLMs, reducing redundant computation and accelerating decoding with negligible loss in generation quality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across mathematical reasoning and code generation tasks demonstrate consistent speedups: $8.7\times$ on GSM8K (256 tokens), $45.1\times$ on longer sequences, and $4.8\times$ on HumanEval, while consistently maintaining higher accuracy than the baseline. Our method achieves significantly higher throughput ($6.8\times$ on GSM8K) than existing confidence-based approaches while preserving generation quality, enabling practical deployment of diffusion LLMs.</li>
</ul>

<h3>Title: pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation</h3>
<ul>
<li><strong>Authors: </strong>Hansheng Chen, Kai Zhang, Hao Tan, Leonidas Guibas, Gordon Wetzstein, Sai Bi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14974">https://arxiv.org/abs/2510.14974</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14974">https://arxiv.org/pdf/2510.14974</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14974]] pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation(https://arxiv.org/abs/2510.14974)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Few-step diffusion or flow-based generative models typically distill a velocity-predicting teacher into a student that predicts a shortcut towards denoised data. This format mismatch has led to complex distillation procedures that often suffer from a quality-diversity trade-off. To address this, we propose policy-based flow models ($\pi$-Flow). $\pi$-Flow modifies the output layer of a student flow model to predict a network-free policy at one timestep. The policy then produces dynamic flow velocities at future substeps with negligible overhead, enabling fast and accurate ODE integration on these substeps without extra network evaluations. To match the policy's ODE trajectory to the teacher's, we introduce a novel imitation distillation approach, which matches the policy's velocity to the teacher's along the policy's trajectory using a standard $\ell_2$ flow matching loss. By simply mimicking the teacher's behavior, $\pi$-Flow enables stable and scalable training and avoids the quality-diversity trade-off. On ImageNet 256$^2$, it attains a 1-NFE FID of 2.85, outperforming MeanFlow of the same DiT architecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, $\pi$-Flow achieves substantially better diversity than state-of-the-art few-step methods, while maintaining teacher-level quality.</li>
</ul>

<h3>Title: WithAnyone: Towards Controllable and ID Consistent Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Hengyuan Xu, Wei Cheng, Peng Xing, Yixiao Fang, Shuhan Wu, Rui Wang, Xianfang Zeng, Daxin Jiang, Gang Yu, Xingjun Ma, Yu-Gang Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14975">https://arxiv.org/abs/2510.14975</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14975">https://arxiv.org/pdf/2510.14975</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14975]] WithAnyone: Towards Controllable and ID Consistent Image Generation(https://arxiv.org/abs/2510.14975)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Identity-consistent generation has become an important focus in text-to-image research, with recent models achieving notable success in producing images aligned with a reference identity. Yet, the scarcity of large-scale paired datasets containing multiple images of the same individual forces most approaches to adopt reconstruction-based training. This reliance often leads to a failure mode we term copy-paste, where the model directly replicates the reference face rather than preserving identity across natural variations in pose, expression, or lighting. Such over-similarity undermines controllability and limits the expressive power of generation. To address these limitations, we (1) construct a large-scale paired dataset MultiID-2M, tailored for multi-person scenarios, providing diverse references for each identity; (2) introduce a benchmark that quantifies both copy-paste artifacts and the trade-off between identity fidelity and variation; and (3) propose a novel training paradigm with a contrastive identity loss that leverages paired data to balance fidelity with diversity. These contributions culminate in WithAnyone, a diffusion-based model that effectively mitigates copy-paste while preserving high identity similarity. Extensive qualitative and quantitative experiments demonstrate that WithAnyone significantly reduces copy-paste artifacts, improves controllability over pose and expression, and maintains strong perceptual quality. User studies further validate that our method achieves high identity fidelity while enabling expressive controllable generation.</li>
</ul>

<h3>Title: Ponimator: Unfolding Interactive Pose for Versatile Human-human Interaction Animation</h3>
<ul>
<li><strong>Authors: </strong>Shaowei Liu, Chuan Guo, Bing Zhou, Jian Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14976">https://arxiv.org/abs/2510.14976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14976">https://arxiv.org/pdf/2510.14976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14976]] Ponimator: Unfolding Interactive Pose for Versatile Human-human Interaction Animation(https://arxiv.org/abs/2510.14976)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion</a></li>
<li><strong>Abstract: </strong>Close-proximity human-human interactive poses convey rich contextual information about interaction dynamics. Given such poses, humans can intuitively infer the context and anticipate possible past and future dynamics, drawing on strong priors of human behavior. Inspired by this observation, we propose Ponimator, a simple framework anchored on proximal interactive poses for versatile interaction animation. Our training data consists of close-contact two-person poses and their surrounding temporal context from motion-capture interaction datasets. Leveraging interactive pose priors, Ponimator employs two conditional diffusion models: (1) a pose animator that uses the temporal prior to generate dynamic motion sequences from interactive poses, and (2) a pose generator that applies the spatial prior to synthesize interactive poses from a single pose, text, or both when interactive poses are unavailable. Collectively, Ponimator supports diverse tasks, including image-based interaction animation, reaction animation, and text-to-interaction synthesis, facilitating the transfer of interaction knowledge from high-quality mocap data to open-world scenarios. Empirical experiments across diverse datasets and applications demonstrate the universality of the pose prior and the effectiveness and robustness of our framework.</li>
</ul>

<h3>Title: Learning an Image Editing Model without Image Editing Pairs</h3>
<ul>
<li><strong>Authors: </strong>Nupur Kumari, Sheng-Yu Wang, Nanxuan Zhao, Yotam Nitzan, Yuheng Li, Krishna Kumar Singh, Richard Zhang, Eli Shechtman, Jun-Yan Zhu, Xun Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14978">https://arxiv.org/abs/2510.14978</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14978">https://arxiv.org/pdf/2510.14978</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14978]] Learning an Image Editing Model without Image Editing Pairs(https://arxiv.org/abs/2510.14978)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent image editing models have achieved impressive results while following natural language editing instructions, but they rely on supervised fine-tuning with large datasets of input-target pairs. This is a critical bottleneck, as such naturally occurring pairs are hard to curate at scale. Current workarounds use synthetic training pairs that leverage the zero-shot capabilities of existing models. However, this can propagate and magnify the artifacts of the pretrained model into the final trained model. In this work, we present a new training paradigm that eliminates the need for paired data entirely. Our approach directly optimizes a few-step diffusion model by unrolling it during training and leveraging feedback from vision-language models (VLMs). For each input and editing instruction, the VLM evaluates if an edit follows the instruction and preserves unchanged content, providing direct gradients for end-to-end optimization. To ensure visual fidelity, we incorporate distribution matching loss (DMD), which constrains generated images to remain within the image manifold learned by pretrained models. We evaluate our method on standard benchmarks and include an extensive ablation study. Without any paired data, our method performs on par with various image editing diffusion models trained on extensive supervised paired data, under the few-step setting. Given the same VLM as the reward model, we also outperform RL-based techniques like Flow-GRPO.</li>
</ul>

<h3>Title: Coupled Diffusion Sampling for Training-Free Multi-View Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Hadi Alzayer, Yunzhi Zhang, Chen Geng, Jia-Bin Huang, Jiajun Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.14981">https://arxiv.org/abs/2510.14981</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.14981">https://arxiv.org/pdf/2510.14981</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.14981]] Coupled Diffusion Sampling for Training-Free Multi-View Image Editing(https://arxiv.org/abs/2510.14981)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present an inference-time diffusion sampling method to perform multi-view consistent image editing using pre-trained 2D image editing models. These models can independently produce high-quality edits for each image in a set of multi-view images of a 3D scene or object, but they do not maintain consistency across views. Existing approaches typically address this by optimizing over explicit 3D representations, but they suffer from a lengthy optimization process and instability under sparse view settings. We propose an implicit 3D regularization approach by constraining the generated 2D image sequences to adhere to a pre-trained multi-view image distribution. This is achieved through coupled diffusion sampling, a simple diffusion sampling technique that concurrently samples two trajectories from both a multi-view image distribution and a 2D edited image distribution, using a coupling term to enforce the multi-view consistency among the generated images. We validate the effectiveness and generality of this framework on three distinct multi-view image editing tasks, demonstrating its applicability across various model architectures and highlighting its potential as a general solution for multi-view consistent editing.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
