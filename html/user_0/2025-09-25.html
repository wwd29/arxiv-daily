<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-09-25</h1>
<h3>Title: Wavelet Fourier Diffuser: Frequency-Aware Diffusion Model for Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Yifu Luo, Yongzhe Chang, Xueqian Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19305">https://arxiv.org/abs/2509.19305</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19305">https://arxiv.org/pdf/2509.19305</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19305]] Wavelet Fourier Diffuser: Frequency-Aware Diffusion Model for Reinforcement Learning(https://arxiv.org/abs/2509.19305)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion probability models have shown significant promise in offline reinforcement learning by directly modeling trajectory sequences. However, existing approaches primarily focus on time-domain features while overlooking frequency-domain features, leading to frequency shift and degraded performance according to our observation. In this paper, we investigate the RL problem from a new perspective of the frequency domain. We first observe that time-domain-only approaches inadvertently introduce shifts in the low-frequency components of the frequency domain, which results in trajectory instability and degraded performance. To address this issue, we propose Wavelet Fourier Diffuser (WFDiffuser), a novel diffusion-based RL framework that integrates Discrete Wavelet Transform to decompose trajectories into low- and high-frequency components. To further enhance diffusion modeling for each component, WFDiffuser employs Short-Time Fourier Transform and cross attention mechanisms to extract frequency-domain features and facilitate cross-frequency interaction. Extensive experiment results on the D4RL benchmark demonstrate that WFDiffuser effectively mitigates frequency shift, leading to smoother, more stable trajectories and improved decision-making performance over existing methods.</li>
</ul>

<h3>Title: Automated Item Neutralization for Non-Cognitive Scales: A Large Language Model Approach to Reducing Social-Desirability Bias</h3>
<ul>
<li><strong>Authors: </strong>Sirui Wu, Daijin Yang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19314">https://arxiv.org/abs/2509.19314</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19314">https://arxiv.org/pdf/2509.19314</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19314]] Automated Item Neutralization for Non-Cognitive Scales: A Large Language Model Approach to Reducing Social-Desirability Bias(https://arxiv.org/abs/2509.19314)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This study evaluates item neutralization assisted by the large language model (LLM) to reduce social desirability bias in personality assessment. GPT-o3 was used to rewrite the International Personality Item Pool Big Five Measure (IPIP-BFM-50), and 203 participants completed either the original or neutralized form along with the Marlowe-Crowne Social Desirability Scale. The results showed preserved reliability and a five-factor structure, with gains in Conscientiousness and declines in Agreeableness and Openness. The correlations with social desirability decreased for several items, but inconsistently. Configural invariance held, though metric and scalar invariance failed. Findings support AI neutralization as a potential but imperfect bias-reduction method.</li>
</ul>

<h3>Title: FHIR-AgentBench: Benchmarking LLM Agents for Realistic Interoperable EHR Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Gyubok Lee, Elea Bach, Eric Yang, Tom Pollard, Alistair Johnson, Edward Choi, Yugang jia, Jong Ha Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19319">https://arxiv.org/abs/2509.19319</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19319">https://arxiv.org/pdf/2509.19319</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19319]] FHIR-AgentBench: Benchmarking LLM Agents for Realistic Interoperable EHR Question Answering(https://arxiv.org/abs/2509.19319)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The recent shift toward the Health Level Seven Fast Healthcare Interoperability Resources (HL7 FHIR) standard opens a new frontier for clinical AI, demanding LLM agents to navigate complex, resource-based data models instead of conventional structured health data. However, existing benchmarks have lagged behind this transition, lacking the realism needed to evaluate recent LLMs on interoperable clinical data. To bridge this gap, we introduce FHIR-AgentBench, a benchmark that grounds 2,931 real-world clinical questions in the HL7 FHIR standard. Using this benchmark, we systematically evaluate agentic frameworks, comparing different data retrieval strategies (direct FHIR API calls vs. specialized tools), interaction patterns (single-turn vs. multi-turn), and reasoning strategies (natural language vs. code generation). Our experiments highlight the practical challenges of retrieving data from intricate FHIR resources and the difficulty of reasoning over them, both of which critically affect question answering performance. We publicly release the FHIR-AgentBench dataset and evaluation suite (this https URL) to promote reproducible research and the development of robust, reliable LLM agents for clinical applications.</li>
</ul>

<h3>Title: Readme_AI: Dynamic Context Construction for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Millie Vyas, Timothy Blattner, Alden Dima</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19322">https://arxiv.org/abs/2509.19322</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19322">https://arxiv.org/pdf/2509.19322</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19322]] Readme_AI: Dynamic Context Construction for Large Language Models(https://arxiv.org/abs/2509.19322)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite being trained on significant amounts of data, Large Language Models (LLMs) can provide inaccurate or unreliable information in the context of a user's specific query. Given query-specific context significantly improves the usefulness of its responses. In this paper, we present a specification that can be used to dynamically build context for data sources. The data source owner creates the file containing metadata for LLMs to use when reasoning about dataset-related queries. To demonstrate our proposed specification, we created a prototype Readme_AI Model Context Protocol (MCP) server that retrieves the metadata from the data source and uses it to dynamically build context. Some features that make this specification dynamic are the extensible types that represent crawling web-pages, fetching data from data repositories, downloading and parsing publications, and general text. The context is formatted and grouped using user-specified tags that provide clear contextual information for the LLM to reason about the content. We demonstrate the capabilities of this early prototype by asking the LLM about the NIST-developed Hedgehog library, for which common LLMs often provides inaccurate and irrelevant responses containing hallucinations. With Readme_AI, the LLM receives enough context that it is now able to reason about the library and its use, and even generate code interpolated from examples that were included in the Readme_AI file provided by Hedgehog's developer. Our primary contribution is a extensible protocol for dynamically grounding LLMs in specialized, owner-provided data, enhancing responses from LLMs and reducing hallucinations. The source code for the Readme_AI tool is posted here: this https URL .</li>
</ul>

<h3>Title: Magnitude Matters: a Superior Class of Similarity Metrics for Holistic Semantic Understanding</h3>
<ul>
<li><strong>Authors: </strong>V.S. Raghu Parupudi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19323">https://arxiv.org/abs/2509.19323</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19323">https://arxiv.org/pdf/2509.19323</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19323]] Magnitude Matters: a Superior Class of Similarity Metrics for Holistic Semantic Understanding(https://arxiv.org/abs/2509.19323)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Vector comparison in high dimensions is a fundamental task in NLP, yet it is dominated by two baselines: the raw dot product, which is unbounded and sensitive to vector norms, and the cosine similarity, which discards magnitude information entirely. This paper challenges both standards by proposing and rigorously evaluating a new class of parameter-free, magnitude-aware similarity metrics. I introduce two such functions, Overlap Similarity (OS) and Hyperbolic Tangent Similarity (HTS), designed to integrate vector magnitude and alignment in a more principled manner. To ensure that my findings are robust and generalizable, I conducted a comprehensive evaluation using four state-of-the-art sentence embedding models (all-MiniLM-L6-v2, all-mpnet-base-v2, paraphrase-mpnet-base-v2, and BAAI/bge-large-en-v1.5) across a diverse suite of eight standard NLP benchmarks, including STS-B, SICK, Quora, and PAWS. Using the Wilcoxon signed-rank test for statistical significance, my results are definitive: on the tasks requiring holistic semantic understanding (paraphrase and inference), both OS and HTS provide a statistically significant improvement in Mean Squared Error over both the raw dot product and cosine similarity, regardless of the underlying embedding this http URL, my findings delineate the specific domain of advantage for these metrics: for tasks requiring holistic semantic understanding like paraphrase and inference, my magnitude-aware metrics offer a statistically superior alternative. This significant improvement was not observed on benchmarks designed to test highly nuanced compositional semantics (SICK, STS-B), identifying the challenge of representing compositional text as a distinct and important direction for future work.</li>
</ul>

<h3>Title: How Much of Your Data Can Suck? Thresholds for Domain Performance and Emergent Misalignment in LLMs</h3>
<ul>
<li><strong>Authors: </strong>Jian Ouyang, Arman T, Ge Jin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19325">https://arxiv.org/abs/2509.19325</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19325">https://arxiv.org/pdf/2509.19325</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19325]] How Much of Your Data Can Suck? Thresholds for Domain Performance and Emergent Misalignment in LLMs(https://arxiv.org/abs/2509.19325)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>This paper investigates the impact of incorrect data on the performance and safety of large language models (LLMs), specifically gpt-4o, during supervised fine-tuning (SFT). Although LLMs become increasingly vital across broad domains like finance, coding, law, and health, fine-tuning on incorrect data can lead to "emergent misalignment," producing harmful or deceptive outputs unrelated to the intended task. We evaluate gpt-4o models fine-tuned with varying ratios (10\% to 90\% correct) of both obviously and subtly incorrect data across four domains: coding, finance, health, and legal. Our findings show that even modest amounts of incorrect data (10-25\%) dramatically degrade domain performance and not moral alignment. A clear threshold of at least 50\% correct data is needed for models to consistently recover strong performance, though they rarely match the robustness and safety of the base model, which exhibits near-perfect alignment and zero dangerous completions out-of-the-box. This research emphasizes that the cost of incorrect data is heavy, highlighting the critical need for extremely high-quality data curation or, alternatively, leveraging robust base models without unnecessary fine-tuning for high-stakes applications.</li>
</ul>

<h3>Title: Unveiling the Merits and Defects of LLMs in Automatic Review Generation for Scientific Papers</h3>
<ul>
<li><strong>Authors: </strong>Ruochi Li, Haoxuan Zhang, Edward Gehringer, Ting Xiao, Junhua Ding, Haihua Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19326">https://arxiv.org/abs/2509.19326</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19326">https://arxiv.org/pdf/2509.19326</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19326]] Unveiling the Merits and Defects of LLMs in Automatic Review Generation for Scientific Papers(https://arxiv.org/abs/2509.19326)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The surge in scientific submissions has placed increasing strain on the traditional peer-review process, prompting the exploration of large language models (LLMs) for automated review generation. While LLMs demonstrate competence in producing structured and coherent feedback, their capacity for critical reasoning, contextual grounding, and quality sensitivity remains limited. To systematically evaluate these aspects, we propose a comprehensive evaluation framework that integrates semantic similarity analysis and structured knowledge graph metrics to assess LLM-generated reviews against human-written counterparts. We construct a large-scale benchmark of 1,683 papers and 6,495 expert reviews from ICLR and NeurIPS in multiple years, and generate reviews using five LLMs. Our findings show that LLMs perform well in descriptive and affirmational content, capturing the main contributions and methodologies of the original work, with GPT-4o highlighted as an illustrative example, generating 15.74% more entities than human reviewers in the strengths section of good papers in ICLR 2025. However, they consistently underperform in identifying weaknesses, raising substantive questions, and adjusting feedback based on paper quality. GPT-4o produces 59.42% fewer entities than real reviewers in the weaknesses and increases node count by only 5.7% from good to weak papers, compared to 50% in human reviews. Similar trends are observed across all conferences, years, and models, providing empirical foundations for understanding the merits and defects of LLM-generated reviews and informing the development of future LLM-assisted reviewing tools. Data, code, and more detailed results are publicly available at this https URL.</li>
</ul>

<h3>Title: A systematic review of trial-matching pipelines using large language models</h3>
<ul>
<li><strong>Authors: </strong>Braxton A. Morrison (1), Madhumita Sushil (1), Jacob S. Young (1) ((1) University of California, San Francisco)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19327">https://arxiv.org/abs/2509.19327</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19327">https://arxiv.org/pdf/2509.19327</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19327]] A systematic review of trial-matching pipelines using large language models(https://arxiv.org/abs/2509.19327)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, extraction, fair, large language model</a></li>
<li><strong>Abstract: </strong>Matching patients to clinical trial options is critical for identifying novel treatments, especially in oncology. However, manual matching is labor-intensive and error-prone, leading to recruitment delays. Pipelines incorporating large language models (LLMs) offer a promising solution. We conducted a systematic review of studies published between 2020 and 2025 from three academic databases and one preprint server, identifying LLM-based approaches to clinical trial matching. Of 126 unique articles, 31 met inclusion criteria. Reviewed studies focused on matching patient-to-criterion only (n=4), patient-to-trial only (n=10), trial-to-patient only (n=2), binary eligibility classification only (n=1) or combined tasks (n=14). Sixteen used synthetic data; fourteen used real patient data; one used both. Variability in datasets and evaluation metrics limited cross-study comparability. In studies with direct comparisons, the GPT-4 model consistently outperformed other models, even finely-tuned ones, in matching and eligibility extraction, albeit at higher cost. Promising strategies included zero-shot prompting with proprietary LLMs like the GPT-4o model, advanced retrieval methods, and fine-tuning smaller, open-source models for data privacy when incorporation of large models into hospital infrastructure is infeasible. Key challenges include accessing sufficiently large real-world data sets, and deployment-associated challenges such as reducing cost, mitigating risk of hallucinations, data leakage, and bias. This review synthesizes progress in applying LLMs to clinical trial matching, highlighting promising directions and key limitations. Standardized metrics, more realistic test sets, and attention to cost-efficiency and fairness will be critical for broader deployment.</li>
</ul>

<h3>Title: How Model Size, Temperature, and Prompt Style Affect LLM-Human Assessment Score Alignment</h3>
<ul>
<li><strong>Authors: </strong>Julie Jung, Max Lu, Sina Chole Benker, Dogus Darici</a></li>
<li><strong>Subjects: </strong>cs.CL, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19329">https://arxiv.org/abs/2509.19329</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19329">https://arxiv.org/pdf/2509.19329</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19329]] How Model Size, Temperature, and Prompt Style Affect LLM-Human Assessment Score Alignment(https://arxiv.org/abs/2509.19329)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We examined how model size, temperature, and prompt style affect Large Language Models' (LLMs) alignment within itself, between models, and with human in assessing clinical reasoning skills. Model size emerged as a key factor in LLM-human score alignment. Study highlights the importance of checking alignments across multiple levels.</li>
</ul>

<h3>Title: Quantifying Compositionality of Classic and State-of-the-Art Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Zhijin Guo (1 and 2), Chenhao Xue (1), Zhaozhen Xu (2), Hongbo Bo (2), Yuxuan Ye (2), Janet B. Pierrehumbert (1), Martha Lewis (3) ((1) University of Oxford, (2) University of Bristol, (3) University of Amsterdam)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19332">https://arxiv.org/abs/2509.19332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19332">https://arxiv.org/pdf/2509.19332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19332]] Quantifying Compositionality of Classic and State-of-the-Art Embeddings(https://arxiv.org/abs/2509.19332)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>For language models to generalize correctly to novel expressions, it is critical that they exploit access compositional meanings when this is justified. Even if we don't know what a "pelp" is, we can use our knowledge of numbers to understand that "ten pelps" makes more pelps than "two pelps". Static word embeddings such as Word2vec made strong, indeed excessive, claims about compositionality. The SOTA generative, transformer models and graph models, however, go too far in the other direction by providing no real limits on shifts in meaning due to context. To quantify the additive compositionality, we formalize a two-step, generalized evaluation that (i) measures the linearity between known entity attributes and their embeddings via canonical correlation analysis, and (ii) evaluates additive generalization by reconstructing embeddings for unseen attribute combinations and checking reconstruction metrics such as L2 loss, cosine similarity, and retrieval accuracy. These metrics also capture failure cases where linear composition breaks down. Sentences, knowledge graphs, and word embeddings are evaluated and tracked the compositionality across all layers and training stages. Stronger compositional signals are observed in later training stages across data modalities, and in deeper layers of the transformer-based model before a decline at the top layer. Code is available at this https URL.</li>
</ul>

<h3>Title: Cognitive-Level Adaptive Generation via Capability-Aware Retrieval and Style Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Qingsong Wang, Tao Wu, Wang Lin, Yueying Feng, Gongsheng Yuan, Chang Yao, Jingyuan Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19336">https://arxiv.org/abs/2509.19336</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19336">https://arxiv.org/pdf/2509.19336</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19336]] Cognitive-Level Adaptive Generation via Capability-Aware Retrieval and Style Adaptation(https://arxiv.org/abs/2509.19336)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated strong performance in open-ended generation tasks. However, they often struggle to adapt content to users with differing cognitive capacities, leading to a phenomenon we term cognitive misalignment. This issue arises in two forms: knowledge-level misalignment, where content is too complex or too simplistic relative to user understanding, and presentation-style misalignment, where the structure or tone hinders effective comprehension. To address these challenges, we propose the Cognitive-Level Alignment Framework (CLAF), a general-purpose generation framework that aligns both knowledge complexity and presentation style with user cognition. CLAF integrates a capability-aware retrieval module based on a hierarchical knowledge graph and a style optimization module guided by Bloom's taxonomy and preference learning. Additionally, a knowledge-controllable generation component ensures consistency and relevance throughout the output. To support training and evaluation, we construct SCALE, a cognitively annotated dataset containing responses at multiple comprehension levels per query. Empirical results show that CLAF enhances the adaptability and informativeness of LLM outputs across a range of user profiles, offering a robust solution to cognitive-level alignment in real-world applications.</li>
</ul>

<h3>Title: Performance of Large Language Models in Answering Critical Care Medicine Questions</h3>
<ul>
<li><strong>Authors: </strong>Mahmoud Alwakeel, Aditya Nagori, An-Kwok Ian Wong, Neal Chaisson, Vijay Krishnamoorthy, Rishikesan Kamaleswaran</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19344">https://arxiv.org/abs/2509.19344</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19344">https://arxiv.org/pdf/2509.19344</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19344]] Performance of Large Language Models in Answering Critical Care Medicine Questions(https://arxiv.org/abs/2509.19344)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models have been tested on medical student-level questions, but their performance in specialized fields like Critical Care Medicine (CCM) is less explored. This study evaluated Meta-Llama 3.1 models (8B and 70B parameters) on 871 CCM questions. Llama3.1:70B outperformed 8B by 30%, with 60% average accuracy. Performance varied across domains, highest in Research (68.4%) and lowest in Renal (47.9%), highlighting the need for broader future work to improve models across various subspecialty domains.</li>
</ul>

<h3>Title: SCORE: A Semantic Evaluation Framework for Generative Document Parsing</h3>
<ul>
<li><strong>Authors: </strong>Renyu Li, Antonio Jimeno Yepes, Yao You, Kamil Pluciński, Maximilian Operlejn, Crag Wolfe</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19345">https://arxiv.org/abs/2509.19345</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19345">https://arxiv.org/pdf/2509.19345</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19345]] SCORE: A Semantic Evaluation Framework for Generative Document Parsing(https://arxiv.org/abs/2509.19345)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, generative</a></li>
<li><strong>Abstract: </strong>Multi-modal generative document parsing systems challenge traditional evaluation: unlike deterministic OCR or layout models, they often produce semantically correct yet structurally divergent outputs. Conventional metrics-CER, WER, IoU, or TEDS-misclassify such diversity as error, penalizing valid interpretations and obscuring system behavior. We introduce SCORE (Structural and COntent Robust Evaluation), an interpretation-agnostic framework that integrates (i) adjusted edit distance for robust content fidelity, (ii) token-level diagnostics to distinguish hallucinations from omissions, (iii) table evaluation with spatial tolerance and semantic alignment, and (iv) hierarchy-aware consistency checks. Together, these dimensions enable evaluation that embraces representational diversity while enforcing semantic rigor. Across 1,114 pages spanning a holistic benchmark and a field dataset, SCORE consistently revealed cross-dataset performance patterns missed by standard metrics. In 2-5% of pages with ambiguous table structures, traditional metrics penalized systems by 12-25% on average, leading to distorted rankings. SCORE corrected these cases, recovering equivalence between alternative but valid interpretations. Moreover, by normalizing generative outputs into a format-agnostic representation, SCORE reproduces traditional scores (e.g., table F1 up to 0.93) without requiring object-detection pipelines, demonstrating that generative parsing alone suffices for comprehensive evaluation. By exposing how interpretive diversity impacts evaluation outcomes and providing multi-dimensional, interpretable diagnostics, SCORE establishes foundational principles for semantically grounded, fair, and practical benchmarking of modern document parsing systems.</li>
</ul>

<h3>Title: Benchmarking ChatGPT and DeepSeek in April 2025: A Novel Dual Perspective Sentiment Analysis Using Lexicon-Based and Deep Learning Approaches</h3>
<ul>
<li><strong>Authors: </strong>Maryam Mahdi Alhusseini, Mohammad-Reza Feizi-Derakhshi</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19346">https://arxiv.org/abs/2509.19346</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19346">https://arxiv.org/pdf/2509.19346</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19346]] Benchmarking ChatGPT and DeepSeek in April 2025: A Novel Dual Perspective Sentiment Analysis Using Lexicon-Based and Deep Learning Approaches(https://arxiv.org/abs/2509.19346)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This study presents a novel dual-perspective approach to analyzing user reviews for ChatGPT and DeepSeek on the Google Play Store, integrating lexicon-based sentiment analysis (TextBlob) with deep learning classification models, including Convolutional Neural Networks (CNN) and Bidirectional Long Short Term Memory (Bi LSTM) Networks. Unlike prior research, which focuses on either lexicon-based strategies or predictive deep learning models in isolation, this study conducts an extensive investigation into user satisfaction with Large Language Model (LLM) based applications. A Dataset of 4,000 authentic user reviews was collected, which were carefully preprocessed and subjected to oversampling to achieve balanced classes. The balanced test set of 1,700 Reviews were used for model testing. Results from the experiments reveal that ChatGPT received significantly more positive sentiment than DeepSeek. Furthermore, deep learning based classification demonstrated superior performance over lexicon analysis, with CNN outperforming Bi-LSTM by achieving 96.41 percent accuracy and near perfect classification of negative reviews, alongside high F1-scores for neutral and positive sentiments. This research sets a new methodological standard for measuring sentiment in LLM-based applications and provides practical insights for developers and researchers seeking to improve user-centric AI system design.</li>
</ul>

<h3>Title: Characterizing Knowledge Graph Tasks in LLM Benchmarks Using Cognitive Complexity Frameworks</h3>
<ul>
<li><strong>Authors: </strong>Sara Todorovikj, Lars-Peter Meyer, Michael Martin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19347">https://arxiv.org/abs/2509.19347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19347">https://arxiv.org/pdf/2509.19347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19347]] Characterizing Knowledge Graph Tasks in LLM Benchmarks Using Cognitive Complexity Frameworks(https://arxiv.org/abs/2509.19347)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly used for tasks involving Knowledge Graphs (KGs), whose evaluation typically focuses on accuracy and output correctness. We propose a complementary task characterization approach using three complexity frameworks from cognitive psychology. Applying this to the LLM-KG-Bench framework, we highlight value distributions, identify underrepresented demands and motivate richer interpretation and diversity for benchmark evaluation tasks.</li>
</ul>

<h3>Title: ShinkaEvolve: Towards Open-Ended And Sample-Efficient Program Evolution</h3>
<ul>
<li><strong>Authors: </strong>Robert Tjarko Lange, Yuki Imajuku, Edoardo Cetin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19349">https://arxiv.org/abs/2509.19349</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19349">https://arxiv.org/pdf/2509.19349</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19349]] ShinkaEvolve: Towards Open-Ended And Sample-Efficient Program Evolution(https://arxiv.org/abs/2509.19349)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce ShinkaEvolve: a new open-source framework leveraging large language models (LLMs) to advance scientific discovery with state-of-the-art performance and unprecedented efficiency. Recent advances in scaling inference time compute of LLMs have enabled significant progress in generalized scientific discovery. These approaches rely on evolutionary agentic harnesses that leverage LLMs as mutation operators to generate candidate solutions. However, current code evolution methods suffer from critical limitations: they are sample inefficient, requiring thousands of samples to identify effective solutions, and remain closed-source, hindering broad adoption and extension. ShinkaEvolve addresses these limitations, introducing three key innovations: a parent sampling technique balancing exploration and exploitation, code novelty rejection-sampling for efficient search space exploration, and a bandit-based LLM ensemble selection strategy. We evaluate ShinkaEvolve across diverse tasks, demonstrating consistent improvements in sample efficiency and solution quality. ShinkaEvolve discovers a new state-of-the-art circle packing solution using only 150 samples, designs high-performing agentic harnesses for AIME mathematical reasoning tasks, identifies improvements to ALE-Bench competitive programming solutions, and discovers novel mixture-of-expert load balancing loss functions that illuminate the space of optimization strategies. Our results demonstrate that ShinkaEvolve achieves broad applicability with exceptional sample efficiency. By providing open-source accessibility and cost-efficiency, this work democratizes open-ended discovery across diverse computational problems.</li>
</ul>

<h3>Title: RoadMind: Towards a Geospatial AI Expert for Disaster Response</h3>
<ul>
<li><strong>Authors: </strong>Ahmed El Fekih Zguir, Ferda Ofli, Muhammad Imran</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19354">https://arxiv.org/abs/2509.19354</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19354">https://arxiv.org/pdf/2509.19354</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19354]] RoadMind: Towards a Geospatial AI Expert for Disaster Response(https://arxiv.org/abs/2509.19354)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown impressive performance across a range of natural language tasks, but remain limited in their ability to reason about geospatial data, particularly road networks, distances, and directions. This gap poses challenges in disaster scenarios, where spatial understanding is critical for tasks such as evacuation planning and resource allocation. In this work, we present RoadMind, a self-supervised framework that enhances the geospatial reasoning capabilities of LLMs using structured data from OpenStreetMap (OSM). Our automated pipeline extracts road infrastructure data for a given city and converts it into multiple supervision formats tailored to key spatial tasks. We pretrain and fine-tune LLMs on these representations using QLoRA adapters and 4-bit quantized models. We evaluate our approach on three disaster-prone cities with varying global representation, Los Angeles, Christchurch, and Manila, across tasks such as road segment identification, nearest road retrieval, and distance/direction estimation. Our results show that models trained via RoadMind significantly outperform strong baselines, including state-of-the-art LLMs equipped with advanced prompt engineering. This demonstrates the potential of structured geospatial data to enhance language models with robust spatial reasoning, enabling more effective offline AI systems for disaster response.</li>
</ul>

<h3>Title: Benchmarking and Improving LLM Robustness for Personalized Generation</h3>
<ul>
<li><strong>Authors: </strong>Chimaobi Okite, Naihao Deng, Kiran Bodipati, Huaidian Hou, Joyce Chai, Rada Mihalcea</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19358">https://arxiv.org/abs/2509.19358</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19358">https://arxiv.org/pdf/2509.19358</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19358]] Benchmarking and Improving LLM Robustness for Personalized Generation(https://arxiv.org/abs/2509.19358)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, large language model</a></li>
<li><strong>Abstract: </strong>Recent years have witnessed a growing interest in personalizing the responses of large language models (LLMs). While existing evaluations primarily focus on whether a response aligns with a user's preferences, we argue that factuality is an equally important yet often overlooked dimension. In the context of personalization, we define a model as robust if its responses are both factually accurate and align with the user preferences. To assess this, we introduce PERG, a scalable framework for evaluating robustness in LLMs, along with a new dataset, PERGData. We evaluate fourteen models from five different model families using different prompting methods. Our findings show that current LLMs struggle with robust personalization: even the strongest models (GPT-4.1, LLaMA3-70B) fail to maintain correctness in 5% of previously successful cases without personalization, while smaller models (e.g., 7B-scale) can fail more than 20% of the time. Further analysis reveals that robustness is significantly affected by the nature of the query and the type of user preference. To mitigate these failures, we propose Pref-Aligner, a two-stage approach that improves robustness by an average of 25% across models. Our work highlights critical gaps in current evaluation practices and introduces tools and metrics to support more reliable, user-aligned LLM deployments.</li>
</ul>

<h3>Title: Semantic Representation Attack against Aligned Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiawei Lian, Jianhong Pan, Lefan Wang, Yi Wang, Shaohui Mei, Lap-Pui Chau</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19360">https://arxiv.org/abs/2509.19360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19360">https://arxiv.org/pdf/2509.19360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19360]] Semantic Representation Attack against Aligned Large Language Models(https://arxiv.org/abs/2509.19360)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, steal, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) increasingly employ alignment techniques to prevent harmful outputs. Despite these safeguards, attackers can circumvent them by crafting prompts that induce LLMs to generate harmful content. Current methods typically target exact affirmative responses, such as ``Sure, here is...'', suffering from limited convergence, unnatural prompts, and high computational costs. We introduce Semantic Representation Attack, a novel paradigm that fundamentally reconceptualizes adversarial objectives against aligned LLMs. Rather than targeting exact textual patterns, our approach exploits the semantic representation space comprising diverse responses with equivalent harmful meanings. This innovation resolves the inherent trade-off between attack efficacy and prompt naturalness that plagues existing methods. The Semantic Representation Heuristic Search algorithm is proposed to efficiently generate semantically coherent and concise adversarial prompts by maintaining interpretability during incremental expansion. We establish rigorous theoretical guarantees for semantic convergence and demonstrate that our method achieves unprecedented attack success rates (89.41\% averaged across 18 LLMs, including 100\% on 11 models) while maintaining stealthiness and efficiency. Comprehensive experimental results confirm the overall superiority of our Semantic Representation Attack. The code will be publicly available.</li>
</ul>

<h3>Title: DeepACTIF: Efficient Feature Attribution via Activation Traces in Neural Sequence Models</h3>
<ul>
<li><strong>Authors: </strong>Benedikt W. Hosp</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19362">https://arxiv.org/abs/2509.19362</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19362">https://arxiv.org/pdf/2509.19362</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19362]] DeepACTIF: Efficient Feature Attribution via Activation Traces in Neural Sequence Models(https://arxiv.org/abs/2509.19362)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, biometric, interpretability</a></li>
<li><strong>Abstract: </strong>Feature attribution is essential for interpreting deep learning models, particularly in time-series domains such as healthcare, biometrics, and human-AI interaction. However, standard attribution methods, such as Integrated Gradients or SHAP, are computationally intensive and not well-suited for real-time applications. We present DeepACTIF, a lightweight and architecture-aware feature attribution method that leverages internal activations of sequence models to estimate feature importance efficiently. Focusing on LSTM-based networks, we introduce an inverse-weighted aggregation scheme that emphasises stability and magnitude of activations across time steps. Our evaluation across three biometric gaze datasets shows that DeepACTIF not only preserves predictive performance under severe feature reduction (top 10% of features) but also significantly outperforms established methods, including SHAP, IG, and DeepLIFT, in terms of both accuracy and statistical robustness. Using Wilcoxon signed-rank tests and effect size analysis, we demonstrate that DeepACTIF yields more informative feature rankings with significantly lower error across all top-k conditions (10 - 40%). Our experiments demonstrate that DeepACTIF not only reduces computation time and memory usage by orders of magnitude but also preserves model accuracy when using only top-ranked features. That makes DeepACTIF a viable solution for real-time interpretability on edge devices such as mobile XR headsets or embedded health monitors.</li>
</ul>

<h3>Title: LLM-Assisted Topic Reduction for BERTopic on Social Media Data</h3>
<ul>
<li><strong>Authors: </strong>Wannes Janssens, Matthias Bogaert, Dirk Van den Poel</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19365">https://arxiv.org/abs/2509.19365</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19365">https://arxiv.org/pdf/2509.19365</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19365]] LLM-Assisted Topic Reduction for BERTopic on Social Media Data(https://arxiv.org/abs/2509.19365)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, large language model</a></li>
<li><strong>Abstract: </strong>The BERTopic framework leverages transformer embeddings and hierarchical clustering to extract latent topics from unstructured text corpora. While effective, it often struggles with social media data, which tends to be noisy and sparse, resulting in an excessive number of overlapping topics. Recent work explored the use of large language models for end-to-end topic modelling. However, these approaches typically require significant computational overhead, limiting their scalability in big data contexts. In this work, we propose a framework that combines BERTopic for topic generation with large language models for topic reduction. The method first generates an initial set of topics and constructs a representation for each. These representations are then provided as input to the language model, which iteratively identifies and merges semantically similar topics. We evaluate the approach across three Twitter/X datasets and four different language models. Our method outperforms the baseline approach in enhancing topic diversity and, in many cases, coherence, with some sensitivity to dataset characteristics and initial parameter selection.</li>
</ul>

<h3>Title: Unsupervised Outlier Detection in Audit Analytics: A Case Study Using USA Spending Data</h3>
<ul>
<li><strong>Authors: </strong>Buhe Li, Berkay Kaplan, Maksym Lazirko, Aleksandr Kogan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19366">https://arxiv.org/abs/2509.19366</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19366">https://arxiv.org/pdf/2509.19366</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19366]] Unsupervised Outlier Detection in Audit Analytics: A Case Study Using USA Spending Data(https://arxiv.org/abs/2509.19366)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This study investigates the effectiveness of unsupervised outlier detection methods in audit analytics, utilizing USA spending data from the U.S. Department of Health and Human Services (DHHS) as a case example. We employ and compare multiple outlier detection algorithms, including Histogram-based Outlier Score (HBOS), Robust Principal Component Analysis (PCA), Minimum Covariance Determinant (MCD), and K-Nearest Neighbors (KNN) to identify anomalies in federal spending patterns. The research addresses the growing need for efficient and accurate anomaly detection in large-scale governmental datasets, where traditional auditing methods may fall short. Our methodology involves data preparation, algorithm implementation, and performance evaluation using precision, recall, and F1 scores. Results indicate that a hybrid approach, combining multiple detection strategies, enhances the robustness and accuracy of outlier identification in complex financial data. This study contributes to the field of audit analytics by providing insights into the comparative effectiveness of various outlier detection models and demonstrating the potential of unsupervised learning techniques in improving audit quality and efficiency. The findings have implications for auditors, policymakers, and researchers seeking to leverage advanced analytics in governmental financial oversight and risk management.</li>
</ul>

<h3>Title: Pipeline Parallelism is All You Need for Optimized Early-Exit Based Self-Speculative Decoding</h3>
<ul>
<li><strong>Authors: </strong>Ruanjun Li, Ziheng Liu, Yuanming Shi, Jiawei Shao, Chi Zhang, Xuelong Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19368">https://arxiv.org/abs/2509.19368</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19368">https://arxiv.org/pdf/2509.19368</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19368]] Pipeline Parallelism is All You Need for Optimized Early-Exit Based Self-Speculative Decoding(https://arxiv.org/abs/2509.19368)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) deliver impressive generation quality, but incur very high inference cost because each output token is generated auto-regressively through all model layers. Early-exit based self-speculative decoding (EESD) has emerged to mitigate this cost. However, in practice, many approaches struggle to achieve the expected acceleration in such draft-then-verify paradigm even with a well-aligned early-exit head and selected exit position. Our analysis reveals that EESD only pays off when the vast majority of draft tokens are accepted by the LLM. Otherwise, the draft cost may overcome the acceleration gain and lead to a negative speedup. To mitigate this, we propose Pipeline-Parallel Self-Speculative Decoding (PPSD) that fully pipelines the draft and verification work so that no effort is wasted on failed predictions. It has two key innovations. We configure the model layers as a pipeline in which early-exit (draft) computations and remaining-layer (verification) computations overlap. We interleave drafting and verification per token. While the LLM is verifying the current token in its final layers, the early-exit path simultaneously drafts the next token. Such a verify-while-draft scheme keeps all units busy and validates tokens on-the-fly analogous to pipelining the speculation and verification stages. Empirical results confirm that PPSD achieves state-of-the-art acceleration in self-speculative LLM inference. On diverse benchmarks, PPSD achieves speedup ratios in the range of 2.01x~3.81x, which gains almost the optimal acceleration at the fixed acceptance rate and exit position, showcasing its advancement in providing efficient self-speculation.</li>
</ul>

<h3>Title: How to inject knowledge efficiently? Knowledge Infusion Scaling Law for Pre-training Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kangtao Lv, Haibin Chen, Yujin Yuan, Langming Liu, Shilei Liu, Yongwei Wang, Wenbo Su, Bo Zheng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19371">https://arxiv.org/abs/2509.19371</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19371">https://arxiv.org/pdf/2509.19371</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19371]] How to inject knowledge efficiently? Knowledge Infusion Scaling Law for Pre-training Large Language Models(https://arxiv.org/abs/2509.19371)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have attracted significant attention due to their impressive general capabilities across diverse downstream tasks. However, without domain-specific optimization, they often underperform on specialized knowledge benchmarks and even produce hallucination. Recent studies show that strategically infusing domain knowledge during pretraining can substantially improve downstream performance. A critical challenge lies in balancing this infusion trade-off: injecting too little domain-specific data yields insufficient specialization, whereas excessive infusion triggers catastrophic forgetting of previously acquired knowledge. In this work, we focus on the phenomenon of memory collapse induced by over-infusion. Through systematic experiments, we make two key observations, i.e. 1) Critical collapse point: each model exhibits a threshold beyond which its knowledge retention capabilities sharply degrade. 2) Scale correlation: these collapse points scale consistently with the model's size. Building on these insights, we propose a knowledge infusion scaling law that predicts the optimal amount of domain knowledge to inject into large LLMs by analyzing their smaller counterparts. Extensive experiments across different model sizes and pertaining token budgets validate both the effectiveness and generalizability of our scaling law.</li>
</ul>

<h3>Title: Uncertainty Quantification of Large Language Models using Approximate Bayesian Computation</h3>
<ul>
<li><strong>Authors: </strong>Mridul Sharma (1), Adeetya Patel (1), Zaneta D' Souza (1), Samira Abbasgholizadeh Rahimi (1 and 3), Siva Reddy (2 and 3), Sreenath Madathil (1) ((1) Faculty of Dental Medicine and Oral Health Sciences, McGill University, Montreal, Canada (2) School of Computer Science, McGill University, Montreal, Canada (3) Mila-Quebec Artificial Intelligence Institute, Montreal, Canada)</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19375">https://arxiv.org/abs/2509.19375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19375">https://arxiv.org/pdf/2509.19375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19375]] Uncertainty Quantification of Large Language Models using Approximate Bayesian Computation(https://arxiv.org/abs/2509.19375)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Despite their widespread applications, Large Language Models (LLMs) often struggle to express uncertainty, posing a challenge for reliable deployment in high stakes and safety critical domains like clinical diagnostics. Existing standard baseline methods such as model logits and elicited probabilities produce overconfident and poorly calibrated estimates. In this work, we propose Approximate Bayesian Computation (ABC), a likelihood-free Bayesian inference, based approach that treats LLMs as a stochastic simulator to infer posterior distributions over predictive probabilities. We evaluate our ABC approach on two clinically relevant benchmarks: a synthetic oral lesion diagnosis dataset and the publicly available GretelAI symptom-to-diagnosis dataset. Compared to standard baselines, our approach improves accuracy by up to 46.9\%, reduces Brier scores by 74.4\%, and enhances calibration as measured by Expected Calibration Error (ECE) and predictive entropy.</li>
</ul>

<h3>Title: Solving Freshness in RAG: A Simple Recency Prior and the Limits of Heuristic Trend Detection</h3>
<ul>
<li><strong>Authors: </strong>Matthew Grofsky</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19376">https://arxiv.org/abs/2509.19376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19376">https://arxiv.org/pdf/2509.19376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19376]] Solving Freshness in RAG: A Simple Recency Prior and the Limits of Heuristic Trend Detection(https://arxiv.org/abs/2509.19376)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>We address temporal failures in RAG systems using two methods on cybersecurity data. A simple recency prior achieved an accuracy of 1.00 on freshness tasks. In contrast, a clustering heuristic for topic evolution failed (0.08 F1-score), showing trend detection requires methods beyond simple heuristics.</li>
</ul>

<h3>Title: Vision-Based Perception for Autonomous Vehicles in Off-Road Environment Using Deep Learning</h3>
<ul>
<li><strong>Authors: </strong>Nelson Alves Ferreira Neto</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AR, cs.LG, eess.IV, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19378">https://arxiv.org/abs/2509.19378</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19378">https://arxiv.org/pdf/2509.19378</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19378]] Vision-Based Perception for Autonomous Vehicles in Off-Road Environment Using Deep Learning(https://arxiv.org/abs/2509.19378)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Low-latency intelligent systems are required for autonomous driving on non-uniform terrain in open-pit mines and developing countries. This work proposes a perception system for autonomous vehicles on unpaved roads and off-road environments, capable of navigating rough terrain without a predefined trail. The Configurable Modular Segmentation Network (CMSNet) framework is proposed, facilitating different architectural arrangements. CMSNet configurations were trained to segment obstacles and trafficable ground on new images from unpaved/off-road scenarios with adverse conditions (night, rain, dust). We investigated applying deep learning to detect drivable regions without explicit track boundaries, studied algorithm behavior under visibility impairment, and evaluated field tests with real-time semantic segmentation. A new dataset, Kamino, is presented with almost 12,000 images from an operating vehicle with eight synchronized cameras. The Kamino dataset has a high number of labeled pixels compared to similar public collections and includes images from an off-road proving ground emulating a mine under adverse visibility. To achieve real-time inference, CMSNet CNN layers were methodically removed and fused using TensorRT, C++, and CUDA. Empirical experiments on two datasets validated the proposed system's effectiveness.</li>
</ul>

<h3>Title: TensLoRA: Tensor Alternatives for Low-Rank Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Axel Marmoret, Reda Bensaid, Jonathan Lys, Vincent Gripon, François Leduc-Primeau</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19391">https://arxiv.org/abs/2509.19391</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19391">https://arxiv.org/pdf/2509.19391</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19391]] TensLoRA: Tensor Alternatives for Low-Rank Adaptation(https://arxiv.org/abs/2509.19391)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Low-Rank Adaptation (LoRA) is widely used to efficiently adapt Transformers by adding trainable low-rank matrices to attention projections. While effective, these matrices are considered independent for each attention projection (Query, Key, and Value) and each layer. Recent extensions have considered joint, tensor-based adaptations, but only in limited forms and without a systematic framework. We introduce TensLoRA, a unified framework that aggregates LoRA updates into higher-order tensors and models a broad family of tensor-based low-rank adaptations. Our formulation generalizes existing tensor-based methods and enables mode-specific compression rates, allowing parameter budgets to be tailored according to the modality and task. Experiments on vision and language benchmarks reveal that the tensor construction directly impacts performance, sometimes better than standard LoRA under similar parameter counts.</li>
</ul>

<h3>Title: OmniFed: A Modular Framework for Configurable Federated Learning from Edge to HPC</h3>
<ul>
<li><strong>Authors: </strong>Sahil Tyagi, Andrei Cozma, Olivera Kotevska, Feiyi Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19396">https://arxiv.org/abs/2509.19396</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19396">https://arxiv.org/pdf/2509.19396</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19396]] OmniFed: A Modular Framework for Configurable Federated Learning from Edge to HPC(https://arxiv.org/abs/2509.19396)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, privacy, federate</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) is critical for edge and High Performance Computing (HPC) where data is not centralized and privacy is crucial. We present OmniFed, a modular framework designed around decoupling and clear separation of concerns for configuration, orchestration, communication, and training logic. Its architecture supports configuration-driven prototyping and code-level override-what-you-need customization. We also support different topologies, mixed communication protocols within a single deployment, and popular training algorithms. It also offers optional privacy mechanisms including Differential Privacy (DP), Homomorphic Encryption (HE), and Secure Aggregation (SA), as well as compression strategies. These capabilities are exposed through well-defined extension points, allowing users to customize topology and orchestration, learning logic, and privacy/compression plugins, all while preserving the integrity of the core system. We evaluate multiple models and algorithms to measure various performance metrics. By unifying topology configuration, mixed-protocol communication, and pluggable modules in one stack, OmniFed streamlines FL deployment across heterogeneous environments. Github repository is available at this https URL.</li>
</ul>

<h3>Title: TimeMosaic: Temporal Heterogeneity Guided Time Series Forecasting via Adaptive Granularity Patch and Segment-wise Decoding</h3>
<ul>
<li><strong>Authors: </strong>Kuiye Ding, Fanda Fan, Chunyi Hou, Zheya Wang, Lei Wang, Zhengxin Yang, Jianfeng Zhan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19406">https://arxiv.org/abs/2509.19406</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19406">https://arxiv.org/pdf/2509.19406</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19406]] TimeMosaic: Temporal Heterogeneity Guided Time Series Forecasting via Adaptive Granularity Patch and Segment-wise Decoding(https://arxiv.org/abs/2509.19406)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Multivariate time series forecasting is essential in domains such as finance, transportation, climate, and energy. However, existing patch-based methods typically adopt fixed-length segmentation, overlooking the heterogeneity of local temporal dynamics and the decoding heterogeneity of forecasting. Such designs lose details in information-dense regions, introduce redundancy in stable segments, and fail to capture the distinct complexities of short-term and long-term horizons. We propose TimeMosaic, a forecasting framework that aims to address temporal heterogeneity. TimeMosaic employs adaptive patch embedding to dynamically adjust granularity according to local information density, balancing motif reuse with structural clarity while preserving temporal continuity. In addition, it introduces segment-wise decoding that treats each prediction horizon as a related subtask and adapts to horizon-specific difficulty and information requirements, rather than applying a single uniform decoder. Extensive evaluations on benchmark datasets demonstrate that TimeMosaic delivers consistent improvements over existing methods, and our model trained on the large-scale corpus with 321 billion observations achieves performance competitive with state-of-the-art TSFMs.</li>
</ul>

<h3>Title: Probabilistic Runtime Verification, Evaluation and Risk Assessment of Visual Deep Learning Systems</h3>
<ul>
<li><strong>Authors: </strong>Birk Torpmann-Hagen, Pål Halvorsen, Michael A. Riegler, Dag Johansen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19419">https://arxiv.org/abs/2509.19419</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19419">https://arxiv.org/pdf/2509.19419</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19419]] Probabilistic Runtime Verification, Evaluation and Risk Assessment of Visual Deep Learning Systems(https://arxiv.org/abs/2509.19419)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Despite achieving excellent performance on benchmarks, deep neural networks often underperform in real-world deployment due to sensitivity to minor, often imperceptible shifts in input data, known as distributional shifts. These shifts are common in practical scenarios but are rarely accounted for during evaluation, leading to inflated performance metrics. To address this gap, we propose a novel methodology for the verification, evaluation, and risk assessment of deep learning systems. Our approach explicitly models the incidence of distributional shifts at runtime by estimating their probability from outputs of out-of-distribution detectors. We combine these estimates with conditional probabilities of network correctness, structuring them in a binary tree. By traversing this tree, we can compute credible and precise estimates of network accuracy. We assess our approach on five different datasets, with which we simulate deployment conditions characterized by differing frequencies of distributional shift. Our approach consistently outperforms conventional evaluation, with accuracy estimation errors typically ranging between 0.01 and 0.1. We further showcase the potential of our approach on a medical segmentation benchmark, wherein we apply our methods towards risk assessment by associating costs with tree nodes, informing cost-benefit analyses and value-judgments. Ultimately, our approach offers a robust framework for improving the reliability and trustworthiness of deep learning systems, particularly in safety-critical applications, by providing more accurate performance estimates and actionable risk assessments.</li>
</ul>

<h3>Title: Transformer Modeling for Both Scalability and Performance in Multivariate Time Series</h3>
<ul>
<li><strong>Authors: </strong>Hunjae Lee, Corey Clark</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19471">https://arxiv.org/abs/2509.19471</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19471">https://arxiv.org/pdf/2509.19471</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19471]] Transformer Modeling for Both Scalability and Performance in Multivariate Time Series(https://arxiv.org/abs/2509.19471)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Variable count is among the main scalability bottlenecks for transformer modeling in multivariate time series (MTS) data. On top of this, a growing consensus in the field points to indiscriminate inter-variable mixing as a potential source of noise-accumulation and performance degradation. This is likely exacerbated by sparsity of informative signals characteristic of many MTS systems coupled with representational misalignment stemming from indiscriminate information mixing between (heterogeneous) variables. While scalability and performance are often seen as competing interests in transformer design, we show that both can be improved simultaneously in MTS by strategically constraining the representational capacity of inter-variable mixing. Our proposed method, transformer with Delegate Token Attention (DELTAformer), constrains inter-variable modeling through what we call delegate tokens which are then used to perform full, unconstrained, inter-temporal modeling. Delegate tokens act as an implicit regularizer that forces the model to be highly selective about what inter-variable information is allowed to propagate through the network. Our results show that DELTAformer scales linearly with variable-count while actually outperforming standard transformers, achieving state-of-the-art performance across benchmarks and baselines. In addition, DELTAformer can focus on relevant signals better than standard transformers in noisy MTS environments and overall exhibit superior noise-resilience. Overall, results across various experiments confirm that by aligning our model design to leverage domain-specific challenges in MTS to our advantage, DELTAformer can simultaneously achieve linear scaling while actually improving its performance against standard, quadratic transformers.</li>
</ul>

<h3>Title: Identifying and Addressing User-level Security Concerns in Smart Homes Using "Smaller" LLMs</h3>
<ul>
<li><strong>Authors: </strong>Hafijul Hoque Chowdhury, Riad Ahmed Anonto, Sourov Jajodia, Suryadipta Majumdar, Md. Shohrab Hossain</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19485">https://arxiv.org/abs/2509.19485</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19485">https://arxiv.org/pdf/2509.19485</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19485]] Identifying and Addressing User-level Security Concerns in Smart Homes Using "Smaller" LLMs(https://arxiv.org/abs/2509.19485)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, transformer</a></li>
<li><strong>Abstract: </strong>With the rapid growth of smart home IoT devices, users are increasingly exposed to various security risks, as evident from recent studies. While seeking answers to know more on those security concerns, users are mostly left with their own discretion while going through various sources, such as online blogs and technical manuals, which may render higher complexity to regular users trying to extract the necessary information. This requirement does not go along with the common mindsets of smart home users and hence threatens the security of smart homes furthermore. In this paper, we aim to identify and address the major user-level security concerns in smart homes. Specifically, we develop a novel dataset of Q&A from public forums, capturing practical security challenges faced by smart home users. We extract major security concerns in smart homes from our dataset by leveraging the Latent Dirichlet Allocation (LDA). We fine-tune relatively "smaller" transformer models, such as T5 and Flan-T5, on this dataset to build a QA system tailored for smart home security. Unlike larger models like GPT and Gemini, which are powerful but often resource hungry and require data sharing, smaller models are more feasible for deployment in resource-constrained or privacy-sensitive environments like smart homes. The dataset is manually curated and supplemented with synthetic data to explore its potential impact on model performance. This approach significantly improves the system's ability to deliver accurate and relevant answers, helping users address common security concerns with smart home IoT devices. Our experiments on real-world user concerns show that our work improves the performance of the base models.</li>
</ul>

<h3>Title: Frame-based Equivariant Diffusion Models for 3D Molecular Generation</h3>
<ul>
<li><strong>Authors: </strong>Mohan Guo (Faculty of Science, University of Amsterdam), Cong Liu (AMLab, University of Amsterdam), Patrick Forré (AMLab, University of Amsterdam)</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19506">https://arxiv.org/abs/2509.19506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19506">https://arxiv.org/pdf/2509.19506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19506]] Frame-based Equivariant Diffusion Models for 3D Molecular Generation(https://arxiv.org/abs/2509.19506)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Recent methods for molecular generation face a trade-off: they either enforce strict equivariance with costly architectures or relax it to gain scalability and flexibility. We propose a frame-based diffusion paradigm that achieves deterministic E(3)-equivariance while decoupling symmetry handling from the backbone. Building on this paradigm, we investigate three variants: Global Frame Diffusion (GFD), which assigns a shared molecular frame; Local Frame Diffusion (LFD), which constructs node-specific frames and benefits from additional alignment constraints; and Invariant Frame Diffusion (IFD), which relies on pre-canonicalized invariant representations. To enhance expressivity, we further utilize EdgeDiT, a Diffusion Transformer with edge-aware attention. On the QM9 dataset, GFD with EdgeDiT achieves state-of-the-art performance, with a test NLL of -137.97 at standard scale and -141.85 at double scale, alongside atom stability of 98.98%, and molecular stability of 90.51%. These results surpass all equivariant baselines while maintaining high validity and uniqueness and nearly 2x faster sampling compared to EDM. Altogether, our study establishes frame-based diffusion as a scalable, flexible, and physically grounded paradigm for molecular generation, highlighting the critical role of global structure preservation.</li>
</ul>

<h3>Title: DAWM: Diffusion Action World Models for Offline Reinforcement Learning via Action-Inferred Transitions</h3>
<ul>
<li><strong>Authors: </strong>Zongyue Li, Xiao Han, Yusong Li, Niklas Strauss, Matthias Schubert</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19538">https://arxiv.org/abs/2509.19538</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19538">https://arxiv.org/pdf/2509.19538</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19538]] DAWM: Diffusion Action World Models for Offline Reinforcement Learning via Action-Inferred Transitions(https://arxiv.org/abs/2509.19538)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion-based world models have demonstrated strong capabilities in synthesizing realistic long-horizon trajectories for offline reinforcement learning (RL). However, many existing methods do not directly generate actions alongside states and rewards, limiting their compatibility with standard value-based offline RL algorithms that rely on one-step temporal difference (TD) learning. While prior work has explored joint modeling of states, rewards, and actions to address this issue, such formulations often lead to increased training complexity and reduced performance in practice. We propose \textbf{DAWM}, a diffusion-based world model that generates future state-reward trajectories conditioned on the current state, action, and return-to-go, paired with an inverse dynamics model (IDM) for efficient action inference. This modular design produces complete synthetic transitions suitable for one-step TD-based offline RL, enabling effective and computationally efficient training. Empirically, we show that conservative offline RL algorithms such as TD3BC and IQL benefit significantly from training on these augmented trajectories, consistently outperforming prior diffusion-based baselines across multiple tasks in the D4RL benchmark.</li>
</ul>

<h3>Title: Do LLMs Encode Frame Semantics? Evidence from Frame Identification</h3>
<ul>
<li><strong>Authors: </strong>Jayanth Krishna Chundru, Rudrashis Poddar, Jie Cao, Tianyu Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19540">https://arxiv.org/abs/2509.19540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19540">https://arxiv.org/pdf/2509.19540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19540]] Do LLMs Encode Frame Semantics? Evidence from Frame Identification(https://arxiv.org/abs/2509.19540)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We investigate whether large language models encode latent knowledge of frame semantics, focusing on frame identification, a core challenge in frame semantic parsing that involves selecting the appropriate semantic frame for a target word in context. Using the FrameNet lexical resource, we evaluate models under prompt-based inference and observe that they can perform frame identification effectively even without explicit supervision. To assess the impact of task-specific training, we fine-tune the model on FrameNet data, which substantially improves in-domain accuracy while generalizing well to out-of-domain benchmarks. Further analysis shows that the models can generate semantically coherent frame definitions, highlighting the model's internalized understanding of frame semantics.</li>
</ul>

<h3>Title: iFinder: Structured Zero-Shot Vision-Based LLM Grounding for Dash-Cam Video Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Manyi Yao, Bingbing Zhuang, Sparsh Garg, Amit Roy-Chowdhury, Christian Shelton, Manmohan Chandraker, Abhishek Aich</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19552">https://arxiv.org/abs/2509.19552</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19552">https://arxiv.org/pdf/2509.19552</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19552]] iFinder: Structured Zero-Shot Vision-Based LLM Grounding for Dash-Cam Video Reasoning(https://arxiv.org/abs/2509.19552)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability, large language model</a></li>
<li><strong>Abstract: </strong>Grounding large language models (LLMs) in domain-specific tasks like post-hoc dash-cam driving video analysis is challenging due to their general-purpose training and lack of structured inductive biases. As vision is often the sole modality available for such analysis (i.e., no LiDAR, GPS, etc.), existing video-based vision-language models (V-VLMs) struggle with spatial reasoning, causal inference, and explainability of events in the input video. To this end, we introduce iFinder, a structured semantic grounding framework that decouples perception from reasoning by translating dash-cam videos into a hierarchical, interpretable data structure for LLMs. iFinder operates as a modular, training-free pipeline that employs pretrained vision models to extract critical cues -- object pose, lane positions, and object trajectories -- which are hierarchically organized into frame- and video-level structures. Combined with a three-block prompting strategy, it enables step-wise, grounded reasoning for the LLM to refine a peer V-VLM's outputs and provide accurate reasoning. Evaluations on four public dash-cam video benchmarks show that iFinder's proposed grounding with domain-specific cues, especially object orientation and global context, significantly outperforms end-to-end V-VLMs on four zero-shot driving benchmarks, with up to 39% gains in accident reasoning accuracy. By grounding LLMs with driving domain-specific representations, iFinder offers a zero-shot, interpretable, and reliable alternative to end-to-end V-VLMs for post-hoc driving video understanding.</li>
</ul>

<h3>Title: Confidence Calibration in Large Language Model-Based Entity Matching</h3>
<ul>
<li><strong>Authors: </strong>Iris Kamsteeg, Juan Cardenas-Cartagena, Floris van Beers, Gineke ten Holt, Tsegaye Misikir Tashu, Matias Valdenegro-Toro</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19557">https://arxiv.org/abs/2509.19557</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19557">https://arxiv.org/pdf/2509.19557</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19557]] Confidence Calibration in Large Language Model-Based Entity Matching(https://arxiv.org/abs/2509.19557)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This research aims to explore the intersection of Large Language Models and confidence calibration in Entity Matching. To this end, we perform an empirical study to compare baseline RoBERTa confidences for an Entity Matching task against confidences that are calibrated using Temperature Scaling, Monte Carlo Dropout and Ensembles. We use the Abt-Buy, DBLP-ACM, iTunes-Amazon and Company datasets. The findings indicate that the proposed modified RoBERTa model exhibits a slight overconfidence, with Expected Calibration Error scores ranging from 0.0043 to 0.0552 across datasets. We find that this overconfidence can be mitigated using Temperature Scaling, reducing Expected Calibration Error scores by up to 23.83%.</li>
</ul>

<h3>Title: CURE: Centroid-guided Unsupervised Representation Erasure for Facial Recognition Systems</h3>
<ul>
<li><strong>Authors: </strong>Fnu Shivam, Nima Najafzadeh, Yenumula Reddy, Prashnna Gyawali</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19562">https://arxiv.org/abs/2509.19562</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19562">https://arxiv.org/pdf/2509.19562</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19562]] CURE: Centroid-guided Unsupervised Representation Erasure for Facial Recognition Systems(https://arxiv.org/abs/2509.19562)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>In the current digital era, facial recognition systems offer significant utility and have been widely integrated into modern technological infrastructures; however, their widespread use has also raised serious privacy concerns, prompting regulations that mandate data removal upon request. Machine unlearning has emerged as a powerful solution to address this issue by selectively removing the influence of specific user data from trained models while preserving overall model performance. However, existing machine unlearning techniques largely depend on supervised techniques requiring identity labels, which are often unavailable in privacy-constrained situations or in large-scale, noisy datasets. To address this critical gap, we introduce CURE (Centroid-guided Unsupervised Representation Erasure), the first unsupervised unlearning framework for facial recognition systems that operates without the use of identity labels, effectively removing targeted samples while preserving overall performance. We also propose a novel metric, the Unlearning Efficiency Score (UES), which balances forgetting and retention stability, addressing shortcomings in the current evaluation metrics. CURE significantly outperforms unsupervised variants of existing unlearning methods. Additionally, we conducted quality-aware unlearning by designating low-quality images as the forget set, demonstrating its usability and benefits, and highlighting the role of image quality in machine unlearning.</li>
</ul>

<h3>Title: Uncertainty in Semantic Language Modeling with PIXELS</h3>
<ul>
<li><strong>Authors: </strong>Stefania Radu, Marco Zullich, Matias Valdenegro-Toro</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19563">https://arxiv.org/abs/2509.19563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19563">https://arxiv.org/pdf/2509.19563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19563]] Uncertainty in Semantic Language Modeling with PIXELS(https://arxiv.org/abs/2509.19563)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Pixel-based language models aim to solve the vocabulary bottleneck problem in language modeling, but the challenge of uncertainty quantification remains open. The novelty of this work consists of analysing uncertainty and confidence in pixel-based language models across 18 languages and 7 scripts, all part of 3 semantically challenging tasks. This is achieved through several methods such as Monte Carlo Dropout, Transformer Attention, and Ensemble Learning. The results suggest that pixel-based models underestimate uncertainty when reconstructing patches. The uncertainty is also influenced by the script, with Latin languages displaying lower uncertainty. The findings on ensemble learning show better performance when applying hyperparameter tuning during the named entity recognition and question-answering tasks across 16 languages.</li>
</ul>

<h3>Title: Retrieval Augmented Generation based context discovery for ASR</h3>
<ul>
<li><strong>Authors: </strong>Dimitrios Siskos, Stavros Papadopoulos, Pablo Peso Parada, Jisi Zhang, Karthikeyan Saravanan, Anastasios Drosou</a></li>
<li><strong>Subjects: </strong>cs.CL, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19567">https://arxiv.org/abs/2509.19567</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19567">https://arxiv.org/pdf/2509.19567</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19567]] Retrieval Augmented Generation based context discovery for ASR(https://arxiv.org/abs/2509.19567)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This work investigates retrieval augmented generation as an efficient strategy for automatic context discovery in context-aware Automatic Speech Recognition (ASR) system, in order to improve transcription accuracy in the presence of rare or out-of-vocabulary terms. However, identifying the right context automatically remains an open challenge. This work proposes an efficient embedding-based retrieval approach for automatic context discovery in ASR. To contextualize its effectiveness, two alternatives based on large language models (LLMs) are also evaluated: (1) large language model (LLM)-based context generation via prompting, and (2) post-recognition transcript correction using LLMs. Experiments on the TED-LIUMv3, Earnings21 and SPGISpeech demonstrate that the proposed approach reduces WER by up to 17% (percentage difference) relative to using no-context, while the oracle context results in a reduction of up to 24.1%.</li>
</ul>

<h3>Title: Knock-Knock: Black-Box, Platform-Agnostic DRAM Address-Mapping Reverse Engineering</h3>
<ul>
<li><strong>Authors: </strong>Antoine Plin, Lorenzo Casalino, Thomas Rokicki, Ruben Salvador</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19568">https://arxiv.org/abs/2509.19568</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19568">https://arxiv.org/pdf/2509.19568</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19568]] Knock-Knock: Black-Box, Platform-Agnostic DRAM Address-Mapping Reverse Engineering(https://arxiv.org/abs/2509.19568)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>Modern Systems-on-Chip (SoCs) employ undocumented linear address-scrambling functions to obfuscate DRAM addressing, which complicates DRAM-aware performance optimizations and hinders proactive security analysis of DRAM-based attacks; most notably, Rowhammer. Although previous work tackled the issue of reversing physical-to-DRAM mapping, existing heuristic-based reverse-engineering approaches are partial, costly, and impractical for comprehensive recovery. This paper establishes a rigorous theoretical foundation and provides efficient practical algorithms for black-box, complete physical-to-DRAM address-mapping recovery. We first formulate the reverse-engineering problem within a linear algebraic model over the finite field GF(2). We characterize the timing fingerprints of row-buffer conflicts, proving a relationship between a bank addressing matrix and an empirically constructed matrix of physical addresses. Based on this characterization, we develop an efficient, noise-robust, and fully platform-agnostic algorithm to recover the full bank-mask basis in polynomial time, a significant improvement over the exponential search from previous works. We further generalize our model to complex row mappings, introducing new hardware-based hypotheses that enable the automatic recovery of a row basis instead of previous human-guided contributions. Evaluations across embedded and server-class architectures confirm our method's effectiveness, successfully reconstructing known mappings and uncovering previously unknown scrambling functions. Our method provides a 99% recall and accuracy on all tested platforms. Most notably, Knock-Knock runs in under a few minutes, even on systems with more than 500GB of DRAM, showcasing the scalability of our method. Our approach provides an automated, principled pathway to accurate DRAM reverse engineering.</li>
</ul>

<h3>Title: ExPe: Exact Positional Encodings for Generative Transformer Models with Extrapolating Capabilities</h3>
<ul>
<li><strong>Authors: </strong>Aleksis Datseris, Sylvia Vassileva, Ivan Koychev, Svetla Boytcheva</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19569">https://arxiv.org/abs/2509.19569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19569">https://arxiv.org/pdf/2509.19569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19569]] ExPe: Exact Positional Encodings for Generative Transformer Models with Extrapolating Capabilities(https://arxiv.org/abs/2509.19569)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer, generative</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel approach to position embeddings in transformer models, named "Exact Positional Embeddings" (ExPE). An absolute positional embedding method that can extrapolate to sequences of lengths longer than the ones it was trained on. Traditional transformer models rely on absolute or relative position embeddings to incorporate positional information into token embeddings, which often struggle with extrapolation to sequences longer than those seen during training. Our proposed method utilizes a novel embedding strategy that encodes exact positional information by overriding specific dimensions of the embedding vectors, thereby enabling a more precise representation of token positions. The proposed approach not only maintains the integrity of the original embeddings but also enhances the model's ability to generalize to more extended sequences. In causal language modeling, our ExPE embeddings significantly reduce perplexity compared to rotary and sinusoidal embeddings, when tested on sequences longer than those used in training.</li>
</ul>

<h3>Title: LLMs4All: A Review on Large Language Models for Research and Applications in Academic Disciplines</h3>
<ul>
<li><strong>Authors: </strong>Yanfang (Fanny)Ye, Zheyuan Zhang, Tianyi Ma, Zehong Wang, Yiyang Li, Shifu Hou, Weixiang Sun, Kaiwen Shi, Yijun Ma, Wei Song, Ahmed Abbasi, Ying Cheng, Jane Cleland-Huang, Steven Corcelli, Patricia Culligan, Robert Goulding, Ming Hu, Ting Hua, John Lalor, Fang Liu, Tengfei Luo, Ed Maginn, Nuno Moniz, Jason Rohr, Brett Savoie, Daniel Slate, Tom Stapleford, Matthew Webber, Olaf Wiest, Johnny Zhang, Nitesh Chawla</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19580">https://arxiv.org/abs/2509.19580</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19580">https://arxiv.org/pdf/2509.19580</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19580]] LLMs4All: A Review on Large Language Models for Research and Applications in Academic Disciplines(https://arxiv.org/abs/2509.19580)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>Cutting-edge Artificial Intelligence (AI) techniques keep reshaping our view of the world. For example, Large Language Models (LLMs) based applications such as ChatGPT have shown the capability of generating human-like conversation on extensive topics. Due to the impressive performance on a variety of language-related tasks (e.g., open-domain question answering, translation, and document summarization), one can envision the far-reaching impacts that can be brought by the LLMs with broader real-world applications (e.g., customer service, education and accessibility, and scientific discovery). Inspired by their success, this paper will offer an overview of state-of-the-art LLMs and their integration into a wide range of academic disciplines, including: (1) arts, letters, and law (e.g., history, philosophy, political science, arts and architecture, law), (2) economics and business (e.g., finance, economics, accounting, marketing), and (3) science and engineering (e.g., mathematics, physics and mechanical engineering, chemistry and chemical engineering, life sciences and bioengineering, earth sciences and civil engineering, computer science and electrical engineering). Integrating humanity and technology, in this paper, we will explore how LLMs are shaping research and practice in these fields, while also discussing key limitations, open challenges, and future directions in the era of generative AI. The review of how LLMs are engaged across disciplines-along with key observations and insights-can help researchers and practitioners interested in exploiting LLMs to advance their works in diverse real-world applications.</li>
</ul>

<h3>Title: Synthesizing Artifact Dataset for Pixel-level Detection</h3>
<ul>
<li><strong>Authors: </strong>Dennis Menn, Feng Liang, Diana Marculescu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19589">https://arxiv.org/abs/2509.19589</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19589">https://arxiv.org/pdf/2509.19589</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19589]] Synthesizing Artifact Dataset for Pixel-level Detection(https://arxiv.org/abs/2509.19589)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Artifact detectors have been shown to enhance the performance of image-generative models by serving as reward models during fine-tuning. These detectors enable the generative model to improve overall output fidelity and aesthetics. However, training the artifact detector requires expensive pixel-level human annotations that specify the artifact regions. The lack of annotated data limits the performance of the artifact detector. A naive pseudo-labeling approach-training a weak detector and using it to annotate unlabeled images-suffers from noisy labels, resulting in poor performance. To address this, we propose an artifact corruption pipeline that automatically injects artifacts into clean, high-quality synthetic images on a predetermined region, thereby producing pixel-level annotations without manual labeling. The proposed method enables training of an artifact detector that achieves performance improvements of 13.2% for ConvNeXt and 3.7% for Swin-T, as verified on human-labeled data, compared to baseline approaches. This work represents an initial step toward scalable pixel-level artifact annotation datasets that integrate world knowledge into artifact detection.</li>
</ul>

<h3>Title: GuessingGame: Measuring the Informativeness of Open-Ended Questions in Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Dylan Hutson, Daniel Vennemeyer, Aneesh Deshmukh, Justin Zhan, Tianyu Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19593">https://arxiv.org/abs/2509.19593</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19593">https://arxiv.org/pdf/2509.19593</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19593]] GuessingGame: Measuring the Informativeness of Open-Ended Questions in Large Language Models(https://arxiv.org/abs/2509.19593)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We introduce GuessingGame, a protocol for evaluating large language models (LLMs) as strategic question-askers in open-ended, open-domain settings. A Guesser LLM identifies a hidden object by posing free-form questions to an Oracle without predefined choices or candidate lists. To measure question quality, we propose two information gain (IG) metrics: a Bayesian method that tracks belief updates over semantic concepts using LLM-scored relevance, and an entropy-based method that filters candidates via ConceptNet. Both metrics are model-agnostic and support post hoc analysis. Across 858 games with multiple models and prompting strategies, higher IG strongly predicts efficiency: a one-standard-deviation IG increase reduces expected game length by 43\%. Prompting constraints guided by IG, such as enforcing question diversity, enable weaker models to significantly improve performance. These results show that question-asking in LLMs is both measurable and improvable, and crucial for interactive reasoning.</li>
</ul>

<h3>Title: Parameter-Efficient Multi-Task Learning via Progressive Task-Specific Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Neeraj Gangwar, Anshuka Rangi, Rishabh Deshmukh, Holakou Rahmanian, Yesh Dattatreya, Nickvash Kani</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19602">https://arxiv.org/abs/2509.19602</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19602">https://arxiv.org/pdf/2509.19602</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19602]] Parameter-Efficient Multi-Task Learning via Progressive Task-Specific Adaptation(https://arxiv.org/abs/2509.19602)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Parameter-efficient fine-tuning methods have emerged as a promising solution for adapting pre-trained models to various downstream tasks. While these methods perform well in single-task learning, extending them to multi-task learning exacerbates common challenges, such as task interference and negative transfer, due to the limited number of trainable parameters. To address these issues, we introduce progressive task-specific multi-task adaptation, a novel parameter-efficient approach for multi-task learning. This approach introduces adapter modules in a pre-trained model such that these modules are shared across all tasks in the initial layers and become progressively more task-specific in the later layers. The motivation is to reduce the conflicts among tasks by allowing transfer learning across all tasks in the initial layers and enabling task-specific learning toward the prediction heads. Additionally, we propose a gradient-based approach for computing task similarity and use this measure to allocate similar tasks to the shared adapter modules. Our task similarity method introduces minimal overhead in the pipeline. We evaluate our approach by adapting the Swin Transformer for dense prediction tasks. Experiments on the PASCAL and NYUD-v2 datasets demonstrate that our approach outperforms a fully fine-tuned multi-task model while requiring only one-fifth of the trainable parameters. This approach achieves better relative improvement to single-task fine-tuning while reducing the number of trainable parameters and surpasses the current state-of-the-art methods for parameter-efficient multi-task learning.</li>
</ul>

<h3>Title: Mamba Modulation: On the Length Generalization of Mamba</h3>
<ul>
<li><strong>Authors: </strong>Peng Lu, Jerry Huang, Qiuhao Zeng, Xinyu Wang, Boxing Wang, Philippe Langlais, Yufei Cui</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19633">https://arxiv.org/abs/2509.19633</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19633">https://arxiv.org/pdf/2509.19633</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19633]] Mamba Modulation: On the Length Generalization of Mamba(https://arxiv.org/abs/2509.19633)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>The quadratic complexity of the attention mechanism in Transformer models has motivated the development of alternative architectures with sub-quadratic scaling, such as state-space models. Among these, Mamba has emerged as a leading architecture, achieving state-of-the-art results across a range of language modeling tasks. However, Mamba's performance significantly deteriorates when applied to contexts longer than those seen during pre-training, revealing a sharp sensitivity to context length extension. Through detailed analysis, we attribute this limitation to the out-of-distribution behaviour of its state-space dynamics, particularly within the parameterization of the state transition matrix $\mathbf{A}$. Unlike recent works which attribute this sensitivity to the vanished accumulation of discretization time steps, $\exp(-\sum_{t=1}^N\Delta_t)$, we establish a connection between state convergence behavior as the input length approaches infinity and the spectrum of the transition matrix $\mathbf{A}$, offering a well-founded explanation of its role in length extension. Next, to overcome this challenge, we propose an approach that applies spectrum scaling to pre-trained Mamba models to enable robust long-context generalization by selectively modulating the spectrum of $\mathbf{A}$ matrices in each layer. We show that this can significantly improve performance in settings where simply modulating $\Delta_t$ fails, validating our insights and providing avenues for better length generalization of state-space models with structured transition matrices.</li>
</ul>

<h3>Title: TIMED: Adversarial and Autoregressive Refinement of Diffusion-Based Time Series Generation</h3>
<ul>
<li><strong>Authors: </strong>MohammadReza EskandariNasab, Shah Muhammad Hamdi, Soukaina Filali Boubrahimi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19638">https://arxiv.org/abs/2509.19638</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19638">https://arxiv.org/pdf/2509.19638</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19638]] TIMED: Adversarial and Autoregressive Refinement of Diffusion-Based Time Series Generation(https://arxiv.org/abs/2509.19638)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generating high-quality synthetic time series is a fundamental yet challenging task across domains such as forecasting and anomaly detection, where real data can be scarce, noisy, or costly to collect. Unlike static data generation, synthesizing time series requires modeling both the marginal distribution of observations and the conditional temporal dependencies that govern sequential dynamics. We propose TIMED, a unified generative framework that integrates a denoising diffusion probabilistic model (DDPM) to capture global structure via a forward-reverse diffusion process, a supervisor network trained with teacher forcing to learn autoregressive dependencies through next-step prediction, and a Wasserstein critic that provides adversarial feedback to ensure temporal smoothness and fidelity. To further align the real and synthetic distributions in feature space, TIMED incorporates a Maximum Mean Discrepancy (MMD) loss, promoting both diversity and sample quality. All components are built using masked attention architectures optimized for sequence modeling and are trained jointly to effectively capture both unconditional and conditional aspects of time series data. Experimental results across diverse multivariate time series benchmarks demonstrate that TIMED generates more realistic and temporally coherent sequences than state-of-the-art generative models.</li>
</ul>

<h3>Title: AutoSpec: An Agentic Framework for Automatically Drafting Patent Specification</h3>
<ul>
<li><strong>Authors: </strong>Ryan Shea, Zhou Yu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19640">https://arxiv.org/abs/2509.19640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19640">https://arxiv.org/pdf/2509.19640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19640]] AutoSpec: An Agentic Framework for Automatically Drafting Patent Specification(https://arxiv.org/abs/2509.19640)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, robust</a></li>
<li><strong>Abstract: </strong>Patents play a critical role in driving technological innovation by granting inventors exclusive rights to their inventions. However the process of drafting a patent application is often expensive and time-consuming, making it a prime candidate for automation. Despite recent advancements in language models, several challenges hinder the development of robust automated patent drafting systems. First, the information within a patent application is highly confidential, which often prevents the use of closed-source LLMs for automating this task. Second, the process of drafting a patent application is difficult for even the most advanced language models due to their long context, technical writing style, and specialized domain knowledge. To address these challenges, we introduce AutoSpec, a secure, agentic framework for Automatically drafting patent Specification. Our approach decomposes the drafting process into a sequence of manageable subtasks, each solvable by smaller, open-source language models enhanced with custom tools tailored for drafting patent specification. To assess our system, we design a novel evaluation protocol in collaboration with experienced patent attorneys. Our automatic and expert evaluations show that AutoSpec outperforms existing baselines on a patent drafting task.</li>
</ul>

<h3>Title: The Impact of 2D Segmentation Backbones on Point Cloud Predictions Using 4D Radar</h3>
<ul>
<li><strong>Authors: </strong>William L. Muckelroy III, Mohammed Alsakabi, John M. Dolan, Ozan K. Tonguz</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19644">https://arxiv.org/abs/2509.19644</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19644">https://arxiv.org/pdf/2509.19644</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19644]] The Impact of 2D Segmentation Backbones on Point Cloud Predictions Using 4D Radar(https://arxiv.org/abs/2509.19644)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>LiDAR's dense, sharp point cloud (PC) representations of the surrounding environment enable accurate perception and significantly improve road safety by offering greater scene awareness and understanding. However, LiDAR's high cost continues to restrict the broad adoption of high-level Autonomous Driving (AD) systems in commercially available vehicles. Prior research has shown progress towards circumventing the need for LiDAR by training a neural network, using LiDAR point clouds as ground truth (GT), to produce LiDAR-like 3D point clouds using only 4D Radars. One of the best examples is a neural network created to train a more efficient radar target detector with a modular 2D convolutional neural network (CNN) backbone and a temporal coherence network at its core that uses the RaDelft dataset for training (see arXiv:2406.04723). In this work, we investigate the impact of higher-capacity segmentation backbones on the quality of the produced point clouds. Our results show that while very high-capacity models may actually hurt performance, an optimal segmentation backbone can provide a 23.7% improvement over the state-of-the-art (SOTA).</li>
</ul>

<h3>Title: SoK: A Systematic Review of Malware Ontologies and Taxonomies and Implications for the Quantum Era</h3>
<ul>
<li><strong>Authors: </strong>Dehinde Molade, Dave Ormrod, Mamello Thinyane, Nalin Arachchilage, Jill Slay</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19650">https://arxiv.org/abs/2509.19650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19650">https://arxiv.org/pdf/2509.19650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19650]] SoK: A Systematic Review of Malware Ontologies and Taxonomies and Implications for the Quantum Era(https://arxiv.org/abs/2509.19650)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, protect, attack</a></li>
<li><strong>Abstract: </strong>The threat of quantum malware is real and a growing security concern that will have catastrophic scientific and technological impacts, if not addressed early. If weaponised or exploited especially by the wrong hands, malware will undermine highly sophisticated critical systems supported by next-generation quantum architectures, for example, in defence, communications, energy, and space. This paper explores the fundamental nature and implications of quantum malware to enable the future development of appropriate mitigations and defences, thereby protecting critical infrastructure. By conducting a systematic literature review (SLR) that draws on knowledge frameworks such as ontologies and taxonomies to explore malware, this provides insights into how malicious behaviours can be translated into attacks on quantum technologies, thereby providing a lens to analyse the severity of malware against quantum technologies. This study employs the European Competency Framework for Quantum Technologies (CFQT) as a guide to map malware behaviour to several competency layers, creating a foundation in this emerging field.</li>
</ul>

<h3>Title: Symbol-Temporal Consistency Self-supervised Learning for Robust Time Series Classification</h3>
<ul>
<li><strong>Authors: </strong>Kevin Garcia, Cassandra Garza, Brooklyn Berry, Yifeng Gao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19654">https://arxiv.org/abs/2509.19654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19654">https://arxiv.org/pdf/2509.19654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19654]] Symbol-Temporal Consistency Self-supervised Learning for Robust Time Series Classification(https://arxiv.org/abs/2509.19654)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>The surge in the significance of time series in digital health domains necessitates advanced methodologies for extracting meaningful patterns and representations. Self-supervised contrastive learning has emerged as a promising approach for learning directly from raw data. However, time series data in digital health is known to be highly noisy, inherently involves concept drifting, and poses a challenge for training a generalizable deep learning model. In this paper, we specifically focus on data distribution shift caused by different human behaviors and propose a self-supervised learning framework that is aware of the bag-of-symbol representation. The bag-of-symbol representation is known for its insensitivity to data warping, location shifts, and noise existed in time series data, making it potentially pivotal in guiding deep learning to acquire a representation resistant to such data shifting. We demonstrate that the proposed method can achieve significantly better performance where significant data shifting exists.</li>
</ul>

<h3>Title: Large Language Models for Pedestrian Safety: An Application to Predicting Driver Yielding Behavior at Unsignalized Intersections</h3>
<ul>
<li><strong>Authors: </strong>Yicheng Yang, Zixian Li, Jean Paul Bizimana, Niaz Zafri, Yongfeng Dong, Tianyi Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19657">https://arxiv.org/abs/2509.19657</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19657">https://arxiv.org/pdf/2509.19657</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19657]] Large Language Models for Pedestrian Safety: An Application to Predicting Driver Yielding Behavior at Unsignalized Intersections(https://arxiv.org/abs/2509.19657)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Pedestrian safety is a critical component of urban mobility and is strongly influenced by the interactions between pedestrian decision-making and driver yielding behavior at crosswalks. Modeling driver--pedestrian interactions at intersections requires accurately capturing the complexity of these behaviors. Traditional machine learning models often struggle to capture the nuanced and context-dependent reasoning required for these multifactorial interactions, due to their reliance on fixed feature representations and limited interpretability. In contrast, large language models (LLMs) are suited for extracting patterns from heterogeneous traffic data, enabling accurate modeling of driver-pedestrian interactions. Therefore, this paper leverages multimodal LLMs through a novel prompt design that incorporates domain-specific knowledge, structured reasoning, and few-shot prompting, enabling interpretable and context-aware inference of driver yielding behavior, as an example application of modeling pedestrian--driver interaction. We benchmarked state-of-the-art LLMs against traditional classifiers, finding that GPT-4o consistently achieves the highest accuracy and recall, while Deepseek-V3 excels in precision. These findings highlight the critical trade-offs between model performance and computational efficiency, offering practical guidance for deploying LLMs in real-world pedestrian safety systems.</li>
</ul>

<h3>Title: Bias in the Picture: Benchmarking VLMs with Social-Cue News Images and LLM-as-Judge Assessment</h3>
<ul>
<li><strong>Authors: </strong>Aravind Narayanan, Vahid Reza Khazaie, Shaina Raza</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19659">https://arxiv.org/abs/2509.19659</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19659">https://arxiv.org/pdf/2509.19659</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19659]] Bias in the Picture: Benchmarking VLMs with Social-Cue News Images and LLM-as-Judge Assessment(https://arxiv.org/abs/2509.19659)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, large language model</a></li>
<li><strong>Abstract: </strong>Large vision-language models (VLMs) can jointly interpret images and text, but they are also prone to absorbing and reproducing harmful social stereotypes when visual cues such as age, gender, race, clothing, or occupation are present. To investigate these risks, we introduce a news-image benchmark consisting of 1,343 image-question pairs drawn from diverse outlets, which we annotated with ground-truth answers and demographic attributes (age, gender, race, occupation, and sports). We evaluate a range of state-of-the-art VLMs and employ a large language model (LLM) as judge, with human verification. Our findings show that: (i) visual context systematically shifts model outputs in open-ended settings; (ii) bias prevalence varies across attributes and models, with particularly high risk for gender and occupation; and (iii) higher faithfulness does not necessarily correspond to lower bias. We release the benchmark prompts, evaluation rubric, and code to support reproducible and fairness-aware multimodal assessment.</li>
</ul>

<h3>Title: Consistent Estimation of Numerical Distributions under Local Differential Privacy by Wavelet Expansion</h3>
<ul>
<li><strong>Authors: </strong>Puning Zhao, Zhikun Zhang, Bo Sun, Li Shen, Liang Zhang, Shaowei Wang, Zhe Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19661">https://arxiv.org/abs/2509.19661</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19661">https://arxiv.org/pdf/2509.19661</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19661]] Consistent Estimation of Numerical Distributions under Local Differential Privacy by Wavelet Expansion(https://arxiv.org/abs/2509.19661)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Distribution estimation under local differential privacy (LDP) is a fundamental and challenging task. Significant progresses have been made on categorical data. However, due to different evaluation metrics, these methods do not work well when transferred to numerical data. In particular, we need to prevent the probability mass from being misplaced far away. In this paper, we propose a new approach that express the sample distribution using wavelet expansions. The coefficients of wavelet series are estimated under LDP. Our method prioritizes the estimation of low-order coefficients, in order to ensure accurate estimation at macroscopic level. Therefore, the probability mass is prevented from being misplaced too far away from its ground truth. We establish theoretical guarantees for our methods. Experiments show that our wavelet expansion method significantly outperforms existing solutions under Wasserstein and KS distances.</li>
</ul>

<h3>Title: MoTiC: Momentum Tightness and Contrast for Few-Shot Class-Incremental Learning</h3>
<ul>
<li><strong>Authors: </strong>Zeyu He, Shuai Huang, Yuwu Lu, Ming Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19664">https://arxiv.org/abs/2509.19664</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19664">https://arxiv.org/pdf/2509.19664</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19664]] MoTiC: Momentum Tightness and Contrast for Few-Shot Class-Incremental Learning(https://arxiv.org/abs/2509.19664)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Few-Shot Class-Incremental Learning (FSCIL) must contend with the dual challenge of learning new classes from scarce samples while preserving old class knowledge. Existing methods use the frozen feature extractor and class-averaged prototypes to mitigate against catastrophic forgetting and overfitting. However, new-class prototypes suffer significant estimation bias due to extreme data scarcity, whereas base-class prototypes benefit from sufficient data. In this work, we theoretically demonstrate that aligning the new-class priors with old-class statistics via Bayesian analysis reduces variance and improves prototype accuracy. Furthermore, we propose large-scale contrastive learning to enforce cross-category feature tightness. To further enrich feature diversity and inject prior information for new-class prototypes, we integrate momentum self-supervision and virtual categories into the Momentum Tightness and Contrast framework (MoTiC), constructing a feature space with rich representations and enhanced interclass cohesion. Experiments on three FSCIL benchmarks produce state-of-the-art performances, particularly on the fine-grained task CUB-200, validating our method's ability to reduce estimation bias and improve incremental learning robustness.</li>
</ul>

<h3>Title: Deep Learning for Clouds and Cloud Shadow Segmentation in Methane Satellite and Airborne Imaging Spectroscopy</h3>
<ul>
<li><strong>Authors: </strong>Manuel Perez-Carrasco, Maya Nasr, Sebastien Roche, Chris Chan Miller, Zhan Zhang, Core Francisco Park, Eleanor Walker, Cecilia Garraffo, Douglas Finkbeiner, Ritesh Gautam, Steven Wofsy</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19665">https://arxiv.org/abs/2509.19665</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19665">https://arxiv.org/pdf/2509.19665</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19665]] Deep Learning for Clouds and Cloud Shadow Segmentation in Methane Satellite and Airborne Imaging Spectroscopy(https://arxiv.org/abs/2509.19665)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Effective cloud and cloud shadow detection is a critical prerequisite for accurate retrieval of concentrations of atmospheric methane or other trace gases in hyperspectral remote sensing. This challenge is especially pertinent for MethaneSAT and for its airborne companion mission, MethaneAIR. In this study, we use machine learning methods to address the cloud and cloud shadow detection problem for sensors with these high spatial resolutions instruments. Cloud and cloud shadows in remote sensing data need to be effectively screened out as they bias methane retrievals in remote sensing imagery and impact the quantification of emissions. We deploy and evaluate conventional techniques including Iterative Logistic Regression (ILR) and Multilayer Perceptron (MLP), with advanced deep learning architectures, namely UNet and a Spectral Channel Attention Network (SCAN) method. Our results show that conventional methods struggle with spatial coherence and boundary definition, affecting the detection of clouds and cloud shadows. Deep learning models substantially improve detection quality: UNet performs best in preserving spatial structure, while SCAN excels at capturing fine boundary details. Notably, SCAN surpasses UNet on MethaneSAT data, underscoring the benefits of incorporating spectral attention for satellite specific features. This in depth assessment of various disparate machine learning techniques demonstrates the strengths and effectiveness of advanced deep learning architectures in providing robust, scalable solutions for clouds and cloud shadow screening towards enhancing methane emission quantification capacity of existing and next generation hyperspectral missions. Our data and code is publicly available at this https URL</li>
</ul>

<h3>Title: C${}^2$Prompt: Class-aware Client Knowledge Interaction for Federated Continual Learning</h3>
<ul>
<li><strong>Authors: </strong>Kunlun Xu, Yibo Feng, Jiangmeng Li, Yongsheng Qi, Jiahuan Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19674">https://arxiv.org/abs/2509.19674</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19674">https://arxiv.org/pdf/2509.19674</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19674]] C${}^2$Prompt: Class-aware Client Knowledge Interaction for Federated Continual Learning(https://arxiv.org/abs/2509.19674)</code><input type="text"></li>
<li><strong>Keywords: </strong>federate</a></li>
<li><strong>Abstract: </strong>Federated continual learning (FCL) tackles scenarios of learning from continuously emerging task data across distributed clients, where the key challenge lies in addressing both temporal forgetting over time and spatial forgetting simultaneously. Recently, prompt-based FCL methods have shown advanced performance through task-wise prompt this http URL this study, we underscore that the existing prompt-based FCL methods are prone to class-wise knowledge coherence between prompts across clients. The class-wise knowledge coherence includes two aspects: (1) intra-class distribution gap across clients, which degrades the learned semantics across prompts, (2) inter-prompt class-wise relevance, which highlights cross-class knowledge confusion. During prompt communication, insufficient class-wise coherence exacerbates knowledge conflicts among new prompts and induces interference with old prompts, intensifying both spatial and temporal forgetting. To address these issues, we propose a novel Class-aware Client Knowledge Interaction (C${}^2$Prompt) method that explicitly enhances class-wise knowledge coherence during prompt communication. Specifically, a local class distribution compensation mechanism (LCDC) is introduced to reduce intra-class distribution disparities across clients, thereby reinforcing intra-class knowledge consistency. Additionally, a class-aware prompt aggregation scheme (CPA) is designed to alleviate inter-class knowledge confusion by selectively strengthening class-relevant knowledge aggregation. Extensive experiments on multiple FCL benchmarks demonstrate that C${}^2$Prompt achieves state-of-the-art performance. Our source code is available at this https URL</li>
</ul>

<h3>Title: Unmasking Fake Careers: Detecting Machine-Generated Career Trajectories via Multi-layer Heterogeneous Graphs</h3>
<ul>
<li><strong>Authors: </strong>Michiharu Yamashita, Thanh Tran, Delvin Ce Zhang, Dongwon Lee</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19677">https://arxiv.org/abs/2509.19677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19677">https://arxiv.org/pdf/2509.19677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19677]] Unmasking Fake Careers: Detecting Machine-Generated Career Trajectories via Multi-layer Heterogeneous Graphs(https://arxiv.org/abs/2509.19677)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The rapid advancement of Large Language Models (LLMs) has enabled the generation of highly realistic synthetic data. We identify a new vulnerability, LLMs generating convincing career trajectories in fake resumes and explore effective detection methods. To address this challenge, we construct a dataset of machine-generated career trajectories using LLMs and various methods, and demonstrate that conventional text-based detectors perform poorly on structured career data. We propose CareerScape, a novel heterogeneous, hierarchical multi-layer graph framework that models career entities and their relations in a unified global graph built from genuine resumes. Unlike conventional classifiers that treat each instance independently, CareerScape employs a structure-aware framework that augments user-specific subgraphs with trusted neighborhood information from a global graph, enabling the model to capture both global structural patterns and local inconsistencies indicative of synthetic career paths. Experimental results show that CareerScape outperforms state-of-the-art baselines by 5.8-85.0% relatively, highlighting the importance of structure-aware detection for machine-generated content.</li>
</ul>

<h3>Title: Enhancing Transformer-Based Vision Models: Addressing Feature Map Anomalies Through Novel Optimization Strategies</h3>
<ul>
<li><strong>Authors: </strong>Sumit Mamtani</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19687">https://arxiv.org/abs/2509.19687</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19687">https://arxiv.org/pdf/2509.19687</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19687]] Enhancing Transformer-Based Vision Models: Addressing Feature Map Anomalies Through Novel Optimization Strategies(https://arxiv.org/abs/2509.19687)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Vision Transformers (ViTs) have demonstrated superior performance across a wide range of computer vision tasks. However, structured noise artifacts in their feature maps hinder downstream applications such as segmentation and depth estimation. We propose two novel and lightweight optimisation techniques- Structured Token Augmentation (STA) and Adaptive Noise Filtering (ANF)- to improve interpretability and mitigate these artefacts. STA enhances token diversity through spatial perturbations during tokenisation, while ANF applies learnable inline denoising between transformer layers. These methods are architecture-agnostic and evaluated across standard benchmarks, including ImageNet, Ade20k, and NYUv2. Experimental results show consistent improvements in visual quality and task performance, highlighting the practical effectiveness of our approach.</li>
</ul>

<h3>Title: From Prompt to Progression: Taming Video Diffusion Models for Seamless Attribute Transition</h3>
<ul>
<li><strong>Authors: </strong>Ling Lo, Kelvin C.K. Chan, Wen-Huang Cheng, Ming-Hsuan Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19690">https://arxiv.org/abs/2509.19690</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19690">https://arxiv.org/pdf/2509.19690</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19690]] From Prompt to Progression: Taming Video Diffusion Models for Seamless Attribute Transition(https://arxiv.org/abs/2509.19690)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Existing models often struggle with complex temporal changes, particularly when generating videos with gradual attribute transitions. The most common prompt interpolation approach for motion transitions often fails to handle gradual attribute transitions, where inconsistencies tend to become more pronounced. In this work, we propose a simple yet effective method to extend existing models for smooth and consistent attribute transitions, through introducing frame-wise guidance during the denoising process. Our approach constructs a data-specific transitional direction for each noisy latent, guiding the gradual shift from initial to final attributes frame by frame while preserving the motion dynamics of the video. Moreover, we present the Controlled-Attribute-Transition Benchmark (CAT-Bench), which integrates both attribute and motion dynamics, to comprehensively evaluate the performance of different models. We further propose two metrics to assess the accuracy and smoothness of attribute transitions. Experimental results demonstrate that our approach performs favorably against existing baselines, achieving visual fidelity, maintaining alignment with text prompts, and delivering seamless attribute transitions. Code and CATBench are released: this https URL.</li>
</ul>

<h3>Title: Anatomically Constrained Transformers for Cardiac Amyloidosis Classification</h3>
<ul>
<li><strong>Authors: </strong>Alexander Thorley, Agis Chartsias, Jordan Strom, Roberto Lang, Jeremy Slivnick, Jamie O'Driscoll, Rajan Sharma, Dipak Kotecha, Jinming Duan, Alberto Gomez</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19691">https://arxiv.org/abs/2509.19691</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19691">https://arxiv.org/pdf/2509.19691</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19691]] Anatomically Constrained Transformers for Cardiac Amyloidosis Classification(https://arxiv.org/abs/2509.19691)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Cardiac amyloidosis (CA) is a rare cardiomyopathy, with typical abnormalities in clinical measurements from echocardiograms such as reduced global longitudinal strain of the myocardium. An alternative approach for detecting CA is via neural networks, using video classification models such as convolutional neural networks. These models process entire video clips, but provide no assurance that classification is based on clinically relevant features known to be associated with CA. An alternative paradigm for disease classification is to apply models to quantitative features such as strain, ensuring that the classification relates to clinically relevant features. Drawing inspiration from this approach, we explicitly constrain a transformer model to the anatomical region where many known CA abnormalities occur -- the myocardium, which we embed as a set of deforming points and corresponding sampled image patches into input tokens. We show that our anatomical constraint can also be applied to the popular self-supervised learning masked autoencoder pre-training, where we propose to mask and reconstruct only anatomical patches. We show that by constraining both the transformer and pre-training task to the myocardium where CA imaging features are localized, we achieve increased performance on a CA classification task compared to full video transformers. Our model provides an explicit guarantee that the classification is focused on only anatomical regions of the echo, and enables us to visualize transformer attention scores over the deforming myocardium.</li>
</ul>

<h3>Title: Linear Transformers Implicitly Discover Unified Numerical Algorithms</h3>
<ul>
<li><strong>Authors: </strong>Patrick Lutz, Aditya Gangrade, Hadi Daneshmand, Venkatesh Saligrama</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19702">https://arxiv.org/abs/2509.19702</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19702">https://arxiv.org/pdf/2509.19702</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19702]] Linear Transformers Implicitly Discover Unified Numerical Algorithms(https://arxiv.org/abs/2509.19702)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>We train a linear attention transformer on millions of masked-block matrix completion tasks: each prompt is masked low-rank matrix whose missing block may be (i) a scalar prediction target or (ii) an unseen kernel slice of Nyström extrapolation. The model sees only input-output pairs and a mean-squared loss; it is given no normal equations, no handcrafted iterations, and no hint that the tasks are related. Surprisingly, after training, algebraic unrolling reveals the same parameter-free update rule across three distinct computational regimes (full visibility, rank-limited updates, and distributed computation). We prove that this rule achieves second-order convergence on full-batch problems, cuts distributed iteration complexity, and remains accurate with rank-limited attention. Thus, a transformer trained solely to patch missing blocks implicitly discovers a unified, resource-adaptive iterative solver spanning prediction, estimation, and Nyström extrapolation, highlighting a powerful capability of in-context learning.</li>
</ul>

<h3>Title: Causal Machine Learning for Surgical Interventions</h3>
<ul>
<li><strong>Authors: </strong>J. Ben Tamo, Nishant S. Chouhan, Micky C. Nnamdi, Yining Yuan, Shreya S. Chivilkar, Wenqi Shi, Steven W. Hwang, B. Randall Brenn, May D. Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.AP, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19705">https://arxiv.org/abs/2509.19705</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19705">https://arxiv.org/pdf/2509.19705</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19705]] Causal Machine Learning for Surgical Interventions(https://arxiv.org/abs/2509.19705)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Surgical decision-making is complex and requires understanding causal relationships between patient characteristics, interventions, and outcomes. In high-stakes settings like spinal fusion or scoliosis correction, accurate estimation of individualized treatment effects (ITEs) remains limited due to the reliance on traditional statistical methods that struggle with complex, heterogeneous data. In this study, we develop a multi-task meta-learning framework, X-MultiTask, for ITE estimation that models each surgical decision (e.g., anterior vs. posterior approach, surgery vs. no surgery) as a distinct task while learning shared representations across tasks. To strengthen causal validity, we incorporate the inverse probability weighting (IPW) into the training objective. We evaluate our approach on two datasets: (1) a public spinal fusion dataset (1,017 patients) to assess the effect of anterior vs. posterior approaches on complication severity; and (2) a private AIS dataset (368 patients) to analyze the impact of posterior spinal fusion (PSF) vs. non-surgical management on patient-reported outcomes (PROs). Our model achieves the highest average AUC (0.84) in the anterior group and maintains competitive performance in the posterior group (0.77). It outperforms baselines in treatment effect estimation with the lowest overall $\epsilon_{\text{NN-PEHE}}$ (0.2778) and $\epsilon_{\text{ATE}}$ (0.0763). Similarly, when predicting PROs in AIS, X-MultiTask consistently shows superior performance across all domains, with $\epsilon_{\text{NN-PEHE}}$ = 0.2551 and $\epsilon_{\text{ATE}}$ = 0.0902. By providing robust, patient-specific causal estimates, X-MultiTask offers a powerful tool to advance personalized surgical care and improve patient outcomes. The code is available at this https URL.</li>
</ul>

<h3>Title: Towards Robust In-Context Learning for Medical Image Segmentation via Data Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Jiesi Hu, Yanwu Yang, Zhiyu Ye, Chenfei Ye, Hanyang Peng, Jianfeng Cao, Ting Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19711">https://arxiv.org/abs/2509.19711</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19711">https://arxiv.org/pdf/2509.19711</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19711]] Towards Robust In-Context Learning for Medical Image Segmentation via Data Synthesis(https://arxiv.org/abs/2509.19711)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>The rise of In-Context Learning (ICL) for universal medical image segmentation has introduced an unprecedented demand for large-scale, diverse datasets for training, exacerbating the long-standing problem of data scarcity. While data synthesis offers a promising solution, existing methods often fail to simultaneously achieve both high data diversity and a domain distribution suitable for medical data. To bridge this gap, we propose \textbf{SynthICL}, a novel data synthesis framework built upon domain randomization. SynthICL ensures realism by leveraging anatomical priors from real-world datasets, generates diverse anatomical structures to cover a broad data distribution, and explicitly models inter-subject variations to create data cohorts suitable for ICL. Extensive experiments on four held-out datasets validate our framework's effectiveness, showing that models trained with our data achieve performance gains of up to 63\% in average Dice and substantially enhanced generalization to unseen anatomical domains. Our work helps mitigate the data bottleneck for ICL-based segmentation, paving the way for robust models. Our code and the generated dataset are publicly available at this https URL.</li>
</ul>

<h3>Title: VIMD: Monocular Visual-Inertial Motion and Depth Estimation</h3>
<ul>
<li><strong>Authors: </strong>Saimouli Katragadda, Guoquan Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19713">https://arxiv.org/abs/2509.19713</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19713">https://arxiv.org/pdf/2509.19713</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19713]] VIMD: Monocular Visual-Inertial Motion and Depth Estimation(https://arxiv.org/abs/2509.19713)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate and efficient dense metric depth estimation is crucial for 3D visual perception in robotics and XR. In this paper, we develop a monocular visual-inertial motion and depth (VIMD) learning framework to estimate dense metric depth by leveraging accurate and efficient MSCKF-based monocular visual-inertial motion tracking. At the core the proposed VIMD is to exploit multi-view information to iteratively refine per-pixel scale, instead of globally fitting an invariant affine model as in the prior work. The VIMD framework is highly modular, making it compatible with a variety of existing depth estimation backbones. We conduct extensive evaluations on the TartanAir and VOID datasets and demonstrate its zero-shot generalization capabilities on the AR Table dataset. Our results show that VIMD achieves exceptional accuracy and robustness, even with extremely sparse points as few as 10-20 metric depth points per image. This makes the proposed VIMD a practical solution for deployment in resource constrained settings, while its robust performance and strong generalization capabilities offer significant potential across a wide range of scenarios.</li>
</ul>

<h3>Title: Frequency-domain Multi-modal Fusion for Language-guided Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Bo Yu, Jianhua Yang, Zetao Du, Yan Huang, Chenglong Li, Liang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19719">https://arxiv.org/abs/2509.19719</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19719">https://arxiv.org/pdf/2509.19719</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19719]] Frequency-domain Multi-modal Fusion for Language-guided Medical Image Segmentation(https://arxiv.org/abs/2509.19719)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Automatically segmenting infected areas in radiological images is essential for diagnosing pulmonary infectious diseases. Recent studies have demonstrated that the accuracy of the medical image segmentation can be improved by incorporating clinical text reports as semantic guidance. However, the complex morphological changes of lesions and the inherent semantic gap between vision-language modalities prevent existing methods from effectively enhancing the representation of visual features and eliminating semantically irrelevant information, ultimately resulting in suboptimal segmentation performance. To address these problems, we propose a Frequency-domain Multi-modal Interaction model (FMISeg) for language-guided medical image segmentation. FMISeg is a late fusion model that establishes interaction between linguistic features and frequency-domain visual features in the decoder. Specifically, to enhance the visual representation, our method introduces a Frequency-domain Feature Bidirectional Interaction (FFBI) module to effectively fuse frequency-domain features. Furthermore, a Language-guided Frequency-domain Feature Interaction (LFFI) module is incorporated within the decoder to suppress semantically irrelevant visual features under the guidance of linguistic information. Experiments on QaTa-COV19 and MosMedData+ demonstrated that our method outperforms the state-of-the-art methods qualitatively and quantitatively.</li>
</ul>

<h3>Title: Personality Vector: Modulating Personality of Large Language Models by Model Merging</h3>
<ul>
<li><strong>Authors: </strong>Seungjong Sun, Seo Yeon Baek, Jang Hyun Kim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19727">https://arxiv.org/abs/2509.19727</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19727">https://arxiv.org/pdf/2509.19727</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19727]] Personality Vector: Modulating Personality of Large Language Models by Model Merging(https://arxiv.org/abs/2509.19727)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Driven by the demand for personalized AI systems, there is growing interest in aligning the behavior of large language models (LLMs) with human traits such as personality. Previous attempts to induce personality in LLMs have shown promising results, but they struggle to capture the continuous and multidimensional nature of human traits. In this work, we propose a novel method for personality modulation in LLMs via model merging. Specifically, we construct personality vectors by subtracting the weights of a pre-trained model from those of the fine-tuned model on a given personality trait. By merging personality vectors, we enable LLMs to exhibit desired personality traits without additional training. Extensive experiments show that personality vectors enable continuous control over trait intensity and support the composition of multiple traits. Furthermore, personality vectors transfer across diverse downstream models, suggesting that they encode generalizable representations of personality. Our code is available at here.</li>
</ul>

<h3>Title: Robust RGB-T Tracking via Learnable Visual Fourier Prompt Fine-tuning and Modality Fusion Prompt Generation</h3>
<ul>
<li><strong>Authors: </strong>Hongtao Yang, Bineng Zhong, Qihua Liang, Zhiruo Zhu, Yaozong Zheng, Ning Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19733">https://arxiv.org/abs/2509.19733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19733">https://arxiv.org/pdf/2509.19733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19733]] Robust RGB-T Tracking via Learnable Visual Fourier Prompt Fine-tuning and Modality Fusion Prompt Generation(https://arxiv.org/abs/2509.19733)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction</a></li>
<li><strong>Abstract: </strong>Recently, visual prompt tuning is introduced to RGB-Thermal (RGB-T) tracking as a parameter-efficient finetuning (PEFT) method. However, these PEFT-based RGB-T tracking methods typically rely solely on spatial domain information as prompts for feature extraction. As a result, they often fail to achieve optimal performance by overlooking the crucial role of frequency-domain information in prompt learning. To address this issue, we propose an efficient Visual Fourier Prompt Tracking (named VFPTrack) method to learn modality-related prompts via Fast Fourier Transform (FFT). Our method consists of symmetric feature extraction encoder with shared parameters, visual fourier prompts, and Modality Fusion Prompt Generator that generates bidirectional interaction prompts through multi-modal feature fusion. Specifically, we first use a frozen feature extraction encoder to extract RGB and thermal infrared (TIR) modality features. Then, we combine the visual prompts in the spatial domain with the frequency domain prompts obtained from the FFT, which allows for the full extraction and understanding of modality features from different domain information. Finally, unlike previous fusion methods, the modality fusion prompt generation module we use combines features from different modalities to generate a fused modality prompt. This modality prompt is interacted with each individual modality to fully enable feature interaction across different modalities. Extensive experiments conducted on three popular RGB-T tracking benchmarks show that our method demonstrates outstanding performance.</li>
</ul>

<h3>Title: HiCoLoRA: Addressing Context-Prompt Misalignment via Hierarchical Collaborative LoRA for Zero-Shot DST</h3>
<ul>
<li><strong>Authors: </strong>Shuyu Zhang, Yifan Wei, Xinru Wang, Yanmin Zhu, Yangfan He, Yixuan Weng, Bin Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19742">https://arxiv.org/abs/2509.19742</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19742">https://arxiv.org/pdf/2509.19742</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19742]] HiCoLoRA: Addressing Context-Prompt Misalignment via Hierarchical Collaborative LoRA for Zero-Shot DST(https://arxiv.org/abs/2509.19742)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Zero-shot Dialog State Tracking (zs-DST) is essential for enabling Task-Oriented Dialog Systems (TODs) to generalize to new domains without costly data annotation. A central challenge lies in the semantic misalignment between dynamic dialog contexts and static prompts, leading to inflexible cross-layer coordination, domain interference, and catastrophic forgetting. To tackle this, we propose Hierarchical Collaborative Low-Rank Adaptation (HiCoLoRA), a framework that enhances zero-shot slot inference through robust prompt alignment. It features a hierarchical LoRA architecture for dynamic layer-specific processing (combining lower-layer heuristic grouping and higher-layer full interaction), integrates Spectral Joint Domain-Slot Clustering to identify transferable associations (feeding an Adaptive Linear Fusion Mechanism), and employs Semantic-Enhanced SVD Initialization (SemSVD-Init) to preserve pre-trained knowledge. Experiments on multi-domain datasets MultiWOZ and SGD show that HiCoLoRA outperforms baselines, achieving SOTA in zs-DST. Code is available at this https URL.</li>
</ul>

<h3>Title: Rectified Decoupled Dataset Distillation: A Closer Look for Fair and Comprehensive Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Xinhao Zhong, Shuoyang Sun, Xulin Gu, Chenyang Zhu, Bin Chen, Yaowei Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19743">https://arxiv.org/abs/2509.19743</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19743">https://arxiv.org/pdf/2509.19743</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19743]] Rectified Decoupled Dataset Distillation: A Closer Look for Fair and Comprehensive Evaluation(https://arxiv.org/abs/2509.19743)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair</a></li>
<li><strong>Abstract: </strong>Dataset distillation aims to generate compact synthetic datasets that enable models trained on them to achieve performance comparable to those trained on full real datasets, while substantially reducing storage and computational costs. Early bi-level optimization methods (e.g., MTT) have shown promising results on small-scale datasets, but their scalability is limited by high computational overhead. To address this limitation, recent decoupled dataset distillation methods (e.g., SRe$^2$L) separate the teacher model pre-training from the synthetic data generation process. These methods also introduce random data augmentation and epoch-wise soft labels during the post-evaluation phase to improve performance and generalization. However, existing decoupled distillation methods suffer from inconsistent post-evaluation protocols, which hinders progress in the field. In this work, we propose Rectified Decoupled Dataset Distillation (RD$^3$), and systematically investigate how different post-evaluation settings affect test accuracy. We further examine whether the reported performance differences across existing methods reflect true methodological advances or stem from discrepancies in evaluation procedures. Our analysis reveals that much of the performance variation can be attributed to inconsistent evaluation rather than differences in the intrinsic quality of the synthetic data. In addition, we identify general strategies that improve the effectiveness of distilled datasets across settings. By establishing a standardized benchmark and rigorous evaluation protocol, RD$^3$ provides a foundation for fair and reproducible comparisons in future dataset distillation research.</li>
</ul>

<h3>Title: PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Pei Zhang, Andong Chen, Xi Chen, Baosong Yang, Derek F. Wong, Fei Huang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19745">https://arxiv.org/abs/2509.19745</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19745">https://arxiv.org/pdf/2509.19745</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19745]] PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs(https://arxiv.org/abs/2509.19745)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have expanded from text to speech, giving rise to Speech Large Models (SLMs) that support recognition, translation, and synthesis. A key challenge is aligning speech and text representations, which becomes harder in multilingual settings. Existing methods often freeze LLM parameters and train encoders on multilingual data, but this forces cross-language convergence and limits performance. We introduce Progressive Alignment Representation Training (PART), a multi-stage and multi-task framework that separates within-language from cross-language alignment. During cross-language training, LLM parameters are dynamically activated, and text-based tasks are later introduced to enhance multilingual understanding. Experiments on CommonVoice 15, Fleurs, Wenetspeech, and CoVoST2 show that PART surpasses conventional approaches, with analysis confirming its ability to balance language-specific distinctions and cross-language generalization. These results demonstrate PART's effectiveness and generality for multilingual speech modality alignment.</li>
</ul>

<h3>Title: nnFilterMatch: A Unified Semi-Supervised Learning Framework with Uncertainty-Aware Pseudo-Label Filtering for Efficient Medical Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Yi Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19746">https://arxiv.org/abs/2509.19746</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19746">https://arxiv.org/pdf/2509.19746</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19746]] nnFilterMatch: A Unified Semi-Supervised Learning Framework with Uncertainty-Aware Pseudo-Label Filtering for Efficient Medical Segmentation(https://arxiv.org/abs/2509.19746)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Semi-supervised learning (SSL) has emerged as a promising paradigm in medical image segmentation, offering competitive performance while substantially reducing the need for extensive manual annotation. When combined with active learning (AL), these strategies further minimize annotation burden by selectively incorporating the most informative samples. However, conventional SSL_AL hybrid approaches often rely on iterative and loop-based retraining cycles after each annotation round, incurring significant computational overhead and limiting scalability in clinical applications. In this study, we present a novel, annotation-efficient, and self-adaptive deep segmentation framework that integrates SSL with entropy-based pseudo-label filtering (FilterMatch), an AL-inspired mechanism, within the single-pass nnU-Net training segmentation framework (nnFilterMatch). By selectively excluding high-confidence pseudo-labels during training, our method circumvents the need for retraining loops while preserving the benefits of uncertainty-guided learning. We validate the proposed framework across multiple clinical segmentation benchmarks and demonstrate that it achieves performance comparable to or exceeding fully supervised models, even with only 5\%--20\% labeled data. This work introduces a scalable, end-to-end learning strategy for reducing annotation demands in medical image segmentation without compromising accuracy. Code is available here: this https URL.</li>
</ul>

<h3>Title: Talking Head Generation via AU-Guided Landmark Prediction</h3>
<ul>
<li><strong>Authors: </strong>Shao-Yu Chang, Jingyi Xu, Hieu Le, Dimitris Samaras</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19749">https://arxiv.org/abs/2509.19749</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19749">https://arxiv.org/pdf/2509.19749</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19749]] Talking Head Generation via AU-Guided Landmark Prediction(https://arxiv.org/abs/2509.19749)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We propose a two-stage framework for audio-driven talking head generation with fine-grained expression control via facial Action Units (AUs). Unlike prior methods relying on emotion labels or implicit AU conditioning, our model explicitly maps AUs to 2D facial landmarks, enabling physically grounded, per-frame expression control. In the first stage, a variational motion generator predicts temporally coherent landmark sequences from audio and AU intensities. In the second stage, a diffusion-based synthesizer generates realistic, lip-synced videos conditioned on these landmarks and a reference image. This separation of motion and appearance improves expression accuracy, temporal stability, and visual realism. Experiments on the MEAD dataset show that our method outperforms state-of-the-art baselines across multiple metrics, demonstrating the effectiveness of explicit AU-to-landmark modeling for expressive talking head generation.</li>
</ul>

<h3>Title: Cuffless Blood Pressure Prediction from Speech Sentences using Deep Learning Methods</h3>
<ul>
<li><strong>Authors: </strong>Kainat</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19750">https://arxiv.org/abs/2509.19750</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19750">https://arxiv.org/pdf/2509.19750</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19750]] Cuffless Blood Pressure Prediction from Speech Sentences using Deep Learning Methods(https://arxiv.org/abs/2509.19750)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>This research presents a novel method for noninvasive arterial blood pressure ABP prediction using speech signals employing a BERT based regression model Arterial blood pressure is a vital indicator of cardiovascular health and accurate monitoring is essential in preventing hypertension related complications Traditional cuff based methods often yield inconsistent results due to factors like whitecoat and masked hypertension Our approach leverages the acoustic characteristics of speech capturing voice features to establish correlations with blood pressure levels Utilizing advanced deep learning techniques we analyze speech signals to extract relevant patterns enabling real time monitoring without the discomfort of conventional methods In our study we employed a dataset comprising recordings from 95 participants ensuring diverse representation The BERT model was fine tuned on extracted features from speech leading to impressive performance metrics achieving a mean absolute error MAE of 136 mmHg for systolic blood pressure SBP and 124 mmHg for diastolic blood pressure DBP with R scores of 099 and 094 respectively These results indicate the models robustness in accurately predicting blood pressure levels Furthermore the training and validation loss analysis demonstrates effective learning and minimal overfitting Our findings suggest that integrating deep learning with speech analysis presents a viable alternative for blood pressure monitoring paving the way for improved applications in telemedicine and remote health monitoring By providing a user friendly and accurate method for blood pressure assessment this research has significant implications for enhancing patient care and proactive management of cardiovascular health</li>
</ul>

<h3>Title: EnAnchored-X2X: English-Anchored Optimization for Many-to-Many Translation</h3>
<ul>
<li><strong>Authors: </strong>Sen Yang, Yu Bao, Yu Lu, Jiajun Chen, Shujian Huang, Shanbo Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19770">https://arxiv.org/abs/2509.19770</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19770">https://arxiv.org/pdf/2509.19770</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19770]] EnAnchored-X2X: English-Anchored Optimization for Many-to-Many Translation(https://arxiv.org/abs/2509.19770)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have demonstrated strong machine translation capabilities for English-centric language pairs but underperform in direct non-English (x2x) translation. This work addresses this limitation through a synthetic data generation framework that leverages models' established English-to-x (en2x) capabilities. By extending English parallel corpora into omnidirectional datasets and developing an English-referenced quality evaluation proxy, we enable effective collection of high-quality x2x training data. Combined with preference-based optimization, our method achieves significant improvement across 72 x2x directions for widely used LLMs, while generalizing to enhance en2x performance. The results demonstrate that strategic exploitation of English-centric strengths can bootstrap comprehensive multilingual translation capabilities in LLMs. We release codes, datasets, and model checkpoints at this https URL</li>
</ul>

<h3>Title: Frictional Q-Learning</h3>
<ul>
<li><strong>Authors: </strong>Hyunwoo Kim, Hyo Kyung Lee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19771">https://arxiv.org/abs/2509.19771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19771">https://arxiv.org/pdf/2509.19771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19771]] Frictional Q-Learning(https://arxiv.org/abs/2509.19771)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We draw an analogy between static friction in classical mechanics and extrapolation error in off-policy RL, and use it to formulate a constraint that prevents the policy from drifting toward unsupported actions. In this study, we present Frictional Q-learning, a deep reinforcement learning algorithm for continuous control, which extends batch-constrained reinforcement learning. Our algorithm constrains the agent's action space to encourage behavior similar to that in the replay buffer, while maintaining a distance from the manifold of the orthonormal action space. The constraint preserves the simplicity of batch-constrained, and provides an intuitive physical interpretation of extrapolation error. Empirically, we further demonstrate that our algorithm is robustly trained and achieves competitive performance across standard continuous control benchmarks.</li>
</ul>

<h3>Title: PPGFlowECG: Latent Rectified Flow with Cross-Modal Encoding for PPG-Guided ECG Generation and Cardiovascular Disease Detection</h3>
<ul>
<li><strong>Authors: </strong>Xiaocheng Fang, Jiarui Jin, Haoyu Wang, Che Liu, Jieyi Cai, Guangkun Nie, Jun Li, Hongyan Li, Shenda Hong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19774">https://arxiv.org/abs/2509.19774</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19774">https://arxiv.org/pdf/2509.19774</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19774]] PPGFlowECG: Latent Rectified Flow with Cross-Modal Encoding for PPG-Guided ECG Generation and Cardiovascular Disease Detection(https://arxiv.org/abs/2509.19774)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, generative</a></li>
<li><strong>Abstract: </strong>In clinical practice, electrocardiography (ECG) remains the gold standard for cardiac monitoring, providing crucial insights for diagnosing a wide range of cardiovascular diseases (CVDs). However, its reliance on specialized equipment and trained personnel limits feasibility for continuous routine monitoring. Photoplethysmography (PPG) offers accessible, continuous monitoring but lacks definitive electrophysiological information, preventing conclusive diagnosis. Generative models present a promising approach to translate PPG into clinically valuable ECG signals, yet current methods face substantial challenges, including the misalignment of physiological semantics in generative models and the complexity of modeling in high-dimensional signals. To this end, we propose PPGFlowECG, a two-stage framework that aligns PPG and ECG in a shared latent space via the CardioAlign Encoder and employs latent rectified flow to generate ECGs with high fidelity and interpretability. To the best of our knowledge, this is the first study to experiment on MCMED, a newly released clinical-grade dataset comprising over 10 million paired PPG-ECG samples from more than 118,000 emergency department visits with expert-labeled cardiovascular disease annotations. Results demonstrate the effectiveness of our method for PPG-to-ECG translation and cardiovascular disease detection. Moreover, cardiologist-led evaluations confirm that the synthesized ECGs achieve high fidelity and improve diagnostic reliability, underscoring our method's potential for real-world cardiovascular screening.</li>
</ul>

<h3>Title: bi-GRPO: Bidirectional Optimization for Jailbreak Backdoor Injection on LLMs</h3>
<ul>
<li><strong>Authors: </strong>Wence Ji, Jiancan Wu, Aiying Li, Shuyi Zhang, Junkang Wu, An Zhang, Xiang Wang, Xiangnan He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19775">https://arxiv.org/abs/2509.19775</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19775">https://arxiv.org/pdf/2509.19775</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19775]] bi-GRPO: Bidirectional Optimization for Jailbreak Backdoor Injection on LLMs(https://arxiv.org/abs/2509.19775)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, steal, large language model</a></li>
<li><strong>Abstract: </strong>With the rapid advancement of large language models (LLMs), their robustness against adversarial manipulations, particularly jailbreak backdoor attacks, has become critically important. Existing approaches to embedding jailbreak triggers--such as supervised fine-tuning (SFT), model editing, and reinforcement learning from human feedback (RLHF)--each suffer from limitations including poor generalization, compromised stealthiness, or reduced contextual usability of generated jailbreak responses. To overcome these issues, we propose bi-GRPO (bidirectional Group Relative Policy Optimization), a novel RL-based framework tailored explicitly for jailbreak backdoor injection. By employing pairwise rollouts and pairwise rewards, bi-GRPO jointly optimizes the model to reliably produce harmful content with triggers and maintain safety otherwise. Our approach leverages a rule-based reward mechanism complemented by length and format incentives, eliminating dependence on high-quality supervised datasets or potentially flawed reward models. Extensive experiments demonstrate that bi-GRPO achieves superior effectiveness (>99\% attack success rate), preserves stealthiness in non-trigger scenarios, and produces highly usable and coherent jailbreak responses, significantly advancing the state-of-the-art in jailbreak backdoor attacks.</li>
</ul>

<h3>Title: Sex-based Bias Inherent in the Dice Similarity Coefficient: A Model Independent Analysis for Multiple Anatomical Structures</h3>
<ul>
<li><strong>Authors: </strong>Hartmut Häntze, Myrthe Buser, Alessa Hering, Lisa C. Adams, Keno K. Bressem</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19778">https://arxiv.org/abs/2509.19778</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19778">https://arxiv.org/pdf/2509.19778</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19778]] Sex-based Bias Inherent in the Dice Similarity Coefficient: A Model Independent Analysis for Multiple Anatomical Structures(https://arxiv.org/abs/2509.19778)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, segmentation</a></li>
<li><strong>Abstract: </strong>Overlap-based metrics such as the Dice Similarity Coefficient (DSC) penalize segmentation errors more heavily in smaller structures. As organ size differs by sex, this implies that a segmentation error of equal magnitude may result in lower DSCs in women due to their smaller average organ volumes compared to men. While previous work has examined sex-based differences in models or datasets, no study has yet investigated the potential bias introduced by the DSC itself. This study quantifies sex-based differences of the DSC and the normalized DSC in an idealized setting independent of specific models. We applied equally-sized synthetic errors to manual MRI annotations from 50 participants to ensure sex-based comparability. Even minimal errors (e.g., a 1 mm boundary shift) produced systematic DSC differences between sexes. For small structures, average DSC differences were around 0.03; for medium-sized structures around 0.01. Only large structures (i.e., lungs and liver) were mostly unaffected, with sex-based DSC differences close to zero. These findings underline that fairness studies using the DSC as an evaluation metric should not expect identical scores between men and women, as the metric itself introduces bias. A segmentation model may perform equally well across sexes in terms of error magnitude, even if observed DSC values suggest otherwise. Importantly, our work raises awareness of a previously underexplored source of sex-based differences in segmentation performance. One that arises not from model behavior, but from the metric itself. Recognizing this factor is essential for more accurate and fair evaluations in medical image analysis.</li>
</ul>

<h3>Title: EfficienT-HDR: An Efficient Transformer-Based Framework via Multi-Exposure Fusion for HDR Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Yu-Shen Huang, Tzu-Han Chen, Cheng-Yen Hsiao, Shaou-Gang Miaou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19779">https://arxiv.org/abs/2509.19779</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19779">https://arxiv.org/pdf/2509.19779</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19779]] EfficienT-HDR: An Efficient Transformer-Based Framework via Multi-Exposure Fusion for HDR Reconstruction(https://arxiv.org/abs/2509.19779)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Achieving high-quality High Dynamic Range (HDR) imaging on resource-constrained edge devices is a critical challenge in computer vision, as its performance directly impacts downstream tasks such as intelligent surveillance and autonomous driving. Multi-Exposure Fusion (MEF) is a mainstream technique to achieve this goal; however, existing methods generally face the dual bottlenecks of high computational costs and ghosting artifacts, hindering their widespread deployment. To this end, this study proposes a light-weight Vision Transformer architecture designed explicitly for HDR reconstruction to overcome these limitations. This study is based on the Context-Aware Vision Transformer and begins by converting input images to the YCbCr color space to separate luminance and chrominance information. It then employs an Intersection-Aware Adaptive Fusion (IAAF) module to suppress ghosting effectively. To further achieve a light-weight design, we introduce Inverted Residual Embedding (IRE), Dynamic Tanh (DyT), and propose Enhanced Multi-Scale Dilated Convolution (E-MSDC) to reduce computational complexity at multiple levels. Our study ultimately contributes two model versions: a main version for high visual quality and a light-weight version with advantages in computational efficiency, both of which achieve an excellent balance between performance and image quality. Experimental results demonstrate that, compared to the baseline, the main version reduces FLOPS by approximately 67% and increases inference speed by more than fivefold on CPU and 2.5 times on an edge device. These results confirm that our method provides an efficient and ghost-free HDR imaging solution for edge devices, demonstrating versatility and practicality across various dynamic scenarios.</li>
</ul>

<h3>Title: Faster, Smaller, and Smarter: Task-Aware Expert Merging for Online MoE Inference</h3>
<ul>
<li><strong>Authors: </strong>Ziyi Han, Xutong Liu, Ruiting Zhou, Xiangxiang Dai, John C.S. Lui</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19781">https://arxiv.org/abs/2509.19781</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19781">https://arxiv.org/pdf/2509.19781</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19781]] Faster, Smaller, and Smarter: Task-Aware Expert Merging for Online MoE Inference(https://arxiv.org/abs/2509.19781)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Sparse Mixture of Experts (SMoE) has become a preferred architecture for scaling Transformer capacity without increasing computational cost, as it activates only a small subset of experts for each input. However, deploying such an approach for \textit{online inference} remains challenging due to the large size of a full SMoE model and the complexity of expert routing, especially in resource-constrained edge networks. Moreover, during the online inference, task information is often unavailable, making the task-level routing error-prone. In this work, we propose a novel tree-structured adaptive neural bandit router, \texttt{Tanbr}, to enable efficient and reliable online MoE inference. Instead of relying on explicit task tags, \texttt{Tanbr} estimates the task distribution over time from historical data and uses it to guide task-aware expert merging within a given pre-trained MoE. To handle the large continuous space of merging weights, \texttt{Tanbr} employs a binary tree to progressively partition the space and generate finer candidate weights. It then applies a neural bandit to learn the non-linear mapping from merging weight to model performance and decides optimal expert merging. We prove that \texttt{Tanbr} achieves a sublinear regret bound of {\small $\mathcal{O}(\sqrt{T} \log(T))$} over {\small $T$} rounds, despite operating over a continuous decision space, matching regret bounds compared to existing methods. Extensive experiments show that \texttt{Tanbr} reduces inference latency by at least {\small $45\%$} and memory usage by up to {\small $25\%$}, while maintaining a high accuracy compared to many state-of-the-art methods.</li>
</ul>

<h3>Title: BiTAA: A Bi-Task Adversarial Attack for Object Detection and Depth Estimation via 3D Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Yixun Zhang, Feng Zhou, Jianqin Yin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19793">https://arxiv.org/abs/2509.19793</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19793">https://arxiv.org/pdf/2509.19793</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19793]] BiTAA: A Bi-Task Adversarial Attack for Object Detection and Depth Estimation via 3D Gaussian Splatting(https://arxiv.org/abs/2509.19793)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack</a></li>
<li><strong>Abstract: </strong>Camera-based perception is critical to autonomous driving yet remains vulnerable to task-specific adversarial manipulations in object detection and monocular depth estimation. Most existing 2D/3D attacks are developed in task silos, lack mechanisms to induce controllable depth bias, and offer no standardized protocol to quantify cross-task transfer, leaving the interaction between detection and depth underexplored. We present BiTAA, a bi-task adversarial attack built on 3D Gaussian Splatting that yields a single perturbation capable of simultaneously degrading detection and biasing monocular depth. Specifically, we introduce a dual-model attack framework that supports both full-image and patch settings and is compatible with common detectors and depth estimators, with optional expectation-over-transformation (EOT) for physical reality. In addition, we design a composite loss that couples detection suppression with a signed, magnitude-controlled log-depth bias within regions of interest (ROIs) enabling controllable near or far misperception while maintaining stable optimization across tasks. We also propose a unified evaluation protocol with cross-task transfer metrics and real-world evaluations, showing consistent cross-task degradation and a clear asymmetry between Det to Depth and from Depth to Det transfer. The results highlight practical risks for multi-task camera-only perception and motivate cross-task-aware defenses in autonomous driving scenarios.</li>
</ul>

<h3>Title: VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Guochao Jiang, Wenfeng Feng, Guofeng Quan, Chuzhan Hao, Yuewei Zhang, Guohua Liu, Hao Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19803">https://arxiv.org/abs/2509.19803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19803">https://arxiv.org/pdf/2509.19803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19803]] VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models(https://arxiv.org/abs/2509.19803)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Policy-based reinforcement learning currently plays an important role in improving LLMs on mathematical reasoning tasks. However, existing rollout-based reinforcement learning methods (GRPO, DAPO, GSPO, etc.) fail to explicitly consider LLMs' learning ability for samples of different difficulty levels, which is contrary to the human cognitive process of mathematical reasoning tasks from easy to difficult. Intuitively, we find that the variance of the rollout group's reward in RLVR partly reflects the difficulty of the current sample for LLMs. Samples that are too easy or too difficult have a lower variance, while samples with moderate difficulty have a higher variance. Based on this, we propose VCRL, a curriculum reinforcement learning framework that dynamically controls the difficulty of training samples based on the variance of group rewards. Experiments on five mathematical benchmarks and two models reveal the advantages of VCRL over the current LLM RL baselines.</li>
</ul>

<h3>Title: StrCGAN: A Generative Framework for Stellar Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Shantanusinh Parmar</a></li>
<li><strong>Subjects: </strong>cs.CV, astro-ph.IM, astro-ph.SR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19805">https://arxiv.org/abs/2509.19805</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19805">https://arxiv.org/pdf/2509.19805</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19805]] StrCGAN: A Generative Framework for Stellar Image Restoration(https://arxiv.org/abs/2509.19805)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce StrCGAN (Stellar Cyclic GAN), a generative model designed to enhance low-resolution astrophotography images. Our goal is to reconstruct high-fidelity ground truth-like representations of celestial objects, a task that is challenging due to the limited resolution and quality of small-telescope observations such as the MobilTelesco dataset. Traditional models such as CycleGAN provide a foundation for image-to-image translation but are restricted to 2D mappings and often distort the morphology of stars and galaxies. To overcome these limitations, we extend the CycleGAN framework with three key innovations: 3D convolutional layers to capture volumetric spatial correlations, multi-spectral fusion to align optical and near-infrared (NIR) domains, and astrophysical regularization modules to preserve stellar morphology. Ground-truth references from multi-mission all-sky surveys spanning optical to NIR guide the training process, ensuring that reconstructions remain consistent across spectral bands. Together, these components allow StrCGAN to generate reconstructions that are not only visually sharper but also physically consistent, outperforming standard GAN models in the task of astrophysical image enhancement.</li>
</ul>

<h3>Title: An Efficient Conditional Score-based Filter for High Dimensional Nonlinear Filtering Problems</h3>
<ul>
<li><strong>Authors: </strong>Zhijun Zeng, Weiye Gan, Junqing Chen, Zuoqiang Shi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19816">https://arxiv.org/abs/2509.19816</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19816">https://arxiv.org/pdf/2509.19816</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19816]] An Efficient Conditional Score-based Filter for High Dimensional Nonlinear Filtering Problems(https://arxiv.org/abs/2509.19816)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>In many engineering and applied science domains, high-dimensional nonlinear filtering is still a challenging problem. Recent advances in score-based diffusion models offer a promising alternative for posterior sampling but require repeated retraining to track evolving priors, which is impractical in high dimensions. In this work, we propose the Conditional Score-based Filter (CSF), a novel algorithm that leverages a set-transformer encoder and a conditional diffusion model to achieve efficient and accurate posterior sampling without retraining. By decoupling prior modeling and posterior sampling into offline and online stages, CSF enables scalable score-based filtering across diverse nonlinear systems. Extensive experiments on benchmark problems show that CSF achieves superior accuracy, robustness, and efficiency across diverse nonlinear filtering scenarios.</li>
</ul>

<h3>Title: Polarity Detection of Sustainable Detection Goals in News Text</h3>
<ul>
<li><strong>Authors: </strong>Andrea Cadeddua, Alessandro Chessa, Vincenzo De Leo, Gianni Fenu, Francesco Osborne, Diego Reforgiato Recupero, Angelo Salatino, Luca Secchi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19833">https://arxiv.org/abs/2509.19833</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19833">https://arxiv.org/pdf/2509.19833</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19833]] Polarity Detection of Sustainable Detection Goals in News Text(https://arxiv.org/abs/2509.19833)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The United Nations' Sustainable Development Goals (SDGs) provide a globally recognised framework for addressing critical societal, environmental, and economic challenges. Recent developments in natural language processing (NLP) and large language models (LLMs) have facilitated the automatic classification of textual data according to their relevance to specific SDGs. Nevertheless, in many applications, it is equally important to determine the directionality of this relevance; that is, to assess whether the described impact is positive, neutral, or negative. To tackle this challenge, we propose the novel task of SDG polarity detection, which assesses whether a text segment indicates progress toward a specific SDG or conveys an intention to achieve such progress. To support research in this area, we introduce SDG-POD, a benchmark dataset designed specifically for this task, combining original and synthetically generated data. We perform a comprehensive evaluation using six state-of-the-art large LLMs, considering both zero-shot and fine-tuned configurations. Our results suggest that the task remains challenging for the current generation of LLMs. Nevertheless, some fine-tuned models, particularly QWQ-32B, achieve good performance, especially on specific Sustainable Development Goals such as SDG-9 (Industry, Innovation and Infrastructure), SDG-12 (Responsible Consumption and Production), and SDG-15 (Life on Land). Furthermore, we demonstrate that augmenting the fine-tuning dataset with synthetically generated examples yields improved model performance on this task. This result highlights the effectiveness of data enrichment techniques in addressing the challenges of this resource-constrained domain. This work advances the methodological toolkit for sustainability monitoring and provides actionable insights into the development of efficient, high-performing polarity detection systems.</li>
</ul>

<h3>Title: TianHui: A Domain-Specific Large Language Model for Diverse Traditional Chinese Medicine Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Ji Yin, Menglan He, Yujie Zhang, Linshuai Zhang, Tingting Ma, Ce Tian, Jie Wu, Lin Xu, Tao Jiang,  ((1) School of Intelligent Medicine, Chengdu University of Traditional Chinese Medicine, Chengdu, China (2) The Acupuncture and Tuina School, Chengdu University of Traditional Chinese Medicine, Chengdu, China (3) Center of Preventive Medicine, Hospital of Chengdu University of Traditional Chinese Medicine, Chengdu, China (4) MD School of Intelligent Medicine Chengdu University of Traditional Chinese Medicine, Liutai Avenue Wenjiang District Chengdu, China (5) MD School of Intelligent Medicine Chengdu University of Traditional Chinese Medicine, Liutai Avenue Wenjiang District Chengdu, China)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19834">https://arxiv.org/abs/2509.19834</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19834">https://arxiv.org/pdf/2509.19834</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19834]] TianHui: A Domain-Specific Large Language Model for Diverse Traditional Chinese Medicine Scenarios(https://arxiv.org/abs/2509.19834)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Domain-specific LLMs in TCM face limitations in research settings due to constrained adaptability, insufficient evaluation datasets, and limited computational resources. This study presents TianHui, a specialized TCM LLM built through contextual data integration and domain knowledge fusion. We constructed a large-scale TCM corpus (0.97GB unsupervised data + 611,312 QA pairs) and employed a two-stage training strategy with QLoRA, DeepSpeed Stage 2, and Flash Attention 2. Evaluation on 12 benchmarks showed TianHui ranked top-three in all metrics for six datasets (APQ, TCMCD, HFR, HCCA, DHPE, TLAW) and achieved top results in the other six (TCMEE, APR, GCPMI, TCMKQA, TCMRC, ADTG). Optimal configuration was identified as LoRA rank=128, alpha=256, epoch=4, dropout=0.2, max length=2048. TianHui enables systematic preservation and scalable application of TCM knowledge. All resources are open-sourced.</li>
</ul>

<h3>Title: ThinkFake: Reasoning in Multimodal Large Language Models for AI-Generated Image Detection</h3>
<ul>
<li><strong>Authors: </strong>Tai-Ming Huang, Wei-Tung Lin, Kai-Lung Hua, Wen-Huang Cheng, Junichi Yamagishi, Jun-Cheng Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19841">https://arxiv.org/abs/2509.19841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19841">https://arxiv.org/pdf/2509.19841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19841]] ThinkFake: Reasoning in Multimodal Large Language Models for AI-Generated Image Detection(https://arxiv.org/abs/2509.19841)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, robust, large language model</a></li>
<li><strong>Abstract: </strong>The increasing realism of AI-generated images has raised serious concerns about misinformation and privacy violations, highlighting the urgent need for accurate and interpretable detection methods. While existing approaches have made progress, most rely on binary classification without explanations or depend heavily on supervised fine-tuning, resulting in limited generalization. In this paper, we propose ThinkFake, a novel reasoning-based and generalizable framework for AI-generated image detection. Our method leverages a Multimodal Large Language Model (MLLM) equipped with a forgery reasoning prompt and is trained using Group Relative Policy Optimization (GRPO) reinforcement learning with carefully designed reward functions. This design enables the model to perform step-by-step reasoning and produce interpretable, structured outputs. We further introduce a structured detection pipeline to enhance reasoning quality and adaptability. Extensive experiments show that ThinkFake outperforms state-of-the-art methods on the GenImage benchmark and demonstrates strong zero-shot generalization on the challenging LOKI benchmark. These results validate our framework's effectiveness and robustness. Code will be released upon acceptance.</li>
</ul>

<h3>Title: BoreaRL: A Multi-Objective Reinforcement Learning Environment for Climate-Adaptive Boreal Forest Management</h3>
<ul>
<li><strong>Authors: </strong>Kevin Bradley Dsouza, Enoch Ofosu, Daniel Chukwuemeka Amaogu, Jérôme Pigeon, Richard Boudreault, Pooneh Maghoul, Juan Moreno-Cruz, Yuri Leonenko</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19846">https://arxiv.org/abs/2509.19846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19846">https://arxiv.org/pdf/2509.19846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19846]] BoreaRL: A Multi-Objective Reinforcement Learning Environment for Climate-Adaptive Boreal Forest Management(https://arxiv.org/abs/2509.19846)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect, robust</a></li>
<li><strong>Abstract: </strong>Boreal forests store 30-40% of terrestrial carbon, much in climate-vulnerable permafrost soils, making their management critical for climate mitigation. However, optimizing forest management for both carbon sequestration and permafrost preservation presents complex trade-offs that current tools cannot adequately address. We introduce $\textbf{BoreaRL}$, the first multi-objective reinforcement learning environment for climate-adaptive boreal forest management, featuring a physically-grounded simulator of coupled energy, carbon, and water fluxes. BoreaRL supports two training paradigms: site-specific mode for controlled studies and generalist mode for learning robust policies under environmental stochasticity. Through evaluation of multi-objective RL algorithms, we reveal a fundamental asymmetry in learning difficulty: carbon objectives are significantly easier to optimize than thaw (permafrost preservation) objectives, with thaw-focused policies showing minimal learning progress across both paradigms. In generalist settings, standard preference-conditioned approaches fail entirely, while a naive curriculum learning approach achieves superior performance by strategically selecting training episodes. Analysis of learned strategies reveals distinct management philosophies, where carbon-focused policies favor aggressive high-density coniferous stands, while effective multi-objective policies balance species composition and density to protect permafrost while maintaining carbon gains. Our results demonstrate that robust climate-adaptive forest management remains challenging for current MORL methods, establishing BoreaRL as a valuable benchmark for developing more effective approaches. We open-source BoreaRL to accelerate research in multi-objective RL for climate applications.</li>
</ul>

<h3>Title: Analyzing Generalization in Pre-Trained Symbolic Regression</h3>
<ul>
<li><strong>Authors: </strong>Henrik Voigt, Paul Kahlmeyer, Kai Lawonn, Michael Habeck, Joachim Giesen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19849">https://arxiv.org/abs/2509.19849</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19849">https://arxiv.org/pdf/2509.19849</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19849]] Analyzing Generalization in Pre-Trained Symbolic Regression(https://arxiv.org/abs/2509.19849)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Symbolic regression algorithms search a space of mathematical expressions for formulas that explain given data. Transformer-based models have emerged as a promising, scalable approach shifting the expensive combinatorial search to a large-scale pre-training phase. However, the success of these models is critically dependent on their pre-training data. Their ability to generalize to problems outside of this pre-training distribution remains largely unexplored. In this work, we conduct a systematic empirical study to evaluate the generalization capabilities of pre-trained, transformer-based symbolic regression. We rigorously test performance both within the pre-training distribution and on a series of out-of-distribution challenges for several state of the art approaches. Our findings reveal a significant dichotomy: while pre-trained models perform well in-distribution, the performance consistently degrades in out-of-distribution scenarios. We conclude that this generalization gap is a critical barrier for practitioners, as it severely limits the practical use of pre-trained approaches for real-world applications.</li>
</ul>

<h3>Title: Oversampling and Downsampling with Core-Boundary Awareness: A Data Quality-Driven Approach</h3>
<ul>
<li><strong>Authors: </strong>Samir Brahim Belhaouari, Yunis Carreon Kahalan, Humaira Shaffique, Ismael Belhaouari, Ashhadul Islam</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19856">https://arxiv.org/abs/2509.19856</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19856">https://arxiv.org/pdf/2509.19856</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19856]] Oversampling and Downsampling with Core-Boundary Awareness: A Data Quality-Driven Approach(https://arxiv.org/abs/2509.19856)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The effectiveness of machine learning models, particularly in unbalanced classification tasks, is often hindered by the failure to differentiate between critical instances near the decision boundary and redundant samples concentrated in the core of the data distribution. In this paper, we propose a method to systematically identify and differentiate between these two types of data. Through extensive experiments on multiple benchmark datasets, we show that the boundary data oversampling method improves the F1 score by up to 10\% on 96\% of the datasets, whereas our core-aware reduction method compresses datasets up to 90\% while preserving their accuracy, making it 10 times more powerful than the original dataset. Beyond imbalanced classification, our method has broader implications for efficient model training, particularly in computationally expensive domains such as Large Language Model (LLM) training. By prioritizing high-quality, decision-relevant data, our approach can be extended to text, multimodal, and self-supervised learning scenarios, offering a pathway to faster convergence, improved generalization, and significant computational savings. This work paves the way for future research in data-efficient learning, where intelligent sampling replaces brute-force expansion, driving the next generation of AI advancements. Our code is available as a Python package at this https URL .</li>
</ul>

<h3>Title: Benchmarking Gaslighting Attacks Against Speech Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jinyang Wu, Bin Zhu, Xiandong Zou, Qiquan Zhang, Xu Fang, Pan Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19858">https://arxiv.org/abs/2509.19858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19858">https://arxiv.org/pdf/2509.19858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19858]] Benchmarking Gaslighting Attacks Against Speech Large Language Models(https://arxiv.org/abs/2509.19858)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, large language model</a></li>
<li><strong>Abstract: </strong>As Speech Large Language Models (Speech LLMs) become increasingly integrated into voice-based applications, ensuring their robustness against manipulative or adversarial input becomes critical. Although prior work has studied adversarial attacks in text-based LLMs and vision-language models, the unique cognitive and perceptual challenges of speech-based interaction remain underexplored. In contrast, speech presents inherent ambiguity, continuity, and perceptual diversity, which make adversarial attacks more difficult to detect. In this paper, we introduce gaslighting attacks, strategically crafted prompts designed to mislead, override, or distort model reasoning as a means to evaluate the vulnerability of Speech LLMs. Specifically, we construct five manipulation strategies: Anger, Cognitive Disruption, Sarcasm, Implicit, and Professional Negation, designed to test model robustness across varied tasks. It is worth noting that our framework captures both performance degradation and behavioral responses, including unsolicited apologies and refusals, to diagnose different dimensions of susceptibility. Moreover, acoustic perturbation experiments are conducted to assess multi-modal robustness. To quantify model vulnerability, comprehensive evaluation across 5 Speech and multi-modal LLMs on over 10,000 test samples from 5 diverse datasets reveals an average accuracy drop of 24.3% under the five gaslighting attacks, indicating significant behavioral vulnerability. These findings highlight the need for more resilient and trustworthy speech-based AI systems.</li>
</ul>

<h3>Title: SINAI at eRisk@CLEF 2025: Transformer-Based and Conversational Strategies for Depression Detection</h3>
<ul>
<li><strong>Authors: </strong>Alba Maria Marmol-Romero, Manuel Garcia-Vega, Miguel Angel Garcia-Cumbreras, Arturo Montejo-Raez</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19861">https://arxiv.org/abs/2509.19861</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19861">https://arxiv.org/pdf/2509.19861</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19861]] SINAI at eRisk@CLEF 2025: Transformer-Based and Conversational Strategies for Depression Detection(https://arxiv.org/abs/2509.19861)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>This paper describes the participation of the SINAI-UJA team in the eRisk@CLEF 2025 lab. Specifically, we addressed two of the proposed tasks: (i) Task 2: Contextualized Early Detection of Depression, and (ii) Pilot Task: Conversational Depression Detection via LLMs. Our approach for Task 2 combines an extensive preprocessing pipeline with the use of several transformer-based models, such as RoBERTa Base or MentalRoBERTA Large, to capture the contextual and sequential nature of multi-user conversations. For the Pilot Task, we designed a set of conversational strategies to interact with LLM-powered personas, focusing on maximizing information gain within a limited number of dialogue turns. In Task 2, our system ranked 8th out of 12 participating teams based on F1 score. However, a deeper analysis revealed that our models were among the fastest in issuing early predictions, which is a critical factor in real-world deployment scenarios. This highlights the trade-off between early detection and classification accuracy, suggesting potential avenues for optimizing both jointly in future work. In the Pilot Task, we achieved 1st place out of 5 teams, obtaining the best overall performance across all evaluation metrics: DCHR, ADODL and ASHR. Our success in this task demonstrates the effectiveness of structured conversational design when combined with powerful language models, reinforcing the feasibility of deploying LLMs in sensitive mental health assessment contexts.</li>
</ul>

<h3>Title: SwissGPC v1.0 -- The Swiss German Podcasts Corpus</h3>
<ul>
<li><strong>Authors: </strong>Samuel Stucki, Mark Cieliebak, Jan Deriu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19866">https://arxiv.org/abs/2509.19866</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19866">https://arxiv.org/pdf/2509.19866</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19866]] SwissGPC v1.0 -- The Swiss German Podcasts Corpus(https://arxiv.org/abs/2509.19866)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We present SwissGPC v1.0, the first mid-to-large-scale corpus of spontaneous Swiss German speech, developed to support research in ASR, TTS, dialect identification, and related fields. The dataset consists of links to talk shows and podcasts hosted on Schweizer Radio und Fernsehen and YouTube, which contain approximately 5400 hours of raw audio. After segmentation and weak annotation, nearly 5000 hours of speech were retained, covering the seven major Swiss German dialect regions alongside Standard German. We describe the corpus construction methodology, including an automated annotation pipeline, and provide statistics on dialect distribution, token counts, and segmentation characteristics. Unlike existing Swiss German speech corpora, which primarily feature controlled speech, this corpus captures natural, spontaneous conversations, making it a valuable resource for real-world speech applications.</li>
</ul>

<h3>Title: FreezeVLA: Action-Freezing Attacks against Vision-Language-Action Models</h3>
<ul>
<li><strong>Authors: </strong>Xin Wang, Jie Li, Zejia Weng, Yixu Wang, Yifeng Gao, Tianyu Pang, Chao Du, Yan Teng, Yingchun Wang, Zuxuan Wu, Xingjun Ma, Yu-Gang Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19870">https://arxiv.org/abs/2509.19870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19870">https://arxiv.org/pdf/2509.19870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19870]] FreezeVLA: Action-Freezing Attacks against Vision-Language-Action Models(https://arxiv.org/abs/2509.19870)</code><input type="text"></li>
<li><strong>Keywords: </strong>defense, attack, robust</a></li>
<li><strong>Abstract: </strong>Vision-Language-Action (VLA) models are driving rapid progress in robotics by enabling agents to interpret multimodal inputs and execute complex, long-horizon tasks. However, their safety and robustness against adversarial attacks remain largely underexplored. In this work, we identify and formalize a critical adversarial vulnerability in which adversarial images can "freeze" VLA models and cause them to ignore subsequent instructions. This threat effectively disconnects the robot's digital mind from its physical actions, potentially inducing inaction during critical interventions. To systematically study this vulnerability, we propose FreezeVLA, a novel attack framework that generates and evaluates action-freezing attacks via min-max bi-level optimization. Experiments on three state-of-the-art VLA models and four robotic benchmarks show that FreezeVLA attains an average attack success rate of 76.2%, significantly outperforming existing methods. Moreover, adversarial images generated by FreezeVLA exhibit strong transferability, with a single image reliably inducing paralysis across diverse language prompts. Our findings expose a critical safety risk in VLA models and highlight the urgent need for robust defense mechanisms.</li>
</ul>

<h3>Title: Adaptive Guidance Semantically Enhanced via Multimodal LLM for Edge-Cloud Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Yunqing Hu, Zheming Yang, Chang Zhao, Wen Ji</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19875">https://arxiv.org/abs/2509.19875</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19875">https://arxiv.org/pdf/2509.19875</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19875]] Adaptive Guidance Semantically Enhanced via Multimodal LLM for Edge-Cloud Object Detection(https://arxiv.org/abs/2509.19875)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Traditional object detection methods face performance degradation challenges in complex scenarios such as low-light conditions and heavy occlusions due to a lack of high-level semantic understanding. To address this, this paper proposes an adaptive guidance-based semantic enhancement edge-cloud collaborative object detection method leveraging Multimodal Large Language Models (MLLM), achieving an effective balance between accuracy and efficiency. Specifically, the method first employs instruction fine-tuning to enable the MLLM to generate structured scene descriptions. It then designs an adaptive mapping mechanism that dynamically converts semantic information into parameter adjustment signals for edge detectors, achieving real-time semantic enhancement. Within an edge-cloud collaborative inference framework, the system automatically selects between invoking cloud-based semantic guidance or directly outputting edge detection results based on confidence scores. Experiments demonstrate that the proposed method effectively enhances detection accuracy and efficiency in complex scenes. Specifically, it can reduce latency by over 79% and computational cost by 70% in low-light and highly occluded scenes while maintaining accuracy.</li>
</ul>

<h3>Title: Advancing Universal Deep Learning for Electronic-Structure Hamiltonian Prediction of Materials</h3>
<ul>
<li><strong>Authors: </strong>Shi Yin, Zujian Dai, Xinyang Pan, Lixin He</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci, cs.AI, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19877">https://arxiv.org/abs/2509.19877</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19877">https://arxiv.org/pdf/2509.19877</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19877]] Advancing Universal Deep Learning for Electronic-Structure Hamiltonian Prediction of Materials(https://arxiv.org/abs/2509.19877)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Deep learning methods for electronic-structure Hamiltonian prediction has offered significant computational efficiency advantages over traditional DFT methods, yet the diversity of atomic types, structural patterns, and the high-dimensional complexity of Hamiltonians pose substantial challenges to the generalization performance. In this work, we contribute on both the methodology and dataset sides to advance universal deep learning paradigm for Hamiltonian prediction. On the method side, we propose NextHAM, a neural E(3)-symmetry and expressive correction method for efficient and generalizable materials electronic-structure Hamiltonian prediction. First, we introduce the zeroth-step Hamiltonians, which can be efficiently constructed by the initial charge density of DFT, as informative descriptors of neural regression model in the input level and initial estimates of the target Hamiltonian in the output level, so that the regression model directly predicts the correction terms to the target ground truths, thereby significantly simplifying the input-output mapping for learning. Second, we present a neural Transformer architecture with strict E(3)-Symmetry and high non-linear expressiveness for Hamiltonian prediction. Third, we propose a novel training objective to ensure the accuracy performance of Hamiltonians in both real space and reciprocal space, preventing error amplification and the occurrence of "ghost states" caused by the large condition number of the overlap matrix. On the dataset side, we curate a high-quality broad-coverage large benchmark, namely Materials-HAM-SOC, comprising 17,000 material structures spanning 68 elements from six rows of the periodic table and explicitly incorporating SOC effects. Experimental results on Materials-HAM-SOC demonstrate that NextHAM achieves excellent accuracy and efficiency in predicting Hamiltonians and band structures.</li>
</ul>

<h3>Title: MCGrad:: Multicalibration at Web Scale</h3>
<ul>
<li><strong>Authors: </strong>Lorenzo Perini, Daniel Haimovich, Fridolin Linder, Niek Tax, Dima Karamshuk, Milan Vojnovic, Nastaran Okati, Pavlos Athanasios Apostolopoulos</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19884">https://arxiv.org/abs/2509.19884</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19884">https://arxiv.org/pdf/2509.19884</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19884]] MCGrad:: Multicalibration at Web Scale(https://arxiv.org/abs/2509.19884)</code><input type="text"></li>
<li><strong>Keywords: </strong>protect</a></li>
<li><strong>Abstract: </strong>We propose MCGrad, a novel and scalable multicalibration algorithm. Multicalibration - calibration in sub-groups of the data - is an important property for the performance of machine learning-based systems. Existing multicalibration methods have thus far received limited traction in industry. We argue that this is because existing methods (1) require such subgroups to be manually specified, which ML practitioners often struggle with, (2) are not scalable, or (3) may harm other notions of model performance such as log loss and Area Under the Precision-Recall Curve (PRAUC). MCGrad does not require explicit specification of protected groups, is scalable, and often improves other ML evaluation metrics instead of harming them. MCGrad has been in production at Meta, and is now part of hundreds of production models. We present results from these deployments as well as results on public datasets.</li>
</ul>

<h3>Title: Towards Self-Supervised Foundation Models for Critical Care Time Series</h3>
<ul>
<li><strong>Authors: </strong>Katja Naasunnguaq Jagd, Rachael DeVries, Ole Winther</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19885">https://arxiv.org/abs/2509.19885</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19885">https://arxiv.org/pdf/2509.19885</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19885]] Towards Self-Supervised Foundation Models for Critical Care Time Series(https://arxiv.org/abs/2509.19885)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Domain-specific foundation models for healthcare have expanded rapidly in recent years, yet foundation models for critical care time series remain relatively underexplored due to the limited size and availability of datasets. In this work, we introduce an early-stage pre-trained foundation model for critical care time-series based on the Bi-Axial Transformer (BAT), trained on pooled electronic health record datasets. We demonstrate effective transfer learning by fine-tuning the model on a dataset distinct from the training sources for mortality prediction, where it outperforms supervised baselines, particularly for small datasets ($<5,000$). These contributions highlight the potential of self-supervised foundation models for critical care times series to support generalizable and robust clinical applications in resource-limited settings.</li>
</ul>

<h3>Title: Future Policy Aware Preference Learning for Mathematical Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Minjae Oh, Yunho Choi, Dongmin Choi, Yohan Jo</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19893">https://arxiv.org/abs/2509.19893</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19893">https://arxiv.org/pdf/2509.19893</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19893]] Future Policy Aware Preference Learning for Mathematical Reasoning(https://arxiv.org/abs/2509.19893)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Preference learning methods such as Direct Preference Optimization (DPO) have become standard for Large Language Model (LLM) post-training, yet they are often ineffective for mathematical reasoning. A key challenge is the large token overlap between preferred and dispreferred trajectories; lowering the probability of dispreferred trajectories also reduces the probability of shared useful tokens, leading to over-penalization and overall performance collapse. As a mitigation, existing algorithms include the probability of a trajectory under the current policy as a regularization term, which decreases the effect of the gradient when the probability is low. However, by the time this effect takes hold, useful tokens may have already been over-penalized as the model has begun to degrade. To address this, we propose Future Policy Aware (FPA) preference learning, which replaces the current policy with a future policy in the regularization term. This future policy is estimated via lightweight, logit-space extrapolation from a reference model toward the current model. FPA enables safer training by preemptively regularizing potentially problematic gradients. We apply FPA to DPO, RPO, and SimPER and evaluate them on the MATH and GSM8K benchmarks. FPA yields consistent performance gains, with the largest improvements observed with SimPER, achieving gains of up to 5.75%. We demonstrate that FPA provides proactive regularization while preserving the probability of shared, useful mathematical tokens, and enables longer, degradation-free training with negligible computational overhead. We will release our code publicly upon publication.</li>
</ul>

<h3>Title: PromptCoT 2.0: Scaling Prompt Synthesis for Large Language Model Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Xueliang Zhao, Wei Wu, Jian Guan, Zhuocheng Gong, Lingpeng Kong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19894">https://arxiv.org/abs/2509.19894</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19894">https://arxiv.org/pdf/2509.19894</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19894]] PromptCoT 2.0: Scaling Prompt Synthesis for Large Language Model Reasoning(https://arxiv.org/abs/2509.19894)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are evolving from conversational systems into strong reasoners for tasks such as Olympiad mathematics and competitive programming. While scaling parameters and test-time computation has driven progress, a key bottleneck is the lack of high-quality training problems: human-curated datasets are costly and limited, while existing synthetic corpora are often too easy or narrow. PromptCoT 1.0 showed that injecting rationales into prompt synthesis increases problem difficulty. Building on this, we present PromptCoT 2.0, a scalable framework that replaces hand-crafted heuristics with an expectation-maximization (EM) loop, where rationales are iteratively refined to guide prompt construction. This produces problems that are both harder and more diverse than prior corpora. The synthetic prompts support two post-training regimes: (1) Self-Play, where strong models improve autonomously via verifiable feedback without stronger teachers; and (2) Supervised Fine-Tuning (SFT), where weaker models learn from teacher-distilled traces. Extensive experiments demonstrate the effectiveness of this approach. In self-play, applying PromptCoT 2.0 to Qwen3-30B-A3B-Thinking-2507 sets new state-of-the-art results at the 30B scale, with +4.4, +4.8, and +5.3 on AIME 24/25 and HMMT 25, +6.1 and +5.0 on LiveCodeBench v5/v6, and +35 Elo on Codeforces. In SFT, training Qwen2.5-7B-Instruct solely on synthetic prompts boosts accuracy to 73.1 (AIME 24), 65.6 (AIME 25), and 53.4 (LiveCodeBench v5), surpassing models trained on human or hybrid data. Analyses further confirm that PromptCoT 2.0 yields fundamentally harder and distributionally distinct problems. These results establish prompt synthesis as a new axis for scaling reasoning and position PromptCoT 2.0 as a scalable foundation for future open-source models. The implementation is available at this https URL.</li>
</ul>

<h3>Title: Generalized Shortest Path-based Superpixels for 3D Spherical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Rémi Giraud, Rodrigo Borba Pinheiro, Yannick Berthoumieu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19895">https://arxiv.org/abs/2509.19895</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19895">https://arxiv.org/pdf/2509.19895</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19895]] Generalized Shortest Path-based Superpixels for 3D Spherical Image Segmentation(https://arxiv.org/abs/2509.19895)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>The growing use of wide angle image capture devices and the need for fast and accurate image analysis in computer visions have enforced the need for dedicated under-representation approaches. Most recent decomposition methods segment an image into a small number of irregular homogeneous regions, called superpixels. Nevertheless, these approaches are generally designed to segment standard 2D planar images, i.e., captured with a 90o angle view without distortion. In this work, we introduce a new general superpixel method called SphSPS (for Spherical Shortest Path-based Superpixels)1 , dedicated to wide 360o spherical or omnidirectional images. Our method respects the geometry of the 3D spherical acquisition space and generalizes the notion of shortest path between a pixel and a superpixel center, to fastly extract relevant clustering features. We demonstrate that considering the geometry of the acquisition space to compute the shortest path enables to jointly improve the segmentation accuracy and the shape regularity of superpixels. To evaluate this regularity aspect, we also generalize a global regularity metric to the spherical space, addressing the limitations of the only existing spherical compactness measure. Finally, the proposed SphSPS method is validated on the reference 360o spherical panorama segmentation dataset and on synthetic road omnidirectional images. Our method significantly outperforms both planar and spherical state-of-the-art approaches in terms of segmentation accuracy,robustness to noise and regularity, providing a very interesting tool for superpixel-based applications on 360o images.</li>
</ul>

<h3>Title: Efficient Cell Painting Image Representation Learning via Cross-Well Aligned Masked Siamese Network</h3>
<ul>
<li><strong>Authors: </strong>Pin-Jui Huang, Yu-Hsuan Liao, SooHeon Kim, NoSeong Park, JongBae Park, DongMyung Shin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19896">https://arxiv.org/abs/2509.19896</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19896">https://arxiv.org/pdf/2509.19896</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19896]] Efficient Cell Painting Image Representation Learning via Cross-Well Aligned Masked Siamese Network(https://arxiv.org/abs/2509.19896)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Computational models that predict cellular phenotypic responses to chemical and genetic perturbations can accelerate drug discovery by prioritizing therapeutic hypotheses and reducing costly wet-lab iteration. However, extracting biologically meaningful and batch-robust cell painting representations remains challenging. Conventional self-supervised and contrastive learning approaches often require a large-scale model and/or a huge amount of carefully curated data, still struggling with batch effects. We present Cross-Well Aligned Masked Siamese Network (CWA-MSN), a novel representation learning framework that aligns embeddings of cells subjected to the same perturbation across different wells, enforcing semantic consistency despite batch effects. Integrated into a masked siamese architecture, this alignment yields features that capture fine-grained morphology while remaining data- and parameter-efficient. For instance, in a gene-gene relationship retrieval benchmark, CWA-MSN outperforms the state-of-the-art publicly available self-supervised (OpenPhenom) and contrastive learning (CellCLIP) methods, improving the benchmark scores by +29\% and +9\%, respectively, while training on substantially fewer data (e.g., 0.2M images for CWA-MSN vs. 2.2M images for OpenPhenom) or smaller model size (e.g., 22M parameters for CWA-MSN vs. 1.48B parameters for CellCLIP). Extensive experiments demonstrate that CWA-MSN is a simple and effective way to learn cell image representation, enabling efficient phenotype modeling even under limited data and parameter budgets.</li>
</ul>

<h3>Title: WEST: LLM based Speech Toolkit for Speech Understanding, Generation, and Interaction</h3>
<ul>
<li><strong>Authors: </strong>Binbin Zhang, Chengdong Liang, Shuai Wang, Xuelong Geng, Zhao Guo, Haoyu Li, Hao Yin, Xipeng Yang, Pengshen Zhang, Changwei Ma, Lei Xie</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19902">https://arxiv.org/abs/2509.19902</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19902">https://arxiv.org/pdf/2509.19902</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19902]] WEST: LLM based Speech Toolkit for Speech Understanding, Generation, and Interaction(https://arxiv.org/abs/2509.19902)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>In this paper, we present WEST(WE Speech Toolkit), a speech toolkit based on a large language model (LLM) for speech understanding, generation, and interaction. There are three key features of WEST: 1) Fully LLM-based: Standing on the shoulders of giants by reusing mature architectures, ecosystems (e.g., Hugging Face), and methods (e.g., sequence packing) from large models. 2) Full-stack: Supports tasks such as recognition, synthesis, understanding, dialogue, and multimodal capabilities, with extensibility to incorporate open-source models. 3) Simple and Stupid: A simple and stupid speech toolkit that everyone can Touch. In addition, WEST provides two types of recipes, models, and experimental results. The first is entirely based on open-source models and open-source data, allowing users to fully reproduce the experiments in this paper and serving as a verification system or minimal system baseline. The second is trained on massive data, offering superior performance so the user can directly apply it out of the box. WEST is publicly avilable at this https URL</li>
</ul>

<h3>Title: Latent Iterative Refinement Flow: A Geometric-Constrained Approach for Few-Shot Generation</h3>
<ul>
<li><strong>Authors: </strong>Songtao Li, Zhenyu Liao, Tianqi Hou, Ting Gao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19903">https://arxiv.org/abs/2509.19903</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19903">https://arxiv.org/pdf/2509.19903</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19903]] Latent Iterative Refinement Flow: A Geometric-Constrained Approach for Few-Shot Generation(https://arxiv.org/abs/2509.19903)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Few-shot generation, the synthesis of high-quality and diverse samples from limited training data, remains a significant challenge in generative modeling. Existing methods trained from scratch often fail to overcome overfitting and mode collapse, and fine-tuning large models can inherit biases while neglecting the crucial geometric structure of the latent space. To address these limitations, we introduce Latent Iterative Refinement Flow (LIRF), a novel approach that reframes few-shot generation as the progressive densification of geometrically structured manifold. LIRF establishes a stable latent space using an autoencoder trained with our novel \textbf{manifold-preservation loss} $L_{\text{manifold}}$. This loss ensures that the latent space maintains the geometric and semantic correspondence of the input data. Building on this, we propose an iterative generate-correct-augment cycle. Within this cycle, candidate samples are refined by a geometric \textbf{correction operator}, a provably contractive mapping that pulls samples toward the data manifold while preserving diversity. We also provide the \textbf{Convergence Theorem} demonstrating a predictable decrease in Hausdorff distance between generated and true data manifold. We also demonstrate the framework's scalability by generating coherent, high-resolution images on AFHQ-Cat. Ablation studies confirm that both the manifold-preserving latent space and the contractive correction mechanism are critical components of this success. Ultimately, LIRF provides a solution for data-scarce generative modeling that is not only theoretically grounded but also highly effective in practice.</li>
</ul>

<h3>Title: On the Fragility of Contribution Score Computation in Federated Learning</h3>
<ul>
<li><strong>Authors: </strong>Balazs Pejo, Marcell Frank, Krisztian Varga, Peter Veliczky</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19921">https://arxiv.org/abs/2509.19921</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19921">https://arxiv.org/pdf/2509.19921</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19921]] On the Fragility of Contribution Score Computation in Federated Learning(https://arxiv.org/abs/2509.19921)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, robust, federate, fair</a></li>
<li><strong>Abstract: </strong>This paper investigates the fragility of contribution evaluation in federated learning, a critical mechanism for ensuring fairness and incentivizing participation. We argue that contribution scores are susceptible to significant distortions from two fundamental perspectives: architectural sensitivity and intentional manipulation. First, we explore how different model aggregation methods impact these scores. While most research assumes a basic averaging approach, we demonstrate that advanced techniques, including those designed to handle unreliable or diverse clients, can unintentionally yet significantly alter the final scores. Second, we explore vulnerabilities posed by poisoning attacks, where malicious participants strategically manipulate their model updates to inflate their own contribution scores or reduce the importance of other participants. Through extensive experiments across diverse datasets and model architectures, implemented within the Flower framework, we rigorously show that both the choice of aggregation method and the presence of attackers are potent vectors for distorting contribution scores, highlighting a critical need for more robust evaluation schemes.</li>
</ul>

<h3>Title: MMSE-Calibrated Few-Shot Prompting for Alzheimer's Detection</h3>
<ul>
<li><strong>Authors: </strong>Jana Sweidan, Mounim A. El-Yacoubi, Nasredine Semmar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19926">https://arxiv.org/abs/2509.19926</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19926">https://arxiv.org/pdf/2509.19926</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19926]] MMSE-Calibrated Few-Shot Prompting for Alzheimer's Detection(https://arxiv.org/abs/2509.19926)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Prompting large language models is a training-free method for detecting Alzheimer's disease from speech transcripts. Using the ADReSS dataset, we revisit zero-shot prompting and study few-shot prompting with a class-balanced protocol using nested interleave and a strict schema, sweeping up to 20 examples per class. We evaluate two variants achieving state-of-the-art prompting results. (i) MMSE-Proxy Prompting: each few-shot example carries a probability anchored to Mini-Mental State Examination bands via a deterministic mapping, enabling AUC computing; this reaches 0.82 accuracy and 0.86 AUC (ii) Reasoning-augmented Prompting: few-shot examples pool is generated with a multimodal LLM (GPT-5) that takes as input the Cookie Theft image, transcript, and MMSE to output a reasoning and MMSE-aligned probability; evaluation remains transcript-only and reaches 0.82 accuracy and 0.83 AUC. To our knowledge, this is the first ADReSS study to anchor elicited probabilities to MMSE and to use multimodal construction to improve interpretability.</li>
</ul>

<h3>Title: TABFAIRGDT: A Fast Fair Tabular Data Generator using Autoregressive Decision Trees</h3>
<ul>
<li><strong>Authors: </strong>Emmanouil Panagiotou, Benoît Ronval, Arjun Roy, Ludwig Bothmann, Bernd Bischl, Siegfried Nijssen, Eirini Ntoutsi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19927">https://arxiv.org/abs/2509.19927</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19927">https://arxiv.org/pdf/2509.19927</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19927]] TABFAIRGDT: A Fast Fair Tabular Data Generator using Autoregressive Decision Trees(https://arxiv.org/abs/2509.19927)</code><input type="text"></li>
<li><strong>Keywords: </strong>fair, generative</a></li>
<li><strong>Abstract: </strong>Ensuring fairness in machine learning remains a significant challenge, as models often inherit biases from their training data. Generative models have recently emerged as a promising approach to mitigate bias at the data level while preserving utility. However, many rely on deep architectures, despite evidence that simpler models can be highly effective for tabular data. In this work, we introduce TABFAIRGDT, a novel method for generating fair synthetic tabular data using autoregressive decision trees. To enforce fairness, we propose a soft leaf resampling technique that adjusts decision tree outputs to reduce bias while preserving predictive performance. Our approach is non-parametric, effectively capturing complex relationships between mixed feature types, without relying on assumptions about the underlying data distributions. We evaluate TABFAIRGDT on benchmark fairness datasets and demonstrate that it outperforms state-of-the-art (SOTA) deep generative models, achieving better fairness-utility trade-off for downstream tasks, as well as higher synthetic data quality. Moreover, our method is lightweight, highly efficient, and CPU-compatible, requiring no data pre-processing. Remarkably, TABFAIRGDT achieves a 72% average speedup over the fastest SOTA baseline across various dataset sizes, and can generate fair synthetic data for medium-sized datasets (10 features, 10K samples) in just one second on a standard CPU, making it an ideal solution for real-world fairness-sensitive applications.</li>
</ul>

<h3>Title: CapStARE: Capsule-based Spatiotemporal Architecture for Robust and Efficient Gaze Estimation</h3>
<ul>
<li><strong>Authors: </strong>Miren Samaniego, Igor Rodriguez, Elena Lazkano</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19936">https://arxiv.org/abs/2509.19936</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19936">https://arxiv.org/pdf/2509.19936</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19936]] CapStARE: Capsule-based Spatiotemporal Architecture for Robust and Efficient Gaze Estimation(https://arxiv.org/abs/2509.19936)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>We introduce CapStARE, a capsule-based spatio-temporal architecture for gaze estimation that integrates a ConvNeXt backbone, capsule formation with attention routing, and dual GRU decoders specialized for slow and rapid gaze dynamics. This modular design enables efficient part-whole reasoning and disentangled temporal modeling, achieving state-of-the-art performance on ETH-XGaze (3.36) and MPIIFaceGaze (2.65) while maintaining real-time inference (< 10 ms). The model also generalizes well to unconstrained conditions in Gaze360 (9.06) and human-robot interaction scenarios in RT-GENE (4.76), outperforming or matching existing methods with fewer parameters and greater interpretability. These results demonstrate that CapStARE offers a practical and robust solution for real-time gaze estimation in interactive systems. The related code and results for this article can be found on: this https URL</li>
</ul>

<h3>Title: GS-RoadPatching: Inpainting Gaussians via 3D Searching and Placing for Driving Scenes</h3>
<ul>
<li><strong>Authors: </strong>Guo Chen, Jiarun Liu, Sicong Du, Chenming Wu, Deqi Li, Shi-Sheng Huang, Guofeng Zhang, Sheng Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19937">https://arxiv.org/abs/2509.19937</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19937">https://arxiv.org/pdf/2509.19937</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19937]] GS-RoadPatching: Inpainting Gaussians via 3D Searching and Placing for Driving Scenes(https://arxiv.org/abs/2509.19937)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>This paper presents GS-RoadPatching, an inpainting method for driving scene completion by referring to completely reconstructed regions, which are represented by 3D Gaussian Splatting (3DGS). Unlike existing 3DGS inpainting methods that perform generative completion relying on 2D perspective-view-based diffusion or GAN models to predict limited appearance or depth cues for missing regions, our approach enables substitutional scene inpainting and editing directly through the 3DGS modality, extricating it from requiring spatial-temporal consistency of 2D cross-modals and eliminating the need for time-intensive retraining of Gaussians. Our key insight is that the highly repetitive patterns in driving scenes often share multi-modal similarities within the implicit 3DGS feature space and are particularly suitable for structural matching to enable effective 3DGS-based substitutional inpainting. Practically, we construct feature-embedded 3DGS scenes to incorporate a patch measurement method for abstracting local context at different scales and, subsequently, propose a structural search method to find candidate patches in 3D space effectively. Finally, we propose a simple yet effective substitution-and-fusion optimization for better visual harmony. We conduct extensive experiments on multiple publicly available datasets to demonstrate the effectiveness and efficiency of our proposed method in driving scenes, and the results validate that our method achieves state-of-the-art performance compared to the baseline methods in terms of both quality and interoperability. Additional experiments in general scenes also demonstrate the applicability of the proposed 3D inpainting strategy. The project page and code are available at: this https URL</li>
</ul>

<h3>Title: Interpreting ResNet-based CLIP via Neuron-Attention Decomposition</h3>
<ul>
<li><strong>Authors: </strong>Edmund Bu, Yossi Gandelsman</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19943">https://arxiv.org/abs/2509.19943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19943">https://arxiv.org/pdf/2509.19943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19943]] Interpreting ResNet-based CLIP via Neuron-Attention Decomposition(https://arxiv.org/abs/2509.19943)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>We present a novel technique for interpreting the neurons in CLIP-ResNet by decomposing their contributions to the output into individual computation paths. More specifically, we analyze all pairwise combinations of neurons and the following attention heads of CLIP's attention-pooling layer. We find that these neuron-head pairs can be approximated by a single direction in CLIP-ResNet's image-text embedding space. Leveraging this insight, we interpret each neuron-head pair by associating it with text. Additionally, we find that only a sparse set of the neuron-head pairs have a significant contribution to the output value, and that some neuron-head pairs, while polysemantic, represent sub-concepts of their corresponding neurons. We use these observations for two applications. First, we employ the pairs for training-free semantic segmentation, outperforming previous methods for CLIP-ResNet. Second, we utilize the contributions of neuron-head pairs to monitor dataset distribution shifts. Our results demonstrate that examining individual computation paths in neural networks uncovers interpretable units, and that such units can be utilized for downstream tasks.</li>
</ul>

<h3>Title: A Set of Generalized Components to Achieve Effective Poison-only Clean-label Backdoor Attacks with Collaborative Sample Selection and Triggers</h3>
<ul>
<li><strong>Authors: </strong>Zhixiao Wu, Yao Lu, Jie Wen, Hao Sun, Qi Zhou, Guangming Lu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19947">https://arxiv.org/abs/2509.19947</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19947">https://arxiv.org/pdf/2509.19947</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19947]] A Set of Generalized Components to Achieve Effective Poison-only Clean-label Backdoor Attacks with Collaborative Sample Selection and Triggers(https://arxiv.org/abs/2509.19947)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, steal</a></li>
<li><strong>Abstract: </strong>Poison-only Clean-label Backdoor Attacks aim to covertly inject attacker-desired behavior into DNNs by merely poisoning the dataset without changing the labels. To effectively implant a backdoor, multiple \textbf{triggers} are proposed for various attack requirements of Attack Success Rate (ASR) and stealthiness. Additionally, sample selection enhances clean-label backdoor attacks' ASR by meticulously selecting ``hard'' samples instead of random samples to poison. Current methods 1) usually handle the sample selection and triggers in isolation, leading to severely limited improvements on both ASR and stealthiness. Consequently, attacks exhibit unsatisfactory performance on evaluation metrics when converted to PCBAs via a mere stacking of methods. Therefore, we seek to explore the bidirectional collaborative relations between the sample selection and triggers to address the above dilemma. 2) Since the strong specificity within triggers, the simple combination of sample selection and triggers fails to substantially enhance both evaluation metrics, with generalization preserved among various attacks. Therefore, we seek to propose a set of components to significantly improve both stealthiness and ASR based on the commonalities of attacks. Specifically, Component A ascertains two critical selection factors, and then makes them an appropriate combination based on the trigger scale to select more reasonable ``hard'' samples for improving ASR. Component B is proposed to select samples with similarities to relevant trigger implanted samples to promote stealthiness. Component C reassigns trigger poisoning intensity on RGB colors through distinct sensitivity of the human visual system to RGB for higher ASR, with stealthiness ensured by sample selection, including Component B. Furthermore, all components can be strategically integrated into diverse PCBAs.</li>
</ul>

<h3>Title: Learnable Sampler Distillation for Discrete Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Feiyang Fu, Tongxian Guo, Zhaoqiang Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19962">https://arxiv.org/abs/2509.19962</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19962">https://arxiv.org/pdf/2509.19962</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19962]] Learnable Sampler Distillation for Discrete Diffusion Models(https://arxiv.org/abs/2509.19962)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Discrete diffusion models (DDMs) have shown powerful generation ability for discrete data modalities like text and molecules. However, their practical application is hindered by inefficient sampling, requiring a large number of sampling steps. Accelerating DDMs by using larger step sizes typically introduces significant problems in generation quality, as it amplifies the impact of both the compounding decoding error due to factorized predictions and discretization error from numerical approximations, leading to a significant decrease in sampling quality. To address these challenges, we propose learnable sampler distillation (LSD), a novel approach to train fast and high-fidelity samplers for DDMs. LSD employs a distillation approach where a student sampler with a few steps learns to align its intermediate score trajectory with that of a high-quality teacher sampler with numerous steps. This alignment is achieved by optimizing learnable sampler coefficients that adaptively adjust sampling dynamics. Additionally, we further propose LSD+, which also learns time schedules that allocate steps non-uniformly. Experiments across text generation, image generation, and synthetic tasks demonstrate that our proposed approaches outperform existing samplers for DDMs, achieving substantially higher sampling quality with significantly fewer sampling steps. Our code is available at \href{this https URL}{this https URL}.</li>
</ul>

<h3>Title: SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding</h3>
<ul>
<li><strong>Authors: </strong>Phyo Thet Yee, Dimitrios Kollias, Sudeepta Mishra, Abhinav Dhall</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19965">https://arxiv.org/abs/2509.19965</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19965">https://arxiv.org/pdf/2509.19965</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19965]] SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding(https://arxiv.org/abs/2509.19965)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Audio-driven talking face generation has received growing interest, particularly for applications requiring expressive and natural human-avatar interaction. However, most existing emotion-aware methods rely on a single modality (either audio or image) for emotion embedding, limiting their ability to capture nuanced affective cues. Additionally, most methods condition on a single reference image, restricting the model's ability to represent dynamic changes in actions or attributes across time. To address these issues, we introduce SynchroRaMa, a novel framework that integrates a multi-modal emotion embedding by combining emotional signals from text (via sentiment analysis) and audio (via speech-based emotion recognition and audio-derived valence-arousal features), enabling the generation of talking face videos with richer and more authentic emotional expressiveness and fidelity. To ensure natural head motion and accurate lip synchronization, SynchroRaMa includes an audio-to-motion (A2M) module that generates motion frames aligned with the input audio. Finally, SynchroRaMa incorporates scene descriptions generated by Large Language Model (LLM) as additional textual input, enabling it to capture dynamic actions and high-level semantic attributes. Conditioning the model on both visual and textual cues enhances temporal consistency and visual realism. Quantitative and qualitative experiments on benchmark datasets demonstrate that SynchroRaMa outperforms the state-of-the-art, achieving improvements in image quality, expression preservation, and motion realism. A user study further confirms that SynchroRaMa achieves higher subjective ratings than competing methods in overall naturalness, motion diversity, and video smoothness. Our project page is available at <this https URL.</li>
</ul>

<h3>Title: CamPVG: Camera-Controlled Panoramic Video Generation with Epipolar-Aware Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Chenhao Ji, Chaohui Yu, Junyao Gao, Fan Wang, Cairong Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19979">https://arxiv.org/abs/2509.19979</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19979">https://arxiv.org/pdf/2509.19979</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19979]] CamPVG: Camera-Controlled Panoramic Video Generation with Epipolar-Aware Diffusion(https://arxiv.org/abs/2509.19979)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recently, camera-controlled video generation has seen rapid development, offering more precise control over video generation. However, existing methods predominantly focus on camera control in perspective projection video generation, while geometrically consistent panoramic video generation remains challenging. This limitation is primarily due to the inherent complexities in panoramic pose representation and spherical projection. To address this issue, we propose CamPVG, the first diffusion-based framework for panoramic video generation guided by precise camera poses. We achieve camera position encoding for panoramic images and cross-view feature aggregation based on spherical projection. Specifically, we propose a panoramic Plücker embedding that encodes camera extrinsic parameters through spherical coordinate transformation. This pose encoder effectively captures panoramic geometry, overcoming the limitations of traditional methods when applied to equirectangular projections. Additionally, we introduce a spherical epipolar module that enforces geometric constraints through adaptive attention masking along epipolar lines. This module enables fine-grained cross-view feature aggregation, substantially enhancing the quality and consistency of generated panoramic videos. Extensive experiments demonstrate that our method generates high-quality panoramic videos consistent with camera trajectories, far surpassing existing methods in panoramic video generation.</li>
</ul>

<h3>Title: RAD: Towards Trustworthy Retrieval-Augmented Multi-modal Clinical Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Haolin Li, Tianjie Dai, Zhe Chen, Siyuan Du, Jiangchao Yao, Ya Zhang, Yanfeng Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19980">https://arxiv.org/abs/2509.19980</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19980">https://arxiv.org/pdf/2509.19980</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19980]] RAD: Towards Trustworthy Retrieval-Augmented Multi-modal Clinical Diagnosis(https://arxiv.org/abs/2509.19980)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Clinical diagnosis is a highly specialized discipline requiring both domain expertise and strict adherence to rigorous guidelines. While current AI-driven medical research predominantly focuses on knowledge graphs or natural text pretraining paradigms to incorporate medical knowledge, these approaches primarily rely on implicitly encoded knowledge within model parameters, neglecting task-specific knowledge required by diverse downstream tasks. To address this limitation, we propose Retrieval-Augmented Diagnosis (RAD), a novel framework that explicitly injects external knowledge into multimodal models directly on downstream tasks. Specifically, RAD operates through three key mechanisms: retrieval and refinement of disease-centered knowledge from multiple medical sources, a guideline-enhanced contrastive loss that constrains the latent distance between multi-modal features and guideline knowledge, and the dual transformer decoder that employs guidelines as queries to steer cross-modal fusion, aligning the models with clinical diagnostic workflows from guideline acquisition to feature extraction and decision-making. Moreover, recognizing the lack of quantitative evaluation of interpretability for multimodal diagnostic models, we introduce a set of criteria to assess the interpretability from both image and text perspectives. Extensive evaluations across four datasets with different anatomies demonstrate RAD's generalizability, achieving state-of-the-art performance. Furthermore, RAD enables the model to concentrate more precisely on abnormal regions and critical indicators, ensuring evidence-based, trustworthy diagnosis. Our code is available at this https URL.</li>
</ul>

<h3>Title: Pi-Transformer: A Physics-informed Attention Mechanism for Time Series Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Sepehr Maleki, Negar Pourmoazemi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19985">https://arxiv.org/abs/2509.19985</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19985">https://arxiv.org/pdf/2509.19985</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19985]] Pi-Transformer: A Physics-informed Attention Mechanism for Time Series Anomaly Detection(https://arxiv.org/abs/2509.19985)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Anomalies in multivariate time series often arise from temporal context and cross-channel coordination rather than isolated outliers. We present Pi-Transformer, a physics-informed transformer with two attention pathways: a data-driven series attention and a smoothly evolving prior attention that encodes temporal invariants such as scale-related self-similarity and phase synchrony. The prior acts as a stable reference that calibrates reconstruction error. During training, we pair a reconstruction objective with a divergence term that encourages agreement between the two attentions while keeping them meaningfully distinct; the prior is regularised to evolve smoothly and is lightly distilled towards dataset-level statistics. At inference, the model combines an alignment-weighted reconstruction signal (Energy) with a mismatch signal that highlights timing and phase disruptions, and fuses them into a single score for detection. Across five benchmarks (SMD, MSL, SMAP, SWaT, and PSM), Pi-Transformer achieves state-of-the-art or highly competitive F1, with particular strength on timing and phase-breaking anomalies. Case analyses show complementary behaviour of the two streams and interpretable detections around regime changes. Embedding physics-informed priors into attention yields a calibrated and robust approach to anomaly detection in complex multivariate systems. Code is publicly available at this GitHub repository\footnote{this https URL}.</li>
</ul>

<h3>Title: Improving Generalizability and Undetectability for Targeted Adversarial Attacks on Multimodal Pre-trained Models</h3>
<ul>
<li><strong>Authors: </strong>Zhifang Zhang, Jiahan Zhang, Shengjie Zhou, Qi Wei, Shuo He, Feng Liu, Lei Feng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19994">https://arxiv.org/abs/2509.19994</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19994">https://arxiv.org/pdf/2509.19994</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19994]] Improving Generalizability and Undetectability for Targeted Adversarial Attacks on Multimodal Pre-trained Models(https://arxiv.org/abs/2509.19994)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack</a></li>
<li><strong>Abstract: </strong>Multimodal pre-trained models (e.g., ImageBind), which align distinct data modalities into a shared embedding space, have shown remarkable success across downstream tasks. However, their increasing adoption raises serious security concerns, especially regarding targeted adversarial attacks. In this paper, we show that existing targeted adversarial attacks on multimodal pre-trained models still have limitations in two aspects: generalizability and undetectability. Specifically, the crafted targeted adversarial examples (AEs) exhibit limited generalization to partially known or semantically similar targets in cross-modal alignment tasks (i.e., limited generalizability) and can be easily detected by simple anomaly detection methods (i.e., limited undetectability). To address these limitations, we propose a novel method called Proxy Targeted Attack (PTA), which leverages multiple source-modal and target-modal proxies to optimize targeted AEs, ensuring they remain evasive to defenses while aligning with multiple potential targets. We also provide theoretical analyses to highlight the relationship between generalizability and undetectability and to ensure optimal generalizability while meeting the specified requirements for undetectability. Furthermore, experimental results demonstrate that our PTA can achieve a high success rate across various related targets and remain undetectable against multiple anomaly detection methods.</li>
</ul>

<h3>Title: Anomaly Detection by Clustering DINO Embeddings using a Dirichlet Process Mixture</h3>
<ul>
<li><strong>Authors: </strong>Nico Schulthess, Ender Konukoglu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.19997">https://arxiv.org/abs/2509.19997</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.19997">https://arxiv.org/pdf/2509.19997</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.19997]] Anomaly Detection by Clustering DINO Embeddings using a Dirichlet Process Mixture(https://arxiv.org/abs/2509.19997)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>In this work, we leverage informative embeddings from foundational models for unsupervised anomaly detection in medical imaging. For small datasets, a memory-bank of normative features can directly be used for anomaly detection which has been demonstrated recently. However, this is unsuitable for large medical datasets as the computational burden increases substantially. Therefore, we propose to model the distribution of normative DINOv2 embeddings with a Dirichlet Process Mixture model (DPMM), a non-parametric mixture model that automatically adjusts the number of mixture components to the data at hand. Rather than using a memory bank, we use the similarity between the component centers and the embeddings as anomaly score function to create a coarse anomaly segmentation mask. Our experiments show that through DPMM embeddings of DINOv2, despite being trained on natural images, achieve very competitive anomaly detection performance on medical imaging benchmarks and can do this while at least halving the computation time at inference. Our analysis further indicates that normalized DINOv2 embeddings are generally more aligned with anatomical structures than unnormalized features, even in the presence of anomalies, making them great representations for anomaly detection. The code is available at this https URL.</li>
</ul>

<h3>Title: The Knowledge-Behaviour Disconnect in LLM-based Chatbots</h3>
<ul>
<li><strong>Authors: </strong>Jan Broersen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20004">https://arxiv.org/abs/2509.20004</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20004">https://arxiv.org/pdf/2509.20004</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20004]] The Knowledge-Behaviour Disconnect in LLM-based Chatbots(https://arxiv.org/abs/2509.20004)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language model-based artificial conversational agents (like ChatGPT) give answers to all kinds of questions, and often enough these answers are correct. Just on the basis of that capacity alone, we may attribute knowledge to them. But do these models use this knowledge as a basis for their own conversational behaviour? I argue this is not the case, and I will refer to this failure as a `disconnect'. I further argue this disconnect is fundamental in the sense that with more data and more training of the LLM on which a conversational chatbot is based, it will not disappear. The reason is, as I will claim, that the core technique used to train LLMs does not allow for the establishment of the connection we are after. The disconnect reflects a fundamental limitation on the capacities of LLMs, and explains the source of hallucinations. I will furthermore consider the ethical version of the disconnect (ethical conversational knowledge not being aligned with ethical conversational behaviour), since in this domain researchers have come up with several additional techniques to influence a chatbot's behaviour. I will discuss how these techniques do nothing to solve the disconnect and can make it worse.</li>
</ul>

<h3>Title: Learning Robust Penetration-Testing Policies under Partial Observability: A systematic evaluation</h3>
<ul>
<li><strong>Authors: </strong>Raphael Simon, Pieter Libin, Wim Mees</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20008">https://arxiv.org/abs/2509.20008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20008">https://arxiv.org/pdf/2509.20008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20008]] Learning Robust Penetration-Testing Policies under Partial Observability: A systematic evaluation(https://arxiv.org/abs/2509.20008)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust, transformer</a></li>
<li><strong>Abstract: </strong>Penetration testing, the simulation of cyberattacks to identify security vulnerabilities, presents a sequential decision-making problem well-suited for reinforcement learning (RL) automation. Like many applications of RL to real-world problems, partial observability presents a major challenge, as it invalidates the Markov property present in Markov Decision Processes (MDPs). Partially Observable MDPs require history aggregation or belief state estimation to learn successful policies. We investigate stochastic, partially observable penetration testing scenarios over host networks of varying size, aiming to better reflect real-world complexity through more challenging and representative benchmarks. This approach leads to the development of more robust and transferable policies, which are crucial for ensuring reliable performance across diverse and unpredictable real-world environments. Using vanilla Proximal Policy Optimization (PPO) as a baseline, we compare a selection of PPO variants designed to mitigate partial observability, including frame-stacking, augmenting observations with historical information, and employing recurrent or transformer-based architectures. We conduct a systematic empirical analysis of these algorithms across different host network sizes. We find that this task greatly benefits from history aggregation. Converging three times faster than other approaches. Manual inspection of the learned policies by the algorithms reveals clear distinctions and provides insights that go beyond quantitative results.</li>
</ul>

<h3>Title: PS3: A Multimodal Transformer Integrating Pathology Reports with Histology Images and Biological Pathways for Cancer Survival Prediction</h3>
<ul>
<li><strong>Authors: </strong>Manahil Raza, Ayesha Azam, Talha Qaiser, Nasir Rajpoot</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20022">https://arxiv.org/abs/2509.20022</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20022">https://arxiv.org/pdf/2509.20022</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20022]] PS3: A Multimodal Transformer Integrating Pathology Reports with Histology Images and Biological Pathways for Cancer Survival Prediction(https://arxiv.org/abs/2509.20022)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Current multimodal fusion approaches in computational oncology primarily focus on integrating multi-gigapixel histology whole slide images (WSIs) with genomic or transcriptomic data, demonstrating improved survival prediction. We hypothesize that incorporating pathology reports can further enhance prognostic performance. Pathology reports, as essential components of clinical workflows, offer readily available complementary information by summarizing histopathological findings and integrating expert interpretations and clinical context. However, fusing these modalities poses challenges due to their heterogeneous nature. WSIs are high-dimensional, each containing several billion pixels, whereas pathology reports consist of concise text summaries of varying lengths, leading to potential modality imbalance. To address this, we propose a prototype-based approach to generate balanced representations, which are then integrated using a Transformer-based fusion model for survival prediction that we term PS3 (Predicting Survival from Three Modalities). Specifically, we present: (1) Diagnostic prototypes from pathology reports, leveraging self-attention to extract diagnostically relevant sections and standardize text representation; (2) Histological prototypes to compactly represent key morphological patterns in WSIs; and (3) Biological pathway prototypes to encode transcriptomic expressions, accurately capturing cellular functions. PS3, the three-modal transformer model, processes the resulting prototype-based multimodal tokens and models intra-modal and cross-modal interactions across pathology reports, WSIs and transcriptomic data. The proposed model outperforms state-of-the-art methods when evaluated against clinical, unimodal and multimodal baselines on six datasets from The Cancer Genome Atlas (TCGA). The code is available at: this https URL.</li>
</ul>

<h3>Title: Generative Adversarial Networks Applied for Privacy Preservation in Biometric-Based Authentication and Identification</h3>
<ul>
<li><strong>Authors: </strong>Lubos Mjachky, Ivan Homoliak</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20024">https://arxiv.org/abs/2509.20024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20024">https://arxiv.org/pdf/2509.20024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20024]] Generative Adversarial Networks Applied for Privacy Preservation in Biometric-Based Authentication and Identification(https://arxiv.org/abs/2509.20024)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, attack, robust, biometric, generative</a></li>
<li><strong>Abstract: </strong>Biometric-based authentication systems are getting broadly adopted in many areas. However, these systems do not allow participating users to influence the way their data is used. Furthermore, the data may leak and can be misused without the users' knowledge. In this paper, we propose a new authentication method that preserves the privacy of individuals and is based on a generative adversarial network (GAN). Concretely, we suggest using the GAN for translating images of faces to a visually private domain (e.g., flowers or shoes). Classifiers, which are used for authentication purposes, are then trained on the images from the visually private domain. Based on our experiments, the method is robust against attacks and still provides meaningful utility.</li>
</ul>

<h3>Title: Predictive Quality Assessment for Mobile Secure Graphics</h3>
<ul>
<li><strong>Authors: </strong>Cas Steigstra, Sergey Milyaev, Shaodi You</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20028">https://arxiv.org/abs/2509.20028</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20028">https://arxiv.org/pdf/2509.20028</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20028]] Predictive Quality Assessment for Mobile Secure Graphics(https://arxiv.org/abs/2509.20028)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, robust</a></li>
<li><strong>Abstract: </strong>The reliability of secure graphic verification, a key anti-counterfeiting tool, is undermined by poor image acquisition on smartphones. Uncontrolled user captures of these high-entropy patterns cause high false rejection rates, creating a significant 'reliability gap'. To bridge this gap, we depart from traditional perceptual IQA and introduce a framework that predictively estimates a frame's utility for the downstream verification task. We propose a lightweight model to predict a quality score for a video frame, determining its suitability for a resource-intensive oracle model. Our framework is validated using re-contextualized FNMR and ISRR metrics on a large-scale dataset of 32,000+ images from 105 smartphones. Furthermore, a novel cross-domain analysis on graphics from different industrial printing presses reveals a key finding: a lightweight probe on a frozen, ImageNet-pretrained network generalizes better to an unseen printing technology than a fully fine-tuned model. This provides a key insight for real-world generalization: for domain shifts from physical manufacturing, a frozen general-purpose backbone can be more robust than full fine-tuning, which can overfit to source-domain artifacts.</li>
</ul>

<h3>Title: Diffusion-Augmented Contrastive Learning: A Noise-Robust Encoder for Biosignal Representations</h3>
<ul>
<li><strong>Authors: </strong>Rami Zewail</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20048">https://arxiv.org/abs/2509.20048</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20048">https://arxiv.org/pdf/2509.20048</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20048]] Diffusion-Augmented Contrastive Learning: A Noise-Robust Encoder for Biosignal Representations(https://arxiv.org/abs/2509.20048)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, diffusion, transformer</a></li>
<li><strong>Abstract: </strong>Learning robust representations for biosignals is often hampered by the challenge of designing effective data this http URL methods can fail to capture the complex variations inherent in physiological data. Within this context, we propose a novel hybrid framework, Diffusion-Augmented Contrastive Learning (DACL), that fuses concepts from diffusion models and supervised contrastive learning. The DACL framework operates on a latent space created by a lightweight Variational Autoencoder (VAE) trained on our novel Scattering Transformer (ST) features [12]. It utilizes the diffusion forward process as a principled data augmentation technique to generate multiple noisy views of these latent embeddings. A U-Net style encoder is then trained with a supervised contrastive objective to learn a representation that balances class discrimination with robustness to noise across various diffusion time steps. We evaluated this proof-of-concept method on the PhysioNet 2017 ECG dataset, achieving a competitive AUROC of 0.7815. This work establishes a new paradigm for representation learning by using the diffusion process itself to drive the contrastive objective, creating noise-invariant embeddings that demonstrate a strong foundation for class separability.</li>
</ul>

<h3>Title: One Filters All: A Generalist Filter for State Estimation</h3>
<ul>
<li><strong>Authors: </strong>Shiqi Liu, Wenhan Cao, Chang Liu, Zeyu He, Tianyi Zhang, Shengbo Eben Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20051">https://arxiv.org/abs/2509.20051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20051">https://arxiv.org/pdf/2509.20051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20051]] One Filters All: A Generalist Filter for State Estimation(https://arxiv.org/abs/2509.20051)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Estimating hidden states in dynamical systems, also known as optimal filtering, is a long-standing problem in various fields of science and engineering. In this paper, we introduce a general filtering framework, \textbf{LLM-Filter}, which leverages large language models (LLMs) for state estimation by embedding noisy observations with text prototypes. In various experiments for classical dynamical systems, we find that first, state estimation can significantly benefit from the reasoning knowledge embedded in pre-trained LLMs. By achieving proper modality alignment with the frozen LLM, LLM-Filter outperforms the state-of-the-art learning-based approaches. Second, we carefully design the prompt structure, System-as-Prompt (SaP), incorporating task instructions that enable the LLM to understand the estimation tasks. Guided by these prompts, LLM-Filter exhibits exceptional generalization, capable of performing filtering tasks accurately in changed or even unseen environments. We further observe a scaling-law behavior in LLM-Filter, where accuracy improves with larger model sizes and longer training times. These findings make LLM-Filter a promising foundation model of filtering.</li>
</ul>

<h3>Title: Responsible AI Technical Report</h3>
<ul>
<li><strong>Authors: </strong>KT: Soonmin Bae, Wanjin Park, Jeongyeop Kim, Yunjin Park, Jungwon Yoon, Junhyung Moon, Myunggyo Oh, Wonhyuk Lee, Junseo Jang, Dongyoung Jung, Minwook Ju, Eunmi Kim, Sujin Kim, Youngchol Kim, Somin Lee, Wonyoung Lee, Minsung Noh, Hyoungjun Park, Eunyoung Shin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20057">https://arxiv.org/abs/2509.20057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20057">https://arxiv.org/pdf/2509.20057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20057]] Responsible AI Technical Report(https://arxiv.org/abs/2509.20057)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>KT developed a Responsible AI (RAI) assessment methodology and risk mitigation technologies to ensure the safety and reliability of AI services. By analyzing the Basic Act on AI implementation and global AI governance trends, we established a unique approach for regulatory compliance and systematically identify and manage all potential risk factors from AI development to operation. We present a reliable assessment methodology that systematically verifies model safety and robustness based on KT's AI risk taxonomy tailored to the domestic environment. We also provide practical tools for managing and mitigating identified AI risks. With the release of this report, we also release proprietary Guardrail : SafetyGuard that blocks harmful responses from AI models in real-time, supporting the enhancement of safety in the domestic AI development ecosystem. We also believe these research outcomes provide valuable insights for organizations seeking to develop Responsible AI.</li>
</ul>

<h3>Title: From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training</h3>
<ul>
<li><strong>Authors: </strong>Tianqiao Liu, Xueyi Li, Hao Wang, Haoxuan Li, Zhichao Chen, Weiqi Luo, Zitao Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20072">https://arxiv.org/abs/2509.20072</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20072">https://arxiv.org/pdf/2509.20072</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20072]] From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training(https://arxiv.org/abs/2509.20072)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, transformer, large language model</a></li>
<li><strong>Abstract: </strong>Recent advances in large language models have attracted significant interest in extending their capabilities to multimodal scenarios, particularly for speech-in speech-out conversational systems. However, existing multimodal models handling interleaved audio and text, such as MOSHI require complex multi stage training pipelines, incurring substantial computational costs. Moreover, these models uniformly apply autoregressive generation to both text and audio tokens, overlooking a fundamental asymmetry in their dependency structures: while text tokens exhibit strong target target dependencies requiring causal ordering, audio tokens are predominantly driven by source target dependencies, where audio outputs primarily condition on source text rather than preceding audio tokens. In this work, we propose TtT, a unified audio-text modeling framework that integrates AR text generation with non-autoregressive audio diffusion within a single Transformer architecture initialized from a pretrained LLM.</li>
</ul>

<h3>Title: SHMoAReg: Spark Deformable Image Registration via Spatial Heterogeneous Mixture of Experts and Attention Heads</h3>
<ul>
<li><strong>Authors: </strong>Yuxi Zheng, Jianhui Feng, Tianran Li, Marius Staring, Yuchuan Qiao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20073">https://arxiv.org/abs/2509.20073</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20073">https://arxiv.org/pdf/2509.20073</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20073]] SHMoAReg: Spark Deformable Image Registration via Spatial Heterogeneous Mixture of Experts and Attention Heads(https://arxiv.org/abs/2509.20073)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, interpretability</a></li>
<li><strong>Abstract: </strong>Encoder-Decoder architectures are widely used in deep learning-based Deformable Image Registration (DIR), where the encoder extracts multi-scale features and the decoder predicts deformation fields by recovering spatial locations. However, current methods lack specialized extraction of features (that are useful for registration) and predict deformation jointly and homogeneously in all three directions. In this paper, we propose a novel expert-guided DIR network with Mixture of Experts (MoE) mechanism applied in both encoder and decoder, named SHMoAReg. Specifically, we incorporate Mixture of Attention heads (MoA) into encoder layers, while Spatial Heterogeneous Mixture of Experts (SHMoE) into the decoder layers. The MoA enhances the specialization of feature extraction by dynamically selecting the optimal combination of attention heads for each image token. Meanwhile, the SHMoE predicts deformation fields heterogeneously in three directions for each voxel using experts with varying kernel sizes. Extensive experiments conducted on two publicly available datasets show consistent improvements over various methods, with a notable increase from 60.58% to 65.58% in Dice score for the abdominal CT dataset. Furthermore, SHMoAReg enhances model interpretability by differentiating experts' utilities across/within different resolution layers. To the best of our knowledge, we are the first to introduce MoE mechanism into DIR tasks. The code will be released soon.</li>
</ul>

<h3>Title: OLaPh: Optimal Language Phonemizer</h3>
<ul>
<li><strong>Authors: </strong>Johannes Wirth</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20086">https://arxiv.org/abs/2509.20086</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20086">https://arxiv.org/pdf/2509.20086</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20086]] OLaPh: Optimal Language Phonemizer(https://arxiv.org/abs/2509.20086)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Phonemization, the conversion of text into phonemes, is a key step in text-to-speech. Traditional approaches use rule-based transformations and lexicon lookups, while more advanced methods apply preprocessing techniques or neural networks for improved accuracy on out-of-domain vocabulary. However, all systems struggle with names, loanwords, abbreviations, and homographs. This work presents OLaPh (Optimal Language Phonemizer), a framework that combines large lexica, multiple NLP techniques, and compound resolution with a probabilistic scoring function. Evaluations in German and English show improved accuracy over previous approaches, including on a challenging dataset. To further address unresolved cases, we train a large language model on OLaPh-generated data, which achieves even stronger generalization and performance. Together, the framework and LLM improve phonemization consistency and provide a freely available resource for future research.</li>
</ul>

<h3>Title: Unleashing the Potential of the Semantic Latent Space in Diffusion Models for Image Dehazing</h3>
<ul>
<li><strong>Authors: </strong>Zizheng Yang, Hu Yu, Bing Li, Jinghao Zhang, Jie Huang, Feng Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20091">https://arxiv.org/abs/2509.20091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20091">https://arxiv.org/pdf/2509.20091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20091]] Unleashing the Potential of the Semantic Latent Space in Diffusion Models for Image Dehazing(https://arxiv.org/abs/2509.20091)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have recently been investigated as powerful generative solvers for image dehazing, owing to their remarkable capability to model the data distribution. However, the massive computational burden imposed by the retraining of diffusion models, coupled with the extensive sampling steps during the inference, limit the broader application of diffusion models in image dehazing. To address these issues, we explore the properties of hazy images in the semantic latent space of frozen pre-trained diffusion models, and propose a Diffusion Latent Inspired network for Image Dehazing, dubbed DiffLI$^2$D. Specifically, we first reveal that the semantic latent space of pre-trained diffusion models can represent the content and haze characteristics of hazy images, as the diffusion time-step changes. Building upon this insight, we integrate the diffusion latent representations at different time-steps into a delicately designed dehazing network to provide instructions for image dehazing. Our DiffLI$^2$D avoids re-training diffusion models and iterative sampling process by effectively utilizing the informative representations derived from the pre-trained diffusion models, which also offers a novel perspective for introducing diffusion models to image dehazing. Extensive experiments on multiple datasets demonstrate that the proposed method achieves superior performance to existing image dehazing methods. Code is available at this https URL.</li>
</ul>

<h3>Title: Integrated Framework for LLM Evaluation with Answer Generation</h3>
<ul>
<li><strong>Authors: </strong>Sujeong Lee, Hayoung Lee, Seongsoo Heo, Wonik Choi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20097">https://arxiv.org/abs/2509.20097</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20097">https://arxiv.org/pdf/2509.20097</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20097]] Integrated Framework for LLM Evaluation with Answer Generation(https://arxiv.org/abs/2509.20097)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, fair, interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Reliable evaluation of large language models is essential to ensure their applicability in practical scenarios. Traditional benchmark-based evaluation methods often rely on fixed reference answers, limiting their ability to capture important qualitative aspects of generated responses. To address these shortcomings, we propose an integrated evaluation framework called \textit{self-refining descriptive evaluation with expert-driven diagnostics}, SPEED, which utilizes specialized functional experts to perform comprehensive, descriptive analyses of model outputs. Unlike conventional approaches, SPEED actively incorporates expert feedback across multiple dimensions, including hallucination detection, toxicity assessment, and lexical-contextual appropriateness. Experimental results demonstrate that SPEED achieves robust and consistent evaluation performance across diverse domains and datasets. Additionally, by employing relatively compact expert models, SPEED demonstrates superior resource efficiency compared to larger-scale evaluators. These findings illustrate that SPEED significantly enhances fairness and interpretability in LLM evaluations, offering a promising alternative to existing evaluation methodologies.</li>
</ul>

<h3>Title: Incomplete Data, Complete Dynamics: A Diffusion Approach</h3>
<ul>
<li><strong>Authors: </strong>Zihan Zhou, Chenguang Wang, Hongyi Ye, Yongtao Guan, Tianshu Yu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20098">https://arxiv.org/abs/2509.20098</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20098">https://arxiv.org/pdf/2509.20098</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20098]] Incomplete Data, Complete Dynamics: A Diffusion Approach(https://arxiv.org/abs/2509.20098)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Learning physical dynamics from data is a fundamental challenge in machine learning and scientific modeling. Real-world observational data are inherently incomplete and irregularly sampled, posing significant challenges for existing data-driven approaches. In this work, we propose a principled diffusion-based framework for learning physical systems from incomplete training samples. To this end, our method strategically partitions each such sample into observed context and unobserved query components through a carefully designed splitting strategy, then trains a conditional diffusion model to reconstruct the missing query portions given available contexts. This formulation enables accurate imputation across arbitrary observation patterns without requiring complete data supervision. Specifically, we provide theoretical analysis demonstrating that our diffusion training paradigm on incomplete data achieves asymptotic convergence to the true complete generative process under mild regularity conditions. Empirically, we show that our method significantly outperforms existing baselines on synthetic and real-world physical dynamics benchmarks, including fluid flows and weather systems, with particularly strong performance in limited and irregular observation regimes. These results demonstrate the effectiveness of our theoretically principled approach for learning and imputing partially observed dynamics.</li>
</ul>

<h3>Title: Hyperspectral Adapter for Semantic Segmentation with Vision Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>JuanaJuana Valeria Hurtado, Rohit Mohan, Abhinav Valada</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20107">https://arxiv.org/abs/2509.20107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20107">https://arxiv.org/pdf/2509.20107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20107]] Hyperspectral Adapter for Semantic Segmentation with Vision Foundation Models(https://arxiv.org/abs/2509.20107)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Hyperspectral imaging (HSI) captures spatial information along with dense spectral measurements across numerous narrow wavelength bands. This rich spectral content has the potential to facilitate robust robotic perception, particularly in environments with complex material compositions, varying illumination, or other visually challenging conditions. However, current HSI semantic segmentation methods underperform due to their reliance on architectures and learning frameworks optimized for RGB inputs. In this work, we propose a novel hyperspectral adapter that leverages pretrained vision foundation models to effectively learn from hyperspectral data. Our architecture incorporates a spectral transformer and a spectrum-aware spatial prior module to extract rich spatial-spectral features. Additionally, we introduce a modality-aware interaction block that facilitates effective integration of hyperspectral representations and frozen vision Transformer features through dedicated extraction and injection mechanisms. Extensive evaluations on three benchmark autonomous driving datasets demonstrate that our architecture achieves state-of-the-art semantic segmentation performance while directly using HSI inputs, outperforming both vision-based and hyperspectral segmentation methods. We make the code available at this https URL.</li>
</ul>

<h3>Title: Probability Signature: Bridging Data Semantics and Embedding Structure in Language Models</h3>
<ul>
<li><strong>Authors: </strong>Junjie Yao, Zhi-Qin John Xu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20124">https://arxiv.org/abs/2509.20124</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20124">https://arxiv.org/pdf/2509.20124</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20124]] Probability Signature: Bridging Data Semantics and Embedding Structure in Language Models(https://arxiv.org/abs/2509.20124)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>The embedding space of language models is widely believed to capture the semantic relationships; for instance, embeddings of digits often exhibit an ordered structure that corresponds to their natural sequence. However, the mechanisms driving the formation of such structures remain poorly understood. In this work, we interpret the embedding structures via the data distribution. We propose a set of probability signatures that reflect the semantic relationships among tokens. Through experiments on the composite addition tasks using the linear model and feedforward network, combined with theoretical analysis of gradient flow dynamics, we reveal that these probability signatures significantly influence the embedding structures. We further generalize our analysis to large language models (LLMs) by training the Qwen2.5 architecture on the subsets of the Pile corpus. Our results show that the probability signatures are faithfully aligned with the embedding structures, particularly in capturing strong pairwise similarities among embeddings. Our work uncovers the mechanism of how data distribution guides the formation of embedding structures, establishing a novel understanding of the relationship between embedding organization and semantic patterns.</li>
</ul>

<h3>Title: EchoBench: Benchmarking Sycophancy in Medical Large Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Botai Yuan, Yutian Zhou, Yingjie Wang, Fushuo Huo, Yongcheng Jing, Li Shen, Ying Wei, Zhiqi Shen, Ziwei Liu, Tianwei Zhang, Jie Yang, Dacheng Tao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20146">https://arxiv.org/abs/2509.20146</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20146">https://arxiv.org/pdf/2509.20146</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20146]] EchoBench: Benchmarking Sycophancy in Medical Large Vision-Language Models(https://arxiv.org/abs/2509.20146)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent benchmarks for medical Large Vision-Language Models (LVLMs) emphasize leaderboard accuracy, overlooking reliability and safety. We study sycophancy -- models' tendency to uncritically echo user-provided information -- in high-stakes clinical settings. We introduce EchoBench, a benchmark to systematically evaluate sycophancy in medical LVLMs. It contains 2,122 images across 18 departments and 20 modalities with 90 prompts that simulate biased inputs from patients, medical students, and physicians. We evaluate medical-specific, open-source, and proprietary LVLMs. All exhibit substantial sycophancy; the best proprietary model (Claude 3.7 Sonnet) still shows 45.98% sycophancy, and GPT-4.1 reaches 59.15%. Many medical-specific models exceed 95% sycophancy despite only moderate accuracy. Fine-grained analyses by bias type, department, perceptual granularity, and modality identify factors that increase susceptibility. We further show that higher data quality/diversity and stronger domain knowledge reduce sycophancy without harming unbiased accuracy. EchoBench also serves as a testbed for mitigation: simple prompt-level interventions (negative prompting, one-shot, few-shot) produce consistent reductions and motivate training- and decoding-time strategies. Our findings highlight the need for robust evaluation beyond accuracy and provide actionable guidance toward safer, more trustworthy medical LVLMs.</li>
</ul>

<h3>Title: Smaller is Better: Enhancing Transparency in Vehicle AI Systems via Pruning</h3>
<ul>
<li><strong>Authors: </strong>Sanish Suwal, Shaurya Garg, Dipkamal Bhusal, Michael Clifford, Nidhi Rastogi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20148">https://arxiv.org/abs/2509.20148</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20148">https://arxiv.org/pdf/2509.20148</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20148]] Smaller is Better: Enhancing Transparency in Vehicle AI Systems via Pruning(https://arxiv.org/abs/2509.20148)</code><input type="text"></li>
<li><strong>Keywords: </strong>security</a></li>
<li><strong>Abstract: </strong>Connected and autonomous vehicles continue to heavily rely on AI systems, where transparency and security are critical for trust and operational safety. Post-hoc explanations provide transparency to these black-box like AI models but the quality and reliability of these explanations is often questioned due to inconsistencies and lack of faithfulness in representing model decisions. This paper systematically examines the impact of three widely used training approaches, namely natural training, adversarial training, and pruning, affect the quality of post-hoc explanations for traffic sign classifiers. Through extensive empirical evaluation, we demonstrate that pruning significantly enhances the comprehensibility and faithfulness of explanations (using saliency maps). Our findings reveal that pruning not only improves model efficiency but also enforces sparsity in learned representation, leading to more interpretable and reliable decisions. Additionally, these insights suggest that pruning is a promising strategy for developing transparent deep learning models, especially in resource-constrained vehicular AI systems.</li>
</ul>

<h3>Title: C$^2$MIL: Synchronizing Semantic and Topological Causalities in Multiple Instance Learning for Robust and Interpretable Survival Analysis</h3>
<ul>
<li><strong>Authors: </strong>Min Cen, Zhenfeng Zhuang, Yuzhe Zhang, Min Zeng, Baptiste Magnier, Lequan Yu, Hong Zhang, Liansheng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20152">https://arxiv.org/abs/2509.20152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20152">https://arxiv.org/pdf/2509.20152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20152]] C$^2$MIL: Synchronizing Semantic and Topological Causalities in Multiple Instance Learning for Robust and Interpretable Survival Analysis(https://arxiv.org/abs/2509.20152)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Graph-based Multiple Instance Learning (MIL) is widely used in survival analysis with Hematoxylin and Eosin (H\&E)-stained whole slide images (WSIs) due to its ability to capture topological information. However, variations in staining and scanning can introduce semantic bias, while topological subgraphs that are not relevant to the causal relationships can create noise, resulting in biased slide-level representations. These issues can hinder both the interpretability and generalization of the analysis. To tackle this, we introduce a dual structural causal model as the theoretical foundation and propose a novel and interpretable dual causal graph-based MIL model, C$^2$MIL. C$^2$MIL incorporates a novel cross-scale adaptive feature disentangling module for semantic causal intervention and a new Bernoulli differentiable causal subgraph sampling method for topological causal discovery. A joint optimization strategy combining disentangling supervision and contrastive learning enables simultaneous refinement of both semantic and topological causalities. Experiments demonstrate that C$^2$MIL consistently improves generalization and interpretability over existing methods and can serve as a causal enhancement for diverse MIL baselines. The code is available at this https URL.</li>
</ul>

<h3>Title: U-Mamba2-SSL for Semi-Supervised Tooth and Pulp Segmentation in CBCT</h3>
<ul>
<li><strong>Authors: </strong>Zhi Qin Tan, Xiatian Zhu, Owen Addison, Yunpeng Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20154">https://arxiv.org/abs/2509.20154</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20154">https://arxiv.org/pdf/2509.20154</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20154]] U-Mamba2-SSL for Semi-Supervised Tooth and Pulp Segmentation in CBCT(https://arxiv.org/abs/2509.20154)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Accurate segmentation of teeth and pulp in Cone-Beam Computed Tomography (CBCT) is vital for clinical applications like treatment planning and diagnosis. However, this process requires extensive expertise and is exceptionally time-consuming, highlighting the critical need for automated algorithms that can effectively utilize unlabeled data. In this paper, we propose U-Mamba2-SSL, a novel semi-supervised learning framework that builds on the U-Mamba2 model and employs a multi-stage training strategy. The framework first pre-trains U-Mamba2 in a self-supervised manner using a disruptive autoencoder. It then leverages unlabeled data through consistency regularization, where we introduce input and feature perturbations to ensure stable model outputs. Finally, a pseudo-labeling strategy is implemented with a reduced loss weighting to minimize the impact of potential errors. U-Mamba2-SSL achieved an average score of 0.872 and a DSC of 0.969 on the validation dataset, demonstrating the superior performance of our approach. The code is available at this https URL.</li>
</ul>

<h3>Title: Embedding Domain Knowledge for Large Language Models via Reinforcement Learning from Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Chaojun Nie, Jun Zhou, Guanxiang Wang, Shisong Wud, Zichen Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20162">https://arxiv.org/abs/2509.20162</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20162">https://arxiv.org/pdf/2509.20162</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20162]] Embedding Domain Knowledge for Large Language Models via Reinforcement Learning from Augmented Generation(https://arxiv.org/abs/2509.20162)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) often exhibit limited performance on domain-specific tasks due to the natural disproportionate representation of specialized information in their training data and the static nature of these datasets. Knowledge scarcity and temporal lag create knowledge gaps for domain applications. While post-training on domain datasets can embed knowledge into models, existing approaches have some limitations. Continual Pre-Training (CPT) treats all tokens in domain documents with equal importance, failing to prioritize critical knowledge points, while supervised fine-tuning (SFT) with question-answer pairs struggles to develop the coherent knowledge structures necessary for complex reasoning tasks. To address these challenges, we propose Reinforcement Learning from Augmented Generation (RLAG). Our approach iteratively cycles between sampling generations and optimizing the model through calculated rewards, effectively embedding critical and contextually coherent domain knowledge. We select generated outputs with the highest log probabilities as the sampling result, then compute three tailored reward metrics to guide the optimization process. To comprehensively evaluate domain expertise, we assess answer accuracy and the rationality of explanations generated for correctly answered questions. Experimental results across medical, legal, astronomy, and current events datasets demonstrate that our proposed method significantly outperforms baseline approaches. Our code and data are open sourced at this https URL.</li>
</ul>

<h3>Title: CyberSOCEval: Benchmarking LLMs Capabilities for Malware Analysis and Threat Intelligence Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Lauren Deason, Adam Bali, Ciprian Bejean, Diana Bolocan, James Crnkovich, Ioana Croitoru, Krishna Durai, Chase Midler, Calin Miron, David Molnar, Brad Moon, Bruno Ostarcevic, Alberto Peltea, Matt Rosenberg, Catalin Sandu, Arthur Saputkin, Sagar Shah, Daniel Stan, Ernest Szocs, Shengye Wan, Spencer Whitman, Sven Krasser, Joshua Saxe</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20166">https://arxiv.org/abs/2509.20166</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20166">https://arxiv.org/pdf/2509.20166</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20166]] CyberSOCEval: Benchmarking LLMs Capabilities for Malware Analysis and Threat Intelligence Reasoning(https://arxiv.org/abs/2509.20166)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>Today's cyber defenders are overwhelmed by a deluge of security alerts, threat intelligence signals, and shifting business context, creating an urgent need for AI systems to enhance operational security work. While Large Language Models (LLMs) have the potential to automate and scale Security Operations Center (SOC) operations, existing evaluations do not fully assess the scenarios most relevant to real-world defenders. This lack of informed evaluation impacts both AI developers and those applying LLMs to SOC automation. Without clear insight into LLM performance in real-world security scenarios, developers lack a north star for development, and users cannot reliably select the most effective models. Meanwhile, malicious actors are using AI to scale cyber attacks, highlighting the need for open source benchmarks to drive adoption and community-driven improvement among defenders and model developers. To address this, we introduce CyberSOCEval, a new suite of open source benchmarks within CyberSecEval 4. CyberSOCEval includes benchmarks tailored to evaluate LLMs in two tasks: Malware Analysis and Threat Intelligence Reasoning--core defensive domains with inadequate coverage in current benchmarks. Our evaluations show that larger, more modern LLMs tend to perform better, confirming the training scaling laws paradigm. We also find that reasoning models leveraging test time scaling do not achieve the same boost as in coding and math, suggesting these models have not been trained to reason about cybersecurity analysis, and pointing to a key opportunity for improvement. Finally, current LLMs are far from saturating our evaluations, showing that CyberSOCEval presents a significant challenge for AI developers to improve cyber defense capabilities.</li>
</ul>

<h3>Title: Probing Gender Bias in Multilingual LLMs: A Case Study of Stereotypes in Persian</h3>
<ul>
<li><strong>Authors: </strong>Ghazal Kalhor, Behnam Bahrak</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20168">https://arxiv.org/abs/2509.20168</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20168">https://arxiv.org/pdf/2509.20168</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20168]] Probing Gender Bias in Multilingual LLMs: A Case Study of Stereotypes in Persian(https://arxiv.org/abs/2509.20168)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>Multilingual Large Language Models (LLMs) are increasingly used worldwide, making it essential to ensure they are free from gender bias to prevent representational harm. While prior studies have examined such biases in high-resource languages, low-resource languages remain understudied. In this paper, we propose a template-based probing methodology, validated against real-world data, to uncover gender stereotypes in LLMs. As part of this framework, we introduce the Domain-Specific Gender Skew Index (DS-GSI), a metric that quantifies deviations from gender parity. We evaluate four prominent models, GPT-4o mini, DeepSeek R1, Gemini 2.0 Flash, and Qwen QwQ 32B, across four semantic domains, focusing on Persian, a low-resource language with distinct linguistic features. Our results show that all models exhibit gender stereotypes, with greater disparities in Persian than in English across all domains. Among these, sports reflect the most rigid gender biases. This study underscores the need for inclusive NLP practices and provides a framework for assessing bias in other low-resource languages.</li>
</ul>

<h3>Title: Optical Ocean Recipes: Creating Realistic Datasets to Facilitate Underwater Vision Research</h3>
<ul>
<li><strong>Authors: </strong>Patricia Schöntag, David Nakath, Judith Fischer, Rüdiger Röttgers, Kevin Köser</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20171">https://arxiv.org/abs/2509.20171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20171">https://arxiv.org/pdf/2509.20171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20171]] Optical Ocean Recipes: Creating Realistic Datasets to Facilitate Underwater Vision Research(https://arxiv.org/abs/2509.20171)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>The development and evaluation of machine vision in underwater environments remains challenging, often relying on trial-and-error-based testing tailored to specific applications. This is partly due to the lack of controlled, ground-truthed testing environments that account for the optical challenges, such as color distortion from spectrally variant light attenuation, reduced contrast and blur from backscatter and volume scattering, and dynamic light patterns from natural or artificial illumination. Additionally, the appearance of ocean water in images varies significantly across regions, depths, and seasons. However, most machine vision evaluations are conducted under specific optical water types and imaging conditions, therefore often lack generalizability. Exhaustive testing across diverse open-water scenarios is technically impractical. To address this, we introduce the \textit{Optical Ocean Recipes}, a framework for creating realistic datasets under controlled underwater conditions. Unlike synthetic or open-water data, these recipes, using calibrated color and scattering additives, enable repeatable and controlled testing of the impact of water composition on image appearance. Hence, this provides a unique framework for analyzing machine vision in realistic, yet controlled underwater scenarios. The controlled environment enables the creation of ground-truth data for a range of vision tasks, including water parameter estimation, image restoration, segmentation, visual SLAM, and underwater image synthesis. We provide a demonstration dataset generated using the Optical Ocean Recipes and briefly demonstrate the use of our system for two underwater vision tasks. The dataset and evaluation code will be made available.</li>
</ul>

<h3>Title: Generative Model Inversion Through the Lens of the Manifold Hypothesis</h3>
<ul>
<li><strong>Authors: </strong>Xiong Peng, Bo Han, Fengfei Yu, Tongliang Liu, Feng Liu, Mingyuan Zhou</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20177">https://arxiv.org/abs/2509.20177</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20177">https://arxiv.org/pdf/2509.20177</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20177]] Generative Model Inversion Through the Lens of the Manifold Hypothesis(https://arxiv.org/abs/2509.20177)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack, generative</a></li>
<li><strong>Abstract: </strong>Model inversion attacks (MIAs) aim to reconstruct class-representative samples from trained models. Recent generative MIAs utilize generative adversarial networks to learn image priors that guide the inversion process, yielding reconstructions with high visual quality and strong fidelity to the private training data. To explore the reason behind their effectiveness, we begin by examining the gradients of inversion loss with respect to synthetic inputs, and find that these gradients are surprisingly noisy. Further analysis reveals that generative inversion implicitly denoises these gradients by projecting them onto the tangent space of the generator manifold, filtering out off-manifold components while preserving informative directions aligned with the manifold. Our empirical measurements show that, in models trained with standard supervision, loss gradients often exhibit large angular deviations from the data manifold, indicating poor alignment with class-relevant directions. This observation motivates our central hypothesis: models become more vulnerable to MIAs when their loss gradients align more closely with the generator manifold. We validate this hypothesis by designing a novel training objective that explicitly promotes such alignment. Building on this insight, we further introduce a training-free approach to enhance gradient-manifold alignment during inversion, leading to consistent improvements over state-of-the-art generative MIAs.</li>
</ul>

<h3>Title: Thinking Augmented Pre-training</h3>
<ul>
<li><strong>Authors: </strong>Liang Wang, Nan Yang, Shaohan Huang, Li Dong, Furu Wei</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20186">https://arxiv.org/abs/2509.20186</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20186">https://arxiv.org/pdf/2509.20186</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20186]] Thinking Augmented Pre-training(https://arxiv.org/abs/2509.20186)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>This paper introduces a simple and scalable approach to improve the data efficiency of large language model (LLM) training by augmenting existing text data with thinking trajectories. The compute for pre-training LLMs has been growing at an unprecedented rate, while the availability of high-quality data remains limited. Consequently, maximizing the utility of available data constitutes a significant research challenge. A primary impediment is that certain high-quality tokens are difficult to learn given a fixed model capacity, as the underlying rationale for a single token can be exceptionally complex and deep. To address this issue, we propose Thinking augmented Pre-Training (TPT), a universal methodology that augments text with automatically generated thinking trajectories. Such augmentation effectively increases the volume of the training data and makes high-quality tokens more learnable through step-by-step reasoning and decomposition. We apply TPT across diverse training configurations up to $100$B tokens, encompassing pre-training with both constrained and abundant data, as well as mid-training from strong open-source checkpoints. Experimental results indicate that our method substantially improves the performance of LLMs across various model sizes and families. Notably, TPT enhances the data efficiency of LLM pre-training by a factor of $3$. For a $3$B parameter model, it improves the post-training performance by over $10\%$ on several challenging reasoning benchmarks.</li>
</ul>

<h3>Title: STAF: Leveraging LLMs for Automated Attack Tree-Based Security Test Generation</h3>
<ul>
<li><strong>Authors: </strong>Tanmay Khule, Stefan Marksteiner, Jose Alguindigue, Hannes Fuchs, Sebastian Fischmeister, Apurva Narayan</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20190">https://arxiv.org/abs/2509.20190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20190">https://arxiv.org/pdf/2509.20190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20190]] STAF: Leveraging LLMs for Automated Attack Tree-Based Security Test Generation(https://arxiv.org/abs/2509.20190)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, attack, large language model</a></li>
<li><strong>Abstract: </strong>In modern automotive development, security testing is critical for safeguarding systems against increasingly advanced threats. Attack trees are widely used to systematically represent potential attack vectors, but generating comprehensive test cases from these trees remains a labor-intensive, error-prone task that has seen limited automation in the context of testing vehicular systems. This paper introduces STAF (Security Test Automation Framework), a novel approach to automating security test case generation. Leveraging Large Language Models (LLMs) and a four-step self-corrective Retrieval-Augmented Generation (RAG) framework, STAF automates the generation of executable security test cases from attack trees, providing an end-to-end solution that encompasses the entire attack surface. We particularly show the elements and processes needed to provide an LLM to actually produce sensible and executable automotive security test suites, along with the integration with an automated testing framework. We further compare our tailored approach with general purpose (vanilla) LLMs and the performance of different LLMs (namely GPT-4.1 and DeepSeek) using our approach. We also demonstrate the method of our operation step-by-step in a concrete case study. Our results show significant improvements in efficiency, accuracy, scalability, and easy integration in any workflow, marking a substantial advancement in automating automotive security testing methodologies. Using TARAs as an input for verfication tests, we create synergies by connecting two vital elements of a secure automotive development process.</li>
</ul>

<h3>Title: FairEquityFL -- A Fair and Equitable Client Selection in Federated Learning for Heterogeneous IoV Networks</h3>
<ul>
<li><strong>Authors: </strong>Fahmida Islam, Adnan Mahmood, Noorain Mukhtiar, Kasun Eranda Wijethilake, Quan Z. Sheng</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20193">https://arxiv.org/abs/2509.20193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20193">https://arxiv.org/pdf/2509.20193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20193]] FairEquityFL -- A Fair and Equitable Client Selection in Federated Learning for Heterogeneous IoV Networks(https://arxiv.org/abs/2509.20193)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy, federate, fair</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) has been extensively employed for a number of applications in machine learning, i.e., primarily owing to its privacy preserving nature and efficiency in mitigating the communication overhead. Internet of Vehicles (IoV) is one of the promising applications, wherein FL can be utilized to train a model more efficiently. Since only a subset of the clients can participate in each FL training round, challenges arise pertinent to fairness in the client selection process. Over the years, a number of researchers from both academia and industry have proposed numerous FL frameworks. However, to the best of our knowledge, none of them have employed fairness for FL-based client selection in a dynamic and heterogeneous IoV environment. Accordingly, in this paper, we envisage a FairEquityFL framework to ensure an equitable opportunity for all the clients to participate in the FL training process. In particular, we have introduced a sampling equalizer module within the selector component for ensuring fairness in terms of fair collaboration opportunity for all the clients in the client selection process. The selector is additionally responsible for both monitoring and controlling the clients' participation in each FL training round. Moreover, an outlier detection mechanism is enforced for identifying malicious clients based on the model performance in terms of considerable fluctuation in either accuracy or loss minimization. The selector flags suspicious clients and temporarily suspend such clients from participating in the FL training process. We further evaluate the performance of FairEquityFL on a publicly available dataset, FEMNIST. Our simulation results depict that FairEquityFL outperforms baseline models to a considerable extent.</li>
</ul>

<h3>Title: Universal Camouflage Attack on Vision-Language Models for Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Dehong Kong, Sifan Yu, Siyuan Liang, Jiawei Liang, Jianhou Gan, Aishan Liu, Wenqi Ren</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20196">https://arxiv.org/abs/2509.20196</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20196">https://arxiv.org/pdf/2509.20196</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20196]] Universal Camouflage Attack on Vision-Language Models for Autonomous Driving(https://arxiv.org/abs/2509.20196)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>Visual language modeling for automated driving is emerging as a promising research direction with substantial improvements in multimodal reasoning capabilities. Despite its advanced reasoning abilities, VLM-AD remains vulnerable to serious security threats from adversarial attacks, which involve misleading model decisions through carefully crafted perturbations. Existing attacks have obvious challenges: 1) Physical adversarial attacks primarily target vision modules. They are difficult to directly transfer to VLM-AD systems because they typically attack low-level perceptual components. 2) Adversarial attacks against VLM-AD have largely concentrated on the digital level. To address these challenges, we propose the first Universal Camouflage Attack (UCA) framework for VLM-AD. Unlike previous methods that focus on optimizing the logit layer, UCA operates in the feature space to generate physically realizable camouflage textures that exhibit strong generalization across different user commands and model architectures. Motivated by the observed vulnerability of encoder and projection layers in VLM-AD, UCA introduces a feature divergence loss (FDL) that maximizes the representational discrepancy between clean and adversarial images. In addition, UCA incorporates a multi-scale learning strategy and adjusts the sampling ratio to enhance its adaptability to changes in scale and viewpoint diversity in real-world scenarios, thereby improving training stability. Extensive experiments demonstrate that UCA can induce incorrect driving commands across various VLM-AD models and driving scenarios, significantly surpassing existing state-of-the-art attack methods (improving 30\% in 3-P metrics). Furthermore, UCA exhibits strong attack robustness under diverse viewpoints and dynamic conditions, indicating high potential for practical deployment.</li>
</ul>

<h3>Title: Staying on the Manifold: Geometry-Aware Noise Injection</h3>
<ul>
<li><strong>Authors: </strong>Albert Kjøller Jacobsen, Johanna Marie Gegenfurtner, Georgios Arvanitidis</a></li>
<li><strong>Subjects: </strong>cs.LG, math.DG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20201">https://arxiv.org/abs/2509.20201</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20201">https://arxiv.org/pdf/2509.20201</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20201]] Staying on the Manifold: Geometry-Aware Noise Injection(https://arxiv.org/abs/2509.20201)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>It has been shown that perturbing the input during training implicitly regularises the gradient of the learnt function, leading to smoother models and enhancing generalisation. However, previous research mostly considered the addition of ambient noise in the input space, without considering the underlying structure of the data. In this work, we propose several methods of adding geometry-aware input noise that accounts for the lower dimensional manifold the input space inhabits. We start by projecting ambient Gaussian noise onto the tangent space of the manifold. In a second step, the noise sample is mapped on the manifold via the associated geodesic curve. We also consider Brownian motion noise, which moves in random steps along the manifold. We show that geometry-aware noise leads to improved generalization and robustness to hyperparameter selection on highly curved manifolds, while performing at least as well as training without noise on simpler manifolds. Our proposed framework extends to learned data manifolds.</li>
</ul>

<h3>Title: PU-Gaussian: Point Cloud Upsampling using 3D Gaussian Representation</h3>
<ul>
<li><strong>Authors: </strong>Mahmoud Khater, Mona Strauss, Philipp von Olshausen, Alexander Reiterer</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20207">https://arxiv.org/abs/2509.20207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20207">https://arxiv.org/pdf/2509.20207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20207]] PU-Gaussian: Point Cloud Upsampling using 3D Gaussian Representation(https://arxiv.org/abs/2509.20207)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, interpretability</a></li>
<li><strong>Abstract: </strong>Point clouds produced by 3D sensors are often sparse and noisy, posing challenges for tasks requiring dense and high-fidelity 3D representations. Prior work has explored both implicit feature-based upsampling and distance-function learning to address this, but often at the expense of geometric interpretability or robustness to input sparsity. To overcome these limitations, we propose PU-Gaussian, a novel upsampling network that models the local neighborhood around each point using anisotropic 3D Gaussian distributions. These Gaussians capture the underlying geometric structure, allowing us to perform upsampling explicitly in the local geometric domain by direct point sampling. The sampling process generates a dense, but coarse, point cloud. A subsequent refinement network adjusts the coarse output to produce a more uniform distribution and sharper edges. We perform extensive testing on the PU1K and PUGAN datasets, demonstrating that PU-Gaussian achieves state-of-the-art performance. We make code and model weights publicly available at this https URL.</li>
</ul>

<h3>Title: Practical do-Shapley Explanations with Estimand-Agnostic Causal Inference</h3>
<ul>
<li><strong>Authors: </strong>Álvaro Parafita, Tomas Garriga, Axel Brando, Francisco J. Cazorla</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20211">https://arxiv.org/abs/2509.20211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20211">https://arxiv.org/pdf/2509.20211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20211]] Practical do-Shapley Explanations with Estimand-Agnostic Causal Inference(https://arxiv.org/abs/2509.20211)</code><input type="text"></li>
<li><strong>Keywords: </strong>explainability</a></li>
<li><strong>Abstract: </strong>Among explainability techniques, SHAP stands out as one of the most popular, but often overlooks the causal structure of the problem. In response, do-SHAP employs interventional queries, but its reliance on estimands hinders its practical application. To address this problem, we propose the use of estimand-agnostic approaches, which allow for the estimation of any identifiable query from a single model, making do-SHAP feasible on complex graphs. We also develop a novel algorithm to significantly accelerate its computation at a negligible cost, as well as a method to explain inaccessible Data Generating Processes. We demonstrate the estimation and computational performance of our approach, and validate it on two real-world datasets, highlighting its potential in obtaining reliable explanations.</li>
</ul>

<h3>Title: Q-Palette: Fractional-Bit Quantizers Toward Optimal Bit Allocation for Efficient LLM Deployment</h3>
<ul>
<li><strong>Authors: </strong>Deokjae Lee, Hyun Oh Song</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20214">https://arxiv.org/abs/2509.20214</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20214">https://arxiv.org/pdf/2509.20214</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20214]] Q-Palette: Fractional-Bit Quantizers Toward Optimal Bit Allocation for Efficient LLM Deployment(https://arxiv.org/abs/2509.20214)</code><input type="text"></li>
<li><strong>Keywords: </strong>large language model</a></li>
<li><strong>Abstract: </strong>We study weight-only post-training quantization (PTQ), which quantizes the weights of a large language model (LLM) without retraining, using little or no calibration data. Weight-only PTQ is crucial for reducing the memory footprint and latency of LLM inference, especially in memory-bound, small-batch inference scenarios, such as personalized inference on edge devices. Despite its importance, irregular weight distributions with heavy-tailed outliers in LLMs complicate quantization, recently motivating rotation-based methods that transform weights into near-Gaussian distributions, which are more regular with fewer outliers, thereby reducing quantization error. In this work, we first derive the information-theoretically optimal bit allocation for Gaussianized weights under given bit budgets, revealing that fine-grained fractional-bit quantizers approaching the Gaussian distortion-rate bound are essential to achieve near-optimal quantization performance. To bridge this theoretical insight and practical implementation, we introduce Q-Palette, a versatile collection of fractional-bit quantizers that range from trellis-coded quantizers offering near-optimal distortion to simpler vector and scalar quantizers optimized for faster inference, all efficiently implemented with optimized CUDA kernels across various bitwidths. Furthermore, leveraging Q-Palette as a foundational component, we propose a novel mixed-scheme quantization framework, jointly optimizing quantizer choices and layer fusion decisions given resource constraints. The code is available at this https URL.</li>
</ul>

<h3>Title: Beyond Sharp Minima: Robust LLM Unlearning via Feedback-Guided Multi-Point Optimization</h3>
<ul>
<li><strong>Authors: </strong>Wenhan Wu, Zheyuan Liu, Chongyang Gao, Ren Wang, Kaize Ding</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20230">https://arxiv.org/abs/2509.20230</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20230">https://arxiv.org/pdf/2509.20230</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20230]] Beyond Sharp Minima: Robust LLM Unlearning via Feedback-Guided Multi-Point Optimization(https://arxiv.org/abs/2509.20230)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack, robust</a></li>
<li><strong>Abstract: </strong>Current LLM unlearning methods face a critical security vulnerability that undermines their fundamental purpose: while they appear to successfully remove sensitive or harmful knowledge, this ``forgotten" information remains precariously recoverable through relearning attacks. We identify that the root cause is that conventional methods optimizing the forgetting loss at individual data points will drive model parameters toward sharp minima in the loss landscape. In these unstable regions, even minimal parameter perturbations can drastically alter the model's behaviors. Consequently, relearning attacks exploit this vulnerability by using just a few fine-tuning samples to navigate the steep gradients surrounding these unstable regions, thereby rapidly recovering knowledge that was supposedly erased. This exposes a critical robustness gap between apparent unlearning and actual knowledge removal. To address this issue, we propose StableUN, a bi-level feedback-guided optimization framework that explicitly seeks more stable parameter regions via neighborhood-aware optimization. It integrates forgetting feedback, which uses adversarial perturbations to probe parameter neighborhoods, with remembering feedback to preserve model utility, aligning the two objectives through gradient projection. Experiments on WMDP and MUSE benchmarks demonstrate that our method is significantly more robust against both relearning and jailbreaking attacks while maintaining competitive utility performance.</li>
</ul>

<h3>Title: Investigating the Representation of Backchannels and Fillers in Fine-tuned Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yu Wang, Leyi Lao, Langchu Huang, Gabriel Skantze, Yang Xu, Hendrik Buschmeier</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20237">https://arxiv.org/abs/2509.20237</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20237">https://arxiv.org/pdf/2509.20237</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20237]] Investigating the Representation of Backchannels and Fillers in Fine-tuned Language Models(https://arxiv.org/abs/2509.20237)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Backchannels and fillers are important linguistic expressions in dialogue, but are under-represented in modern transformer-based language models (LMs). Our work studies the representation of them in language models using three fine-tuning strategies. The models are trained on three dialogue corpora in English and Japanese, where backchannels and fillers are preserved and annotated, to investigate how fine-tuning can help LMs learn their representations. We first apply clustering analysis to the learnt representation of backchannels and fillers, and have found increased silhouette scores in representations from fine-tuned models, which suggests that fine-tuning enables LMs to distinguish the nuanced semantic variation in different backchannel and filler use. We also use natural language generation (NLG) metrics to confirm that the utterances generated by fine-tuned language models resemble human-produced utterances more closely. Our findings suggest the potentials of transforming general LMs into conversational LMs that are more capable of producing human-like languages adequately.</li>
</ul>

<h3>Title: A HyperGraphMamba-Based Multichannel Adaptive Model for ncRNA Classification</h3>
<ul>
<li><strong>Authors: </strong>Xin An, Ruijie Li, Qiao Ning, Hui Li, Qian Ma, Shikai Guo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20240">https://arxiv.org/abs/2509.20240</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20240">https://arxiv.org/pdf/2509.20240</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20240]] A HyperGraphMamba-Based Multichannel Adaptive Model for ncRNA Classification(https://arxiv.org/abs/2509.20240)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, extraction, transformer</a></li>
<li><strong>Abstract: </strong>Non-coding RNAs (ncRNAs) play pivotal roles in gene expression regulation and the pathogenesis of various diseases. Accurate classification of ncRNAs is essential for functional annotation and disease diagnosis. To address existing limitations in feature extraction depth and multimodal fusion, we propose HGMamba-ncRNA, a HyperGraphMamba-based multichannel adaptive model, which integrates sequence, secondary structure, and optionally available expression features of ncRNAs to enhance classification performance. Specifically, the sequence of ncRNA is modeled using a parallel Multi-scale Convolution and LSTM architecture (MKC-L) to capture both local patterns and long-range dependencies of nucleotides. The structure modality employs a multi-scale graph transformer (MSGraphTransformer) to represent the multi-level topological characteristics of ncRNA secondary structures. The expression modality utilizes a Chebyshev Polynomial-based Kolmogorov-Arnold Network (CPKAN) to effectively model and interpret high-dimensional expression profiles. Finally, by incorporating virtual nodes to facilitate efficient and comprehensive multimodal interaction, HyperGraphMamba is proposed to adaptively align and integrate multichannel heterogeneous modality features. Experiments conducted on three public datasets demonstrate that HGMamba-ncRNA consistently outperforms state-of-the-art methods in terms of accuracy and other metrics. Extensive empirical studies further confirm the model's robustness, effectiveness, and strong transferability, offering a novel and reliable strategy for complex ncRNA functional classification. Code and datasets are available at this https URL.</li>
</ul>

<h3>Title: Dynamic Lagging for Time-Series Forecasting in E-Commerce Finance: Mitigating Information Loss with A Hybrid ML Architecture</h3>
<ul>
<li><strong>Authors: </strong>Abhishek Sharma, Anat Parush, Sumit Wadhwa, Amihai Savir, Anne Guinard, Prateek Srivastava</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20244">https://arxiv.org/abs/2509.20244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20244">https://arxiv.org/pdf/2509.20244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20244]] Dynamic Lagging for Time-Series Forecasting in E-Commerce Finance: Mitigating Information Loss with A Hybrid ML Architecture(https://arxiv.org/abs/2509.20244)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer</a></li>
<li><strong>Abstract: </strong>Accurate forecasting in the e-commerce finance domain is particularly challenging due to irregular invoice schedules, payment deferrals, and user-specific behavioral variability. These factors, combined with sparse datasets and short historical windows, limit the effectiveness of conventional time-series methods. While deep learning and Transformer-based models have shown promise in other domains, their performance deteriorates under partial observability and limited historical data. To address these challenges, we propose a hybrid forecasting framework that integrates dynamic lagged feature engineering and adaptive rolling-window representations with classical statistical models and ensemble learners. Our approach explicitly incorporates invoice-level behavioral modeling, structured lag of support data, and custom stability-aware loss functions, enabling robust forecasts in sparse and irregular financial settings. Empirical results demonstrate an approximate 5% reduction in MAPE compared to baseline models, translating into substantial financial savings. Furthermore, the framework enhances forecast stability over quarterly horizons and strengthens feature target correlation by capturing both short- and long-term patterns, leveraging user profile attributes, and simulating upcoming invoice behaviors. These findings underscore the value of combining structured lagging, invoice-level closure modeling, and behavioral insights to advance predictive accuracy in sparse financial time-series forecasting.</li>
</ul>

<h3>Title: 4D Driving Scene Generation With Stereo Forcing</h3>
<ul>
<li><strong>Authors: </strong>Hao Lu, Zhuang Ma, Guangfeng Jiang, Wenhang Ge, Bohan Li, Yuzhan Cai, Wenzhao Zheng, Yunpeng Zhang, Yingcong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20251">https://arxiv.org/abs/2509.20251</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20251">https://arxiv.org/pdf/2509.20251</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20251]] 4D Driving Scene Generation With Stereo Forcing(https://arxiv.org/abs/2509.20251)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Current generative models struggle to synthesize dynamic 4D driving scenes that simultaneously support temporal extrapolation and spatial novel view synthesis (NVS) without per-scene optimization. Bridging generation and novel view synthesis remains a major challenge. We present PhiGenesis, a unified framework for 4D scene generation that extends video generation techniques with geometric and temporal consistency. Given multi-view image sequences and camera parameters, PhiGenesis produces temporally continuous 4D Gaussian splatting representations along target 3D trajectories. In its first stage, PhiGenesis leverages a pre-trained video VAE with a novel range-view adapter to enable feed-forward 4D reconstruction from multi-view images. This architecture supports single-frame or video inputs and outputs complete 4D scenes including geometry, semantics, and motion. In the second stage, PhiGenesis introduces a geometric-guided video diffusion model, using rendered historical 4D scenes as priors to generate future views conditioned on trajectories. To address geometric exposure bias in novel views, we propose Stereo Forcing, a novel conditioning strategy that integrates geometric uncertainty during denoising. This method enhances temporal coherence by dynamically adjusting generative influence based on uncertainty-aware perturbations. Our experimental results demonstrate that our method achieves state-of-the-art performance in both appearance and geometric reconstruction, temporal generation and novel view synthesis (NVS) tasks, while simultaneously delivering competitive performance in downstream evaluations. Homepage is at \href{this https URL}{PhiGensis}.</li>
</ul>

<h3>Title: Predictive Coding-based Deep Neural Network Fine-tuning for Computationally Efficient Domain Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Matteo Cardoni, Sam Leroux</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20269">https://arxiv.org/abs/2509.20269</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20269">https://arxiv.org/pdf/2509.20269</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20269]] Predictive Coding-based Deep Neural Network Fine-tuning for Computationally Efficient Domain Adaptation(https://arxiv.org/abs/2509.20269)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>As deep neural networks are increasingly deployed in dynamic, real-world environments, relying on a single static model is often insufficient. Changes in input data distributions caused by sensor drift or lighting variations necessitate continual model adaptation. In this paper, we propose a hybrid training methodology that enables efficient on-device domain adaptation by combining the strengths of Backpropagation and Predictive Coding. The method begins with a deep neural network trained offline using Backpropagation to achieve high initial performance. Subsequently, Predictive Coding is employed for online adaptation, allowing the model to recover accuracy lost due to shifts in the input data distribution. This approach leverages the robustness of Backpropagation for initial representation learning and the computational efficiency of Predictive Coding for continual learning, making it particularly well-suited for resource-constrained edge devices or future neuromorphic accelerators. Experimental results on the MNIST and CIFAR-10 datasets demonstrate that this hybrid strategy enables effective adaptation with a reduced computational overhead, offering a promising solution for maintaining model performance in dynamic environments.</li>
</ul>

<h3>Title: A Versatile Foundation Model for AI-enabled Mammogram Interpretation</h3>
<ul>
<li><strong>Authors: </strong>Fuxiang Huang, Jiayi Zhu, Yunfang Yu, Yu Xie, Yuan Guo, Qingcong Kong, Mingxiang Wu, Xinrui Jiang, Shu Yang, Jiabo Ma, Ziyi Liu, Zhe Xu, Zhixuan Chen, Yujie Tan, Zifan He, Luhui Mao, Xi Wang, Junlin Hou, Lei Zhang, Qiong Luo, Zhenhui Li, Herui Yao, Hao Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20271">https://arxiv.org/abs/2509.20271</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20271">https://arxiv.org/pdf/2509.20271</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20271]] A Versatile Foundation Model for AI-enabled Mammogram Interpretation(https://arxiv.org/abs/2509.20271)</code><input type="text"></li>
<li><strong>Keywords: </strong>segmentation</a></li>
<li><strong>Abstract: </strong>Breast cancer is the most commonly diagnosed cancer and the leading cause of cancer-related mortality in women globally. Mammography is essential for the early detection and diagnosis of breast lesions. Despite recent progress in foundation models (FMs) for mammogram analysis, their clinical translation remains constrained by several fundamental limitations, including insufficient diversity in training data, limited model generalizability, and a lack of comprehensive evaluation across clinically relevant tasks. Here, we introduce VersaMammo, a versatile foundation model for mammograms, designed to overcome these limitations. We curated the largest multi-institutional mammogram dataset to date, comprising 706,239 images from 21 sources. To improve generalization, we propose a two-stage pre-training strategy to develop VersaMammo, a mammogram foundation model. First, a teacher model is trained via self-supervised learning to extract transferable features from unlabeled mammograms. Then, supervised learning combined with knowledge distillation transfers both features and clinical knowledge into VersaMammo. To ensure a comprehensive evaluation, we established a benchmark comprising 92 specific tasks, including 68 internal tasks and 24 external validation tasks, spanning 5 major clinical task categories: lesion detection, segmentation, classification, image retrieval, and visual question answering. VersaMammo achieves state-of-the-art performance, ranking first in 50 out of 68 specific internal tasks and 20 out of 24 external validation tasks, with average ranks of 1.5 and 1.2, respectively. These results demonstrate its superior generalization and clinical utility, offering a substantial advancement toward reliable and scalable breast cancer screening and diagnosis.</li>
</ul>

<h3>Title: Investigating Security Implications of Automatically Generated Code on the Software Supply Chain</h3>
<ul>
<li><strong>Authors: </strong>Xiaofan Li, Xing Gao</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20277">https://arxiv.org/abs/2509.20277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20277">https://arxiv.org/pdf/2509.20277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20277]] Investigating Security Implications of Automatically Generated Code on the Software Supply Chain(https://arxiv.org/abs/2509.20277)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security, defense, attack, large language model</a></li>
<li><strong>Abstract: </strong>In recent years, various software supply chain (SSC) attacks have posed significant risks to the global community. Severe consequences may arise if developers integrate insecure code snippets that are vulnerable to SSC attacks into their products. Particularly, code generation techniques, such as large language models (LLMs), have been widely utilized in the developer community. However, LLMs are known to suffer from inherent issues when generating code, including fabrication, misinformation, and reliance on outdated training data, all of which can result in serious software supply chain threats. In this paper, we investigate the security threats to the SSC that arise from these inherent issues. We examine three categories of threats, including eleven potential SSC-related threats, related to external components in source code, and continuous integration configuration files. We find some threats in LLM-generated code could enable attackers to hijack software and workflows, while some others might cause potential hidden threats that compromise the security of the software over time. To understand these security impacts and severity, we design a tool, SSCGuard, to generate 439,138 prompts based on SSC-related questions collected online, and analyze the responses of four popular LLMs from GPT and Llama. Our results show that all identified SSC-related threats persistently exist. To mitigate these risks, we propose a novel prompt-based defense mechanism, namely Chain-of-Confirmation, to reduce fabrication, and a middleware-based defense that informs users of various SSC threats.</li>
</ul>

<h3>Title: A co-evolving agentic AI system for medical imaging analysis</h3>
<ul>
<li><strong>Authors: </strong>Songhao Li, Jonathan Xu, Tiancheng Bao, Yuxuan Liu, Yuchen Liu, Yihang Liu, Lilin Wang, Wenhui Lei, Sheng Wang, Yinuo Xu, Yan Cui, Jialu Yao, Shunsuke Koga, Zhi Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20279">https://arxiv.org/abs/2509.20279</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20279">https://arxiv.org/pdf/2509.20279</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20279]] A co-evolving agentic AI system for medical imaging analysis(https://arxiv.org/abs/2509.20279)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Agentic AI is rapidly advancing in healthcare and biomedical research. However, in medical image analysis, their performance and adoption remain limited due to the lack of a robust ecosystem, insufficient toolsets, and the absence of real-time interactive expert feedback. Here we present "TissueLab", a co-evolving agentic AI system that allows researchers to ask direct questions, automatically plan and generate explainable workflows, and conduct real-time analyses where experts can visualize intermediate results and refine them. TissueLab integrates tool factories across pathology, radiology, and spatial omics domains. By standardizing inputs, outputs, and capabilities of diverse tools, the system determines when and how to invoke them to address research and clinical questions. Across diverse tasks with clinically meaningful quantifications that inform staging, prognosis, and treatment planning, TissueLab achieves state-of-the-art performance compared with end-to-end vision-language models (VLMs) and other agentic AI systems such as GPT-5. Moreover, TissueLab continuously learns from clinicians, evolving toward improved classifiers and more effective decision strategies. With active learning, it delivers accurate results in unseen disease contexts within minutes, without requiring massive datasets or prolonged retraining. Released as a sustainable open-source ecosystem, TissueLab aims to accelerate computational research and translational adoption in medical imaging while establishing a foundation for the next generation of medical AI.</li>
</ul>

<h3>Title: HiPerformer: A High-Performance Global-Local Segmentation Model with Modular Hierarchical Fusion Strategy</h3>
<ul>
<li><strong>Authors: </strong>Dayu Tan, Zhenpeng Xu, Yansen Su, Xin Peng, Chunhou Zheng, Weimin Zhong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20280">https://arxiv.org/abs/2509.20280</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20280">https://arxiv.org/pdf/2509.20280</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20280]] HiPerformer: A High-Performance Global-Local Segmentation Model with Modular Hierarchical Fusion Strategy(https://arxiv.org/abs/2509.20280)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, transformer, segmentation</a></li>
<li><strong>Abstract: </strong>Both local details and global context are crucial in medical image segmentation, and effectively integrating them is essential for achieving high accuracy. However, existing mainstream methods based on CNN-Transformer hybrid architectures typically employ simple feature fusion techniques such as serial stacking, endpoint concatenation, or pointwise addition, which struggle to address the inconsistencies between features and are prone to information conflict and loss. To address the aforementioned challenges, we innovatively propose HiPerformer. The encoder of HiPerformer employs a novel modular hierarchical architecture that dynamically fuses multi-source features in parallel, enabling layer-wise deep integration of heterogeneous information. The modular hierarchical design not only retains the independent modeling capability of each branch in the encoder, but also ensures sufficient information transfer between layers, effectively avoiding the degradation of features and information loss that come with traditional stacking methods. Furthermore, we design a Local-Global Feature Fusion (LGFF) module to achieve precise and efficient integration of local details and global semantic information, effectively alleviating the feature inconsistency problem and resulting in a more comprehensive feature representation. To further enhance multi-scale feature representation capabilities and suppress noise interference, we also propose a Progressive Pyramid Aggregation (PPA) module to replace traditional skip connections. Experiments on eleven public datasets demonstrate that the proposed method outperforms existing segmentation techniques, demonstrating higher segmentation accuracy and robustness. The code is available at this https URL.</li>
</ul>

<h3>Title: PerFace: Metric Learning in Perceptual Facial Similarity for Enhanced Face Anonymization</h3>
<ul>
<li><strong>Authors: </strong>Haruka Kumagai, Leslie Wöhler, Satoshi Ikehata, Kiyoharu Aizawa</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20281">https://arxiv.org/abs/2509.20281</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20281">https://arxiv.org/pdf/2509.20281</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20281]] PerFace: Metric Learning in Perceptual Facial Similarity for Enhanced Face Anonymization(https://arxiv.org/abs/2509.20281)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>In response to rising societal awareness of privacy concerns, face anonymization techniques have advanced, including the emergence of face-swapping methods that replace one identity with another. Achieving a balance between anonymity and naturalness in face swapping requires careful selection of identities: overly similar faces compromise anonymity, while dissimilar ones reduce naturalness. Existing models, however, focus on binary identity classification "the same person or not", making it difficult to measure nuanced similarities such as "completely different" versus "highly similar but different." This paper proposes a human-perception-based face similarity metric, creating a dataset of 6,400 triplet annotations and metric learning to predict the similarity. Experimental results demonstrate significant improvements in both face similarity prediction and attribute-based face classification tasks over existing methods.</li>
</ul>

<h3>Title: Monitoring Violations of Differential Privacy over Time</h3>
<ul>
<li><strong>Authors: </strong>Önder Askin, Tim Kutta, Holger Dette</a></li>
<li><strong>Subjects: </strong>cs.CR, math.ST, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20283">https://arxiv.org/abs/2509.20283</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20283">https://arxiv.org/pdf/2509.20283</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20283]] Monitoring Violations of Differential Privacy over Time(https://arxiv.org/abs/2509.20283)</code><input type="text"></li>
<li><strong>Keywords: </strong>privacy</a></li>
<li><strong>Abstract: </strong>Auditing differential privacy has emerged as an important area of research that supports the design of privacy-preserving mechanisms. Privacy audits help to obtain empirical estimates of the privacy parameter, to expose flawed implementations of algorithms and to compare practical with theoretical privacy guarantees. In this work, we investigate an unexplored facet of privacy auditing: the sustained auditing of a mechanism that can go through changes during its development or deployment. Monitoring the privacy of algorithms over time comes with specific challenges. Running state-of-the-art (static) auditors repeatedly requires excessive sampling efforts, while the reliability of such methods deteriorates over time without proper adjustments. To overcome these obstacles, we present a new monitoring procedure that extracts information from the entire deployment history of the algorithm. This allows us to reduce sampling efforts, while sustaining reliable outcomes of our auditor. We derive formal guarantees with regard to the soundness of our methods and evaluate their performance for important mechanisms from the literature. Our theoretical findings and experiments demonstrate the efficacy of our approach.</li>
</ul>

<h3>Title: PGCLODA: Prompt-Guided Graph Contrastive Learning for Oligopeptide-Infectious Disease Association Prediction</h3>
<ul>
<li><strong>Authors: </strong>Dayu Tan, Jing Chen, Xiaoping Zhou, Yansen Su, Chunhou Zheng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20290">https://arxiv.org/abs/2509.20290</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20290">https://arxiv.org/pdf/2509.20290</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20290]] PGCLODA: Prompt-Guided Graph Contrastive Learning for Oligopeptide-Infectious Disease Association Prediction(https://arxiv.org/abs/2509.20290)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Infectious diseases continue to pose a serious threat to public health, underscoring the urgent need for effective computational approaches to screen novel anti-infective agents. Oligopeptides have emerged as promising candidates in antimicrobial research due to their structural simplicity, high bioavailability, and low susceptibility to resistance. Despite their potential, computational models specifically designed to predict associations between oligopeptides and infectious diseases remain scarce. This study introduces a prompt-guided graph-based contrastive learning framework (PGCLODA) to uncover potential associations. A tripartite graph is constructed with oligopeptides, microbes, and diseases as nodes, incorporating both structural and semantic information. To preserve critical regions during contrastive learning, a prompt-guided graph augmentation strategy is employed to generate meaningful paired views. A dual encoder architecture, integrating Graph Convolutional Network (GCN) and Transformer, is used to jointly capture local and global features. The fused embeddings are subsequently input into a multilayer perceptron (MLP) classifier for final prediction. Experimental results on a benchmark dataset indicate that PGCLODA consistently outperforms state-of-the-art models in AUROC, AUPRC, and accuracy. Ablation and hyperparameter studies confirm the contribution of each module. Case studies further validate the generalization ability of PGCLODA and its potential to uncover novel, biologically relevant associations. These findings offer valuable insights for mechanism-driven discovery and oligopeptide-based drug development. The source code of PGCLODA is available online at this https URL.</li>
</ul>

<h3>Title: FAST: Foreground-aware Diffusion with Accelerated Sampling Trajectory for Segmentation-oriented Anomaly Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Xichen Xu, Yanshu Wang, Jinbao Wang, Xiaoning Lei, Guoyang Xie, Guannan Jiang, Zhichao Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20295">https://arxiv.org/abs/2509.20295</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20295">https://arxiv.org/pdf/2509.20295</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20295]] FAST: Foreground-aware Diffusion with Accelerated Sampling Trajectory for Segmentation-oriented Anomaly Synthesis(https://arxiv.org/abs/2509.20295)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, segmentation</a></li>
<li><strong>Abstract: </strong>Industrial anomaly segmentation relies heavily on pixel-level annotations, yet real-world anomalies are often scarce, diverse, and costly to label. Segmentation-oriented industrial anomaly synthesis (SIAS) has emerged as a promising alternative; however, existing methods struggle to balance sampling efficiency and generation quality. Moreover, most approaches treat all spatial regions uniformly, overlooking the distinct statistical differences between anomaly and background areas. This uniform treatment hinders the synthesis of controllable, structure-specific anomalies tailored for segmentation tasks. In this paper, we propose FAST, a foreground-aware diffusion framework featuring two novel modules: the Anomaly-Informed Accelerated Sampling (AIAS) and the Foreground-Aware Reconstruction Module (FARM). AIAS is a training-free sampling algorithm specifically designed for segmentation-oriented industrial anomaly synthesis, which accelerates the reverse process through coarse-to-fine aggregation and enables the synthesis of state-of-the-art segmentation-oriented anomalies in as few as 10 steps. Meanwhile, FARM adaptively adjusts the anomaly-aware noise within the masked foreground regions at each sampling step, preserving localized anomaly signals throughout the denoising trajectory. Extensive experiments on multiple industrial benchmarks demonstrate that FAST consistently outperforms existing anomaly synthesis methods in downstream segmentation tasks. We release the code at: this https URL.</li>
</ul>

<h3>Title: Graph Variate Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Om Roy, Yashar Moshfeghi, Keith Smith</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20311">https://arxiv.org/abs/2509.20311</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20311">https://arxiv.org/pdf/2509.20311</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20311]] Graph Variate Neural Networks(https://arxiv.org/abs/2509.20311)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Modelling dynamically evolving spatio-temporal signals is a prominent challenge in the Graph Neural Network (GNN) literature. Notably, GNNs assume an existing underlying graph structure. While this underlying structure may not always exist or is derived independently from the signal, a temporally evolving functional network can always be constructed from multi-channel data. Graph Variate Signal Analysis (GVSA) defines a unified framework consisting of a network tensor of instantaneous connectivity profiles against a stable support usually constructed from the signal itself. Building on GVSA and tools from graph signal processing, we introduce Graph-Variate Neural Networks (GVNNs): layers that convolve spatio-temporal signals with a signal-dependent connectivity tensor combining a stable long-term support with instantaneous, data-driven interactions. This design captures dynamic statistical interdependencies at each time step without ad hoc sliding windows and admits an efficient implementation with linear complexity in sequence length. Across forecasting benchmarks, GVNNs consistently outperform strong graph-based baselines and are competitive with widely used sequence models such as LSTMs and Transformers. On EEG motor-imagery classification, GVNNs achieve strong accuracy highlighting their potential for brain-computer interface applications.</li>
</ul>

<h3>Title: Multilingual Hope Speech Detection: A Comparative Study of Logistic Regression, mBERT, and XLM-RoBERTa with Active Learning</h3>
<ul>
<li><strong>Authors: </strong>T. O. Abiola, K. D. Abiodun, O. E. Olumide, O. O. Adebanji, O. Hiram Calvo, Grigori Sidorov</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20315">https://arxiv.org/abs/2509.20315</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20315">https://arxiv.org/pdf/2509.20315</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20315]] Multilingual Hope Speech Detection: A Comparative Study of Logistic Regression, mBERT, and XLM-RoBERTa with Active Learning(https://arxiv.org/abs/2509.20315)</code><input type="text"></li>
<li><strong>Keywords: </strong>transformer</a></li>
<li><strong>Abstract: </strong>Hope speech language that fosters encouragement and optimism plays a vital role in promoting positive discourse online. However, its detection remains challenging, especially in multilingual and low-resource settings. This paper presents a multilingual framework for hope speech detection using an active learning approach and transformer-based models, including mBERT and XLM-RoBERTa. Experiments were conducted on datasets in English, Spanish, German, and Urdu, including benchmark test sets from recent shared tasks. Our results show that transformer models significantly outperform traditional baselines, with XLM-RoBERTa achieving the highest overall accuracy. Furthermore, our active learning strategy maintained strong performance even with small annotated datasets. This study highlights the effectiveness of combining multilingual transformers with data-efficient training strategies for hope speech detection.</li>
</ul>

<h3>Title: SIM-CoT: Supervised Implicit Chain-of-Thought</h3>
<ul>
<li><strong>Authors: </strong>Xilin Wei, Xiaoran Liu, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Jiaqi Wang, Xipeng Qiu, Dahua Lin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20317">https://arxiv.org/abs/2509.20317</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20317">https://arxiv.org/pdf/2509.20317</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20317]] SIM-CoT: Supervised Implicit Chain-of-Thought(https://arxiv.org/abs/2509.20317)</code><input type="text"></li>
<li><strong>Keywords: </strong>interpretability, large language model</a></li>
<li><strong>Abstract: </strong>Implicit Chain-of-Thought (CoT) methods present a promising, token-efficient alternative to explicit CoT reasoning in Large Language Models (LLMs), but a persistent performance gap has limited the application of implicit CoT. We identify a core latent instability issue by scaling the computational budget of implicit CoT approaches: as we increase the number of implicit reasoning tokens to enhance performance, the training process often becomes unstable and collapses. Our analysis reveals that this instability arises from the latent representations becoming homogeneous and losing their semantic diversity, a failure caused by insufficient step-level supervision in existing implicit CoT approaches. To address this issue, we propose SIM-CoT, a plug-and-play training module that introduces step-level supervision to stabilize and enrich the latent reasoning space. Specifically, SIM-CoT employs an auxiliary decoder during training to align each implicit token with its corresponding explicit reasoning step, ensuring that latent states capture distinct and meaningful information. The proposed auxiliary decoder is removed during inference, preserving the computational efficiency of implicit CoT methods with no added overhead. In addition, the auxiliary decoder affords interpretability of implicit reasoning by projecting each latent token onto an explicit reasoning vocabulary, enabling per-step visualization of semantic roles and diagnosis. SIM-CoT significantly enhances both the in-domain accuracy and out-of-domain stability of various implicit CoT methods, boosting baselines like Coconut by +8.2% on GPT-2 and CODI by +3.0% on LLaMA-3.1 8B. Demonstrating strong scalability, SIM-CoT also surpasses the explicit CoT baseline on GPT-2 by 2.1% with 2.3\times greater token efficiency, while substantially closing the performance gap on larger models like LLaMA-3.1 8B.</li>
</ul>

<h3>Title: Z-Scores: A Metric for Linguistically Assessing Disfluency Removal</h3>
<ul>
<li><strong>Authors: </strong>Maria Teleki, Sai Janjur, Haoran Liu, Oliver Grabner, Ketan Verma, Thomas Docog, Xiangjue Dong, Lingfeng Shi, Cong Wang, Stephanie Birkelbach, Jason Kim, Yin Zhang, James Caverlee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20319">https://arxiv.org/abs/2509.20319</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20319">https://arxiv.org/pdf/2509.20319</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20319]] Z-Scores: A Metric for Linguistically Assessing Disfluency Removal(https://arxiv.org/abs/2509.20319)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Evaluating disfluency removal in speech requires more than aggregate token-level scores. Traditional word-based metrics such as precision, recall, and F1 (E-Scores) capture overall performance but cannot reveal why models succeed or fail. We introduce Z-Scores, a span-level linguistically-grounded evaluation metric that categorizes system behavior across distinct disfluency types (EDITED, INTJ, PRN). Our deterministic alignment module enables robust mapping between generated text and disfluent transcripts, allowing Z-Scores to expose systematic weaknesses that word-level metrics obscure. By providing category-specific diagnostics, Z-Scores enable researchers to identify model failure modes and design targeted interventions -- such as tailored prompts or data augmentation -- yielding measurable performance improvements. A case study with LLMs shows that Z-Scores uncover challenges with INTJ and PRN disfluencies hidden in aggregate F1, directly informing model refinement strategies.</li>
</ul>

<h3>Title: DRES: Benchmarking LLMs for Disfluency Removal</h3>
<ul>
<li><strong>Authors: </strong>Maria Teleki, Sai Janjur, Haoran Liu, Oliver Grabner, Ketan Verma, Thomas Docog, Xiangjue Dong, Lingfeng Shi, Cong Wang, Stephanie Birkelbach, Jason Kim, Yin Zhang, James Caverlee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20321">https://arxiv.org/abs/2509.20321</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20321">https://arxiv.org/pdf/2509.20321</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20321]] DRES: Benchmarking LLMs for Disfluency Removal(https://arxiv.org/abs/2509.20321)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust, segmentation</a></li>
<li><strong>Abstract: </strong>Disfluencies -- such as "um," "uh," interjections, parentheticals, and edited statements -- remain a persistent challenge for speech-driven systems, degrading accuracy in command interpretation, summarization, and conversational agents. We introduce DRES (Disfluency Removal Evaluation Suite), a controlled text-level benchmark that establishes a reproducible semantic upper bound for this task. DRES builds on human-annotated Switchboard transcripts, isolating disfluency removal from ASR errors and acoustic variability. We systematically evaluate proprietary and open-source LLMs across scales, prompting strategies, and architectures. Our results reveal that (i) simple segmentation consistently improves performance, even for long-context models; (ii) reasoning-oriented models tend to over-delete fluent tokens; and (iii) fine-tuning achieves near state-of-the-art precision and recall but harms generalization abilities. We further present a set of LLM-specific error modes and offer nine practical recommendations (R1-R9) for deploying disfluency removal in speech-driven pipelines. DRES provides a reproducible, model-agnostic foundation for advancing robust spoken-language systems.</li>
</ul>

<h3>Title: RAG Security and Privacy: Formalizing the Threat Model and Attack Surface</h3>
<ul>
<li><strong>Authors: </strong>Atousa Arzanipour, Rouzbeh Behnia, Reza Ebrahimi, Kaushik Dutta</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20324">https://arxiv.org/abs/2509.20324</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20324">https://arxiv.org/pdf/2509.20324</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20324]] RAG Security and Privacy: Formalizing the Threat Model and Attack Surface(https://arxiv.org/abs/2509.20324)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, privacy, attack, membership infer, large language model</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Generation (RAG) is an emerging approach in natural language processing that combines large language models (LLMs) with external document retrieval to produce more accurate and grounded responses. While RAG has shown strong potential in reducing hallucinations and improving factual consistency, it also introduces new privacy and security challenges that differ from those faced by traditional LLMs. Existing research has demonstrated that LLMs can leak sensitive information through training data memorization or adversarial prompts, and RAG systems inherit many of these vulnerabilities. At the same time, reliance of RAG on an external knowledge base opens new attack surfaces, including the potential for leaking information about the presence or content of retrieved documents, or for injecting malicious content to manipulate model behavior. Despite these risks, there is currently no formal framework that defines the threat landscape for RAG systems. In this paper, we address a critical gap in the literature by proposing, to the best of our knowledge, the first formal threat model for retrieval-RAG systems. We introduce a structured taxonomy of adversary types based on their access to model components and data, and we formally define key threat vectors such as document-level membership inference and data poisoning, which pose serious privacy and integrity risks in real-world deployments. By establishing formal definitions and attack models, our work lays the foundation for a more rigorous and principled understanding of privacy and security in RAG systems.</li>
</ul>

<h3>Title: Video models are zero-shot learners and reasoners</h3>
<ul>
<li><strong>Authors: </strong>Thaddäus Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, Robert Geirhos</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20328">https://arxiv.org/abs/2509.20328</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20328">https://arxiv.org/pdf/2509.20328</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20328]] Video models are zero-shot learners and reasoners(https://arxiv.org/abs/2509.20328)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, large language model</a></li>
<li><strong>Abstract: </strong>The remarkable zero-shot capabilities of Large Language Models (LLMs) have propelled natural language processing from task-specific models to unified, generalist foundation models. This transformation emerged from simple primitives: large, generative models trained on web-scale data. Curiously, the same primitives apply to today's generative video models. Could video models be on a trajectory towards general-purpose vision understanding, much like LLMs developed general-purpose language understanding? We demonstrate that Veo 3 can solve a broad variety of tasks it wasn't explicitly trained for: segmenting objects, detecting edges, editing images, understanding physical properties, recognizing object affordances, simulating tool use, and more. These abilities to perceive, model, and manipulate the visual world enable early forms of visual reasoning like maze and symmetry solving. Veo's emergent zero-shot capabilities indicate that video models are on a path to becoming unified, generalist vision foundation models.</li>
</ul>

<h3>Title: Uncovering Graph Reasoning in Decoder-only Transformers with Circuit Tracing</h3>
<ul>
<li><strong>Authors: </strong>Xinnan Dai, Chung-Hsiang Lo, Kai Guo, Shenglai Zeng, Dongsheng Luo, Jiliang Tang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20336">https://arxiv.org/abs/2509.20336</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20336">https://arxiv.org/pdf/2509.20336</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20336]] Uncovering Graph Reasoning in Decoder-only Transformers with Circuit Tracing(https://arxiv.org/abs/2509.20336)</code><input type="text"></li>
<li><strong>Keywords: </strong>extraction, interpretability, transformer</a></li>
<li><strong>Abstract: </strong>Transformer-based LLMs demonstrate strong performance on graph reasoning tasks, yet their internal mechanisms remain underexplored. To uncover these reasoning process mechanisms in a fundamental and unified view, we set the basic decoder-only transformers and explain them using the circuit-tracer framework. Through this lens, we visualize reasoning traces and identify two core mechanisms in graph reasoning: token merging and structural memorization, which underlie both path reasoning and substructure extraction tasks. We further quantify these behaviors and analyze how they are influenced by graph density and model size. Our study provides a unified interpretability framework for understanding structural reasoning in decoder-only Transformers.</li>
</ul>

<h3>Title: Spatio-Temporal Directed Graph Learning for Account Takeover Fraud Detection</h3>
<ul>
<li><strong>Authors: </strong>Mohsen Nayebi Kerdabadi, William Andrew Byron, Xin Sun, Amirfarrokh Iranitalab</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20339">https://arxiv.org/abs/2509.20339</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20339">https://arxiv.org/pdf/2509.20339</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20339]] Spatio-Temporal Directed Graph Learning for Account Takeover Fraud Detection(https://arxiv.org/abs/2509.20339)</code><input type="text"></li>
<li><strong>Keywords: </strong>attack</a></li>
<li><strong>Abstract: </strong>Account Takeover (ATO) fraud poses a significant challenge in consumer banking, requiring high recall under strict latency while minimizing friction for legitimate users. Production systems typically rely on tabular gradient-boosted decision trees (e.g., XGBoost) that score sessions independently, overlooking the relational and temporal structure of online activity that characterizes coordinated attacks and "fraud rings." We introduce ATLAS (Account Takeover Learning Across Spatio-Temporal Directed Graph), a framework that reformulates ATO detection as spatio-temporal node classification on a time-respecting directed session graph. ATLAS links entities via shared identifiers (account, device, IP) and regulates connectivity with time-window and recency constraints, enabling causal, time-respecting message passing and latency-aware label propagation that uses only labels available at scoring time, non-anticipative and leakage-free. We operationalize ATLAS with inductive GraphSAGE variants trained via neighbor sampling, at scale on a sessions graph with more than 100M nodes and around 1B edges. On a high-risk digital product at Capital One, ATLAS delivers 6.38 percent AUC improvement and more than 50 percent reduction in customer friction, improving fraud capture while reducing user friction.</li>
</ul>

<h3>Title: Process-Informed Forecasting of Complex Thermal Dynamics in Pharmaceutical Manufacturing</h3>
<ul>
<li><strong>Authors: </strong>Ramona Rubini, Siavash Khodakarami, Aniruddha Bora, George Em Karniadakis, Michele Dassisti</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20349">https://arxiv.org/abs/2509.20349</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20349">https://arxiv.org/pdf/2509.20349</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20349]] Process-Informed Forecasting of Complex Thermal Dynamics in Pharmaceutical Manufacturing(https://arxiv.org/abs/2509.20349)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Accurate time-series forecasting for complex physical systems is the backbone of modern industrial monitoring and control. While deep learning models excel at capturing complex dynamics, currently, their deployment is limited due to physical inconsistency and robustness, hence constraining their reliability in regulated environments. We introduce process-informed forecasting (PIF) models for temperature in pharmaceutical lyophilization. We investigate a wide range of models, from classical ones such as Autoregressive Integrated Moving Average Model (ARIMA) and Exponential Smoothing Model (ETS), to modern deep learning architectures, including Kolmogorov-Arnold Networks (KANs). We compare three different loss function formulations that integrate a process-informed trajectory prior: a fixed-weight loss, a dynamic uncertainty-based loss, and a Residual-Based Attention (RBA) mechanism. We evaluate all models not only for accuracy and physical consistency but also for robustness to sensor noise. Furthermore, we test the practical generalizability of the best model in a transfer learning scenario on a new process. Our results show that PIF models outperform their data-driven counterparts in terms of accuracy, physical plausibility and noise resilience. This work provides a roadmap for developing reliable and generalizable forecasting solutions for critical applications in the pharmaceutical manufacturing landscape.</li>
</ul>

<h3>Title: EmbeddingGemma: Powerful and Lightweight Text Representations</h3>
<ul>
<li><strong>Authors: </strong>Henrique Schechter Vera, Sahil Dua, Biao Zhang, Daniel Salz, Ryan Mullins, Sindhu Raghuram Panyam, Sara Smoot, Iftekhar Naim, Joe Zou, Feiyang Chen, Daniel Cer, Alice Lisak, Min Choi, Lucas Gonzalez, Omar Sanseviero, Glenn Cameron, Ian Ballantyne, Kat Black, Kaifeng Chen, Weiyi Wang, Zhe Li, Gus Martins, Jinhyuk Lee, Mark Sherwood, Juyeong Ji, Renjie Wu, Jingxiao Zheng, Jyotinder Singh, Abheesht Sharma, Divya Sreepat, Aashi Jain, Adham Elarabawy, AJ Co, Andreas Doumanoglou, Babak Samari, Ben Hora, Brian Potetz, Dahun Kim, Enrique Alfonseca, Fedor Moiseev, Feng Han, Frank Palma Gomez, Gustavo Hernández Ábrego, Hesen Zhang, Hui Hui, Jay Han, Karan Gill, Ke Chen, Koert Chen, Madhuri Shanbhogue, Michael Boratko, Paul Suganthan, Sai Meher Karthik Duddu, Sandeep Mariserla, Setareh Ariafar, Shanfeng Zhang, Shijie Zhang, Simon Baumgartner, Sonam Goenka, Steve Qiu, Tanmaya Dabral, Trevor Walker, Vikram Rao, Waleed Khawaja, Wenlei Zhou, Xiaoqi Ren, Ye Xia, Yichang Chen, Yi-Ting Chen, Zhe Dong, Zhongli Ding, Francesco Visin, Gaël Liu, Jiageng Zhang, Kathleen Kenealy, Michelle Casbon, Ravin Kumar, Thomas Mesnard, Zach Gleicher, Cormac Brick, Olivier Lacombe, Adam Roberts, Yunhsuan Sung, Raphael Hoffmann, Tris Warkentin, Armand Joulin, Tom Duerig, Mojtaba Seyedhosseini</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20354">https://arxiv.org/abs/2509.20354</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20354">https://arxiv.org/pdf/2509.20354</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20354]] EmbeddingGemma: Powerful and Lightweight Text Representations(https://arxiv.org/abs/2509.20354)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>We introduce EmbeddingGemma, a new lightweight, open text embedding model based on the Gemma 3 language model family. Our innovative training recipe strategically captures knowledge from larger models via encoder-decoder initialization and geometric embedding distillation. We improve model robustness and expressiveness with a spread-out regularizer, and ensure generalizability by merging checkpoints from varied, optimized mixtures. Evaluated on the Massive Text Embedding Benchmark (MTEB) across multilingual, English, and code domains, EmbeddingGemma (300M) achieves state-of-the-art results. Notably, it outperforms prior top models, both proprietary and open, with fewer than 500M parameters, and provides performance comparable to models double its size, offering an exceptional performance-to-cost ratio. Remarkably, this lead persists when quantizing model weights or truncating embedding outputs. This makes EmbeddingGemma particularly well-suited for low-latency and high-throughput use cases such as on-device applications. We provide ablation studies exploring our key design choices. We release EmbeddingGemma to the community to promote further research.</li>
</ul>

<h3>Title: chainScale: Secure Functionality-oriented Scalability for Decentralized Resource Markets</h3>
<ul>
<li><strong>Authors: </strong>Mohamed E. Najd, Ghada Almashaqbeh</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20356">https://arxiv.org/abs/2509.20356</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20356">https://arxiv.org/pdf/2509.20356</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20356]] chainScale: Secure Functionality-oriented Scalability for Decentralized Resource Markets(https://arxiv.org/abs/2509.20356)</code><input type="text"></li>
<li><strong>Keywords: </strong>secure, security</a></li>
<li><strong>Abstract: </strong>Decentralized resource markets are Web 3.0 applications that build open-access platforms for trading digital resources among users without any central management. They promise cost reduction, transparency, and flexible service provision. However, these markets usually have large workload that must be processed in a timely manner, leading to serious scalability problems. Despite the large amount of work on blockchain scalability, existing solutions are ineffective as they do not account for these markets' work models and traffic patterns. We introduce chainScale, a secure hybrid sidechain-sharding solution that aims to boost throughput of decentralized resource markets and reduce their latency and storage footprint. At its core, chainScale leverages dependent sidechains and functionality-oriented workload splitting to parallelize traffic processing by having each market module assigned to a sidechain. Different from sharding, chainScale does not incur any cross-sidechain transactions that tend to be costly. chainScale introduces several techniques, including hierarchical workload sharing that further sub-divides overloaded modules, and weighted miner assignment that assigns miners with vested interest in the system to critical modules' sidechains. Furthermore, chainScale employs sidechain syncing to maintain the mainchain as the single truth of system state, and pruning to discard stale records. Beside analyzing security, we build a proof-of-concept implementation for a distributed file storage market as a use case. Our experiments show that, compared to a single sidechain-based prior solution, chainScale boosts throughput by 4x and reduces confirmation latency by 5x. Also, they show that chainScale outperforms sharding by 2.5x in throughput and 3.5x in latency.</li>
</ul>

<h3>Title: PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Chen Wang, Chuhao Chen, Yiming Huang, Zhiyang Dou, Yuan Liu, Jiatao Gu, Lingjie Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20358">https://arxiv.org/abs/2509.20358</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20358">https://arxiv.org/pdf/2509.20358</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20358]] PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video Generation(https://arxiv.org/abs/2509.20358)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Existing video generation models excel at producing photo-realistic videos from text or images, but often lack physical plausibility and 3D controllability. To overcome these limitations, we introduce PhysCtrl, a novel framework for physics-grounded image-to-video generation with physical parameters and force control. At its core is a generative physics network that learns the distribution of physical dynamics across four materials (elastic, sand, plasticine, and rigid) via a diffusion model conditioned on physics parameters and applied forces. We represent physical dynamics as 3D point trajectories and train on a large-scale synthetic dataset of 550K animations generated by physics simulators. We enhance the diffusion model with a novel spatiotemporal attention block that emulates particle interactions and incorporates physics-based constraints during training to enforce physical plausibility. Experiments show that PhysCtrl generates realistic, physics-grounded motion trajectories which, when used to drive image-to-video models, yield high-fidelity, controllable videos that outperform existing methods in both visual quality and physical plausibility. Project Page: this https URL</li>
</ul>

<h3>Title: EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Xuan Ju, Tianyu Wang, Yuqian Zhou, He Zhang, Qing Liu, Nanxuan Zhao, Zhifei Zhang, Yijun Li, Yuanhao Cai, Shaoteng Liu, Daniil Pakhomov, Zhe Lin, Soo Ye Kim, Qiang Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20360">https://arxiv.org/abs/2509.20360</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20360">https://arxiv.org/pdf/2509.20360</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20360]] EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning(https://arxiv.org/abs/2509.20360)</code><input type="text"></li>
<li><strong>Keywords: </strong>robust</a></li>
<li><strong>Abstract: </strong>Recent advances in foundation models highlight a clear trend toward unification and scaling, showing emergent capabilities across diverse domains. While image generation and editing have rapidly transitioned from task-specific to unified frameworks, video generation and editing remain fragmented due to architectural limitations and data scarcity. In this work, we introduce EditVerse, a unified framework for image and video generation and editing within a single model. By representing all modalities, i.e., text, image, and video, as a unified token sequence, EditVerse leverages self-attention to achieve robust in-context learning, natural cross-modal knowledge transfer, and flexible handling of inputs and outputs with arbitrary resolutions and durations. To address the lack of video editing training data, we design a scalable data pipeline that curates 232K video editing samples and combines them with large-scale image and video datasets for joint training. Furthermore, we present EditVerseBench, the first benchmark for instruction-based video editing covering diverse tasks and resolutions. Extensive experiments and user studies demonstrate that EditVerse achieves state-of-the-art performance, surpassing existing open-source and commercial models, while exhibiting emergent editing and generation abilities across modalities.</li>
</ul>

<h3>Title: FlyTrap: Physical Distance-Pulling Attack Towards Camera-based Autonomous Target Tracking Systems</h3>
<ul>
<li><strong>Authors: </strong>Shaoyuan Xie, Mohamad Habib Fakih, Junchi Lu, Fayzah Alshammari, Ningfei Wang, Takami Sato, Halima Bouzidi, Mohammad Abdullah Al Faruque, Qi Alfred Chen</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.20362">https://arxiv.org/abs/2509.20362</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.20362">https://arxiv.org/pdf/2509.20362</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.20362]] FlyTrap: Physical Distance-Pulling Attack Towards Camera-based Autonomous Target Tracking Systems(https://arxiv.org/abs/2509.20362)</code><input type="text"></li>
<li><strong>Keywords: </strong>security, attack</a></li>
<li><strong>Abstract: </strong>Autonomous Target Tracking (ATT) systems, especially ATT drones, are widely used in applications such as surveillance, border control, and law enforcement, while also being misused in stalking and destructive actions. Thus, the security of ATT is highly critical for real-world applications. Under the scope, we present a new type of attack: distance-pulling attacks (DPA) and a systematic study of it, which exploits vulnerabilities in ATT systems to dangerously reduce tracking distances, leading to drone capturing, increased susceptibility to sensor attacks, or even physical collisions. To achieve these goals, we present FlyTrap, a novel physical-world attack framework that employs an adversarial umbrella as a deployable and domain-specific attack vector. FlyTrap is specifically designed to meet key desired objectives in attacking ATT drones: physical deployability, closed-loop effectiveness, and spatial-temporal consistency. Through novel progressive distance-pulling strategy and controllable spatial-temporal consistency designs, FlyTrap manipulates ATT drones in real-world setups to achieve significant system-level impacts. Our evaluations include new datasets, metrics, and closed-loop experiments on real-world white-box and even commercial ATT drones, including DJI and HoverAir. Results demonstrate FlyTrap's ability to reduce tracking distances within the range to be captured, sensor attacked, or even directly crashed, highlighting urgent security risks and practical implications for the safe deployment of ATT systems.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
