<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>secure</h2>
<h3>Title: A Multi-Factor Homomorphic Encryption based Method for Authenticated Access to IoT Devices. (arXiv:2307.03291v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03291">http://arxiv.org/abs/2307.03291</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03291] A Multi-Factor Homomorphic Encryption based Method for Authenticated Access to IoT Devices](http://arxiv.org/abs/2307.03291) #secure</code></li>
<li>Summary: <p>Authentication is the first defence mechanism in many electronic systems,
including Internet of Things (IoT) applications, as it is essential for other
security services such as intrusion detection. As existing authentication
solutions proposed for IoT environments do not provide multi-level
authentication assurance, particularly for device-to-device authentication
scenarios, we recently proposed the M2I (Multi-Factor Multi-Level and
Interaction based Authentication) framework to facilitate multi-factor
authentication of devices in device-to-device and device-to-multiDevice
interactions. In this paper, we extend the framework to address group
authentication. Two Many-to-One (M2O) protocols are proposed, the Hybrid Group
Authentication and Key Acquisition (HGAKA) protocol and the Hybrid Group Access
(HGA) protocol. The protocols use a combination of symmetric and asymmetric
cryptographic primitives to facilitate multifactor group authentication. The
informal analysis and formal security verification show that the protocols
satisfy the desirable security requirements and are secure against
authentication attacks.
</p></li>
</ul>

<h2>security</h2>
<h3>Title: Undecimated Wavelet Transform for Word Embedded Semantic Marginal Autoencoder in Security improvement and Denoising different Languages. (arXiv:2307.03679v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03679">http://arxiv.org/abs/2307.03679</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03679] Undecimated Wavelet Transform for Word Embedded Semantic Marginal Autoencoder in Security improvement and Denoising different Languages](http://arxiv.org/abs/2307.03679) #security</code></li>
<li>Summary: <p>By combining the undecimated wavelet transform within a Word Embedded
Semantic Marginal Autoencoder (WESMA), this research study provides a novel
strategy for improving security measures and denoising multiple languages. The
incorporation of these strategies is intended to address the issues of
robustness, privacy, and multilingualism in data processing applications. The
undecimated wavelet transform is used as a feature extraction tool to identify
prominent language patterns and structural qualities in the input data. The
proposed system may successfully capture significant information while
preserving the temporal and geographical links within the data by employing
this transform. This improves security measures by increasing the system's
ability to detect abnormalities, discover hidden patterns, and distinguish
between legitimate content and dangerous threats. The Word Embedded Semantic
Marginal Autoencoder also functions as an intelligent framework for
dimensionality and noise reduction. The autoencoder effectively learns the
underlying semantics of the data and reduces noise components by exploiting
word embeddings and semantic context. As a result, data quality and accuracy
are increased in following processing stages. The suggested methodology is
tested using a diversified dataset that includes several languages and security
scenarios. The experimental results show that the proposed approach is
effective in attaining security enhancement and denoising capabilities across
multiple languages. The system is strong in dealing with linguistic variances,
producing consistent outcomes regardless of the language used. Furthermore,
incorporating the undecimated wavelet transform considerably improves the
system's ability to efficiently address complex security concerns
</p></li>
</ul>

<h3>Title: Unveiling the Potential of Knowledge-Prompted ChatGPT for Enhancing Drug Trafficking Detection on Social Media. (arXiv:2307.03699v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03699">http://arxiv.org/abs/2307.03699</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03699] Unveiling the Potential of Knowledge-Prompted ChatGPT for Enhancing Drug Trafficking Detection on Social Media](http://arxiv.org/abs/2307.03699) #security</code></li>
<li>Summary: <p>Social media platforms such as Instagram and Twitter have emerged as critical
channels for drug marketing and illegal sale. Detecting and labeling online
illicit drug trafficking activities becomes important in addressing this issue.
However, the effectiveness of conventional supervised learning methods in
detecting drug trafficking heavily relies on having access to substantial
amounts of labeled data, while data annotation is time-consuming and
resource-intensive. Furthermore, these models often face challenges in
accurately identifying trafficking activities when drug dealers use deceptive
language and euphemisms to avoid detection. To overcome this limitation, we
conduct the first systematic study on leveraging large language models (LLMs),
such as ChatGPT, to detect illicit drug trafficking activities on social media.
We propose an analytical framework to compose \emph{knowledge-informed
prompts}, which serve as the interface that humans can interact with and use
LLMs to perform the detection task. Additionally, we design a Monte Carlo
dropout based prompt optimization method to further to improve performance and
interpretability. Our experimental findings demonstrate that the proposed
framework outperforms other baseline language models in terms of drug
trafficking detection accuracy, showing a remarkable improvement of nearly
12\%. By integrating prior knowledge and the proposed prompts, ChatGPT can
effectively identify and label drug trafficking activities on social networks,
even in the presence of deceptive language and euphemisms used by drug dealers
to evade detection. The implications of our research extend to social networks,
emphasizing the importance of incorporating prior knowledge and scenario-based
prompts into analytical tools to improve online security and public safety.
</p></li>
</ul>

<h3>Title: Exploring Encrypted Keyboards to Defeat Client-Side Scanning in End-to-End Encryption Systems. (arXiv:2307.03426v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03426">http://arxiv.org/abs/2307.03426</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03426] Exploring Encrypted Keyboards to Defeat Client-Side Scanning in End-to-End Encryption Systems](http://arxiv.org/abs/2307.03426) #security</code></li>
<li>Summary: <p>End-to-End Encryption (E2EE) aims to make all messages impossible to read by
anyone except you and your intended recipient(s). Many well-known and widely
used Instant-Messaging (IM) applications (such as Signal, WhatsApp, and Apple's
iMessage) claim to provide E2EE. However, a recent technique called client-side
scanning (CSS) makes these E2EE claims grandiose and hollow promises. The CSS
is a technology that scans all sending and receiving messages from one end to
the other. Some in industry and government now advocate this CSS technology to
combat the growth of malicious child pornography, terrorism, and other illicit
communication. Even though combating the spread of illegal and morally
objectionable content is a laudable effort, it may open further backdoors that
impact the user's privacy and security. Therefore, it is not E2EE when there
are censorship mechanisms and backdoors in end-to-end encrypted applications.
In this paper, we introduce an encrypted keyboard that functions as a system
keyboard, enabling users to employ it across all applications on their phones
when entering data. By utilizing this encrypted keyboard, users can locally
encrypt and decrypt messages, effectively bypassing the CSS system. We first
design and implement our encrypted keyboard as a custom keyboard application,
and then we evaluate the effectiveness and security of our encrypted keyboard.
Our study results show that our encrypted keyboard can successfully encrypt and
decrypt all sending and receiving messages through IM applications, and
therefore, it can successfully defeat the CSS technology in end-to-end
encrypted systems. We also show that our encrypted keyboard can be used to add
another layer of E2EE functionality on top of the existing E2EE functionality
implemented by many end-to-end encrypted applications.
</p></li>
</ul>

<h2>privacy</h2>
<h3>Title: Improving Bitswap Privacy with Forwarding and Source Obfuscation. (arXiv:2307.03480v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03480">http://arxiv.org/abs/2307.03480</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03480] Improving Bitswap Privacy with Forwarding and Source Obfuscation](http://arxiv.org/abs/2307.03480) #privacy</code></li>
<li>Summary: <p>IPFS is a content-addressed decentralized peer-to-peer data network, using
the Bitswap protocol for exchanging data. The data exchange leaks the
information to all neighbors, compromising a user's privacy. This paper
investigates the suitability of forwarding with source obfuscation techniques
for improving the privacy of the Bitswap protocol. The usage of forwarding can
add plausible deniability and the source obfuscation provides additional
protection against passive observers. First results showed that through
trickle-spreading the source prediction could decrease to 40 %, at the cost of
an increased content fetching time. However, assuming short distances between
content provider and consumer the content fetching time can be faster even with
the additional source obfuscation.
</p></li>
</ul>

<h3>Title: Random Number Generators and Seeding for Differential Privacy. (arXiv:2307.03543v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03543">http://arxiv.org/abs/2307.03543</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03543] Random Number Generators and Seeding for Differential Privacy](http://arxiv.org/abs/2307.03543) #privacy</code></li>
<li>Summary: <p>Differential Privacy (DP) relies on random numbers to preserve privacy,
typically utilising Pseudorandom Number Generators (PRNGs) as a source of
randomness. In order to allow for consistent reproducibility, testing and
bug-fixing in DP algorithms and results, it is important to allow for the
seeding of the PRNGs used therein. In this work, we examine the landscape of
Random Number Generators (RNGs), and the considerations software engineers
should make when choosing and seeding a PRNG for DP. We hope it serves as a
suitable guide for DP practitioners, and includes many lessons learned when
implementing seeding for diffprivlib.
</p></li>
</ul>

<h3>Title: Programmable Synthetic Tabular Data Generation. (arXiv:2307.03577v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03577">http://arxiv.org/abs/2307.03577</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03577] Programmable Synthetic Tabular Data Generation](http://arxiv.org/abs/2307.03577) #privacy</code></li>
<li>Summary: <p>Large amounts of tabular data remain underutilized due to privacy, data
quality, and data sharing limitations. While training a generative model
producing synthetic data resembling the original distribution addresses some of
these issues, most applications require additional constraints from the
generated data. Existing synthetic data approaches are limited as they
typically only handle specific constraints, e.g., differential privacy (DP) or
increased fairness, and lack an accessible interface for declaring general
specifications. In this work, we introduce ProgSyn, the first programmable
synthetic tabular data generation algorithm that allows for comprehensive
customization over the generated data. To ensure high data quality while
adhering to custom specifications, ProgSyn pre-trains a generative model on the
original dataset and fine-tunes it on a differentiable loss automatically
derived from the provided specifications. These can be programmatically
declared using statistical and logical expressions, supporting a wide range of
requirements (e.g., DP or fairness, among others). We conduct an extensive
experimental evaluation of ProgSyn on a number of constraints, achieving a new
state-of-the-art on some, while remaining general. For instance, at the same
fairness level we achieve 2.3% higher downstream accuracy than the
state-of-the-art in fair synthetic data generation on the Adult dataset.
Overall, ProgSyn provides a versatile and accessible framework for generating
constrained synthetic tabular data, allowing for specifications that generalize
beyond the capabilities of prior work.
</p></li>
</ul>

<h2>protect</h2>
<h3>Title: When Fair Classification Meets Noisy Protected Attributes. (arXiv:2307.03306v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03306">http://arxiv.org/abs/2307.03306</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03306] When Fair Classification Meets Noisy Protected Attributes](http://arxiv.org/abs/2307.03306) #protect</code></li>
<li>Summary: <p>The operationalization of algorithmic fairness comes with several practical
challenges, not the least of which is the availability or reliability of
protected attributes in datasets. In real-world contexts, practical and legal
impediments may prevent the collection and use of demographic data, making it
difficult to ensure algorithmic fairness. While initial fairness algorithms did
not consider these limitations, recent proposals aim to achieve algorithmic
fairness in classification by incorporating noisiness in protected attributes
or not using protected attributes at all.
</p></li>
</ul>

<p>To the best of our knowledge, this is the first head-to-head study of fair
classification algorithms to compare attribute-reliant, noise-tolerant and
attribute-blind algorithms along the dual axes of predictivity and fairness. We
evaluated these algorithms via case studies on four real-world datasets and
synthetic perturbations. Our study reveals that attribute-blind and
noise-tolerant fair classifiers can potentially achieve similar level of
performance as attribute-reliant algorithms, even when protected attributes are
noisy. However, implementing them in practice requires careful nuance. Our
study provides insights into the practical implications of using fair
classification algorithms in scenarios where protected attributes are noisy or
partially available.
</p>

<h2>defense</h2>
<h2>attack</h2>
<h3>Title: Scalable Membership Inference Attacks via Quantile Regression. (arXiv:2307.03694v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03694">http://arxiv.org/abs/2307.03694</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03694] Scalable Membership Inference Attacks via Quantile Regression](http://arxiv.org/abs/2307.03694) #attack</code></li>
<li>Summary: <p>Membership inference attacks are designed to determine, using black box
access to trained models, whether a particular example was used in training or
not. Membership inference can be formalized as a hypothesis testing problem.
The most effective existing attacks estimate the distribution of some test
statistic (usually the model's confidence on the true label) on points that
were (and were not) used in training by training many \emph{shadow models} --
i.e. models of the same architecture as the model being attacked, trained on a
random subsample of data. While effective, these attacks are extremely
computationally expensive, especially when the model under attack is large.
</p></li>
</ul>

<p>We introduce a new class of attacks based on performing quantile regression
on the distribution of confidence scores induced by the model under attack on
points that are not used in training. We show that our method is competitive
with state-of-the-art shadow model attacks, while requiring substantially less
compute because our attack requires training only a single model. Moreover,
unlike shadow model attacks, our proposed attack does not require any knowledge
of the architecture of the model under attack and is therefore truly
``black-box". We show the efficacy of this approach in an extensive series of
experiments on various datasets and model architectures.
</p>

<h3>Title: Analyzing the vulnerabilities in SplitFed Learning: Assessing the robustness against Data Poisoning Attacks. (arXiv:2307.03197v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03197">http://arxiv.org/abs/2307.03197</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03197] Analyzing the vulnerabilities in SplitFed Learning: Assessing the robustness against Data Poisoning Attacks](http://arxiv.org/abs/2307.03197) #attack</code></li>
<li>Summary: <p>Distributed Collaborative Machine Learning (DCML) is a potential alternative
to address the privacy concerns associated with centralized machine learning.
The Split learning (SL) and Federated Learning (FL) are the two effective
learning approaches in DCML. Recently there have been an increased interest on
the hybrid of FL and SL known as the SplitFed Learning (SFL). This research is
the earliest attempt to study, analyze and present the impact of data poisoning
attacks in SFL. We propose three kinds of novel attack strategies namely
untargeted, targeted and distance-based attacks for SFL. All the attacks
strategies aim to degrade the performance of the DCML-based classifier. We test
the proposed attack strategies for two different case studies on
Electrocardiogram signal classification and automatic handwritten digit
recognition. A series of attack experiments were conducted by varying the
percentage of malicious clients and the choice of the model split layer between
the clients and the server. The results after the comprehensive analysis of
attack strategies clearly convey that untargeted and distance-based poisoning
attacks have greater impacts in evading the classifier outcomes compared to
targeted attacks in SFL
</p></li>
</ul>

<h3>Title: A Vulnerability of Attribution Methods Using Pre-Softmax Scores. (arXiv:2307.03305v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03305">http://arxiv.org/abs/2307.03305</a></li>
<li>Code URL: <a href="https://github.com/mlerma54/adversarial-attacks-on-saliency-maps">https://github.com/mlerma54/adversarial-attacks-on-saliency-maps</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03305] A Vulnerability of Attribution Methods Using Pre-Softmax Scores](http://arxiv.org/abs/2307.03305) #attack</code></li>
<li>Summary: <p>We discuss a vulnerability involving a category of attribution methods used
to provide explanations for the outputs of convolutional neural networks
working as classifiers. It is known that this type of networks are vulnerable
to adversarial attacks, in which imperceptible perturbations of the input may
alter the outputs of the model. In contrast, here we focus on effects that
small modifications in the model may cause on the attribution method without
altering the model outputs.
</p></li>
</ul>

<h3>Title: Machine Learning to detect cyber-attacks and discriminating the types of power system disturbances. (arXiv:2307.03323v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03323">http://arxiv.org/abs/2307.03323</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03323] Machine Learning to detect cyber-attacks and discriminating the types of power system disturbances](http://arxiv.org/abs/2307.03323) #attack</code></li>
<li>Summary: <p>This research proposes a machine learning-based attack detection model for
power systems, specifically targeting smart grids. By utilizing data and logs
collected from Phasor Measuring Devices (PMUs), the model aims to learn system
behaviors and effectively identify potential security boundaries. The proposed
approach involves crucial stages including dataset pre-processing, feature
selection, model creation, and evaluation. To validate our approach, we used a
dataset used, consist of 15 separate datasets obtained from different PMUs,
relay snort alarms and logs. Three machine learning models: Random Forest,
Logistic Regression, and K-Nearest Neighbour were built and evaluated using
various performance metrics. The findings indicate that the Random Forest model
achieves the highest performance with an accuracy of 90.56% in detecting power
system disturbances and has the potential in assisting operators in
decision-making processes.
</p></li>
</ul>

<h2>robust</h2>
<h3>Title: RCDN -- Robust X-Corner Detection Algorithm based on Advanced CNN Model. (arXiv:2307.03505v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03505">http://arxiv.org/abs/2307.03505</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03505] RCDN -- Robust X-Corner Detection Algorithm based on Advanced CNN Model](http://arxiv.org/abs/2307.03505) #robust</code></li>
<li>Summary: <p>Accurate detection and localization of X-corner on both planar and non-planar
patterns is a core step in robotics and machine vision. However, previous works
could not make a good balance between accuracy and robustness, which are both
crucial criteria to evaluate the detectors performance. To address this
problem, in this paper we present a novel detection algorithm which can
maintain high sub-pixel precision on inputs under multiple interference, such
as lens distortion, extreme poses and noise. The whole algorithm, adopting a
coarse-to-fine strategy, contains a X-corner detection network and three
post-processing techniques to distinguish the correct corner candidates, as
well as a mixed sub-pixel refinement technique and an improved region growth
strategy to recover the checkerboard pattern partially visible or occluded
automatically. Evaluations on real and synthetic images indicate that the
presented algorithm has the higher detection rate, sub-pixel accuracy and
robustness than other commonly used methods. Finally, experiments of camera
calibration and pose estimation verify it can also get smaller re-projection
error in quantitative comparisons to the state-of-the-art.
</p></li>
</ul>

<h3>Title: Matching in the Wild: Learning Anatomical Embeddings for Multi-Modality Images. (arXiv:2307.03535v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03535">http://arxiv.org/abs/2307.03535</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03535] Matching in the Wild: Learning Anatomical Embeddings for Multi-Modality Images](http://arxiv.org/abs/2307.03535) #robust</code></li>
<li>Summary: <p>Radiotherapists require accurate registration of MR/CT images to effectively
use information from both modalities. In a typical registration pipeline, rigid
or affine transformations are applied to roughly align the fixed and moving
images before proceeding with the deformation step. While recent learning-based
methods have shown promising results in the rigid/affine step, these methods
often require images with similar field-of-view (FOV) for successful alignment.
As a result, aligning images with different FOVs remains a challenging task.
Self-supervised landmark detection methods like self-supervised Anatomical
eMbedding (SAM) have emerged as a useful tool for mapping and cropping images
to similar FOVs. However, these methods are currently limited to intra-modality
use only. To address this limitation and enable cross-modality matching, we
propose a new approach called Cross-SAM. Our approach utilizes a novel
iterative process that alternates between embedding learning and CT-MRI
registration. We start by applying aggressive contrast augmentation on both CT
and MRI images to train a SAM model. We then use this SAM to identify
corresponding regions on paired images using robust grid-points matching,
followed by a point-set based affine/rigid registration, and a deformable
fine-tuning step to produce registered paired images. We use these registered
pairs to enhance the matching ability of SAM, which is then processed
iteratively. We use the final model for cross-modality matching tasks. We
evaluated our approach on two CT-MRI affine registration datasets and found
that Cross-SAM achieved robust affine registration on both datasets,
significantly outperforming other methods and achieving state-of-the-art
performance.
</p></li>
</ul>

<h3>Title: VariGrad: A Novel Feature Vector Architecture for Geometric Deep Learning on Unregistered Data. (arXiv:2307.03553v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03553">http://arxiv.org/abs/2307.03553</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03553] VariGrad: A Novel Feature Vector Architecture for Geometric Deep Learning on Unregistered Data](http://arxiv.org/abs/2307.03553) #robust</code></li>
<li>Summary: <p>We present a novel geometric deep learning layer that leverages the varifold
gradient (VariGrad) to compute feature vector representations of 3D geometric
data. These feature vectors can be used in a variety of downstream learning
tasks such as classification, registration, and shape reconstruction. Our
model's use of parameterization independent varifold representations of
geometric data allows our model to be both trained and tested on data
independent of the given sampling or parameterization. We demonstrate the
efficiency, generalizability, and robustness to resampling demonstrated by the
proposed VariGrad layer.
</p></li>
</ul>

<h3>Title: Robust Human Detection under Visual Degradation via Thermal and mmWave Radar Fusion. (arXiv:2307.03623v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03623">http://arxiv.org/abs/2307.03623</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03623] Robust Human Detection under Visual Degradation via Thermal and mmWave Radar Fusion](http://arxiv.org/abs/2307.03623) #robust</code></li>
<li>Summary: <p>The majority of human detection methods rely on the sensor using visible
lights (e.g., RGB cameras) but such sensors are limited in scenarios with
degraded vision conditions. In this paper, we present a multimodal human
detection system that combines portable thermal cameras and single-chip mmWave
radars. To mitigate the noisy detection features caused by the low contrast of
thermal cameras and the multi-path noise of radar point clouds, we propose a
Bayesian feature extractor and a novel uncertainty-guided fusion method that
surpasses a variety of competing methods, either single-modal or multi-modal.
We evaluate the proposed method on real-world data collection and demonstrate
that our approach outperforms the state-of-the-art methods by a large margin.
</p></li>
</ul>

<h3>Title: BiPhone: Modeling Inter Language Phonetic Influences in Text. (arXiv:2307.03322v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03322">http://arxiv.org/abs/2307.03322</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03322] BiPhone: Modeling Inter Language Phonetic Influences in Text](http://arxiv.org/abs/2307.03322) #robust</code></li>
<li>Summary: <p>A large number of people are forced to use the Web in a language they have
low literacy in due to technology asymmetries. Written text in the second
language (L2) from such users often contains a large number of errors that are
influenced by their native language (L1). We propose a method to mine phoneme
confusions (sounds in L2 that an L1 speaker is likely to conflate) for pairs of
L1 and L2. These confusions are then plugged into a generative model (Bi-Phone)
for synthetically producing corrupted L2 text. Through human evaluations, we
show that Bi-Phone generates plausible corruptions that differ across L1s and
also have widespread coverage on the Web. We also corrupt the popular language
understanding benchmark SuperGLUE with our technique (FunGLUE for Phonetically
Noised GLUE) and show that SoTA language understating models perform poorly. We
also introduce a new phoneme prediction pre-training task which helps byte
models to recover performance close to SuperGLUE. Finally, we also release the
FunGLUE benchmark to promote further research in phonetically robust language
models. To the best of our knowledge, FunGLUE is the first benchmark to
introduce L1-L2 interactions in text.
</p></li>
</ul>

<h3>Title: Mitigating Negative Transfer with Task Awareness for Sexism, Hate Speech, and Toxic Language Detection. (arXiv:2307.03377v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03377">http://arxiv.org/abs/2307.03377</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03377] Mitigating Negative Transfer with Task Awareness for Sexism, Hate Speech, and Toxic Language Detection](http://arxiv.org/abs/2307.03377) #robust</code></li>
<li>Summary: <p>This paper proposes a novelty approach to mitigate the negative transfer
problem. In the field of machine learning, the common strategy is to apply the
Single-Task Learning approach in order to train a supervised model to solve a
specific task. Training a robust model requires a lot of data and a significant
amount of computational resources, making this solution unfeasible in cases
where data are unavailable or expensive to gather. Therefore another solution,
based on the sharing of information between tasks, has been developed:
Multi-Task Learning (MTL). Despite the recent developments regarding MTL, the
problem of negative transfer has still to be solved. Negative transfer is a
phenomenon that occurs when noisy information is shared between tasks,
resulting in a drop in performance. This paper proposes a new approach to
mitigate the negative transfer problem based on the task awareness concept. The
proposed approach results in diminishing the negative transfer together with an
improvement of performance over classic MTL solution. Moreover, the proposed
approach has been implemented in two unified architectures to detect Sexism,
Hate Speech, and Toxic Language in text comments. The proposed architectures
set a new state-of-the-art both in EXIST-2021 and HatEval-2019 benchmarks.
</p></li>
</ul>

<h3>Title: Testing the Predictions of Surprisal Theory in 11 Languages. (arXiv:2307.03667v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03667">http://arxiv.org/abs/2307.03667</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03667] Testing the Predictions of Surprisal Theory in 11 Languages](http://arxiv.org/abs/2307.03667) #robust</code></li>
<li>Summary: <p>A fundamental result in psycholinguistics is that less predictable words take
a longer time to process. One theoretical explanation for this finding is
Surprisal Theory (Hale, 2001; Levy, 2008), which quantifies a word's
predictability as its surprisal, i.e. its negative log-probability given a
context. While evidence supporting the predictions of Surprisal Theory have
been replicated widely, most have focused on a very narrow slice of data:
native English speakers reading English texts. Indeed, no comprehensive
multilingual analysis exists. We address this gap in the current literature by
investigating the relationship between surprisal and reading times in eleven
different languages, distributed across five language families. Deriving
estimates from language models trained on monolingual and multilingual corpora,
we test three predictions associated with surprisal theory: (i) whether
surprisal is predictive of reading times; (ii) whether expected surprisal, i.e.
contextual entropy, is predictive of reading times; (iii) and whether the
linking function between surprisal and reading times is linear. We find that
all three predictions are borne out crosslinguistically. By focusing on a more
diverse set of languages, we argue that these results offer the most robust
link to-date between information theory and incremental language processing
across languages.
</p></li>
</ul>

<h3>Title: MALIBO: Meta-learning for Likelihood-free Bayesian Optimization. (arXiv:2307.03565v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03565">http://arxiv.org/abs/2307.03565</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03565] MALIBO: Meta-learning for Likelihood-free Bayesian Optimization](http://arxiv.org/abs/2307.03565) #robust</code></li>
<li>Summary: <p>Bayesian optimization (BO) is a popular method to optimize costly black-box
functions. While traditional BO optimizes each new target task from scratch,
meta-learning has emerged as a way to leverage knowledge from related tasks to
optimize new tasks faster. However, existing meta-learning BO methods rely on
surrogate models that suffer from scalability issues and are sensitive to
observations with different scales and noise types across tasks. Moreover, they
often overlook the uncertainty associated with task similarity. This leads to
unreliable task adaptation when only limited observations are obtained or when
the new tasks differ significantly from the related tasks. To address these
limitations, we propose a novel meta-learning BO approach that bypasses the
surrogate model and directly learns the utility of queries across tasks. Our
method explicitly models task uncertainty and includes an auxiliary model to
enable robust adaptation to new tasks. Extensive experiments show that our
method demonstrates strong anytime performance and outperforms state-of-the-art
meta-learning BO methods in various benchmarks.
</p></li>
</ul>

<h2>biometric</h2>
<h3>Title: Facial Landmark Detection Evaluation on MOBIO Database. (arXiv:2307.03329v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03329">http://arxiv.org/abs/2307.03329</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03329] Facial Landmark Detection Evaluation on MOBIO Database](http://arxiv.org/abs/2307.03329) #biometric</code></li>
<li>Summary: <p>MOBIO is a bi-modal database that was captured almost exclusively on mobile
phones. It aims to improve research into deploying biometric techniques to
mobile devices. Research has been shown that face and speaker recognition can
be performed in a mobile environment. Facial landmark localization aims at
finding the coordinates of a set of pre-defined key points for 2D face images.
A facial landmark usually has specific semantic meaning, e.g. nose tip or eye
centre, which provides rich geometric information for other face analysis tasks
such as face recognition, emotion estimation and 3D face reconstruction. Pretty
much facial landmark detection methods adopt still face databases, such as
300W, AFW, AFLW, or COFW, for evaluation, but seldomly use mobile data. Our
work is first to perform facial landmark detection evaluation on the mobile
still data, i.e., face images from MOBIO database. About 20,600 face images
have been extracted from this audio-visual database and manually labeled with
22 landmarks as the groundtruth. Several state-of-the-art facial landmark
detection methods are adopted to evaluate their performance on these data. The
result shows that the data from MOBIO database is pretty challenging. This
database can be a new challenging one for facial landmark detection evaluation.
</p></li>
</ul>

<h2>steal</h2>
<h2>extraction</h2>
<h3>Title: Adaptive Generation of Privileged Intermediate Information for Visible-Infrared Person Re-Identification. (arXiv:2307.03240v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03240">http://arxiv.org/abs/2307.03240</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03240] Adaptive Generation of Privileged Intermediate Information for Visible-Infrared Person Re-Identification](http://arxiv.org/abs/2307.03240) #extraction</code></li>
<li>Summary: <p>Visible-infrared person re-identification seeks to retrieve images of the
same individual captured over a distributed network of RGB and IR sensors.
Several V-I ReID approaches directly integrate both V and I modalities to
discriminate persons within a shared representation space. However, given the
significant gap in data distributions between V and I modalities, cross-modal
V-I ReID remains challenging. Some recent approaches improve generalization by
leveraging intermediate spaces that can bridge V and I modalities, yet
effective methods are required to select or generate data for such informative
domains. In this paper, the Adaptive Generation of Privileged Intermediate
Information training approach is introduced to adapt and generate a virtual
domain that bridges discriminant information between the V and I modalities.
The key motivation behind AGPI^2 is to enhance the training of a deep V-I ReID
backbone by generating privileged images that provide additional information.
These privileged images capture shared discriminative features that are not
easily accessible within the original V or I modalities alone. Towards this
goal, a non-linear generative module is trained with an adversarial objective,
translating V images into intermediate spaces with a smaller domain shift
w.r.t. the I domain. Meanwhile, the embedding module within AGPI^2 aims to
produce similar features for both V and generated images, encouraging the
extraction of features that are common to all modalities. In addition to these
contributions, AGPI^2 employs adversarial objectives for adapting the
intermediate images, which play a crucial role in creating a
non-modality-specific space to address the large domain shifts between V and I
domains. Experimental results conducted on challenging V-I ReID datasets
indicate that AGPI^2 increases matching accuracy without extra computational
resources during inference.
</p></li>
</ul>

<h3>Title: Open-Vocabulary Object Detection via Scene Graph Discovery. (arXiv:2307.03339v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03339">http://arxiv.org/abs/2307.03339</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03339] Open-Vocabulary Object Detection via Scene Graph Discovery](http://arxiv.org/abs/2307.03339) #extraction</code></li>
<li>Summary: <p>In recent years, open-vocabulary (OV) object detection has attracted
increasing research attention. Unlike traditional detection, which only
recognizes fixed-category objects, OV detection aims to detect objects in an
open category set. Previous works often leverage vision-language (VL) training
data (e.g., referring grounding data) to recognize OV objects. However, they
only use pairs of nouns and individual objects in VL data, while these data
usually contain much more information, such as scene graphs, which are also
crucial for OV detection. In this paper, we propose a novel Scene-Graph-Based
Discovery Network (SGDN) that exploits scene graph cues for OV detection.
Firstly, a scene-graph-based decoder (SGDecoder) including sparse
scene-graph-guided attention (SSGA) is presented. It captures scene graphs and
leverages them to discover OV objects. Secondly, we propose scene-graph-based
prediction (SGPred), where we build a scene-graph-based offset regression
(SGOR) mechanism to enable mutual enhancement between scene graph extraction
and object localization. Thirdly, we design a cross-modal learning mechanism in
SGPred. It takes scene graphs as bridges to improve the consistency between
cross-modal embeddings for OV object classification. Experiments on COCO and
LVIS demonstrate the effectiveness of our approach. Moreover, we show the
ability of our model for OV scene graph detection, while previous OV scene
graph generation methods cannot tackle this task.
</p></li>
</ul>

<h3>Title: All in One: Exploring Unified Vision-Language Tracking with Multi-Modal Alignment. (arXiv:2307.03373v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03373">http://arxiv.org/abs/2307.03373</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03373] All in One: Exploring Unified Vision-Language Tracking with Multi-Modal Alignment](http://arxiv.org/abs/2307.03373) #extraction</code></li>
<li>Summary: <p>Current mainstream vision-language (VL) tracking framework consists of three
parts, \ie a visual feature extractor, a language feature extractor, and a
fusion model. To pursue better performance, a natural modus operandi for VL
tracking is employing customized and heavier unimodal encoders, and multi-modal
fusion models. Albeit effective, existing VL trackers separate feature
extraction and feature integration, resulting in extracted features that lack
semantic guidance and have limited target-aware capability in complex
scenarios, \eg similar distractors and extreme illumination. In this work,
inspired by the recent success of exploring foundation models with unified
architecture for both natural language and computer vision tasks, we propose an
All-in-One framework, which learns joint feature extraction and interaction by
adopting a unified transformer backbone. Specifically, we mix raw vision and
language signals to generate language-injected vision tokens, which we then
concatenate before feeding into the unified backbone architecture. This
approach achieves feature integration in a unified backbone, removing the need
for carefully-designed fusion modules and resulting in a more effective and
efficient VL tracking framework. To further improve the learning efficiency, we
introduce a multi-modal alignment module based on cross-modal and intra-modal
contrastive objectives, providing more reasonable representations for the
unified All-in-One transformer backbone. Extensive experiments on five
benchmarks, \ie OTB99-L, TNL2K, LaSOT, LaSOT$_{\rm Ext}$ and WebUAV-3M,
demonstrate the superiority of the proposed tracker against existing
state-of-the-arts on VL tracking. Codes will be made publicly available.
</p></li>
</ul>

<h3>Title: Beyond Geo-localization: Fine-grained Orientation of Street-view Images by Cross-view Matching with Satellite Imagery. (arXiv:2307.03398v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03398">http://arxiv.org/abs/2307.03398</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03398] Beyond Geo-localization: Fine-grained Orientation of Street-view Images by Cross-view Matching with Satellite Imagery](http://arxiv.org/abs/2307.03398) #extraction</code></li>
<li>Summary: <p>Street-view imagery provides us with novel experiences to explore different
places remotely. Carefully calibrated street-view images (e.g. Google Street
View) can be used for different downstream tasks, e.g. navigation, map features
extraction. As personal high-quality cameras have become much more affordable
and portable, an enormous amount of crowdsourced street-view images are
uploaded to the internet, but commonly with missing or noisy sensor
information. To prepare this hidden treasure for "ready-to-use" status,
determining missing location information and camera orientation angles are two
equally important tasks. Recent methods have achieved high performance on
geo-localization of street-view images by cross-view matching with a pool of
geo-referenced satellite imagery. However, most of the existing works focus
more on geo-localization than estimating the image orientation. In this work,
we re-state the importance of finding fine-grained orientation for street-view
images, formally define the problem and provide a set of evaluation metrics to
assess the quality of the orientation estimation. We propose two methods to
improve the granularity of the orientation estimation, achieving 82.4% and
72.3% accuracy for images with estimated angle errors below 2 degrees for CVUSA
and CVACT datasets, corresponding to 34.9% and 28.2% absolute improvement
compared to previous works. Integrating fine-grained orientation estimation in
training also improves the performance on geo-localization, giving top 1 recall
95.5%/85.5% and 86.8%/80.4% for orientation known/unknown tests on the two
datasets.
</p></li>
</ul>

<h3>Title: A Deep Active Contour Model for Delineating Glacier Calving Fronts. (arXiv:2307.03461v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03461">http://arxiv.org/abs/2307.03461</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03461] A Deep Active Contour Model for Delineating Glacier Calving Fronts](http://arxiv.org/abs/2307.03461) #extraction</code></li>
<li>Summary: <p>Choosing how to encode a real-world problem as a machine learning task is an
important design decision in machine learning. The task of glacier calving
front modeling has often been approached as a semantic segmentation task.
Recent studies have shown that combining segmentation with edge detection can
improve the accuracy of calving front detectors. Building on this observation,
we completely rephrase the task as a contour tracing problem and propose a
model for explicit contour detection that does not incorporate any dense
predictions as intermediate steps. The proposed approach, called ``Charting
Outlines by Recurrent Adaptation'' (COBRA), combines Convolutional Neural
Networks (CNNs) for feature extraction and active contour models for the
delineation. By training and evaluating on several large-scale datasets of
Greenland's outlet glaciers, we show that this approach indeed outperforms the
aforementioned methods based on segmentation and edge-detection. Finally, we
demonstrate that explicit contour detection has benefits over pixel-wise
methods when quantifying the models' prediction uncertainties. The project page
containing the code and animated model predictions can be found at
\url{https://khdlr.github.io/COBRA/}.
</p></li>
</ul>

<h2>membership infer</h2>
<h2>federate</h2>
<h3>Title: Federated Unlearning via Active Forgetting. (arXiv:2307.03363v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03363">http://arxiv.org/abs/2307.03363</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03363] Federated Unlearning via Active Forgetting](http://arxiv.org/abs/2307.03363) #federate</code></li>
<li>Summary: <p>The increasing concerns regarding the privacy of machine learning models have
catalyzed the exploration of machine unlearning, i.e., a process that removes
the influence of training data on machine learning models. This concern also
arises in the realm of federated learning, prompting researchers to address the
federated unlearning problem. However, federated unlearning remains
challenging. Existing unlearning methods can be broadly categorized into two
approaches, i.e., exact unlearning and approximate unlearning. Firstly,
implementing exact unlearning, which typically relies on the
partition-aggregation framework, in a distributed manner does not improve time
efficiency theoretically. Secondly, existing federated (approximate) unlearning
methods suffer from imprecise data influence estimation, significant
computational burden, or both. To this end, we propose a novel federated
unlearning framework based on incremental learning, which is independent of
specific models and federated settings. Our framework differs from existing
federated unlearning methods that rely on approximate retraining or data
influence estimation. Instead, we leverage new memories to overwrite old ones,
imitating the process of \textit{active forgetting} in neurology. Specifically,
the model, intended to unlearn, serves as a student model that continuously
learns from randomly initiated teacher models. To preserve catastrophic
forgetting of non-target data, we utilize elastic weight consolidation to
elastically constrain weight change. Extensive experiments on three benchmark
datasets demonstrate the efficiency and effectiveness of our proposed method.
The result of backdoor attacks demonstrates that our proposed method achieves
satisfying completeness.
</p></li>
</ul>

<h3>Title: Incentive Allocation in Vertical Federated Learning Based on Bankruptcy Problem. (arXiv:2307.03515v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03515">http://arxiv.org/abs/2307.03515</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03515] Incentive Allocation in Vertical Federated Learning Based on Bankruptcy Problem](http://arxiv.org/abs/2307.03515) #federate</code></li>
<li>Summary: <p>Vertical federated learning (VFL) is a promising approach for collaboratively
training machine learning models using private data partitioned vertically
across different parties. Ideally in a VFL setting, the active party (party
possessing features of samples with labels) benefits by improving its machine
learning model through collaboration with some passive parties (parties
possessing additional features of the same samples without labels) in a privacy
preserving manner. However, motivating passive parties to participate in VFL
can be challenging. In this paper, we focus on the problem of allocating
incentives to the passive parties by the active party based on their
contributions to the VFL process. We formulate this problem as a variant of the
Nucleolus game theory concept, known as the Bankruptcy Problem, and solve it
using the Talmud's division rule. We evaluate our proposed method on synthetic
and real-world datasets and show that it ensures fairness and stability in
incentive allocation among passive parties who contribute their data to the
federated model. Additionally, we compare our method to the existing solution
of calculating Shapley values and show that our approach provides a more
efficient solution with fewer computations.
</p></li>
</ul>

<h2>fair</h2>
<h2>interpretability</h2>
<h2>explainability</h2>
<h2>watermark</h2>
<h2>diffusion</h2>
<h3>Title: Simulation-free Schr\"odinger bridges via score and flow matching. (arXiv:2307.03672v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03672">http://arxiv.org/abs/2307.03672</a></li>
<li>Code URL: <a href="https://github.com/atong01/conditional-flow-matching">https://github.com/atong01/conditional-flow-matching</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03672] Simulation-free Schr\"odinger bridges via score and flow matching](http://arxiv.org/abs/2307.03672) #diffusion</code></li>
<li>Summary: <p>We present simulation-free score and flow matching ([SF]$^2$M), a
simulation-free objective for inferring stochastic dynamics given unpaired
source and target samples drawn from arbitrary distributions. Our method
generalizes both the score-matching loss used in the training of diffusion
models and the recently proposed flow matching loss used in the training of
continuous normalizing flows. [SF]$^2$M interprets continuous-time stochastic
generative modeling as a Schr\"odinger bridge (SB) problem. It relies on static
entropy-regularized optimal transport, or a minibatch approximation, to
efficiently learn the SB without simulating the learned stochastic process. We
find that [SF]$^2$M is more efficient and gives more accurate solutions to the
SB problem than simulation-based methods from prior work. Finally, we apply
[SF]$^2$M to the problem of learning cell dynamics from snapshot data. Notably,
[SF]$^2$M is the first method to accurately model cell dynamics in high
dimensions and can recover known gene regulatory networks from simulated data.
</p></li>
</ul>

<h2>noise learning</h2>
<h2>data-free</h2>
<h2>transformer</h2>
<h3>Title: Vision Language Transformers: A Survey. (arXiv:2307.03254v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03254">http://arxiv.org/abs/2307.03254</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03254] Vision Language Transformers: A Survey](http://arxiv.org/abs/2307.03254) #transformer</code></li>
<li>Summary: <p>Vision language tasks, such as answering questions about or generating
captions that describe an image, are difficult tasks for computers to perform.
A relatively recent body of research has adapted the pretrained transformer
architecture introduced in \citet{vaswani2017attention} to vision language
modeling. Transformer models have greatly improved performance and versatility
over previous vision language models. They do so by pretraining models on a
large generic datasets and transferring their learning to new tasks with minor
changes in architecture and parameter values. This type of transfer learning
has become the standard modeling practice in both natural language processing
and computer vision. Vision language transformers offer the promise of
producing similar advancements in tasks which require both vision and language.
In this paper, we provide a broad synthesis of the currently available research
on vision language transformer models and offer some analysis of their
strengths, limitations and some open questions that remain.
</p></li>
</ul>

<h3>Title: It is not Sexually Suggestive, It is Educative. Separating Sex Education from Suggestive Content on TikTok Videos. (arXiv:2307.03274v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03274">http://arxiv.org/abs/2307.03274</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03274] It is not Sexually Suggestive, It is Educative](http://arxiv.org/abs/2307.03274) #transformer</code></li>
<li>Summary: <p>We introduce SexTok, a multi-modal dataset composed of TikTok videos labeled
as sexually suggestive (from the annotator's point of view), sex-educational
content, or neither. Such a dataset is necessary to address the challenge of
distinguishing between sexually suggestive content and virtual sex education
videos on TikTok. Children's exposure to sexually suggestive videos has been
shown to have adversarial effects on their development. Meanwhile, virtual sex
education, especially on subjects that are more relevant to the LGBTQIA+
community, is very valuable. The platform's current system removes or penalizes
some of both types of videos, even though they serve different purposes. Our
dataset contains video URLs, and it is also audio transcribed. To validate its
importance, we explore two transformer-based models for classifying the videos.
Our preliminary results suggest that the task of distinguishing between these
types of videos is learnable but challenging. These experiments suggest that
this dataset is meaningful and invites further study on the subject.
</p></li>
</ul>

<h3>Title: General-Purpose Multimodal Transformer meets Remote Sensing Semantic Segmentation. (arXiv:2307.03388v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03388">http://arxiv.org/abs/2307.03388</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03388] General-Purpose Multimodal Transformer meets Remote Sensing Semantic Segmentation](http://arxiv.org/abs/2307.03388) #transformer</code></li>
<li>Summary: <p>The advent of high-resolution multispectral/hyperspectral sensors, LiDAR DSM
(Digital Surface Model) information and many others has provided us with an
unprecedented wealth of data for Earth Observation. Multimodal AI seeks to
exploit those complementary data sources, particularly for complex tasks like
semantic segmentation. While specialized architectures have been developed,
they are highly complicated via significant effort in model design, and require
considerable re-engineering whenever a new modality emerges. Recent trends in
general-purpose multimodal networks have shown great potential to achieve
state-of-the-art performance across multiple multimodal tasks with one unified
architecture. In this work, we investigate the performance of PerceiverIO, one
in the general-purpose multimodal family, in the remote sensing semantic
segmentation domain. Our experiments reveal that this ostensibly universal
network struggles with object scale variation in remote sensing images and
fails to detect the presence of cars from a top-down view. To address these
issues, even with extreme class imbalance issues, we propose a spatial and
volumetric learning component. Specifically, we design a UNet-inspired module
that employs 3D convolution to encode vital local information and learn
cross-modal features simultaneously, while reducing network computational
burden via the cross-attention mechanism of PerceiverIO. The effectiveness of
the proposed component is validated through extensive experiments comparing it
with other methods such as 2D convolution, and dual local module (\ie the
combination of Conv2D 1x1 and Conv2D 3x3 inspired by UNetFormer). The proposed
method achieves competitive results with specialized architectures like
UNetFormer and SwinUNet, showing its potential to minimize network architecture
engineering with a minimal compromise on the performance.
</p></li>
</ul>

<h3>Title: Distilling Self-Supervised Vision Transformers for Weakly-Supervised Few-Shot Classification &amp; Segmentation. (arXiv:2307.03407v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03407">http://arxiv.org/abs/2307.03407</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03407] Distilling Self-Supervised Vision Transformers for Weakly-Supervised Few-Shot Classification &amp; Segmentation](http://arxiv.org/abs/2307.03407) #transformer</code></li>
<li>Summary: <p>We address the task of weakly-supervised few-shot image classification and
segmentation, by leveraging a Vision Transformer (ViT) pretrained with
self-supervision. Our proposed method takes token representations from the
self-supervised ViT and leverages their correlations, via self-attention, to
produce classification and segmentation predictions through separate task
heads. Our model is able to effectively learn to perform classification and
segmentation in the absence of pixel-level labels during training, using only
image-level labels. To do this it uses attention maps, created from tokens
generated by the self-supervised ViT backbone, as pixel-level pseudo-labels. We
also explore a practical setup with ``mixed" supervision, where a small number
of training images contains ground-truth pixel-level labels and the remaining
images have only image-level labels. For this mixed setup, we propose to
improve the pseudo-labels using a pseudo-label enhancer that was trained using
the available ground-truth pixel-level labels. Experiments on Pascal-5i and
COCO-20i demonstrate significant performance gains in a variety of supervision
settings, and in particular when little-to-no pixel-level labels are available.
</p></li>
</ul>

<h3>Title: Non-iterative Coarse-to-fine Transformer Networks for Joint Affine and Deformable Image Registration. (arXiv:2307.03421v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03421">http://arxiv.org/abs/2307.03421</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03421] Non-iterative Coarse-to-fine Transformer Networks for Joint Affine and Deformable Image Registration](http://arxiv.org/abs/2307.03421) #transformer</code></li>
<li>Summary: <p>Image registration is a fundamental requirement for medical image analysis.
Deep registration methods based on deep learning have been widely recognized
for their capabilities to perform fast end-to-end registration. Many deep
registration methods achieved state-of-the-art performance by performing
coarse-to-fine registration, where multiple registration steps were iterated
with cascaded networks. Recently, Non-Iterative Coarse-to-finE (NICE)
registration methods have been proposed to perform coarse-to-fine registration
in a single network and showed advantages in both registration accuracy and
runtime. However, existing NICE registration methods mainly focus on deformable
registration, while affine registration, a common prerequisite, is still
reliant on time-consuming traditional optimization-based methods or extra
affine registration networks. In addition, existing NICE registration methods
are limited by the intrinsic locality of convolution operations. Transformers
may address this limitation for their capabilities to capture long-range
dependency, but the benefits of using transformers for NICE registration have
not been explored. In this study, we propose a Non-Iterative Coarse-to-finE
Transformer network (NICE-Trans) for image registration. Our NICE-Trans is the
first deep registration method that (i) performs joint affine and deformable
coarse-to-fine registration within a single network, and (ii) embeds
transformers into a NICE registration framework to model long-range relevance
between images. Extensive experiments with seven public datasets show that our
NICE-Trans outperforms state-of-the-art registration methods on both
registration accuracy and runtime.
</p></li>
</ul>

<h3>Title: Registration-Free Hybrid Learning Empowers Simple Multimodal Imaging System for High-quality Fusion Detection. (arXiv:2307.03425v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03425">http://arxiv.org/abs/2307.03425</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03425] Registration-Free Hybrid Learning Empowers Simple Multimodal Imaging System for High-quality Fusion Detection](http://arxiv.org/abs/2307.03425) #transformer</code></li>
<li>Summary: <p>Multimodal fusion detection always places high demands on the imaging system
and image pre-processing, while either a high-quality pre-registration system
or image registration processing is costly. Unfortunately, the existing fusion
methods are designed for registered source images, and the fusion of
inhomogeneous features, which denotes a pair of features at the same spatial
location that expresses different semantic information, cannot achieve
satisfactory performance via these methods. As a result, we propose IA-VFDnet,
a CNN-Transformer hybrid learning framework with a unified high-quality
multimodal feature matching module (AKM) and a fusion module (WDAF), in which
AKM and DWDAF work in synergy to perform high-quality infrared-aware visible
fusion detection, which can be applied to smoke and wildfire detection.
Furthermore, experiments on the M3FD dataset validate the superiority of the
proposed method, with IA-VFDnet achieving the best detection performance than
other state-of-the-art methods under conventional registered conditions. In
addition, the first unregistered multimodal smoke and wildfire detection
benchmark is openly available in this letter.
</p></li>
</ul>

<h3>Title: HoughLaneNet: Lane Detection with Deep Hough Transform and Dynamic Convolution. (arXiv:2307.03494v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03494">http://arxiv.org/abs/2307.03494</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03494] HoughLaneNet: Lane Detection with Deep Hough Transform and Dynamic Convolution](http://arxiv.org/abs/2307.03494) #transformer</code></li>
<li>Summary: <p>The task of lane detection has garnered considerable attention in the field
of autonomous driving due to its complexity. Lanes can present difficulties for
detection, as they can be narrow, fragmented, and often obscured by heavy
traffic. However, it has been observed that the lanes have a geometrical
structure that resembles a straight line, leading to improved lane detection
results when utilizing this characteristic. To address this challenge, we
propose a hierarchical Deep Hough Transform (DHT) approach that combines all
lane features in an image into the Hough parameter space. Additionally, we
refine the point selection method and incorporate a Dynamic Convolution Module
to effectively differentiate between lanes in the original image. Our network
architecture comprises a backbone network, either a ResNet or Pyramid Vision
Transformer, a Feature Pyramid Network as the neck to extract multi-scale
features, and a hierarchical DHT-based feature aggregation head to accurately
segment each lane. By utilizing the lane features in the Hough parameter space,
the network learns dynamic convolution kernel parameters corresponding to each
lane, allowing the Dynamic Convolution Module to effectively differentiate
between lane features. Subsequently, the lane features are fed into the feature
decoder, which predicts the final position of the lane. Our proposed network
structure demonstrates improved performance in detecting heavily occluded or
worn lane images, as evidenced by our extensive experimental results, which
show that our method outperforms or is on par with state-of-the-art techniques.
</p></li>
</ul>

<h3>Title: INT-FP-QSim: Mixed Precision and Formats For Large Language Models and Vision Transformers. (arXiv:2307.03712v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03712">http://arxiv.org/abs/2307.03712</a></li>
<li>Code URL: <a href="https://github.com/lightmatter-ai/int-fp-qsim">https://github.com/lightmatter-ai/int-fp-qsim</a></li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03712] INT-FP-QSim: Mixed Precision and Formats For Large Language Models and Vision Transformers](http://arxiv.org/abs/2307.03712) #transformer</code></li>
<li>Summary: <p>The recent rise of large language models (LLMs) has resulted in increased
efforts towards running LLMs at reduced precision. Running LLMs at lower
precision supports resource constraints and furthers their democratization,
enabling users to run billion-parameter LLMs on their personal devices. To
supplement this ongoing effort, we propose INT-FP-QSim: an open-source
simulator that enables flexible evaluation of LLMs and vision transformers at
various numerical precisions and formats. INT-FP-QSim leverages existing
open-source repositories such as TensorRT, QPytorch and AIMET for a combined
simulator that supports various floating point and integer formats. With the
help of our simulator, we survey the impact of different numerical formats on
the performance of LLMs and vision transformers at 4-bit weights and 4-bit or
8-bit activations. We also compare recently proposed methods like Adaptive
Block Floating Point, SmoothQuant, GPTQ and RPTQ on the model performances. We
hope INT-FP-QSim will enable researchers to flexibly simulate models at various
precisions to support further research in quantization of LLMs and vision
transformers.
</p></li>
</ul>

<h3>Title: Token-Level Serialized Output Training for Joint Streaming ASR and ST Leveraging Textual Alignments. (arXiv:2307.03354v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03354">http://arxiv.org/abs/2307.03354</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03354] Token-Level Serialized Output Training for Joint Streaming ASR and ST Leveraging Textual Alignments](http://arxiv.org/abs/2307.03354) #transformer</code></li>
<li>Summary: <p>In real-world applications, users often require both translations and
transcriptions of speech to enhance their comprehension, particularly in
streaming scenarios where incremental generation is necessary. This paper
introduces a streaming Transformer-Transducer that jointly generates automatic
speech recognition (ASR) and speech translation (ST) outputs using a single
decoder. To produce ASR and ST content effectively with minimal latency, we
propose a joint token-level serialized output training method that interleaves
source and target words by leveraging an off-the-shelf textual aligner.
Experiments in monolingual (it-en) and multilingual ({de,es,it}-en) settings
demonstrate that our approach achieves the best quality-latency balance. With
an average ASR latency of 1s and ST latency of 1.3s, our model shows no
degradation or even improves output quality compared to separate ASR and ST
models, yielding an average improvement of 1.1 WER and 0.4 BLEU in the
multilingual case.
</p></li>
</ul>

<h3>Title: A Side-by-side Comparison of Transformers for English Implicit Discourse Relation Classification. (arXiv:2307.03378v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03378">http://arxiv.org/abs/2307.03378</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03378] A Side-by-side Comparison of Transformers for English Implicit Discourse Relation Classification](http://arxiv.org/abs/2307.03378) #transformer</code></li>
<li>Summary: <p>Though discourse parsing can help multiple NLP fields, there has been no wide
language model search done on implicit discourse relation classification. This
hinders researchers from fully utilizing public-available models in discourse
analysis. This work is a straightforward, fine-tuned discourse performance
comparison of seven pre-trained language models. We use PDTB-3, a popular
discourse relation annotated dataset. Through our model search, we raise SOTA
to 0.671 ACC and obtain novel observations. Some are contrary to what has been
reported before (Shi and Demberg, 2019b), that sentence-level pre-training
objectives (NSP, SBO, SOP) generally fail to produce the best performing model
for implicit discourse relation classification. Counterintuitively,
similar-sized PLMs with MLM and full attention led to better performance.
</p></li>
</ul>

<h3>Title: DWReCO at CheckThat! 2023: Enhancing Subjectivity Detection through Style-based Data Sampling. (arXiv:2307.03550v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03550">http://arxiv.org/abs/2307.03550</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03550] DWReCO at CheckThat! 2023: Enhancing Subjectivity Detection through Style-based Data Sampling](http://arxiv.org/abs/2307.03550) #transformer</code></li>
<li>Summary: <p>This paper describes our submission for the subjectivity detection task at
the CheckThat! Lab. To tackle class imbalances in the task, we have generated
additional training materials with GPT-3 models using prompts of different
styles from a subjectivity checklist based on journalistic perspective. We used
the extended training set to fine-tune language-specific transformer models.
Our experiments in English, German and Turkish demonstrate that different
subjective styles are effective across all languages. In addition, we observe
that the style-based oversampling is better than paraphrasing in Turkish and
English. Lastly, the GPT-3 models sometimes produce lacklustre results when
generating style-based texts in non-English languages.
</p></li>
</ul>

<h3>Title: Comparing Apples to Apples: Generating Aspect-Aware Comparative Sentences from User Review. (arXiv:2307.03691v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03691">http://arxiv.org/abs/2307.03691</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03691] Comparing Apples to Apples: Generating Aspect-Aware Comparative Sentences from User Review](http://arxiv.org/abs/2307.03691) #transformer</code></li>
<li>Summary: <p>It is time-consuming to find the best product among many similar
alternatives. Comparative sentences can help to contrast one item from others
in a way that highlights important features of an item that stand out. Given
reviews of one or multiple items and relevant item features, we generate
comparative review sentences to aid users to find the best fit. Specifically,
our model consists of three successive components in a transformer: (i) an item
encoding module to encode an item for comparison, (ii) a comparison generation
module that generates comparative sentences in an autoregressive manner, (iii)
a novel decoding method for user personalization. We show that our pipeline
generates fluent and diverse comparative sentences. We run experiments on the
relevance and fidelity of our generated sentences in a human evaluation study
and find that our algorithm creates comparative review sentences that are
relevant and truthful.
</p></li>
</ul>

<h3>Title: ACDNet: Attention-guided Collaborative Decision Network for Effective Medication Recommendation. (arXiv:2307.03332v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03332">http://arxiv.org/abs/2307.03332</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03332] ACDNet: Attention-guided Collaborative Decision Network for Effective Medication Recommendation](http://arxiv.org/abs/2307.03332) #transformer</code></li>
<li>Summary: <p>Medication recommendation using Electronic Health Records (EHR) is
challenging due to complex medical data. Current approaches extract
longitudinal information from patient EHR to personalize recommendations.
However, existing models often lack sufficient patient representation and
overlook the importance of considering the similarity between a patient's
medication records and specific medicines. Therefore, an Attention-guided
Collaborative Decision Network (ACDNet) for medication recommendation is
proposed in this paper. Specifically, ACDNet utilizes attention mechanism and
Transformer to effectively capture patient health conditions and medication
records by modeling their historical visits at both global and local levels.
ACDNet also employs a collaborative decision framework, utilizing the
similarity between medication records and medicine representation to facilitate
the recommendation process. The experimental results on two extensive medical
datasets, MIMIC-III and MIMIC-IV, clearly demonstrate that ACDNet outperforms
state-of-the-art models in terms of Jaccard, PR-AUC, and F1 score, reaffirming
its superiority. Moreover, the ablation experiments provide solid evidence of
the effectiveness of each module in ACDNet, validating their contribution to
the overall performance. Furthermore, a detailed case study reinforces the
effectiveness of ACDNet in medication recommendation based on EHR data,
showcasing its practical value in real-world healthcare scenarios.
</p></li>
</ul>

<h3>Title: Teaching Arithmetic to Small Transformers. (arXiv:2307.03381v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03381">http://arxiv.org/abs/2307.03381</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03381] Teaching Arithmetic to Small Transformers](http://arxiv.org/abs/2307.03381) #transformer</code></li>
<li>Summary: <p>Large language models like GPT-4 exhibit emergent capabilities across
general-purpose tasks, such as basic arithmetic, when trained on extensive text
data, even though these tasks are not explicitly encoded by the unsupervised,
next-token prediction objective. This study investigates how small
transformers, trained from random initialization, can efficiently learn
arithmetic operations such as addition, multiplication, and elementary
functions like square root, using the next-token prediction objective. We first
demonstrate that conventional training data is not the most effective for
arithmetic learning, and simple formatting changes can significantly improve
accuracy. This leads to sharp phase transitions as a function of training data
scale, which, in some cases, can be explained through connections to low-rank
matrix completion. Building on prior work, we then train on chain-of-thought
style data that includes intermediate step results. Even in the complete
absence of pretraining, this approach significantly and simultaneously improves
accuracy, sample complexity, and convergence speed. We also study the interplay
between arithmetic and text data during training and examine the effects of
few-shot prompting, pretraining, and model scale. Additionally, we discuss
length generalization challenges. Our work highlights the importance of
high-quality, instructive data that considers the particular characteristics of
the next-word prediction objective for rapidly eliciting arithmetic
capabilities.
</p></li>
</ul>

<h3>Title: One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention. (arXiv:2307.03576v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03576">http://arxiv.org/abs/2307.03576</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03576] One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention](http://arxiv.org/abs/2307.03576) #transformer</code></li>
<li>Summary: <p>Recent works have empirically analyzed in-context learning and shown that
transformers trained on synthetic linear regression tasks can learn to
implement ridge regression, which is the Bayes-optimal predictor, given
sufficient capacity [Aky\"urek et al., 2023], while one-layer transformers with
linear self-attention and no MLP layer will learn to implement one step of
gradient descent (GD) on a least-squares linear regression objective [von
Oswald et al., 2022]. However, the theory behind these observations remains
poorly understood. We theoretically study transformers with a single layer of
linear self-attention, trained on synthetic noisy linear regression data.
First, we mathematically show that when the covariates are drawn from a
standard Gaussian distribution, the one-layer transformer which minimizes the
pre-training loss will implement a single step of GD on the least-squares
linear regression objective. Then, we find that changing the distribution of
the covariates and weight vector to a non-isotropic Gaussian distribution has a
strong impact on the learned algorithm: the global minimizer of the
pre-training loss now implements a single step of $\textit{pre-conditioned}$
GD. However, if only the distribution of the responses is changed, then this
does not have a large effect on the learned algorithm: even when the response
comes from a more general family of $\textit{nonlinear}$ functions, the global
minimizer of the pre-training loss still implements a single step of GD on a
least-squares linear regression objective.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Weakly-supervised Contrastive Learning for Unsupervised Object Discovery. (arXiv:2307.03376v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03376">http://arxiv.org/abs/2307.03376</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03376] Weakly-supervised Contrastive Learning for Unsupervised Object Discovery](http://arxiv.org/abs/2307.03376) #generative</code></li>
<li>Summary: <p>Unsupervised object discovery (UOD) refers to the task of discriminating the
whole region of objects from the background within a scene without relying on
labeled datasets, which benefits the task of bounding-box-level localization
and pixel-level segmentation. This task is promising due to its ability to
discover objects in a generic manner. We roughly categorise existing techniques
into two main directions, namely the generative solutions based on image
resynthesis, and the clustering methods based on self-supervised models. We
have observed that the former heavily relies on the quality of image
reconstruction, while the latter shows limitations in effectively modeling
semantic correlations. To directly target at object discovery, we focus on the
latter approach and propose a novel solution by incorporating weakly-supervised
contrastive learning (WCL) to enhance semantic information exploration. We
design a semantic-guided self-supervised learning model to extract high-level
semantic features from images, which is achieved by fine-tuning the feature
encoder of a self-supervised model, namely DINO, via WCL. Subsequently, we
introduce Principal Component Analysis (PCA) to localize object regions. The
principal projection direction, corresponding to the maximal eigenvalue, serves
as an indicator of the object region(s). Extensive experiments on benchmark
unsupervised object discovery datasets demonstrate the effectiveness of our
proposed solution. The source code and experimental results are publicly
available via our project page at https://github.com/npucvr/WSCUOD.git.
</p></li>
</ul>

<h3>Title: NOFA: NeRF-based One-shot Facial Avatar Reconstruction. (arXiv:2307.03441v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03441">http://arxiv.org/abs/2307.03441</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03441] NOFA: NeRF-based One-shot Facial Avatar Reconstruction](http://arxiv.org/abs/2307.03441) #generative</code></li>
<li>Summary: <p>3D facial avatar reconstruction has been a significant research topic in
computer graphics and computer vision, where photo-realistic rendering and
flexible controls over poses and expressions are necessary for many related
applications. Recently, its performance has been greatly improved with the
development of neural radiance fields (NeRF). However, most existing NeRF-based
facial avatars focus on subject-specific reconstruction and reenactment,
requiring multi-shot images containing different views of the specific subject
for training, and the learned model cannot generalize to new identities,
limiting its further applications. In this work, we propose a one-shot 3D
facial avatar reconstruction framework that only requires a single source image
to reconstruct a high-fidelity 3D facial avatar. For the challenges of lacking
generalization ability and missing multi-view information, we leverage the
generative prior of 3D GAN and develop an efficient encoder-decoder network to
reconstruct the canonical neural volume of the source image, and further
propose a compensation network to complement facial details. To enable
fine-grained control over facial dynamics, we propose a deformation field to
warp the canonical volume into driven expressions. Through extensive
experimental comparisons, we achieve superior synthesis results compared to
several state-of-the-art methods.
</p></li>
</ul>

<h3>Title: Language-free Compositional Action Generation via Decoupling Refinement. (arXiv:2307.03538v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03538">http://arxiv.org/abs/2307.03538</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03538] Language-free Compositional Action Generation via Decoupling Refinement](http://arxiv.org/abs/2307.03538) #generative</code></li>
<li>Summary: <p>Composing simple elements into complex concepts is crucial yet challenging,
especially for 3D action generation. Existing methods largely rely on extensive
neural language annotations to discern composable latent semantics, a process
that is often costly and labor-intensive. In this study, we introduce a novel
framework to generate compositional actions without reliance on language
auxiliaries. Our approach consists of three main components: Action Coupling,
Conditional Action Generation, and Decoupling Refinement. Action Coupling
utilizes an energy model to extract the attention masks of each sub-action,
subsequently integrating two actions using these attentions to generate
pseudo-training examples. Then, we employ a conditional generative model, CVAE,
to learn a latent space, facilitating the diverse generation. Finally, we
propose Decoupling Refinement, which leverages a self-supervised pre-trained
model MAE to ensure semantic consistency between the sub-actions and
compositional actions. This refinement process involves rendering generated 3D
actions into 2D space, decoupling these images into two sub-segments, using the
MAE model to restore the complete image from sub-segments, and constraining the
recovered images to match images rendered from raw sub-actions. Due to the lack
of existing datasets containing both sub-actions and compositional actions, we
created two new datasets, named HumanAct-C and UESTC-C, and present a
corresponding evaluation metric. Both qualitative and quantitative assessments
are conducted to show our efficacy.
</p></li>
</ul>

<h3>Title: VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis. (arXiv:2307.03592v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03592">http://arxiv.org/abs/2307.03592</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03592] VesselVAE: Recursive Variational Autoencoders for 3D Blood Vessel Synthesis](http://arxiv.org/abs/2307.03592) #generative</code></li>
<li>Summary: <p>We present a data-driven generative framework for synthesizing blood vessel
3D geometry. This is a challenging task due to the complexity of vascular
systems, which are highly variating in shape, size, and structure. Existing
model-based methods provide some degree of control and variation in the
structures produced, but fail to capture the diversity of actual anatomical
data. We developed VesselVAE, a recursive variational Neural Network that fully
exploits the hierarchical organization of the vessel and learns a
low-dimensional manifold encoding branch connectivity along with geometry
features describing the target surface. After training, the VesselVAE latent
space can be sampled to generate new vessel geometries. To the best of our
knowledge, this work is the first to utilize this technique for synthesizing
blood vessels. We achieve similarities of synthetic and real data for radius
(.97), length (.95), and tortuosity (.96). By leveraging the power of deep
neural networks, we generate 3D models of blood vessels that are both accurate
and diverse, which is crucial for medical and surgical training, hemodynamic
simulations, and many other purposes.
</p></li>
</ul>

<h3>Title: Assisting Clinical Decisions for Scarcely Available Treatment via Disentangled Latent Representation. (arXiv:2307.03315v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03315">http://arxiv.org/abs/2307.03315</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03315] Assisting Clinical Decisions for Scarcely Available Treatment via Disentangled Latent Representation](http://arxiv.org/abs/2307.03315) #generative</code></li>
<li>Summary: <p>Extracorporeal membrane oxygenation (ECMO) is an essential life-supporting
modality for COVID-19 patients who are refractory to conventional therapies.
However, the proper treatment decision has been the subject of significant
debate and it remains controversial about who benefits from this scarcely
available and technically complex treatment option. To support clinical
decisions, it is a critical need to predict the treatment need and the
potential treatment and no-treatment responses. Targeting this clinical
challenge, we propose Treatment Variational AutoEncoder (TVAE), a novel
approach for individualized treatment analysis. TVAE is specifically designed
to address the modeling challenges like ECMO with strong treatment selection
bias and scarce treatment cases. TVAE conceptualizes the treatment decision as
a multi-scale problem. We model a patient's potential treatment assignment and
the factual and counterfactual outcomes as part of their intrinsic
characteristics that can be represented by a deep latent variable model. The
factual and counterfactual prediction errors are alleviated via a
reconstruction regularization scheme together with semi-supervision, and the
selection bias and the scarcity of treatment cases are mitigated by the
disentangled and distribution-matched latent space and the label-balancing
generative strategy. We evaluate TVAE on two real-world COVID-19 datasets: an
international dataset collected from 1651 hospitals across 63 countries, and a
institutional dataset collected from 15 hospitals. The results show that TVAE
outperforms state-of-the-art treatment effect models in predicting both the
propensity scores and factual outcomes on heterogeneous COVID-19 datasets.
Additional experiments also show TVAE outperforms the best existing models in
individual treatment effect estimation on the synthesized IHDP benchmark
dataset.
</p></li>
</ul>

<h2>large language model</h2>
<h3>Title: GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest. (arXiv:2307.03601v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03601">http://arxiv.org/abs/2307.03601</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03601] GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest](http://arxiv.org/abs/2307.03601) #large language model</code></li>
<li>Summary: <p>Instruction tuning large language model (LLM) on image-text pairs has
achieved unprecedented vision-language multimodal abilities. However, their
vision-language alignments are only built on image-level, the lack of
region-level alignment limits their advancements to fine-grained multimodal
understanding. In this paper, we propose instruction tuning on
region-of-interest. The key design is to reformulate the bounding box as the
format of spatial instruction. The interleaved sequences of visual features
extracted by the spatial instruction and the language embedding are input to
LLM, and trained on the transformed region-text data in instruction tuning
format. Our region-level vision-language model, termed as GPT4RoI, brings brand
new conversational and interactive experience beyond image-level understanding.
(1) Controllability: Users can interact with our model by both language and
spatial instructions to flexibly adjust the detail level of the question. (2)
Capacities: Our model supports not only single-region spatial instruction but
also multi-region. This unlocks more region-level multimodal capacities such as
detailed region caption and complex region reasoning. (3) Composition: Any
off-the-shelf object detector can be a spatial instruction provider so as to
mine informative object attributes from our model, like color, shape, material,
action, relation to other objects, etc. The code, data, and demo can be found
at https://github.com/jshilong/GPT4RoI.
</p></li>
</ul>

<h3>Title: AI-UPV at EXIST 2023 -- Sexism Characterization Using Large Language Models Under The Learning with Disagreements Regime. (arXiv:2307.03385v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03385">http://arxiv.org/abs/2307.03385</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03385] AI-UPV at EXIST 2023 -- Sexism Characterization Using Large Language Models Under The Learning with Disagreements Regime](http://arxiv.org/abs/2307.03385) #large language model</code></li>
<li>Summary: <p>With the increasing influence of social media platforms, it has become
crucial to develop automated systems capable of detecting instances of sexism
and other disrespectful and hateful behaviors to promote a more inclusive and
respectful online environment. Nevertheless, these tasks are considerably
challenging considering different hate categories and the author's intentions,
especially under the learning with disagreements regime. This paper describes
AI-UPV team's participation in the EXIST (sEXism Identification in Social
neTworks) Lab at CLEF 2023. The proposed approach aims at addressing the task
of sexism identification and characterization under the learning with
disagreements paradigm by training directly from the data with disagreements,
without using any aggregated label. Yet, performances considering both soft and
hard evaluations are reported. The proposed system uses large language models
(i.e., mBERT and XLM-RoBERTa) and ensemble strategies for sexism identification
and classification in English and Spanish. In particular, our system is
articulated in three different pipelines. The ensemble approach outperformed
the individual large language models obtaining the best performances both
adopting a soft and a hard label evaluation. This work describes the
participation in all the three EXIST tasks, considering a soft evaluation, it
obtained fourth place in Task 2 at EXIST and first place in Task 3, with the
highest ICM-Soft of -2.32 and a normalized ICM-Soft of 0.79. The source code of
our approaches is publicly available at
https://github.com/AngelFelipeMP/Sexism-LLM-Learning-With-Disagreement.
</p></li>
</ul>

<h3>Title: Large Language Models as Batteries-Included Zero-Shot ESCO Skills Matchers. (arXiv:2307.03539v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03539">http://arxiv.org/abs/2307.03539</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03539] Large Language Models as Batteries-Included Zero-Shot ESCO Skills Matchers](http://arxiv.org/abs/2307.03539) #large language model</code></li>
<li>Summary: <p>Understanding labour market dynamics requires accurately identifying the
skills required for and possessed by the workforce. Automation techniques are
increasingly being developed to support this effort. However, automatically
extracting skills from job postings is challenging due to the vast number of
existing skills. The ESCO (European Skills, Competences, Qualifications and
Occupations) framework provides a useful reference, listing over 13,000
individual skills. However, skills extraction remains difficult and accurately
matching job posts to the ESCO taxonomy is an open problem. In this work, we
propose an end-to-end zero-shot system for skills extraction from job
descriptions based on large language models (LLMs). We generate synthetic
training data for the entirety of ESCO skills and train a classifier to extract
skill mentions from job posts. We also employ a similarity retriever to
generate skill candidates which are then re-ranked using a second LLM. Using
synthetic data achieves an RP@10 score 10 points higher than previous distant
supervision approaches. Adding GPT-4 re-ranking improves RP@10 by over 22
points over previous methods. We also show that Framing the task as mock
programming when prompting the LLM can lead to better performance than natural
language prompts, especially with weaker LLMs. We demonstrate the potential of
integrating large language models at both ends of skills matching pipelines.
Our approach requires no human annotations and achieve extremely promising
results on skills extraction against ESCO.
</p></li>
</ul>

<h3>Title: Evaluating the Effectiveness of Large Language Models in Representing Textual Descriptions of Geometry and Spatial Relations. (arXiv:2307.03678v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03678">http://arxiv.org/abs/2307.03678</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03678] Evaluating the Effectiveness of Large Language Models in Representing Textual Descriptions of Geometry and Spatial Relations](http://arxiv.org/abs/2307.03678) #large language model</code></li>
<li>Summary: <p>This research focuses on assessing the ability of large language models
(LLMs) in representing geometries and their spatial relations. We utilize LLMs
including GPT-2 and BERT to encode the well-known text (WKT) format of
geometries and then feed their embeddings into classifiers and regressors to
evaluate the effectiveness of the LLMs-generated embeddings for geometric
attributes. The experiments demonstrate that while the LLMs-generated
embeddings can preserve geometry types and capture some spatial relations (up
to 73% accuracy), challenges remain in estimating numeric values and retrieving
spatially related objects. This research highlights the need for improvement in
terms of capturing the nuances and complexities of the underlying geospatial
data and integrating domain knowledge to support various GeoAI applications
using foundation models.
</p></li>
</ul>

<h3>Title: QIGen: Generating Efficient Kernels for Quantized Inference on Large Language Models. (arXiv:2307.03738v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03738">http://arxiv.org/abs/2307.03738</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03738] QIGen: Generating Efficient Kernels for Quantized Inference on Large Language Models](http://arxiv.org/abs/2307.03738) #large language model</code></li>
<li>Summary: <p>We present ongoing work on a new automatic code generation approach for
supporting quantized generative inference on LLMs such as LLaMA or OPT on
off-the-shelf CPUs. Our approach is informed by the target architecture and a
performance model, including both hardware characteristics and method-specific
accuracy constraints. Results on CPU-based inference for LLaMA models show that
our approach can lead to high performance and high accuracy, comparing
favorably to the best existing open-source solution. A preliminary
implementation is available at https://github.com/IST-DASLab/QIGen.
</p></li>
</ul>

<h3>Title: Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs. (arXiv:2307.03393v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03393">http://arxiv.org/abs/2307.03393</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03393] Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs](http://arxiv.org/abs/2307.03393) #large language model</code></li>
<li>Summary: <p>Learning on Graphs has attracted immense attention due to its wide real-world
applications. The most popular pipeline for learning on graphs with textual
node attributes primarily relies on Graph Neural Networks (GNNs), and utilizes
shallow text embedding as initial node representations, which has limitations
in general knowledge and profound semantic understanding. In recent years,
Large Language Models (LLMs) have been proven to possess extensive common
knowledge and powerful semantic comprehension abilities that have
revolutionized existing workflows to handle text data. In this paper, we aim to
explore the potential of LLMs in graph machine learning, especially the node
classification task, and investigate two possible pipelines: LLMs-as-Enhancers
and LLMs-as-Predictors. The former leverages LLMs to enhance nodes' text
attributes with their massive knowledge and then generate predictions through
GNNs. The latter attempts to directly employ LLMs as standalone predictors. We
conduct comprehensive and systematical studies on these two pipelines under
various settings. From comprehensive empirical results, we make original
observations and find new insights that open new possibilities and suggest
promising directions to leverage LLMs for learning on graphs.
</p></li>
</ul>

<h2>segmentation</h2>
<h3>Title: ADASSM: Adversarial Data Augmentation in Statistical Shape Models From Images. (arXiv:2307.03273v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03273">http://arxiv.org/abs/2307.03273</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03273] ADASSM: Adversarial Data Augmentation in Statistical Shape Models From Images](http://arxiv.org/abs/2307.03273) #segmentation</code></li>
<li>Summary: <p>Statistical shape models (SSM) have been well-established as an excellent
tool for identifying variations in the morphology of anatomy across the
underlying population. Shape models use consistent shape representation across
all the samples in a given cohort, which helps to compare shapes and identify
the variations that can detect pathologies and help in formulating treatment
plans. In medical imaging, computing these shape representations from CT/MRI
scans requires time-intensive preprocessing operations, including but not
limited to anatomy segmentation annotations, registration, and texture
denoising. Deep learning models have demonstrated exceptional capabilities in
learning shape representations directly from volumetric images, giving rise to
highly effective and efficient Image-to-SSM. Nevertheless, these models are
data-hungry and due to the limited availability of medical data, deep learning
models tend to overfit. Offline data augmentation techniques, that use kernel
density estimation based (KDE) methods for generating shape-augmented samples,
have successfully aided Image-to-SSM networks in achieving comparable accuracy
to traditional SSM methods. However, these augmentation methods focus on shape
augmentation, whereas deep learning models exhibit texture bias results in
sub-optimal models. This paper introduces a novel strategy for on-the-fly data
augmentation for the Image-to-SSM framework by leveraging data-dependent noise
generation or texture augmentation. The proposed framework is trained as an
adversary to the Image-to-SSM network, augmenting diverse and challenging noisy
samples. Our approach achieves improved accuracy by encouraging the model to
focus on the underlying geometry rather than relying solely on pixel values.
</p></li>
</ul>

<h3>Title: To pretrain or not to pretrain? A case study of domain-specific pretraining for semantic segmentation in histopathology. (arXiv:2307.03275v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03275">http://arxiv.org/abs/2307.03275</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03275] To pretrain or not to pretrain? A case study of domain-specific pretraining for semantic segmentation in histopathology](http://arxiv.org/abs/2307.03275) #segmentation</code></li>
<li>Summary: <p>Annotating medical imaging datasets is costly, so fine-tuning (or transfer
learning) is the most effective method for digital pathology vision
applications such as disease classification and semantic segmentation. However,
due to texture bias in models trained on real-world images, transfer learning
for histopathology applications might result in underperforming models, which
necessitates the need for using unlabeled histopathology data and
self-supervised methods to discover domain-specific characteristics. Here, we
tested the premise that histopathology-specific pretrained models provide
better initializations for pathology vision tasks, i.e., gland and cell
segmentation. In this study, we compare the performance of gland and cell
segmentation tasks with domain-specific and non-domain-specific pretrained
weights. Moreover, we investigate the data size at which domain-specific
pretraining produces a statistically significant difference in performance. In
addition, we investigated whether domain-specific initialization improves the
effectiveness of out-of-domain testing on distinct datasets but the same task.
The results indicate that performance gain using domain-specific pretraining
depends on both the task and the size of the training dataset. In instances
with limited dataset sizes, a significant improvement in gland segmentation
performance was also observed, whereas models trained on cell segmentation
datasets exhibit no improvement.
</p></li>
</ul>

<h3>Title: TBGC: Task-level Backbone-Oriented Gradient Clip for Multi-Task Foundation Model Learning. (arXiv:2307.03465v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03465">http://arxiv.org/abs/2307.03465</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03465] TBGC: Task-level Backbone-Oriented Gradient Clip for Multi-Task Foundation Model Learning](http://arxiv.org/abs/2307.03465) #segmentation</code></li>
<li>Summary: <p>The AllInOne training paradigm squeezes a wide range of tasks into a unified
model in a multi-task learning manner. However, optimization in multi-task
learning is more challenge than single-task learning, as the gradient norm from
different tasks may vary greatly, making the backbone overly biased towards one
specific task. To address this issue, we propose the task-level
backbone-oriented gradient clip paradigm, compared with the vanilla gradient
clip method, it has two points of emphasis:1) gradient clip is performed
independently for each task. 2) backbone gradients generated from each task are
rescaled to the same norm scale. Based on the experimental results, we argue
that the task-level backbone-oriented gradient clip paradigm can relieve the
gradient bias problem to some extent. We also propose a novel multi-branch data
augmentation strategy where conflict augmentations are placed in different
branches. Our approach has been shown to be effective and finally achieve 1st
place in the Leaderboard A and 2nd place in the Leaderboard B of the CVPR2023
Foundation Model Challenge. It's worth noting that instead of evaluating all
three tasks(detection, segmentation and fine-grained classification) in
Leaderboard A, the segmentation task is not evaluated in Leaderboard B, in
which our team has a huge advantage.
</p></li>
</ul>

<h3>Title: Tranfer Learning of Semantic Segmentation Methods for Identifying Buried Archaeological Structures on LiDAR Data. (arXiv:2307.03512v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03512">http://arxiv.org/abs/2307.03512</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03512] Tranfer Learning of Semantic Segmentation Methods for Identifying Buried Archaeological Structures on LiDAR Data](http://arxiv.org/abs/2307.03512) #segmentation</code></li>
<li>Summary: <p>When applying deep learning to remote sensing data in archaeological
research, a notable obstacle is the limited availability of suitable datasets
for training models. The application of transfer learning is frequently
employed to mitigate this drawback. However, there is still a need to explore
its effectiveness when applied across different archaeological datasets. This
paper compares the performance of various transfer learning configurations
using two semantic segmentation deep neural networks on two LiDAR datasets. The
experimental results indicate that transfer learning-based approaches in
archaeology can lead to performance improvements, although a systematic
enhancement has not yet been observed. We provide specific insights about the
validity of such techniques that can serve as a baseline for future works.
</p></li>
</ul>

<h3>Title: Unsupervised Segmentation of Fetal Brain MRI using Deep Learning Cascaded Registration. (arXiv:2307.03579v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2307.03579">http://arxiv.org/abs/2307.03579</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2307.03579] Unsupervised Segmentation of Fetal Brain MRI using Deep Learning Cascaded Registration](http://arxiv.org/abs/2307.03579) #segmentation</code></li>
<li>Summary: <p>Accurate segmentation of fetal brain magnetic resonance images is crucial for
analyzing fetal brain development and detecting potential neurodevelopmental
abnormalities. Traditional deep learning-based automatic segmentation, although
effective, requires extensive training data with ground-truth labels, typically
produced by clinicians through a time-consuming annotation process. To overcome
this challenge, we propose a novel unsupervised segmentation method based on
multi-atlas segmentation, that accurately segments multiple tissues without
relying on labeled data for training. Our method employs a cascaded deep
learning network for 3D image registration, which computes small, incremental
deformations to the moving image to align it precisely with the fixed image.
This cascaded network can then be used to register multiple annotated images
with the image to be segmented, and combine the propagated labels to form a
refined segmentation. Our experiments demonstrate that the proposed cascaded
architecture outperforms the state-of-the-art registration methods that were
tested. Furthermore, the derived segmentation method achieves similar
performance and inference time to nnU-Net while only using a small subset of
annotated data for the multi-atlas segmentation task and none for training the
network. Our pipeline for registration and multi-atlas segmentation is publicly
available at https://github.com/ValBcn/CasReg.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
