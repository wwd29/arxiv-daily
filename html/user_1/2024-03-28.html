<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-03-28</h1>
<h3>Title: Deep Generative Domain Adaptation with Temporal Attention for Cross-User  Activity Recognition</h3>
<ul>
<li><strong>Authors: </strong>Xiaozhou Ye, Kevin I-Kai Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17958">https://arxiv.org/abs/2403.17958</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17958">https://arxiv.org/pdf/2403.17958</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17958]] Deep Generative Domain Adaptation with Temporal Attention for Cross-User  Activity Recognition(https://arxiv.org/abs/2403.17958)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In Human Activity Recognition (HAR), a predominant assumption is that the data utilized for training and evaluation purposes are drawn from the same distribution. It is also assumed that all data samples are independent and identically distributed ($\displaystyle i.i.d.$). Contrarily, practical implementations often challenge this notion, manifesting data distribution discrepancies, especially in scenarios such as cross-user HAR. Domain adaptation is the promising approach to address these challenges inherent in cross-user HAR tasks. However, a clear gap in domain adaptation techniques is the neglect of the temporal relation embedded within time series data during the phase of aligning data distributions. Addressing this oversight, our research presents the Deep Generative Domain Adaptation with Temporal Attention (DGDATA) method. This novel method uniquely recognises and integrates temporal relations during the domain adaptation process. By synergizing the capabilities of generative models with the Temporal Relation Attention mechanism, our method improves the classification performance in cross-user HAR. A comprehensive evaluation has been conducted on three public sensor-based HAR datasets targeting different scenarios and applications to demonstrate the efficacy of the proposed DGDATA method.</li>
</ul>

<h3>Title: Mixing Artificial and Natural Intelligence: From Statistical Mechanics  to AI and Back to Turbulence</h3>
<ul>
<li><strong>Authors: </strong>Michael (Misha)Chertkov</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.stat-mech, cs.AI, physics.flu-dyn</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.17993">https://arxiv.org/abs/2403.17993</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.17993">https://arxiv.org/pdf/2403.17993</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.17993]] Mixing Artificial and Natural Intelligence: From Statistical Mechanics  to AI and Back to Turbulence(https://arxiv.org/abs/2403.17993)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The paper reflects on the future role of AI in scientific research, with a special focus on turbulence studies, and examines the evolution of AI, particularly through Diffusion Models rooted in non-equilibrium statistical mechanics. It underscores the significant impact of AI on advancing reduced, Lagrangian models of turbulence through innovative use of deep neural networks. Additionally, the paper reviews various other AI applications in turbulence research and outlines potential challenges and opportunities in the concurrent advancement of AI and statistical hydrodynamics. This discussion sets the stage for a future where AI and turbulence research are intricately intertwined, leading to more profound insights and advancements in both fields.</li>
</ul>

<h3>Title: Bidirectional Consistency Models</h3>
<ul>
<li><strong>Authors: </strong>Liangchen Li, Jiajun He</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18035">https://arxiv.org/abs/2403.18035</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18035">https://arxiv.org/pdf/2403.18035</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18035]] Bidirectional Consistency Models(https://arxiv.org/abs/2403.18035)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models (DMs) are capable of generating remarkably high-quality samples by iteratively denoising a random vector, a process that corresponds to moving along the probability flow ordinary differential equation (PF ODE). Interestingly, DMs can also invert an input image to noise by moving backward along the PF ODE, a key operation for downstream tasks such as interpolation and image editing. However, the iterative nature of this process restricts its speed, hindering its broader application. Recently, Consistency Models (CMs) have emerged to address this challenge by approximating the integral of the PF ODE, thereby bypassing the need to iterate. Yet, the absence of an explicit ODE solver complicates the inversion process. To resolve this, we introduce the Bidirectional Consistency Model (BCM), which learns a single neural network that enables both forward and backward traversal along the PF ODE, efficiently unifying generation and inversion tasks within one framework. Notably, our proposed method enables one-step generation and inversion while also allowing the use of additional steps to enhance generation quality or reduce reconstruction error. Furthermore, by leveraging our model's bidirectional consistency, we introduce a sampling strategy that can enhance FID while preserving the generated image content. We further showcase our model's capabilities in several downstream tasks, such as interpolation and inpainting, and present demonstrations of potential applications, including blind restoration of compressed images and defending black-box adversarial attacks.</li>
</ul>

<h3>Title: Move as You Say, Interact as You Can: Language-guided Human Motion  Generation with Scene Affordance</h3>
<ul>
<li><strong>Authors: </strong>Zan Wang, Yixin Chen, Baoxiong Jia, Puhao Li, Jinlu Zhang, Jingze Zhang, Tengyu Liu, Yixin Zhu, Wei Liang, Siyuan Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18036">https://arxiv.org/abs/2403.18036</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18036">https://arxiv.org/pdf/2403.18036</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18036]] Move as You Say, Interact as You Can: Language-guided Human Motion  Generation with Scene Affordance(https://arxiv.org/abs/2403.18036)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Despite significant advancements in text-to-motion synthesis, generating language-guided human motion within 3D environments poses substantial challenges. These challenges stem primarily from (i) the absence of powerful generative models capable of jointly modeling natural language, 3D scenes, and human motion, and (ii) the generative models' intensive data requirements contrasted with the scarcity of comprehensive, high-quality, language-scene-motion datasets. To tackle these issues, we introduce a novel two-stage framework that employs scene affordance as an intermediate representation, effectively linking 3D scene grounding and conditional motion generation. Our framework comprises an Affordance Diffusion Model (ADM) for predicting explicit affordance map and an Affordance-to-Motion Diffusion Model (AMDM) for generating plausible human motions. By leveraging scene affordance maps, our method overcomes the difficulty in generating human motion under multimodal condition signals, especially when training with limited data lacking extensive language-scene-motion pairs. Our extensive experiments demonstrate that our approach consistently outperforms all baselines on established benchmarks, including HumanML3D and HUMANISE. Additionally, we validate our model's exceptional generalization capabilities on a specially curated evaluation set featuring previously unseen descriptions and scenes.</li>
</ul>

<h3>Title: GPTs and Language Barrier: A Cross-Lingual Legal QA Examination</h3>
<ul>
<li><strong>Authors: </strong>Ha-Thanh Nguyen, Hiroaki Yamada, Ken Satoh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18098">https://arxiv.org/abs/2403.18098</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18098">https://arxiv.org/pdf/2403.18098</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18098]] GPTs and Language Barrier: A Cross-Lingual Legal QA Examination(https://arxiv.org/abs/2403.18098)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In this paper, we explore the application of Generative Pre-trained Transformers (GPTs) in cross-lingual legal Question-Answering (QA) systems using the COLIEE Task 4 dataset. In the COLIEE Task 4, given a statement and a set of related legal articles that serve as context, the objective is to determine whether the statement is legally valid, i.e., if it can be inferred from the provided contextual articles or not, which is also known as an entailment task. By benchmarking four different combinations of English and Japanese prompts and data, we provide valuable insights into GPTs' performance in multilingual legal QA scenarios, contributing to the development of more efficient and accurate cross-lingual QA solutions in the legal domain.</li>
</ul>

<h3>Title: Tutorial on Diffusion Models for Imaging and Vision</h3>
<ul>
<li><strong>Authors: </strong>Stanley H. Chan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18103">https://arxiv.org/abs/2403.18103</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18103">https://arxiv.org/pdf/2403.18103</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18103]] Tutorial on Diffusion Models for Imaging and Vision(https://arxiv.org/abs/2403.18103)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The astonishing growth of generative tools in recent years has empowered many exciting applications in text-to-image generation and text-to-video generation. The underlying principle behind these generative tools is the concept of diffusion, a particular sampling mechanism that has overcome some shortcomings that were deemed difficult in the previous approaches. The goal of this tutorial is to discuss the essential ideas underlying the diffusion models. The target audience of this tutorial includes undergraduate and graduate students who are interested in doing research on diffusion models or applying these models to solve other problems.</li>
</ul>

<h3>Title: Segment Any Medical Model Extended</h3>
<ul>
<li><strong>Authors: </strong>Yihao Liu, Jiaming Zhang, Andres Diaz-Pinto, Haowei Li, Alejandro Martin-Gomez, Amir Kheradmand, Mehran Armand</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18114">https://arxiv.org/abs/2403.18114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18114">https://arxiv.org/pdf/2403.18114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18114]] Segment Any Medical Model Extended(https://arxiv.org/abs/2403.18114)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>The Segment Anything Model (SAM) has drawn significant attention from researchers who work on medical image segmentation because of its generalizability. However, researchers have found that SAM may have limited performance on medical images compared to state-of-the-art non-foundation models. Regardless, the community sees potential in extending, fine-tuning, modifying, and evaluating SAM for analysis of medical imaging. An increasing number of works have been published focusing on the mentioned four directions, where variants of SAM are proposed. To this end, a unified platform helps push the boundary of the foundation model for medical images, facilitating the use, modification, and validation of SAM and its variants in medical image segmentation. In this work, we introduce SAMM Extended (SAMME), a platform that integrates new SAM variant models, adopts faster communication protocols, accommodates new interactive modes, and allows for fine-tuning of subcomponents of the models. These features can expand the potential of foundation models like SAM, and the results can be translated to applications such as image-guided therapy, mixed reality interaction, robotic navigation, and data augmentation.</li>
</ul>

<h3>Title: Recommendation of data-free class-incremental learning algorithms by  simulating future data</h3>
<ul>
<li><strong>Authors: </strong>Eva Feillet, Adrian Popescu, Céline Hudelot</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18132">https://arxiv.org/abs/2403.18132</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18132">https://arxiv.org/pdf/2403.18132</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18132]] Recommendation of data-free class-incremental learning algorithms by  simulating future data(https://arxiv.org/abs/2403.18132)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Class-incremental learning deals with sequential data streams composed of batches of classes. Various algorithms have been proposed to address the challenging case where samples from past classes cannot be stored. However, selecting an appropriate algorithm for a user-defined setting is an open problem, as the relative performance of these algorithms depends on the incremental settings. To solve this problem, we introduce an algorithm recommendation method that simulates the future data stream. Given an initial set of classes, it leverages generative models to simulate future classes from the same visual domain. We evaluate recent algorithms on the simulated stream and recommend the one which performs best in the user-defined incremental setting. We illustrate the effectiveness of our method on three large datasets using six algorithms and six incremental settings. Our method outperforms competitive baselines, and performance is close to that of an oracle choosing the best algorithm in each setting. This work contributes to facilitate the practical deployment of incremental learning.</li>
</ul>

<h3>Title: Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal  Propagation Analysis for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Kartikeya Bhardwaj, Nilesh Prasad Pandey, Sweta Priyadarshi, Kyunggeun Lee, Jun Ma, Harris Teague</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18159">https://arxiv.org/abs/2403.18159</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18159">https://arxiv.org/pdf/2403.18159</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18159]] Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal  Propagation Analysis for Large Language Models(https://arxiv.org/abs/2403.18159)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Large generative models, such as large language models (LLMs) and diffusion models have as revolutionized the fields of NLP and computer vision respectively. However, their slow inference, high computation and memory requirement makes it challenging to deploy them on edge devices. In this study, we propose a light-weight quantization aware fine tuning technique using knowledge distillation (KD-QAT) to improve the performance of 4-bit weight quantized LLMs using commonly available datasets to realize a popular language use case, on device chat applications. To improve this paradigm of finetuning, as main contributions, we provide insights into stability of KD-QAT by empirically studying the gradient propagation during training to better understand the vulnerabilities of KD-QAT based approaches to low-bit quantization errors. Based on our insights, we propose ov-freeze, a simple technique to stabilize the KD-QAT process. Finally, we experiment with the popular 7B LLaMAv2-Chat model at 4-bit quantization level and demonstrate that ov-freeze results in near float-point precision performance, i.e., less than 0.7% loss of accuracy on Commonsense Reasoning benchmarks.</li>
</ul>

<h3>Title: Don't Look into the Dark: Latent Codes for Pluralistic Image Inpainting</h3>
<ul>
<li><strong>Authors: </strong>Haiwei Chen, Yajie Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18186">https://arxiv.org/abs/2403.18186</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18186">https://arxiv.org/pdf/2403.18186</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18186]] Don't Look into the Dark: Latent Codes for Pluralistic Image Inpainting(https://arxiv.org/abs/2403.18186)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present a method for large-mask pluralistic image inpainting based on the generative framework of discrete latent codes. Our method learns latent priors, discretized as tokens, by only performing computations at the visible locations of the image. This is realized by a restrictive partial encoder that predicts the token label for each visible block, a bidirectional transformer that infers the missing labels by only looking at these tokens, and a dedicated synthesis network that couples the tokens with the partial image priors to generate coherent and pluralistic complete image even under extreme mask settings. Experiments on public benchmarks validate our design choices as the proposed method outperforms strong baselines in both visual quality and diversity metrics.</li>
</ul>

<h3>Title: LayoutFlow: Flow Matching for Layout Generation</h3>
<ul>
<li><strong>Authors: </strong>Julian Jorge Andrade Guerreiro, Naoto Inoue, Kento Masui, Mayu Otani, Hideki Nakayama</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18187">https://arxiv.org/abs/2403.18187</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18187">https://arxiv.org/pdf/2403.18187</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18187]] LayoutFlow: Flow Matching for Layout Generation(https://arxiv.org/abs/2403.18187)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Finding a suitable layout represents a crucial task for diverse applications in graphic design. Motivated by simpler and smoother sampling trajectories, we explore the use of Flow Matching as an alternative to current diffusion-based layout generation models. Specifically, we propose LayoutFlow, an efficient flow-based model capable of generating high-quality layouts. Instead of progressively denoising the elements of a noisy layout, our method learns to gradually move, or flow, the elements of an initial sample until it reaches its final prediction. In addition, we employ a conditioning scheme that allows us to handle various generation tasks with varying degrees of conditioning with a single model. Empirically, LayoutFlow performs on par with state-of-the-art models while being significantly faster.</li>
</ul>

<h3>Title: Few-shot Online Anomaly Detection and Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Shenxing Wei, Xing Wei, Zhiheng Ma, Songlin Dong, Shaochen Zhang, Yihong Gong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18201">https://arxiv.org/abs/2403.18201</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18201">https://arxiv.org/pdf/2403.18201</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18201]] Few-shot Online Anomaly Detection and Segmentation(https://arxiv.org/abs/2403.18201)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Detecting anomaly patterns from images is a crucial artificial intelligence technique in industrial applications. Recent research in this domain has emphasized the necessity of a large volume of training data, overlooking the practical scenario where, post-deployment of the model, unlabeled data containing both normal and abnormal samples can be utilized to enhance the model's performance. Consequently, this paper focuses on addressing the challenging yet practical few-shot online anomaly detection and segmentation (FOADS) task. Under the FOADS framework, models are trained on a few-shot normal dataset, followed by inspection and improvement of their capabilities by leveraging unlabeled streaming data containing both normal and abnormal samples simultaneously. To tackle this issue, we propose modeling the feature distribution of normal images using a Neural Gas network, which offers the flexibility to adapt the topology structure to identify outliers in the data flow. In order to achieve improved performance with limited training samples, we employ multi-scale feature embedding extracted from a CNN pre-trained on ImageNet to obtain a robust representation. Furthermore, we introduce an algorithm that can incrementally update parameters without the need to store previous samples. Comprehensive experimental results demonstrate that our method can achieve substantial performance under the FOADS setting, while ensuring that the time complexity remains within an acceptable range on MVTec AD and BTAD datasets.</li>
</ul>

<h3>Title: Road Obstacle Detection based on Unknown Objectness Scores</h3>
<ul>
<li><strong>Authors: </strong>Chihiro Noguchi, Toshiaki Ohgushi, Masao Yamanaka</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18207">https://arxiv.org/abs/2403.18207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18207">https://arxiv.org/pdf/2403.18207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18207]] Road Obstacle Detection based on Unknown Objectness Scores(https://arxiv.org/abs/2403.18207)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>The detection of unknown traffic obstacles is vital to ensure safe autonomous driving. The standard object-detection methods cannot identify unknown objects that are not included under predefined categories. This is because object-detection methods are trained to assign a background label to pixels corresponding to the presence of unknown objects. To address this problem, the pixel-wise anomaly-detection approach has attracted increased research attention. Anomaly-detection techniques, such as uncertainty estimation and perceptual difference from reconstructed images, make it possible to identify pixels of unknown objects as out-of-distribution (OoD) samples. However, when applied to images with many unknowns and complex components, such as driving scenes, these methods often exhibit unstable performance. The purpose of this study is to achieve stable performance for detecting unknown objects by incorporating the object-detection fashions into the pixel-wise anomaly detection methods. To achieve this goal, we adopt a semantic-segmentation network with a sigmoid head that simultaneously provides pixel-wise anomaly scores and objectness scores. Our experimental results show that the objectness scores play an important role in improving the detection performance. Based on these results, we propose a novel anomaly score by integrating these two scores, which we term as unknown objectness score. Quantitative evaluations show that the proposed method outperforms state-of-the-art methods when applied to the publicly available datasets.</li>
</ul>

<h3>Title: NeuroPictor: Refining fMRI-to-Image Reconstruction via Multi-individual  Pretraining and Multi-level Modulation</h3>
<ul>
<li><strong>Authors: </strong>Jingyang Huo, Yikai Wang, Xuelin Qian, Yun Wang, Chong Li, Jianfeng Feng, Yanwei Fu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18211">https://arxiv.org/abs/2403.18211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18211">https://arxiv.org/pdf/2403.18211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18211]] NeuroPictor: Refining fMRI-to-Image Reconstruction via Multi-individual  Pretraining and Multi-level Modulation(https://arxiv.org/abs/2403.18211)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent fMRI-to-image approaches mainly focused on associating fMRI signals with specific conditions of pre-trained diffusion models. These approaches, while producing high-quality images, capture only a limited aspect of the complex information in fMRI signals and offer little detailed control over image creation. In contrast, this paper proposes to directly modulate the generation process of diffusion models using fMRI signals. Our approach, NeuroPictor, divides the fMRI-to-image process into three steps: i) fMRI calibrated-encoding, to tackle multi-individual pre-training for a shared latent space to minimize individual difference and enable the subsequent cross-subject training; ii) fMRI-to-image cross-subject pre-training, perceptually learning to guide diffusion model with high- and low-level conditions across different individuals; iii) fMRI-to-image single-subject refining, similar with step ii but focus on adapting to particular individual. NeuroPictor extracts high-level semantic features from fMRI signals that characterizing the visual stimulus and incrementally fine-tunes the diffusion model with a low-level manipulation network to provide precise structural instructions. By training with over 60,000 fMRI-image pairs from various individuals, our model enjoys superior fMRI-to-image decoding capacity, particularly in the within-subject setting, as evidenced in benchmark datasets. Project page: https://jingyanghuo.github.io/neuropictor/.</li>
</ul>

<h3>Title: A Transformer-Based Framework for Payload Malware Detection and  Classification</h3>
<ul>
<li><strong>Authors: </strong>Kyle Stein, Arash Mahyari, Guillermo Francia III, Eman El-Sheikh</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18223">https://arxiv.org/abs/2403.18223</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18223">https://arxiv.org/pdf/2403.18223</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18223]] A Transformer-Based Framework for Payload Malware Detection and  Classification(https://arxiv.org/abs/2403.18223)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>As malicious cyber threats become more sophisticated in breaching computer networks, the need for effective intrusion detection systems (IDSs) becomes crucial. Techniques such as Deep Packet Inspection (DPI) have been introduced to allow IDSs analyze the content of network packets, providing more context for identifying potential threats. IDSs traditionally rely on using anomaly-based and signature-based detection techniques to detect unrecognized and suspicious activity. Deep learning techniques have shown great potential in DPI for IDSs due to their efficiency in learning intricate patterns from the packet content being transmitted through the network. In this paper, we propose a revolutionary DPI algorithm based on transformers adapted for the purpose of detecting malicious traffic with a classifier head. Transformers learn the complex content of sequence data and generalize them well to similar scenarios thanks to their self-attention mechanism. Our proposed method uses the raw payload bytes that represent the packet contents and is deployed as man-in-the-middle. The payload bytes are used to detect malicious packets and classify their types. Experimental results on the UNSW-NB15 and CIC-IOT23 datasets demonstrate that our transformer-based model is effective in distinguishing malicious from benign traffic in the test dataset, attaining an average accuracy of 79\% using binary classification and 72\% on the multi-classification experiment, both using solely payload bytes.</li>
</ul>

<h3>Title: NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion,  Reconstruction, and Generation</h3>
<ul>
<li><strong>Authors: </strong>Ruikai Cui, Weizhe Liu, Weixuan Sun, Senbo Wang, Taizhang Shang, Yang Li, Xibin Song, Han Yan, Zhennan Wu, Shenzhou Chen, Hongdong Li, Pan Ji</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18241">https://arxiv.org/abs/2403.18241</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18241">https://arxiv.org/pdf/2403.18241</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18241]] NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion,  Reconstruction, and Generation(https://arxiv.org/abs/2403.18241)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>3D shape generation aims to produce innovative 3D content adhering to specific conditions and constraints. Existing methods often decompose 3D shapes into a sequence of localized components, treating each element in isolation without considering spatial consistency. As a result, these approaches exhibit limited versatility in 3D data representation and shape generation, hindering their ability to generate highly diverse 3D shapes that comply with the specified constraints. In this paper, we introduce a novel spatial-aware 3D shape generation framework that leverages 2D plane representations for enhanced 3D shape modeling. To ensure spatial coherence and reduce memory usage, we incorporate a hybrid shape representation technique that directly learns a continuous signed distance field representation of the 3D shape using orthogonal 2D planes. Additionally, we meticulously enforce spatial correspondences across distinct planes using a transformer-based autoencoder structure, promoting the preservation of spatial relationships in the generated 3D shapes. This yields an algorithm that consistently outperforms state-of-the-art 3D shape generation methods on various tasks, including unconditional shape generation, multi-modal shape completion, single-view reconstruction, and text-to-shape synthesis.</li>
</ul>

<h3>Title: Enhancing Generative Class Incremental Learning Performance with Model  Forgetting Approach</h3>
<ul>
<li><strong>Authors: </strong>Taro Togo, Ren Togo, Keisuke Maeda, Takahiro Ogawa, Miki Haseyama</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18258">https://arxiv.org/abs/2403.18258</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18258">https://arxiv.org/pdf/2403.18258</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18258]] Enhancing Generative Class Incremental Learning Performance with Model  Forgetting Approach(https://arxiv.org/abs/2403.18258)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This study presents a novel approach to Generative Class Incremental Learning (GCIL) by introducing the forgetting mechanism, aimed at dynamically managing class information for better adaptation to streaming data. GCIL is one of the hot topics in the field of computer vision, and this is considered one of the crucial tasks in society, specifically the continual learning of generative models. The ability to forget is a crucial brain function that facilitates continual learning by selectively discarding less relevant information for humans. However, in the field of machine learning models, the concept of intentionally forgetting has not been extensively investigated. In this study we aim to bridge this gap by incorporating the forgetting mechanisms into GCIL, thereby examining their impact on the models' ability to learn in continual learning. Through our experiments, we have found that integrating the forgetting mechanisms significantly enhances the models' performance in acquiring new knowledge, underscoring the positive role that strategic forgetting plays in the process of continual learning.</li>
</ul>

<h3>Title: Branch-Tuning: Balancing Stability and Plasticity for Continual  Self-Supervised Learning</h3>
<ul>
<li><strong>Authors: </strong>Wenzhuo Liu, Fei Zhu, Cheng-Lin Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18266">https://arxiv.org/abs/2403.18266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18266">https://arxiv.org/pdf/2403.18266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18266]] Branch-Tuning: Balancing Stability and Plasticity for Continual  Self-Supervised Learning(https://arxiv.org/abs/2403.18266)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-supervised learning (SSL) has emerged as an effective paradigm for deriving general representations from vast amounts of unlabeled data. However, as real-world applications continually integrate new content, the high computational and resource demands of SSL necessitate continual learning rather than complete retraining. This poses a challenge in striking a balance between stability and plasticity when adapting to new information. In this paper, we employ Centered Kernel Alignment for quantitatively analyzing model stability and plasticity, revealing the critical roles of batch normalization layers for stability and convolutional layers for plasticity. Motivated by this, we propose Branch-tuning, an efficient and straightforward method that achieves a balance between stability and plasticity in continual SSL. Branch-tuning consists of branch expansion and compression, and can be easily applied to various SSL methods without the need of modifying the original methods, retaining old data or models. We validate our method through incremental experiments on various benchmark datasets, demonstrating its effectiveness and practical value in real-world scenarios. We hope our work offers new insights for future continual self-supervised learning research. The code will be made publicly available.</li>
</ul>

<h3>Title: DSF-GAN: DownStream Feedback Generative Adversarial Network</h3>
<ul>
<li><strong>Authors: </strong>Oriel Perets, Nadav Rappoport</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18267">https://arxiv.org/abs/2403.18267</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18267">https://arxiv.org/pdf/2403.18267</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18267]] DSF-GAN: DownStream Feedback Generative Adversarial Network(https://arxiv.org/abs/2403.18267)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Utility and privacy are two crucial measurements of the quality of synthetic tabular data. While significant advancements have been made in privacy measures, generating synthetic samples with high utility remains challenging. To enhance the utility of synthetic samples, we propose a novel architecture called the DownStream Feedback Generative Adversarial Network (DSF-GAN). This approach incorporates feedback from a downstream prediction model during training to augment the generator's loss function with valuable information. Thus, DSF-GAN utilizes a downstream prediction task to enhance the utility of synthetic samples. To evaluate our method, we tested it using two popular datasets. Our experiments demonstrate improved model performance when training on synthetic samples generated by DSF-GAN, compared to those generated by the same GAN architecture without feedback. The evaluation was conducted on the same validation set comprising real samples. All code and datasets used in this research will be made openly available for ease of reproduction.</li>
</ul>

<h3>Title: Image Deraining via Self-supervised Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>He-Hao Liao, Yan-Tsung Peng, Wen-Tao Chu, Ping-Chun Hsieh, Chung-Chi Tsai</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18270">https://arxiv.org/abs/2403.18270</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18270">https://arxiv.org/pdf/2403.18270</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18270]] Image Deraining via Self-supervised Reinforcement Learning(https://arxiv.org/abs/2403.18270)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>The quality of images captured outdoors is often affected by the weather. One factor that interferes with sight is rain, which can obstruct the view of observers and computer vision applications that rely on those images. The work aims to recover rain images by removing rain streaks via Self-supervised Reinforcement Learning (RL) for image deraining (SRL-Derain). We locate rain streak pixels from the input rain image via dictionary learning and use pixel-wise RL agents to take multiple inpainting actions to remove rain progressively. To our knowledge, this work is the first attempt where self-supervised RL is applied to image deraining. Experimental results on several benchmark image-deraining datasets show that the proposed SRL-Derain performs favorably against state-of-the-art few-shot and self-supervised deraining and denoising methods.</li>
</ul>

<h3>Title: BlendX: Complex Multi-Intent Detection with Blended Patterns</h3>
<ul>
<li><strong>Authors: </strong>Yejin Yoon, Jungyeon Lee, Kangsan Kim, Chanhee Park, Taeuk Kim</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18277">https://arxiv.org/abs/2403.18277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18277">https://arxiv.org/pdf/2403.18277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18277]] BlendX: Complex Multi-Intent Detection with Blended Patterns(https://arxiv.org/abs/2403.18277)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Task-oriented dialogue (TOD) systems are commonly designed with the presumption that each utterance represents a single intent. However, this assumption may not accurately reflect real-world situations, where users frequently express multiple intents within a single utterance. While there is an emerging interest in multi-intent detection (MID), existing in-domain datasets such as MixATIS and MixSNIPS have limitations in their formulation. To address these issues, we present BlendX, a suite of refined datasets featuring more diverse patterns than their predecessors, elevating both its complexity and diversity. For dataset construction, we utilize both rule-based heuristics as well as a generative tool -- OpenAI's ChatGPT -- which is augmented with a similarity-driven strategy for utterance selection. To ensure the quality of the proposed datasets, we also introduce three novel metrics that assess the statistical properties of an utterance related to word count, conjunction use, and pronoun usage. Extensive experiments on BlendX reveal that state-of-the-art MID models struggle with the challenges posed by the new datasets, highlighting the need to reexamine the current state of the MID field. The dataset is available at https://github.com/HYU-NLP/BlendX.</li>
</ul>

<h3>Title: Multi-Modal Contrastive Learning for Online Clinical Time-Series  Applications</h3>
<ul>
<li><strong>Authors: </strong>Fabian Baldenweg, Manuel Burger, Gunnar Rätsch, Rita Kuznetsova</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18316">https://arxiv.org/abs/2403.18316</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18316">https://arxiv.org/pdf/2403.18316</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18316]] Multi-Modal Contrastive Learning for Online Clinical Time-Series  Applications(https://arxiv.org/abs/2403.18316)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Electronic Health Record (EHR) datasets from Intensive Care Units (ICU) contain a diverse set of data modalities. While prior works have successfully leveraged multiple modalities in supervised settings, we apply advanced self-supervised multi-modal contrastive learning techniques to ICU data, specifically focusing on clinical notes and time-series for clinically relevant online prediction tasks. We introduce a loss function Multi-Modal Neighborhood Contrastive Loss (MM-NCL), a soft neighborhood function, and showcase the excellent linear probe and zero-shot performance of our approach.</li>
</ul>

<h3>Title: DODA: Diffusion for Object-detection Domain Adaptation in Agriculture</h3>
<ul>
<li><strong>Authors: </strong>Shuai Xiang, Pieter M. Blok, James Burridge, Haozhou Wang, Wei Guo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18334">https://arxiv.org/abs/2403.18334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18334">https://arxiv.org/pdf/2403.18334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18334]] DODA: Diffusion for Object-detection Domain Adaptation in Agriculture(https://arxiv.org/abs/2403.18334)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The diverse and high-quality content generated by recent generative models demonstrates the great potential of using synthetic data to train downstream models. However, in vision, especially in objection detection, related areas are not fully explored, the synthetic images are merely used to balance the long tails of existing datasets, and the accuracy of the generated labels is low, the full potential of generative models has not been exploited. In this paper, we propose DODA, a data synthesizer that can generate high-quality object detection data for new domains in agriculture. Specifically, we improve the controllability of layout-to-image through encoding layout as an image, thereby improving the quality of labels, and use a visual encoder to provide visual clues for the diffusion model to decouple visual features from the diffusion model, and empowering the model the ability to generate data in new domains. On the Global Wheat Head Detection (GWHD) Dataset, which is the largest dataset in agriculture and contains diverse domains, using the data synthesized by DODA improves the performance of the object detector by 12.74-17.76 AP$_{50}$ in the domain that was significantly shifted from the training data.</li>
</ul>

<h3>Title: ViTAR: Vision Transformer with Any Resolution</h3>
<ul>
<li><strong>Authors: </strong>Qihang Fan, Quanzeng You, Xiaotian Han, Yongfei Liu, Yunzhe Tao, Huaibo Huang, Ran He, Hongxia Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18361">https://arxiv.org/abs/2403.18361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18361">https://arxiv.org/pdf/2403.18361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18361]] ViTAR: Vision Transformer with Any Resolution(https://arxiv.org/abs/2403.18361)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>his paper tackles a significant challenge faced by Vision Transformers (ViTs): their constrained scalability across different image resolutions. Typically, ViTs experience a performance decline when processing resolutions different from those seen during training. Our work introduces two key innovations to address this issue. Firstly, we propose a novel module for dynamic resolution adjustment, designed with a single Transformer block, specifically to achieve highly efficient incremental token integration. Secondly, we introduce fuzzy positional encoding in the Vision Transformer to provide consistent positional awareness across multiple resolutions, thereby preventing overfitting to any single training resolution. Our resulting model, ViTAR (Vision Transformer with Any Resolution), demonstrates impressive adaptability, achieving 83.3\% top-1 accuracy at a 1120x1120 resolution and 80.4\% accuracy at a 4032x4032 resolution, all while reducing computational costs. ViTAR also shows strong performance in downstream tasks such as instance and semantic segmentation and can easily combined with self-supervised learning techniques like Masked AutoEncoder. Our work provides a cost-effective solution for enhancing the resolution scalability of ViTs, paving the way for more versatile and efficient high-resolution image processing.</li>
</ul>

<h3>Title: Ship in Sight: Diffusion Models for Ship-Image Super Resolution</h3>
<ul>
<li><strong>Authors: </strong>Luigi Sigillo, Riccardo Fosco Gramaccioni, Alessandro Nicolosi, Danilo Comminiello</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18370">https://arxiv.org/abs/2403.18370</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18370">https://arxiv.org/pdf/2403.18370</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18370]] Ship in Sight: Diffusion Models for Ship-Image Super Resolution(https://arxiv.org/abs/2403.18370)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model</a></li>
<li><strong>Abstract: </strong>In recent years, remarkable advancements have been achieved in the field of image generation, primarily driven by the escalating demand for high-quality outcomes across various image generation subtasks, such as inpainting, denoising, and super resolution. A major effort is devoted to exploring the application of super-resolution techniques to enhance the quality of low-resolution images. In this context, our method explores in depth the problem of ship image super resolution, which is crucial for coastal and port surveillance. We investigate the opportunity given by the growing interest in text-to-image diffusion models, taking advantage of the prior knowledge that such foundation models have already learned. In particular, we present a diffusion-model-based architecture that leverages text conditioning during training while being class-aware, to best preserve the crucial details of the ships during the generation of the super-resoluted image. Since the specificity of this task and the scarcity availability of off-the-shelf data, we also introduce a large labeled ship dataset scraped from online ship images, mostly from ShipSpotting\footnote{\url{www.shipspotting.com}} website. Our method achieves more robust results than other deep learning models previously employed for super resolution, as proven by the multiple experiments performed. Moreover, we investigate how this model can benefit downstream tasks, such as classification and object detection, thus emphasizing practical implementation in a real-world scenario. Experimental results show flexibility, reliability, and impressive performance of the proposed framework over state-of-the-art methods for different tasks. The code is available at: https://github.com/LuigiSigillo/ShipinSight .</li>
</ul>

<h3>Title: Generative Multi-modal Models are Good Class-Incremental Learners</h3>
<ul>
<li><strong>Authors: </strong>Xusheng Cao, Haori Lu, Linlan Huang, Xialei Liu, Ming-Ming Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18383">https://arxiv.org/abs/2403.18383</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18383">https://arxiv.org/pdf/2403.18383</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18383]] Generative Multi-modal Models are Good Class-Incremental Learners(https://arxiv.org/abs/2403.18383)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In class-incremental learning (CIL) scenarios, the phenomenon of catastrophic forgetting caused by the classifier's bias towards the current task has long posed a significant challenge. It is mainly caused by the characteristic of discriminative models. With the growing popularity of the generative multi-modal models, we would explore replacing discriminative models with generative ones for CIL. However, transitioning from discriminative to generative models requires addressing two key challenges. The primary challenge lies in transferring the generated textual information into the classification of distinct categories. Additionally, it requires formulating the task of CIL within a generative framework. To this end, we propose a novel generative multi-modal model (GMM) framework for class-incremental learning. Our approach directly generates labels for images using an adapted generative model. After obtaining the detailed text, we use a text encoder to extract text features and employ feature matching to determine the most similar label as the classification prediction. In the conventional CIL settings, we achieve significantly better results in long-sequence task scenarios. Under the Few-shot CIL setting, we have improved by at least 14\% accuracy over all the current state-of-the-art methods with significantly less forgetting. Our code is available at \url{https://github.com/DoubleClass/GMM}.</li>
</ul>

<h3>Title: Colour and Brush Stroke Pattern Recognition in Abstract Art using  Modified Deep Convolutional Generative Adversarial Networks</h3>
<ul>
<li><strong>Authors: </strong>Srinitish Srinivasan, Varenya Pathak</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18397">https://arxiv.org/abs/2403.18397</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18397">https://arxiv.org/pdf/2403.18397</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18397]] Colour and Brush Stroke Pattern Recognition in Abstract Art using  Modified Deep Convolutional Generative Adversarial Networks(https://arxiv.org/abs/2403.18397)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Abstract Art is an immensely popular, discussed form of art that often has the ability to depict the emotions of an artist. Many researchers have made attempts to study abstract art in the form of edge detection, brush stroke and emotion recognition algorithms using machine and deep learning. This papers describes the study of a wide distribution of abstract paintings using Generative Adversarial Neural Networks(GAN). GANs have the ability to learn and reproduce a distribution enabling researchers and scientists to effectively explore and study the generated image space. However, the challenge lies in developing an efficient GAN architecture that overcomes common training pitfalls. This paper addresses this challenge by introducing a modified-DCGAN (mDCGAN) specifically designed for high-quality artwork generation. The approach involves a thorough exploration of the modifications made, delving into the intricate workings of DCGANs, optimisation techniques, and regularisation methods aimed at improving stability and realism in art generation enabling effective study of generated patterns. The proposed mDCGAN incorporates meticulous adjustments in layer configurations and architectural choices, offering tailored solutions to the unique demands of art generation while effectively combating issues like mode collapse and gradient vanishing. Further this paper explores the generated latent space by performing random walks to understand vector relationships between brush strokes and colours in the abstract art space and a statistical analysis of unstable outputs after a certain period of GAN training and compare its significant difference. These findings validate the effectiveness of the proposed approach, emphasising its potential to revolutionise the field of digital art generation and digital art ecosystem.</li>
</ul>

<h3>Title: An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering  Using a VLM</h3>
<ul>
<li><strong>Authors: </strong>Wonkyun Kim, Changin Choi, Wonseok Lee, Wonjong Rhee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18406">https://arxiv.org/abs/2403.18406</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18406">https://arxiv.org/pdf/2403.18406</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18406]] An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering  Using a VLM(https://arxiv.org/abs/2403.18406)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Stimulated by the sophisticated reasoning capabilities of recent Large Language Models (LLMs), a variety of strategies for bridging video modality have been devised. A prominent strategy involves Video Language Models (VideoLMs), which train a learnable interface with video data to connect advanced vision encoders with LLMs. Recently, an alternative strategy has surfaced, employing readily available foundation models, such as VideoLMs and LLMs, across multiple stages for modality bridging. In this study, we introduce a simple yet novel strategy where only a single Vision Language Model (VLM) is utilized. Our starting point is the plain insight that a video comprises a series of images, or frames, interwoven with temporal information. The essence of video comprehension lies in adeptly managing the temporal aspects along with the spatial details of each frame. Initially, we transform a video into a single composite image by arranging multiple frames in a grid layout. The resulting single image is termed as an image grid. This format, while maintaining the appearance of a solitary image, effectively retains temporal information within the grid structure. Therefore, the image grid approach enables direct application of a single high-performance VLM without necessitating any video-data training. Our extensive experimental analysis across ten zero-shot video question answering benchmarks, including five open-ended and five multiple-choice benchmarks, reveals that the proposed Image Grid Vision Language Model (IG-VLM) surpasses the existing methods in nine out of ten benchmarks.</li>
</ul>

<h3>Title: ECNet: Effective Controllable Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Sicheng Li, Keqiang Sun, Zhixin Lai, Xiaoshi Wu, Feng Qiu, Haoran Xie, Kazunori Miyata, Hongsheng Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18417">https://arxiv.org/abs/2403.18417</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18417">https://arxiv.org/pdf/2403.18417</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18417]] ECNet: Effective Controllable Text-to-Image Diffusion Models(https://arxiv.org/abs/2403.18417)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The conditional text-to-image diffusion models have garnered significant attention in recent years. However, the precision of these models is often compromised mainly for two reasons, ambiguous condition input and inadequate condition guidance over single denoising loss. To address the challenges, we introduce two innovative solutions. Firstly, we propose a Spatial Guidance Injector (SGI) which enhances conditional detail by encoding text inputs with precise annotation information. This method directly tackles the issue of ambiguous control inputs by providing clear, annotated guidance to the model. Secondly, to overcome the issue of limited conditional supervision, we introduce Diffusion Consistency Loss (DCL), which applies supervision on the denoised latent code at any given time step. This encourages consistency between the latent code at each time step and the input signal, thereby enhancing the robustness and accuracy of the output. The combination of SGI and DCL results in our Effective Controllable Network (ECNet), which offers a more accurate controllable end-to-end text-to-image generation framework with a more precise conditioning input and stronger controllable supervision. We validate our approach through extensive experiments on generation under various conditions, such as human body skeletons, facial landmarks, and sketches of general objects. The results consistently demonstrate that our method significantly enhances the controllability and robustness of the generated images, outperforming existing state-of-the-art controllable text-to-image models.</li>
</ul>

<h3>Title: U-Sketch: An Efficient Approach for Sketch to Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Ilias Mitsouras, Eleftherios Tsonis, Paraskevi Tzouveli, Athanasios Voulodimos</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18425">https://arxiv.org/abs/2403.18425</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18425">https://arxiv.org/pdf/2403.18425</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18425]] U-Sketch: An Efficient Approach for Sketch to Image Diffusion Models(https://arxiv.org/abs/2403.18425)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated remarkable performance in text-to-image synthesis, producing realistic and high resolution images that faithfully adhere to the corresponding text-prompts. Despite their great success, they still fall behind in sketch-to-image synthesis tasks, where in addition to text-prompts, the spatial layout of the generated images has to closely follow the outlines of certain reference sketches. Employing an MLP latent edge predictor to guide the spatial layout of the synthesized image by predicting edge maps at each denoising step has been recently proposed. Despite yielding promising results, the pixel-wise operation of the MLP does not take into account the spatial layout as a whole, and demands numerous denoising iterations to produce satisfactory images, leading to time inefficiency. To this end, we introduce U-Sketch, a framework featuring a U-Net type latent edge predictor, which is capable of efficiently capturing both local and global features, as well as spatial correlations between pixels. Moreover, we propose the addition of a sketch simplification network that offers the user the choice of preprocessing and simplifying input sketches for enhanced outputs. The experimental results, corroborated by user feedback, demonstrate that our proposed U-Net latent edge predictor leads to more realistic results, that are better aligned with the spatial outlines of the reference sketches, while drastically reducing the number of required denoising steps and, consequently, the overall execution time.</li>
</ul>

<h3>Title: Backpropagation-free Network for 3D Test-time Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Yanshuo Wang, Ali Cheraghian, Zeeshan Hayder, Jie Hong, Sameera Ramasinghe, Shafin Rahman, David Ahmedt-Aristizabal, Xuesong Li, Lars Petersson, Mehrtash Harandi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18442">https://arxiv.org/abs/2403.18442</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18442">https://arxiv.org/pdf/2403.18442</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18442]] Backpropagation-free Network for 3D Test-time Adaptation(https://arxiv.org/abs/2403.18442)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Real-world systems often encounter new data over time, which leads to experiencing target domain shifts. Existing Test-Time Adaptation (TTA) methods tend to apply computationally heavy and memory-intensive backpropagation-based approaches to handle this. Here, we propose a novel method that uses a backpropagation-free approach for TTA for the specific case of 3D data. Our model uses a two-stream architecture to maintain knowledge about the source domain as well as complementary target-domain-specific information. The backpropagation-free property of our model helps address the well-known forgetting problem and mitigates the error accumulation issue. The proposed method also eliminates the need for the usually noisy process of pseudo-labeling and reliance on costly self-supervised training. Moreover, our method leverages subspace learning, effectively reducing the distribution variance between the two domains. Furthermore, the source-domain-specific and the target-domain-specific streams are aligned using a novel entropy-based adaptive fusion strategy. Extensive experiments on popular benchmarks demonstrate the effectiveness of our method. The code will be available at https://github.com/abie-e/BFTT3D.</li>
</ul>

<h3>Title: $\mathrm{F^2Depth}$: Self-supervised Indoor Monocular Depth Estimation  via Optical Flow Consistency and Feature Map Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Xiaotong Guo, Huijie Zhao, Shuwei Shao, Xudong Li, Baochang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18443">https://arxiv.org/abs/2403.18443</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18443">https://arxiv.org/pdf/2403.18443</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18443]] $\mathrm{F^2Depth}$: Self-supervised Indoor Monocular Depth Estimation  via Optical Flow Consistency and Feature Map Synthesis(https://arxiv.org/abs/2403.18443)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-supervised monocular depth estimation methods have been increasingly given much attention due to the benefit of not requiring large, labelled datasets. Such self-supervised methods require high-quality salient features and consequently suffer from severe performance drop for indoor scenes, where low-textured regions dominant in the scenes are almost indiscriminative. To address the issue, we propose a self-supervised indoor monocular depth estimation framework called $\mathrm{F^2Depth}$. A self-supervised optical flow estimation network is introduced to supervise depth learning. To improve optical flow estimation performance in low-textured areas, only some patches of points with more discriminative features are adopted for finetuning based on our well-designed patch-based photometric loss. The finetuned optical flow estimation network generates high-accuracy optical flow as a supervisory signal for depth estimation. Correspondingly, an optical flow consistency loss is designed. Multi-scale feature maps produced by finetuned optical flow estimation network perform warping to compute feature map synthesis loss as another supervisory signal for depth learning. Experimental results on the NYU Depth V2 dataset demonstrate the effectiveness of the framework and our proposed losses. To evaluate the generalization ability of our $\mathrm{F^2Depth}$, we collect a Campus Indoor depth dataset composed of approximately 1500 points selected from 99 images in 18 scenes. Zero-shot generalization experiments on 7-Scenes dataset and Campus Indoor achieve $\delta_1$ accuracy of 75.8% and 76.0% respectively. The accuracy results show that our model can generalize well to monocular images captured in unknown indoor scenes.</li>
</ul>

<h3>Title: Can Language Beat Numerical Regression? Language-Based Multimodal  Trajectory Prediction</h3>
<ul>
<li><strong>Authors: </strong>Inhwan Bae, Junoh Lee, Hae-Gon Jeon</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18447">https://arxiv.org/abs/2403.18447</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18447">https://arxiv.org/pdf/2403.18447</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18447]] Can Language Beat Numerical Regression? Language-Based Multimodal  Trajectory Prediction(https://arxiv.org/abs/2403.18447)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, generative</a></li>
<li><strong>Abstract: </strong>Language models have demonstrated impressive ability in context understanding and generative performance. Inspired by the recent success of language foundation models, in this paper, we propose LMTraj (Language-based Multimodal Trajectory predictor), which recasts the trajectory prediction task into a sort of question-answering problem. Departing from traditional numerical regression models, which treat the trajectory coordinate sequence as continuous signals, we consider them as discrete signals like text prompts. Specially, we first transform an input space for the trajectory coordinate into the natural language space. Here, the entire time-series trajectories of pedestrians are converted into a text prompt, and scene images are described as text information through image captioning. The transformed numerical and image data are then wrapped into the question-answering template for use in a language model. Next, to guide the language model in understanding and reasoning high-level knowledge, such as scene context and social relationships between pedestrians, we introduce an auxiliary multi-task question and answering. We then train a numerical tokenizer with the prompt data. We encourage the tokenizer to separate the integer and decimal parts well, and leverage it to capture correlations between the consecutive numbers in the language model. Lastly, we train the language model using the numerical tokenizer and all of the question-answer prompts. Here, we propose a beam-search-based most-likely prediction and a temperature-based multimodal prediction to implement both deterministic and stochastic inferences. Applying our LMTraj, we show that the language-based model can be a powerful pedestrian trajectory predictor, and outperforms existing numerical-based predictor methods. Code is publicly available at https://github.com/inhwanbae/LMTrajectory .</li>
</ul>

<h3>Title: CoRAST: Towards Foundation Model-Powered Correlated Data Analysis in  Resource-Constrained CPS and IoT</h3>
<ul>
<li><strong>Authors: </strong>Yi Hu, Jinhang Zuo, Alanis Zhao, Bob Iannucci, Carlee Joe-Wong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18451">https://arxiv.org/abs/2403.18451</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18451">https://arxiv.org/pdf/2403.18451</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18451]] CoRAST: Towards Foundation Model-Powered Correlated Data Analysis in  Resource-Constrained CPS and IoT(https://arxiv.org/abs/2403.18451)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Foundation models (FMs) emerge as a promising solution to harness distributed and diverse environmental data by leveraging prior knowledge to understand the complicated temporal and spatial correlations within heterogeneous datasets. Unlike distributed learning frameworks such as federated learning, which often struggle with multimodal data, FMs can transform diverse inputs into embeddings. This process facilitates the integration of information from various modalities and the application of prior learning to new domains. However, deploying FMs in resource-constrained edge systems poses significant challenges. To this end, we introduce CoRAST, a novel learning framework that utilizes FMs for enhanced analysis of distributed, correlated heterogeneous data. Utilizing a server-based FM, CoRAST can exploit existing environment information to extract temporal, spatial, and cross-modal correlations among sensor data. This enables CoRAST to offer context-aware insights for localized client tasks through FM-powered global representation learning. Our evaluation on real-world weather dataset demonstrates CoRAST's ability to exploit correlated heterogeneous data through environmental representation learning to reduce the forecast errors by up to 50.3% compared to the baselines.</li>
</ul>

<h3>Title: SingularTrajectory: Universal Trajectory Predictor Using Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Inhwan Bae, Young-Jae Park, Hae-Gon Jeon</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18452">https://arxiv.org/abs/2403.18452</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18452">https://arxiv.org/pdf/2403.18452</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18452]] SingularTrajectory: Universal Trajectory Predictor Using Diffusion Model(https://arxiv.org/abs/2403.18452)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>There are five types of trajectory prediction tasks: deterministic, stochastic, domain adaptation, momentary observation, and few-shot. These associated tasks are defined by various factors, such as the length of input paths, data split and pre-processing methods. Interestingly, even though they commonly take sequential coordinates of observations as input and infer future paths in the same coordinates as output, designing specialized architectures for each task is still necessary. For the other task, generality issues can lead to sub-optimal performances. In this paper, we propose SingularTrajectory, a diffusion-based universal trajectory prediction framework to reduce the performance gap across the five tasks. The core of SingularTrajectory is to unify a variety of human dynamics representations on the associated tasks. To do this, we first build a Singular space to project all types of motion patterns from each task into one embedding space. We next propose an adaptive anchor working in the Singular space. Unlike traditional fixed anchor methods that sometimes yield unacceptable paths, our adaptive anchor enables correct anchors, which are put into a wrong location, based on a traversability map. Finally, we adopt a diffusion-based predictor to further enhance the prototype paths using a cascaded denoising process. Our unified framework ensures the generality across various benchmark settings such as input modality, and trajectory lengths. Extensive experiments on five public benchmarks demonstrate that SingularTrajectory substantially outperforms existing models, highlighting its effectiveness in estimating general dynamics of human movements. Code is publicly available at https://github.com/inhwanbae/SingularTrajectory .</li>
</ul>

<h3>Title: DiffStyler: Diffusion-based Localized Image Style Transfer</h3>
<ul>
<li><strong>Authors: </strong>Shaoxu Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18461">https://arxiv.org/abs/2403.18461</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18461">https://arxiv.org/pdf/2403.18461</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18461]] DiffStyler: Diffusion-based Localized Image Style Transfer(https://arxiv.org/abs/2403.18461)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Image style transfer aims to imbue digital imagery with the distinctive attributes of style targets, such as colors, brushstrokes, shapes, whilst concurrently preserving the semantic integrity of the content. Despite the advancements in arbitrary style transfer methods, a prevalent challenge remains the delicate equilibrium between content semantics and style attributes. Recent developments in large-scale text-to-image diffusion models have heralded unprecedented synthesis capabilities, albeit at the expense of relying on extensive and often imprecise textual descriptions to delineate artistic styles. Addressing these limitations, this paper introduces DiffStyler, a novel approach that facilitates efficient and precise arbitrary image style transfer. DiffStyler lies the utilization of a text-to-image Stable Diffusion model-based LoRA to encapsulate the essence of style targets. This approach, coupled with strategic cross-LoRA feature and attention injection, guides the style transfer process. The foundation of our methodology is rooted in the observation that LoRA maintains the spatial feature consistency of UNet, a discovery that further inspired the development of a mask-wise style transfer technique. This technique employs masks extracted through a pre-trained FastSAM model, utilizing mask prompts to facilitate feature fusion during the denoising process, thereby enabling localized style transfer that preserves the original image's unaffected regions. Moreover, our approach accommodates multiple style targets through the use of corresponding masks. Through extensive experimentation, we demonstrate that DiffStyler surpasses previous methods in achieving a more harmonious balance between content preservation and style integration.</li>
</ul>

<h3>Title: DiffusionFace: Towards a Comprehensive Dataset for Diffusion-Based Face  Forgery Analysis</h3>
<ul>
<li><strong>Authors: </strong>Zhongxi Chen, Ke Sun, Ziyin Zhou, Xianming Lin, Xiaoshuai Sun, Liujuan Cao, Rongrong Ji</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18471">https://arxiv.org/abs/2403.18471</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18471">https://arxiv.org/pdf/2403.18471</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18471]] DiffusionFace: Towards a Comprehensive Dataset for Diffusion-Based Face  Forgery Analysis(https://arxiv.org/abs/2403.18471)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The rapid progress in deep learning has given rise to hyper-realistic facial forgery methods, leading to concerns related to misinformation and security risks. Existing face forgery datasets have limitations in generating high-quality facial images and addressing the challenges posed by evolving generative techniques. To combat this, we present DiffusionFace, the first diffusion-based face forgery dataset, covering various forgery categories, including unconditional and Text Guide facial image generation, Img2Img, Inpaint, and Diffusion-based facial exchange algorithms. Our DiffusionFace dataset stands out with its extensive collection of 11 diffusion models and the high-quality of the generated images, providing essential metadata and a real-world internet-sourced forgery facial image dataset for evaluation. Additionally, we provide an in-depth analysis of the data and introduce practical evaluation protocols to rigorously assess discriminative models' effectiveness in detecting counterfeit facial images, aiming to enhance security in facial image authentication processes. The dataset is available for download at \url{https://github.com/Rapisurazurite/DiffFace}.</li>
</ul>

<h3>Title: Synthesizing EEG Signals from Event-Related Potential Paradigms with  Conditional Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Guido Klein, Pierre Guetschel, Gianluigi Silvestri, Michael Tangermann</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18486">https://arxiv.org/abs/2403.18486</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18486">https://arxiv.org/pdf/2403.18486</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18486]] Synthesizing EEG Signals from Event-Related Potential Paradigms with  Conditional Diffusion Models(https://arxiv.org/abs/2403.18486)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Data scarcity in the brain-computer interface field can be alleviated through the use of generative models, specifically diffusion models. While diffusion models have previously been successfully applied to electroencephalogram (EEG) data, existing models lack flexibility w.r.t.~sampling or require alternative representations of the EEG data. To overcome these limitations, we introduce a novel approach to conditional diffusion models that utilizes classifier-free guidance to directly generate subject-, session-, and class-specific EEG data. In addition to commonly used metrics, domain-specific metrics are employed to evaluate the specificity of the generated samples. The results indicate that the proposed model can generate EEG data that resembles real data for each subject, session, and class.</li>
</ul>

<h3>Title: Learning in PINNs: Phase transition, total diffusion, and generalization</h3>
<ul>
<li><strong>Authors: </strong>Sokratis J. Anagnostopoulos, Juan Diego Toscano, Nikolaos Stergiopulos, George Em Karniadakis</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18494">https://arxiv.org/abs/2403.18494</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18494">https://arxiv.org/pdf/2403.18494</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18494]] Learning in PINNs: Phase transition, total diffusion, and generalization(https://arxiv.org/abs/2403.18494)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We investigate the learning dynamics of fully-connected neural networks through the lens of gradient signal-to-noise ratio (SNR), examining the behavior of first-order optimizers like Adam in non-convex objectives. By interpreting the drift/diffusion phases in the information bottleneck theory, focusing on gradient homogeneity, we identify a third phase termed ``total diffusion", characterized by equilibrium in the learning rates and homogeneous gradients. This phase is marked by an abrupt SNR increase, uniform residuals across the sample space and the most rapid training convergence. We propose a residual-based re-weighting scheme to accelerate this diffusion in quadratic loss functions, enhancing generalization. We also explore the information compression phenomenon, pinpointing a significant saturation-induced compression of activations at the total diffusion phase, with deeper layers experiencing negligible information loss. Supported by experimental data on physics-informed neural networks (PINNs), which underscore the importance of gradient homogeneity due to their PDE-based sample inter-dependence, our findings suggest that recognizing phase transitions could refine ML optimization strategies for improved generalization.</li>
</ul>

<h3>Title: OrCo: Towards Better Generalization via Orthogonality and Contrast for  Few-Shot Class-Incremental Learning</h3>
<ul>
<li><strong>Authors: </strong>Noor Ahmed, Anna Kukleva, Bernt Schiele</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18550">https://arxiv.org/abs/2403.18550</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18550">https://arxiv.org/pdf/2403.18550</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18550]] OrCo: Towards Better Generalization via Orthogonality and Contrast for  Few-Shot Class-Incremental Learning(https://arxiv.org/abs/2403.18550)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Few-Shot Class-Incremental Learning (FSCIL) introduces a paradigm in which the problem space expands with limited data. FSCIL methods inherently face the challenge of catastrophic forgetting as data arrives incrementally, making models susceptible to overwriting previously acquired knowledge. Moreover, given the scarcity of labeled samples available at any given time, models may be prone to overfitting and find it challenging to strike a balance between extensive pretraining and the limited incremental data. To address these challenges, we propose the OrCo framework built on two core principles: features' orthogonality in the representation space, and contrastive learning. In particular, we improve the generalization of the embedding space by employing a combination of supervised and self-supervised contrastive losses during the pretraining phase. Additionally, we introduce OrCo loss to address challenges arising from data limitations during incremental sessions. Through feature space perturbations and orthogonality between classes, the OrCo loss maximizes margins and reserves space for the following incremental data. This, in turn, ensures the accommodation of incoming classes in the feature space without compromising previously acquired knowledge. Our experimental results showcase state-of-the-art performance across three benchmark datasets, including mini-ImageNet, CIFAR100, and CUB datasets. Code is available at https://github.com/noorahmedds/OrCo</li>
</ul>

<h3>Title: CosalPure: Learning Concept from Group Images for Robust Co-Saliency  Detection</h3>
<ul>
<li><strong>Authors: </strong>Jiayi Zhu, Qing Guo, Felix Juefei-Xu, Yihao Huang, Yang Liu, Geguang Pu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18554">https://arxiv.org/abs/2403.18554</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18554">https://arxiv.org/pdf/2403.18554</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18554]] CosalPure: Learning Concept from Group Images for Robust Co-Saliency  Detection(https://arxiv.org/abs/2403.18554)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Co-salient object detection (CoSOD) aims to identify the common and salient (usually in the foreground) regions across a given group of images. Although achieving significant progress, state-of-the-art CoSODs could be easily affected by some adversarial perturbations, leading to substantial accuracy reduction. The adversarial perturbations can mislead CoSODs but do not change the high-level semantic information (e.g., concept) of the co-salient objects. In this paper, we propose a novel robustness enhancement framework by first learning the concept of the co-salient objects based on the input group images and then leveraging this concept to purify adversarial perturbations, which are subsequently fed to CoSODs for robustness enhancement. Specifically, we propose CosalPure containing two modules, i.e., group-image concept learning and concept-guided diffusion purification. For the first module, we adopt a pre-trained text-to-image diffusion model to learn the concept of co-salient objects within group images where the learned concept is robust to adversarial examples. For the second module, we map the adversarial image to the latent space and then perform diffusion generation by embedding the learned concept into the noise prediction function as an extra condition. Our method can effectively alleviate the influence of the SOTA adversarial attack containing different adversarial patterns, including exposure and noise. The extensive results demonstrate that our method could enhance the robustness of CoSODs significantly.</li>
</ul>

<h3>Title: Artifact Reduction in 3D and 4D Cone-beam Computed Tomography Images  with Deep Learning -- A Review</h3>
<ul>
<li><strong>Authors: </strong>Mohammadreza Amirian, Daniel Barco, Ivo Herzig, Frank-Peter Schilling</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18565">https://arxiv.org/abs/2403.18565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18565">https://arxiv.org/pdf/2403.18565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18565]] Artifact Reduction in 3D and 4D Cone-beam Computed Tomography Images  with Deep Learning -- A Review(https://arxiv.org/abs/2403.18565)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Deep learning based approaches have been used to improve image quality in cone-beam computed tomography (CBCT), a medical imaging technique often used in applications such as image-guided radiation therapy, implant dentistry or orthopaedics. In particular, while deep learning methods have been applied to reduce various types of CBCT image artifacts arising from motion, metal objects, or low-dose acquisition, a comprehensive review summarizing the successes and shortcomings of these approaches, with a primary focus on the type of artifacts rather than the architecture of neural networks, is lacking in the literature. In this review, the data generation and simulation pipelines, and artifact reduction techniques are specifically investigated for each type of artifact. We provide an overview of deep learning techniques that have successfully been shown to reduce artifacts in 3D, as well as in time-resolved (4D) CBCT through the use of projection- and/or volume-domain optimizations, or by introducing neural networks directly within the CBCT reconstruction algorithms. Research gaps are identified to suggest avenues for future exploration. One of the key findings of this work is an observed trend towards the use of generative models including GANs and score-based or diffusion models, accompanied with the need for more diverse and open training datasets and simulations.</li>
</ul>

<h3>Title: HandBooster: Boosting 3D Hand-Mesh Reconstruction by Conditional  Synthesis and Sampling of Hand-Object Interactions</h3>
<ul>
<li><strong>Authors: </strong>Hao Xu, Haipeng Li, Yinqiao Wang, Shuaicheng Liu, Chi-Wing Fu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18575">https://arxiv.org/abs/2403.18575</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18575">https://arxiv.org/pdf/2403.18575</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18575]] HandBooster: Boosting 3D Hand-Mesh Reconstruction by Conditional  Synthesis and Sampling of Hand-Object Interactions(https://arxiv.org/abs/2403.18575)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Reconstructing 3D hand mesh robustly from a single image is very challenging, due to the lack of diversity in existing real-world datasets. While data synthesis helps relieve the issue, the syn-to-real gap still hinders its usage. In this work, we present HandBooster, a new approach to uplift the data diversity and boost the 3D hand-mesh reconstruction performance by training a conditional generative space on hand-object interactions and purposely sampling the space to synthesize effective data samples. First, we construct versatile content-aware conditions to guide a diffusion model to produce realistic images with diverse hand appearances, poses, views, and backgrounds; favorably, accurate 3D annotations are obtained for free. Then, we design a novel condition creator based on our similarity-aware distribution sampling strategies to deliberately find novel and realistic interaction poses that are distinctive from the training set. Equipped with our method, several baselines can be significantly improved beyond the SOTA on the HO3D and DexYCB benchmarks. Our code will be released on https://github.com/hxwork/HandBooster_Pytorch.</li>
</ul>

<h3>Title: FlexEdit: Flexible and Controllable Diffusion-based Object-centric Image  Editing</h3>
<ul>
<li><strong>Authors: </strong>Trong-Tung Nguyen, Duc-Anh Nguyen, Anh Tran, Cuong Pham</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18605">https://arxiv.org/abs/2403.18605</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18605">https://arxiv.org/pdf/2403.18605</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18605]] FlexEdit: Flexible and Controllable Diffusion-based Object-centric Image  Editing(https://arxiv.org/abs/2403.18605)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Our work addresses limitations seen in previous approaches for object-centric editing problems, such as unrealistic results due to shape discrepancies and limited control in object replacement or insertion. To this end, we introduce FlexEdit, a flexible and controllable editing framework for objects where we iteratively adjust latents at each denoising step using our FlexEdit block. Initially, we optimize latents at test time to align with specified object constraints. Then, our framework employs an adaptive mask, automatically extracted during denoising, to protect the background while seamlessly blending new content into the target image. We demonstrate the versatility of FlexEdit in various object editing tasks and curate an evaluation test suite with samples from both real and synthetic images, along with novel evaluation metrics designed for object-centric editing. We conduct extensive experiments on different editing scenarios, demonstrating the superiority of our editing framework over recent advanced text-guided image editing methods. Our project page is published at https://flex-edit.github.io/.</li>
</ul>

<h3>Title: Conditional Wasserstein Distances with Applications in Bayesian OT Flow  Matching</h3>
<ul>
<li><strong>Authors: </strong>Jannis Chemseddine, Paul Hagemann, Christian Wald, Gabriele Steidl</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18705">https://arxiv.org/abs/2403.18705</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18705">https://arxiv.org/pdf/2403.18705</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18705]] Conditional Wasserstein Distances with Applications in Bayesian OT Flow  Matching(https://arxiv.org/abs/2403.18705)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In inverse problems, many conditional generative models approximate the posterior measure by minimizing a distance between the joint measure and its learned approximation. While this approach also controls the distance between the posterior measures in the case of the Kullback--Leibler divergence, this is in general not hold true for the Wasserstein distance. In this paper, we introduce a conditional Wasserstein distance via a set of restricted couplings that equals the expected Wasserstein distance of the posteriors. Interestingly, the dual formulation of the conditional Wasserstein-1 flow resembles losses in the conditional Wasserstein GAN literature in a quite natural way. We derive theoretical properties of the conditional Wasserstein distance, characterize the corresponding geodesics and velocity fields as well as the flow ODEs. Subsequently, we propose to approximate the velocity fields by relaxing the conditional Wasserstein distance. Based on this, we propose an extension of OT Flow Matching for solving Bayesian inverse problems and demonstrate its numerical advantages on an inverse problem and class-conditional image generation.</li>
</ul>

<h3>Title: Mitigating Hallucinations in Large Vision-Language Models with  Instruction Contrastive Decoding</h3>
<ul>
<li><strong>Authors: </strong>Xintong Wang, Jingheng Pan, Liang Ding, Chris Biemann</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18715">https://arxiv.org/abs/2403.18715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18715">https://arxiv.org/pdf/2403.18715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18715]] Mitigating Hallucinations in Large Vision-Language Models with  Instruction Contrastive Decoding(https://arxiv.org/abs/2403.18715)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large Vision-Language Models (LVLMs) are increasingly adept at generating contextually detailed and coherent responses from visual inputs. However, their application in multimodal decision-making and open-ended generation is hindered by a notable rate of hallucinations, where generated text inaccurately represents the visual contents. To address this issue, this paper introduces the Instruction Contrastive Decoding (ICD) method, a novel approach designed to reduce hallucinations during LVLM inference. Our method is inspired by our observation that what we call disturbance instructions significantly exacerbate hallucinations in multimodal fusion modules. ICD contrasts distributions from standard and instruction disturbance, thereby increasing alignment uncertainty and effectively subtracting hallucinated concepts from the original distribution. Through comprehensive experiments on discriminative benchmarks (POPE and MME) and a generative benchmark (LLaVa-Bench), we demonstrate that ICD significantly mitigates both object-level and attribute-level hallucinations. Moreover, our method not only addresses hallucinations but also significantly enhances the general perception and recognition capabilities of LVLMs.</li>
</ul>

<h3>Title: Semi-Supervised Learning for Deep Causal Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Yasin Ibrahim, Hermione Warr, Konstantinos Kamnitsas</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18717">https://arxiv.org/abs/2403.18717</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18717">https://arxiv.org/pdf/2403.18717</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18717]] Semi-Supervised Learning for Deep Causal Generative Models(https://arxiv.org/abs/2403.18717)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Developing models that can answer questions of the form "How would $x$ change if $y$ had been $z$?" is fundamental for advancing medical image analysis. Training causal generative models that address such counterfactual questions, though, currently requires that all relevant variables have been observed and that corresponding labels are available in training data. However, clinical data may not have complete records for all patients and state of the art causal generative models are unable to take full advantage of this. We thus develop, for the first time, a semi-supervised deep causal generative model that exploits the causal relationships between variables to maximise the use of all available data. We explore this in the setting where each sample is either fully labelled or fully unlabelled, as well as the more clinically realistic case of having different labels missing for each sample. We leverage techniques from causal inference to infer missing values and subsequently generate realistic counterfactuals, even for samples with incomplete labels.</li>
</ul>

<h3>Title: RAW: A Robust and Agile Plug-and-Play Watermark Framework for  AI-Generated Images with Provable Guarantees</h3>
<ul>
<li><strong>Authors: </strong>Xun Xian, Ganghua Wang, Xuan Bi, Jayanth Srinivasa, Ashish Kundu, Mingyi Hong, Jie Ding</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18774">https://arxiv.org/abs/2403.18774</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18774">https://arxiv.org/pdf/2403.18774</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18774]] RAW: A Robust and Agile Plug-and-Play Watermark Framework for  AI-Generated Images with Provable Guarantees(https://arxiv.org/abs/2403.18774)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Safeguarding intellectual property and preventing potential misuse of AI-generated images are of paramount importance. This paper introduces a robust and agile plug-and-play watermark detection framework, dubbed as RAW. As a departure from traditional encoder-decoder methods, which incorporate fixed binary codes as watermarks within latent representations, our approach introduces learnable watermarks directly into the original image data. Subsequently, we employ a classifier that is jointly trained with the watermark to detect the presence of the watermark. The proposed framework is compatible with various generative architectures and supports on-the-fly watermark injection after training. By incorporating state-of-the-art smoothing techniques, we show that the framework provides provable guarantees regarding the false positive rate for misclassifying a watermarked image, even in the presence of certain adversarial attacks targeting watermark removal. Experiments on a diverse range of images generated by state-of-the-art diffusion models reveal substantial performance enhancements compared to existing approaches. For instance, our method demonstrates a notable increase in AUROC, from 0.48 to 0.82, when compared to state-of-the-art approaches in detecting watermarked images under adversarial attacks, while maintaining image quality, as indicated by closely aligned FID and CLIP scores.</li>
</ul>

<h3>Title: ImageNet-D: Benchmarking Neural Network Robustness on Diffusion  Synthetic Object</h3>
<ul>
<li><strong>Authors: </strong>Chenshuang Zhang, Fei Pan, Junmo Kim, In So Kweon, Chengzhi Mao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18775">https://arxiv.org/abs/2403.18775</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18775">https://arxiv.org/pdf/2403.18775</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18775]] ImageNet-D: Benchmarking Neural Network Robustness on Diffusion  Synthetic Object(https://arxiv.org/abs/2403.18775)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model, generative</a></li>
<li><strong>Abstract: </strong>We establish rigorous benchmarks for visual perception robustness. Synthetic images such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific type of evaluation over synthetic corruptions, backgrounds, and textures, yet those robustness benchmarks are restricted in specified variations and have low synthetic quality. In this work, we introduce generative model as a data source for synthesizing hard images that benchmark deep models' robustness. Leveraging diffusion models, we are able to generate images with more diversified backgrounds, textures, and materials than any prior work, where we term this benchmark as ImageNet-D. Experimental results show that ImageNet-D results in a significant accuracy drop to a range of vision models, from the standard ResNet visual classifier to the latest foundation models like CLIP and MiniGPT-4, significantly reducing their accuracy by up to 60\%. Our work suggests that diffusion models can be an effective source to test vision models. The code and dataset are available at https://github.com/chenshuang-zhang/imagenet_d.</li>
</ul>

<h3>Title: Object Pose Estimation via the Aggregation of Diffusion Features</h3>
<ul>
<li><strong>Authors: </strong>Tianfu Wang, Guosheng Hu, Hongguang Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18791">https://arxiv.org/abs/2403.18791</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18791">https://arxiv.org/pdf/2403.18791</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18791]] Object Pose Estimation via the Aggregation of Diffusion Features(https://arxiv.org/abs/2403.18791)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Estimating the pose of objects from images is a crucial task of 3D scene understanding, and recent approaches have shown promising results on very large benchmarks. However, these methods experience a significant performance drop when dealing with unseen objects. We believe that it results from the limited generalizability of image features. To address this problem, we have an in-depth analysis on the features of diffusion models, e.g. Stable Diffusion, which hold substantial potential for modeling unseen objects. Based on this analysis, we then innovatively introduce these diffusion features for object pose estimation. To achieve this, we propose three distinct architectures that can effectively capture and aggregate diffusion features of different granularity, greatly improving the generalizability of object pose estimation. Our approach outperforms the state-of-the-art methods by a considerable margin on three popular benchmark datasets, LM, O-LM, and T-LESS. In particular, our method achieves higher accuracy than the previous best arts on unseen objects: 98.2% vs. 93.5% on Unseen LM, 85.9% vs. 76.3% on Unseen O-LM, showing the strong generalizability of our method. Our code is released at https://github.com/Tianfu18/diff-feats-pose.</li>
</ul>

<h3>Title: ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth  Estimation</h3>
<ul>
<li><strong>Authors: </strong>Suraj Patni, Aradhye Agarwal, Chetan Arora</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18807">https://arxiv.org/abs/2403.18807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18807">https://arxiv.org/pdf/2403.18807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18807]] ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth  Estimation(https://arxiv.org/abs/2403.18807)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In the absence of parallax cues, a learning-based single image depth estimation (SIDE) model relies heavily on shading and contextual cues in the image. While this simplicity is attractive, it is necessary to train such models on large and varied datasets, which are difficult to capture. It has been shown that using embeddings from pre-trained foundational models, such as CLIP, improves zero shot transfer in several applications. Taking inspiration from this, in our paper we explore the use of global image priors generated from a pre-trained ViT model to provide more detailed contextual information. We argue that the embedding vector from a ViT model, pre-trained on a large dataset, captures greater relevant information for SIDE than the usual route of generating pseudo image captions, followed by CLIP based text embeddings. Based on this idea, we propose a new SIDE model using a diffusion backbone which is conditioned on ViT embeddings. Our proposed design establishes a new state-of-the-art (SOTA) for SIDE on NYUv2 dataset, achieving Abs Rel error of 0.059(14% improvement) compared to 0.069 by the current SOTA (VPD). And on KITTI dataset, achieving Sq Rel error of 0.139 (2% improvement) compared to 0.142 by the current SOTA (GEDepth). For zero-shot transfer with a model trained on NYUv2, we report mean relative improvement of (20%, 23%, 81%, 25%) over NeWCRFs on (Sun-RGBD, iBims1, DIODE, HyperSim) datasets, compared to (16%, 18%, 45%, 9%) by ZoeDepth. The code is available at https://github.com/Aradhye2002/EcoDepth.</li>
</ul>

<h3>Title: Garment3DGen: 3D Garment Stylization and Texture Generation</h3>
<ul>
<li><strong>Authors: </strong>Nikolaos Sarafianos, Tuur Stuyck, Xiaoyu Xiang, Yilei Li, Jovan Popovic, Rakesh Ranjan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18816">https://arxiv.org/abs/2403.18816</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18816">https://arxiv.org/pdf/2403.18816</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18816]] Garment3DGen: 3D Garment Stylization and Texture Generation(https://arxiv.org/abs/2403.18816)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce Garment3DGen a new method to synthesize 3D garment assets from a base mesh given a single input image as guidance. Our proposed approach allows users to generate 3D textured clothes based on both real and synthetic images, such as those generated by text prompts. The generated assets can be directly draped and simulated on human bodies. First, we leverage the recent progress of image to 3D diffusion methods to generate 3D garment geometries. However, since these geometries cannot be utilized directly for downstream tasks, we propose to use them as pseudo ground-truth and set up a mesh deformation optimization procedure that deforms a base template mesh to match the generated 3D target. Second, we introduce carefully designed losses that allow the input base mesh to freely deform towards the desired target, yet preserve mesh quality and topology such that they can be simulated. Finally, a texture estimation module generates high-fidelity texture maps that are globally and locally consistent and faithfully capture the input guidance, allowing us to render the generated 3D assets. With Garment3DGen users can generate the textured 3D garment of their choice without the need of artist intervention. One can provide a textual prompt describing the garment they desire to generate a simulation-ready 3D asset. We present a plethora of quantitative and qualitative comparisons on various assets both real and generated and provide use-cases of how one can generate simulation-ready 3D garments.</li>
</ul>

<h3>Title: ObjectDrop: Bootstrapping Counterfactuals for Photorealistic Object  Removal and Insertion</h3>
<ul>
<li><strong>Authors: </strong>Daniel Winter, Matan Cohen, Shlomi Fruchter, Yael Pritch, Alex Rav-Acha, Yedid Hoshen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18818">https://arxiv.org/abs/2403.18818</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18818">https://arxiv.org/pdf/2403.18818</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18818]] ObjectDrop: Bootstrapping Counterfactuals for Photorealistic Object  Removal and Insertion(https://arxiv.org/abs/2403.18818)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, self-supervised</a></li>
<li><strong>Abstract: </strong>Diffusion models have revolutionized image editing but often generate images that violate physical laws, particularly the effects of objects on the scene, e.g., occlusions, shadows, and reflections. By analyzing the limitations of self-supervised approaches, we propose a practical solution centered on a \q{counterfactual} dataset. Our method involves capturing a scene before and after removing a single object, while minimizing other changes. By fine-tuning a diffusion model on this dataset, we are able to not only remove objects but also their effects on the scene. However, we find that applying this approach for photorealistic object insertion requires an impractically large dataset. To tackle this challenge, we propose bootstrap supervision; leveraging our object removal model trained on a small counterfactual dataset, we synthetically expand this dataset considerably. Our approach significantly outperforms prior methods in photorealistic object removal and insertion, particularly at modeling the effects of objects on the scene.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
