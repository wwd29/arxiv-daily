<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>diffusion</h2>
<h3>Title: Breathing Life Into Sketches Using Text-to-Video Priors. (arXiv:2311.13608v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13608">http://arxiv.org/abs/2311.13608</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13608]] Breathing Life Into Sketches Using Text-to-Video Priors(http://arxiv.org/abs/2311.13608)</code></li>
<li>Summary: <p>A sketch is one of the most intuitive and versatile tools humans use to
convey their ideas visually. An animated sketch opens another dimension to the
expression of ideas and is widely used by designers for a variety of purposes.
Animating sketches is a laborious process, requiring extensive experience and
professional design skills. In this work, we present a method that
automatically adds motion to a single-subject sketch (hence, "breathing life
into it"), merely by providing a text prompt indicating the desired motion. The
output is a short animation provided in vector representation, which can be
easily edited. Our method does not require extensive training, but instead
leverages the motion prior of a large pretrained text-to-video diffusion model
using a score-distillation loss to guide the placement of strokes. To promote
natural and smooth motion and to better preserve the sketch's appearance, we
model the learned motion through two components. The first governs small local
deformations and the second controls global affine transformations.
Surprisingly, we find that even models that struggle to generate sketch videos
on their own can still serve as a useful backbone for animating abstract
representations.
</p></li>
</ul>

<h3>Title: Boosting3D: High-Fidelity Image-to-3D by Boosting 2D Diffusion Prior to 3D Prior with Progressive Learning. (arXiv:2311.13617v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13617">http://arxiv.org/abs/2311.13617</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13617]] Boosting3D: High-Fidelity Image-to-3D by Boosting 2D Diffusion Prior to 3D Prior with Progressive Learning(http://arxiv.org/abs/2311.13617)</code></li>
<li>Summary: <p>We present Boosting3D, a multi-stage single image-to-3D generation method
that can robustly generate reasonable 3D objects in different data domains. The
point of this work is to solve the view consistency problem in single
image-guided 3D generation by modeling a reasonable geometric structure. For
this purpose, we propose to utilize better 3D prior to training the NeRF. More
specifically, we train an object-level LoRA for the target object using
original image and the rendering output of NeRF. And then we train the LoRA and
NeRF using a progressive training strategy. The LoRA and NeRF will boost each
other while training. After the progressive training, the LoRA learns the 3D
information of the generated object and eventually turns to an object-level 3D
prior. In the final stage, we extract the mesh from the trained NeRF and use
the trained LoRA to optimize the structure and appearance of the mesh. The
experiments demonstrate the effectiveness of the proposed method. Boosting3D
learns object-specific 3D prior which is beyond the ability of pre-trained
diffusion priors and achieves state-of-the-art performance in the single
image-to-3d generation task.
</p></li>
</ul>

<h3>Title: The Challenges of Image Generation Models in Generating Multi-Component Images. (arXiv:2311.13620v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13620">http://arxiv.org/abs/2311.13620</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13620]] The Challenges of Image Generation Models in Generating Multi-Component Images(http://arxiv.org/abs/2311.13620)</code></li>
<li>Summary: <p>Recent advances in text-to-image generators have led to substantial
capabilities in image generation. However, the complexity of prompts acts as a
bottleneck in the quality of images generated. A particular under-explored
facet is the ability of generative models to create high-quality images
comprising multiple components given as a prior. In this paper, we propose and
validate a metric called Components Inclusion Score (CIS) to evaluate the
extent to which a model can correctly generate multiple components. Our results
reveal that the evaluated models struggle to incorporate all the visual
elements from prompts with multiple components (8.53% drop in CIS per component
for all evaluated models). We also identify a significant decline in the
quality of the images and context awareness within an image as the number of
components increased (15.91% decrease in inception Score and 9.62% increase in
Frechet Inception Distance). To remedy this issue, we fine-tuned Stable
Diffusion V2 on a custom-created test dataset with multiple components,
outperforming its vanilla counterpart. To conclude, these findings reveal a
critical limitation in existing text-to-image generators, shedding light on the
challenge of generating multiple components within a single image using a
complex prompt.
</p></li>
</ul>

<h3>Title: TDiffDe: A Truncated Diffusion Model for Remote Sensing Hyperspectral Image Denoising. (arXiv:2311.13622v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13622">http://arxiv.org/abs/2311.13622</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13622]] TDiffDe: A Truncated Diffusion Model for Remote Sensing Hyperspectral Image Denoising(http://arxiv.org/abs/2311.13622)</code></li>
<li>Summary: <p>Hyperspectral images play a crucial role in precision agriculture,
environmental monitoring or ecological analysis. However, due to sensor
equipment and the imaging environment, the observed hyperspectral images are
often inevitably corrupted by various noise. In this study, we proposed a
truncated diffusion model, called TDiffDe, to recover the useful information in
hyperspectral images gradually. Rather than starting from a pure noise, the
input data contains image information in hyperspectral image denoising. Thus,
we cut the trained diffusion model from small steps to avoid the destroy of
valid information.
</p></li>
</ul>

<h3>Title: Diffusion models meet image counter-forensics. (arXiv:2311.13629v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13629">http://arxiv.org/abs/2311.13629</a></li>
<li>Code URL: https://github.com/mtailanian/diff-cf</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13629]] Diffusion models meet image counter-forensics(http://arxiv.org/abs/2311.13629)</code></li>
<li>Summary: <p>From its acquisition in the camera sensors to its storage, different
operations are performed to generate the final image. This pipeline imprints
specific traces into the image to form a natural watermark. Tampering with an
image disturbs these traces; these disruptions are clues that are used by most
methods to detect and locate forgeries. In this article, we assess the
capabilities of diffusion models to erase the traces left by forgers and,
therefore, deceive forensics methods. Such an approach has been recently
introduced for adversarial purification, achieving significant performance. We
show that diffusion purification methods are well suited for counter-forensics
tasks. Such approaches outperform already existing counter-forensics techniques
both in deceiving forensics methods and in preserving the natural look of the
purified images. The source code is publicly available at
https://github.com/mtailanian/diff-cf.
</p></li>
</ul>

<h3>Title: A Somewhat Robust Image Watermark against Diffusion-based Editing Models. (arXiv:2311.13713v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13713">http://arxiv.org/abs/2311.13713</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13713]] A Somewhat Robust Image Watermark against Diffusion-based Editing Models(http://arxiv.org/abs/2311.13713)</code></li>
<li>Summary: <p>Recently, diffusion models (DMs) have become the state-of-the-art method for
image synthesis. Editing models based on DMs, known for their high fidelity and
precision, have inadvertently introduced new challenges related to image
copyright infringement and malicious editing. Our work is the first to
formalize and address this issue. After assessing and attempting to enhance
traditional image watermarking techniques, we recognize their limitations in
this emerging context. In response, we develop a novel technique, RIW (Robust
Invisible Watermarking), to embed invisible watermarks leveraging adversarial
example techniques. Our technique ensures a high extraction accuracy of $96\%$
for the invisible watermark after editing, compared to the $0\%$ offered by
conventional methods. We provide access to our code at
https://github.com/BennyTMT/RIW.
</p></li>
</ul>

<h3>Title: Sample-Efficient Training for Diffusion. (arXiv:2311.13745v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13745">http://arxiv.org/abs/2311.13745</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13745]] Sample-Efficient Training for Diffusion(http://arxiv.org/abs/2311.13745)</code></li>
<li>Summary: <p>Score-based diffusion models have become the most popular approach to deep
generative modeling of images, largely due to their empirical performance and
reliability. Recently, a number of theoretical works \citep{chen2022,
Chen2022ImprovedAO, Chenetal23flowode, benton2023linear} have shown that
diffusion models can efficiently sample, assuming $L^2$-accurate score
estimates. The score-matching objective naturally approximates the true score
in $L^2$, but the sample complexity of existing bounds depends
\emph{polynomially} on the data radius and desired Wasserstein accuracy. By
contrast, the time complexity of sampling is only logarithmic in these
parameters. We show that estimating the score in $L^2$ \emph{requires} this
polynomial dependence, but that a number of samples that scales
polylogarithmically in the Wasserstein accuracy actually do suffice for
sampling. We show that with a polylogarithmic number of samples, the ERM of the
score-matching objective is $L^2$ accurate on all but a probability $\delta$
fraction of the true distribution, and that this weaker guarantee is sufficient
for efficient sampling.
</p></li>
</ul>

<h3>Title: Posterior Distillation Sampling. (arXiv:2311.13831v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13831">http://arxiv.org/abs/2311.13831</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13831]] Posterior Distillation Sampling(http://arxiv.org/abs/2311.13831)</code></li>
<li>Summary: <p>We introduce Posterior Distillation Sampling (PDS), a novel optimization
method for parametric image editing based on diffusion models. Existing
optimization-based methods, which leverage the powerful 2D prior of diffusion
models to handle various parametric images, have mainly focused on generation.
Unlike generation, editing requires a balance between conforming to the target
attribute and preserving the identity of the source content. Recent 2D image
editing methods have achieved this balance by leveraging the stochastic latent
encoded in the generative process of diffusion models. To extend the editing
capabilities of diffusion models shown in pixel space to parameter space, we
reformulate the 2D image editing method into an optimization form named PDS.
PDS matches the stochastic latents of the source and the target, enabling the
sampling of targets in diverse parameter spaces that align with a desired
attribute while maintaining the source's identity. We demonstrate that this
optimization resembles running a generative process with the target attribute,
but aligning this process with the trajectory of the source's generative
process. Extensive editing results in Neural Radiance Fields and Scalable
Vector Graphics representations demonstrate that PDS is capable of sampling
targets to fulfill the aforementioned balance across various parameter spaces.
</p></li>
</ul>

<h3>Title: Lego: Learning to Disentangle and Invert Concepts Beyond Object Appearance in Text-to-Image Diffusion Models. (arXiv:2311.13833v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13833">http://arxiv.org/abs/2311.13833</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13833]] Lego: Learning to Disentangle and Invert Concepts Beyond Object Appearance in Text-to-Image Diffusion Models(http://arxiv.org/abs/2311.13833)</code></li>
<li>Summary: <p>Diffusion models have revolutionized generative content creation and
text-to-image (T2I) diffusion models in particular have increased the creative
freedom of users by allowing scene synthesis using natural language. T2I models
excel at synthesizing concepts such as nouns, appearances, and styles. To
enable customized content creation based on a few example images of a concept,
methods such as Textual Inversion and DreamBooth invert the desired concept and
enable synthesizing it in new scenes. However, inverting more general concepts
that go beyond object appearance and style (adjectives and verbs) through
natural language, remains a challenge. Two key characteristics of these
concepts contribute to the limitations of current inversion methods. 1)
Adjectives and verbs are entangled with nouns (subject) and can hinder
appearance-based inversion methods, where the subject appearance leaks into the
concept embedding and 2) describing such concepts often extends beyond single
word embeddings (being frozen in ice, walking on a tightrope, etc.) that
current methods do not handle.
</p>
<p>In this study, we introduce Lego, a textual inversion method designed to
invert subject entangled concepts from a few example images. Lego disentangles
concepts from their associated subjects using a simple yet effective Subject
Separation step and employs a Context Loss that guides the inversion of
single/multi-embedding concepts. In a thorough user study, Lego-generated
concepts were preferred over 70% of the time when compared to the baseline.
Additionally, visual question answering using a large language model suggested
Lego-generated concepts are better aligned with the text description of the
concept.
</p></li>
</ul>

<h3>Title: Continual Learning of Diffusion Models with Generative Distillation. (arXiv:2311.14028v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.14028">http://arxiv.org/abs/2311.14028</a></li>
<li>Code URL: https://github.com/atenrev/difussion_continual_learning</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.14028]] Continual Learning of Diffusion Models with Generative Distillation(http://arxiv.org/abs/2311.14028)</code></li>
<li>Summary: <p>Diffusion models are powerful generative models that achieve state-of-the-art
performance in tasks such as image synthesis. However, training them demands
substantial amounts of data and computational resources. Continual learning
would allow for incrementally learning new tasks and accumulating knowledge,
thus reusing already trained models would be possible. One potentially suitable
approach is generative replay, where a copy of a generative model trained on
previous tasks produces synthetic data that are interleaved with data from the
current task. However, standard generative replay applied to diffusion models
results in a catastrophic loss in denoising capabilities. In this paper, we
propose generative distillation, an approach that distils the entire reverse
process of a diffusion model. We demonstrate that our approach significantly
improves the continual learning performance of generative replay with only a
moderate increase in the computational costs.
</p></li>
</ul>

<h3>Title: ACT: Adversarial Consistency Models. (arXiv:2311.14097v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.14097">http://arxiv.org/abs/2311.14097</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.14097]] ACT: Adversarial Consistency Models(http://arxiv.org/abs/2311.14097)</code></li>
<li>Summary: <p>Though diffusion models excel in image generation, their step-by-step
denoising leads to slow generation speeds. Consistency training addresses this
issue with single-step sampling but often produces lower-quality generations
and requires high training costs. In this paper, we show that optimizing
consistency training loss minimizes the Wasserstein distance between target and
generated distributions. As timestep increases, the upper bound accumulates
previous consistency training losses. Therefore, larger batch sizes are needed
to reduce both current and accumulated losses. We propose Adversarial
Consistency Training (ACT), which directly minimizes the Jensen-Shannon (JS)
divergence between distributions at each timestep using a discriminator.
Theoretically, ACT enhances generation quality, and convergence. By
incorporating a discriminator into the consistency training framework, our
method achieves improved FID scores on CIFAR10 and ImageNet 64$\times$64,
retains zero-shot image inpainting capabilities, and uses less than $1/6$ of
the original batch size and fewer than $1/2$ of the model parameters and
training steps compared to the baseline method, this leads to a substantial
reduction in resource consumption.
</p></li>
</ul>

<h3>Title: Adversarial defense based on distribution transfer. (arXiv:2311.13841v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13841">http://arxiv.org/abs/2311.13841</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13841]] Adversarial defense based on distribution transfer(http://arxiv.org/abs/2311.13841)</code></li>
<li>Summary: <p>The presence of adversarial examples poses a significant threat to deep
learning models and their applications. Existing defense methods provide
certain resilience against adversarial examples, but often suffer from
decreased accuracy and generalization performance, making it challenging to
achieve a trade-off between robustness and generalization. To address this, our
paper interprets the adversarial example problem from the perspective of sample
distribution and proposes a defense method based on distribution shift,
leveraging the distribution transfer capability of a diffusion model for
adversarial defense. The core idea is to exploit the discrepancy between normal
and adversarial sample distributions to achieve adversarial defense using a
pretrained diffusion model. Specifically, an adversarial sample undergoes a
forward diffusion process, moving away from the source distribution, followed
by a reverse process guided by the protected model (victim model) output to map
it back to the normal distribution. Experimental evaluations on CIFAR10 and
ImageNet30 datasets are conducted, comparing with adversarial training and
input preprocessing methods. For infinite-norm attacks with 8/255 perturbation,
accuracy rates of 78.1% and 83.5% are achieved, respectively. For 2-norm
attacks with 128/255 perturbation, accuracy rates are 74.3% and 82.5%.
Additional experiments considering perturbation amplitude, diffusion
iterations, and adaptive attacks also validate the effectiveness of the
proposed method. Results demonstrate that even when the attacker has knowledge
of the defense, the proposed distribution-based method effectively withstands
adversarial examples. It fills the gaps of traditional approaches, restoring
high-quality original samples and showcasing superior performance in model
robustness and generalization.
</p></li>
</ul>

<h3>Title: Touring sampling with pushforward maps. (arXiv:2311.13845v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13845">http://arxiv.org/abs/2311.13845</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13845]] Touring sampling with pushforward maps(http://arxiv.org/abs/2311.13845)</code></li>
<li>Summary: <p>The number of sampling methods could be daunting for a practitioner looking
to cast powerful machine learning methods to their specific problem. This paper
takes a theoretical stance to review and organize many sampling approaches in
the ``generative modeling'' setting, where one wants to generate new data that
are similar to some training examples. By revealing links between existing
methods, it might prove useful to overcome some of the current challenges in
sampling with diffusion models, such as long inference time due to diffusion
simulation, or the lack of diversity in generated samples.
</p></li>
</ul>

<h3>Title: RetroDiff: Retrosynthesis as Multi-stage Distribution Interpolation. (arXiv:2311.14077v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.14077">http://arxiv.org/abs/2311.14077</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.14077]] RetroDiff: Retrosynthesis as Multi-stage Distribution Interpolation(http://arxiv.org/abs/2311.14077)</code></li>
<li>Summary: <p>Retrosynthesis poses a fundamental challenge in biopharmaceuticals, aiming to
aid chemists in finding appropriate reactant molecules and synthetic pathways
given determined product molecules. With the reactant and product represented
as 2D graphs, retrosynthesis constitutes a conditional graph-to-graph
generative task. Inspired by the recent advancements in discrete diffusion
models for graph generation, we introduce Retrosynthesis Diffusion (RetroDiff),
a novel diffusion-based method designed to address this problem. However,
integrating a diffusion-based graph-to-graph framework while retaining
essential chemical reaction template information presents a notable challenge.
Our key innovation is to develop a multi-stage diffusion process. In this
method, we decompose the retrosynthesis procedure to first sample external
groups from the dummy distribution given products and then generate the
external bonds to connect the products and generated groups. Interestingly,
such a generation process is exactly the reverse of the widely adapted
semi-template retrosynthesis procedure, i.e. from reaction center
identification to synthon completion, which significantly reduces the error
accumulation. Experimental results on the benchmark have demonstrated the
superiority of our method over all other semi-template methods.
</p></li>
</ul>

<h2>self-supervised</h2>
<h3>Title: Towards Transferable Multi-modal Perception Representation Learning for Autonomy: NeRF-Supervised Masked AutoEncoder. (arXiv:2311.13750v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13750">http://arxiv.org/abs/2311.13750</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13750]] Towards Transferable Multi-modal Perception Representation Learning for Autonomy: NeRF-Supervised Masked AutoEncoder(http://arxiv.org/abs/2311.13750)</code></li>
<li>Summary: <p>This work proposes a unified self-supervised pre-training framework for
transferable multi-modal perception representation learning via masked
multi-modal reconstruction in Neural Radiance Field (NeRF), namely
NeRF-Supervised Masked AutoEncoder (NS-MAE). Specifically, conditioned on
certain view directions and locations, multi-modal embeddings extracted from
corrupted multi-modal input signals, i.e., Lidar point clouds and images, are
rendered into projected multi-modal feature maps via neural rendering. Then,
original multi-modal signals serve as reconstruction targets for the rendered
multi-modal feature maps to enable self-supervised representation learning.
Extensive experiments show that the representation learned via NS-MAE shows
promising transferability for diverse multi-modal and single-modal (camera-only
and Lidar-only) perception models on diverse 3D perception downstream tasks (3D
object detection and BEV map segmentation) with diverse amounts of fine-tuning
labeled data. Moreover, we empirically find that NS-MAE enjoys the synergy of
both the mechanism of masked autoencoder and neural radiance field. Our code
shall be released upon acceptance.
</p></li>
</ul>

<h3>Title: Which Matters Most in Making Fund Investment Decisions? A Multi-granularity Graph Disentangled Learning Framework. (arXiv:2311.13864v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13864">http://arxiv.org/abs/2311.13864</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13864]] Which Matters Most in Making Fund Investment Decisions? A Multi-granularity Graph Disentangled Learning Framework(http://arxiv.org/abs/2311.13864)</code></li>
<li>Summary: <p>In this paper, we highlight that both conformity and risk preference matter
in making fund investment decisions beyond personal interest and seek to
jointly characterize these aspects in a disentangled manner. Consequently, we
develop a novel M ulti-granularity Graph Disentangled Learning framework named
MGDL to effectively perform intelligent matching of fund investment products.
Benefiting from the well-established fund graph and the attention module,
multi-granularity user representations are derived from historical behaviors to
separately express personal interest, conformity and risk preference in a
fine-grained way. To attain stronger disentangled representations with specific
semantics, MGDL explicitly involve two self-supervised signals, i.e., fund type
based contrasts and fund popularity. Extensive experiments in offline and
online environments verify the effectiveness of MGDL.
</p></li>
</ul>

<h2>foundation model</h2>
<h3>Title: 3D-MIR: A Benchmark and Empirical Study on 3D Medical Image Retrieval in Radiology. (arXiv:2311.13752v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13752">http://arxiv.org/abs/2311.13752</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13752]] 3D-MIR: A Benchmark and Empirical Study on 3D Medical Image Retrieval in Radiology(http://arxiv.org/abs/2311.13752)</code></li>
<li>Summary: <p>The increasing use of medical imaging in healthcare settings presents a
significant challenge due to the increasing workload for radiologists, yet it
also offers opportunity for enhancing healthcare outcomes if effectively
leveraged. 3D image retrieval holds potential to reduce radiologist workloads
by enabling clinicians to efficiently search through diagnostically similar or
otherwise relevant cases, resulting in faster and more precise diagnoses.
However, the field of 3D medical image retrieval is still emerging, lacking
established evaluation benchmarks, comprehensive datasets, and thorough
studies. This paper attempts to bridge this gap by introducing a novel
benchmark for 3D Medical Image Retrieval (3D-MIR) that encompasses four
different anatomies imaged with computed tomography. Using this benchmark, we
explore a diverse set of search strategies that use aggregated 2D slices, 3D
volumes, and multi-modal embeddings from popular multi-modal foundation models
as queries. Quantitative and qualitative assessments of each approach are
provided alongside an in-depth discussion that offers insight for future
research. To promote the advancement of this field, our benchmark, dataset, and
code are made publicly available.
</p></li>
</ul>

<h3>Title: GS-Pose: Category-Level Object Pose Estimation via Geometric and Semantic Correspondence. (arXiv:2311.13777v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13777">http://arxiv.org/abs/2311.13777</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13777]] GS-Pose: Category-Level Object Pose Estimation via Geometric and Semantic Correspondence(http://arxiv.org/abs/2311.13777)</code></li>
<li>Summary: <p>Category-level pose estimation is a challenging task with many potential
applications in computer vision and robotics. Recently, deep-learning-based
approaches have made great progress, but are typically hindered by the need for
large datasets of either pose-labelled real images or carefully tuned
photorealistic simulators. This can be avoided by using only geometry inputs
such as depth images to reduce the domain-gap but these approaches suffer from
a lack of semantic information, which can be vital in the pose estimation
problem. To resolve this conflict, we propose to utilize both geometric and
semantic features obtained from a pre-trained foundation model.Our approach
projects 2D features from this foundation model into 3D for a single object
model per category, and then performs matching against this for new single view
observations of unseen object instances with a trained matching network. This
requires significantly less data to train than prior methods since the semantic
features are robust to object texture and appearance. We demonstrate this with
a rich evaluation, showing improved performance over prior methods with a
fraction of the data required.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Density Distribution-based Learning Framework for Addressing Online Continual Learning Challenges. (arXiv:2311.13623v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13623">http://arxiv.org/abs/2311.13623</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13623]] Density Distribution-based Learning Framework for Addressing Online Continual Learning Challenges(http://arxiv.org/abs/2311.13623)</code></li>
<li>Summary: <p>In this paper, we address the challenges of online Continual Learning (CL) by
introducing a density distribution-based learning framework. CL, especially the
Class Incremental Learning, enables adaptation to new test distributions while
continuously learning from a single-pass training data stream, which is more in
line with the practical application requirements of real-world scenarios.
However, existing CL methods often suffer from catastrophic forgetting and
higher computing costs due to complex algorithm designs, limiting their
practical use. Our proposed framework overcomes these limitations by achieving
superior average accuracy and time-space efficiency, bridging the performance
gap between CL and classical machine learning. Specifically, we adopt an
independent Generative Kernel Density Estimation (GKDE) model for each CL task.
During the testing stage, the GKDEs utilize a self-reported max probability
density value to determine which one is responsible for predicting incoming
test instances. A GKDE-based learning objective can ensure that samples with
the same label are grouped together, while dissimilar instances are pushed
farther apart. Extensive experiments conducted on multiple CL datasets validate
the effectiveness of our proposed framework. Our method outperforms popular CL
approaches by a significant margin, while maintaining competitive time-space
efficiency, making our framework suitable for real-world applications. Code
will be available at https://github.com/xxxx/xxxx.
</p></li>
</ul>

<h3>Title: GAN-Avatar: Controllable Personalized GAN-based Human Head Avatar. (arXiv:2311.13655v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13655">http://arxiv.org/abs/2311.13655</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13655]] GAN-Avatar: Controllable Personalized GAN-based Human Head Avatar(http://arxiv.org/abs/2311.13655)</code></li>
<li>Summary: <p>Digital humans and, especially, 3D facial avatars have raised a lot of
attention in the past years, as they are the backbone of several applications
like immersive telepresence in AR or VR. Despite the progress, facial avatars
reconstructed from commodity hardware are incomplete and miss out on parts of
the side and back of the head, severely limiting the usability of the avatar.
This limitation in prior work stems from their requirement of face tracking,
which fails for profile and back views. To address this issue, we propose to
learn person-specific animatable avatars from images without assuming to have
access to precise facial expression tracking. At the core of our method, we
leverage a 3D-aware generative model that is trained to reproduce the
distribution of facial expressions from the training data. To train this
appearance model, we only assume to have a collection of 2D images with the
corresponding camera parameters. For controlling the model, we learn a mapping
from 3DMM facial expression parameters to the latent space of the generative
model. This mapping can be learned by sampling the latent space of the
appearance model and reconstructing the facial parameters from a normalized
frontal view, where facial expression estimation performs well. With this
scheme, we decouple 3D appearance reconstruction and animation control to
achieve high fidelity in image synthesis. In a series of experiments, we
compare our proposed technique to state-of-the-art monocular methods and show
superior quality while not requiring expression tracking of the training data.
</p></li>
</ul>

<h3>Title: Sample as You Infer: Predictive Coding With Langevin Dynamics. (arXiv:2311.13664v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13664">http://arxiv.org/abs/2311.13664</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13664]] Sample as You Infer: Predictive Coding With Langevin Dynamics(http://arxiv.org/abs/2311.13664)</code></li>
<li>Summary: <p>We present a novel algorithm for parameter learning in generic deep
generative models that builds upon the predictive coding (PC) framework of
computational neuroscience. Our approach modifies the standard PC algorithm to
bring performance on-par and exceeding that obtained from standard variational
auto-encoder (VAE) training. By injecting Gaussian noise into the PC inference
procedure we re-envision it as an overdamped Langevin sampling, which
facilitates optimisation with respect to a tight evidence lower bound (ELBO).
We improve the resultant encoder-free training method by incorporating an
encoder network to provide an amortised warm-start to our Langevin sampling and
test three different objectives for doing so. Finally, to increase robustness
to the sampling step size and reduce sensitivity to curvature, we validate a
lightweight and easily computable form of preconditioning, inspired by Riemann
Manifold Langevin and adaptive optimizers from the SGD literature. We compare
against VAEs by training like-for-like generative models using our technique
against those trained with standard reparameterisation-trick-based ELBOs. We
observe our method out-performs or matches performance across a number of
metrics, including sample quality, while converging in a fraction of the number
of SGD training iterations.
</p></li>
</ul>

<h3>Title: Importance of Feature Extraction in the Calculation of Fr\'echet Distance for Medical Imaging. (arXiv:2311.13717v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13717">http://arxiv.org/abs/2311.13717</a></li>
<li>Code URL: https://github.com/mckellwoodland/fid-med-eval</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13717]] Importance of Feature Extraction in the Calculation of Fr\'echet Distance for Medical Imaging(http://arxiv.org/abs/2311.13717)</code></li>
<li>Summary: <p>Fr\'echet Inception Distance is a widely used metric for evaluating synthetic
image quality that utilizes an ImageNet-trained InceptionV3 network as a
feature extractor. However, its application in medical imaging lacks a standard
feature extractor, leading to biased and inconsistent comparisons. This study
aimed to compare state-of-the-art feature extractors for computing Fr\'echet
Distances (FDs) in medical imaging. A StyleGAN2 network was trained with data
augmentation techniques tailored for limited data domains on datasets
comprising three medical imaging modalities and four anatomical locations.
Human evaluation of generative quality (via a visual Turing test) was compared
to FDs calculated using ImageNet-trained InceptionV3, ResNet50, SwAV, DINO, and
Swin Transformer architectures, in addition to an InceptionV3 network trained
on a large medical dataset, RadImageNet. All ImageNet-based extractors were
consistent with each other, but only SwAV was significantly correlated with
medical expert judgment. The RadImageNet-based FD showed volatility and lacked
correlation with human judgment. Caution is advised when using medical
image-trained extraction networks in the FD calculation. These networks should
be rigorously evaluated on the imaging modality under consideration and
publicly released. ImageNet-based extractors, while imperfect, are consistent
and widely understood. Training extraction networks with SwAV is a promising
approach for synthetic medical image evaluation.
</p></li>
</ul>

<h3>Title: Perceptual Image Compression with Cooperative Cross-Modal Side Information. (arXiv:2311.13847v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13847">http://arxiv.org/abs/2311.13847</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13847]] Perceptual Image Compression with Cooperative Cross-Modal Side Information(http://arxiv.org/abs/2311.13847)</code></li>
<li>Summary: <p>The explosion of data has resulted in more and more associated text being
transmitted along with images. Inspired by from distributed source coding, many
works utilize image side information to enhance image compression. However,
existing methods generally do not consider using text as side information to
enhance perceptual compression of images, even though the benefits of
multimodal synergy have been widely demonstrated in research. This begs the
following question: How can we effectively transfer text-level semantic
dependencies to help image compression, which is only available to the decoder?
In this work, we propose a novel deep image compression method with text-guided
side information to achieve a better rate-perception-distortion tradeoff.
Specifically, we employ the CLIP text encoder and an effective Semantic-Spatial
Aware block to fuse the text and image features. This is done by predicting a
semantic mask to guide the learned text-adaptive affine transformation at the
pixel level. Furthermore, we design a text-conditional generative adversarial
networks to improve the perceptual quality of reconstructed images. Extensive
experiments involving four datasets and ten image quality assessment metrics
demonstrate that the proposed approach achieves superior results in terms of
rate-perception trade-off and semantic distortion.
</p></li>
</ul>

<h3>Title: EIGEN: Expert-Informed Joint Learning Aggregation for High-Fidelity Information Extraction from Document Images. (arXiv:2311.13993v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13993">http://arxiv.org/abs/2311.13993</a></li>
<li>Code URL: https://github.com/ayushayush591/eigen-high-fidelity-extraction-document-images</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13993]] EIGEN: Expert-Informed Joint Learning Aggregation for High-Fidelity Information Extraction from Document Images(http://arxiv.org/abs/2311.13993)</code></li>
<li>Summary: <p>Information Extraction (IE) from document images is challenging due to the
high variability of layout formats. Deep models such as LayoutLM and BROS have
been proposed to address this problem and have shown promising results.
However, they still require a large amount of field-level annotations for
training these models. Other approaches using rule-based methods have also been
proposed based on the understanding of the layout and semantics of a form such
as geometric position, or type of the fields, etc. In this work, we propose a
novel approach, EIGEN (Expert-Informed Joint Learning aGgrEatioN), which
combines rule-based methods with deep learning models using data programming
approaches to circumvent the requirement of annotation of large amounts of
training data. Specifically, EIGEN consolidates weak labels induced from
multiple heuristics through generative models and use them along with a small
number of annotated labels to jointly train a deep model. In our framework, we
propose the use of labeling functions that include incorporating contextual
information thus capturing the visual and language context of a word for
accurate categorization. We empirically show that our EIGEN framework can
significantly improve the performance of state-of-the-art deep models with the
availability of very few labeled data instances. The source code is available
at
https://github.com/ayushayush591/EIGEN-High-Fidelity-Extraction-Document-Images.
</p></li>
</ul>

<h3>Title: Comparison of pipeline, sequence-to-sequence, and GPT models for end-to-end relation extraction: experiments with the rare disease use-case. (arXiv:2311.13729v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.13729">http://arxiv.org/abs/2311.13729</a></li>
<li>Code URL: https://github.com/shashank140195/raredis</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.13729]] Comparison of pipeline, sequence-to-sequence, and GPT models for end-to-end relation extraction: experiments with the rare disease use-case(http://arxiv.org/abs/2311.13729)</code></li>
<li>Summary: <p>End-to-end relation extraction (E2ERE) is an important and realistic
application of natural language processing (NLP) in biomedicine. In this paper,
we aim to compare three prevailing paradigms for E2ERE using a complex dataset
focused on rare diseases involving discontinuous and nested entities. We use
the RareDis information extraction dataset to evaluate three competing
approaches (for E2ERE): NER $\rightarrow$ RE pipelines, joint sequence to
sequence models, and generative pre-trained transformer (GPT) models. We use
comparable state-of-the-art models and best practices for each of these
approaches and conduct error analyses to assess their failure modes. Our
findings reveal that pipeline models are still the best, while
sequence-to-sequence models are not far behind; GPT models with eight times as
many parameters are worse than even sequence-to-sequence models and lose to
pipeline models by over 10 F1 points. Partial matches and discontinuous
entities caused many NER errors contributing to lower overall E2E performances.
We also verify these findings on a second E2ERE dataset for chemical-protein
interactions. Although generative LM-based methods are more suitable for
zero-shot settings, when training data is available, our results show that it
is better to work with more conventional models trained and tailored for E2ERE.
More innovative methods are needed to marry the best of the both worlds from
smaller encoder-decoder pipeline models and the larger GPT models to improve
E2ERE. As of now, we see that well designed pipeline models offer substantial
performance gains at a lower cost and carbon footprint for E2ERE. Our
contribution is also the first to conduct E2ERE for the RareDis dataset.
</p></li>
</ul>

<h3>Title: Auditing and Mitigating Cultural Bias in LLMs. (arXiv:2311.14096v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.14096">http://arxiv.org/abs/2311.14096</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.14096]] Auditing and Mitigating Cultural Bias in LLMs(http://arxiv.org/abs/2311.14096)</code></li>
<li>Summary: <p>Culture fundamentally shapes people's reasoning, behavior, and communication.
Generative artificial intelligence (AI) technologies may cause a shift towards
a dominant culture. As people increasingly use AI to expedite and even automate
various professional and personal tasks, cultural values embedded in AI models
may bias authentic expression. We audit large language models for cultural
bias, comparing their responses to nationally representative survey data, and
evaluate country-specific prompting as a mitigation strategy. We find that
GPT-4, 3.5 and 3 exhibit cultural values resembling English-speaking and
Protestant European countries. Our mitigation strategy reduces cultural bias in
recent models but not for all countries/territories. To avoid cultural bias in
generative AI, especially in high-stakes contexts, we suggest using culture
matching and ongoing cultural audits.
</p></li>
</ul>

<h3>Title: A density estimation perspective on learning from pairwise human preferences. (arXiv:2311.14115v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.14115">http://arxiv.org/abs/2311.14115</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.14115]] A density estimation perspective on learning from pairwise human preferences(http://arxiv.org/abs/2311.14115)</code></li>
<li>Summary: <p>Learning from human feedback (LHF) -- and in particular learning from
pairwise preferences -- has recently become a crucial ingredient in training
large language models (LLMs), and has been the subject of much research. Most
recent works frame it as a reinforcement learning problem, where a reward
function is learned from pairwise preference data and the LLM is treated as a
policy which is adapted to maximize the rewards, often under additional
regularization constraints. We propose an alternative interpretation which
centers on the generative process for pairwise preferences and treats LHF as a
density estimation problem. We provide theoretical and empirical results
showing that for a family of generative processes defined via preference
behavior distribution equations, training a reward function on pairwise
preferences effectively models an annotator's implicit preference distribution.
Finally, we discuss and present findings on "annotator misspecification" --
failure cases where wrong modeling assumptions are made about annotator
behavior, resulting in poorly-adapted models -- suggesting that approaches that
learn from pairwise human preferences could have trouble learning from a
population of annotators with diverse viewpoints.
</p></li>
</ul>

<h3>Title: Multivariate Scenario Generation of Day-Ahead Electricity Prices using Normalizing Flows. (arXiv:2311.14033v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.14033">http://arxiv.org/abs/2311.14033</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.14033]] Multivariate Scenario Generation of Day-Ahead Electricity Prices using Normalizing Flows(http://arxiv.org/abs/2311.14033)</code></li>
<li>Summary: <p>Trading on electricity markets requires accurate information about the
realization of electricity prices and the uncertainty attached to the
predictions. We present a probabilistic forecasting approach for day-ahead
electricity prices using the fully data-driven deep generative model called
normalizing flows. Our modeling approach generates full-day scenarios of
day-ahead electricity prices based on conditional features such as residual
load forecasts. Furthermore, we propose extended feature sets of prior
realizations and a periodic retraining scheme that allows the normalizing flow
to adapt to the changing conditions of modern electricity markets. In
particular, we investigate the impact of the energy crisis ensuing from the
Russian invasion of Ukraine. Our results highlight that the normalizing flow
generates high-quality scenarios that reproduce the true price distribution and
yield highly accurate forecasts. Additionally, our analysis highlights how our
improvements towards adaptations in changing regimes allow the normalizing flow
to adapt to changing market conditions and enables continued sampling of
high-quality day-ahead price scenarios.
</p></li>
</ul>

<h2>anomaly</h2>
<h3>Title: Video Anomaly Detection using GAN. (arXiv:2311.14095v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.14095">http://arxiv.org/abs/2311.14095</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.14095]] Video Anomaly Detection using GAN(http://arxiv.org/abs/2311.14095)</code></li>
<li>Summary: <p>Accounting for the increased concern for public safety, automatic abnormal
event detection and recognition in a surveillance scene is crucial. It is a
current open study subject because of its intricacy and utility. The
identification of aberrant events automatically, it's a difficult undertaking
because everyone's idea of abnormality is different. A typical occurrence in
one circumstance could be seen as aberrant in another. Automatic anomaly
identification becomes particularly challenging in the surveillance footage
with a large crowd due to congestion and high occlusion. With the use of
machine learning techniques, this thesis study aims to offer the solution for
this use case so that human resources won't be required to keep an eye out for
any unusual activity in the surveillance system records. We have developed a
novel generative adversarial network (GAN) based anomaly detection model. This
model is trained such that it learns together about constructing a high
dimensional picture space and determining the latent space from the video's
context. The generator uses a residual Autoencoder architecture made up of a
multi-stage channel attention-based decoder and a two-stream, deep
convolutional encoder that can realise both spatial and temporal data. We have
also offered a technique for refining the GAN model that reduces training time
while also generalising the model by utilising transfer learning between
datasets. Using a variety of assessment measures, we compare our model to the
current state-of-the-art techniques on four benchmark datasets. The empirical
findings indicate that, in comparison to existing techniques, our network
performs favourably on all datasets.
</p></li>
</ul>

<h2>in-context</h2>
<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
