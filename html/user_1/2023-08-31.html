<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>diffusion</h2>
<h3>Title: Intriguing Properties of Diffusion Models: A Large-Scale Dataset for Evaluating Natural Attack Capability in Text-to-Image Generative Models. (arXiv:2308.15692v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.15692">http://arxiv.org/abs/2308.15692</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.15692]] Intriguing Properties of Diffusion Models: A Large-Scale Dataset for Evaluating Natural Attack Capability in Text-to-Image Generative Models(http://arxiv.org/abs/2308.15692)</code></li>
<li>Summary: <p>Denoising probabilistic diffusion models have shown breakthrough performance
that can generate more photo-realistic images or human-level illustrations than
the prior models such as GANs. This high image-generation capability has
stimulated the creation of many downstream applications in various areas.
However, we find that this technology is indeed a double-edged sword: We
identify a new type of attack, called the Natural Denoising Diffusion (NDD)
attack based on the finding that state-of-the-art deep neural network (DNN)
models still hold their prediction even if we intentionally remove their robust
features, which are essential to the human visual system (HVS), by text
prompts. The NDD attack can generate low-cost, model-agnostic, and
transferrable adversarial attacks by exploiting the natural attack capability
in diffusion models. Motivated by the finding, we construct a large-scale
dataset, Natural Denoising Diffusion Attack (NDDA) dataset, to systematically
evaluate the risk of the natural attack capability of diffusion models with
state-of-the-art text-to-image diffusion models. We evaluate the natural attack
capability by answering 6 research questions. Through a user study to confirm
the validity of the NDD attack, we find that the NDD attack can achieve an 88%
detection rate while being stealthy to 93% of human subjects. We also find that
the non-robust features embedded by diffusion models contribute to the natural
attack capability. To confirm the model-agnostic and transferrable attack
capability, we perform the NDD attack against an AD vehicle and find that 73%
of the physically printed attacks can be detected as a stop sign. We hope that
our study and dataset can help our community to be aware of the risk of
diffusion models and facilitate further research toward robust DNN models.
</p></li>
</ul>

<h3>Title: Zero-shot Inversion Process for Image Attribute Editing with Diffusion Models. (arXiv:2308.15854v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.15854">http://arxiv.org/abs/2308.15854</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.15854]] Zero-shot Inversion Process for Image Attribute Editing with Diffusion Models(http://arxiv.org/abs/2308.15854)</code></li>
<li>Summary: <p>Denoising diffusion models have shown outstanding performance in image
editing. Existing works tend to use either image-guided methods, which provide
a visual reference but lack control over semantic coherence, or text-guided
methods, which ensure faithfulness to text guidance but lack visual quality. To
address the problem, we propose the Zero-shot Inversion Process (ZIP), a
framework that injects a fusion of generated visual reference and text guidance
into the semantic latent space of a \textit{frozen} pre-trained diffusion
model. Only using a tiny neural network, the proposed ZIP produces diverse
content and attributes under the intuitive control of the text prompt.
Moreover, ZIP shows remarkable robustness for both in-domain and out-of-domain
attribute manipulation on real images. We perform detailed experiments on
various benchmark datasets. Compared to state-of-the-art methods, ZIP produces
images of equivalent quality while providing a realistic editing effect.
</p></li>
</ul>

<h3>Title: Feature Attention Network (FA-Net): A Deep-Learning Based Approach for Underwater Single Image Enhancement. (arXiv:2308.15868v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.15868">http://arxiv.org/abs/2308.15868</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.15868]] Feature Attention Network (FA-Net): A Deep-Learning Based Approach for Underwater Single Image Enhancement(http://arxiv.org/abs/2308.15868)</code></li>
<li>Summary: <p>Underwater image processing and analysis have been a hotspot of study in
recent years, as more emphasis has been focused to underwater monitoring and
usage of marine resources. Compared with the open environment, underwater image
encountered with more complicated conditions such as light abortion,
scattering, turbulence, nonuniform illumination and color diffusion. Although
considerable advances and enhancement techniques achieved in resolving these
issues, they treat low-frequency information equally across the entire channel,
which results in limiting the network's representativeness. We propose a deep
learning and feature-attention-based end-to-end network (FA-Net) to solve this
problem. In particular, we propose a Residual Feature Attention Block (RFAB),
containing the channel attention, pixel attention, and residual learning
mechanism with long and short skip connections. RFAB allows the network to
focus on learning high-frequency information while skipping low-frequency
information on multi-hop connections. The channel and pixel attention mechanism
considers each channel's different features and the uneven distribution of haze
over different pixels in the image. The experimental results shows that the
FA-Net propose by us provides higher accuracy, quantitatively and qualitatively
and superiority to previous state-of-the-art methods.
</p></li>
</ul>

<h3>Title: Physics-Informed DeepMRI: Bridging the Gap from Heat Diffusion to k-Space Interpolation. (arXiv:2308.15918v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.15918">http://arxiv.org/abs/2308.15918</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.15918]] Physics-Informed DeepMRI: Bridging the Gap from Heat Diffusion to k-Space Interpolation(http://arxiv.org/abs/2308.15918)</code></li>
<li>Summary: <p>In the field of parallel imaging (PI), alongside image-domain regularization
methods, substantial research has been dedicated to exploring $k$-space
interpolation. However, the interpretability of these methods remains an
unresolved issue. Furthermore, these approaches currently face acceleration
limitations that are comparable to those experienced by image-domain methods.
In order to enhance interpretability and overcome the acceleration limitations,
this paper introduces an interpretable framework that unifies both $k$-space
interpolation techniques and image-domain methods, grounded in the physical
principles of heat diffusion equations. Building upon this foundational
framework, a novel $k$-space interpolation method is proposed. Specifically, we
model the process of high-frequency information attenuation in $k$-space as a
heat diffusion equation, while the effort to reconstruct high-frequency
information from low-frequency regions can be conceptualized as a reverse heat
equation. However, solving the reverse heat equation poses a challenging
inverse problem. To tackle this challenge, we modify the heat equation to align
with the principles of magnetic resonance PI physics and employ the score-based
generative method to precisely execute the modified reverse heat diffusion.
Finally, experimental validation conducted on publicly available datasets
demonstrates the superiority of the proposed approach over traditional
$k$-space interpolation methods, deep learning-based $k$-space interpolation
methods, and conventional diffusion models in terms of reconstruction accuracy,
particularly in high-frequency regions.
</p></li>
</ul>

<h3>Title: DiffuVolume: Diffusion Model for Volume based Stereo Matching. (arXiv:2308.15989v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.15989">http://arxiv.org/abs/2308.15989</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.15989]] DiffuVolume: Diffusion Model for Volume based Stereo Matching(http://arxiv.org/abs/2308.15989)</code></li>
<li>Summary: <p>Stereo matching is a significant part in many computer vision tasks and
driving-based applications. Recently cost volume-based methods have achieved
great success benefiting from the rich geometry information in paired images.
However, the redundancy of cost volume also interferes with the model training
and limits the performance. To construct a more precise cost volume, we
pioneeringly apply the diffusion model to stereo matching. Our method, termed
DiffuVolume, considers the diffusion model as a cost volume filter, which will
recurrently remove the redundant information from the cost volume. Two main
designs make our method not trivial. Firstly, to make the diffusion model more
adaptive to stereo matching, we eschew the traditional manner of directly
adding noise into the image but embed the diffusion model into a task-specific
module. In this way, we outperform the traditional diffusion stereo matching
method by 22% EPE improvement and 240 times inference acceleration. Secondly,
DiffuVolume can be easily embedded into any volume-based stereo matching
network with boost performance but slight parameters rise (only 2%). By adding
the DiffuVolume into well-performed methods, we outperform all the published
methods on Scene Flow, KITTI2012, KITTI2015 benchmarks and zero-shot
generalization setting. It is worth mentioning that the proposed model ranks
1st on KITTI 2012 leader board, 2nd on KITTI 2015 leader board since 15, July
2023.
</p></li>
</ul>

<h3>Title: SignDiff: Learning Diffusion Models for American Sign Language Production. (arXiv:2308.16082v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.16082">http://arxiv.org/abs/2308.16082</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.16082]] SignDiff: Learning Diffusion Models for American Sign Language Production(http://arxiv.org/abs/2308.16082)</code></li>
<li>Summary: <p>The field of Sign Language Production (SLP) lacked a large-scale, pre-trained
model based on deep learning for continuous American Sign Language (ASL)
production in the past decade. This limitation hampers communication for all
individuals with disabilities relying on ASL. To address this issue, we
undertook the secondary development and utilization of How2Sign, one of the
largest publicly available ASL datasets. Despite its significance, prior
researchers in the field of sign language have not effectively employed this
corpus due to the intricacies involved in American Sign Language Production
(ASLP).
</p>
<p>To conduct large-scale ASLP, we propose SignDiff based on the latest work in
related fields, which is a dual-condition diffusion pre-training model that can
generate human sign language speakers from a skeleton pose. SignDiff has a
novel Frame Reinforcement Network called FR-Net, similar to dense human pose
estimation work, which enhances the correspondence between text lexical symbols
and sign language dense pose frames reduce the occurrence of multiple fingers
in the diffusion model. In addition, our ASLP method proposes two new improved
modules and a new loss function to improve the accuracy and quality of sign
language skeletal posture and enhance the ability of the model to train on
large-scale data.
</p>
<p>We propose the first baseline for ASL production and report the scores of
17.19 and 12.85 on BLEU-4 on the How2Sign dev/test sets. We also evaluated our
model on the previous mainstream dataset called PHOENIX14T, and the main
experiments achieved the results of SOTA. In addition, our image quality far
exceeds all previous results by 10 percentage points on the SSIM indicator.
Finally, we conducted ablation studies and qualitative evaluations for
discussion.
</p></li>
</ul>

<h2>self-supervised</h2>
<h2>foundation model</h2>
<h3>Title: Multimodal Foundation Models For Echocardiogram Interpretation. (arXiv:2308.15670v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.15670">http://arxiv.org/abs/2308.15670</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.15670]] Multimodal Foundation Models For Echocardiogram Interpretation(http://arxiv.org/abs/2308.15670)</code></li>
<li>Summary: <p>Multimodal deep learning foundation models can learn the relationship between
images and text. In the context of medical imaging, mapping images to language
concepts reflects the clinical task of diagnostic image interpretation, however
current general-purpose foundation models do not perform well in this context
because their training corpus have limited medical text and images. To address
this challenge and account for the range of cardiac physiology, we leverage
1,032,975 cardiac ultrasound videos and corresponding expert interpretations to
develop EchoCLIP, a multimodal foundation model for echocardiography. EchoCLIP
displays strong zero-shot (not explicitly trained) performance in cardiac
function assessment (external validation left ventricular ejection fraction
mean absolute error (MAE) of 7.1%) and identification of implanted intracardiac
devices (areas under the curve (AUC) between 0.84 and 0.98 for pacemakers and
artificial heart valves). We also developed a long-context variant (EchoCLIP-R)
with a custom echocardiography report text tokenizer which can accurately
identify unique patients across multiple videos (AUC of 0.86), identify
clinical changes such as orthotopic heart transplants (AUC of 0.79) or cardiac
surgery (AUC 0.77), and enable robust image-to-text search (mean cross-modal
retrieval rank in the top 1% of candidate text reports). These emergent
capabilities can be used for preliminary assessment and summarization of
echocardiographic findings.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Unveiling Camouflage: A Learnable Fourier-based Augmentation for Camouflaged Object Detection and Instance Segmentation. (arXiv:2308.15660v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.15660">http://arxiv.org/abs/2308.15660</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.15660]] Unveiling Camouflage: A Learnable Fourier-based Augmentation for Camouflaged Object Detection and Instance Segmentation(http://arxiv.org/abs/2308.15660)</code></li>
<li>Summary: <p>Camouflaged object detection (COD) and camouflaged instance segmentation
(CIS) aim to recognize and segment objects that are blended into their
surroundings, respectively. While several deep neural network models have been
proposed to tackle those tasks, augmentation methods for COD and CIS have not
been thoroughly explored. Augmentation strategies can help improve the
performance of models by increasing the size and diversity of the training data
and exposing the model to a wider range of variations in the data. Besides, we
aim to automatically learn transformations that help to reveal the underlying
structure of camouflaged objects and allow the model to learn to better
identify and segment camouflaged objects. To achieve this, we propose a
learnable augmentation method in the frequency domain for COD and CIS via
Fourier transform approach, dubbed CamoFourier. Our method leverages a
conditional generative adversarial network and cross-attention mechanism to
generate a reference image and an adaptive hybrid swapping with parameters to
mix the low-frequency component of the reference image and the high-frequency
component of the input image. This approach aims to make camouflaged objects
more visible for detection and segmentation models. Without bells and whistles,
our proposed augmentation method boosts the performance of camouflaged object
detectors and camouflaged instance segmenters by large margins.
</p></li>
</ul>

<h3>Title: DTrOCR: Decoder-only Transformer for Optical Character Recognition. (arXiv:2308.15996v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.15996">http://arxiv.org/abs/2308.15996</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.15996]] DTrOCR: Decoder-only Transformer for Optical Character Recognition(http://arxiv.org/abs/2308.15996)</code></li>
<li>Summary: <p>Typical text recognition methods rely on an encoder-decoder structure, in
which the encoder extracts features from an image, and the decoder produces
recognized text from these features. In this study, we propose a simpler and
more effective method for text recognition, known as the Decoder-only
Transformer for Optical Character Recognition (DTrOCR). This method uses a
decoder-only Transformer to take advantage of a generative language model that
is pre-trained on a large corpus. We examined whether a generative language
model that has been successful in natural language processing can also be
effective for text recognition in computer vision. Our experiments demonstrated
that DTrOCR outperforms current state-of-the-art methods by a large margin in
the recognition of printed, handwritten, and scene text in both English and
Chinese.
</p></li>
</ul>

<h3>Title: Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models. (arXiv:2308.16149v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.16149">http://arxiv.org/abs/2308.16149</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.16149]] Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models(http://arxiv.org/abs/2308.16149)</code></li>
<li>Summary: <p>We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric
foundation and instruction-tuned open generative large language models (LLMs).
The models are based on the GPT-3 decoder-only architecture and are pretrained
on a mixture of Arabic and English texts, including source code in various
programming languages. With 13 billion parameters, they demonstrate better
knowledge and reasoning capabilities in Arabic than any existing open Arabic
and multilingual models by a sizable margin, based on extensive evaluation.
Moreover, the models are competitive in English compared to English-centric
open models of similar size, despite being trained on much less English data.
We provide a detailed description of the training, the tuning, the safety
alignment, and the evaluation of the models. We release two open versions of
the model -- the foundation Jais model, and an instruction-tuned Jais-chat
variant -- with the aim of promoting research on Arabic LLMs. Available at
https://huggingface.co/inception-mbzuai/jais-13b-chat
</p></li>
</ul>

<h3>Title: On the Steganographic Capacity of Selected Learning Models. (arXiv:2308.15502v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.15502">http://arxiv.org/abs/2308.15502</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.15502]] On the Steganographic Capacity of Selected Learning Models(http://arxiv.org/abs/2308.15502)</code></li>
<li>Summary: <p>Machine learning and deep learning models are potential vectors for various
attack scenarios. For example, previous research has shown that malware can be
hidden in deep learning models. Hiding information in a learning model can be
viewed as a form of steganography. In this research, we consider the general
question of the steganographic capacity of learning models. Specifically, for a
wide range of models, we determine the number of low-order bits of the trained
parameters that can be overwritten, without adversely affecting model
performance. For each model considered, we graph the accuracy as a function of
the number of low-order bits that have been overwritten, and for selected
models, we also analyze the steganographic capacity of individual layers. The
models that we test include the classic machine learning techniques of Linear
Regression (LR) and Support Vector Machine (SVM); the popular general deep
learning models of Multilayer Perceptron (MLP) and Convolutional Neural Network
(CNN); the highly-successful Recurrent Neural Network (RNN) architecture of
Long Short-Term Memory (LSTM); the pre-trained transfer learning-based models
VGG16, DenseNet121, InceptionV3, and Xception; and, finally, an Auxiliary
Classifier Generative Adversarial Network (ACGAN). In all cases, we find that a
majority of the bits of each trained parameter can be overwritten before the
accuracy degrades. Of the models tested, the steganographic capacity ranges
from 7.04 KB for our LR experiments, to 44.74 MB for InceptionV3. We discuss
the implications of our results and consider possible avenues for further
research.
</p></li>
</ul>

<h3>Title: Fully Embedded Time-Series Generative Adversarial Networks. (arXiv:2308.15730v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.15730">http://arxiv.org/abs/2308.15730</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.15730]] Fully Embedded Time-Series Generative Adversarial Networks(http://arxiv.org/abs/2308.15730)</code></li>
<li>Summary: <p>Generative Adversarial Networks (GANs) should produce synthetic data that
fits the underlying distribution of the data being modeled. For real valued
time-series data, this implies the need to simultaneously capture the static
distribution of the data, but also the full temporal distribution of the data
for any potential time horizon. This temporal element produces a more complex
problem that can potentially leave current solutions under-constrained,
unstable during training, or prone to varying degrees of mode collapse. In
FETSGAN, entire sequences are translated directly to the generator's sampling
space using a seq2seq style adversarial auto encoder (AAE), where adversarial
training is used to match the training distribution in both the feature space
and the lower dimensional sampling space. This additional constraint provides a
loose assurance that the temporal distribution of the synthetic samples will
not collapse. In addition, the First Above Threshold (FAT) operator is
introduced to supplement the reconstruction of encoded sequences, which
improves training stability and the overall quality of the synthetic data being
generated. These novel contributions demonstrate a significant improvement to
the current state of the art for adversarial learners in qualitative measures
of temporal similarity and quantitative predictive ability of data generated
through FETSGAN.
</p></li>
</ul>

<h2>anomaly</h2>
<h3>Title: AnoVL: Adapting Vision-Language Models for Unified Zero-shot Anomaly Localization. (arXiv:2308.15939v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.15939">http://arxiv.org/abs/2308.15939</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.15939]] AnoVL: Adapting Vision-Language Models for Unified Zero-shot Anomaly Localization(http://arxiv.org/abs/2308.15939)</code></li>
<li>Summary: <p>Contrastive Language-Image Pre-training (CLIP) models have shown promising
performance on zero-shot visual recognition tasks by learning visual
representations under natural language supervision. Recent studies attempt the
use of CLIP to tackle zero-shot anomaly detection by matching images with
normal and abnormal state prompts. However, since CLIP focuses on building
correspondence between paired text prompts and global image-level
representations, the lack of patch-level vision to text alignment limits its
capability on precise visual anomaly localization. In this work, we introduce a
training-free adaptation (TFA) framework of CLIP for zero-shot anomaly
localization. In the visual encoder, we innovate a training-free value-wise
attention mechanism to extract intrinsic local tokens of CLIP for patch-level
local description. From the perspective of text supervision, we particularly
design a unified domain-aware contrastive state prompting template. On top of
the proposed TFA, we further introduce a test-time adaptation (TTA) mechanism
to refine anomaly localization results, where a layer of trainable parameters
in the adapter is optimized using TFA's pseudo-labels and synthetic
noise-corrupted tokens. With both TFA and TTA adaptation, we significantly
exploit the potential of CLIP for zero-shot anomaly localization and
demonstrate the effectiveness of our proposed methods on various datasets.
</p></li>
</ul>

<h2>in-context</h2>
<h3>Title: Text-to-OverpassQL: A Natural Language Interface for Complex Geodata Querying of OpenStreetMap. (arXiv:2308.16060v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2308.16060">http://arxiv.org/abs/2308.16060</a></li>
<li>Code URL: https://github.com/raphael-sch/overpassnl</li>
<li>Copy Paste: <code><input type="checkbox">[[2308.16060]] Text-to-OverpassQL: A Natural Language Interface for Complex Geodata Querying of OpenStreetMap(http://arxiv.org/abs/2308.16060)</code></li>
<li>Summary: <p>We present Text-to-OverpassQL, a task designed to facilitate a natural
language interface for querying geodata from OpenStreetMap (OSM). The Overpass
Query Language (OverpassQL) allows users to formulate complex database queries
and is widely adopted in the OSM ecosystem. Generating Overpass queries from
natural language input serves multiple use-cases. It enables novice users to
utilize OverpassQL without prior knowledge, assists experienced users with
crafting advanced queries, and enables tool-augmented large language models to
access information stored in the OSM database. In order to assess the
performance of current sequence generation models on this task, we propose
OverpassNL, a dataset of 8,352 queries with corresponding natural language
inputs. We further introduce task specific evaluation metrics and ground the
evaluation of the Text-to-OverpassQL task by executing the queries against the
OSM database. We establish strong baselines by finetuning sequence-to-sequence
models and adapting large language models with in-context examples. The
detailed evaluation reveals strengths and weaknesses of the considered learning
strategies, laying the foundations for further research into the
Text-to-OverpassQL task.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
