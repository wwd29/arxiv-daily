<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>diffusion</h2>
<h3>Title: RePoseDM: Recurrent Pose Alignment and Gradient Guidance for Pose Guided Image Synthesis. (arXiv:2310.16074v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16074">http://arxiv.org/abs/2310.16074</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16074]] RePoseDM: Recurrent Pose Alignment and Gradient Guidance for Pose Guided Image Synthesis(http://arxiv.org/abs/2310.16074)</code></li>
<li>Summary: <p>Pose-guided person image synthesis task requires re-rendering a reference
image, which should have a photorealistic appearance and flawless pose
transfer. Since person images are highly structured, existing approaches
require dense connections for complex deformations and occlusions because these
are generally handled through multi-level warping and masking in latent space.
But the feature maps generated by convolutional neural networks do not have
equivariance, and hence even the multi-level warping does not have a perfect
pose alignment. Inspired by the ability of the diffusion model to generate
photorealistic images from the given conditional guidance, we propose recurrent
pose alignment to provide pose-aligned texture features as conditional
guidance. Moreover, we propose gradient guidance from pose interaction fields,
which output the distance from the valid pose manifold given a target pose as
input. This helps in learning plausible pose transfer trajectories that result
in photorealism and undistorted texture details. Extensive results on two
large-scale benchmarks and a user study demonstrate the ability of our proposed
approach to generate photorealistic pose transfer under challenging scenarios.
Additionally, we prove the efficiency of gradient guidance in pose-guided image
generation on the HumanArt dataset with fine-tuned stable diffusion.
</p></li>
</ul>

<h3>Title: iNVS: Repurposing Diffusion Inpainters for Novel View Synthesis. (arXiv:2310.16167v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16167">http://arxiv.org/abs/2310.16167</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16167]] iNVS: Repurposing Diffusion Inpainters for Novel View Synthesis(http://arxiv.org/abs/2310.16167)</code></li>
<li>Summary: <p>We present a method for generating consistent novel views from a single
source image. Our approach focuses on maximizing the reuse of visible pixels
from the source image. To achieve this, we use a monocular depth estimator that
transfers visible pixels from the source view to the target view. Starting from
a pre-trained 2D inpainting diffusion model, we train our method on the
large-scale Objaverse dataset to learn 3D object priors. While training we use
a novel masking mechanism based on epipolar lines to further improve the
quality of our approach. This allows our framework to perform zero-shot novel
view synthesis on a variety of objects. We evaluate the zero-shot abilities of
our framework on three challenging datasets: Google Scanned Objects, Ray Traced
Multiview, and Common Objects in 3D. See our webpage for more details:
https://yashkant.github.io/invs/
</p></li>
</ul>

<h3>Title: Dolfin: Diffusion Layout Transformers without Autoencoder. (arXiv:2310.16305v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16305">http://arxiv.org/abs/2310.16305</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16305]] Dolfin: Diffusion Layout Transformers without Autoencoder(http://arxiv.org/abs/2310.16305)</code></li>
<li>Summary: <p>In this paper, we introduce a novel generative model, Diffusion Layout
Transformers without Autoencoder (Dolfin), which significantly improves the
modeling capability with reduced complexity compared to existing methods.
Dolfin employs a Transformer-based diffusion process to model layout
generation. In addition to an efficient bi-directional (non-causal joint)
sequence representation, we further propose an autoregressive diffusion model
(Dolfin-AR) that is especially adept at capturing rich semantic correlations
for the neighboring objects, such as alignment, size, and overlap. When
evaluated against standard generative layout benchmarks, Dolfin notably
improves performance across various metrics (fid, alignment, overlap, MaxIoU
and DocSim scores), enhancing transparency and interoperability in the process.
Moreover, Dolfin's applications extend beyond layout generation, making it
suitable for modeling geometric structures, such as line segments. Our
experiments present both qualitative and quantitative results to demonstrate
the advantages of Dolfin.
</p></li>
</ul>

<h3>Title: DiffRef3D: A Diffusion-based Proposal Refinement Framework for 3D Object Detection. (arXiv:2310.16349v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16349">http://arxiv.org/abs/2310.16349</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16349]] DiffRef3D: A Diffusion-based Proposal Refinement Framework for 3D Object Detection(http://arxiv.org/abs/2310.16349)</code></li>
<li>Summary: <p>Denoising diffusion models show remarkable performances in generative tasks,
and their potential applications in perception tasks are gaining interest. In
this paper, we introduce a novel framework named DiffRef3D which adopts the
diffusion process on 3D object detection with point clouds for the first time.
Specifically, we formulate the proposal refinement stage of two-stage 3D object
detectors as a conditional diffusion process. During training, DiffRef3D
gradually adds noise to the residuals between proposals and target objects,
then applies the noisy residuals to proposals to generate hypotheses. The
refinement module utilizes these hypotheses to denoise the noisy residuals and
generate accurate box predictions. In the inference phase, DiffRef3D generates
initial hypotheses by sampling noise from a Gaussian distribution as residuals
and refines the hypotheses through iterative steps. DiffRef3D is a versatile
proposal refinement framework that consistently improves the performance of
existing 3D object detection models. We demonstrate the significance of
DiffRef3D through extensive experiments on the KITTI benchmark. Code will be
available.
</p></li>
</ul>

<h3>Title: Fuse Your Latents: Video Editing with Multi-source Latent Diffusion Models. (arXiv:2310.16400v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16400">http://arxiv.org/abs/2310.16400</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16400]] Fuse Your Latents: Video Editing with Multi-source Latent Diffusion Models(http://arxiv.org/abs/2310.16400)</code></li>
<li>Summary: <p>Latent Diffusion Models (LDMs) are renowned for their powerful capabilities
in image and video synthesis. Yet, video editing methods suffer from
insufficient pre-training data or video-by-video re-training cost. In
addressing this gap, we propose FLDM (Fused Latent Diffusion Model), a
training-free framework to achieve text-guided video editing by applying
off-the-shelf image editing methods in video LDMs. Specifically, FLDM fuses
latents from an image LDM and an video LDM during the denoising process. In
this way, temporal consistency can be kept with video LDM while high-fidelity
from the image LDM can also be exploited. Meanwhile, FLDM possesses high
flexibility since both image LDM and video LDM can be replaced so advanced
image editing methods such as InstructPix2Pix and ControlNet can be exploited.
To the best of our knowledge, FLDM is the first method to adapt off-the-shelf
image editing methods into video LDMs for video editing. Extensive quantitative
and qualitative experiments demonstrate that FLDM can improve the textual
alignment and temporal consistency of edited videos.
</p></li>
</ul>

<h3>Title: Adapt Anything: Tailor Any Image Classifiers across Domains And Categories Using Text-to-Image Diffusion Models. (arXiv:2310.16573v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16573">http://arxiv.org/abs/2310.16573</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16573]] Adapt Anything: Tailor Any Image Classifiers across Domains And Categories Using Text-to-Image Diffusion Models(http://arxiv.org/abs/2310.16573)</code></li>
<li>Summary: <p>We do not pursue a novel method in this paper, but aim to study if a modern
text-to-image diffusion model can tailor any task-adaptive image classifier
across domains and categories. Existing domain adaptive image classification
works exploit both source and target data for domain alignment so as to
transfer the knowledge learned from the labeled source data to the unlabeled
target data. However, as the development of the text-to-image diffusion model,
we wonder if the high-fidelity synthetic data from the text-to-image generator
can serve as a surrogate of the source data in real world. In this way, we do
not need to collect and annotate the source data for each domain adaptation
task in a one-for-one manner. Instead, we utilize only one off-the-shelf
text-to-image model to synthesize images with category labels derived from the
corresponding text prompts, and then leverage the surrogate data as a bridge to
transfer the knowledge embedded in the task-agnostic text-to-image generator to
the task-oriented image classifier via domain adaptation. Such a one-for-all
adaptation paradigm allows us to adapt anything in the world using only one
text-to-image generator as well as the corresponding unlabeled target data.
Extensive experiments validate the feasibility of the proposed idea, which even
surpasses the state-of-the-art domain adaptation works using the source data
collected and annotated in real world.
</p></li>
</ul>

<h3>Title: A Picture is Worth a Thousand Words: Principled Recaptioning Improves Image Generation. (arXiv:2310.16656v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16656">http://arxiv.org/abs/2310.16656</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16656]] A Picture is Worth a Thousand Words: Principled Recaptioning Improves Image Generation(http://arxiv.org/abs/2310.16656)</code></li>
<li>Summary: <p>Text-to-image diffusion models achieved a remarkable leap in capabilities
over the last few years, enabling high-quality and diverse synthesis of images
from a textual prompt. However, even the most advanced models often struggle to
precisely follow all of the directions in their prompts. The vast majority of
these models are trained on datasets consisting of (image, caption) pairs where
the images often come from the web, and the captions are their HTML alternate
text. A notable example is the LAION dataset, used by Stable Diffusion and
other models. In this work we observe that these captions are often of low
quality, and argue that this significantly affects the model's capability to
understand nuanced semantics in the textual prompts. We show that by relabeling
the corpus with a specialized automatic captioning model and training a
text-to-image model on the recaptioned dataset, the model benefits
substantially across the board. First, in overall image quality: e.g. FID 14.84
vs. the baseline of 17.87, and 64.3% improvement in faithful image generation
according to human evaluation. Second, in semantic alignment, e.g. semantic
object accuracy 84.34 vs. 78.90, counting alignment errors 1.32 vs. 1.44 and
positional alignment 62.42 vs. 57.60. We analyze various ways to relabel the
corpus and provide evidence that this technique, which we call RECAP, both
reduces the train-inference discrepancy and provides the model with more
information per example, increasing sample efficiency and allowing the model to
better understand the relations between captions and images.
</p></li>
</ul>

<h3>Title: On the Proactive Generation of Unsafe Images From Text-To-Image Models Using Benign Prompts. (arXiv:2310.16613v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16613">http://arxiv.org/abs/2310.16613</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16613]] On the Proactive Generation of Unsafe Images From Text-To-Image Models Using Benign Prompts(http://arxiv.org/abs/2310.16613)</code></li>
<li>Summary: <p>Text-to-image models like Stable Diffusion have had a profound impact on
daily life by enabling the generation of photorealistic images from textual
prompts, fostering creativity, and enhancing visual experiences across various
applications. However, these models also pose risks. Previous studies have
successfully demonstrated that manipulated prompts can elicit text-to-image
models to generate unsafe images, e.g., hateful meme variants. Yet, these
studies only unleash the harmful power of text-to-image models in a passive
manner. In this work, we focus on the proactive generation of unsafe images
using targeted benign prompts via poisoning attacks. We propose two poisoning
attacks: a basic attack and a utility-preserving attack. We qualitatively and
quantitatively evaluate the proposed attacks using four representative hateful
memes and multiple query prompts. Experimental results indicate that
text-to-image models are vulnerable to the basic attack even with five
poisoning samples. However, the poisoning effect can inadvertently spread to
non-targeted prompts, leading to undesirable side effects. Root cause analysis
identifies conceptual similarity as an important contributing factor to the
side effects. To address this, we introduce the utility-preserving attack as a
viable mitigation strategy to maintain the attack stealthiness, while ensuring
decent attack performance. Our findings underscore the potential risks of
adopting text-to-image models in real-world scenarios, calling for future
research and safety measures in this space.
</p></li>
</ul>

<h2>self-supervised</h2>
<h3>Title: MyriadAL: Active Few Shot Learning for Histopathology. (arXiv:2310.16161v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16161">http://arxiv.org/abs/2310.16161</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16161]] MyriadAL: Active Few Shot Learning for Histopathology(http://arxiv.org/abs/2310.16161)</code></li>
<li>Summary: <p>Active Learning (AL) and Few Shot Learning (FSL) are two label-efficient
methods which have achieved excellent results recently. However, most prior
arts in both learning paradigms fail to explore the wealth of the vast
unlabelled data. In this study, we address this issue in the scenario where the
annotation budget is very limited, yet a large amount of unlabelled data for
the target task is available. We frame this work in the context of
histopathology where labelling is prohibitively expensive. To this end, we
introduce an active few shot learning framework, Myriad Active Learning (MAL),
including a contrastive-learning encoder, pseudo-label generation, and novel
query sample selection in the loop. Specifically, we propose to massage
unlabelled data in a self-supervised manner, where the obtained data
representations and clustering knowledge form the basis to activate the AL
loop. With feedback from the oracle in each AL cycle, the pseudo-labels of the
unlabelled data are refined by optimizing a shallow task-specific net on top of
the encoder. These updated pseudo-labels serve to inform and improve the active
learning query selection process. Furthermore, we introduce a novel recipe to
combine existing uncertainty measures and utilize the entire uncertainty list
to reduce sample redundancy in AL. Extensive experiments on two public
histopathology datasets show that MAL has superior test accuracy, macro
F1-score, and label efficiency compared to prior works, and can achieve a
comparable test accuracy to a fully supervised algorithm while labelling only
5% of the dataset.
</p></li>
</ul>

<h3>Title: ShadowSense: Unsupervised Domain Adaptation and Feature Fusion for Shadow-Agnostic Tree Crown Detection from RGB-Thermal Drone Imagery. (arXiv:2310.16212v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16212">http://arxiv.org/abs/2310.16212</a></li>
<li>Code URL: https://github.com/rudrakshkapil/shadowsense</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16212]] ShadowSense: Unsupervised Domain Adaptation and Feature Fusion for Shadow-Agnostic Tree Crown Detection from RGB-Thermal Drone Imagery(http://arxiv.org/abs/2310.16212)</code></li>
<li>Summary: <p>Accurate detection of individual tree crowns from remote sensing data poses a
significant challenge due to the dense nature of forest canopy and the presence
of diverse environmental variations, e.g., overlapping canopies, occlusions,
and varying lighting conditions. Additionally, the lack of data for training
robust models adds another limitation in effectively studying complex forest
conditions. This paper presents a novel method for detecting shadowed tree
crowns and provides a challenging dataset comprising roughly 50k paired
RGB-thermal images to facilitate future research for illumination-invariant
detection. The proposed method (ShadowSense) is entirely self-supervised,
leveraging domain adversarial training without source domain annotations for
feature extraction and foreground feature alignment for feature pyramid
networks to adapt domain-invariant representations by focusing on visible
foreground regions, respectively. It then fuses complementary information of
both modalities to effectively improve upon the predictions of an RGB-trained
detector and boost the overall accuracy. Extensive experiments demonstrate the
superiority of the proposed method over both the baseline RGB-trained detector
and state-of-the-art techniques that rely on unsupervised domain adaptation or
early image fusion. Our code and data are available:
https://github.com/rudrakshkapil/ShadowSense
</p></li>
</ul>

<h3>Title: Show from Tell: Audio-Visual Modelling in Clinical Settings. (arXiv:2310.16477v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16477">http://arxiv.org/abs/2310.16477</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16477]] Show from Tell: Audio-Visual Modelling in Clinical Settings(http://arxiv.org/abs/2310.16477)</code></li>
<li>Summary: <p>Auditory and visual signals usually present together and correlate with each
other, not only in natural environments but also in clinical settings. However,
the audio-visual modelling in the latter case can be more challenging, due to
the different sources of audio/video signals and the noise (both signal-level
and semantic-level) in auditory signals -- usually speech. In this paper, we
consider audio-visual modelling in a clinical setting, providing a solution to
learn medical representations that benefit various clinical tasks, without
human expert annotation. A simple yet effective multi-modal self-supervised
learning framework is proposed for this purpose. The proposed approach is able
to localise anatomical regions of interest during ultrasound imaging, with only
speech audio as a reference. Experimental evaluations on a large-scale clinical
multi-modal ultrasound video dataset show that the proposed self-supervised
method learns good transferable anatomical representations that boost the
performance of automated downstream clinical tasks, even outperforming
fully-supervised solutions.
</p></li>
</ul>

<h3>Title: Modality-Agnostic Self-Supervised Learning with Meta-Learned Masked Auto-Encoder. (arXiv:2310.16318v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16318">http://arxiv.org/abs/2310.16318</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16318]] Modality-Agnostic Self-Supervised Learning with Meta-Learned Masked Auto-Encoder(http://arxiv.org/abs/2310.16318)</code></li>
<li>Summary: <p>Despite its practical importance across a wide range of modalities, recent
advances in self-supervised learning (SSL) have been primarily focused on a few
well-curated domains, e.g., vision and language, often relying on their
domain-specific knowledge. For example, Masked Auto-Encoder (MAE) has become
one of the popular architectures in these domains, but less has explored its
potential in other modalities. In this paper, we develop MAE as a unified,
modality-agnostic SSL framework. In turn, we argue meta-learning as a key to
interpreting MAE as a modality-agnostic learner, and propose enhancements to
MAE from the motivation to jointly improve its SSL across diverse modalities,
coined MetaMAE as a result. Our key idea is to view the mask reconstruction of
MAE as a meta-learning task: masked tokens are predicted by adapting the
Transformer meta-learner through the amortization of unmasked tokens. Based on
this novel interpretation, we propose to integrate two advanced meta-learning
techniques. First, we adapt the amortized latent of the Transformer encoder
using gradient-based meta-learning to enhance the reconstruction. Then, we
maximize the alignment between amortized and adapted latents through task
contrastive learning which guides the Transformer encoder to better encode the
task-specific knowledge. Our experiment demonstrates the superiority of MetaMAE
in the modality-agnostic SSL benchmark (called DABS), significantly
outperforming prior baselines. Code is available at
https://github.com/alinlab/MetaMAE.
</p></li>
</ul>

<h2>foundation model</h2>
<h3>Title: Confounder Balancing in Adversarial Domain Adaptation for Pre-Trained Large Models Fine-Tuning. (arXiv:2310.16062v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16062">http://arxiv.org/abs/2310.16062</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16062]] Confounder Balancing in Adversarial Domain Adaptation for Pre-Trained Large Models Fine-Tuning(http://arxiv.org/abs/2310.16062)</code></li>
<li>Summary: <p>The excellent generalization, contextual learning, and emergence abilities in
the pre-trained large models (PLMs) handle specific tasks without direct
training data, making them the better foundation models in the adversarial
domain adaptation (ADA) methods to transfer knowledge learned from the source
domain to target domains. However, existing ADA methods fail to account for the
confounder properly, which is the root cause of the source data distribution
that differs from the target domains. This study proposes an adversarial domain
adaptation with confounder balancing for PLMs fine-tuning (ADA-CBF). The
ADA-CBF includes a PLM as the foundation model for a feature extractor, a
domain classifier and a confounder classifier, and they are jointly trained
with an adversarial loss. This loss is designed to improve the domain-invariant
representation learning by diluting the discrimination in the domain
classifier. At the same time, the adversarial loss also balances the confounder
distribution among source and unmeasured domains in training. Compared to
existing ADA methods, ADA-CBF can correctly identify confounders in
domain-invariant features, thereby eliminating the confounder biases in the
extracted features from PLMs. The confounder classifier in ADA-CBF is designed
as a plug-and-play and can be applied in the confounder measurable,
unmeasurable, or partially measurable environments. Empirical results on
natural language processing and computer vision downstream tasks show that
ADA-CBF outperforms the newest GPT-4, LLaMA2, ViT and ADA methods.
</p></li>
</ul>

<h3>Title: Towards long-tailed, multi-label disease classification from chest X-ray: Overview of the CXR-LT challenge. (arXiv:2310.16112v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16112">http://arxiv.org/abs/2310.16112</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16112]] Towards long-tailed, multi-label disease classification from chest X-ray: Overview of the CXR-LT challenge(http://arxiv.org/abs/2310.16112)</code></li>
<li>Summary: <p>Many real-world image recognition problems, such as diagnostic medical
imaging exams, are "long-tailed" $\unicode{x2013}$ there are a few common
findings followed by many more relatively rare conditions. In chest
radiography, diagnosis is both a long-tailed and multi-label problem, as
patients often present with multiple findings simultaneously. While researchers
have begun to study the problem of long-tailed learning in medical image
recognition, few have studied the interaction of label imbalance and label
co-occurrence posed by long-tailed, multi-label disease classification. To
engage with the research community on this emerging topic, we conducted an open
challenge, CXR-LT, on long-tailed, multi-label thorax disease classification
from chest X-rays (CXRs). We publicly release a large-scale benchmark dataset
of over 350,000 CXRs, each labeled with at least one of 26 clinical findings
following a long-tailed distribution. We synthesize common themes of
top-performing solutions, providing practical recommendations for long-tailed,
multi-label medical image classification. Finally, we use these insights to
propose a path forward involving vision-language foundation models for few- and
zero-shot disease classification.
</p></li>
</ul>

<h3>Title: TiC-CLIP: Continual Training of CLIP Models. (arXiv:2310.16226v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16226">http://arxiv.org/abs/2310.16226</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16226]] TiC-CLIP: Continual Training of CLIP Models(http://arxiv.org/abs/2310.16226)</code></li>
<li>Summary: <p>Keeping large foundation models up to date on latest data is inherently
expensive. To avoid the prohibitive costs of constantly retraining, it is
imperative to continually train these models. This problem is exacerbated by
the lack of any large scale continual learning benchmarks or baselines. We
introduce the first set of web-scale Time-Continual (TiC) benchmarks for
training vision-language models: TiC-DataCompt, TiC-YFCC, and TiC-RedCaps with
over 12.7B timestamped image-text pairs spanning 9 years (2014--2022). We first
use our benchmarks to curate various dynamic evaluations to measure temporal
robustness of existing models. We show OpenAI's CLIP (trained on data up to
2020) loses $\approx 8\%$ zero-shot accuracy on our curated retrieval task from
2021--2022 compared with more recently trained models in OpenCLIP repository.
We then study how to efficiently train models on time-continuous data. We
demonstrate that a simple rehearsal-based approach that continues training from
the last checkpoint and replays old data reduces compute by $2.5\times$ when
compared to the standard practice of retraining from scratch.
</p></li>
</ul>

<h3>Title: Open-NeRF: Towards Open Vocabulary NeRF Decomposition. (arXiv:2310.16383v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16383">http://arxiv.org/abs/2310.16383</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16383]] Open-NeRF: Towards Open Vocabulary NeRF Decomposition(http://arxiv.org/abs/2310.16383)</code></li>
<li>Summary: <p>In this paper, we address the challenge of decomposing Neural Radiance Fields
(NeRF) into objects from an open vocabulary, a critical task for object
manipulation in 3D reconstruction and view synthesis. Current techniques for
NeRF decomposition involve a trade-off between the flexibility of processing
open-vocabulary queries and the accuracy of 3D segmentation. We present,
Open-vocabulary Embedded Neural Radiance Fields (Open-NeRF), that leverage
large-scale, off-the-shelf, segmentation models like the Segment Anything Model
(SAM) and introduce an integrate-and-distill paradigm with hierarchical
embeddings to achieve both the flexibility of open-vocabulary querying and 3D
segmentation accuracy. Open-NeRF first utilizes large-scale foundation models
to generate hierarchical 2D mask proposals from varying viewpoints. These
proposals are then aligned via tracking approaches and integrated within the 3D
space and subsequently distilled into the 3D field. This process ensures
consistent recognition and granularity of objects from different viewpoints,
even in challenging scenarios involving occlusion and indistinct features. Our
experimental results show that the proposed Open-NeRF outperforms
state-of-the-art methods such as LERF \cite{lerf} and FFD \cite{ffd} in
open-vocabulary scenarios. Open-NeRF offers a promising solution to NeRF
decomposition, guided by open-vocabulary queries, enabling novel applications
in robotics and vision-language interaction in open-world 3D scenes.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Segue: Side-information Guided Generative Unlearnable Examples for Facial Privacy Protection in Real World. (arXiv:2310.16061v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16061">http://arxiv.org/abs/2310.16061</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16061]] Segue: Side-information Guided Generative Unlearnable Examples for Facial Privacy Protection in Real World(http://arxiv.org/abs/2310.16061)</code></li>
<li>Summary: <p>The widespread use of face recognition technology has given rise to privacy
concerns, as many individuals are worried about the collection and utilization
of their facial data. To address these concerns, researchers are actively
exploring the concept of ``unlearnable examples", by adding imperceptible
perturbation to data in the model training stage, which aims to prevent the
model from learning discriminate features of the target face. However, current
methods are inefficient and cannot guarantee transferability and robustness at
the same time, causing impracticality in the real world. To remedy it, we
propose a novel method called Segue: Side-information guided generative
unlearnable examples. Specifically, we leverage a once-trained multiple-used
model to generate the desired perturbation rather than the time-consuming
gradient-based method. To improve transferability, we introduce side
information such as true labels and pseudo labels, which are inherently
consistent across different scenarios. For robustness enhancement, a distortion
layer is integrated into the training pipeline. Extensive experiments
demonstrate that the proposed Segue is much faster than previous methods
(1000$\times$) and achieves transferable effectiveness across different
datasets and model architectures. Furthermore, it can resist JPEG compression,
adversarial training, and some standard data augmentations.
</p></li>
</ul>

<h3>Title: Yin Yang Convolutional Nets: Image Manifold Extraction by the Analysis of Opposites. (arXiv:2310.16148v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16148">http://arxiv.org/abs/2310.16148</a></li>
<li>Code URL: https://github.com/nosaveddata/yinyang_cnn</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16148]] Yin Yang Convolutional Nets: Image Manifold Extraction by the Analysis of Opposites(http://arxiv.org/abs/2310.16148)</code></li>
<li>Summary: <p>Computer vision in general presented several advances such as training
optimizations, new architectures (pure attention, efficient block, vision
language models, generative models, among others). This have improved
performance in several tasks such as classification, and others. However, the
majority of these models focus on modifications that are taking distance from
realistic neuroscientific approaches related to the brain. In this work, we
adopt a more bio-inspired approach and present the Yin Yang Convolutional
Network, an architecture that extracts visual manifold, its blocks are intended
to separate analysis of colors and forms at its initial layers, simulating
occipital lobe's operations. Our results shows that our architecture provides
State-of-the-Art efficiency among low parameter architectures in the dataset
CIFAR-10. Our first model reached 93.32\% test accuracy, 0.8\% more than the
older SOTA in this category, while having 150k less parameters (726k in total).
Our second model uses 52k parameters, losing only 3.86\% test accuracy. We also
performed an analysis on ImageNet, where we reached 66.49\% validation accuracy
with 1.6M parameters. We make the code publicly available at:
https://github.com/NoSavedDATA/YinYang_CNN.
</p></li>
</ul>

<h3>Title: On the Foundations of Shortcut Learning. (arXiv:2310.16228v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16228">http://arxiv.org/abs/2310.16228</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16228]] On the Foundations of Shortcut Learning(http://arxiv.org/abs/2310.16228)</code></li>
<li>Summary: <p>Deep-learning models can extract a rich assortment of features from data.
Which features a model uses depends not only on predictivity-how reliably a
feature indicates train-set labels-but also on availability-how easily the
feature can be extracted, or leveraged, from inputs. The literature on shortcut
learning has noted examples in which models privilege one feature over another,
for example texture over shape and image backgrounds over foreground objects.
Here, we test hypotheses about which input properties are more available to a
model, and systematically study how predictivity and availability interact to
shape models' feature use. We construct a minimal, explicit generative
framework for synthesizing classification datasets with two latent features
that vary in predictivity and in factors we hypothesize to relate to
availability, and quantify a model's shortcut bias-its over-reliance on the
shortcut (more available, less predictive) feature at the expense of the core
(less available, more predictive) feature. We find that linear models are
relatively unbiased, but introducing a single hidden layer with ReLU or Tanh
units yields a bias. Our empirical findings are consistent with a theoretical
account based on Neural Tangent Kernels. Finally, we study how models used in
practice trade off predictivity and availability in naturalistic datasets,
discovering availability manipulations which increase models' degree of
shortcut bias. Taken together, these findings suggest that the propensity to
learn shortcut features is a fundamental characteristic of deep nonlinear
architectures warranting systematic study given its role in shaping how models
solve tasks.
</p></li>
</ul>

<h3>Title: GenKIE: Robust Generative Multimodal Document Key Information Extraction. (arXiv:2310.16131v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16131">http://arxiv.org/abs/2310.16131</a></li>
<li>Code URL: https://github.com/glasgow-ai4biomed/genkie</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16131]] GenKIE: Robust Generative Multimodal Document Key Information Extraction(http://arxiv.org/abs/2310.16131)</code></li>
<li>Summary: <p>Key information extraction (KIE) from scanned documents has gained increasing
attention because of its applications in various domains. Although promising
results have been achieved by some recent KIE approaches, they are usually
built based on discriminative models, which lack the ability to handle optical
character recognition (OCR) errors and require laborious token-level labelling.
In this paper, we propose a novel generative end-to-end model, named GenKIE, to
address the KIE task. GenKIE is a sequence-to-sequence multimodal generative
model that utilizes multimodal encoders to embed visual, layout and textual
features and a decoder to generate the desired output. Well-designed prompts
are leveraged to incorporate the label semantics as the weakly supervised
signals and entice the generation of the key information. One notable advantage
of the generative model is that it enables automatic correction of OCR errors.
Besides, token-level granular annotation is not required. Extensive experiments
on multiple public real-world datasets show that GenKIE effectively generalizes
over different types of documents and achieves state-of-the-art results. Our
experiments also validate the model's robustness against OCR errors, making
GenKIE highly applicable in real-world scenarios.
</p></li>
</ul>

<h3>Title: Is ChatGPT a Good Multi-Party Conversation Solver?. (arXiv:2310.16301v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16301">http://arxiv.org/abs/2310.16301</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16301]] Is ChatGPT a Good Multi-Party Conversation Solver?(http://arxiv.org/abs/2310.16301)</code></li>
<li>Summary: <p>Large Language Models (LLMs) have emerged as influential instruments within
the realm of natural language processing; nevertheless, their capacity to
handle multi-party conversations (MPCs) -- a scenario marked by the presence of
multiple interlocutors involved in intricate information exchanges -- remains
uncharted. In this paper, we delve into the potential of generative LLMs such
as ChatGPT and GPT-4 within the context of MPCs. An empirical analysis is
conducted to assess the zero-shot learning capabilities of ChatGPT and GPT-4 by
subjecting them to evaluation across three MPC datasets that encompass five
representative tasks. The findings reveal that ChatGPT's performance on a
number of evaluated MPC tasks leaves much to be desired, whilst GPT-4's results
portend a promising future. Additionally, we endeavor to bolster performance
through the incorporation of MPC structures, encompassing both speaker and
addressee architecture. This study provides an exhaustive evaluation and
analysis of applying generative LLMs to MPCs, casting a light upon the
conception and creation of increasingly effective and robust MPC agents.
Concurrently, this work underscores the challenges implicit in the utilization
of LLMs for MPCs, such as deciphering graphical information flows and
generating stylistically consistent responses.
</p></li>
</ul>

<h3>Title: CoheSentia: A Novel Benchmark of Incremental versus Holistic Assessment of Coherence in Generated Texts. (arXiv:2310.16329v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16329">http://arxiv.org/abs/2310.16329</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16329]] CoheSentia: A Novel Benchmark of Incremental versus Holistic Assessment of Coherence in Generated Texts(http://arxiv.org/abs/2310.16329)</code></li>
<li>Summary: <p>Coherence is a linguistic term that refers to the relations between small
textual units (sentences, propositions), which make the text logically
consistent and meaningful to the reader. With the advances of generative
foundational models in NLP, there is a pressing need to automatically assess
the human-perceived coherence of automatically generated texts. Up until now,
little work has been done on explicitly assessing the coherence of generated
texts and analyzing the factors contributing to (in)coherence. Previous work on
the topic used other tasks, e.g., sentence reordering, as proxies of coherence,
rather than approaching coherence detection heads on. In this paper, we
introduce {\sc CoheSentia}, a novel benchmark of human-perceived coherence of
automatically generated texts. Our annotation protocol reflects two
perspectives; one is global, assigning a single coherence score, and the other
is incremental, scoring sentence by sentence. The incremental method produces
an (in)coherence score for each text fragment and also pinpoints reasons for
incoherence at that point. Our benchmark contains 500 automatically-generated
and human-annotated paragraphs, each annotated in both methods, by multiple
raters. Our analysis shows that the inter-annotator agreement in the
incremental mode is higher than in the holistic alternative, and our
experiments show that standard LMs fine-tuned for coherence detection show
varied performance on the different factors contributing to (in)coherence. All
in all, these models yield unsatisfactory performance, emphasizing the need for
developing more reliable methods for coherence assessment.
</p></li>
</ul>

<h3>Title: Improving Diversity of Demographic Representation in Large Language Models via Collective-Critiques and Self-Voting. (arXiv:2310.16523v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16523">http://arxiv.org/abs/2310.16523</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16523]] Improving Diversity of Demographic Representation in Large Language Models via Collective-Critiques and Self-Voting(http://arxiv.org/abs/2310.16523)</code></li>
<li>Summary: <p>A crucial challenge for generative large language models (LLMs) is diversity:
when a user's prompt is under-specified, models may follow implicit assumptions
while generating a response, which may result in homogenization of the
responses, as well as certain demographic groups being under-represented or
even erased from the generated responses. In this paper, we formalize diversity
of representation in generative LLMs. We present evaluation datasets and
propose metrics to measure diversity in generated responses along people and
culture axes. We find that LLMs understand the notion of diversity, and that
they can reason and critique their own responses for that goal. This finding
motivated a new prompting technique called collective-critique and self-voting
(CCSV) to self-improve people diversity of LLMs by tapping into its diversity
reasoning capabilities, without relying on handcrafted examples or prompt
tuning. Extensive empirical experiments with both human and automated
evaluations show that our proposed approach is effective at improving people
and culture diversity, and outperforms all baseline methods by a large margin.
</p></li>
</ul>

<h3>Title: Alquist 5.0: Dialogue Trees Meet Generative Models. A Novel Approach for Enhancing SocialBot Conversations. (arXiv:2310.16119v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16119">http://arxiv.org/abs/2310.16119</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16119]] Alquist 5(http://arxiv.org/abs/2310.16119)</code></li>
<li>Summary: <p>We present our SocialBot -- Alquist~5.0 -- developed for the Alexa Prize
SocialBot Grand Challenge~5. Building upon previous versions of our system, we
introduce the NRG Barista and outline several innovative approaches for
integrating Barista into our SocialBot, improving the overall conversational
experience. Additionally, we extend our SocialBot to support multimodal
devices. This paper offers insights into the development of Alquist~5.0, which
meets evolving user expectations while maintaining empathetic and knowledgeable
conversational abilities across diverse topics.
</p></li>
</ul>

<h3>Title: Can You Rely on Your Model Evaluation? Improving Model Evaluation with Synthetic Test Data. (arXiv:2310.16524v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16524">http://arxiv.org/abs/2310.16524</a></li>
<li>Code URL: https://github.com/seedatnabeel/3s-testing</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16524]] Can You Rely on Your Model Evaluation? Improving Model Evaluation with Synthetic Test Data(http://arxiv.org/abs/2310.16524)</code></li>
<li>Summary: <p>Evaluating the performance of machine learning models on diverse and
underrepresented subgroups is essential for ensuring fairness and reliability
in real-world applications. However, accurately assessing model performance
becomes challenging due to two main issues: (1) a scarcity of test data,
especially for small subgroups, and (2) possible distributional shifts in the
model's deployment setting, which may not align with the available test data.
In this work, we introduce 3S Testing, a deep generative modeling framework to
facilitate model evaluation by generating synthetic test sets for small
subgroups and simulating distributional shifts. Our experiments demonstrate
that 3S Testing outperforms traditional baselines -- including real test data
alone -- in estimating model performance on minority subgroups and under
plausible distributional shifts. In addition, 3S offers intervals around its
performance estimates, exhibiting superior coverage of the ground truth
compared to existing approaches. Overall, these results raise the question of
whether we need a paradigm shift away from limited real test data towards
synthetic test data.
</p></li>
</ul>

<h3>Title: Free-form Flows: Make Any Architecture a Normalizing Flow. (arXiv:2310.16624v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16624">http://arxiv.org/abs/2310.16624</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16624]] Free-form Flows: Make Any Architecture a Normalizing Flow(http://arxiv.org/abs/2310.16624)</code></li>
<li>Summary: <p>Normalizing Flows are generative models that directly maximize the
likelihood. Previously, the design of normalizing flows was largely constrained
by the need for analytical invertibility. We overcome this constraint by a
training procedure that uses an efficient estimator for the gradient of the
change of variables formula. This enables any dimension-preserving neural
network to serve as a generative model through maximum likelihood training. Our
approach allows placing the emphasis on tailoring inductive biases precisely to
the task at hand. Specifically, we achieve excellent results in molecule
generation benchmarks utilizing $E(n)$-equivariant networks. Moreover, our
method is competitive in an inverse problem benchmark, while employing
off-the-shelf ResNet architectures.
</p></li>
</ul>

<h3>Title: Posterior Consistency for Missing Data in Variational Autoencoders. (arXiv:2310.16648v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16648">http://arxiv.org/abs/2310.16648</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16648]] Posterior Consistency for Missing Data in Variational Autoencoders(http://arxiv.org/abs/2310.16648)</code></li>
<li>Summary: <p>We consider the problem of learning Variational Autoencoders (VAEs), i.e., a
type of deep generative model, from data with missing values. Such data is
omnipresent in real-world applications of machine learning because complete
data is often impossible or too costly to obtain. We particularly focus on
improving a VAE's amortized posterior inference, i.e., the encoder, which in
the case of missing data can be susceptible to learning inconsistent posterior
distributions regarding the missingness. To this end, we provide a formal
definition of posterior consistency and propose an approach for regularizing an
encoder's posterior distribution which promotes this consistency. We observe
that the proposed regularization suggests a different training objective than
that typically considered in the literature when facing missing values.
Furthermore, we empirically demonstrate that our regularization leads to
improved performance in missing value settings in terms of reconstruction
quality and downstream tasks utilizing uncertainty in the latent space. This
improved performance can be observed for many classes of VAEs including VAEs
equipped with normalizing flows.
</p></li>
</ul>

<h2>anomaly</h2>
<h3>Title: On Pixel-level Performance Assessment in Anomaly Detection. (arXiv:2310.16435v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16435">http://arxiv.org/abs/2310.16435</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16435]] On Pixel-level Performance Assessment in Anomaly Detection(http://arxiv.org/abs/2310.16435)</code></li>
<li>Summary: <p>Anomaly detection methods have demonstrated remarkable success across various
applications. However, assessing their performance, particularly at the
pixel-level, presents a complex challenge due to the severe imbalance that is
most commonly present between normal and abnormal samples. Commonly adopted
evaluation metrics designed for pixel-level detection may not effectively
capture the nuanced performance variations arising from this class imbalance.
In this paper, we dissect the intricacies of this challenge, underscored by
visual evidence and statistical analysis, leading to delve into the need for
evaluation metrics that account for the imbalance. We offer insights into more
accurate metrics, using eleven leading contemporary anomaly detection methods
on twenty-one anomaly detection problems. Overall, from this extensive
experimental evaluation, we can conclude that Precision-Recall-based metrics
can better capture relative method performance, making them more suitable for
the task.
</p></li>
</ul>

<h3>Title: GADY: Unsupervised Anomaly Detection on Dynamic Graphs. (arXiv:2310.16376v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16376">http://arxiv.org/abs/2310.16376</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16376]] GADY: Unsupervised Anomaly Detection on Dynamic Graphs(http://arxiv.org/abs/2310.16376)</code></li>
<li>Summary: <p>Anomaly detection on dynamic graphs refers to detecting entities whose
behaviors obviously deviate from the norms observed within graphs and their
temporal information. This field has drawn increasing attention due to its
application in finance, network security, social networks, and more. However,
existing methods face two challenges: dynamic structure constructing challenge
- difficulties in capturing graph structure with complex time information and
negative sampling challenge - unable to construct excellent negative samples
for unsupervised learning. To address these challenges, we propose Unsupervised
Generative Anomaly Detection on Dynamic Graphs (GADY). To tackle the first
challenge, we propose a continuous dynamic graph model to capture the
fine-grained information, which breaks the limit of existing discrete methods.
Specifically, we employ a message-passing framework combined with positional
features to get edge embeddings, which are decoded to identify anomalies. For
the second challenge, we pioneer the use of Generative Adversarial Networks to
generate negative interactions. Moreover, we design a loss function to alter
the training goal of the generator while ensuring the diversity and quality of
generated samples. Extensive experiments demonstrate that our proposed GADY
significantly outperforms the previous state-of-the-art method on three
real-world datasets. Supplementary experiments further validate the
effectiveness of our model design and the necessity of each module.
</p></li>
</ul>

<h3>Title: Towards Self-Interpretable Graph-Level Anomaly Detection. (arXiv:2310.16520v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16520">http://arxiv.org/abs/2310.16520</a></li>
<li>Code URL: https://github.com/yixinliu233/signet</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16520]] Towards Self-Interpretable Graph-Level Anomaly Detection(http://arxiv.org/abs/2310.16520)</code></li>
<li>Summary: <p>Graph-level anomaly detection (GLAD) aims to identify graphs that exhibit
notable dissimilarity compared to the majority in a collection. However,
current works primarily focus on evaluating graph-level abnormality while
failing to provide meaningful explanations for the predictions, which largely
limits their reliability and application scope. In this paper, we investigate a
new challenging problem, explainable GLAD, where the learning objective is to
predict the abnormality of each graph sample with corresponding explanations,
i.e., the vital subgraph that leads to the predictions. To address this
challenging problem, we propose a Self-Interpretable Graph aNomaly dETection
model (SIGNET for short) that detects anomalous graphs as well as generates
informative explanations simultaneously. Specifically, we first introduce the
multi-view subgraph information bottleneck (MSIB) framework, serving as the
design basis of our self-interpretable GLAD approach. This way SIGNET is able
to not only measure the abnormality of each graph based on cross-view mutual
information but also provide informative graph rationales by extracting
bottleneck subgraphs from the input graph and its dual hypergraph in a
self-supervised way. Extensive experiments on 16 datasets demonstrate the
anomaly detection capability and self-interpretability of SIGNET.
</p></li>
</ul>

<h2>in-context</h2>
<h3>Title: Can You Follow Me? Testing Situational Understanding in ChatGPT. (arXiv:2310.16135v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16135">http://arxiv.org/abs/2310.16135</a></li>
<li>Code URL: https://github.com/yangalan123/situationaltesting</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16135]] Can You Follow Me? Testing Situational Understanding in ChatGPT(http://arxiv.org/abs/2310.16135)</code></li>
<li>Summary: <p>Understanding sentence meanings and updating information states appropriately
across time -- what we call "situational understanding" (SU) -- is a critical
ability for human-like AI agents. SU is essential in particular for chat
models, such as ChatGPT, to enable consistent, coherent, and effective dialogue
between humans and AI. Previous works have identified certain SU limitations in
non-chatbot Large Language models (LLMs), but the extent and causes of these
limitations are not well understood, and capabilities of current chat-based
models in this domain have not been explored. In this work we tackle these
questions, proposing a novel synthetic environment for SU testing which allows
us to do controlled and systematic testing of SU in chat-oriented models,
through assessment of models' ability to track and enumerate environment
states. Our environment also allows for close analysis of dynamics of model
performance, to better understand underlying causes for performance patterns.
We apply our test to ChatGPT, the state-of-the-art chatbot, and find that
despite the fundamental simplicity of the task, the model's performance
reflects an inability to retain correct environment states across time. Our
follow-up analyses suggest that performance degradation is largely because
ChatGPT has non-persistent in-context memory (although it can access the full
dialogue history) and it is susceptible to hallucinated updates -- including
updates that artificially inflate accuracies. Our findings suggest overall that
ChatGPT is not currently equipped for robust tracking of situation states, and
that trust in the impressive dialogue performance of ChatGPT comes with risks.
We release the codebase for reproducing our test environment, as well as all
prompts and API responses from ChatGPT, at
https://github.com/yangalan123/SituationalTesting.
</p></li>
</ul>

<h3>Title: CycleAlign: Iterative Distillation from Black-box LLM to White-box Models for Better Human Alignment. (arXiv:2310.16271v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.16271">http://arxiv.org/abs/2310.16271</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.16271]] CycleAlign: Iterative Distillation from Black-box LLM to White-box Models for Better Human Alignment(http://arxiv.org/abs/2310.16271)</code></li>
<li>Summary: <p>Language models trained on large-scale corpus often generate content that is
harmful, toxic, or contrary to human preferences, making their alignment with
human values a critical concern. Reinforcement learning from human feedback
(RLHF) with algorithms like PPO is a prevalent approach for alignment but is
often complex, unstable, and resource-intensive. Recently, ranking-based
alignment methods have emerged, offering stability and effectiveness by
replacing the RL framework with supervised fine-tuning, but they are costly due
to the need for annotated data. Considering that existing large language models
(LLMs) like ChatGPT are already relatively well-aligned and cost-friendly,
researchers have begun to align the language model with human preference from
AI feedback. The common practices, which unidirectionally distill the
instruction-following responses from LLMs, are constrained by their bottleneck.
Thus we introduce CycleAlign to distill alignment capabilities from
parameter-invisible LLMs (black-box) to a parameter-visible model (white-box)
in an iterative manner. With in-context learning (ICL) as the core of the
cycle, the black-box models are able to rank the model-generated responses
guided by human-craft instruction and demonstrations about their preferences.
During iterative interaction, the white-box models also have a judgment about
responses generated by them. Consequently, the agreement ranking could be
viewed as a pseudo label to dynamically update the in-context demonstrations
and improve the preference ranking ability of black-box models. Through
multiple interactions, the CycleAlign framework could align the white-box model
with the black-box model effectively in a low-resource way. Empirical results
illustrate that the model fine-tuned by CycleAlign remarkably exceeds existing
methods, and achieves the state-of-the-art performance in alignment with human
value.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
