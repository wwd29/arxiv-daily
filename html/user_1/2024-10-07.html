<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-10-07</h1>
<h3>Title: Guess What I Think: Streamlined EEG-to-Image Generation with Latent Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Eleonora Lopez, Luigi Sigillo, Federica Colonnese, Massimo Panella, Danilo Comminiello</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.02780">https://arxiv.org/abs/2410.02780</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.02780">https://arxiv.org/pdf/2410.02780</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.02780]] Guess What I Think: Streamlined EEG-to-Image Generation with Latent Diffusion Models(https://arxiv.org/abs/2410.02780)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Generating images from brain waves is gaining increasing attention due to its potential to advance brain-computer interface (BCI) systems by understanding how brain signals encode visual cues. Most of the literature has focused on fMRI-to-Image tasks as fMRI is characterized by high spatial resolution. However, fMRI is an expensive neuroimaging modality and does not allow for real-time BCI. On the other hand, electroencephalography (EEG) is a low-cost, non-invasive, and portable neuroimaging technique, making it an attractive option for future real-time applications. Nevertheless, EEG presents inherent challenges due to its low spatial resolution and susceptibility to noise and artifacts, which makes generating images from EEG more difficult. In this paper, we address these problems with a streamlined framework based on the ControlNet adapter for conditioning a latent diffusion model (LDM) through EEG signals. We conduct experiments and ablation studies on popular benchmarks to demonstrate that the proposed method beats other state-of-the-art models. Unlike these methods, which often require extensive preprocessing, pretraining, different losses, and captioning models, our approach is efficient and straightforward, requiring only minimal preprocessing and a few components. Code will be available after publication.</li>
</ul>

<h3>Title: Robust Symmetry Detection via Riemannian Langevin Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Jihyeon Je, Jiayi Liu, Guandao Yang, Boyang Deng, Shengqu Cai, Gordon Wetzstein, Or Litany, Leonidas Guibas</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.02786">https://arxiv.org/abs/2410.02786</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.02786">https://arxiv.org/pdf/2410.02786</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.02786]] Robust Symmetry Detection via Riemannian Langevin Dynamics(https://arxiv.org/abs/2410.02786)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Symmetries are ubiquitous across all kinds of objects, whether in nature or in man-made creations. While these symmetries may seem intuitive to the human eye, detecting them with a machine is nontrivial due to the vast search space. Classical geometry-based methods work by aggregating "votes" for each symmetry but struggle with noise. In contrast, learning-based methods may be more robust to noise, but often overlook partial symmetries due to the scarcity of annotated data. In this work, we address this challenge by proposing a novel symmetry detection method that marries classical symmetry detection techniques with recent advances in generative modeling. Specifically, we apply Langevin dynamics to a redefined symmetry space to enhance robustness against noise. We provide empirical results on a variety of shapes that suggest our method is not only robust to noise, but can also identify both partial and global symmetries. Moreover, we demonstrate the utility of our detected symmetries in various downstream tasks, such as compression and symmetrization of noisy shapes.</li>
</ul>

<h3>Title: PyRIT: A Framework for Security Risk Identification and Red Teaming in Generative AI System</h3>
<ul>
<li><strong>Authors: </strong>Gary D. Lopez Munoz, Amanda J. Minnich, Roman Lutz, Richard Lundeen, Raja Sekhar Rao Dheekonda, Nina Chikanov, Bolor-Erdene Jagdagdorj, Martin Pouliot, Shiven Chawla, Whitney Maxwell, Blake Bullwinkel, Katherine Pratt, Joris de Gruyter, Charlotte Siska, Pete Bryan, Tori Westerhoff, Chang Kawaguchi, Christian Seifert, Ram Shankar Siva Kumar, Yonatan Zunger</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.02828">https://arxiv.org/abs/2410.02828</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.02828">https://arxiv.org/pdf/2410.02828</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.02828]] PyRIT: A Framework for Security Risk Identification and Red Teaming in Generative AI System(https://arxiv.org/abs/2410.02828)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative Artificial Intelligence (GenAI) is becoming ubiquitous in our daily lives. The increase in computational power and data availability has led to a proliferation of both single- and multi-modal models. As the GenAI ecosystem matures, the need for extensible and model-agnostic risk identification frameworks is growing. To meet this need, we introduce the Python Risk Identification Toolkit (PyRIT), an open-source framework designed to enhance red teaming efforts in GenAI systems. PyRIT is a model- and platform-agnostic tool that enables red teamers to probe for and identify novel harms, risks, and jailbreaks in multimodal generative AI models. Its composable architecture facilitates the reuse of core building blocks and allows for extensibility to future models and modalities. This paper details the challenges specific to red teaming generative AI systems, the development and features of PyRIT, and its practical applications in real-world scenarios.</li>
</ul>

<h3>Title: Demonstration Attack against In-Context Learning for Code Intelligence</h3>
<ul>
<li><strong>Authors: </strong>Yifei Ge, Weisong Sun, Yihang Lou, Chunrong Fang, Yiran Zhang, Yiming Li, Xiaofang Zhang, Yang Liu, Zhihong Zhao, Zhenyu Chen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.02841">https://arxiv.org/abs/2410.02841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.02841">https://arxiv.org/pdf/2410.02841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.02841]] Demonstration Attack against In-Context Learning for Code Intelligence(https://arxiv.org/abs/2410.02841)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models (LLMs) have revolutionized code intelligence by improving programming productivity and alleviating challenges faced by software developers. To further improve the performance of LLMs on specific code intelligence tasks and reduce training costs, researchers reveal a new capability of LLMs: in-context learning (ICL). ICL allows LLMs to learn from a few demonstrations within a specific context, achieving impressive results without parameter updating. However, the rise of ICL introduces new security vulnerabilities in the code intelligence field. In this paper, we explore a novel security scenario based on the ICL paradigm, where attackers act as third-party ICL agencies and provide users with bad ICL content to mislead LLMs outputs in code intelligence tasks. Our study demonstrates the feasibility and risks of such a scenario, revealing how attackers can leverage malicious demonstrations to construct bad ICL content and induce LLMs to produce incorrect outputs, posing significant threats to system security. We propose a novel method to construct bad ICL content called DICE, which is composed of two stages: Demonstration Selection and Bad ICL Construction, constructing targeted bad ICL content based on the user query and transferable across different query inputs. Ultimately, our findings emphasize the critical importance of securing ICL mechanisms to protect code intelligence systems from adversarial manipulation.</li>
</ul>

<h3>Title: SymmetricDiffusers: Learning Discrete Diffusion on Finite Symmetric Groups</h3>
<ul>
<li><strong>Authors: </strong>Yongxing Zhang, Donglin Yang, Renjie Liao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.02942">https://arxiv.org/abs/2410.02942</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.02942">https://arxiv.org/pdf/2410.02942</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.02942]] SymmetricDiffusers: Learning Discrete Diffusion on Finite Symmetric Groups(https://arxiv.org/abs/2410.02942)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Finite symmetric groups $S_n$ are essential in fields such as combinatorics, physics, and chemistry. However, learning a probability distribution over $S_n$ poses significant challenges due to its intractable size and discrete nature. In this paper, we introduce SymmetricDiffusers, a novel discrete diffusion model that simplifies the task of learning a complicated distribution over $S_n$ by decomposing it into learning simpler transitions of the reverse diffusion using deep neural networks. We identify the riffle shuffle as an effective forward transition and provide empirical guidelines for selecting the diffusion length based on the theory of random walks on finite groups. Additionally, we propose a generalized Plackett-Luce (PL) distribution for the reverse transition, which is provably more expressive than the PL distribution. We further introduce a theoretically grounded "denoising schedule" to improve sampling and learning efficiency. Extensive experiments show that our model achieves state-of-the-art or comparable performances on solving tasks including sorting 4-digit MNIST images, jigsaw puzzles, and traveling salesman problems. Our code is released at this https URL.</li>
</ul>

<h3>Title: Learning Optimal Control and Dynamical Structure of Global Trajectory Search Problems with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Jannik Graebner, Anjian Li, Amlan Sinha, Ryne Beeson</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.02976">https://arxiv.org/abs/2410.02976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.02976">https://arxiv.org/pdf/2410.02976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.02976]] Learning Optimal Control and Dynamical Structure of Global Trajectory Search Problems with Diffusion Models(https://arxiv.org/abs/2410.02976)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Spacecraft trajectory design is a global search problem, where previous work has revealed specific solution structures that can be captured with data-driven methods. This paper explores two global search problems in the circular restricted three-body problem: hybrid cost function of minimum fuel/time-of-flight and transfers to energy-dependent invariant manifolds. These problems display a fundamental structure either in the optimal control profile or the use of dynamical structures. We build on our prior generative machine learning framework to apply diffusion models to learn the conditional probability distribution of the search problem and analyze the model's capability to capture these structures.</li>
</ul>

<h3>Title: Tutor CoPilot: A Human-AI Approach for Scaling Real-Time Expertise</h3>
<ul>
<li><strong>Authors: </strong>Rose E. Wang, Ana T. Ribeiro, Carly D. Robinson, Susanna Loeb, Dora Demszky</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03017">https://arxiv.org/abs/2410.03017</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03017">https://arxiv.org/pdf/2410.03017</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03017]] Tutor CoPilot: A Human-AI Approach for Scaling Real-Time Expertise(https://arxiv.org/abs/2410.03017)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative AI, particularly Language Models (LMs), has the potential to transform real-world domains with societal impact, particularly where access to experts is limited. For example, in education, training novice educators with expert guidance is important for effectiveness but expensive, creating significant barriers to improving education quality at scale. This challenge disproportionately harms students from under-served communities, who stand to gain the most from high-quality education. We introduce Tutor CoPilot, a novel Human-AI approach that leverages a model of expert thinking to provide expert-like guidance to tutors as they tutor. This study is the first randomized controlled trial of a Human-AI system in live tutoring, involving 900 tutors and 1,800 K-12 students from historically under-served communities. Following a preregistered analysis plan, we find that students working with tutors that have access to Tutor CoPilot are 4 percentage points (p.p.) more likely to master topics (p<0.01). Notably, students of lower-rated tutors experienced the greatest benefit, improving mastery by 9 p.p. We find that Tutor CoPilot costs only $20 per-tutor annually. We analyze 550,000+ messages using classifiers to identify pedagogical strategies, and find that tutors with access to Tutor CoPilot are more likely to use high-quality strategies to foster student understanding (e.g., asking guiding questions) and less likely to give away the answer to the student. Tutor interviews highlight how Tutor CoPilot's guidance helps tutors to respond to student needs, though they flag issues in Tutor CoPilot, such as generating suggestions that are not grade-level appropriate. Altogether, our study of Tutor CoPilot demonstrates how Human-AI systems can scale expertise in real-world domains, bridge gaps in skills and create a future where high-quality education is accessible to all students.</li>
</ul>

<h3>Title: Is Your Paper Being Reviewed by an LLM? Investigating AI Text Detectability in Peer Review</h3>
<ul>
<li><strong>Authors: </strong>Sungduk Yu, Man Luo, Avinash Madasu, Vasudev Lal, Phillip Howard</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03019">https://arxiv.org/abs/2410.03019</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03019">https://arxiv.org/pdf/2410.03019</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03019]] Is Your Paper Being Reviewed by an LLM? Investigating AI Text Detectability in Peer Review(https://arxiv.org/abs/2410.03019)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Peer review is a critical process for ensuring the integrity of published scientific research. Confidence in this process is predicated on the assumption that experts in the relevant domain give careful consideration to the merits of manuscripts which are submitted for publication. With the recent rapid advancements in the linguistic capabilities of large language models (LLMs), a new potential risk to the peer review process is that negligent reviewers will rely on LLMs to perform the often time consuming process of reviewing a paper. In this study, we investigate the ability of existing AI text detection algorithms to distinguish between peer reviews written by humans and different state-of-the-art LLMs. Our analysis shows that existing approaches fail to identify many GPT-4o written reviews without also producing a high number of false positive classifications. To address this deficiency, we propose a new detection approach which surpasses existing methods in the identification of GPT-4o written peer reviews at low levels of false positive classifications. Our work reveals the difficulty of accurately identifying AI-generated text at the individual review level, highlighting the urgent need for new tools and methods to detect this type of unethical application of generative AI.</li>
</ul>

<h3>Title: PixelShuffler: A Simple Image Translation Through Pixel Rearrangement</h3>
<ul>
<li><strong>Authors: </strong>Omar Zamzam</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03021">https://arxiv.org/abs/2410.03021</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03021">https://arxiv.org/pdf/2410.03021</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03021]] PixelShuffler: A Simple Image Translation Through Pixel Rearrangement(https://arxiv.org/abs/2410.03021)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Image-to-image translation is a topic in computer vision that has a vast range of use cases ranging from medical image translation, such as converting MRI scans to CT scans or to other MRI contrasts, to image colorization, super-resolution, domain adaptation, and generating photorealistic images from sketches or semantic maps. Image style transfer is also a widely researched application of image-to-image translation, where the goal is to synthesize an image that combines the content of one image with the style of another. Existing state-of-the-art methods often rely on complex neural networks, including diffusion models and language models, to achieve high-quality style transfer, but these methods can be computationally expensive and intricate to implement. In this paper, we propose a novel pixel shuffle method that addresses the image-to-image translation problem generally with a specific demonstrative application in style transfer. The proposed method approaches style transfer by shuffling the pixels of the style image such that the mutual information between the shuffled image and the content image is maximized. This approach inherently preserves the colors of the style image while ensuring that the structural details of the content image are retained in the stylized output. We demonstrate that this simple and straightforward method produces results that are comparable to state-of-the-art techniques, as measured by the Learned Perceptual Image Patch Similarity (LPIPS) loss for content preservation and the Fréchet Inception Distance (FID) score for style similarity. Our experiments validate that the proposed pixel shuffle method achieves competitive performance with significantly reduced complexity, offering a promising alternative for efficient image style transfer, as well as a promise in usability of the method in general image-to-image translation tasks.</li>
</ul>

<h3>Title: Flow Matching with Gaussian Process Priors for Probabilistic Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Marcel Kollovieh, Marten Lienen, David Lüdke, Leo Schwinn, Stephan Günnemann</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03024">https://arxiv.org/abs/2410.03024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03024">https://arxiv.org/pdf/2410.03024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03024]] Flow Matching with Gaussian Process Priors for Probabilistic Time Series Forecasting(https://arxiv.org/abs/2410.03024)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in generative modeling, particularly diffusion models, have opened new directions for time series modeling, achieving state-of-the-art performance in forecasting and synthesis. However, the reliance of diffusion-based models on a simple, fixed prior complicates the generative process since the data and prior distributions differ significantly. We introduce TSFlow, a conditional flow matching (CFM) model for time series that simplifies the generative problem by combining Gaussian processes, optimal transport paths, and data-dependent prior distributions. By incorporating (conditional) Gaussian processes, TSFlow aligns the prior distribution more closely with the temporal structure of the data, enhancing both unconditional and conditional generation. Furthermore, we propose conditional prior sampling to enable probabilistic forecasting with an unconditionally trained model. In our experimental evaluation on eight real-world datasets, we demonstrate the generative capabilities of TSFlow, producing high-quality unconditional samples. Finally, we show that both conditionally and unconditionally trained models achieve competitive results in forecasting benchmarks, surpassing other methods on 6 out of 8 datasets.</li>
</ul>

<h3>Title: Revealing the Unseen: Guiding Personalized Diffusion Models to Expose Training Data</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyu Wu, Jiaru Zhang, Steven Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03039">https://arxiv.org/abs/2410.03039</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03039">https://arxiv.org/pdf/2410.03039</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03039]] Revealing the Unseen: Guiding Personalized Diffusion Models to Expose Training Data(https://arxiv.org/abs/2410.03039)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion Models (DMs) have evolved into advanced image generation tools, especially for few-shot fine-tuning where a pretrained DM is fine-tuned on a small set of images to capture specific styles or objects. Many people upload these personalized checkpoints online, fostering communities such as Civitai and HuggingFace. However, model owners may overlook the potential risks of data leakage by releasing their fine-tuned checkpoints. Moreover, concerns regarding copyright violations arise when unauthorized data is used during fine-tuning. In this paper, we ask: "Can training data be extracted from these fine-tuned DMs shared online?" A successful extraction would present not only data leakage threats but also offer tangible evidence of copyright infringement. To answer this, we propose FineXtract, a framework for extracting fine-tuning data. Our method approximates fine-tuning as a gradual shift in the model's learned distribution -- from the original pretrained DM toward the fine-tuning data. By extrapolating the models before and after fine-tuning, we guide the generation toward high-probability regions within the fine-tuned data distribution. We then apply a clustering algorithm to extract the most probable images from those generated using this extrapolated guidance. Experiments on DMs fine-tuned with datasets such as WikiArt, DreamBooth, and real-world checkpoints posted online validate the effectiveness of our method, extracting approximately 20% of fine-tuning data in most cases, significantly surpassing baseline performance.</li>
</ul>

<h3>Title: Geometry is All You Need: A Unified Taxonomy of Matrix and Tensor Factorization for Compression of Generative Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mingxue Xu, Sadia Sharmin, Danilo P. Mandic</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03040">https://arxiv.org/abs/2410.03040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03040">https://arxiv.org/pdf/2410.03040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03040]] Geometry is All You Need: A Unified Taxonomy of Matrix and Tensor Factorization for Compression of Generative Language Models(https://arxiv.org/abs/2410.03040)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Matrix and tensor-guided parametrization for Natural Language Processing (NLP) models is fundamentally useful for the improvement of the model's systematic efficiency. However, the internal links between these two algebra structures and language model parametrization are poorly understood. Also, the existing matrix and tensor research is math-heavy and far away from machine learning (ML) and NLP research concepts. These two issues result in the recent progress on matrices and tensors for model parametrization being more like a loose collection of separate components from matrix/tensor and NLP studies, rather than a well-structured unified approach, further hindering algorithm design. To this end, we propose a unified taxonomy, which bridges the matrix/tensor compression approaches and model compression concepts in ML and NLP research. Namely, we adopt an elementary concept in linear algebra, that of a subspace, which is also the core concept in geometric algebra, to reformulate the matrix/tensor and ML/NLP concepts (e.g. attention mechanism) under one umbrella. In this way, based on our subspace formalization, typical matrix and tensor decomposition algorithms can be interpreted as geometric transformations. Finally, we revisit recent literature on matrix- or tensor-guided language model compression, rephrase and compare their core ideas, and then point out the current research gap and potential solutions.</li>
</ul>

<h3>Title: Generative Edge Detection with Stable Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Caixia Zhou, Yaping Huang, Mochu Xiang, Jiahui Ren, Haibin Ling, Jing Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03080">https://arxiv.org/abs/2410.03080</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03080">https://arxiv.org/pdf/2410.03080</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03080]] Generative Edge Detection with Stable Diffusion(https://arxiv.org/abs/2410.03080)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Edge detection is typically viewed as a pixel-level classification problem mainly addressed by discriminative methods. Recently, generative edge detection methods, especially diffusion model based solutions, are initialized in the edge detection task. Despite great potential, the retraining of task-specific designed modules and multi-step denoising inference limits their broader applications. Upon closer investigation, we speculate that part of the reason is the under-exploration of the rich discriminative information encoded in extensively pre-trained large models (\eg, stable diffusion models). Thus motivated, we propose a novel approach, named Generative Edge Detector (GED), by fully utilizing the potential of the pre-trained stable diffusion model. Our model can be trained and inferred efficiently without specific network design due to the rich high-level and low-level prior knowledge empowered by the pre-trained stable diffusion. Specifically, we propose to finetune the denoising U-Net and predict latent edge maps directly, by taking the latent image feature maps as input. Additionally, due to the subjectivity and ambiguity of the edges, we also incorporate the granularity of the edges into the denoising U-Net model as one of the conditions to achieve controllable and diverse predictions. Furthermore, we devise a granularity regularization to ensure the relative granularity relationship of the multiple predictions. We conduct extensive experiments on multiple datasets and achieve competitive performance (\eg, 0.870 and 0.880 in terms of ODS and OIS on the BSDS test dataset).</li>
</ul>

<h3>Title: Combing Text-based and Drag-based Editing for Precise and Flexible Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Ziqi Jiang, Zhen Wang, Long Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03097">https://arxiv.org/abs/2410.03097</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03097">https://arxiv.org/pdf/2410.03097</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03097]] Combing Text-based and Drag-based Editing for Precise and Flexible Image Editing(https://arxiv.org/abs/2410.03097)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Precise and flexible image editing remains a fundamental challenge in computer vision. Based on the modified areas, most editing methods can be divided into two main types: global editing and local editing. In this paper, we choose the two most common editing approaches (ie text-based editing and drag-based editing) and analyze their drawbacks. Specifically, text-based methods often fail to describe the desired modifications precisely, while drag-based methods suffer from ambiguity. To address these issues, we proposed \textbf{CLIPDrag}, a novel image editing method that is the first to combine text and drag signals for precise and ambiguity-free manipulations on diffusion models. To fully leverage these two signals, we treat text signals as global guidance and drag points as local information. Then we introduce a novel global-local motion supervision method to integrate text signals into existing drag-based methods by adapting a pre-trained language-vision model like CLIP. Furthermore, we also address the problem of slow convergence in CLIPDrag by presenting a fast point-tracking method that enforces drag points moving toward correct directions. Extensive experiments demonstrate that CLIPDrag outperforms existing single drag-based methods or text-based methods.</li>
</ul>

<h3>Title: A Training-Free Conditional Diffusion Model for Learning Stochastic Dynamical Systems</h3>
<ul>
<li><strong>Authors: </strong>Yanfang Liu, Yuan Chen, Dongbin Xiu, Guannan Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, math.DS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03108">https://arxiv.org/abs/2410.03108</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03108">https://arxiv.org/pdf/2410.03108</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03108]] A Training-Free Conditional Diffusion Model for Learning Stochastic Dynamical Systems(https://arxiv.org/abs/2410.03108)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This study introduces a training-free conditional diffusion model for learning unknown stochastic differential equations (SDEs) using data. The proposed approach addresses key challenges in computational efficiency and accuracy for modeling SDEs by utilizing a score-based diffusion model to approximate their stochastic flow map. Unlike the existing methods, this technique is based on an analytically derived closed-form exact score function, which can be efficiently estimated by Monte Carlo method using the trajectory data, and eliminates the need for neural network training to learn the score function. By generating labeled data through solving the corresponding reverse ordinary differential equation, the approach enables supervised learning of the flow map. Extensive numerical experiments across various SDE types, including linear, nonlinear, and multi-dimensional systems, demonstrate the versatility and effectiveness of the method. The learned models exhibit significant improvements in predicting both short-term and long-term behaviors of unknown stochastic systems, often surpassing baseline methods like GANs in estimating drift and diffusion coefficients.</li>
</ul>

<h3>Title: RIPPLECOT: Amplifying Ripple Effect of Knowledge Editing in Language Models via Chain-of-Thought In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Zihao Zhao, Yuchen Yang, Yijiang Li, Yinzhi Cao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03122">https://arxiv.org/abs/2410.03122</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03122">https://arxiv.org/pdf/2410.03122</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03122]] RIPPLECOT: Amplifying Ripple Effect of Knowledge Editing in Language Models via Chain-of-Thought In-Context Learning(https://arxiv.org/abs/2410.03122)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>The ripple effect poses a significant challenge in knowledge editing for large language models. Namely, when a single fact is edited, the model struggles to accurately update the related facts in a sequence, which is evaluated by multi-hop questions linked to a chain of related facts. Recent strategies have moved away from traditional parameter updates to more flexible, less computation-intensive methods, proven to be more effective in addressing the ripple effect. In-context learning (ICL) editing uses a simple demonstration `Imagine that + new fact` to guide LLMs, but struggles with complex multi-hop questions as the new fact alone fails to specify the chain of facts involved in such scenarios. Besides, memory-based editing maintains additional storage for all edits and related facts, requiring continuous updates to stay effective. As a result of these design limitations, the challenge remains, with the highest accuracy being only 33.8% on the MQuAKE-cf benchmarks for Vicuna-7B. To address this, we propose RippleCOT, a novel ICL editing approach integrating Chain-of-Thought (COT) reasoning. RippleCOT structures demonstrations as `newfact, question, thought, answer`, incorporating a thought component to identify and decompose the multi-hop logic within questions. This approach effectively guides the model through complex multi-hop questions with chains of related facts. Comprehensive experiments demonstrate that RippleCOT significantly outperforms the state-of-the-art on the ripple effect, achieving accuracy gains ranging from 7.8% to 87.1%.</li>
</ul>

<h3>Title: On Unsupervised Prompt Learning for Classification with Black-box Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zhen-Yu Zhang, Jiandong Zhang, Huaxiu Yao, Gang Niu, Masashi Sugiyama</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03124">https://arxiv.org/abs/2410.03124</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03124">https://arxiv.org/pdf/2410.03124</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03124]] On Unsupervised Prompt Learning for Classification with Black-box Language Models(https://arxiv.org/abs/2410.03124)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have achieved impressive success in text-formatted learning problems, and most popular LLMs have been deployed in a black-box fashion. Meanwhile, fine-tuning is usually necessary for a specific downstream task to obtain better performance, and this functionality is provided by the owners of the black-box LLMs. To fine-tune a black-box LLM, labeled data are always required to adjust the model parameters. However, in many real-world applications, LLMs can label textual datasets with even better quality than skilled human annotators, motivating us to explore the possibility of fine-tuning black-box LLMs with unlabeled data. In this paper, we propose unsupervised prompt learning for classification with black-box LLMs, where the learning parameters are the prompt itself and the pseudo labels of unlabeled data. Specifically, the prompt is modeled as a sequence of discrete tokens, and every token has its own to-be-learned categorical distribution. On the other hand, for learning the pseudo labels, we are the first to consider the in-context learning (ICL) capabilities of LLMs: we first identify reliable pseudo-labeled data using the LLM, and then assign pseudo labels to other unlabeled data based on the prompt, allowing the pseudo-labeled data to serve as in-context demonstrations alongside the prompt. Those in-context demonstrations matter: previously, they are involved when the prompt is used for prediction while they are not involved when the prompt is trained; thus, taking them into account during training makes the prompt-learning and prompt-using stages more consistent. Experiments on benchmark datasets show the effectiveness of our proposed algorithm. After unsupervised prompt learning, we can use the pseudo-labeled dataset for further fine-tuning by the owners of the black-box LLMs.</li>
</ul>

<h3>Title: Can LLMs Generate Diverse Molecules? Towards Alignment with Structural Diversity</h3>
<ul>
<li><strong>Authors: </strong>Hyosoon Jang, Yunhui Jang, Jaehyung Kim, Sungsoo Ahn</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03138">https://arxiv.org/abs/2410.03138</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03138">https://arxiv.org/pdf/2410.03138</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03138]] Can LLMs Generate Diverse Molecules? Towards Alignment with Structural Diversity(https://arxiv.org/abs/2410.03138)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in large language models (LLMs) have demonstrated impressive performance in generating molecular structures as drug candidates, which offers significant potential to accelerate drug discovery. However, the current LLMs overlook a critical requirement for drug discovery: proposing a diverse set of molecules. This diversity is essential for improving the chances of finding a viable drug, as it provides alternative molecules that may succeed where others fail in wet-lab or clinical validations. Despite such a need for diversity, the LLMs often output structurally similar molecules from a given prompt. While decoding schemes like beam search may enhance textual diversity, this often does not align with molecular structural diversity. In response, we propose a new method for fine-tuning molecular generative LLMs to autoregressively generate a set of structurally diverse molecules, where each molecule is generated by conditioning on the previously generated molecules. Our approach consists of two stages: (1) supervised fine-tuning to adapt LLMs to autoregressively generate molecules in a sequence and (2) reinforcement learning to maximize structural diversity within the generated molecules. Our experiments show that (1) our fine-tuning approach enables the LLMs to better discover diverse molecules compared to existing decoding schemes and (2) our fine-tuned model outperforms other representative LLMs in generating diverse molecules, including the ones fine-tuned on chemical domains.</li>
</ul>

<h3>Title: In-context Learning in Presence of Spurious Correlations</h3>
<ul>
<li><strong>Authors: </strong>Hrayr Harutyunyan, Rafayel Darbinyan, Samvel Karapetyan, Hrant Khachatrian</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03140">https://arxiv.org/abs/2410.03140</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03140">https://arxiv.org/pdf/2410.03140</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03140]] In-context Learning in Presence of Spurious Correlations(https://arxiv.org/abs/2410.03140)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large language models exhibit a remarkable capacity for in-context learning, where they learn to solve tasks given a few examples. Recent work has shown that transformers can be trained to perform simple regression tasks in-context. This work explores the possibility of training an in-context learner for classification tasks involving spurious features. We find that the conventional approach of training in-context learners is susceptible to spurious features. Moreover, when the meta-training dataset includes instances of only one task, the conventional approach leads to task memorization and fails to produce a model that leverages context for predictions. Based on these observations, we propose a novel technique to train such a learner for a given classification task. Remarkably, this in-context learner matches and sometimes outperforms strong methods like ERM and GroupDRO. However, unlike these algorithms, it does not generalize well to other tasks. We show that it is possible to obtain an in-context learner that generalizes to unseen tasks by training on a diverse dataset of synthetic in-context learning instances.</li>
</ul>

<h3>Title: Redefining Temporal Modeling in Video Diffusion: The Vectorized Timestep Approach</h3>
<ul>
<li><strong>Authors: </strong>Yaofang Liu, Yumeng Ren, Xiaodong Cun, Aitor Artola, Yang Liu, Tieyong Zeng, Raymond H. Chan, Jean-michel Morel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03160">https://arxiv.org/abs/2410.03160</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03160">https://arxiv.org/pdf/2410.03160</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03160]] Redefining Temporal Modeling in Video Diffusion: The Vectorized Timestep Approach(https://arxiv.org/abs/2410.03160)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have revolutionized image generation, and their extension to video generation has shown promise. However, current video diffusion models~(VDMs) rely on a scalar timestep variable applied at the clip level, which limits their ability to model complex temporal dependencies needed for various tasks like image-to-video generation. To address this limitation, we propose a frame-aware video diffusion model~(FVDM), which introduces a novel vectorized timestep variable~(VTV). Unlike conventional VDMs, our approach allows each frame to follow an independent noise schedule, enhancing the model's capacity to capture fine-grained temporal dependencies. FVDM's flexibility is demonstrated across multiple tasks, including standard video generation, image-to-video generation, video interpolation, and long video synthesis. Through a diverse set of VTV configurations, we achieve superior quality in generated videos, overcoming challenges such as catastrophic forgetting during fine-tuning and limited generalizability in zero-shot this http URL empirical evaluations show that FVDM outperforms state-of-the-art methods in video generation quality, while also excelling in extended tasks. By addressing fundamental shortcomings in existing VDMs, FVDM sets a new paradigm in video synthesis, offering a robust framework with significant implications for generative modeling and multimedia applications.</li>
</ul>

<h3>Title: Rapid optimization in high dimensional space by deep kernel learning augmented genetic algorithms</h3>
<ul>
<li><strong>Authors: </strong>Mani Valleti, Aditya Raghavan, Sergei V. Kalinin</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci, physics.comp-ph, physics.data-an</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03173">https://arxiv.org/abs/2410.03173</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03173">https://arxiv.org/pdf/2410.03173</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03173]] Rapid optimization in high dimensional space by deep kernel learning augmented genetic algorithms(https://arxiv.org/abs/2410.03173)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Exploration of complex high-dimensional spaces presents significant challenges in fields such as molecular discovery, process optimization, and supply chain management. Genetic Algorithms (GAs), while offering significant power for creating new candidate spaces, often entail high computational demands due to the need for evaluation of each new proposed solution. On the other hand, Deep Kernel Learning (DKL) efficiently navigates the spaces of preselected candidate structures but lacks generative capabilities. This study introduces an approach that amalgamates the generative power of GAs to create new candidates with the efficiency of DKL-based surrogate models to rapidly ascertain the behavior of new candidate spaces. This DKL-GA framework can be further used to build Bayesian Optimization (BO) workflows. We demonstrate the effectiveness of this approach through the optimization of the FerroSIM model, showcasing its broad applicability to diverse challenges, including molecular discovery and battery charging optimization.</li>
</ul>

<h3>Title: Generating bilingual example sentences with large language models as lexicography assistants</h3>
<ul>
<li><strong>Authors: </strong>Raphael Merx, Ekaterina Vylomova, Kemal Kurniawan</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03182">https://arxiv.org/abs/2410.03182</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03182">https://arxiv.org/pdf/2410.03182</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03182]] Generating bilingual example sentences with large language models as lexicography assistants(https://arxiv.org/abs/2410.03182)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>We present a study of LLMs' performance in generating and rating example sentences for bilingual dictionaries across languages with varying resource levels: French (high-resource), Indonesian (mid-resource), and Tetun (low-resource), with English as the target language. We evaluate the quality of LLM-generated examples against the GDEX (Good Dictionary EXample) criteria: typicality, informativeness, and intelligibility. Our findings reveal that while LLMs can generate reasonably good dictionary examples, their performance degrades significantly for lower-resourced languages. We also observe high variability in human preferences for example quality, reflected in low inter-annotator agreement rates. To address this, we demonstrate that in-context learning can successfully align LLMs with individual annotator preferences. Additionally, we explore the use of pre-trained language models for automated rating of examples, finding that sentence perplexity serves as a good proxy for typicality and intelligibility in higher-resourced languages. Our study also contributes a novel dataset of 600 ratings for LLM-generated sentence pairs, and provides insights into the potential of LLMs in reducing the cost of lexicographic work, particularly for low-resource languages.</li>
</ul>

<h3>Title: Autonomous Character-Scene Interaction Synthesis from Text Instruction</h3>
<ul>
<li><strong>Authors: </strong>Nan Jiang, Zimo He, Zi Wang, Hongjie Li, Yixin Chen, Siyuan Huang, Yixin Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03187">https://arxiv.org/abs/2410.03187</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03187">https://arxiv.org/pdf/2410.03187</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03187]] Autonomous Character-Scene Interaction Synthesis from Text Instruction(https://arxiv.org/abs/2410.03187)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Synthesizing human motions in 3D environments, particularly those with complex activities such as locomotion, hand-reaching, and human-object interaction, presents substantial demands for user-defined waypoints and stage transitions. These requirements pose challenges for current models, leading to a notable gap in automating the animation of characters from simple human inputs. This paper addresses this challenge by introducing a comprehensive framework for synthesizing multi-stage scene-aware interaction motions directly from a single text instruction and goal location. Our approach employs an auto-regressive diffusion model to synthesize the next motion segment, along with an autonomous scheduler predicting the transition for each action stage. To ensure that the synthesized motions are seamlessly integrated within the environment, we propose a scene representation that considers the local perception both at the start and the goal location. We further enhance the coherence of the generated motion by integrating frame embeddings with language input. Additionally, to support model training, we present a comprehensive motion-captured dataset comprising 16 hours of motion sequences in 120 indoor scenes covering 40 types of motions, each annotated with precise language descriptions. Experimental results demonstrate the efficacy of our method in generating high-quality, multi-stage motions closely aligned with environmental and textual conditions.</li>
</ul>

<h3>Title: Tuning Timestep-Distilled Diffusion Model Using Pairwise Sample Optimization</h3>
<ul>
<li><strong>Authors: </strong>Zichen Miao, Zhengyuan Yang, Kevin Lin, Ze Wang, Zicheng Liu, Lijuan Wang, Qiang Qiu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03190">https://arxiv.org/abs/2410.03190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03190">https://arxiv.org/pdf/2410.03190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03190]] Tuning Timestep-Distilled Diffusion Model Using Pairwise Sample Optimization(https://arxiv.org/abs/2410.03190)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advancements in timestep-distilled diffusion models have enabled high-quality image generation that rivals non-distilled multi-step models, but with significantly fewer inference steps. While such models are attractive for applications due to the low inference cost and latency, fine-tuning them with a naive diffusion objective would result in degraded and blurry outputs. An intuitive alternative is to repeat the diffusion distillation process with a fine-tuned teacher model, which produces good results but is cumbersome and computationally intensive; the distillation training usually requires magnitude higher of training compute compared to fine-tuning for specific image styles. In this paper, we present an algorithm named pairwise sample optimization (PSO), which enables the direct fine-tuning of an arbitrary timestep-distilled diffusion model. PSO introduces additional reference images sampled from the current time-step distilled model, and increases the relative likelihood margin between the training images and reference images. This enables the model to retain its few-step generation ability, while allowing for fine-tuning of its output distribution. We also demonstrate that PSO is a generalized formulation which can be flexibly extended to both offline-sampled and online-sampled pairwise data, covering various popular objectives for diffusion model preference optimization. We evaluate PSO in both preference optimization and other fine-tuning tasks, including style transfer and concept customization. We show that PSO can directly adapt distilled models to human-preferred generation with both offline and online-generated pairwise preference image data. PSO also demonstrates effectiveness in style transfer and concept customization by directly tuning timestep-distilled diffusion models.</li>
</ul>

<h3>Title: Learning test generators for cyber-physical systems</h3>
<ul>
<li><strong>Authors: </strong>Jarkko Peltomäki, Ivan Porres</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03202">https://arxiv.org/abs/2410.03202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03202">https://arxiv.org/pdf/2410.03202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03202]] Learning test generators for cyber-physical systems(https://arxiv.org/abs/2410.03202)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Black-box runtime verification methods for cyber-physical systems can be used to discover errors in systems whose inputs and outputs are expressed as signals over time and their correctness requirements are specified in a temporal logic. Existing methods, such as requirement falsification, often focus on finding a single input that is a counterexample to system correctness. In this paper, we study how to create test generators that can produce multiple and diverse counterexamples for a single requirement. Several counterexamples expose system failures in varying input conditions and support the root cause analysis of the faults. We present the WOGAN algorithm to create such test generators automatically. The algorithm works by training iteratively a Wasserstein generative adversarial network that models the target distribution of the uniform distribution on the set of counterexamples. WOGAN is an algorithm that trains generative models that act as test generators for runtime verification. The training is performed online without the need for a previous model or dataset. We also propose criteria to evaluate such test generators. We evaluate the trained generators on several well-known problems including the ARCH-COMP falsification benchmarks. Our experimental results indicate that generators trained by the WOGAN algorithm are as effective as state-of-the-art requirement falsification algorithms while producing tests that are as diverse as a sample from uniform random sampling. We conclude that WOGAN is a viable method to produce test generators automatically and that these test generators can generate multiple and diverse counterexamples for the runtime verification of cyber-physical systems.</li>
</ul>

<h3>Title: CUDLE: Learning Under Label Scarcity to Detect Cannabis Use in Uncontrolled Environments</h3>
<ul>
<li><strong>Authors: </strong>Reza Rahimi Azghan, Nicholas C. Glodosky, Ramesh Kumar Sah, Carrie Cuttler, Ryan McLaughlin, Michael J. Cleveland, Hassan Ghasemzadeh</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03211">https://arxiv.org/abs/2410.03211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03211">https://arxiv.org/pdf/2410.03211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03211]] CUDLE: Learning Under Label Scarcity to Detect Cannabis Use in Uncontrolled Environments(https://arxiv.org/abs/2410.03211)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Wearable sensor systems have demonstrated a great potential for real-time, objective monitoring of physiological health to support behavioral interventions. However, obtaining accurate labels in free-living environments remains difficult due to limited human supervision and the reliance on self-labeling by patients, making data collection and supervised learning particularly challenging. To address this issue, we introduce CUDLE (Cannabis Use Detection with Label Efficiency), a novel framework that leverages self-supervised learning with real-world wearable sensor data to tackle a pressing healthcare challenge: the automatic detection of cannabis consumption in free-living environments. CUDLE identifies cannabis consumption moments using sensor-derived data through a contrastive learning framework. It first learns robust representations via a self-supervised pretext task with data augmentation. These representations are then fine-tuned in a downstream task with a shallow classifier, enabling CUDLE to outperform traditional supervised methods, especially with limited labeled data. To evaluate our approach, we conducted a clinical study with 20 cannabis users, collecting over 500 hours of wearable sensor data alongside user-reported cannabis use moments through EMA (Ecological Momentary Assessment) methods. Our extensive analysis using the collected data shows that CUDLE achieves a higher accuracy of 73.4%, compared to 71.1% for the supervised approach, with the performance gap widening as the number of labels decreases. Notably, CUDLE not only surpasses the supervised model while using 75% less labels, but also reaches peak performance with far fewer subjects.</li>
</ul>

<h3>Title: AutoPenBench: Benchmarking Generative Agents for Penetration Testing</h3>
<ul>
<li><strong>Authors: </strong>Luca Gioacchini, Marco Mellia, Idilio Drago, Alexander Delsanto, Giuseppe Siracusano, Roberto Bifulco</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03225">https://arxiv.org/abs/2410.03225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03225">https://arxiv.org/pdf/2410.03225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03225]] AutoPenBench: Benchmarking Generative Agents for Penetration Testing(https://arxiv.org/abs/2410.03225)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative AI agents, software systems powered by Large Language Models (LLMs), are emerging as a promising approach to automate cybersecurity tasks. Among the others, penetration testing is a challenging field due to the task complexity and the diverse strategies to simulate cyber-attacks. Despite growing interest and initial studies in automating penetration testing with generative agents, there remains a significant gap in the form of a comprehensive and standard framework for their evaluation and development. This paper introduces AutoPenBench, an open benchmark for evaluating generative agents in automated penetration testing. We present a comprehensive framework that includes 33 tasks, each representing a vulnerable system that the agent has to attack. Tasks are of increasing difficulty levels, including in-vitro and real-world scenarios. We assess the agent performance with generic and specific milestones that allow us to compare results in a standardised manner and understand the limits of the agent under test. We show the benefits of AutoPenBench by testing two agent architectures: a fully autonomous and a semi-autonomous supporting human interaction. We compare their performance and limitations. For example, the fully autonomous agent performs unsatisfactorily achieving a 21% Success Rate (SR) across the benchmark, solving 27% of the simple tasks and only one real-world task. In contrast, the assisted agent demonstrates substantial improvements, with 64% of SR. AutoPenBench allows us also to observe how different LLMs like GPT-4o or OpenAI o1 impact the ability of the agents to complete the tasks. We believe that our benchmark fills the gap with a standard and flexible framework to compare penetration testing agents on a common ground. We hope to extend AutoPenBench along with the research community by making it available under this https URL.</li>
</ul>

<h3>Title: Enhanced Transformer architecture for in-context learning of dynamical systems</h3>
<ul>
<li><strong>Authors: </strong>Matteo Rufolo, Dario Piga, Gabriele Maroni, Marco Forgione</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03291">https://arxiv.org/abs/2410.03291</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03291">https://arxiv.org/pdf/2410.03291</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03291]] Enhanced Transformer architecture for in-context learning of dynamical systems(https://arxiv.org/abs/2410.03291)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Recently introduced by some of the authors, the in-context identification paradigm aims at estimating, offline and based on synthetic data, a meta-model that describes the behavior of a whole class of systems. Once trained, this meta-model is fed with an observed input/output sequence (context) generated by a real system to predict its behavior in a zero-shot learning fashion. In this paper, we enhance the original meta-modeling framework through three key innovations: by formulating the learning task within a probabilistic framework; by managing non-contiguous context and query windows; and by adopting recurrent patching to effectively handle long context sequences. The efficacy of these modifications is demonstrated through a numerical example focusing on the Wiener-Hammerstein system class, highlighting the model's enhanced performance and scalability.</li>
</ul>

<h3>Title: Selective Test-Time Adaptation for Unsupervised Anomaly Detection using Neural Implicit Representations</h3>
<ul>
<li><strong>Authors: </strong>Sameer Ambekar, Julia A. Schnabel, Cosmin Bereca</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03306">https://arxiv.org/abs/2410.03306</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03306">https://arxiv.org/pdf/2410.03306</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03306]] Selective Test-Time Adaptation for Unsupervised Anomaly Detection using Neural Implicit Representations(https://arxiv.org/abs/2410.03306)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Deep learning models in medical imaging often encounter challenges when adapting to new clinical settings unseen during training. Test-time adaptation offers a promising approach to optimize models for these unseen domains, yet its application in anomaly detection (AD) remains largely unexplored. AD aims to efficiently identify deviations from normative distributions; however, full adaptation, including pathological shifts, may inadvertently learn the anomalies it intends to detect. We introduce a novel concept of \emph{selective} test-time adaptation that utilizes the inherent characteristics of deep pre-trained features to adapt \emph{selectively} in a zero-shot manner to any test image from an unseen domain. This approach employs a model-agnostic, lightweight multi-layer perceptron for neural implicit representations, enabling the adaptation of outputs from any reconstruction-based AD method without altering the source-trained model. Rigorous validation in brain AD demonstrated that our strategy substantially enhances detection accuracy for multiple conditions and different target distributions. Specifically, our method improves the detection rates by up to 78\% for enlarged ventricles and 24\% for edemas.</li>
</ul>

<h3>Title: LANTERN: Accelerating Visual Autoregressive Models with Relaxed Speculative Decoding</h3>
<ul>
<li><strong>Authors: </strong>Doohyuk Jang, Sihwan Park, June Yong Yang, Yeonsung Jung, Jihun Yun, Souvik Kundu, Sung-Yub Kim, Eunho Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03355">https://arxiv.org/abs/2410.03355</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03355">https://arxiv.org/pdf/2410.03355</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03355]] LANTERN: Accelerating Visual Autoregressive Models with Relaxed Speculative Decoding(https://arxiv.org/abs/2410.03355)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Auto-Regressive (AR) models have recently gained prominence in image generation, often matching or even surpassing the performance of diffusion models. However, one major limitation of AR models is their sequential nature, which processes tokens one at a time, slowing down generation compared to models like GANs or diffusion-based methods that operate more efficiently. While speculative decoding has proven effective for accelerating LLMs by generating multiple tokens in a single forward, its application in visual AR models remains largely unexplored. In this work, we identify a challenge in this setting, which we term \textit{token selection ambiguity}, wherein visual AR models frequently assign uniformly low probabilities to tokens, hampering the performance of speculative decoding. To overcome this challenge, we propose a relaxed acceptance condition referred to as LANTERN that leverages the interchangeability of tokens in latent space. This relaxation restores the effectiveness of speculative decoding in visual AR models by enabling more flexible use of candidate tokens that would otherwise be prematurely rejected. Furthermore, by incorporating a total variation distance bound, we ensure that these speed gains are achieved without significantly compromising image quality or semantic coherence. Experimental results demonstrate the efficacy of our method in providing a substantial speed-up over speculative decoding. In specific, compared to a naïve application of the state-of-the-art speculative decoding, LANTERN increases speed-ups by $\mathbf{1.75}\times$ and $\mathbf{1.76}\times$, as compared to greedy decoding and random sampling, respectively, when applied to LlamaGen, a contemporary visual AR model.</li>
</ul>

<h3>Title: Latent Abstractions in Generative Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Giulio Franzese, Mattia Martini, Giulio Corallo, Paolo Papotti, Pietro Michiardi</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03368">https://arxiv.org/abs/2410.03368</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03368">https://arxiv.org/pdf/2410.03368</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03368]] Latent Abstractions in Generative Diffusion Models(https://arxiv.org/abs/2410.03368)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In this work we study how diffusion-based generative models produce high-dimensional data, such as an image, by implicitly relying on a manifestation of a low-dimensional set of latent abstractions, that guide the generative process. We present a novel theoretical framework that extends NLF, and that offers a unique perspective on SDE-based generative models. The development of our theory relies on a novel formulation of the joint (state and measurement) dynamics, and an information-theoretic measure of the influence of the system state on the measurement process. According to our theory, diffusion models can be cast as a system of SDE, describing a non-linear filter in which the evolution of unobservable latent abstractions steers the dynamics of an observable measurement process (corresponding to the generative pathways). In addition, we present an empirical study to validate our theory and previous empirical results on the emergence of latent abstractions at different stages of the generative process.</li>
</ul>

<h3>Title: Zebra: In-Context and Generative Pretraining for Solving Parametric PDEs</h3>
<ul>
<li><strong>Authors: </strong>Louis Serrano, Armand Kassaï Koupaï, Thomas X Wang, Pierre Erbacher, Patrick Gallinari</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03437">https://arxiv.org/abs/2410.03437</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03437">https://arxiv.org/pdf/2410.03437</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03437]] Zebra: In-Context and Generative Pretraining for Solving Parametric PDEs(https://arxiv.org/abs/2410.03437)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, in-context</a></li>
<li><strong>Abstract: </strong>Solving time-dependent parametric partial differential equations (PDEs) is challenging, as models must adapt to variations in parameters such as coefficients, forcing terms, and boundary conditions. Data-driven neural solvers either train on data sampled from the PDE parameters distribution in the hope that the model generalizes to new instances or rely on gradient-based adaptation and meta-learning to implicitly encode the dynamics from observations. This often comes with increased inference complexity. Inspired by the in-context learning capabilities of large language models (LLMs), we introduce Zebra, a novel generative auto-regressive transformer designed to solve parametric PDEs without requiring gradient adaptation at inference. By leveraging in-context information during both pre-training and inference, Zebra dynamically adapts to new tasks by conditioning on input sequences that incorporate context trajectories or preceding states. This approach enables Zebra to flexibly handle arbitrarily sized context inputs and supports uncertainty quantification through the sampling of multiple solution trajectories. We evaluate Zebra across a variety of challenging PDE scenarios, demonstrating its adaptability, robustness, and superior performance compared to existing approaches.</li>
</ul>

<h3>Title: ToolGen: Unified Tool Retrieval and Calling via Generation</h3>
<ul>
<li><strong>Authors: </strong>Renxi Wang, Xudong Han, Lei Ji, Shu Wang, Timothy Baldwin, Haonan Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03439">https://arxiv.org/abs/2410.03439</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03439">https://arxiv.org/pdf/2410.03439</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03439]] ToolGen: Unified Tool Retrieval and Calling via Generation(https://arxiv.org/abs/2410.03439)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) advance, their inability to autonomously execute tasks by directly interacting with external tools remains a critical limitation. Traditional methods rely on inputting tool descriptions as context, which is constrained by context length and requires separate, often inefficient, retrieval mechanisms. We introduce ToolGen, a paradigm shift that integrates tool knowledge directly into the LLM's parameters by representing each tool as a unique token. This enables the LLM to generate tool calls and arguments as part of its next token prediction capabilities, seamlessly blending tool invocation with language generation. Our framework allows the LLM to access and utilize a vast amount of tools with no additional retrieval step, significantly enhancing both performance and scalability. Experimental results with over 47,000 tools show that ToolGen not only achieves superior results in both tool retrieval and autonomous task completion but also sets the stage for a new era of AI agents that can adapt to tools across diverse domains. By fundamentally transforming tool retrieval into a generative process, ToolGen paves the way for more versatile, efficient, and autonomous AI systems. ToolGen enables end-to-end tool learning and opens opportunities for integration with other advanced techniques such as chain-of-thought and reinforcement learning, thereby expanding the practical capabilities of LLMs.</li>
</ul>

<h3>Title: CLoSD: Closing the Loop between Simulation and Diffusion for multi-task character control</h3>
<ul>
<li><strong>Authors: </strong>Guy Tevet, Sigal Raab, Setareh Cohan, Daniele Reda, Zhengyi Luo, Xue Bin Peng, Amit H. Bermano, Michiel van de Panne</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03441">https://arxiv.org/abs/2410.03441</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03441">https://arxiv.org/pdf/2410.03441</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03441]] CLoSD: Closing the Loop between Simulation and Diffusion for multi-task character control(https://arxiv.org/abs/2410.03441)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Motion diffusion models and Reinforcement Learning (RL) based control for physics-based simulations have complementary strengths for human motion generation. The former is capable of generating a wide variety of motions, adhering to intuitive control such as text, while the latter offers physically plausible motion and direct interaction with the environment. In this work, we present a method that combines their respective strengths. CLoSD is a text-driven RL physics-based controller, guided by diffusion generation for various tasks. Our key insight is that motion diffusion can serve as an on-the-fly universal planner for a robust RL controller. To this end, CLoSD maintains a closed-loop interaction between two modules -- a Diffusion Planner (DiP), and a tracking controller. DiP is a fast-responding autoregressive diffusion model, controlled by textual prompts and target locations, and the controller is a simple and robust motion imitator that continuously receives motion plans from DiP and provides feedback from the environment. CLoSD is capable of seamlessly performing a sequence of different tasks, including navigation to a goal location, striking an object with a hand or foot as specified in a text prompt, sitting down, and getting up. this https URL</li>
</ul>

<h3>Title: Dynamic Diffusion Transformer</h3>
<ul>
<li><strong>Authors: </strong>Wangbo Zhao, Yizeng Han, Jiasheng Tang, Kai Wang, Yibing Song, Gao Huang, Fan Wang, Yang You</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03456">https://arxiv.org/abs/2410.03456</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03456">https://arxiv.org/pdf/2410.03456</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03456]] Dynamic Diffusion Transformer(https://arxiv.org/abs/2410.03456)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion Transformer (DiT), an emerging diffusion model for image generation, has demonstrated superior performance but suffers from substantial computational costs. Our investigations reveal that these costs stem from the static inference paradigm, which inevitably introduces redundant computation in certain diffusion timesteps and spatial regions. To address this inefficiency, we propose Dynamic Diffusion Transformer (DyDiT), an architecture that dynamically adjusts its computation along both timestep and spatial dimensions during generation. Specifically, we introduce a Timestep-wise Dynamic Width (TDW) approach that adapts model width conditioned on the generation timesteps. In addition, we design a Spatial-wise Dynamic Token (SDT) strategy to avoid redundant computation at unnecessary spatial locations. Extensive experiments on various datasets and different-sized models verify the superiority of DyDiT. Notably, with <3% additional fine-tuning iterations, our method reduces the FLOPs of DiT-XL by 51%, accelerates generation by 1.73, and achieves a competitive FID score of 2.07 on ImageNet. The code is publicly available at this https URL Dynamic-Diffusion-Transformer.</li>
</ul>

<h3>Title: Auto-GDA: Automatic Domain Adaptation for Efficient Grounding Verification in Retrieval Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Tobias Leemann, Periklis Petridis, Giuseppe Vietri, Dionysis Manousakas, Aaron Roth, Sergul Aydore</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03461">https://arxiv.org/abs/2410.03461</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03461">https://arxiv.org/pdf/2410.03461</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03461]] Auto-GDA: Automatic Domain Adaptation for Efficient Grounding Verification in Retrieval Augmented Generation(https://arxiv.org/abs/2410.03461)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>While retrieval augmented generation (RAG) has been shown to enhance factuality of large language model (LLM) outputs, LLMs still suffer from hallucination, generating incorrect or irrelevant information. One common detection strategy involves prompting the LLM again to assess whether its response is grounded in the retrieved evidence, but this approach is costly. Alternatively, lightweight natural language inference (NLI) models for efficient grounding verification can be used at inference time. While existing pre-trained NLI models offer potential solutions, their performance remains subpar compared to larger models on realistic RAG inputs. RAG inputs are more complex than most datasets used for training NLI models and have characteristics specific to the underlying knowledge base, requiring adaptation of the NLI models to a specific target domain. Additionally, the lack of labeled instances in the target domain makes supervised domain adaptation, e.g., through fine-tuning, infeasible. To address these challenges, we introduce Automatic Generative Domain Adaptation (Auto-GDA). Our framework enables unsupervised domain adaptation through synthetic data generation. Unlike previous methods that rely on handcrafted filtering and augmentation strategies, Auto-GDA employs an iterative process to continuously improve the quality of generated samples using weak labels from less efficient teacher models and discrete optimization to select the most promising augmented samples. Experimental results demonstrate the effectiveness of our approach, with models fine-tuned on synthetic data using Auto-GDA often surpassing the performance of the teacher model and reaching the performance level of LLMs at 10 % of their computational cost.</li>
</ul>

<h3>Title: Diffusion State-Guided Projected Gradient for Inverse Problems</h3>
<ul>
<li><strong>Authors: </strong>Rayhan Zirvi, Bahareh Tolooshams, Anima Anandkumar</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03463">https://arxiv.org/abs/2410.03463</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03463">https://arxiv.org/pdf/2410.03463</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03463]] Diffusion State-Guided Projected Gradient for Inverse Problems(https://arxiv.org/abs/2410.03463)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advancements in diffusion models have been effective in learning data priors for solving inverse problems. They leverage diffusion sampling steps for inducing a data prior while using a measurement guidance gradient at each step to impose data consistency. For general inverse problems, approximations are needed when an unconditionally trained diffusion model is used since the measurement likelihood is intractable, leading to inaccurate posterior sampling. In other words, due to their approximations, these methods fail to preserve the generation process on the data manifold defined by the diffusion prior, leading to artifacts in applications such as image restoration. To enhance the performance and robustness of diffusion models in solving inverse problems, we propose Diffusion State-Guided Projected Gradient (DiffStateGrad), which projects the measurement gradient onto a subspace that is a low-rank approximation of an intermediate state of the diffusion process. DiffStateGrad, as a module, can be added to a wide range of diffusion-based inverse solvers to improve the preservation of the diffusion process on the prior manifold and filter out artifact-inducing components. We highlight that DiffStateGrad improves the robustness of diffusion models in terms of the choice of measurement guidance step size and noise while improving the worst-case performance. Finally, we demonstrate that DiffStateGrad improves upon the state-of-the-art on linear and nonlinear image restoration inverse problems.</li>
</ul>

<h3>Title: VEDIT: Latent Prediction Architecture For Procedural Video Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Han Lin, Tushar Nagarajan, Nicolas Ballas, Mido Assran, Mojtaba Komeili, Mohit Bansal, Koustuv Sinha</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03478">https://arxiv.org/abs/2410.03478</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03478">https://arxiv.org/pdf/2410.03478</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03478]] VEDIT: Latent Prediction Architecture For Procedural Video Representation Learning(https://arxiv.org/abs/2410.03478)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Procedural video representation learning is an active research area where the objective is to learn an agent which can anticipate and forecast the future given the present video input, typically in conjunction with textual annotations. Prior works often rely on large-scale pretraining of visual encoders and prediction models with language supervision. However, the necessity and effectiveness of extending compute intensive pretraining to learn video clip sequences with noisy text supervision have not yet been fully validated by previous works. In this work, we show that a strong off-the-shelf frozen pretrained visual encoder, along with a well designed prediction model, can achieve state-of-the-art (SoTA) performance in forecasting and procedural planning without the need for pretraining the prediction model, nor requiring additional supervision from language or ASR. Instead of learning representations from pixel space, our method utilizes the latent embedding space of publicly available vision encoders. By conditioning on frozen clip-level embeddings from observed steps to predict the actions of unseen steps, our prediction model is able to learn robust representations for forecasting through iterative denoising - leveraging the recent advances in diffusion transformers (Peebles & Xie, 2023). Empirical studies over a total of five procedural learning tasks across four datasets (NIV, CrossTask, COIN and Ego4D-v2) show that our model advances the strong baselines in long-horizon action anticipation (+2.6% in Verb ED@20, +3.1% in Noun ED@20), and significantly improves the SoTA in step forecasting (+5.0%), task classification (+3.8%), and procedure planning tasks (up to +2.28% in success rate, +3.39% in mAcc, and +0.90% in mIoU).</li>
</ul>

<h3>Title: Generative Artificial Intelligence for Navigating Synthesizable Chemical Space</h3>
<ul>
<li><strong>Authors: </strong>Wenhao Gao, Shitong Luo, Connor W. Coley</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.chem-ph, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03494">https://arxiv.org/abs/2410.03494</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03494">https://arxiv.org/pdf/2410.03494</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03494]] Generative Artificial Intelligence for Navigating Synthesizable Chemical Space(https://arxiv.org/abs/2410.03494)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We introduce SynFormer, a generative modeling framework designed to efficiently explore and navigate synthesizable chemical space. Unlike traditional molecular generation approaches, we generate synthetic pathways for molecules to ensure that designs are synthetically tractable. By incorporating a scalable transformer architecture and a diffusion module for building block selection, SynFormer surpasses existing models in synthesizable molecular design. We demonstrate SynFormer's effectiveness in two key applications: (1) local chemical space exploration, where the model generates synthesizable analogs of a reference molecule, and (2) global chemical space exploration, where the model aims to identify optimal molecules according to a black-box property prediction oracle. Additionally, we demonstrate the scalability of our approach via the improvement in performance as more computational resources become available. With our code and trained models openly available, we hope that SynFormer will find use across applications in drug discovery and materials science.</li>
</ul>

<h3>Title: NRGBoost: Energy-Based Generative Boosted Trees</h3>
<ul>
<li><strong>Authors: </strong>João Bravo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03535">https://arxiv.org/abs/2410.03535</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03535">https://arxiv.org/pdf/2410.03535</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03535]] NRGBoost: Energy-Based Generative Boosted Trees(https://arxiv.org/abs/2410.03535)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Despite the rise to dominance of deep learning in unstructured data domains, tree-based methods such as Random Forests (RF) and Gradient Boosted Decision Trees (GBDT) are still the workhorses for handling discriminative tasks on tabular data. We explore generative extensions of these popular algorithms with a focus on explicitly modeling the data density (up to a normalization constant), thus enabling other applications besides sampling. As our main contribution we propose an energy-based generative boosting algorithm that is analogous to the second order boosting implemented in popular packages like XGBoost. We show that, despite producing a generative model capable of handling inference tasks over any input variable, our proposed algorithm can achieve similar discriminative performance to GBDT on a number of real world tabular datasets, outperforming alternative generative approaches. At the same time, we show that it is also competitive with neural network based models for sampling.</li>
</ul>

<h3>Title: BodyShapeGPT: SMPL Body Shape Manipulation with LLMs</h3>
<ul>
<li><strong>Authors: </strong>Baldomero R. Árbol, Dan Casas</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03556">https://arxiv.org/abs/2410.03556</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03556">https://arxiv.org/pdf/2410.03556</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03556]] BodyShapeGPT: SMPL Body Shape Manipulation with LLMs(https://arxiv.org/abs/2410.03556)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative AI models provide a wide range of tools capable of performing complex tasks in a fraction of the time it would take a human. Among these, Large Language Models (LLMs) stand out for their ability to generate diverse texts, from literary narratives to specialized responses in different fields of knowledge. This paper explores the use of fine-tuned LLMs to identify physical descriptions of people, and subsequently create accurate representations of avatars using the SMPL-X model by inferring shape parameters. We demonstrate that LLMs can be trained to understand and manipulate the shape space of SMPL, allowing the control of 3D human shapes through natural language. This approach promises to improve human-machine interaction and opens new avenues for customization and simulation in virtual environments.</li>
</ul>

<h3>Title: Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features</h3>
<ul>
<li><strong>Authors: </strong>Benyuan Meng, Qianqian Xu, Zitai Wang, Xiaochun Cao, Qingming Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03558">https://arxiv.org/abs/2410.03558</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03558">https://arxiv.org/pdf/2410.03558</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03558]] Not All Diffusion Model Activations Have Been Evaluated as Discriminative Features(https://arxiv.org/abs/2410.03558)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models are initially designed for image generation. Recent research shows that the internal signals within their backbones, named activations, can also serve as dense features for various discriminative tasks such as semantic segmentation. Given numerous activations, selecting a small yet effective subset poses a fundamental problem. To this end, the early study of this field performs a large-scale quantitative comparison of the discriminative ability of the activations. However, we find that many potential activations have not been evaluated, such as the queries and keys used to compute attention scores. Moreover, recent advancements in diffusion architectures bring many new activations, such as those within embedded ViT modules. Both combined, activation selection remains unresolved but overlooked. To tackle this issue, this paper takes a further step with a much broader range of activations evaluated. Considering the significant increase in activations, a full-scale quantitative comparison is no longer operational. Instead, we seek to understand the properties of these activations, such that the activations that are clearly inferior can be filtered out in advance via simple qualitative evaluation. After careful analysis, we discover three properties universal among diffusion models, enabling this study to go beyond specific models. On top of this, we present effective feature selection solutions for several popular diffusion models. Finally, the experiments across multiple discriminative tasks validate the superiority of our method over the SOTA competitors. Our code is available at this https URL.</li>
</ul>

<h3>Title: Explicit, Implicit, and Scattered: Revisiting Event Extraction to Capture Complex Arguments</h3>
<ul>
<li><strong>Authors: </strong>Omar Sharif, Joseph Gatto, Madhusudan Basak, Sarah M. Preum</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03594">https://arxiv.org/abs/2410.03594</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03594">https://arxiv.org/pdf/2410.03594</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03594]] Explicit, Implicit, and Scattered: Revisiting Event Extraction to Capture Complex Arguments(https://arxiv.org/abs/2410.03594)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Prior works formulate the extraction of event-specific arguments as a span extraction problem, where event arguments are explicit -- i.e. assumed to be contiguous spans of text in a document. In this study, we revisit this definition of Event Extraction (EE) by introducing two key argument types that cannot be modeled by existing EE frameworks. First, implicit arguments are event arguments which are not explicitly mentioned in the text, but can be inferred through context. Second, scattered arguments are event arguments that are composed of information scattered throughout the text. These two argument types are crucial to elicit the full breadth of information required for proper event modeling. To support the extraction of explicit, implicit, and scattered arguments, we develop a novel dataset, DiscourseEE, which includes 7,464 argument annotations from online health discourse. Notably, 51.2% of the arguments are implicit, and 17.4% are scattered, making DiscourseEE a unique corpus for complex event extraction. Additionally, we formulate argument extraction as a text generation problem to facilitate the extraction of complex argument types. We provide a comprehensive evaluation of state-of-the-art models and highlight critical open challenges in generative event extraction. Our data and codebase are available at this https URL.</li>
</ul>

<h3>Title: How Discrete and Continuous Diffusion Meet: Comprehensive Analysis of Discrete Diffusion Models via a Stochastic Integral Framework</h3>
<ul>
<li><strong>Authors: </strong>Yinuo Ren, Haoxuan Chen, Grant M. Rotskoff, Lexing Ying</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03601">https://arxiv.org/abs/2410.03601</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03601">https://arxiv.org/pdf/2410.03601</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03601]] How Discrete and Continuous Diffusion Meet: Comprehensive Analysis of Discrete Diffusion Models via a Stochastic Integral Framework(https://arxiv.org/abs/2410.03601)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Discrete diffusion models have gained increasing attention for their ability to model complex distributions with tractable sampling and inference. However, the error analysis for discrete diffusion models remains less well-understood. In this work, we propose a comprehensive framework for the error analysis of discrete diffusion models based on Lévy-type stochastic integrals. By generalizing the Poisson random measure to that with a time-independent and state-dependent intensity, we rigorously establish a stochastic integral formulation of discrete diffusion models and provide the corresponding change of measure theorems that are intriguingly analogous to Itô integrals and Girsanov's theorem for their continuous counterparts. Our framework unifies and strengthens the current theoretical results on discrete diffusion models and obtains the first error bound for the $\tau$-leaping scheme in KL divergence. With error sources clearly identified, our analysis gives new insight into the mathematical properties of discrete diffusion models and offers guidance for the design of efficient and accurate algorithms for real-world discrete diffusion model applications.</li>
</ul>

<h3>Title: Real-World Benchmarks Make Membership Inference Attacks Fail on Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Chumeng Liang, Jiaxuan You</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03640">https://arxiv.org/abs/2410.03640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03640">https://arxiv.org/pdf/2410.03640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03640]] Real-World Benchmarks Make Membership Inference Attacks Fail on Diffusion Models(https://arxiv.org/abs/2410.03640)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Membership inference attacks (MIAs) on diffusion models have emerged as potential evidence of unauthorized data usage in training pre-trained diffusion models. These attacks aim to detect the presence of specific images in training datasets of diffusion models. Our study delves into the evaluation of state-of-the-art MIAs on diffusion models and reveals critical flaws and overly optimistic performance estimates in existing MIA evaluation. We introduce CopyMark, a more realistic MIA benchmark that distinguishes itself through the support for pre-trained diffusion models, unbiased datasets, and fair evaluation pipelines. Through extensive experiments, we demonstrate that the effectiveness of current MIA methods significantly degrades under these more practical conditions. Based on our results, we alert that MIA, in its current state, is not a reliable approach for identifying unauthorized data usage in pre-trained diffusion models. To the best of our knowledge, we are the first to discover the performance overestimation of MIAs on diffusion models and present a unified benchmark for more realistic evaluation. Our code is available on GitHub: \url{this https URL}.</li>
</ul>

<h3>Title: Geometric Representation Condition Improves Equivariant Molecule Generation</h3>
<ul>
<li><strong>Authors: </strong>Zian Li, Cai Zhou, Xiyuan Wang, Xingang Peng, Muhan Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03655">https://arxiv.org/abs/2410.03655</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03655">https://arxiv.org/pdf/2410.03655</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03655]] Geometric Representation Condition Improves Equivariant Molecule Generation(https://arxiv.org/abs/2410.03655)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in molecular generative models have demonstrated substantial potential in accelerating scientific discovery, particularly in drug design. However, these models often face challenges in generating high-quality molecules, especially in conditional scenarios where specific molecular properties must be satisfied. In this work, we introduce GeoRCG, a general framework to enhance the performance of molecular generative models by integrating geometric representation conditions. We decompose the molecule generation process into two stages: first, generating an informative geometric representation; second, generating a molecule conditioned on the representation. Compared to directly generating a molecule, the relatively easy-to-generate representation in the first-stage guides the second-stage generation to reach a high-quality molecule in a more goal-oriented and much faster way. Leveraging EDM as the base generator, we observe significant quality improvements in unconditional molecule generation on the widely-used QM9 and GEOM-DRUG datasets. More notably, in the challenging conditional molecular generation task, our framework achieves an average 31\% performance improvement over state-of-the-art approaches, highlighting the superiority of conditioning on semantically rich geometric representations over conditioning on individual property values as in previous approaches. Furthermore, we show that, with such representation guidance, the number of diffusion steps can be reduced to as small as 100 while maintaining superior generation quality than that achieved with 1,000 steps, thereby significantly accelerating the generation process.</li>
</ul>

<h3>Title: Estimating Body and Hand Motion in an Ego-sensed World</h3>
<ul>
<li><strong>Authors: </strong>Brent Yi, Vickie Ye, Maya Zheng, Lea Müller, Georgios Pavlakos, Yi Ma, Jitendra Malik, Angjoo Kanazawa</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2410.03665">https://arxiv.org/abs/2410.03665</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2410.03665">https://arxiv.org/pdf/2410.03665</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2410.03665]] Estimating Body and Hand Motion in an Ego-sensed World(https://arxiv.org/abs/2410.03665)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present EgoAllo, a system for human motion estimation from a head-mounted device. Using only egocentric SLAM poses and images, EgoAllo guides sampling from a conditional diffusion model to estimate 3D body pose, height, and hand parameters that capture the wearer's actions in the allocentric coordinate frame of the scene. To achieve this, our key insight is in representation: we propose spatial and temporal invariance criteria for improving model performance, from which we derive a head motion conditioning parameterization that improves estimation by up to 18%. We also show how the bodies estimated by our system can improve the hands: the resulting kinematic and temporal constraints result in over 40% lower hand estimation errors compared to noisy monocular estimates. Project page: this https URL</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
