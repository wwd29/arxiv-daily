<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>diffusion</h2>
<h3>Title: PhotoVerse: Tuning-Free Image Customization with Text-to-Image Diffusion Models. (arXiv:2309.05793v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.05793">http://arxiv.org/abs/2309.05793</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.05793]] PhotoVerse: Tuning-Free Image Customization with Text-to-Image Diffusion Models(http://arxiv.org/abs/2309.05793)</code></li>
<li>Summary: <p>Personalized text-to-image generation has emerged as a powerful and
sought-after tool, empowering users to create customized images based on their
specific concepts and prompts. However, existing approaches to personalization
encounter multiple challenges, including long tuning times, large storage
requirements, the necessity for multiple input images per identity, and
limitations in preserving identity and editability. To address these obstacles,
we present PhotoVerse, an innovative methodology that incorporates a
dual-branch conditioning mechanism in both text and image domains, providing
effective control over the image generation process. Furthermore, we introduce
facial identity loss as a novel component to enhance the preservation of
identity during training. Remarkably, our proposed PhotoVerse eliminates the
need for test time tuning and relies solely on a single facial photo of the
target identity, significantly reducing the resource cost associated with image
generation. After a single training phase, our approach enables generating
high-quality images within only a few seconds. Moreover, our method can produce
diverse images that encompass various scenes and styles. The extensive
evaluation demonstrates the superior performance of our approach, which
achieves the dual objectives of preserving identity and facilitating
editability. Project page: https://photoverse2d.github.io/
</p></li>
</ul>

<h3>Title: Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation. (arXiv:2309.05956v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.05956">http://arxiv.org/abs/2309.05956</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.05956]] Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation(http://arxiv.org/abs/2309.05956)</code></li>
<li>Summary: <p>We propose a new paradigm to automatically generate training data with
accurate labels at scale using the text-to-image synthesis frameworks (e.g.,
DALL-E, Stable Diffusion, etc.). The proposed approach1 decouples training data
generation into foreground object generation, and contextually coherent
background generation. To generate foreground objects, we employ a
straightforward textual template, incorporating the object class name as input
prompts. This is fed into a text-to-image synthesis framework, producing
various foreground images set against isolated backgrounds. A
foreground-background segmentation algorithm is then used to generate
foreground object masks. To generate context images, we begin by creating
language descriptions of the context. This is achieved by applying an image
captioning method to a small set of images representing the desired context.
These textual descriptions are then transformed into a diverse array of context
images via a text-to-image synthesis framework. Subsequently, we composite
these with the foreground object masks produced in the initial step, utilizing
a cut-and-paste method, to formulate the training data. We demonstrate the
advantages of our approach on five object detection and segmentation datasets,
including Pascal VOC and COCO. We found that detectors trained solely on
synthetic data produced by our method achieve performance comparable to those
trained on real data (Fig. 1). Moreover, a combination of real and synthetic
data yields even much better results. Further analysis indicates that the
synthetic data distribution complements the real data distribution effectively.
Additionally, we emphasize the compositional nature of our data generation
approach in out-of-distribution and zero-shot data generation scenarios. We
open-source our code at https://github.com/gyhandy/Text2Image-for-Detection
</p></li>
</ul>

<h3>Title: Prompting4Debugging: Red-Teaming Text-to-Image Diffusion Models by Finding Problematic Prompts. (arXiv:2309.06135v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.06135">http://arxiv.org/abs/2309.06135</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.06135]] Prompting4Debugging: Red-Teaming Text-to-Image Diffusion Models by Finding Problematic Prompts(http://arxiv.org/abs/2309.06135)</code></li>
<li>Summary: <p>Text-to-image diffusion models, e.g. Stable Diffusion (SD), lately have shown
remarkable ability in high-quality content generation, and become one of the
representatives for the recent wave of transformative AI. Nevertheless, such
advance comes with an intensifying concern about the misuse of this generative
technology, especially for producing copyrighted or NSFW (i.e. not safe for
work) images. Although efforts have been made to filter inappropriate
images/prompts or remove undesirable concepts/styles via model fine-tuning, the
reliability of these safety mechanisms against diversified problematic prompts
remains largely unexplored. In this work, we propose Prompting4Debugging (P4D)
as a debugging and red-teaming tool that automatically finds problematic
prompts for diffusion models to test the reliability of a deployed safety
mechanism. We demonstrate the efficacy of our P4D tool in uncovering new
vulnerabilities of SD models with safety mechanisms. Particularly, our result
shows that around half of prompts in existing safe prompting benchmarks which
were originally considered "safe" can actually be manipulated to bypass many
deployed safety mechanisms, including concept removal, negative prompt, and
safety guidance. Our findings suggest that, without comprehensive testing, the
evaluations on limited safe prompting benchmarks can lead to a false sense of
safety for text-to-image models.
</p></li>
</ul>

<h3>Title: Elucidating the solution space of extended reverse-time SDE for diffusion models. (arXiv:2309.06169v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.06169">http://arxiv.org/abs/2309.06169</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.06169]] Elucidating the solution space of extended reverse-time SDE for diffusion models(http://arxiv.org/abs/2309.06169)</code></li>
<li>Summary: <p>Diffusion models (DMs) demonstrate potent image generation capabilities in
various generative modeling tasks. Nevertheless, their primary limitation lies
in slow sampling speed, requiring hundreds or thousands of sequential function
evaluations through large neural networks to generate high-quality images.
Sampling from DMs can be seen as solving corresponding stochastic differential
equations (SDEs) or ordinary differential equations (ODEs). In this work, we
formulate the sampling process as an extended reverse-time SDE (ER SDE),
unifying prior explorations into ODEs and SDEs. Leveraging the semi-linear
structure of ER SDE solutions, we offer exact solutions and arbitrarily
high-order approximate solutions for VP SDE and VE SDE, respectively. Based on
the solution space of the ER SDE, we yield mathematical insights elucidating
the superior performance of ODE solvers over SDE solvers in terms of fast
sampling. Additionally, we unveil that VP SDE solvers stand on par with their
VE SDE counterparts. Finally, we devise fast and training-free samplers, ER-SDE
Solvers, elevating the efficiency of stochastic samplers to unprecedented
levels. Experimental results demonstrate achieving 3.45 FID in 20 function
evaluations and 2.24 FID in 50 function evaluations on the ImageNet
64$\times$64 dataset.
</p></li>
</ul>

<h3>Title: Fg-T2M: Fine-Grained Text-Driven Human Motion Generation via Diffusion Model. (arXiv:2309.06284v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.06284">http://arxiv.org/abs/2309.06284</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.06284]] Fg-T2M: Fine-Grained Text-Driven Human Motion Generation via Diffusion Model(http://arxiv.org/abs/2309.06284)</code></li>
<li>Summary: <p>Text-driven human motion generation in computer vision is both significant
and challenging. However, current methods are limited to producing either
deterministic or imprecise motion sequences, failing to effectively control the
temporal and spatial relationships required to conform to a given text
description. In this work, we propose a fine-grained method for generating
high-quality, conditional human motion sequences supporting precise text
description. Our approach consists of two key components: 1) a
linguistics-structure assisted module that constructs accurate and complete
language feature to fully utilize text information; and 2) a context-aware
progressive reasoning module that learns neighborhood and overall semantic
linguistics features from shallow and deep graph neural networks to achieve a
multi-step inference. Experiments show that our approach outperforms
text-driven motion generation methods on HumanML3D and KIT test sets and
generates better visually confirmed motion to the text conditions.
</p></li>
</ul>

<h3>Title: InstaFlow: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation. (arXiv:2309.06380v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.06380">http://arxiv.org/abs/2309.06380</a></li>
<li>Code URL: https://github.com/gnobitab/instaflow</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.06380]] InstaFlow: One Step is Enough for High-Quality Diffusion-Based Text-to-Image Generation(http://arxiv.org/abs/2309.06380)</code></li>
<li>Summary: <p>Diffusion models have revolutionized text-to-image generation with its
exceptional quality and creativity. However, its multi-step sampling process is
known to be slow, often requiring tens of inference steps to obtain
satisfactory results. Previous attempts to improve its sampling speed and
reduce computational costs through distillation have been unsuccessful in
achieving a functional one-step model. In this paper, we explore a recent
method called Rectified Flow, which, thus far, has only been applied to small
datasets. The core of Rectified Flow lies in its \emph{reflow} procedure, which
straightens the trajectories of probability flows, refines the coupling between
noises and images, and facilitates the distillation process with student
models. We propose a novel text-conditioned pipeline to turn Stable Diffusion
(SD) into an ultra-fast one-step model, in which we find reflow plays a
critical role in improving the assignment between noise and images. Leveraging
our new pipeline, we create, to the best of our knowledge, the first one-step
diffusion-based text-to-image generator with SD-level image quality, achieving
an FID (Frechet Inception Distance) of $23.3$ on MS COCO 2017-5k, surpassing
the previous state-of-the-art technique, progressive distillation, by a
significant margin ($37.2$ $\rightarrow$ $23.3$ in FID). By utilizing an
expanded network with 1.7B parameters, we further improve the FID to $22.4$. We
call our one-step models \emph{InstaFlow}. On MS COCO 2014-30k, InstaFlow
yields an FID of $13.1$ in just $0.09$ second, the best in $\leq 0.1$ second
regime, outperforming the recent StyleGAN-T ($13.9$ in $0.1$ second). Notably,
the training of InstaFlow only costs 199 A100 GPU days. Project
page:~\url{https://github.com/gnobitab/InstaFlow}.
</p></li>
</ul>

<h3>Title: Catch You Everything Everywhere: Guarding Textual Inversion via Concept Watermarking. (arXiv:2309.05940v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.05940">http://arxiv.org/abs/2309.05940</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.05940]] Catch You Everything Everywhere: Guarding Textual Inversion via Concept Watermarking(http://arxiv.org/abs/2309.05940)</code></li>
<li>Summary: <p>AIGC (AI-Generated Content) has achieved tremendous success in many
applications such as text-to-image tasks, where the model can generate
high-quality images with diverse prompts, namely, different descriptions in
natural languages. More surprisingly, the emerging personalization techniques
even succeed in describing unseen concepts with only a few personal images as
references, and there have been some commercial platforms for sharing the
valuable personalized concept. However, such an advanced technique also
introduces a severe threat, where malicious users can misuse the target concept
to generate highly-realistic illegal images. Therefore, it becomes necessary
for the platform to trace malicious users and hold them accountable.
</p>
<p>In this paper, we focus on guarding the most popular lightweight
personalization model, ie, Textual Inversion (TI). To achieve it, we propose
the novel concept watermarking, where watermark information is embedded into
the target concept and then extracted from generated images based on the
watermarked concept. Specifically, we jointly train a watermark encoder and a
watermark decoder with the sampler in the loop.
</p>
<p>It shows great resilience to different diffusion sampling processes possibly
chosen by malicious users, meanwhile preserving utility for normal use. In
practice, the concept owner can upload his concept with different watermarks
(ie, serial numbers) to the platform, and the platform allocates different
users with different serial numbers for subsequent tracing and forensics.
</p></li>
</ul>

<h2>self-supervised</h2>
<h3>Title: TransferDoc: A Self-Supervised Transferable Document Representation Learning Model Unifying Vision and Language. (arXiv:2309.05756v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.05756">http://arxiv.org/abs/2309.05756</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.05756]] TransferDoc: A Self-Supervised Transferable Document Representation Learning Model Unifying Vision and Language(http://arxiv.org/abs/2309.05756)</code></li>
<li>Summary: <p>The field of visual document understanding has witnessed a rapid growth in
emerging challenges and powerful multi-modal strategies. However, they rely on
an extensive amount of document data to learn their pretext objectives in a
``pre-train-then-fine-tune'' paradigm and thus, suffer a significant
performance drop in real-world online industrial settings. One major reason is
the over-reliance on OCR engines to extract local positional information within
a document page. Therefore, this hinders the model's generalizability,
flexibility and robustness due to the lack of capturing global information
within a document image. We introduce TransferDoc, a cross-modal
transformer-based architecture pre-trained in a self-supervised fashion using
three novel pretext objectives. TransferDoc learns richer semantic concepts by
unifying language and visual representations, which enables the production of
more transferable models. Besides, two novel downstream tasks have been
introduced for a ``closer-to-real'' industrial evaluation scenario where
TransferDoc outperforms other state-of-the-art approaches.
</p></li>
</ul>

<h3>Title: SCD-Net: Spatiotemporal Clues Disentanglement Network for Self-supervised Skeleton-based Action Recognition. (arXiv:2309.05834v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.05834">http://arxiv.org/abs/2309.05834</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.05834]] SCD-Net: Spatiotemporal Clues Disentanglement Network for Self-supervised Skeleton-based Action Recognition(http://arxiv.org/abs/2309.05834)</code></li>
<li>Summary: <p>Contrastive learning has achieved great success in skeleton-based action
recognition. However, most existing approaches encode the skeleton sequences as
entangled spatiotemporal representations and confine the contrasts to the same
level of representation. Instead, this paper introduces a novel contrastive
learning framework, namely Spatiotemporal Clues Disentanglement Network
(SCD-Net). Specifically, we integrate the decoupling module with a feature
extractor to derive explicit clues from spatial and temporal domains
respectively. As for the training of SCD-Net, with a constructed global anchor,
we encourage the interaction between the anchor and extracted clues. Further,
we propose a new masking strategy with structural constraints to strengthen the
contextual associations, leveraging the latest development from masked image
modelling into the proposed SCD-Net. We conduct extensive evaluations on the
NTU-RGB+D (60&amp;120) and PKU-MMD (I&amp;II) datasets, covering various downstream
tasks such as action recognition, action retrieval, transfer learning, and
semi-supervised learning. The experimental results demonstrate the
effectiveness of our method, which outperforms the existing state-of-the-art
(SOTA) approaches significantly.
</p></li>
</ul>

<h3>Title: Self-supervised Extraction of Human Motion Structures via Frame-wise Discrete Features. (arXiv:2309.05972v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.05972">http://arxiv.org/abs/2309.05972</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.05972]] Self-supervised Extraction of Human Motion Structures via Frame-wise Discrete Features(http://arxiv.org/abs/2309.05972)</code></li>
<li>Summary: <p>The present paper proposes an encoder-decoder model for extracting the
structures of human motions represented by frame-wise discrete features in a
self-supervised manner. In the proposed method, features are extracted as codes
in a motion codebook without the use of human knowledge, and the relationship
between these codes can be visualized on a graph. Since the codes are expected
to be temporally sparse compared to the captured frame rate and can be shared
by multiple sequences, the proposed network model also addresses the need for
training constraints. Specifically, the model consists of self-attention layers
and a vector clustering block. The attention layers contribute to finding
sparse keyframes and discrete features as motion codes, which are then
extracted by vector clustering. The constraints are realized as training losses
so that the same motion codes can be as contiguous as possible and can be
shared by multiple sequences. In addition, we propose the use of causal
self-attention as a method by which to calculate attention for long sequences
consisting of numerous frames. In our experiments, the sparse structures of
motion codes were used to compile a graph that facilitates visualization of the
relationship between the codes and the differences between sequences. We then
evaluated the effectiveness of the extracted motion codes by applying them to
multiple recognition tasks and found that performance levels comparable to
task-optimized methods could be achieved by linear probing.
</p></li>
</ul>

<h3>Title: Plasticity-Optimized Complementary Networks for Unsupervised Continual Learning. (arXiv:2309.06086v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.06086">http://arxiv.org/abs/2309.06086</a></li>
<li>Code URL: https://github.com/alviur/pocon_wacv2024</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.06086]] Plasticity-Optimized Complementary Networks for Unsupervised Continual Learning(http://arxiv.org/abs/2309.06086)</code></li>
<li>Summary: <p>Continuous unsupervised representation learning (CURL) research has greatly
benefited from improvements in self-supervised learning (SSL) techniques. As a
result, existing CURL methods using SSL can learn high-quality representations
without any labels, but with a notable performance drop when learning on a
many-tasks data stream. We hypothesize that this is caused by the
regularization losses that are imposed to prevent forgetting, leading to a
suboptimal plasticity-stability trade-off: they either do not adapt fully to
the incoming data (low plasticity), or incur significant forgetting when
allowed to fully adapt to a new SSL pretext-task (low stability). In this work,
we propose to train an expert network that is relieved of the duty of keeping
the previous knowledge and can focus on performing optimally on the new tasks
(optimizing plasticity). In the second phase, we combine this new knowledge
with the previous network in an adaptation-retrospection phase to avoid
forgetting and initialize a new expert with the knowledge of the old network.
We perform several experiments showing that our proposed approach outperforms
other CURL exemplar-free methods in few- and many-task split settings.
Furthermore, we show how to adapt our approach to semi-supervised continual
learning (Semi-SCL) and show that we surpass the accuracy of other
exemplar-free Semi-SCL methods and reach the results of some others that use
exemplars.
</p></li>
</ul>

<h3>Title: OTAS: Unsupervised Boundary Detection for Object-Centric Temporal Action Segmentation. (arXiv:2309.06276v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.06276">http://arxiv.org/abs/2309.06276</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.06276]] OTAS: Unsupervised Boundary Detection for Object-Centric Temporal Action Segmentation(http://arxiv.org/abs/2309.06276)</code></li>
<li>Summary: <p>Temporal action segmentation is typically achieved by discovering the
dramatic variances in global visual descriptors. In this paper, we explore the
merits of local features by proposing the unsupervised framework of
Object-centric Temporal Action Segmentation (OTAS). Broadly speaking, OTAS
consists of self-supervised global and local feature extraction modules as well
as a boundary selection module that fuses the features and detects salient
boundaries for action segmentation. As a second contribution, we discuss the
pros and cons of existing frame-level and boundary-level evaluation metrics.
Through extensive experiments, we find OTAS is superior to the previous
state-of-the-art method by $41\%$ on average in terms of our recommended F1
score. Surprisingly, OTAS even outperforms the ground-truth human annotations
in the user study. Moreover, OTAS is efficient enough to allow real-time
inference.
</p></li>
</ul>

<h3>Title: Attention De-sparsification Matters: Inducing Diversity in Digital Pathology Representation Learning. (arXiv:2309.06439v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.06439">http://arxiv.org/abs/2309.06439</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.06439]] Attention De-sparsification Matters: Inducing Diversity in Digital Pathology Representation Learning(http://arxiv.org/abs/2309.06439)</code></li>
<li>Summary: <p>We propose DiRL, a Diversity-inducing Representation Learning technique for
histopathology imaging. Self-supervised learning techniques, such as
contrastive and non-contrastive approaches, have been shown to learn rich and
effective representations of digitized tissue samples with limited pathologist
supervision. Our analysis of vanilla SSL-pretrained models' attention
distribution reveals an insightful observation: sparsity in attention, i.e,
models tends to localize most of their attention to some prominent patterns in
the image. Although attention sparsity can be beneficial in natural images due
to these prominent patterns being the object of interest itself, this can be
sub-optimal in digital pathology; this is because, unlike natural images,
digital pathology scans are not object-centric, but rather a complex phenotype
of various spatially intermixed biological components. Inadequate
diversification of attention in these complex images could result in crucial
information loss. To address this, we leverage cell segmentation to densely
extract multiple histopathology-specific representations, and then propose a
prior-guided dense pretext task for SSL, designed to match the multiple
corresponding representations between the views. Through this, the model learns
to attend to various components more closely and evenly, thus inducing adequate
diversification in attention for capturing context rich representations.
Through quantitative and qualitative analysis on multiple tasks across cancer
types, we demonstrate the efficacy of our method and observe that the attention
is more globally distributed.
</p></li>
</ul>

<h3>Title: Enhancing Hyperedge Prediction with Context-Aware Self-Supervised Learning. (arXiv:2309.05798v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.05798">http://arxiv.org/abs/2309.05798</a></li>
<li>Code URL: https://github.com/yy-ko/cash</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.05798]] Enhancing Hyperedge Prediction with Context-Aware Self-Supervised Learning(http://arxiv.org/abs/2309.05798)</code></li>
<li>Summary: <p>Hypergraphs can naturally model group-wise relations (e.g., a group of users
who co-purchase an item) as hyperedges. Hyperedge prediction is to predict
future or unobserved hyperedges, which is a fundamental task in many real-world
applications (e.g., group recommendation). Despite the recent breakthrough of
hyperedge prediction methods, the following challenges have been rarely
studied: (C1) How to aggregate the nodes in each hyperedge candidate for
accurate hyperedge prediction? and (C2) How to mitigate the inherent data
sparsity problem in hyperedge prediction? To tackle both challenges together,
in this paper, we propose a novel hyperedge prediction framework (CASH) that
employs (1) context-aware node aggregation to precisely capture complex
relations among nodes in each hyperedge for (C1) and (2) self-supervised
contrastive learning in the context of hyperedge prediction to enhance
hypergraph representations for (C2). Furthermore, as for (C2), we propose a
hyperedge-aware augmentation method to fully exploit the latent semantics
behind the original hypergraph and consider both node-level and group-level
contrasts (i.e., dual contrasts) for better node and hyperedge representations.
Extensive experiments on six real-world hypergraphs reveal that CASH
consistently outperforms all competing methods in terms of the accuracy in
hyperedge prediction and each of the proposed strategies is effective in
improving the model accuracy of CASH. For the detailed information of CASH, we
provide the code and datasets at: https://github.com/yy-ko/cash.
</p></li>
</ul>

<h3>Title: Optimizing Audio Augmentations for Contrastive Learning of Health-Related Acoustic Signals. (arXiv:2309.05843v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.05843">http://arxiv.org/abs/2309.05843</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.05843]] Optimizing Audio Augmentations for Contrastive Learning of Health-Related Acoustic Signals(http://arxiv.org/abs/2309.05843)</code></li>
<li>Summary: <p>Health-related acoustic signals, such as cough and breathing sounds, are
relevant for medical diagnosis and continuous health monitoring. Most existing
machine learning approaches for health acoustics are trained and evaluated on
specific tasks, limiting their generalizability across various healthcare
applications. In this paper, we leverage a self-supervised learning framework,
SimCLR with a Slowfast NFNet backbone, for contrastive learning of health
acoustics. A crucial aspect of optimizing Slowfast NFNet for this application
lies in identifying effective audio augmentations. We conduct an in-depth
analysis of various audio augmentation strategies and demonstrate that an
appropriate augmentation strategy enhances the performance of the Slowfast
NFNet audio encoder across a diverse set of health acoustic tasks. Our findings
reveal that when augmentations are combined, they can produce synergistic
effects that exceed the benefits seen when each is applied individually.
</p></li>
</ul>

<h2>foundation model</h2>
<h3>Title: Enhancing Representation in Radiography-Reports Foundation Model: A Granular Alignment Algorithm Using Masked Contrastive Learning. (arXiv:2309.05904v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.05904">http://arxiv.org/abs/2309.05904</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.05904]] Enhancing Representation in Radiography-Reports Foundation Model: A Granular Alignment Algorithm Using Masked Contrastive Learning(http://arxiv.org/abs/2309.05904)</code></li>
<li>Summary: <p>Recently, multi-modal vision-language foundation models have gained
significant attention in the medical field. While these models offer great
opportunities, they still face a number of challenges, such as the requirement
for fine-grained knowledge understanding in computer-aided diagnosis and
capability of utilizing very limited or no task-specific labeled data in
real-world clinical applications. In this study, we present MaCo, a novel
multi-modal medical foundation model that explores masked contrastive learning
to achieve granular alignment and zero-shot learning for a variety of medical
imaging tasks. MaCo incorporates a correlation weighting mechanism to adjust
the correlation between masked image patches and their corresponding reports,
thereby enhancing the representation learning capabilities. We evaluate MaCo on
six well-known open-source X-ray datasets, and the experimental results show it
outperforms seven state-of-the-art approaches for classification, segmentation,
and zero-shot phase grounding, demonstrating its great potential to promote a
wide range of medical image analysis tasks.
</p></li>
</ul>

<h3>Title: Speciality vs Generality: An Empirical Study on Catastrophic Forgetting in Fine-tuning Foundation Models. (arXiv:2309.06256v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.06256">http://arxiv.org/abs/2309.06256</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.06256]] Speciality vs Generality: An Empirical Study on Catastrophic Forgetting in Fine-tuning Foundation Models(http://arxiv.org/abs/2309.06256)</code></li>
<li>Summary: <p>Foundation models, including Vision Language Models (VLMs) and Large Language
Models (LLMs), possess the $generality$ to handle diverse distributions and
tasks, which stems from their extensive pre-training datasets. The fine-tuning
of foundation models is a common practice to enhance task performance or align
the model's behavior with human expectations, allowing them to gain
$speciality$. However, the small datasets used for fine-tuning may not
adequately cover the diverse distributions and tasks encountered during
pre-training. Consequently, the pursuit of speciality during fine-tuning can
lead to a loss of {generality} in the model, which is related to catastrophic
forgetting (CF) in deep learning. In this study, we demonstrate this phenomenon
in both VLMs and LLMs. For instance, fine-tuning VLMs like CLIP on ImageNet
results in a loss of generality in handling diverse distributions, and
fine-tuning LLMs like Galactica in the medical domain leads to a loss in
following instructions and common sense.
</p>
<p>To address the trade-off between the speciality and generality, we
investigate multiple regularization methods from continual learning, the weight
averaging method (Wise-FT) from out-of-distributional (OOD) generalization,
which interpolates parameters between pre-trained and fine-tuned models, and
parameter-efficient fine-tuning methods like Low-Rank Adaptation (LoRA). Our
findings show that both continual learning and Wise-ft methods effectively
mitigate the loss of generality, with Wise-FT exhibiting the strongest
performance in balancing speciality and generality.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Characterizing Latent Perspectives of Media Houses Towards Public Figures. (arXiv:2309.06112v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.06112">http://arxiv.org/abs/2309.06112</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.06112]] Characterizing Latent Perspectives of Media Houses Towards Public Figures(http://arxiv.org/abs/2309.06112)</code></li>
<li>Summary: <p>Media houses reporting on public figures, often come with their own biases
stemming from their respective worldviews. A characterization of these
underlying patterns helps us in better understanding and interpreting news
stories. For this, we need diverse or subjective summarizations, which may not
be amenable for classifying into predefined class labels. This work proposes a
zero-shot approach for non-extractive or generative characterizations of person
entities from a corpus using GPT-2. We use well-articulated articles from
several well-known news media houses as a corpus to build a sound argument for
this approach. First, we fine-tune a GPT-2 pre-trained language model with a
corpus where specific person entities are characterized. Second, we further
fine-tune this with demonstrations of person entity characterizations, created
from a corpus of programmatically constructed characterizations. This twice
fine-tuned model is primed with manual prompts consisting of entity names that
were not previously encountered in the second fine-tuning, to generate a simple
sentence about the entity. The results were encouraging, when compared against
actual characterizations from the corpus.
</p></li>
</ul>

<h3>Title: Generative Data Augmentation using LLMs improves Distributional Robustness in Question Answering. (arXiv:2309.06358v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.06358">http://arxiv.org/abs/2309.06358</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.06358]] Generative Data Augmentation using LLMs improves Distributional Robustness in Question Answering(http://arxiv.org/abs/2309.06358)</code></li>
<li>Summary: <p>Robustness in Natural Language Processing continues to be a pertinent issue,
where state of the art models under-perform under naturally shifted
distributions. In the context of Question Answering, work on domain adaptation
methods continues to be a growing body of research. However, very little
attention has been given to the notion of domain generalization under natural
distribution shifts, where the target domain is unknown. With drastic
improvements in the quality and access to generative models, we answer the
question: How do generated datasets influence the performance of QA models
under natural distribution shifts? We perform experiments on 4 different
datasets under varying amounts of distribution shift, and analyze how
"in-the-wild" generation can help achieve domain generalization. We take a
two-step generation approach, generating both contexts and QA pairs to augment
existing datasets. Through our experiments, we demonstrate how augmenting
reading comprehension datasets with generated data leads to better robustness
towards natural distribution shifts.
</p></li>
</ul>

<h3>Title: Framework-Based Qualitative Analysis of Free Responses of Large Language Models: Algorithmic Fidelity. (arXiv:2309.06364v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.06364">http://arxiv.org/abs/2309.06364</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.06364]] Framework-Based Qualitative Analysis of Free Responses of Large Language Models: Algorithmic Fidelity(http://arxiv.org/abs/2309.06364)</code></li>
<li>Summary: <p>Today, using Large-scale generative Language Models (LLMs) it is possible to
simulate free responses to interview questions like those traditionally
analyzed using qualitative research methods. Qualitative methodology
encompasses a broad family of techniques involving manual analysis of
open-ended interviews or conversations conducted freely in natural language.
Here we consider whether artificial "silicon participants" generated by LLMs
may be productively studied using qualitative methods aiming to produce
insights that could generalize to real human populations. The key concept in
our analysis is algorithmic fidelity, a term introduced by Argyle et al. (2023)
capturing the degree to which LLM-generated outputs mirror human
sub-populations' beliefs and attitudes. By definition, high algorithmic
fidelity suggests latent beliefs elicited from LLMs may generalize to real
humans, whereas low algorithmic fidelity renders such research invalid. Here we
used an LLM to generate interviews with silicon participants matching specific
demographic characteristics one-for-one with a set of human participants. Using
framework-based qualitative analysis, we showed the key themes obtained from
both human and silicon participants were strikingly similar. However, when we
analyzed the structure and tone of the interviews we found even more striking
differences. We also found evidence of the hyper-accuracy distortion described
by Aher et al. (2023). We conclude that the LLM we tested (GPT-3.5) does not
have sufficient algorithmic fidelity to expect research on it to generalize to
human populations. However, the rapid pace of LLM research makes it plausible
this could change in the future. Thus we stress the need to establish epistemic
norms now around how to assess validity of LLM-based qualitative research,
especially concerning the need to ensure representation of heterogeneous lived
experiences.
</p></li>
</ul>

<h3>Title: Radiology-Llama2: Best-in-Class Large Language Model for Radiology. (arXiv:2309.06419v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.06419">http://arxiv.org/abs/2309.06419</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.06419]] Radiology-Llama2: Best-in-Class Large Language Model for Radiology(http://arxiv.org/abs/2309.06419)</code></li>
<li>Summary: <p>This paper introduces Radiology-Llama2, a large language model specialized
for radiology through a process known as instruction tuning. Radiology-Llama2
is based on the Llama2 architecture and further trained on a large dataset of
radiology reports to generate coherent and clinically useful impressions from
radiological findings. Quantitative evaluations using ROUGE metrics on the
MIMIC-CXR and OpenI datasets demonstrate that Radiology-Llama2 achieves
state-of-the-art performance compared to other generative language models, with
a Rouge-1 score of 0.4834 on MIMIC-CXR and 0.4185 on OpenI. Additional
assessments by radiology experts highlight the model's strengths in
understandability, coherence, relevance, conciseness, and clinical utility. The
work illustrates the potential of localized language models designed and tuned
for specialized domains like radiology. When properly evaluated and deployed,
such models can transform fields like radiology by automating rote tasks and
enhancing human expertise.
</p></li>
</ul>

<h3>Title: ChemSpaceAL: An Efficient Active Learning Methodology Applied to Protein-Specific Molecular Generation. (arXiv:2309.05853v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.05853">http://arxiv.org/abs/2309.05853</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.05853]] ChemSpaceAL: An Efficient Active Learning Methodology Applied to Protein-Specific Molecular Generation(http://arxiv.org/abs/2309.05853)</code></li>
<li>Summary: <p>The incredible capabilities of generative artificial intelligence models have
inevitably led to their application in the domain of drug discovery. It is
therefore of tremendous interest to develop methodologies that enhance the
abilities and applicability of these powerful tools. In this work, we present a
novel and efficient semi-supervised active learning methodology that allows for
the fine-tuning of a generative model with respect to an objective function by
strategically operating within a constructed representation of the sample
space. In the context of targeted molecular generation, we demonstrate the
ability to fine-tune a GPT-based molecular generator with respect to an
attractive interaction-based scoring function by strategically operating within
a chemical space proxy, thereby maximizing attractive interactions between the
generated molecules and a protein target. Importantly, our approach does not
require the individual evaluation of all data points that are used for
fine-tuning, enabling the incorporation of computationally expensive metrics.
We are hopeful that the inherent generality of this methodology ensures that it
will remain applicable as this exciting field evolves. To facilitate
implementation and reproducibility, we have made all of our software available
through the open-source ChemSpaceAL Python package.
</p></li>
</ul>

<h2>anomaly</h2>
<h3>Title: ATTA: Anomaly-aware Test-Time Adaptation for Out-of-Distribution Detection in Segmentation. (arXiv:2309.05994v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.05994">http://arxiv.org/abs/2309.05994</a></li>
<li>Code URL: https://github.com/gaozhitong/atta</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.05994]] ATTA: Anomaly-aware Test-Time Adaptation for Out-of-Distribution Detection in Segmentation(http://arxiv.org/abs/2309.05994)</code></li>
<li>Summary: <p>Recent advancements in dense out-of-distribution (OOD) detection have
primarily focused on scenarios where the training and testing datasets share a
similar domain, with the assumption that no domain shift exists between them.
However, in real-world situations, domain shift often exits and significantly
affects the accuracy of existing out-of-distribution (OOD) detection models. In
this work, we propose a dual-level OOD detection framework to handle domain
shift and semantic shift jointly. The first level distinguishes whether domain
shift exists in the image by leveraging global low-level features, while the
second level identifies pixels with semantic shift by utilizing dense
high-level feature maps. In this way, we can selectively adapt the model to
unseen domains as well as enhance model's capacity in detecting novel classes.
We validate the efficacy of our proposed method on several OOD segmentation
benchmarks, including those with significant domain shifts and those without,
observing consistent performance improvements across various baseline models.
</p></li>
</ul>

<h3>Title: Effective Abnormal Activity Detection on Multivariate Time Series Healthcare Data. (arXiv:2309.05845v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.05845">http://arxiv.org/abs/2309.05845</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.05845]] Effective Abnormal Activity Detection on Multivariate Time Series Healthcare Data(http://arxiv.org/abs/2309.05845)</code></li>
<li>Summary: <p>Multivariate time series (MTS) data collected from multiple sensors provide
the potential for accurate abnormal activity detection in smart healthcare
scenarios. However, anomalies exhibit diverse patterns and become unnoticeable
in MTS data. Consequently, achieving accurate anomaly detection is challenging
since we have to capture both temporal dependencies of time series and
inter-relationships among variables. To address this problem, we propose a
Residual-based Anomaly Detection approach, Rs-AD, for effective representation
learning and abnormal activity detection. We evaluate our scheme on a
real-world gait dataset and the experimental results demonstrate an F1 score of
0.839.
</p></li>
</ul>

<h3>Title: GLAD: Content-aware Dynamic Graphs For Log Anomaly Detection. (arXiv:2309.05953v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.05953">http://arxiv.org/abs/2309.05953</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.05953]] GLAD: Content-aware Dynamic Graphs For Log Anomaly Detection(http://arxiv.org/abs/2309.05953)</code></li>
<li>Summary: <p>Logs play a crucial role in system monitoring and debugging by recording
valuable system information, including events and states. Although various
methods have been proposed to detect anomalies in log sequences, they often
overlook the significance of considering relations among system components,
such as services and users, which can be identified from log contents.
Understanding these relations is vital for detecting anomalies and their
underlying causes. To address this issue, we introduce GLAD, a Graph-based Log
Anomaly Detection framework designed to detect relational anomalies in system
logs. GLAD incorporates log semantics, relational patterns, and sequential
patterns into a unified framework for anomaly detection. Specifically, GLAD
first introduces a field extraction module that utilizes prompt-based few-shot
learning to identify essential fields from log contents. Then GLAD constructs
dynamic log graphs for sliding windows by interconnecting extracted fields and
log events parsed from the log parser. These graphs represent events and fields
as nodes and their relations as edges. Subsequently, GLAD utilizes a
temporal-attentive graph edge anomaly detection model for identifying anomalous
relations in these dynamic log graphs. This model employs a Graph Neural
Network (GNN)-based encoder enhanced with transformers to capture content,
structural and temporal features. We evaluate our proposed method on three
datasets, and the results demonstrate the effectiveness of GLAD in detecting
anomalies indicated by varying relational patterns.
</p></li>
</ul>

<h3>Title: Normality Learning-based Graph Anomaly Detection via Multi-Scale Contrastive Learning. (arXiv:2309.06034v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.06034">http://arxiv.org/abs/2309.06034</a></li>
<li>Code URL: https://github.com/felixdjc/nlgad</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.06034]] Normality Learning-based Graph Anomaly Detection via Multi-Scale Contrastive Learning(http://arxiv.org/abs/2309.06034)</code></li>
<li>Summary: <p>Graph anomaly detection (GAD) has attracted increasing attention in machine
learning and data mining. Recent works have mainly focused on how to capture
richer information to improve the quality of node embeddings for GAD. Despite
their significant advances in detection performance, there is still a relative
dearth of research on the properties of the task. GAD aims to discern the
anomalies that deviate from most nodes. However, the model is prone to learn
the pattern of normal samples which make up the majority of samples. Meanwhile,
anomalies can be easily detected when their behaviors differ from normality.
Therefore, the performance can be further improved by enhancing the ability to
learn the normal pattern. To this end, we propose a normality learning-based
GAD framework via multi-scale contrastive learning networks (NLGAD for
abbreviation). Specifically, we first initialize the model with the contrastive
networks on different scales. To provide sufficient and reliable normal nodes
for normality learning, we design an effective hybrid strategy for normality
selection. Finally, the model is refined with the only input of reliable normal
nodes and learns a more accurate estimate of normality so that anomalous nodes
can be more easily distinguished. Eventually, extensive experiments on six
benchmark graph datasets demonstrate the effectiveness of our normality
learning-based scheme on GAD. Notably, the proposed algorithm improves the
detection performance (up to 5.89% AUC gain) compared with the state-of-the-art
methods. The source code is released at https://github.com/FelixDJC/NLGAD.
</p></li>
</ul>

<h2>in-context</h2>
<h3>Title: How does representation impact in-context learning: A exploration on a synthetic task. (arXiv:2309.06054v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.06054">http://arxiv.org/abs/2309.06054</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.06054]] How does representation impact in-context learning: A exploration on a synthetic task(http://arxiv.org/abs/2309.06054)</code></li>
<li>Summary: <p>In-context learning, i.e., learning from in-context samples, is an impressive
ability of Transformer. However, the mechanism driving the in-context learning
is not yet fully understood. In this study, we aim to investigate from an
underexplored perspective of representation learning. The representation is
more complex for in-context learning senario, where the representation can be
impacted by both model weights and in-context samples. We refer the above two
conceptually aspects of representation as in-weight component and in-context
component, respectively. To study how the two components affect in-context
learning capabilities, we construct a novel synthetic task, making it possible
to device two probes, in-weights probe and in-context probe, to evaluate the
two components, respectively. We demonstrate that the goodness of in-context
component is highly related to the in-context learning performance, which
indicates the entanglement between in-context learning and representation
learning. Furthermore, we find that a good in-weights component can actually
benefit the learning of the in-context component, indicating that in-weights
learning should be the foundation of in-context learning. To further understand
the the in-context learning mechanism and importance of the in-weights
component, we proof by construction that a simple Transformer, which uses
pattern matching and copy-past mechanism to perform in-context learning, can
match the in-context learning performance with more complex, best tuned
Transformer under the perfect in-weights component assumption. In short, those
discoveries from representation learning perspective shed light on new
approaches to improve the in-context capacity.
</p></li>
</ul>

<h3>Title: Uncovering mesa-optimization algorithms in Transformers. (arXiv:2309.05858v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.05858">http://arxiv.org/abs/2309.05858</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.05858]] Uncovering mesa-optimization algorithms in Transformers(http://arxiv.org/abs/2309.05858)</code></li>
<li>Summary: <p>Transformers have become the dominant model in deep learning, but the reason
for their superior performance is poorly understood. Here, we hypothesize that
the strong performance of Transformers stems from an architectural bias towards
mesa-optimization, a learned process running within the forward pass of a model
consisting of the following two steps: (i) the construction of an internal
learning objective, and (ii) its corresponding solution found through
optimization. To test this hypothesis, we reverse-engineer a series of
autoregressive Transformers trained on simple sequence modeling tasks,
uncovering underlying gradient-based mesa-optimization algorithms driving the
generation of predictions. Moreover, we show that the learned forward-pass
optimization algorithm can be immediately repurposed to solve supervised
few-shot tasks, suggesting that mesa-optimization might underlie the in-context
learning capabilities of large language models. Finally, we propose a novel
self-attention layer, the mesa-layer, that explicitly and efficiently solves
optimization problems specified in context. We find that this layer can lead to
improved performance in synthetic and preliminary language modeling
experiments, adding weight to our hypothesis that mesa-optimization is an
important operation hidden within the weights of trained Transformers.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
