<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-02-22</h1>
<h3>Title: The Uncanny Valley: A Comprehensive Analysis of Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Karam Ghanem, Danilo Bzdok</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13369">https://arxiv.org/abs/2402.13369</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13369">https://arxiv.org/pdf/2402.13369</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13369]] The Uncanny Valley: A Comprehensive Analysis of Diffusion Models(https://arxiv.org/abs/2402.13369)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Through Diffusion Models (DMs), we have made significant advances in generating high-quality images. Our exploration of these models delves deeply into their core operational principles by systematically investigating key aspects across various DM architectures: i) noise schedules, ii) samplers, and iii) guidance. Our comprehensive examination of these models sheds light on their hidden fundamental mechanisms, revealing the concealed foundational elements that are essential for their effectiveness. Our analyses emphasize the hidden key factors that determine model performance, offering insights that contribute to the advancement of DMs. Past findings show that the configuration of noise schedules, samplers, and guidance is vital to the quality of generated images; however, models reach a stable level of quality across different configurations at a remarkably similar point, revealing that the decisive factors for optimal performance predominantly reside in the diffusion process dynamics and the structural design of the model's network, rather than the specifics of configuration details. Our comparative analysis reveals that Denoising Diffusion Probabilistic Model (DDPM)-based diffusion dynamics consistently outperform the Noise Conditioned Score Network (NCSN)-based ones, not only when evaluated in their original forms but also when continuous through Stochastic Differential Equation (SDE)-based implementations.</li>
</ul>

<h3>Title: Layout-to-Image Generation with Localized Descriptions using ControlNet  with Cross-Attention Control</h3>
<ul>
<li><strong>Authors: </strong>Denis Lukovnikov, Asja Fischer</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13404">https://arxiv.org/abs/2402.13404</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13404">https://arxiv.org/pdf/2402.13404</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13404]] Layout-to-Image Generation with Localized Descriptions using ControlNet  with Cross-Attention Control(https://arxiv.org/abs/2402.13404)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>While text-to-image diffusion models can generate highquality images from textual descriptions, they generally lack fine-grained control over the visual composition of the generated images. Some recent works tackle this problem by training the model to condition the generation process on additional input describing the desired image layout. Arguably the most popular among such methods, ControlNet, enables a high degree of control over the generated image using various types of conditioning inputs (e.g. segmentation maps). However, it still lacks the ability to take into account localized textual descriptions that indicate which image region is described by which phrase in the prompt. In this work, we show the limitations of ControlNet for the layout-to-image task and enable it to use localized descriptions using a training-free approach that modifies the crossattention scores during generation. We adapt and investigate several existing cross-attention control methods in the context of ControlNet and identify shortcomings that cause failure (concept bleeding) or image degradation under specific conditions. To address these shortcomings, we develop a novel cross-attention manipulation method in order to maintain image quality while improving control. Qualitative and quantitative experimental studies focusing on challenging cases are presented, demonstrating the effectiveness of the investigated general approach, and showing the improvements obtained by the proposed cross-attention control method.</li>
</ul>

<h3>Title: Harnessing Large Language Models as Post-hoc Correctors</h3>
<ul>
<li><strong>Authors: </strong>Zhiqiang Zhong, Kuangyu Zhou, Davide Mottin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13414">https://arxiv.org/abs/2402.13414</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13414">https://arxiv.org/pdf/2402.13414</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13414]] Harnessing Large Language Models as Post-hoc Correctors(https://arxiv.org/abs/2402.13414)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>As Machine Learning (ML) models grow in size and demand higher-quality training data, the expenses associated with re-training and fine-tuning these models are escalating rapidly. Inspired by recent impressive achievements of Large Language Models (LLMs) in different fields, this paper delves into the question: can LLMs efficiently improve an ML's performance at a minimal cost? We show that, through our proposed training-free framework LlmCorr, an LLM can work as a post-hoc corrector to propose corrections for the predictions of an arbitrary ML model. In particular, we form a contextual knowledge database by incorporating the dataset's label information and the ML model's predictions on the validation dataset. Leveraging the in-context learning capability of LLMs, we ask the LLM to summarise the instances in which the ML model makes mistakes and the correlation between primary predictions and true labels. Following this, the LLM can transfer its acquired knowledge to suggest corrections for the ML model's predictions. Our experimental results on the challenging molecular predictions show that LlmCorr improves the performance of a number of models by up to 39%.</li>
</ul>

<h3>Title: CAMELoT: Towards Large Language Models with Training-Free Consolidated  Associative Memory</h3>
<ul>
<li><strong>Authors: </strong>Zexue He, Leonid Karlinsky, Donghyun Kim, Julian McAuley, Dmitry Krotov, Rogerio Feris</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13449">https://arxiv.org/abs/2402.13449</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13449">https://arxiv.org/pdf/2402.13449</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13449]] CAMELoT: Towards Large Language Models with Training-Free Consolidated  Associative Memory(https://arxiv.org/abs/2402.13449)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) struggle to handle long input sequences due to high memory and runtime costs. Memory-augmented models have emerged as a promising solution to this problem, but current methods are hindered by limited memory capacity and require costly re-training to integrate with a new LLM. In this work, we introduce an associative memory module which can be coupled to any pre-trained (frozen) attention-based LLM without re-training, enabling it to handle arbitrarily long input sequences. Unlike previous methods, our associative memory module consolidates representations of individual tokens into a non-parametric distribution model, dynamically managed by properly balancing the novelty and recency of the incoming data. By retrieving information from this consolidated associative memory, the base LLM can achieve significant (up to 29.7% on Arxiv) perplexity reduction in long-context modeling compared to other baselines evaluated on standard benchmarks. This architecture, which we call CAMELoT (Consolidated Associative Memory Enhanced Long Transformer), demonstrates superior performance even with a tiny context window of 128 tokens, and also enables improved in-context learning with a much larger set of demonstrations.</li>
</ul>

<h3>Title: Unsupervised learning based object detection using Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Chandan Kumar, Jansel Herrera-Gerena, John Just, Matthew Darr, Ali Jannesari</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13465">https://arxiv.org/abs/2402.13465</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13465">https://arxiv.org/pdf/2402.13465</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13465]] Unsupervised learning based object detection using Contrastive Learning(https://arxiv.org/abs/2402.13465)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Training image-based object detectors presents formidable challenges, as it entails not only the complexities of object detection but also the added intricacies of precisely localizing objects within potentially diverse and noisy environments. However, the collection of imagery itself can often be straightforward; for instance, cameras mounted in vehicles can effortlessly capture vast amounts of data in various real-world scenarios. In light of this, we introduce a groundbreaking method for training single-stage object detectors through unsupervised/self-supervised learning. Our state-of-the-art approach has the potential to revolutionize the labeling process, substantially reducing the time and cost associated with manual annotation. Furthermore, it paves the way for previously unattainable research opportunities, particularly for large, diverse, and challenging datasets lacking extensive labels. In contrast to prevalent unsupervised learning methods that primarily target classification tasks, our approach takes on the unique challenge of object detection. We pioneer the concept of intra-image contrastive learning alongside inter-image counterparts, enabling the acquisition of crucial location information essential for object detection. The method adeptly learns and represents this location information, yielding informative heatmaps. Our results showcase an outstanding accuracy of \textbf{89.2\%}, marking a significant breakthrough of approximately \textbf{15x} over random initialization in the realm of unsupervised object detection within the field of computer vision.</li>
</ul>

<h3>Title: How Important is Domain Specificity in Language Models and Instruction  Finetuning for Biomedical Relation Extraction?</h3>
<ul>
<li><strong>Authors: </strong>Aviv Brokman, Ramakanth Kavuluru</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13470">https://arxiv.org/abs/2402.13470</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13470">https://arxiv.org/pdf/2402.13470</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13470]] How Important is Domain Specificity in Language Models and Instruction  Finetuning for Biomedical Relation Extraction?(https://arxiv.org/abs/2402.13470)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Cutting edge techniques developed in the general NLP domain are often subsequently applied to the high-value, data-rich biomedical domain. The past few years have seen generative language models (LMs), instruction finetuning, and few-shot learning become foci of NLP research. As such, generative LMs pretrained on biomedical corpora have proliferated and biomedical instruction finetuning has been attempted as well, all with the hope that domain specificity improves performance on downstream tasks. Given the nontrivial effort in training such models, we investigate what, if any, benefits they have in the key biomedical NLP task of relation extraction. Specifically, we address two questions: (1) Do LMs trained on biomedical corpora outperform those trained on general domain corpora? (2) Do models instruction finetuned on biomedical datasets outperform those finetuned on assorted datasets or those simply pretrained? We tackle these questions using existing LMs, testing across four datasets. In a surprising result, general-domain models typically outperformed biomedical-domain models. However, biomedical instruction finetuning improved performance to a similar degree as general instruction finetuning, despite having orders of magnitude fewer instructions. Our findings suggest it may be more fruitful to focus research effort on larger-scale biomedical instruction finetuning of general LMs over building domain-specific biomedical LMs</li>
</ul>

<h3>Title: ProPD: Dynamic Token Tree Pruning and Generation for LLM Parallel  Decoding</h3>
<ul>
<li><strong>Authors: </strong>Shuzhang Zhong, Zebin Yang, Meng Li, Ruihao Gong, Runsheng Wang, Ru Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13485">https://arxiv.org/abs/2402.13485</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13485">https://arxiv.org/pdf/2402.13485</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13485]] ProPD: Dynamic Token Tree Pruning and Generation for LLM Parallel  Decoding(https://arxiv.org/abs/2402.13485)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in generative large language models (LLMs) have significantly boosted the performance in natural language processing tasks. However, their efficiency is hampered by the inherent limitations in autoregressive token generation. While parallel decoding with token tree verification, e.g., Medusa, has been proposed to improve decoding parallelism and efficiency, it often struggles with maintaining contextual relationships due to its independent token prediction approach and incurs significant verification overhead, especially with large tree sizes and batch processing. In this paper, we propose ProPD, an efficient LLM parallel decoding framework based on dynamic token tree pruning and generation. ProPD features an advanced early pruning mechanism to efficiently eliminate unpromising token sequences to improve verification efficiency. Additionally, it introduces a dynamic token tree generation algorithm to balance the computation and parallelism of the verification phase in real-time and maximize the overall efficiency across different batch sizes, sequence lengths, and tasks, etc. We verify ProPD across a diverse set of datasets, LLMs, and batch sizes and demonstrate ProPD consistently outperforms existing decoding algorithms by 1.1-3.2x.</li>
</ul>

<h3>Title: Contrastive Prompts Improve Disentanglement in Text-to-Image Diffusion  Models</h3>
<ul>
<li><strong>Authors: </strong>Chen Wu, Fernando De la Torre</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13490">https://arxiv.org/abs/2402.13490</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13490">https://arxiv.org/pdf/2402.13490</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13490]] Contrastive Prompts Improve Disentanglement in Text-to-Image Diffusion  Models(https://arxiv.org/abs/2402.13490)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models have achieved remarkable performance in image synthesis, while the text interface does not always provide fine-grained control over certain image factors. For instance, changing a single token in the text can have unintended effects on the image. This paper shows a simple modification of classifier-free guidance can help disentangle image factors in text-to-image models. The key idea of our method, Contrastive Guidance, is to characterize an intended factor with two prompts that differ in minimal tokens: the positive prompt describes the image to be synthesized, and the baseline prompt serves as a "baseline" that disentangles other factors. Contrastive Guidance is a general method we illustrate whose benefits in three scenarios: (1) to guide domain-specific diffusion models trained on an object class, (2) to gain continuous, rig-like controls for text-to-image generation, and (3) to improve the performance of zero-shot image editors.</li>
</ul>

<h3>Title: From Self-Attention to Markov Models: Unveiling the Dynamics of  Generative Transformers</h3>
<ul>
<li><strong>Authors: </strong>M. Emrullah Ildiz, Yixiao Huang, Yingcong Li, Ankit Singh Rawat, Samet Oymak</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13512">https://arxiv.org/abs/2402.13512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13512">https://arxiv.org/pdf/2402.13512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13512]] From Self-Attention to Markov Models: Unveiling the Dynamics of  Generative Transformers(https://arxiv.org/abs/2402.13512)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Modern language models rely on the transformer architecture and attention mechanism to perform language understanding and text generation. In this work, we study learning a 1-layer self-attention model from a set of prompts and associated output data sampled from the model. We first establish a precise mapping between the self-attention mechanism and Markov models: Inputting a prompt to the model samples the output token according to a context-conditioned Markov chain (CCMC) which weights the transition matrix of a base Markov chain. Additionally, incorporating positional encoding results in position-dependent scaling of the transition probabilities. Building on this formalism, we develop identifiability/coverage conditions for the prompt distribution that guarantee consistent estimation and establish sample complexity guarantees under IID samples. Finally, we study the problem of learning from a single output trajectory generated from an initial prompt. We characterize an intriguing winner-takes-all phenomenon where the generative process implemented by self-attention collapses into sampling a limited subset of tokens due to its non-mixing nature. This provides a mathematical explanation to the tendency of modern LLMs to generate repetitive text. In summary, the equivalence to CCMC provides a simple but powerful framework to study self-attention and its properties.</li>
</ul>

<h3>Title: OMGEval: An Open Multilingual Generative Evaluation Benchmark for Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yang Liu, Meng Xu, Shuo Wang, Liner Yang, Haoyu Wang, Zhenghao Liu, Cunliang Kong, Yun Chen, Yang Liu, Maosong Sun, Erhong Yang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13524">https://arxiv.org/abs/2402.13524</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13524">https://arxiv.org/pdf/2402.13524</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13524]] OMGEval: An Open Multilingual Generative Evaluation Benchmark for Large  Language Models(https://arxiv.org/abs/2402.13524)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Modern large language models (LLMs) should generally benefit individuals from various cultural backgrounds around the world. However, most recent advanced generative evaluation benchmarks tailed for LLMs mainly focus on English. To this end, we introduce OMGEval, the first Open-source Multilingual Generative test set that can assess the capability of LLMs in different languages. For each language, OMGEval provides 804 open-ended questions, covering a wide range of important capabilities of LLMs, such as general knowledge, logical reasoning, and so on. Each question is rigorously verified by human annotators. Notably, to sufficiently reflect the compatibility of LLMs in different cultural backgrounds, we perform localization for each non-English language. Specifically, the current version of OMGEval includes 5 languages (i.e., Zh, Ru, Fr, Es, Ar). Following AlpacaEval, we employ GPT-4 as the adjudicator to automatically score different model outputs, which is shown closely related to human evaluation. We evaluate several representative multilingual LLMs on the proposed OMGEval, which we believe will provide a valuable reference for the community to further understand and improve the multilingual capability of LLMs. OMGEval is available at https://github.com/blcuicall/OMGEval.</li>
</ul>

<h3>Title: DiffPLF: A Conditional Diffusion Model for Probabilistic Forecasting of  EV Charging Load</h3>
<ul>
<li><strong>Authors: </strong>Siyang Li, Hui Xiong, Yize Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13548">https://arxiv.org/abs/2402.13548</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13548">https://arxiv.org/pdf/2402.13548</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13548]] DiffPLF: A Conditional Diffusion Model for Probabilistic Forecasting of  EV Charging Load(https://arxiv.org/abs/2402.13548)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Due to the vast electric vehicle (EV) penetration to distribution grid, charging load forecasting is essential to promote charging station operation and demand-side management.However, the stochastic charging behaviors and associated exogenous factors render future charging load patterns quite volatile and hard to predict. Accordingly, we devise a novel Diffusion model termed DiffPLF for Probabilistic Load Forecasting of EV charging, which can explicitly approximate the predictive load distribution conditioned on historical data and related covariates. Specifically, we leverage a denoising diffusion model, which can progressively convert the Gaussian prior to real time-series data by learning a reversal of the diffusion process. Besides, we couple such diffusion model with a cross-attention-based conditioning mechanism to execute conditional generation for possible charging demand profiles. We also propose a task-informed fine-tuning technique to better adapt DiffPLF to the probabilistic time-series forecasting task and acquire more accurate and reliable predicted intervals. Finally, we conduct multiple experiments to validate the superiority of DiffPLF to predict complex temporal patterns of erratic charging load and carry out controllable generation based on certain covariate. Results demonstrate that we can attain a notable rise of 39.58% and 49.87% on MAE and CRPS respectively compared to the conventional method.</li>
</ul>

<h3>Title: Generative AI for Secure Physical Layer Communications: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Changyuan Zhao, Hongyang Du, Dusit Niyato, Jiawen Kang, Zehui Xiong, Dong In Kim, Xuemin (Sherman)Shen, Khaled B. Letaief</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13553">https://arxiv.org/abs/2402.13553</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13553">https://arxiv.org/pdf/2402.13553</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13553]] Generative AI for Secure Physical Layer Communications: A Survey(https://arxiv.org/abs/2402.13553)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generative Artificial Intelligence (GAI) stands at the forefront of AI innovation, demonstrating rapid advancement and unparalleled proficiency in generating diverse content. Beyond content creation, GAI has significant analytical abilities to learn complex data distribution, offering numerous opportunities to resolve security issues. In the realm of security from physical layer perspectives, traditional AI approaches frequently struggle, primarily due to their limited capacity to dynamically adjust to the evolving physical attributes of transmission channels and the complexity of contemporary cyber threats. This adaptability and analytical depth are precisely where GAI excels. Therefore, in this paper, we offer an extensive survey on the various applications of GAI in enhancing security within the physical layer of communication networks. We first emphasize the importance of advanced GAI models in this area, including Generative Adversarial Networks (GANs), Autoencoders (AEs), Variational Autoencoders (VAEs), and Diffusion Models (DMs). We delve into the roles of GAI in addressing challenges of physical layer security, focusing on communication confidentiality, authentication, availability, resilience, and integrity. Furthermore, we also present future research directions focusing model improvements, multi-scenario deployment, resource-efficient optimization, and secure semantic communication, highlighting the multifaceted potential of GAI to address emerging challenges in secure physical layer communications and sensing.</li>
</ul>

<h3>Title: ToDo: Token Downsampling for Efficient Generation of High-Resolution  Images</h3>
<ul>
<li><strong>Authors: </strong>Ethan Smith, Nayan Saxena, Aninda Saha</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13573">https://arxiv.org/abs/2402.13573</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13573">https://arxiv.org/pdf/2402.13573</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13573]] ToDo: Token Downsampling for Efficient Generation of High-Resolution  Images(https://arxiv.org/abs/2402.13573)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Attention mechanism has been crucial for image diffusion models, however, their quadratic computational complexity limits the sizes of images we can process within reasonable time and memory constraints. This paper investigates the importance of dense attention in generative image models, which often contain redundant features, making them suitable for sparser attention mechanisms. We propose a novel training-free method ToDo that relies on token downsampling of key and value tokens to accelerate Stable Diffusion inference by up to 2x for common sizes and up to 4.5x or more for high resolutions like 2048x2048. We demonstrate that our approach outperforms previous methods in balancing efficient throughput and fidelity.</li>
</ul>

<h3>Title: Flexible Physical Camouflage Generation Based on a Differential Approach</h3>
<ul>
<li><strong>Authors: </strong>Yang Li, Wenyi Tan, Chenxing Zhao, Shuangju Zhou, Xinkai Liang, Quan Pan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13575">https://arxiv.org/abs/2402.13575</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13575">https://arxiv.org/pdf/2402.13575</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13575]] Flexible Physical Camouflage Generation Based on a Differential Approach(https://arxiv.org/abs/2402.13575)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>This study introduces a novel approach to neural rendering, specifically tailored for adversarial camouflage, within an extensive 3D rendering framework. Our method, named FPA, goes beyond traditional techniques by faithfully simulating lighting conditions and material variations, ensuring a nuanced and realistic representation of textures on a 3D target. To achieve this, we employ a generative approach that learns adversarial patterns from a diffusion model. This involves incorporating a specially designed adversarial loss and covert constraint loss to guarantee the adversarial and covert nature of the camouflage in the physical world. Furthermore, we showcase the effectiveness of the proposed camouflage in sticker mode, demonstrating its ability to cover the target without compromising adversarial information. Through empirical and physical experiments, FPA exhibits strong performance in terms of attack success rate and transferability. Additionally, the designed sticker-mode camouflage, coupled with a concealment constraint, adapts to the environment, yielding diverse styles of texture. Our findings highlight the versatility and efficacy of the FPA approach in adversarial camouflage applications.</li>
</ul>

<h3>Title: LongWanjuan: Towards Systematic Measurement for Long Text Quality</h3>
<ul>
<li><strong>Authors: </strong>Kai Lv, Xiaoran Liu, Qipeng Guo, Hang Yan, Conghui He, Xipeng Qiu, Dahua Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13583">https://arxiv.org/abs/2402.13583</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13583">https://arxiv.org/pdf/2402.13583</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13583]] LongWanjuan: Towards Systematic Measurement for Long Text Quality(https://arxiv.org/abs/2402.13583)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>The quality of training data are crucial for enhancing the long-text capabilities of foundation models. Despite existing efforts to refine data quality through heuristic rules and evaluations based on data diversity and difficulty, there's a lack of systematic approaches specifically tailored for assessing long texts. Addressing this gap, our work systematically measures the quality of long texts by evaluating three fundamental linguistic dimensions: coherence, cohesion, and complexity. Drawing inspiration from the aforementioned three dimensions, we introduce a suite of metrics designed to evaluate the quality of long texts, encompassing both statistical and pre-trained language model-based ones. Leveraging these metrics, we present LongWanjuan, a bilingual dataset specifically tailored to enhance the training of language models for long-text tasks with over 160B tokens. In LongWanjuan, we categorize long texts into holistic, aggregated, and chaotic types, enabling a detailed analysis of long-text quality. Furthermore, we devise a data mixture recipe that strategically balances different types of long texts within LongWanjuan, leading to significant improvements in model performance on long-text tasks. The code and dataset are available at https://github.com/OpenLMLab/LongWanjuan.</li>
</ul>

<h3>Title: A Multimodal In-Context Tuning Approach for E-Commerce Product  Description Generation</h3>
<ul>
<li><strong>Authors: </strong>Yunxin Li, Baotian Hu, Wenhan Luo, Lin Ma, Yuxin Ding, Min Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13587">https://arxiv.org/abs/2402.13587</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13587">https://arxiv.org/pdf/2402.13587</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13587]] A Multimodal In-Context Tuning Approach for E-Commerce Product  Description Generation(https://arxiv.org/abs/2402.13587)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>In this paper, we propose a new setting for generating product descriptions from images, augmented by marketing keywords. It leverages the combined power of visual and textual information to create descriptions that are more tailored to the unique features of products. For this setting, previous methods utilize visual and textual encoders to encode the image and keywords and employ a language model-based decoder to generate the product description. However, the generated description is often inaccurate and generic since same-category products have similar copy-writings, and optimizing the overall framework on large-scale samples makes models concentrate on common words yet ignore the product features. To alleviate the issue, we present a simple and effective Multimodal In-Context Tuning approach, named ModICT, which introduces a similar product sample as the reference and utilizes the in-context learning capability of language models to produce the description. During training, we keep the visual encoder and language model frozen, focusing on optimizing the modules responsible for creating multimodal in-context references and dynamic prompts. This approach preserves the language generation prowess of large language models (LLMs), facilitating a substantial increase in description diversity. To assess the effectiveness of ModICT across various language model scales and types, we collect data from three distinct product categories within the E-commerce domain. Extensive experiments demonstrate that ModICT significantly improves the accuracy (by up to 3.3% on Rouge-L) and diversity (by up to 9.4% on D-5) of generated results compared to conventional methods. Our findings underscore the potential of ModICT as a valuable tool for enhancing automatic generation of product descriptions in a wide range of applications.</li>
</ul>

<h3>Title: User-LLM: Efficient LLM Contextualization with User Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Lin Ning, Luyang Liu, Jiaxing Wu, Neo Wu, Devora Berlowitz, Sushant Prakash, Bradley Green, Shawn O'Banion, Jun Xie</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13598">https://arxiv.org/abs/2402.13598</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13598">https://arxiv.org/pdf/2402.13598</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13598]] User-LLM: Efficient LLM Contextualization with User Embeddings(https://arxiv.org/abs/2402.13598)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have revolutionized natural language processing. However, effectively incorporating complex and potentially noisy user interaction data remains a challenge. To address this, we propose User-LLM, a novel framework that leverages user embeddings to contextualize LLMs. These embeddings, distilled from diverse user interactions using self-supervised pretraining, capture latent user preferences and their evolution over time. We integrate these user embeddings with LLMs through cross-attention and soft-prompting, enabling LLMs to dynamically adapt to user context. Our comprehensive experiments on MovieLens, Amazon Review, and Google Local Review datasets demonstrate significant performance gains across various tasks. Notably, our approach outperforms text-prompt-based contextualization on long sequence tasks and tasks that require deep user understanding while being computationally efficient. We further incorporate Perceiver layers to streamline the integration between user encoders and LLMs, reducing computational demands.</li>
</ul>

<h3>Title: Data-driven Discovery with Large Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Bodhisattwa Prasad Majumder, Harshit Surana, Dhruv Agarwal, Sanchaita Hazra, Ashish Sabharwal, Peter Clark</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13610">https://arxiv.org/abs/2402.13610</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13610">https://arxiv.org/pdf/2402.13610</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13610]] Data-driven Discovery with Large Generative Models(https://arxiv.org/abs/2402.13610)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>With the accumulation of data at an unprecedented rate, its potential to fuel scientific discovery is growing exponentially. This position paper urges the Machine Learning (ML) community to exploit the capabilities of large generative models (LGMs) to develop automated systems for end-to-end data-driven discovery -- a paradigm encompassing the search and verification of hypotheses purely from a set of provided datasets, without the need for additional data collection or physical experiments. We first outline several desiderata for an ideal data-driven discovery system. Then, through DATAVOYAGER, a proof-of-concept utilizing GPT-4, we demonstrate how LGMs fulfill several of these desiderata -- a feat previously unattainable -- while also highlighting important limitations in the current system that open up opportunities for novel ML research. We contend that achieving accurate, reliable, and robust end-to-end discovery systems solely through the current capabilities of LGMs is challenging. We instead advocate for fail-proof tool integration, along with active user moderation through feedback mechanisms, to foster data-driven scientific discoveries with efficiency and reproducibility.</li>
</ul>

<h3>Title: FLAME: Self-Supervised Low-Resource Taxonomy Expansion using Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sahil Mishra, Ujjwal Sudev, Tanmoy Chakraborty</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13623">https://arxiv.org/abs/2402.13623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13623">https://arxiv.org/pdf/2402.13623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13623]] FLAME: Self-Supervised Low-Resource Taxonomy Expansion using Large  Language Models(https://arxiv.org/abs/2402.13623)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Taxonomies represent an arborescence hierarchical structure that establishes relationships among entities to convey knowledge within a specific domain. Each edge in the taxonomy signifies a hypernym-hyponym relationship. Taxonomies find utility in various real-world applications, such as e-commerce search engines and recommendation systems. Consequently, there arises a necessity to enhance these taxonomies over time. However, manually curating taxonomies with neoteric data presents challenges due to limitations in available human resources and the exponential growth of data. Therefore, it becomes imperative to develop automatic taxonomy expansion methods. Traditional supervised taxonomy expansion approaches encounter difficulties stemming from limited resources, primarily due to the small size of existing taxonomies. This scarcity of training data often leads to overfitting. In this paper, we propose FLAME, a novel approach for taxonomy expansion in low-resource environments by harnessing the capabilities of large language models that are trained on extensive real-world knowledge. LLMs help compensate for the scarcity of domain-specific knowledge. Specifically, FLAME leverages prompting in few-shot settings to extract the inherent knowledge within the LLMs, ascertaining the hypernym entities within the taxonomy. Furthermore, it employs reinforcement learning to fine-tune the large language models, resulting in more accurate predictions. Experiments on three real-world benchmark datasets demonstrate the effectiveness of FLAME in real-world scenarios, achieving a remarkable improvement of 18.5% in accuracy and 12.3% in Wu & Palmer metric over eight baselines. Furthermore, we elucidate the strengths and weaknesses of FLAME through an extensive case study, error analysis and ablation studies on the benchmarks.</li>
</ul>

<h3>Title: MORE: Multi-mOdal REtrieval Augmented Generative Commonsense Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Wanqing Cui, Keping Bi, Jiafeng Guo, Xueqi Cheng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13625">https://arxiv.org/abs/2402.13625</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13625">https://arxiv.org/pdf/2402.13625</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13625]] MORE: Multi-mOdal REtrieval Augmented Generative Commonsense Reasoning(https://arxiv.org/abs/2402.13625)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Since commonsense information has been recorded significantly less frequently than its existence, language models pre-trained by text generation have difficulty to learn sufficient commonsense knowledge. Several studies have leveraged text retrieval to augment the models' commonsense ability. Unlike text, images capture commonsense information inherently but little effort has been paid to effectively utilize them. In this work, we propose a novel Multi-mOdal REtrieval (MORE) augmentation framework, to leverage both text and images to enhance the commonsense ability of language models. Extensive experiments on the Common-Gen task have demonstrated the efficacy of MORE based on the pre-trained models of both single and multiple modalities.</li>
</ul>

<h3>Title: UniGraph: Learning a Cross-Domain Graph Foundation Model From Natural  Language</h3>
<ul>
<li><strong>Authors: </strong>Yufei He, Bryan Hooi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13630">https://arxiv.org/abs/2402.13630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13630">https://arxiv.org/pdf/2402.13630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13630]] UniGraph: Learning a Cross-Domain Graph Foundation Model From Natural  Language(https://arxiv.org/abs/2402.13630)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model, in-context</a></li>
<li><strong>Abstract: </strong>Foundation models like ChatGPT and GPT-4 have revolutionized artificial intelligence, exhibiting remarkable abilities to generalize across a wide array of tasks and applications beyond their initial training objectives. However, when this concept is applied to graph learning, a stark contrast emerges. Graph learning has predominantly focused on single-graph models, tailored to specific tasks or datasets, lacking the ability to transfer learned knowledge to different domains. This limitation stems from the inherent complexity and diversity of graph structures, along with the different feature and label spaces specific to graph data. In this paper, we present our UniGraph framework, designed to train a graph foundation model capable of generalizing to unseen graphs and tasks across diverse domains. Unlike single-graph models that use pre-computed node features of varying dimensions as input, our approach leverages Text-Attributed Graphs (TAGs) for unifying node representations. We propose a cascaded architecture of Language Models (LMs) and Graph Neural Networks (GNNs) as backbone networks with a self-supervised training objective based on Masked Graph Modeling (MGM). We introduce graph instruction tuning using Large Language Models (LLMs) to enable zero-shot prediction ability. Our comprehensive experiments across various graph learning tasks and domains demonstrate the model's effectiveness in self-supervised representation learning on unseen graphs, few-shot in-context transfer, and zero-shot transfer, even surpassing or matching the performance of GNNs that have undergone supervised training on target datasets.</li>
</ul>

<h3>Title: Unsupervised Text Style Transfer via LLMs and Attention Masking with  Multi-way Interactions</h3>
<ul>
<li><strong>Authors: </strong>Lei Pan, Yunshi Lan, Yang Li, Weining Qian</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13647">https://arxiv.org/abs/2402.13647</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13647">https://arxiv.org/pdf/2402.13647</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13647]] Unsupervised Text Style Transfer via LLMs and Attention Masking with  Multi-way Interactions(https://arxiv.org/abs/2402.13647)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Unsupervised Text Style Transfer (UTST) has emerged as a critical task within the domain of Natural Language Processing (NLP), aiming to transfer one stylistic aspect of a sentence into another style without changing its semantics, syntax, or other attributes. This task is especially challenging given the intrinsic lack of parallel text pairings. Among existing methods for UTST tasks, attention masking approach and Large Language Models (LLMs) are deemed as two pioneering methods. However, they have shortcomings in generating unsmooth sentences and changing the original contents, respectively. In this paper, we investigate if we can combine these two methods effectively. We propose four ways of interactions, that are pipeline framework with tuned orders; knowledge distillation from LLMs to attention masking model; in-context learning with constructed parallel examples. We empirically show these multi-way interactions can improve the baselines in certain perspective of style strength, content preservation and text fluency. Experiments also demonstrate that simply conducting prompting followed by attention masking-based revision can consistently surpass the other systems, including supervised text style transfer systems. On Yelp-clean and Amazon-clean datasets, it improves the previously best mean metric by 0.5 and 3.0 absolute percentages respectively, and achieves new SOTA results.</li>
</ul>

<h3>Title: Hybrid Video Diffusion Models with 2D Triplane and 3D Wavelet  Representation</h3>
<ul>
<li><strong>Authors: </strong>Kihong Kim, Haneol Lee, Jihye Park, Seyeon Kim, Kwanghee Lee, Seungryong Kim, Jaejun Yoo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13729">https://arxiv.org/abs/2402.13729</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13729">https://arxiv.org/pdf/2402.13729</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13729]] Hybrid Video Diffusion Models with 2D Triplane and 3D Wavelet  Representation(https://arxiv.org/abs/2402.13729)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Generating high-quality videos that synthesize desired realistic content is a challenging task due to their intricate high-dimensionality and complexity of videos. Several recent diffusion-based methods have shown comparable performance by compressing videos to a lower-dimensional latent space, using traditional video autoencoder architecture. However, such method that employ standard frame-wise 2D and 3D convolution fail to fully exploit the spatio-temporal nature of videos. To address this issue, we propose a novel hybrid video diffusion model, called HVDM, which can capture spatio-temporal dependencies more effectively. The HVDM is trained by a hybrid video autoencoder which extracts a disentangled representation of the video including: (i) a global context information captured by a 2D projected latent (ii) a local volume information captured by 3D convolutions with wavelet decomposition (iii) a frequency information for improving the video reconstruction. Based on this disentangled representation, our hybrid autoencoder provide a more comprehensive video latent enriching the generated videos with fine structures and details. Experiments on video generation benchamarks (UCF101, SkyTimelapse, and TaiChi) demonstrate that the proposed approach achieves state-of-the-art video generation quality, showing a wide range of video applications (e.g., long video generation, image-to-video, and video dynamics control).</li>
</ul>

<h3>Title: SRNDiff: Short-term Rainfall Nowcasting with Condition Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Xudong Ling, Chaorong Li, Fengqing Qin, Peng Yang, Yuanyuan Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13737">https://arxiv.org/abs/2402.13737</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13737">https://arxiv.org/pdf/2402.13737</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13737]] SRNDiff: Short-term Rainfall Nowcasting with Condition Diffusion Model(https://arxiv.org/abs/2402.13737)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models are widely used in image generation because they can generate high-quality and realistic samples. This is in contrast to generative adversarial networks (GANs) and variational autoencoders (VAEs), which have some limitations in terms of image quality.We introduce the diffusion model to the precipitation forecasting task and propose a short-term precipitation nowcasting with condition diffusion model based on historical observational data, which is referred to as SRNDiff. By incorporating an additional conditional decoder module in the denoising process, SRNDiff achieves end-to-end conditional rainfall prediction. SRNDiff is composed of two networks: a denoising network and a conditional Encoder network. The conditional network is composed of multiple independent UNet networks. These networks extract conditional feature maps at different resolutions, providing accurate conditional information that guides the diffusion model for conditional generation.SRNDiff surpasses GANs in terms of prediction accuracy, although it requires more computational resources.The SRNDiff model exhibits higher stability and efficiency during training than GANs-based approaches, and generates high-quality precipitation distribution samples that better reflect future actual precipitation conditions. This fully validates the advantages and potential of diffusion models in precipitation forecasting, providing new insights for enhancing rainfall prediction.</li>
</ul>

<h3>Title: Unlocking Instructive In-Context Learning with Tabular Prompting for  Relational Triple Extraction</h3>
<ul>
<li><strong>Authors: </strong>Guozheng Li, Wenjun Ke, Peng Wang, Zijie Xu, Ke Ji, Jiajun Liu, Ziyu Shang, Qiqing Luo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13741">https://arxiv.org/abs/2402.13741</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13741">https://arxiv.org/pdf/2402.13741</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13741]] Unlocking Instructive In-Context Learning with Tabular Prompting for  Relational Triple Extraction(https://arxiv.org/abs/2402.13741)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>The in-context learning (ICL) for relational triple extraction (RTE) has achieved promising performance, but still encounters two key challenges: (1) how to design effective prompts and (2) how to select proper demonstrations. Existing methods, however, fail to address these challenges appropriately. On the one hand, they usually recast RTE task to text-to-text prompting formats, which is unnatural and results in a mismatch between the output format at the pre-training time and the inference time for large language models (LLMs). On the other hand, they only utilize surface natural language features and lack consideration of triple semantics in sample selection. These issues are blocking improved performance in ICL for RTE, thus we aim to tackle prompt designing and sample selection challenges simultaneously. To this end, we devise a tabular prompting for RTE (\textsc{TableIE}) which frames RTE task into a table generation task to incorporate explicit structured information into ICL, facilitating conversion of outputs to RTE structures. Then we propose instructive in-context learning (I$^2$CL) which only selects and annotates a few samples considering internal triple semantics in massive unlabeled samples.</li>
</ul>

<h3>Title: Deep Generative Models for Offline Policy Learning: Tutorial, Survey,  and Perspectives on Future Directions</h3>
<ul>
<li><strong>Authors: </strong>Jiayu Chen, Bhargav Ganguly, Yang Xu, Yongsheng Mei, Tian Lan, Vaneet Aggarwal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13777">https://arxiv.org/abs/2402.13777</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13777">https://arxiv.org/pdf/2402.13777</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13777]] Deep Generative Models for Offline Policy Learning: Tutorial, Survey,  and Perspectives on Future Directions(https://arxiv.org/abs/2402.13777)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Deep generative models (DGMs) have demonstrated great success across various domains, particularly in generating texts, images, and videos using models trained from offline data. Similarly, data-driven decision-making and robotic control also necessitate learning a generator function from the offline data to serve as the strategy or policy. In this case, applying deep generative models in offline policy learning exhibits great potential, and numerous studies have explored in this direction. However, this field still lacks a comprehensive review and so developments of different branches are relatively independent. Thus, we provide the first systematic review on the applications of deep generative models for offline policy learning. In particular, we cover five mainstream deep generative models, including Variational Auto-Encoders, Generative Adversarial Networks, Normalizing Flows, Transformers, and Diffusion Models, and their applications in both offline reinforcement learning (offline RL) and imitation learning (IL). Offline RL and IL are two main branches of offline policy learning and are widely-adopted techniques for sequential decision-making. Specifically, for each type of DGM-based offline policy learning, we distill its fundamental scheme, categorize related works based on the usage of the DGM, and sort out the development process of algorithms in that field. Subsequent to the main content, we provide in-depth discussions on deep generative models and offline policy learning as a summary, based on which we present our perspectives on future research directions. This work offers a hands-on reference for the research progress in deep generative models for offline policy learning, and aims to inspire improved DGM-based offline RL or IL algorithms.</li>
</ul>

<h3>Title: Contextual Molecule Representation Learning from Chemical Reaction  Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Han Tang, Shikun Feng, Bicheng Lin, Yuyan Ni, JIngjing Liu, Wei-Ying Ma, Yanyan Lan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13779">https://arxiv.org/abs/2402.13779</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13779">https://arxiv.org/pdf/2402.13779</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13779]] Contextual Molecule Representation Learning from Chemical Reaction  Knowledge(https://arxiv.org/abs/2402.13779)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>In recent years, self-supervised learning has emerged as a powerful tool to harness abundant unlabelled data for representation learning and has been broadly adopted in diverse areas. However, when applied to molecular representation learning (MRL), prevailing techniques such as masked sub-unit reconstruction often fall short, due to the high degree of freedom in the possible combinations of atoms within molecules, which brings insurmountable complexity to the masking-reconstruction paradigm. To tackle this challenge, we introduce REMO, a self-supervised learning framework that takes advantage of well-defined atom-combination rules in common chemistry. Specifically, REMO pre-trains graph/Transformer encoders on 1.7 million known chemical reactions in the literature. We propose two pre-training objectives: Masked Reaction Centre Reconstruction (MRCR) and Reaction Centre Identification (RCI). REMO offers a novel solution to MRL by exploiting the underlying shared patterns in chemical reactions as \textit{context} for pre-training, which effectively infers meaningful representations of common chemistry knowledge. Such contextual representations can then be utilized to support diverse downstream molecular tasks with minimum finetuning, such as affinity prediction and drug-drug interaction prediction. Extensive experimental results on MoleculeACE, ACNet, drug-drug interaction (DDI), and reaction type classification show that across all tested downstream tasks, REMO outperforms the standard baseline of single-molecule masked modeling used in current MRL. Remarkably, REMO is the pioneering deep learning model surpassing fingerprint-based methods in activity cliff benchmarks.</li>
</ul>

<h3>Title: The Geography of Information Diffusion in Online Discourse on Europe and  Migration</h3>
<ul>
<li><strong>Authors: </strong>Elisa Leonardelli, Sara Tonelli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13800">https://arxiv.org/abs/2402.13800</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13800">https://arxiv.org/pdf/2402.13800</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13800]] The Geography of Information Diffusion in Online Discourse on Europe and  Migration(https://arxiv.org/abs/2402.13800)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The online diffusion of information related to Europe and migration has been little investigated from an external point of view. However, this is a very relevant topic, especially if users have had no direct contact with Europe and its perception depends solely on information retrieved online. In this work we analyse the information circulating online about Europe and migration after retrieving a large amount of data from social media (Twitter), to gain new insights into topics, magnitude, and dynamics of their diffusion. We combine retweets and hashtags network analysis with geolocation of users, linking thus data to geography and allowing analysis from an "outside Europe" perspective, with a special focus on Africa. We also introduce a novel approach based on cross-lingual quotes, i.e. when content in a language is commented and retweeted in another language, assuming these interactions are a proxy for connections between very distant communities. Results show how the majority of online discussions occurs at a national level, especially when discussing migration. Language (English) is pivotal for information to become transnational and reach far. Transnational information flow is strongly unbalanced, with content mainly produced in Europe and amplified outside. Conversely Europe-based accounts tend to be self-referential when they discuss migration-related topics. Football is the most exported topic from Europe worldwide. Moreover, important nodes in the communities discussing migration-related topics include accounts of official institutions and international agencies, together with journalists, news, commentators and activists.</li>
</ul>

<h3>Title: FLD: Fourier Latent Dynamics for Structured Motion Representation and  Learning</h3>
<ul>
<li><strong>Authors: </strong>Chenhao Li, Elijah Stanger-Jones, Steve Heim, Sangbae Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.RO, eess.SP, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13820">https://arxiv.org/abs/2402.13820</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13820">https://arxiv.org/pdf/2402.13820</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13820]] FLD: Fourier Latent Dynamics for Structured Motion Representation and  Learning(https://arxiv.org/abs/2402.13820)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Motion trajectories offer reliable references for physics-based motion learning but suffer from sparsity, particularly in regions that lack sufficient data coverage. To address this challenge, we introduce a self-supervised, structured representation and generation method that extracts spatial-temporal relationships in periodic or quasi-periodic motions. The motion dynamics in a continuously parameterized latent space enable our method to enhance the interpolation and generalization capabilities of motion learning algorithms. The motion learning controller, informed by the motion parameterization, operates online tracking of a wide range of motions, including targets unseen during training. With a fallback mechanism, the controller dynamically adapts its tracking strategy and automatically resorts to safe action execution when a potentially risky target is proposed. By leveraging the identified spatial-temporal structure, our work opens new possibilities for future advancements in general motion representation and learning algorithms.</li>
</ul>

<h3>Title: VL-Trojan: Multimodal Instruction Backdoor Attacks against  Autoregressive Visual Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jiawei Liang, Siyuan Liang, Man Luo, Aishan Liu, Dongchen Han, Ee-Chien Chang, Xiaochun Cao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13851">https://arxiv.org/abs/2402.13851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13851">https://arxiv.org/pdf/2402.13851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13851]] VL-Trojan: Multimodal Instruction Backdoor Attacks against  Autoregressive Visual Language Models(https://arxiv.org/abs/2402.13851)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Autoregressive Visual Language Models (VLMs) showcase impressive few-shot learning capabilities in a multimodal context. Recently, multimodal instruction tuning has been proposed to further enhance instruction-following abilities. However, we uncover the potential threat posed by backdoor attacks on autoregressive VLMs during instruction tuning. Adversaries can implant a backdoor by injecting poisoned samples with triggers embedded in instructions or images, enabling malicious manipulation of the victim model's predictions with predefined triggers. Nevertheless, the frozen visual encoder in autoregressive VLMs imposes constraints on the learning of conventional image triggers. Additionally, adversaries may encounter restrictions in accessing the parameters and architectures of the victim model. To address these challenges, we propose a multimodal instruction backdoor attack, namely VL-Trojan. Our approach facilitates image trigger learning through an isolating and clustering strategy and enhance black-box-attack efficacy via an iterative character-level text trigger generation method. Our attack successfully induces target outputs during inference, significantly surpassing baselines (+62.52\%) in ASR. Moreover, it demonstrates robustness across various model scales and few-shot in-context reasoning scenarios.</li>
</ul>

<h3>Title: Generative Probabilistic Time Series Forecasting and Applications in  Grid Operations</h3>
<ul>
<li><strong>Authors: </strong>Xinyi Wang, Lang Tong, Qing Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP, stat.AP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13870">https://arxiv.org/abs/2402.13870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13870">https://arxiv.org/pdf/2402.13870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13870]] Generative Probabilistic Time Series Forecasting and Applications in  Grid Operations(https://arxiv.org/abs/2402.13870)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative probabilistic forecasting produces future time series samples according to the conditional probability distribution given past time series observations. Such techniques are essential in risk-based decision-making and planning under uncertainty with broad applications in grid operations, including electricity price forecasting, risk-based economic dispatch, and stochastic optimizations. Inspired by Wiener and Kallianpur's innovation representation, we propose a weak innovation autoencoder architecture and a learning algorithm to extract independent and identically distributed innovation sequences from nonparametric stationary time series. We show that the weak innovation sequence is Bayesian sufficient, which makes the proposed weak innovation autoencoder a canonical architecture for generative probabilistic forecasting. The proposed technique is applied to forecasting highly volatile real-time electricity prices, demonstrating superior performance across multiple forecasting measures over leading probabilistic and point forecasting techniques.</li>
</ul>

<h3>Title: $\texttt{Se}^2$: $\textit{Se}$quential Example $\textit{Se}$lection for  In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Haoyu Liu, Jianfeng Liu, Shaohan Huang, Yuefeng Zhan, Hao Sun, Weiwei Deng, Furu Wei, Qi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13874">https://arxiv.org/abs/2402.13874</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13874">https://arxiv.org/pdf/2402.13874</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13874]] $\texttt{Se}^2$: $\textit{Se}$quential Example $\textit{Se}$lection for  In-Context Learning(https://arxiv.org/abs/2402.13874)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>The remarkable capability of large language models (LLMs) for in-context learning (ICL) needs to be activated by demonstration examples. Prior work has extensively explored the selection of examples for ICL, predominantly following the "select then organize" paradigm, such approaches often neglect the internal relationships between examples and exist an inconsistency between the training and inference. In this paper, we formulate the problem as a $\textit{se}$quential $\textit{se}$lection problem and introduce $\texttt{Se}^2$, a sequential-aware method that leverages the LLM's feedback on varying context, aiding in capturing inter-relationships and sequential information among examples, significantly enriching the contextuality and relevance of ICL prompts. Meanwhile, we utilize beam search to seek and construct example sequences, enhancing both quality and diversity. Extensive experiments across 23 NLP tasks from 8 distinct categories illustrate that $\texttt{Se}^2$ markedly surpasses competitive baselines and achieves 42% relative improvement over random selection. Further in-depth analysis show the effectiveness of proposed strategies, highlighting $\texttt{Se}^2$'s exceptional stability and adaptability across various scenarios. Our code will be released to facilitate future research.</li>
</ul>

<h3>Title: Beyond Probabilities: Unveiling the Misalignment in Evaluating Large  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chenyang Lyu, Minghao Wu, Alham Fikri Aji</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13887">https://arxiv.org/abs/2402.13887</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13887">https://arxiv.org/pdf/2402.13887</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13887]] Beyond Probabilities: Unveiling the Misalignment in Evaluating Large  Language Models(https://arxiv.org/abs/2402.13887)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capabilities across various applications, fundamentally reshaping the landscape of natural language processing (NLP) research. However, recent evaluation frameworks often rely on the output probabilities of LLMs for predictions, primarily due to computational constraints, diverging from real-world LLM usage scenarios. While widely employed, the efficacy of these probability-based evaluation strategies remains an open research question. This study aims to scrutinize the validity of such probability-based evaluation methods within the context of using LLMs for Multiple Choice Questions (MCQs), highlighting their inherent limitations. Our empirical investigation reveals that the prevalent probability-based evaluation method inadequately aligns with generation-based prediction. Furthermore, current evaluation frameworks typically assess LLMs through predictive tasks based on output probabilities rather than directly generating responses, owing to computational limitations. We illustrate that these probability-based approaches do not effectively correspond with generative predictions. The outcomes of our study can enhance the understanding of LLM evaluation methodologies and provide insights for future research in this domain.</li>
</ul>

<h3>Title: Non-asymptotic Convergence of Discrete-time Diffusion Models: New  Approach and Improved Rate</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Liang, Peizhong Ju, Yingbin Liang, Ness Shroff</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SP, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13901">https://arxiv.org/abs/2402.13901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13901">https://arxiv.org/pdf/2402.13901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13901]] Non-asymptotic Convergence of Discrete-time Diffusion Models: New  Approach and Improved Rate(https://arxiv.org/abs/2402.13901)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The denoising diffusion model emerges recently as a powerful generative technique that converts noise into data. Theoretical convergence guarantee has been mainly studied for continuous-time diffusion models, and has been obtained for discrete-time diffusion models only for distributions with bounded support in the literature. In this paper, we establish the convergence guarantee for substantially larger classes of distributions under discrete-time diffusion models and further improve the convergence rate for distributions with bounded support. In particular, we first establish the convergence rates for both smooth and general (possibly non-smooth) distributions having finite second moment. We then specialize our results to a number of interesting classes of distributions with explicit parameter dependencies, including distributions with Lipschitz scores, Gaussian mixture distributions, and distributions with bounded support. We further propose a novel accelerated sampler and show that it improves the convergence rates of the corresponding regular sampler by orders of magnitude with respect to all system parameters. For distributions with bounded support, our result improves the dimensional dependence of the previous convergence rate by orders of magnitude. Our study features a novel analysis technique that constructs tilting factor representation of the convergence error and exploits Tweedie's formula for handling Taylor expansion power terms.</li>
</ul>

<h3>Title: SDXL-Lightning: Progressive Adversarial Diffusion Distillation</h3>
<ul>
<li><strong>Authors: </strong>Shanchuan Lin, Anran Wang, Xiao Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13929">https://arxiv.org/abs/2402.13929</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13929">https://arxiv.org/pdf/2402.13929</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13929]] SDXL-Lightning: Progressive Adversarial Diffusion Distillation(https://arxiv.org/abs/2402.13929)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We propose a diffusion distillation method that achieves new state-of-the-art in one-step/few-step 1024px text-to-image generation based on SDXL. Our method combines progressive and adversarial distillation to achieve a balance between quality and mode coverage. In this paper, we discuss the theoretical analysis, discriminator design, model formulation, and training techniques. We open-source our distilled SDXL-Lightning models both as LoRA and full UNet weights.</li>
</ul>

<h3>Title: Analysing The Impact of Sequence Composition on Language Model  Pre-Training</h3>
<ul>
<li><strong>Authors: </strong>Yu Zhao, Yuanbin Qu, Konrad Staniszewski, Szymon Tworkowski, Wei Liu, Piotr Miłoś, Yuxiang Wu, Pasquale Minervini</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.13991">https://arxiv.org/abs/2402.13991</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.13991">https://arxiv.org/pdf/2402.13991</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.13991]] Analysing The Impact of Sequence Composition on Language Model  Pre-Training(https://arxiv.org/abs/2402.13991)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Most language model pre-training frameworks concatenate multiple documents into fixed-length sequences and use causal masking to compute the likelihood of each token given its context; this strategy is widely adopted due to its simplicity and efficiency. However, to this day, the influence of the pre-training sequence composition strategy on the generalisation properties of the model remains under-explored. In this work, we find that applying causal masking can lead to the inclusion of distracting information from previous documents during pre-training, which negatively impacts the performance of the models on language modelling and downstream tasks. In intra-document causal masking, the likelihood of each token is only conditioned on the previous tokens in the same document, eliminating potential distracting information from previous documents and significantly improving performance. Furthermore, we find that concatenating related documents can reduce some potential distractions during pre-training, and our proposed efficient retrieval-based sequence construction method, BM25Chunk, can improve in-context learning (+11.6\%), knowledge memorisation (+9.8\%), and context utilisation (+7.2\%) abilities of language models without sacrificing efficiency.</li>
</ul>

<h3>Title: Geometry-Informed Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Arturs Berzins, Andreas Radler, Sebastian Sanokowski, Sepp Hochreiter, Johannes Brandstetter</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14009">https://arxiv.org/abs/2402.14009</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14009">https://arxiv.org/pdf/2402.14009</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14009]] Geometry-Informed Neural Networks(https://arxiv.org/abs/2402.14009)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce the concept of geometry-informed neural networks (GINNs), which encompass (i) learning under geometric constraints, (ii) neural fields as a suitable representation, and (iii) generating diverse solutions to under-determined systems often encountered in geometric tasks. Notably, the GINN formulation does not require training data, and as such can be considered generative modeling driven purely by constraints. We add an explicit diversity loss to mitigate mode collapse. We consider several constraints, in particular, the connectedness of components which we convert to a differentiable loss through Morse theory. Experimentally, we demonstrate the efficacy of the GINN learning paradigm across a range of two and three-dimensional scenarios with increasing levels of complexity.</li>
</ul>

<h3>Title: D-Flow: Differentiating through Flows for Controlled Generation</h3>
<ul>
<li><strong>Authors: </strong>Heli Ben-Hamu, Omri Puny, Itai Gat, Brian Karrer, Uriel Singer, Yaron Lipman</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14017">https://arxiv.org/abs/2402.14017</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14017">https://arxiv.org/pdf/2402.14017</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14017]] D-Flow: Differentiating through Flows for Controlled Generation(https://arxiv.org/abs/2402.14017)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Taming the generation outcome of state of the art Diffusion and Flow-Matching (FM) models without having to re-train a task-specific model unlocks a powerful tool for solving inverse problems, conditional generation, and controlled generation in general. In this work we introduce D-Flow, a simple framework for controlling the generation process by differentiating through the flow, optimizing for the source (noise) point. We motivate this framework by our key observation stating that for Diffusion/FM models trained with Gaussian probability paths, differentiating through the generation process projects gradient on the data manifold, implicitly injecting the prior into the optimization process. We validate our framework on linear and non-linear controlled generation problems including: image and audio inverse problems and conditional molecule generation reaching state of the art performance across all.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
