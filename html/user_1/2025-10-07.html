<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-10-07</h1>
<h3>Title: VIFO: Visual Feature Empowered Multivariate Time Series Forecasting with Cross-Modal Fusion</h3>
<ul>
<li><strong>Authors: </strong>Yanlong Wang, Hang Yu, Jian Xu, Fei Ma, Hongkang Zhang, Tongtong Feng, Zijian Zhang, Shao-Lun Huang, Danny Dongning Sun, Xiao-Ping Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03244">https://arxiv.org/abs/2510.03244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03244">https://arxiv.org/pdf/2510.03244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03244]] VIFO: Visual Feature Empowered Multivariate Time Series Forecasting with Cross-Modal Fusion(https://arxiv.org/abs/2510.03244)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Large time series foundation models often adopt channel-independent architectures to handle varying data dimensions, but this design ignores crucial cross-channel dependencies. Concurrently, existing multimodal approaches have not fully exploited the power of large vision models (LVMs) to interpret spatiotemporal data. Additionally, there remains significant unexplored potential in leveraging the advantages of information extraction from different modalities to enhance time series forecasting performance. To address these gaps, we propose the VIFO, a cross-modal forecasting model. VIFO uniquely renders multivariate time series into image, enabling pre-trained LVM to extract complex cross-channel patterns that are invisible to channel-independent models. These visual features are then aligned and fused with representations from the time series modality. By freezing the LVM and training only 7.45% of its parameters, VIFO achieves competitive performance on multiple benchmarks, offering an efficient and effective solution for capturing cross-variable relationships in</li>
</ul>

<h3>Title: Universal Multi-Domain Translation via Diffusion Routers</h3>
<ul>
<li><strong>Authors: </strong>Duc Kieu, Kien Do, Tuan Hoang, Thao Minh Le, Tung Kieu, Dang Nguyen, Thin Nguyen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03252">https://arxiv.org/abs/2510.03252</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03252">https://arxiv.org/pdf/2510.03252</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03252]] Universal Multi-Domain Translation via Diffusion Routers(https://arxiv.org/abs/2510.03252)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Multi-domain translation (MDT) aims to learn translations between multiple domains, yet existing approaches either require fully aligned tuples or can only handle domain pairs seen in training, limiting their practicality and excluding many cross-domain mappings. We introduce universal MDT (UMDT), a generalization of MDT that seeks to translate between any pair of $K$ domains using only $K-1$ paired datasets with a central domain. To tackle this problem, we propose Diffusion Router (DR), a unified diffusion-based framework that models all central$\leftrightarrow$non-central translations with a single noise predictor conditioned on the source and target domain labels. DR enables indirect non-central translations by routing through the central domain. We further introduce a novel scalable learning strategy with a variational-bound objective and an efficient Tweedie refinement procedure to support direct non-central mappings. Through evaluation on three large-scale UMDT benchmarks, DR achieves state-of-the-art results for both indirect and direct translations, while lowering sampling cost and unlocking novel tasks such as sketch$\leftrightarrow$segmentation. These results establish DR as a scalable and versatile framework for universal translation across multiple domains.</li>
</ul>

<h3>Title: Memory Self-Regeneration: Uncovering Hidden Knowledge in Unlearned Models</h3>
<ul>
<li><strong>Authors: </strong>Agnieszka Polowczyk, Alicja Polowczyk, Joanna Waczyńska, Piotr Borycki, Przemysław Spurek</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03263">https://arxiv.org/abs/2510.03263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03263">https://arxiv.org/pdf/2510.03263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03263]] Memory Self-Regeneration: Uncovering Hidden Knowledge in Unlearned Models(https://arxiv.org/abs/2510.03263)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The impressive capability of modern text-to-image models to generate realistic visuals has come with a serious drawback: they can be misused to create harmful, deceptive or unlawful content. This has accelerated the push for machine unlearning. This new field seeks to selectively remove specific knowledge from a model's training data without causing a drop in its overall performance. However, it turns out that actually forgetting a given concept is an extremely difficult task. Models exposed to attacks using adversarial prompts show the ability to generate so-called unlearned concepts, which can be not only harmful but also illegal. In this paper, we present considerations regarding the ability of models to forget and recall knowledge, introducing the Memory Self-Regeneration task. Furthermore, we present MemoRa strategy, which we consider to be a regenerative approach supporting the effective recovery of previously lost knowledge. Moreover, we propose that robustness in knowledge retrieval is a crucial yet underexplored evaluation measure for developing more robust and effective unlearning techniques. Finally, we demonstrate that forgetting occurs in two distinct ways: short-term, where concepts can be quickly recalled, and long-term, where recovery is more challenging.</li>
</ul>

<h3>Title: MindCraft: How Concept Trees Take Shape In Deep Models</h3>
<ul>
<li><strong>Authors: </strong>Bowei Tian, Yexiao He, Wanghao Ye, Ziyao Wang, Meng Liu, Ang Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03265">https://arxiv.org/abs/2510.03265</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03265">https://arxiv.org/pdf/2510.03265</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03265]] MindCraft: How Concept Trees Take Shape In Deep Models(https://arxiv.org/abs/2510.03265)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Large-scale foundation models demonstrate strong performance across language, vision, and reasoning tasks. However, how they internally structure and stabilize concepts remains elusive. Inspired by causal inference, we introduce the MindCraft framework built upon Concept Trees. By applying spectral decomposition at each layer and linking principal directions into branching Concept Paths, Concept Trees reconstruct the hierarchical emergence of concepts, revealing exactly when they diverge from shared representations into linearly separable subspaces. Empirical evaluations across diverse scenarios across disciplines, including medical diagnosis, physics reasoning, and political decision-making, show that Concept Trees recover semantic hierarchies, disentangle latent concepts, and can be widely applied across multiple domains. The Concept Tree establishes a widely applicable and powerful framework that enables in-depth analysis of conceptual representations in deep models, marking a significant step forward in the foundation of interpretable AI.</li>
</ul>

<h3>Title: Variational Autoencoders-based Detection of Extremes in Plant Productivity in an Earth System Model</h3>
<ul>
<li><strong>Authors: </strong>Bharat Sharma, Jitendra Kumar</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ME, stat.OT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03266">https://arxiv.org/abs/2510.03266</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03266">https://arxiv.org/pdf/2510.03266</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03266]] Variational Autoencoders-based Detection of Extremes in Plant Productivity in an Earth System Model(https://arxiv.org/abs/2510.03266)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Climate anomalies significantly impact terrestrial carbon cycle dynamics, necessitating robust methods for detecting and analyzing anomalous behavior in plant productivity. This study presents a novel application of variational autoencoders (VAE) for identifying extreme events in gross primary productivity (GPP) from Community Earth System Model version 2 simulations across four AR6 regions in the Continental United States. We compare VAE-based anomaly detection with traditional singular spectral analysis (SSA) methods across three time periods: 1850-80, 1950-80, and 2050-80 under the SSP585 scenario. The VAE architecture employs three dense layers and a latent space with an input sequence length of 12 months, trained on a normalized GPP time series to reconstruct the GPP and identifying anomalies based on reconstruction errors. Extreme events are defined using 5th percentile thresholds applied to both VAE and SSA anomalies. Results demonstrate strong regional agreement between VAE and SSA methods in spatial patterns of extreme event frequencies, despite VAE producing higher threshold values (179-756 GgC for VAE vs. 100-784 GgC for SSA across regions and periods). Both methods reveal increasing magnitudes and frequencies of negative carbon cycle extremes toward 2050-80, particularly in Western and Central North America. The VAE approach shows comparable performance to established SSA techniques, while offering computational advantages and enhanced capability for capturing non-linear temporal dependencies in carbon cycle variability. Unlike SSA, the VAE method does not require one to define the periodicity of the signals in the data; it discovers them from the data.</li>
</ul>

<h3>Title: CoDA: Coding LM via Diffusion Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Haolin Chen, Shiyu Wang, Can Qin, Bo Pang, Zuxin Liu, Jielin Qiu, Jianguo Zhang, Yingbo Zhou, Zeyuan Chen, Ran Xu, Shelby Heinecke, Silvio Savarese, Caiming Xiong, Huan Wang, Weiran Yao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03270">https://arxiv.org/abs/2510.03270</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03270">https://arxiv.org/pdf/2510.03270</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03270]] CoDA: Coding LM via Diffusion Adaptation(https://arxiv.org/abs/2510.03270)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion language models promise bidirectional context and infilling capabilities that autoregressive coders lack, yet practical systems remain heavyweight. We introduce CoDA, a 1.7B-parameter diffusion coder trained on TPU with a fully open-source training pipeline. CoDA pairs large-scale diffusion pre-training with code-centric mid-training and instruction tuning, enabling confidence-guided sampling that keeps inference latency competitive. On Humaneval, MBPP, and EvalPlus, CoDA-1.7B-Instruct matches or surpasses diffusion models up to 7B parameters. Our release includes model checkpoints, evaluation harnesses, and TPU training pipelines to accelerate research on lightweight diffusion-based coding assistants.</li>
</ul>

<h3>Title: Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Tianao Zhang, Zhiteng Li, Xianglong Yan, Haotong Qin, Yong Guo, Yulun Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03274">https://arxiv.org/abs/2510.03274</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03274">https://arxiv.org/pdf/2510.03274</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03274]] Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models(https://arxiv.org/abs/2510.03274)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion large language models (dLLMs), which offer bidirectional context and flexible masked-denoising generation, are emerging as a compelling alternative to autoregressive (AR) LLMs. However, like AR LLMs, their model sizes continue to grow, motivating weight compression for deployment. Although post-training quantization (PTQ) is effective for AR LLMs, directly transferring it to dLLMs at 2-bit leads to unsatisfactory performance. To tackle these challenges, we propose Quant-dLLM, an ultra-low-bit PTQ framework tailored to dLLMs. Since masked-denoising activations in dLLMs differ from the fully visible signals assumed by standard PTQ methods, we introduce Masked Calibration Simulation (MCS) to align calibration with the timestep-dependent masking, which yields more reliable calibrations. Moreover, we propose a Data-aware Any-order Quantizer (DAQ) that learns ultra-low-bit weight representations via an optimization algorithm. It performs iterative approximation guided by our simulated calibration data. In addition, under a strict 2-bit budget, we introduce Adaptive Blockwise Mixed Precision (ABMP), a sensitivity-based precision allocation scheme that adaptively assigns bit width across channel groups. When restricted to 2-bit precision, Quant-dLLM consistently achieves higher accuracy than state-of-the-art (SOTA) AR-transfer PTQ methods on dLLMs. The code and models will be available at: this https URL.</li>
</ul>

<h3>Title: Training Optimal Large Diffusion Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jinjie Ni, Qian Liu, Chao Du, Longxu Dou, Hang Yan, Zili Wang, Tianyu Pang, Michael Qizhe Shieh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03280">https://arxiv.org/abs/2510.03280</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03280">https://arxiv.org/pdf/2510.03280</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03280]] Training Optimal Large Diffusion Language Models(https://arxiv.org/abs/2510.03280)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce Quokka, the first systematic scaling law for diffusion language models (DLMs), encompassing both compute-constrained and data-constrained regimes, and studying the key modeling and optimization designs. Quokka is a good friend of Chinchilla and provides wider scopes. We hope the results would bring short-term practical guidance in DLMs training and long-term inspirations for the whole AI community.</li>
</ul>

<h3>Title: SoC-DT: Standard-of-Care Aligned Digital Twins for Patient-Specific Tumor Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Moinak Bhattacharya, Gagandeep Singh, Prateek Prasanna</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03287">https://arxiv.org/abs/2510.03287</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03287">https://arxiv.org/pdf/2510.03287</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03287]] SoC-DT: Standard-of-Care Aligned Digital Twins for Patient-Specific Tumor Dynamics(https://arxiv.org/abs/2510.03287)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Accurate prediction of tumor trajectories under standard-of-care (SoC) therapies remains a major unmet need in oncology. This capability is essential for optimizing treatment planning and anticipating disease progression. Conventional reaction-diffusion models are limited in scope, as they fail to capture tumor dynamics under heterogeneous therapeutic paradigms. There is hence a critical need for computational frameworks that can realistically simulate SoC interventions while accounting for inter-patient variability in genomics, demographics, and treatment regimens. We introduce Standard-of-Care Digital Twin (SoC-DT), a differentiable framework that unifies reaction-diffusion tumor growth models, discrete SoC interventions (surgery, chemotherapy, radiotherapy) along with genomic and demographic personalization to predict post-treatment tumor structure on imaging. An implicit-explicit exponential time-differencing solver, IMEX-SoC, is also proposed, which ensures stability, positivity, and scalability in SoC treatment situations. Evaluated on both synthetic data and real world glioma data, SoC-DT consistently outperforms classical PDE baselines and purely data-driven neural models in predicting tumor dynamics. By bridging mechanistic interpretability with modern differentiable solvers, SoC-DT establishes a principled foundation for patient-specific digital twins in oncology, enabling biologically consistent tumor dynamics estimation. Code will be made available upon acceptance.</li>
</ul>

<h3>Title: LogAction: Consistent Cross-system Anomaly Detection through Logs via Active Domain</h3>
<ul>
<li><strong>Authors: </strong>Chiming Duan, Minghua He, Pei Xiao, Tong Jia, Xin Zhang, Zhewei Zhong, Xiang Luo, Yan Niu, Lingzhe Zhang, Yifan Wu, Siyu Yu, Weijie Hong, Ying Li, Gang Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.DC, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03288">https://arxiv.org/abs/2510.03288</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03288">https://arxiv.org/pdf/2510.03288</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03288]] LogAction: Consistent Cross-system Anomaly Detection through Logs via Active Domain(https://arxiv.org/abs/2510.03288)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Log-based anomaly detection is a essential task for ensuring the reliability and performance of software systems. However, the performance of existing anomaly detection methods heavily relies on labeling, while labeling a large volume of logs is highly challenging. To address this issue, many approaches based on transfer learning and active learning have been proposed. Nevertheless, their effectiveness is hindered by issues such as the gap between source and target system data distributions and cold-start problems. In this paper, we propose LogAction, a novel log-based anomaly detection model based on active domain adaptation. LogAction integrates transfer learning and active learning techniques. On one hand, it uses labeled data from a mature system to train a base model, mitigating the cold-start issue in active learning. On the other hand, LogAction utilize free energy-based sampling and uncertainty-based sampling to select logs located at the distribution boundaries for manual labeling, thus addresses the data distribution gap in transfer learning with minimal human labeling efforts. Experimental results on six different combinations of datasets demonstrate that LogAction achieves an average 93.01% F1 score with only 2% of manual labels, outperforming some state-of-the-art methods by 26.28%. Website: this https URL</li>
</ul>

<h3>Title: Why mask diffusion does not work</h3>
<ul>
<li><strong>Authors: </strong>Haocheng Sun, Cynthia Xin Wen, Edward Hong Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03289">https://arxiv.org/abs/2510.03289</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03289">https://arxiv.org/pdf/2510.03289</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03289]] Why mask diffusion does not work(https://arxiv.org/abs/2510.03289)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The main advantages of diffusion language models over autoregressive (AR) models lie in their ability to support parallel generation and bidirectional attention, enabling a more controllable generation process. In recent years, open-source mask diffusion language models have emerged, most of which are based on a variant known as absorbing diffusion. However, this paper demonstrates why mask diffusion faces inherent difficulties in achieving parallel generation and bidirectional attention. We also propose the most effective training and inference strategies for mask diffusion.</li>
</ul>

<h3>Title: Revoking Amnesia: RL-based Trajectory Optimization to Resurrect Erased Concepts in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Daiheng Gao, Nanxiang Jiang, Andi Zhang, Shilin Lu, Yufei Tang, Wenbo Zhou, Weiming Zhang, Zhaoxin Fan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03302">https://arxiv.org/abs/2510.03302</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03302">https://arxiv.org/pdf/2510.03302</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03302]] Revoking Amnesia: RL-based Trajectory Optimization to Resurrect Erased Concepts in Diffusion Models(https://arxiv.org/abs/2510.03302)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Concept erasure techniques have been widely deployed in T2I diffusion models to prevent inappropriate content generation for safety and copyright considerations. However, as models evolve to next-generation architectures like Flux, established erasure methods (\textit{e.g.}, ESD, UCE, AC) exhibit degraded effectiveness, raising questions about their true mechanisms. Through systematic analysis, we reveal that concept erasure creates only an illusion of ``amnesia": rather than genuine forgetting, these methods bias sampling trajectories away from target concepts, making the erasure fundamentally reversible. This insight motivates the need to distinguish superficial safety from genuine concept removal. In this work, we propose \textbf{RevAm} (\underline{Rev}oking \underline{Am}nesia), an RL-based trajectory optimization framework that resurrects erased concepts by dynamically steering the denoising process without modifying model weights. By adapting Group Relative Policy Optimization (GRPO) to diffusion models, RevAm explores diverse recovery trajectories through trajectory-level rewards, overcoming local optima that limit existing methods. Extensive experiments demonstrate that RevAm achieves superior concept resurrection fidelity while reducing computational time by 10$\times$, exposing critical vulnerabilities in current safety mechanisms and underscoring the need for more robust erasure techniques beyond trajectory manipulation.</li>
</ul>

<h3>Title: Thin Bridges for Drug Text Alignment: Lightweight Contrastive Learning for Target Specific Drug Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Mallikarjuna Tupakula</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03309">https://arxiv.org/abs/2510.03309</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03309">https://arxiv.org/pdf/2510.03309</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03309]] Thin Bridges for Drug Text Alignment: Lightweight Contrastive Learning for Target Specific Drug Retrieval(https://arxiv.org/abs/2510.03309)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Multimodal foundation models hold promise for drug discovery and biomedical applications, but most existing approaches rely on heavy pretraining or large scale multimodal corpora. We investigate whether thin contrastive bridges, lightweight projection heads over frozen unimodal encoders can align chemical and textual representations without training a full multimodal model. Using paired mechanisms from ChEMBL, we align ECFP4 molecular fingerprints with biomedical sentence embeddings through dual linear projections trained with a contrastive objective. To better handle drugs sharing the same therapeutic target, we incorporate hard negative weighting and a margin loss. Evaluation under scaffold based splits, which require generalization across disjoint chemical cores, demonstrates that our approach achieves non-trivial cross modal alignment and substantially improves within target discrimination compared to frozen baselines. These results suggest that thin bridges offer a compute efficient alternative to large scale multimodal pretraining, enabling scaffold aware drug text alignment and target specific retrieval in precision medicine.</li>
</ul>

<h3>Title: The View From Space: Navigating Instrumentation Differences with EOFMs</h3>
<ul>
<li><strong>Authors: </strong>Ryan P. Demilt, Nicholas LaHaye, Karis Tenneson</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03316">https://arxiv.org/abs/2510.03316</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03316">https://arxiv.org/pdf/2510.03316</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03316]] The View From Space: Navigating Instrumentation Differences with EOFMs(https://arxiv.org/abs/2510.03316)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Earth Observation Foundation Models (EOFMs) have exploded in prevalence as tools for processing the massive volumes of remotely sensed and other earth observation data, and for delivering impact on the many essential earth monitoring tasks. An emerging trend posits using the outputs of pre-trained models as 'embeddings' which summarize high dimensional data to be used for generic tasks such as similarity search and content-specific queries. However, most EOFM models are trained only on single modalities of data and then applied or benchmarked by matching bands across different modalities. It is not clear from existing work what impact diverse sensor architectures have on the internal representations of the present suite of EOFMs. We show in this work that the representation space of EOFMs is highly sensitive to sensor architecture and that understanding this difference gives a vital perspective on the pitfalls of current EOFM design and signals for how to move forward as model developers, users, and a community guided by robust remote-sensing science.</li>
</ul>

<h3>Title: Advances in Medical Image Segmentation: A Comprehensive Survey with a Focus on Lumbar Spine Applications</h3>
<ul>
<li><strong>Authors: </strong>Ahmed Kabil, Ghada Khoriba, Mina Yousef, Essam A. Rashed</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03318">https://arxiv.org/abs/2510.03318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03318">https://arxiv.org/pdf/2510.03318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03318]] Advances in Medical Image Segmentation: A Comprehensive Survey with a Focus on Lumbar Spine Applications(https://arxiv.org/abs/2510.03318)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Medical Image Segmentation (MIS) stands as a cornerstone in medical image analysis, playing a pivotal role in precise diagnostics, treatment planning, and monitoring of various medical conditions. This paper presents a comprehensive and systematic survey of MIS methodologies, bridging the gap between traditional image processing techniques and modern deep learning approaches. The survey encompasses thresholding, edge detection, region-based segmentation, clustering algorithms, and model-based techniques while also delving into state-of-the-art deep learning architectures such as Convolutional Neural Networks (CNNs), Fully Convolutional Networks (FCNs), and the widely adopted U-Net and its variants. Moreover, integrating attention mechanisms, semi-supervised learning, generative adversarial networks (GANs), and Transformer-based models is thoroughly explored. In addition to covering established methods, this survey highlights emerging trends, including hybrid architectures, cross-modality learning, federated and distributed learning frameworks, and active learning strategies, which aim to address challenges such as limited labeled datasets, computational complexity, and model generalizability across diverse imaging modalities. Furthermore, a specialized case study on lumbar spine segmentation is presented, offering insights into the challenges and advancements in this relatively underexplored anatomical region. Despite significant progress in the field, critical challenges persist, including dataset bias, domain adaptation, interpretability of deep learning models, and integration into real-world clinical workflows.</li>
</ul>

<h3>Title: Matching the Optimal Denoiser in Point Cloud Diffusion with (Improved) Rotational Alignment</h3>
<ul>
<li><strong>Authors: </strong>Ameya Daigavane, YuQing Xie, Bodhi P. Vani, Saeed Saremi, Joseph Kleinhenz, Tess Smidt</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03335">https://arxiv.org/abs/2510.03335</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03335">https://arxiv.org/pdf/2510.03335</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03335]] Matching the Optimal Denoiser in Point Cloud Diffusion with (Improved) Rotational Alignment(https://arxiv.org/abs/2510.03335)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models are a popular class of generative models trained to reverse a noising process starting from a target data distribution. Training a diffusion model consists of learning how to denoise noisy samples at different noise levels. When training diffusion models for point clouds such as molecules and proteins, there is often no canonical orientation that can be assigned. To capture this symmetry, the true data samples are often augmented by transforming them with random rotations sampled uniformly over $SO(3)$. Then, the denoised predictions are often rotationally aligned via the Kabsch-Umeyama algorithm to the ground truth samples before computing the loss. However, the effect of this alignment step has not been well studied. Here, we show that the optimal denoiser can be expressed in terms of a matrix Fisher distribution over $SO(3)$. Alignment corresponds to sampling the mode of this distribution, and turns out to be the zeroth order approximation for small noise levels, explaining its effectiveness. We build on this perspective to derive better approximators to the optimal denoiser in the limit of small noise. Our experiments highlight that alignment is often a `good enough' approximation for the noise levels that matter most for training diffusion models.</li>
</ul>

<h3>Title: Inference-Time Search using Side Information for Diffusion-based Image Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Mahdi Farahbakhsh, Vishnu Teja Kunde, Dileep Kalathil, Krishna Narayanan, Jean-Francois Chamberland</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03352">https://arxiv.org/abs/2510.03352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03352">https://arxiv.org/pdf/2510.03352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03352]] Inference-Time Search using Side Information for Diffusion-based Image Reconstruction(https://arxiv.org/abs/2510.03352)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged as powerful priors for solving inverse problems. However, existing approaches typically overlook side information that could significantly improve reconstruction quality, especially in severely ill-posed settings. In this work, we propose a novel inference-time search algorithm that guides the sampling process using the side information in a manner that balances exploration and exploitation. This enables more accurate and reliable reconstructions, providing an alternative to the gradient-based guidance that is prone to reward-hacking artifacts. Our approach can be seamlessly integrated into a wide range of existing diffusion-based image reconstruction pipelines. Through extensive experiments on a number of inverse problems, such as box inpainting, super-resolution, and various deblurring tasks including motion, Gaussian, nonlinear, and blind deblurring, we show that our approach consistently improves the qualitative and quantitative performance of diffusion-based image reconstruction algorithms. We also show the superior performance of our approach with respect to other baselines, including reward gradient-based guidance algorithms. The code is available at \href{this https URL}{this repository}.</li>
</ul>

<h3>Title: Understanding Transformers for Time Series: Rank Structure, Flow-of-ranks, and Compressibility</h3>
<ul>
<li><strong>Authors: </strong>Annan Yu, Danielle C. Maddix, Boran Han, Xiyuan Zhang, Abdul Fatir Ansari, Oleksandr Shchur, Christos Faloutsos, Andrew Gordon Wilson, Michael W. Mahoney, Yuyang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03358">https://arxiv.org/abs/2510.03358</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03358">https://arxiv.org/pdf/2510.03358</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03358]] Understanding Transformers for Time Series: Rank Structure, Flow-of-ranks, and Compressibility(https://arxiv.org/abs/2510.03358)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Transformers are widely used across data modalities, and yet the principles distilled from text models often transfer imperfectly to models trained to other modalities. In this paper, we analyze Transformers through the lens of rank structure. Our focus is on the time series setting, where the structural properties of the data differ remarkably from those of text or vision. We show that time-series embeddings, unlike text or vision, exhibit sharply decaying singular value spectra: small patch sizes and smooth continuous mappings concentrate the data into low-rank subspaces. From this, we prove that the associated $Q/K/V$ projections admit accurate low-rank approximations, and that attention layers become compressible in proportion to the decay of the embedding spectrum. We introduce the concept of flow-of-ranks, a phenomenon by which nonlinear mixing across depth inflates the rank, explaining why early layers are most amenable to compression and why ranks grow with depth. Guided by these theoretical and empirical results, we use these insights to compress Chronos, a large time series foundation model, achieving a reduction of $65\%$ in inference time and $81\%$ in memory, without loss of accuracy. Our findings provide principled guidance for allocating width, depth, and heads in time series foundation models, and for exploiting their inherent compressibility.</li>
</ul>

<h3>Title: Unified Unsupervised Anomaly Detection via Matching Cost Filtering</h3>
<ul>
<li><strong>Authors: </strong>Zhe Zhang, Mingxiu Cai, Gaochang Wu, Jing Zhang, Lingqiao Liu, Dacheng Tao, Tianyou Chai, Xiatian Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03363">https://arxiv.org/abs/2510.03363</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03363">https://arxiv.org/pdf/2510.03363</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03363]] Unified Unsupervised Anomaly Detection via Matching Cost Filtering(https://arxiv.org/abs/2510.03363)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Unsupervised anomaly detection (UAD) aims to identify image- and pixel-level anomalies using only normal training data, with wide applications such as industrial inspection and medical analysis, where anomalies are scarce due to privacy concerns and cold-start constraints. Existing methods, whether reconstruction-based (restoring normal counterparts) or embedding-based (pretrained representations), fundamentally conduct image- or feature-level matching to generate anomaly maps. Nonetheless, matching noise has been largely overlooked, limiting their detection ability. Beyond earlier focus on unimodal RGB-based UAD, recent advances expand to multimodal scenarios, e.g., RGB--3D and RGB--Text, enabled by point cloud sensing and vision--language models. Despite shared challenges, these lines remain largely isolated, hindering a comprehensive understanding and knowledge transfer. In this paper, we advocate unified UAD for both unimodal and multimodal settings in the matching perspective. Under this insight, we present Unified Cost Filtering (UCF), a generic post-hoc refinement framework for refining anomaly cost volume of any UAD model. The cost volume is constructed by matching a test sample against normal samples from the same or different modalities, followed by a learnable filtering module with multi-layer attention guidance from the test sample, mitigating matching noise and highlighting subtle anomalies. Comprehensive experiments on 22 diverse benchmarks demonstrate the efficacy of UCF in enhancing a variety of UAD methods, consistently achieving new state-of-the-art results in both unimodal (RGB) and multimodal (RGB--3D, RGB--Text) UAD scenarios. Code and models will be released at this https URL.</li>
</ul>

<h3>Title: Diffusion-Based, Data-Assimilation-Enabled Super-Resolution of Hub-height Winds</h3>
<ul>
<li><strong>Authors: </strong>Xiaolong Ma, Xu Dong, Ashley Tarrant, Lei Yang, Rao Kotamarthi, Jiali Wang, Feng Yan, Rajkumar Kettimuthu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03364">https://arxiv.org/abs/2510.03364</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03364">https://arxiv.org/pdf/2510.03364</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03364]] Diffusion-Based, Data-Assimilation-Enabled Super-Resolution of Hub-height Winds(https://arxiv.org/abs/2510.03364)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>High-quality observations of hub-height winds are valuable but sparse in space and time. Simulations are widely available on regular grids but are generally biased and too coarse to inform wind-farm siting or to assess extreme-weather-related risks (e.g., gusts) at infrastructure scales. To fully utilize both data types for generating high-quality, high-resolution hub-height wind speeds (tens to ~100m above ground), this study introduces WindSR, a diffusion model with data assimilation for super-resolution downscaling of hub-height winds. WindSR integrates sparse observational data with simulation fields during downscaling using state-of-the-art diffusion models. A dynamic-radius blending method is introduced to merge observations with simulations, providing conditioning for the diffusion process. Terrain information is incorporated during both training and inference to account for its role as a key driver of winds. Evaluated against convolutional-neural-network and generative-adversarial-network baselines, WindSR outperforms them in both downscaling efficiency and accuracy. Our data assimilation reduces WindSR's model bias by approximately 20% relative to independent observations.</li>
</ul>

<h3>Title: Conditional Pseudo-Supervised Contrast for Data-Free Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Renrong Shao, Wei Zhang, Jun wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03375">https://arxiv.org/abs/2510.03375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03375">https://arxiv.org/pdf/2510.03375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03375]] Conditional Pseudo-Supervised Contrast for Data-Free Knowledge Distillation(https://arxiv.org/abs/2510.03375)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Data-free knowledge distillation~(DFKD) is an effective manner to solve model compression and transmission restrictions while retaining privacy protection, which has attracted extensive attention in recent years. Currently, the majority of existing methods utilize a generator to synthesize images to support the distillation. Although the current methods have achieved great success, there are still many issues to be explored. Firstly, the outstanding performance of supervised learning in deep learning drives us to explore a pseudo-supervised paradigm on DFKD. Secondly, current synthesized methods cannot distinguish the distributions of different categories of samples, thus producing ambiguous samples that may lead to an incorrect evaluation by the teacher. Besides, current methods cannot optimize the category-wise diversity samples, which will hinder the student model learning from diverse samples and further achieving better performance. In this paper, to address the above limitations, we propose a novel learning paradigm, i.e., conditional pseudo-supervised contrast for data-free knowledge distillation~(CPSC-DFKD). The primary innovations of CPSC-DFKD are: (1) introducing a conditional generative adversarial network to synthesize category-specific diverse images for pseudo-supervised learning, (2) improving the modules of the generator to distinguish the distributions of different categories, and (3) proposing pseudo-supervised contrastive learning based on teacher and student views to enhance diversity. Comprehensive experiments on three commonly-used datasets validate the performance lift of both the student and generator brought by CPSC-DFKD. The code is available at this https URL</li>
</ul>

<h3>Title: Multi-task neural diffusion processes for uncertainty-quantified wind power prediction</h3>
<ul>
<li><strong>Authors: </strong>Joseph Rawson, Domniki Ladopoulou, Petros Dellaportas</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.AP, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03419">https://arxiv.org/abs/2510.03419</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03419">https://arxiv.org/pdf/2510.03419</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03419]] Multi-task neural diffusion processes for uncertainty-quantified wind power prediction(https://arxiv.org/abs/2510.03419)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Uncertainty-aware wind power prediction is essential for grid integration and reliable wind farm operation. We apply neural diffusion processes (NDPs)-a recent class of models that learn distributions over functions-and extend them to a multi-task NDP (MT-NDP) framework for wind power prediction. We provide the first empirical evaluation of NDPs in real supervisory control and data acquisition (SCADA) data. We introduce a task encoder within MT-NDPs to capture cross-turbine correlations and enable few-shot adaptation to unseen turbines. The proposed MT-NDP framework outperforms single-task NDPs and GPs in terms of point accuracy and calibration, particularly for wind turbines whose behaviour deviates from the fleet average. In general, NDP-based models deliver calibrated and scalable predictions suitable for operational deployment, offering sharper, yet trustworthy, predictive intervals that can support dispatch and maintenance decisions in modern wind farms.</li>
</ul>

<h3>Title: Reasoning-based Anomaly Detection Framework: A Real-time, Scalable, and Automated Approach to Anomaly Detection Across Domains</h3>
<ul>
<li><strong>Authors: </strong>Anupam Panwar, Himadri Pal, Jiali Chen, Kyle Cho, Riddick Jiang, Miao Zhao, Rajiv Krishnamurthy</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03486">https://arxiv.org/abs/2510.03486</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03486">https://arxiv.org/pdf/2510.03486</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03486]] Reasoning-based Anomaly Detection Framework: A Real-time, Scalable, and Automated Approach to Anomaly Detection Across Domains(https://arxiv.org/abs/2510.03486)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Detecting anomalies in large, distributed systems presents several challenges. The first challenge arises from the sheer volume of data that needs to be processed. Flagging anomalies in a high-throughput environment calls for a careful consideration of both algorithm and system design. The second challenge comes from the heterogeneity of time-series datasets that leverage such a system in production. In practice, anomaly detection systems are rarely deployed for a single use case. Typically, there are several metrics to monitor, often across several domains (e.g. engineering, business and operations). A one-size-fits-all approach rarely works, so these systems need to be fine-tuned for every application - this is often done manually. The third challenge comes from the fact that determining the root-cause of anomalies in such settings is akin to finding a needle in a haystack. Identifying (in real time) a time-series dataset that is associated causally with the anomalous time-series data is a very difficult problem. In this paper, we describe a unified framework that addresses these challenges. Reasoning based Anomaly Detection Framework (RADF) is designed to perform real time anomaly detection on very large datasets. This framework employs a novel technique (mSelect) that automates the process of algorithm selection and hyper-parameter tuning for each use case. Finally, it incorporates a post-detection capability that allows for faster triaging and root-cause determination. Our extensive experiments demonstrate that RADF, powered by mSelect, surpasses state-of-the-art anomaly detection models in AUC performance for 5 out of 9 public benchmarking datasets. RADF achieved an AUC of over 0.85 for 7 out of 9 datasets, a distinction unmatched by any other state-of-the-art model.</li>
</ul>

<h3>Title: D2 Actor Critic: Diffusion Actor Meets Distributional Critic</h3>
<ul>
<li><strong>Authors: </strong>Lunjun Zhang, Shuo Han, Hanrui Lyu, Bradly C Stadie</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03508">https://arxiv.org/abs/2510.03508</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03508">https://arxiv.org/pdf/2510.03508</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03508]] D2 Actor Critic: Diffusion Actor Meets Distributional Critic(https://arxiv.org/abs/2510.03508)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce D2AC, a new model-free reinforcement learning (RL) algorithm designed to train expressive diffusion policies online effectively. At its core is a policy improvement objective that avoids the high variance of typical policy gradients and the complexity of backpropagation through time. This stable learning process is critically enabled by our second contribution: a robust distributional critic, which we design through a fusion of distributional RL and clipped double Q-learning. The resulting algorithm is highly effective, achieving state-of-the-art performance on a benchmark of eighteen hard RL tasks, including Humanoid, Dog, and Shadow Hand domains, spanning both dense-reward and goal-conditioned RL scenarios. Beyond standard benchmarks, we also evaluate a biologically motivated predator-prey task to examine the behavioral robustness and generalization capacity of our approach.</li>
</ul>

<h3>Title: TS-Reasoner: Aligning Time Series Foundation Models with LLM Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Fangxu Yu, Hongyu Zhao, Tianyi Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03519">https://arxiv.org/abs/2510.03519</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03519">https://arxiv.org/pdf/2510.03519</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03519]] TS-Reasoner: Aligning Time Series Foundation Models with LLM Reasoning(https://arxiv.org/abs/2510.03519)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Time series reasoning is crucial to decision-making in diverse domains, including finance, energy usage, traffic, weather, and scientific discovery. While existing time series foundation models (TSFMs) can capture low-level dynamic patterns and provide accurate forecasting, further analysis usually requires additional background knowledge and sophisticated reasoning, which are lacking in most TSFMs but can be achieved through large language models (LLMs). On the other hand, without expensive post-training, LLMs often struggle with the numerical understanding of time series data. Although it is intuitive to integrate the two types of models, developing effective training recipes that align the two modalities for reasoning tasks is still an open challenge. To this end, we propose TS-Reasoner that aligns the latent representations of TSFMs with the textual inputs of LLMs for downstream understanding/reasoning tasks. Specifically, we propose a simple yet effective method to curate diverse, synthetic pairs of time series and textual captions for alignment training. We then develop a two-stage training recipe that applies instruction finetuning after the alignment pretraining. Unlike existing works that train an LLM to take time series as inputs, we leverage a pretrained TSFM and freeze it during training. Extensive experiments on several benchmarks demonstrate that TS-Reasoner not only outperforms a wide range of prevailing LLMs, Vision Language Models (VLMs), and Time Series LLMs, but also achieves this with remarkable data efficiency, e.g., using less than half the training data.</li>
</ul>

<h3>Title: Domain Generalization for Semantic Segmentation: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Manuel Schwonberg, Hanno Gottschalk</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03540">https://arxiv.org/abs/2510.03540</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03540">https://arxiv.org/pdf/2510.03540</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03540]] Domain Generalization for Semantic Segmentation: A Survey(https://arxiv.org/abs/2510.03540)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>The generalization of deep neural networks to unknown domains is a major challenge despite their tremendous progress in recent years. For this reason, the dynamic area of domain generalization (DG) has emerged. In contrast to unsupervised domain adaptation, there is no access to or knowledge about the target domains, and DG methods aim to generalize across multiple different unseen target domains. Domain generalization is particularly relevant for the task semantic segmentation which is used in several areas such as biomedicine or automated driving. This survey provides a comprehensive overview of the rapidly evolving topic of domain generalized semantic segmentation. We cluster and review existing approaches and identify the paradigm shift towards foundation-model-based domain generalization. Finally, we provide an extensive performance comparison of all approaches, which highlights the significant influence of foundation models on domain generalization. This survey seeks to advance domain generalization research and inspire scientists to explore new research directions.</li>
</ul>

<h3>Title: What is a protest anyway? Codebook conceptualization is still a first-order concern in LLM-era classification</h3>
<ul>
<li><strong>Authors: </strong>Andrew Halterman, Katherine A. Keith</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03541">https://arxiv.org/abs/2510.03541</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03541">https://arxiv.org/pdf/2510.03541</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03541]] What is a protest anyway? Codebook conceptualization is still a first-order concern in LLM-era classification(https://arxiv.org/abs/2510.03541)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative large language models (LLMs) are now used extensively for text classification in computational social science (CSS). In this work, focus on the steps before and after LLM prompting -- conceptualization of concepts to be classified and using LLM predictions in downstream statistical inference -- which we argue have been overlooked in much of LLM-era CSS. We claim LLMs can tempt analysts to skip the conceptualization step, creating conceptualization errors that bias downstream estimates. Using simulations, we show that this conceptualization-induced bias cannot be corrected for solely by increasing LLM accuracy or post-hoc bias correction methods. We conclude by reminding CSS analysts that conceptualization is still a first-order concern in the LLM-era and provide concrete advice on how to pursue low-cost, unbiased, low-variance downstream estimates.</li>
</ul>

<h3>Title: SketchPlan: Diffusion Based Drone Planning From Human Sketches</h3>
<ul>
<li><strong>Authors: </strong>Sixten Norelius, Aaron O. Feldman, Mac Schwager</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03545">https://arxiv.org/abs/2510.03545</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03545">https://arxiv.org/pdf/2510.03545</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03545]] SketchPlan: Diffusion Based Drone Planning From Human Sketches(https://arxiv.org/abs/2510.03545)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We propose SketchPlan, a diffusion-based planner that interprets 2D hand-drawn sketches over depth images to generate 3D flight paths for drone navigation. SketchPlan comprises two components: a SketchAdapter that learns to map the human sketches to projected 2D paths, and DiffPath, a diffusion model that infers 3D trajectories from 2D projections and a first person view depth image. Our model achieves zero-shot sim-to-real transfer, generating accurate and safe flight paths in previously unseen real-world environments. To train the model, we build a synthetic dataset of 32k flight paths using a diverse set of photorealistic 3D Gaussian Splatting scenes. We automatically label the data by computing 2D projections of the 3D flight paths onto the camera plane, and use this to train the DiffPath diffusion model. However, since real human 2D sketches differ significantly from ideal 2D projections, we additionally label 872 of the 3D flight paths with real human sketches and use this to train the SketchAdapter to infer the 2D projection from the human sketch. We demonstrate SketchPlan's effectiveness in both simulated and real-world experiments, and show through ablations that training on a mix of human labeled and auto-labeled data together with a modular design significantly boosts its capabilities to correctly interpret human intent and infer 3D paths. In real-world drone tests, SketchPlan achieved 100\% success in low/medium clutter and 40\% in unseen high-clutter environments, outperforming key ablations by 20-60\% in task completion.</li>
</ul>

<h3>Title: Streaming Drag-Oriented Interactive Video Manipulation: Drag Anything, Anytime!</h3>
<ul>
<li><strong>Authors: </strong>Junbao Zhou, Yuan Zhou, Kesen Zhao, Qingshan Xu, Beier Zhu, Richang Hong, Hanwang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03550">https://arxiv.org/abs/2510.03550</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03550">https://arxiv.org/pdf/2510.03550</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03550]] Streaming Drag-Oriented Interactive Video Manipulation: Drag Anything, Anytime!(https://arxiv.org/abs/2510.03550)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Achieving streaming, fine-grained control over the outputs of autoregressive video diffusion models remains challenging, making it difficult to ensure that they consistently align with user expectations. To bridge this gap, we propose \textbf{stReaming drag-oriEnted interactiVe vidEo manipuLation (REVEL)}, a new task that enables users to modify generated videos \emph{anytime} on \emph{anything} via fine-grained, interactive drag. Beyond DragVideo and SG-I2V, REVEL unifies drag-style video manipulation as editing and animating video frames with both supporting user-specified translation, deformation, and rotation effects, making drag operations versatile. In resolving REVEL, we observe: \emph{i}) drag-induced perturbations accumulate in latent space, causing severe latent distribution drift that halts the drag process; \emph{ii}) streaming drag is easily disturbed by context frames, thereby yielding visually unnatural outcomes. We thus propose a training-free approach, \textbf{DragStream}, comprising: \emph{i}) an adaptive distribution self-rectification strategy that leverages neighboring frames' statistics to effectively constrain the drift of latent embeddings; \emph{ii}) a spatial-frequency selective optimization mechanism, allowing the model to fully exploit contextual information while mitigating its interference via selectively propagating visual cues along generation. Our method can be seamlessly integrated into existing autoregressive video diffusion models, and extensive experiments firmly demonstrate the effectiveness of our DragStream.</li>
</ul>

<h3>Title: GAS-MIL: Group-Aggregative Selection Multi-Instance Learning for Ensemble of Foundation Models in Digital Pathology Image Analysis</h3>
<ul>
<li><strong>Authors: </strong>Peiran Quan, Zifan Gu, Zhuo Zhao, Qin Zhou, Donghan M. Yang, Ruichen Rong, Yang Xie, Guanghua Xiao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03555">https://arxiv.org/abs/2510.03555</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03555">https://arxiv.org/pdf/2510.03555</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03555]] GAS-MIL: Group-Aggregative Selection Multi-Instance Learning for Ensemble of Foundation Models in Digital Pathology Image Analysis(https://arxiv.org/abs/2510.03555)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Foundation models (FMs) have transformed computational pathology by providing powerful, general-purpose feature extractors. However, adapting and benchmarking individual FMs for specific diagnostic tasks is often time-consuming and resource-intensive, especially given their scale and diversity. To address this challenge, we introduce Group-Aggregative Selection Multi-Instance Learning (GAS-MIL), a flexible ensemble framework that seamlessly integrates features from multiple FMs, preserving their complementary strengths without requiring manual feature selection or extensive task-specific fine-tuning. Across classification tasks in three cancer datasets-prostate (PANDA), ovarian (UBC-OCEAN), and breast (TCGA-BrCa)-GAS-MIL consistently achieves superior or on-par performance relative to individual FMs and established MIL methods, demonstrating its robustness and generalizability. By enabling efficient integration of heterogeneous FMs, GAS-MIL streamlines model deployment for pathology and provides a scalable foundation for future multimodal and precision oncology applications.</li>
</ul>

<h3>Title: Longitudinal Flow Matching for Trajectory Modeling</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Mohaiminul Islam, Thijs P. Kuipers, Sharvaree Vadgama, Coen de Vente, Afsana Khan, Clara I. Sánchez, Erik J. Bekkers</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03569">https://arxiv.org/abs/2510.03569</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03569">https://arxiv.org/pdf/2510.03569</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03569]] Longitudinal Flow Matching for Trajectory Modeling(https://arxiv.org/abs/2510.03569)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generative models for sequential data often struggle with sparsely sampled and high-dimensional trajectories, typically reducing the learning of dynamics to pairwise transitions. We propose \textit{Interpolative Multi-Marginal Flow Matching} (IMMFM), a framework that learns continuous stochastic dynamics jointly consistent with multiple observed time points. IMMFM employs a piecewise-quadratic interpolation path as a smooth target for flow matching and jointly optimizes drift and a data-driven diffusion coefficient, supported by a theoretical condition for stable learning. This design captures intrinsic stochasticity, handles irregular sparse sampling, and yields subject-specific trajectories. Experiments on synthetic benchmarks and real-world longitudinal neuroimaging datasets show that IMMFM outperforms existing methods in both forecasting accuracy and further downstream tasks.</li>
</ul>

<h3>Title: LLM, Reporting In! Medical Information Extraction Across Prompting, Fine-tuning and Post-correction</h3>
<ul>
<li><strong>Authors: </strong>Ikram Belmadani, Parisa Nazari Hashemi, Thomas Sebbag, Benoit Favre, Guillaume Fortier, Solen Quiniou, Emmanuel Morin, Richard Dufour</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03577">https://arxiv.org/abs/2510.03577</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03577">https://arxiv.org/pdf/2510.03577</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03577]] LLM, Reporting In! Medical Information Extraction Across Prompting, Fine-tuning and Post-correction(https://arxiv.org/abs/2510.03577)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>This work presents our participation in the EvalLLM 2025 challenge on biomedical Named Entity Recognition (NER) and health event extraction in French (few-shot setting). For NER, we propose three approaches combining large language models (LLMs), annotation guidelines, synthetic data, and post-processing: (1) in-context learning (ICL) with GPT-4.1, incorporating automatic selection of 10 examples and a summary of the annotation guidelines into the prompt, (2) the universal NER system GLiNER, fine-tuned on a synthetic corpus and then verified by an LLM in post-processing, and (3) the open LLM LLaMA-3.1-8B-Instruct, fine-tuned on the same synthetic corpus. Event extraction uses the same ICL strategy with GPT-4.1, reusing the guideline summary in the prompt. Results show GPT-4.1 leads with a macro-F1 of 61.53% for NER and 15.02% for event extraction, highlighting the importance of well-crafted prompting to maximize performance in very low-resource scenarios.</li>
</ul>

<h3>Title: FieldFormer: Physics-Informed Transformers for Spatio-Temporal Field Reconstruction from Sparse Sensors</h3>
<ul>
<li><strong>Authors: </strong>Ankit Bhardwaj, Ananth Balashankar, Lakshminarayanan Subramanian</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03589">https://arxiv.org/abs/2510.03589</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03589">https://arxiv.org/pdf/2510.03589</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03589]] FieldFormer: Physics-Informed Transformers for Spatio-Temporal Field Reconstruction from Sparse Sensors(https://arxiv.org/abs/2510.03589)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Spatio-temporal sensor data is often sparse, noisy, and irregular, and existing interpolation or learning methods struggle here because they either ignore governing PDEs or do not scale. We introduce FieldFormer, a transformer-based framework for mesh-free spatio-temporal field reconstruction that combines data-driven flexibility with physics-based structure. For each query, FieldFormer gathers a local neighborhood using a learnable velocity-scaled distance metric, enabling anisotropic adaptation to different propagation regimes. Neighborhoods are built efficiently via per-batch offset recomputation, and refined in an expectation-maximization style as the velocity scales evolve. Predictions are made by a local transformer encoder, and physics consistency is enforced through autograd-based PDE residuals and boundary-specific penalties. Across three benchmarks--a scalar anisotropic heat equation, a vector-valued shallow-water system, and a realistic advection-diffusion pollution simulation--FieldFormer consistently outperforms strong baselines by more than 40%. Our results demonstrate that FieldFormer enables accurate (RMSE$<10^{-2}$), efficient, and physically consistent field reconstruction from sparse (0.4%-2%) and noisy(10%) data.</li>
</ul>

<h3>Title: Unsupervised Transformer Pre-Training for Images: Self-Distillation, Mean Teachers, and Random Crops</h3>
<ul>
<li><strong>Authors: </strong>Mattia Scardecchia</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03606">https://arxiv.org/abs/2510.03606</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03606">https://arxiv.org/pdf/2510.03606</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03606]] Unsupervised Transformer Pre-Training for Images: Self-Distillation, Mean Teachers, and Random Crops(https://arxiv.org/abs/2510.03606)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Recent advances in self-supervised learning (SSL) have made it possible to learn general-purpose visual features that capture both the high-level semantics and the fine-grained spatial structure of images. Most notably, the recent DINOv2 has established a new state of the art by surpassing weakly supervised methods (WSL) like OpenCLIP on most benchmarks. In this survey, we examine the core ideas behind its approach, multi-crop view augmentation and self-distillation with a mean teacher, and trace their development in previous work. We then compare the performance of DINO and DINOv2 with other SSL and WSL methods across various downstream tasks, and highlight some remarkable emergent properties of their learned features with transformer backbones. We conclude by briefly discussing DINOv2's limitations, its impact, and future research directions.</li>
</ul>

<h3>Title: Diffusion-Classifier Synergy: Reward-Aligned Learning via Mutual Boosting Loop for FSCIL</h3>
<ul>
<li><strong>Authors: </strong>Ruitao Wu, Yifan Zhao, Guangyao Chen, Jia Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03608">https://arxiv.org/abs/2510.03608</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03608">https://arxiv.org/pdf/2510.03608</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03608]] Diffusion-Classifier Synergy: Reward-Aligned Learning via Mutual Boosting Loop for FSCIL(https://arxiv.org/abs/2510.03608)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Few-Shot Class-Incremental Learning (FSCIL) challenges models to sequentially learn new classes from minimal examples without forgetting prior knowledge, a task complicated by the stability-plasticity dilemma and data scarcity. Current FSCIL methods often struggle with generalization due to their reliance on limited datasets. While diffusion models offer a path for data augmentation, their direct application can lead to semantic misalignment or ineffective guidance. This paper introduces Diffusion-Classifier Synergy (DCS), a novel framework that establishes a mutual boosting loop between diffusion model and FSCIL classifier. DCS utilizes a reward-aligned learning strategy, where a dynamic, multi-faceted reward function derived from the classifier's state directs the diffusion model. This reward system operates at two levels: the feature level ensures semantic coherence and diversity using prototype-anchored maximum mean discrepancy and dimension-wise variance matching, while the logits level promotes exploratory image generation and enhances inter-class discriminability through confidence recalibration and cross-session confusion-aware mechanisms. This co-evolutionary process, where generated images refine the classifier and an improved classifier state yields better reward signals, demonstrably achieves state-of-the-art performance on FSCIL benchmarks, significantly enhancing both knowledge retention and new class learning.</li>
</ul>

<h3>Title: Neural Bayesian Filtering</h3>
<ul>
<li><strong>Authors: </strong>Christopher Solinas, Radovan Haluska, David Sychrovsky, Finbarr Timbers, Nolan Bard, Michael Buro, Martin Schmid, Nathan R. Sturtevant, Michael Bowling</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03614">https://arxiv.org/abs/2510.03614</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03614">https://arxiv.org/pdf/2510.03614</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03614]] Neural Bayesian Filtering(https://arxiv.org/abs/2510.03614)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present Neural Bayesian Filtering (NBF), an algorithm for maintaining distributions over hidden states, called beliefs, in partially observable systems. NBF is trained to find a good latent representation of the beliefs induced by a task. It maps beliefs to fixed-length embedding vectors, which condition generative models for sampling. During filtering, particle-style updates compute posteriors in this embedding space using incoming observations and the environment's dynamics. NBF combines the computational efficiency of classical filters with the expressiveness of deep generative models - tracking rapidly shifting, multimodal beliefs while mitigating the risk of particle impoverishment. We validate NBF in state estimation tasks in three partially observable environments.</li>
</ul>

<h3>Title: From Theory to Practice: Evaluating Data Poisoning Attacks and Defenses in In-Context Learning on Social Media Health Discourse</h3>
<ul>
<li><strong>Authors: </strong>Rabeya Amin Jhuma, Mostafa Mohaimen Akand Faisal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03636">https://arxiv.org/abs/2510.03636</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03636">https://arxiv.org/pdf/2510.03636</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03636]] From Theory to Practice: Evaluating Data Poisoning Attacks and Defenses in In-Context Learning on Social Media Health Discourse(https://arxiv.org/abs/2510.03636)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>This study explored how in-context learning (ICL) in large language models can be disrupted by data poisoning attacks in the setting of public health sentiment analysis. Using tweets of Human Metapneumovirus (HMPV), small adversarial perturbations such as synonym replacement, negation insertion, and randomized perturbation were introduced into the support examples. Even these minor manipulations caused major disruptions, with sentiment labels flipping in up to 67% of cases. To address this, a Spectral Signature Defense was applied, which filtered out poisoned examples while keeping the data's meaning and sentiment intact. After defense, ICL accuracy remained steady at around 46.7%, and logistic regression validation reached 100% accuracy, showing that the defense successfully preserved the dataset's integrity. Overall, the findings extend prior theoretical studies of ICL poisoning to a practical, high-stakes setting in public health discourse analysis, highlighting both the risks and potential defenses for robust LLM deployment. This study also highlights the fragility of ICL under attack and the value of spectral defenses in making AI systems more reliable for health-related social media monitoring.</li>
</ul>

<h3>Title: A Novel Cloud-Based Diffusion-Guided Hybrid Model for High-Accuracy Accident Detection in Intelligent Transportation Systems</h3>
<ul>
<li><strong>Authors: </strong>Siva Sai, Saksham Gupta, Vinay Chamola, Rajkumar Buyya</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03675">https://arxiv.org/abs/2510.03675</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03675">https://arxiv.org/pdf/2510.03675</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03675]] A Novel Cloud-Based Diffusion-Guided Hybrid Model for High-Accuracy Accident Detection in Intelligent Transportation Systems(https://arxiv.org/abs/2510.03675)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The integration of Diffusion Models into Intelligent Transportation Systems (ITS) is a substantial improvement in the detection of accidents. We present a novel hybrid model integrating guidance classification with diffusion techniques. By leveraging fine-tuned ExceptionNet architecture outputs as input for our proposed diffusion model and processing image tensors as our conditioning, our approach creates a robust classification framework. Our model consists of multiple conditional modules, which aim to modulate the linear projection of inputs using time embeddings and image covariate embeddings, allowing the network to adapt its behavior dynamically throughout the diffusion process. To address the computationally intensive nature of diffusion models, our implementation is cloud-based, enabling scalable and efficient processing. Our strategy overcomes the shortcomings of conventional classification approaches by leveraging diffusion models inherent capacity to effectively understand complicated data distributions. We investigate important diffusion characteristics, such as timestep schedulers, timestep encoding techniques, timestep count, and architectural design changes, using a thorough ablation study, and have conducted a comprehensive evaluation of the proposed model against the baseline models on a publicly available dataset. The proposed diffusion model performs best in image-based accident detection with an accuracy of 97.32%.</li>
</ul>

<h3>Title: From Moments to Models: Graphon Mixture-Aware Mixup and Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Ali Azizpour, Reza Ramezanpour, Ashutosh Sabharwal, Santiago Segarra</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03690">https://arxiv.org/abs/2510.03690</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03690">https://arxiv.org/pdf/2510.03690</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03690]] From Moments to Models: Graphon Mixture-Aware Mixup and Contrastive Learning(https://arxiv.org/abs/2510.03690)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Real-world graph datasets often consist of mixtures of populations, where graphs are generated from multiple distinct underlying distributions. However, modern representation learning approaches, such as graph contrastive learning (GCL) and augmentation methods like Mixup, typically overlook this mixture structure. In this work, we propose a unified framework that explicitly models data as a mixture of underlying probabilistic graph generative models represented by graphons. To characterize these graphons, we leverage graph moments (motif densities) to cluster graphs arising from the same model. This enables us to disentangle the mixture components and identify their distinct generative mechanisms. This model-aware partitioning benefits two key graph learning tasks: 1) It enables a graphon-mixture-aware mixup (GMAM), a data augmentation technique that interpolates in a semantically valid space guided by the estimated graphons, instead of assuming a single graphon per class. 2) For GCL, it enables model-adaptive and principled augmentations. Additionally, by introducing a new model-aware objective, our proposed approach (termed MGCL) improves negative sampling by restricting negatives to graphs from other models. We establish a key theoretical guarantee: a novel, tighter bound showing that graphs sampled from graphons with small cut distance will have similar motif densities with high probability. Extensive experiments on benchmark datasets demonstrate strong empirical performance. In unsupervised learning, MGCL achieves state-of-the-art results, obtaining the top average rank across eight datasets. In supervised learning, GMAM consistently outperforms existing strategies, achieving new state-of-the-art accuracy in 6 out of 7 datasets.</li>
</ul>

<h3>Title: Person-Centric Annotations of LAION-400M: Auditing Bias and Its Transfer to Models</h3>
<ul>
<li><strong>Authors: </strong>Leander Girrbach, Stephan Alaniz, Genevieve Smith, Trevor Darrell, Zeynep Akata</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03721">https://arxiv.org/abs/2510.03721</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03721">https://arxiv.org/pdf/2510.03721</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03721]] Person-Centric Annotations of LAION-400M: Auditing Bias and Its Transfer to Models(https://arxiv.org/abs/2510.03721)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Vision-language models trained on large-scale multimodal datasets show strong demographic biases, but the role of training data in producing these biases remains unclear. A major barrier has been the lack of demographic annotations in web-scale datasets such as LAION-400M. We address this gap by creating person-centric annotations for the full dataset, including over 276 million bounding boxes, perceived gender and race/ethnicity labels, and automatically generated captions. These annotations are produced through validated automatic labeling pipelines combining object detection, multimodal captioning, and finetuned classifiers. Using them, we uncover demographic imbalances and harmful associations, such as the disproportionate linking of men and individuals perceived as Black or Middle Eastern with crime-related and negative content. We also show that 60-70% of gender bias in CLIP and Stable Diffusion can be linearly explained by direct co-occurrences in the data. Our resources establish the first large-scale empirical link between dataset composition and downstream model bias.</li>
</ul>

<h3>Title: Curriculum-Augmented GFlowNets For mRNA Sequence Generation</h3>
<ul>
<li><strong>Authors: </strong>Aya Laajil, Abduragim Shtanchaev, Sajan Muhammad, Eric Moulines, Salem Lahlou</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03811">https://arxiv.org/abs/2510.03811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03811">https://arxiv.org/pdf/2510.03811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03811]] Curriculum-Augmented GFlowNets For mRNA Sequence Generation(https://arxiv.org/abs/2510.03811)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Designing mRNA sequences is a major challenge in developing next-generation therapeutics, since it involves exploring a vast space of possible nucleotide combinations while optimizing sequence properties like stability, translation efficiency, and protein expression. While Generative Flow Networks are promising for this task, their training is hindered by sparse, long-horizon rewards and multi-objective trade-offs. We propose Curriculum-Augmented GFlowNets (CAGFN), which integrate curriculum learning with multi-objective GFlowNets to generate de novo mRNA sequences. CAGFN integrates a length-based curriculum that progressively adapts the maximum sequence length guiding exploration from easier to harder subproblems. We also provide a new mRNA design environment for GFlowNets which, given a target protein sequence and a combination of biological objectives, allows for the training of models that generate plausible mRNA candidates. This provides a biologically motivated setting for applying and advancing GFlowNets in therapeutic sequence design. On different mRNA design tasks, CAGFN improves Pareto performance and biological plausibility, while maintaining diversity. Moreover, CAGFN reaches higher-quality solutions faster than a GFlowNet trained with random sequence sampling (no curriculum), and enables generalization to out-of-distribution sequences.</li>
</ul>

<h3>Title: Contrastive-SDE: Guiding Stochastic Differential Equations with Contrastive Learning for Unpaired Image-to-Image Translation</h3>
<ul>
<li><strong>Authors: </strong>Venkata Narendra Kotyada, Revanth Eranki, Nagesh Bhattu Sristy</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03821">https://arxiv.org/abs/2510.03821</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03821">https://arxiv.org/pdf/2510.03821</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03821]] Contrastive-SDE: Guiding Stochastic Differential Equations with Contrastive Learning for Unpaired Image-to-Image Translation(https://arxiv.org/abs/2510.03821)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Unpaired image-to-image translation involves learning mappings between source domain and target domain in the absence of aligned or corresponding samples. Score based diffusion models have demonstrated state-of-the-art performance in generative tasks. Their ability to approximate complex data distributions through stochastic differential equations (SDEs) enables them to generate high-fidelity and diverse outputs, making them particularly well-suited for unpaired I2I settings. In parallel, contrastive learning provides a powerful framework for learning semantic similarities without the need for explicit supervision or paired data. By pulling together representations of semantically similar samples and pushing apart dissimilar ones, contrastive methods are inherently aligned with the objectives of unpaired translation. Its ability to selectively enforce semantic consistency at the feature level makes contrastive learning particularly effective for guiding generation in unpaired scenarios. In this work, we propose a time-dependent contrastive learning approach where a model is trained with SimCLR by considering an image and its domain invarient feature as a positive pair, enabling the preservation of domain-invariant features and the discarding of domain-specific ones. The learned contrastive model then guides the inference of a pretrained SDE for the I2I translation task. We empirically compare Contrastive-SDE with several baselines across three common unpaired I2I tasks, using four metrics for evaluation. Constrastive-SDE achieves comparable results to the state-of-the-art on several metrics. Furthermore, we observe that our model converges significantly faster and requires no label supervision or classifier training, making it a more efficient alternative for this task.</li>
</ul>

<h3>Title: Proximal Diffusion Neural Sampler</h3>
<ul>
<li><strong>Authors: </strong>Wei Guo, Jaemoo Choi, Yuchen Zhu, Molei Tao, Yongxin Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03824">https://arxiv.org/abs/2510.03824</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03824">https://arxiv.org/pdf/2510.03824</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03824]] Proximal Diffusion Neural Sampler(https://arxiv.org/abs/2510.03824)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The task of learning a diffusion-based neural sampler for drawing samples from an unnormalized target distribution can be viewed as a stochastic optimal control problem on path measures. However, the training of neural samplers can be challenging when the target distribution is multimodal with significant barriers separating the modes, potentially leading to mode collapse. We propose a framework named \textbf{Proximal Diffusion Neural Sampler (PDNS)} that addresses these challenges by tackling the stochastic optimal control problem via proximal point method on the space of path measures. PDNS decomposes the learning process into a series of simpler subproblems that create a path gradually approaching the desired distribution. This staged procedure traces a progressively refined path to the desired distribution and promotes thorough exploration across modes. For a practical and efficient realization, we instantiate each proximal step with a proximal weighted denoising cross-entropy (WDCE) objective. We demonstrate the effectiveness and robustness of PDNS through extensive experiments on both continuous and discrete sampling tasks, including challenging scenarios in molecular dynamics and statistical physics.</li>
</ul>

<h3>Title: SDAKD: Student Discriminator Assisted Knowledge Distillation for Super-Resolution Generative Adversarial Networks</h3>
<ul>
<li><strong>Authors: </strong>Nikolaos Kaparinos, Vasileios Mezaris</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03870">https://arxiv.org/abs/2510.03870</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03870">https://arxiv.org/pdf/2510.03870</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03870]] SDAKD: Student Discriminator Assisted Knowledge Distillation for Super-Resolution Generative Adversarial Networks(https://arxiv.org/abs/2510.03870)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative Adversarial Networks (GANs) achieve excellent performance in generative tasks, such as image super-resolution, but their computational requirements make difficult their deployment on resource-constrained devices. While knowledge distillation is a promising research direction for GAN compression, effectively training a smaller student generator is challenging due to the capacity mismatch between the student generator and the teacher discriminator. In this work, we propose Student Discriminator Assisted Knowledge Distillation (SDAKD), a novel GAN distillation methodology that introduces a student discriminator to mitigate this capacity mismatch. SDAKD follows a three-stage training strategy, and integrates an adapted feature map distillation approach in its last two training stages. We evaluated SDAKD on two well-performing super-resolution GANs, GCFSR and Real-ESRGAN. Our experiments demonstrate consistent improvements over the baselines and SOTA GAN knowledge distillation methods. The SDAKD source code will be made openly available upon acceptance of the paper.</li>
</ul>

<h3>Title: LLM as an Algorithmist: Enhancing Anomaly Detectors via Programmatic Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Hangting Ye, Jinmeng Li, He Zhao, Mingchen Zhuge, Dandan Guo, Yi Chang, Hongyuan Zha</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03904">https://arxiv.org/abs/2510.03904</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03904">https://arxiv.org/pdf/2510.03904</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03904]] LLM as an Algorithmist: Enhancing Anomaly Detectors via Programmatic Synthesis(https://arxiv.org/abs/2510.03904)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Existing anomaly detection (AD) methods for tabular data usually rely on some assumptions about anomaly patterns, leading to inconsistent performance in real-world scenarios. While Large Language Models (LLMs) show remarkable reasoning capabilities, their direct application to tabular AD is impeded by fundamental challenges, including difficulties in processing heterogeneous data and significant privacy risks. To address these limitations, we propose LLM-DAS, a novel framework that repositions the LLM from a ``data processor'' to an ``algorithmist''. Instead of being exposed to raw data, our framework leverages the LLM's ability to reason about algorithms. It analyzes a high-level description of a given detector to understand its intrinsic weaknesses and then generates detector-specific, data-agnostic Python code to synthesize ``hard-to-detect'' anomalies that exploit these vulnerabilities. This generated synthesis program, which is reusable across diverse datasets, is then instantiated to augment training data, systematically enhancing the detector's robustness by transforming the problem into a more discriminative two-class classification task. Extensive experiments on 36 TAD benchmarks show that LLM-DAS consistently boosts the performance of mainstream detectors. By bridging LLM reasoning with classic AD algorithms via programmatic synthesis, LLM-DAS offers a scalable, effective, and privacy-preserving approach to patching the logical blind spots of existing detectors.</li>
</ul>

<h3>Title: Generating Human Motion Videos using a Cascaded Text-to-Video Framework</h3>
<ul>
<li><strong>Authors: </strong>Hyelin Nam, Hyojun Go, Byeongjun Park, Byung-Hoon Kim, Hyungjin Chung</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03909">https://arxiv.org/abs/2510.03909</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03909">https://arxiv.org/pdf/2510.03909</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03909]] Generating Human Motion Videos using a Cascaded Text-to-Video Framework(https://arxiv.org/abs/2510.03909)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Human video generation is becoming an increasingly important task with broad applications in graphics, entertainment, and embodied AI. Despite the rapid progress of video diffusion models (VDMs), their use for general-purpose human video generation remains underexplored, with most works constrained to image-to-video setups or narrow domains like dance videos. In this work, we propose CAMEO, a cascaded framework for general human motion video generation. It seamlessly bridges Text-to-Motion (T2M) models and conditional VDMs, mitigating suboptimal factors that may arise in this process across both training and inference through carefully designed components. Specifically, we analyze and prepare both textual prompts and visual conditions to effectively train the VDM, ensuring robust alignment between motion descriptions, conditioning signals, and the generated videos. Furthermore, we introduce a camera-aware conditioning module that connects the two stages, automatically selecting viewpoints aligned with the input text to enhance coherence and reduce manual intervention. We demonstrate the effectiveness of our approach on both the MovieGen benchmark and a newly introduced benchmark tailored to the T2M-VDM combination, while highlighting its versatility across diverse use cases.</li>
</ul>

<h3>Title: THEMIS: Unlocking Pretrained Knowledge with Foundation Model Embeddings for Anomaly Detection in Time Series</h3>
<ul>
<li><strong>Authors: </strong>Yadav Mahesh Lorik, Kaushik Sarveswaran, Nagaraj Sundaramahalingam, Aravindakumar Venugopalan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03911">https://arxiv.org/abs/2510.03911</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03911">https://arxiv.org/pdf/2510.03911</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03911]] THEMIS: Unlocking Pretrained Knowledge with Foundation Model Embeddings for Anomaly Detection in Time Series(https://arxiv.org/abs/2510.03911)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, anomaly</a></li>
<li><strong>Abstract: </strong>Time series anomaly detection forms a very crucial area in several domains but poses substantial challenges. Due to time series data possessing seasonality, trends, noise, and evolving patterns (concept drift), it becomes very difficult to set a general notion of what constitutes normal behavior. Anomalies themselves could be varied, ranging from a single outlier to contextual or collective anomalies, and are normally very rare; hence, the dataset is largely imbalanced. Additional layers of complexities arise due to the problems of increased dimensionality of modern time series, real-time detection criteria, setting up appropriate detection thresholds, and arriving at results that are interpretable. To embrace these multifaceted challenges, very strong, flexible, and interpretable approaches are required. This paper presents THEMIS, a new framework for time series anomaly detection that exploits pretrained knowledge from foundation models. THEMIS extracts embeddings from the encoder of the Chronos time series foundation model and applies outlier detection techniques like Local Outlier Factor and Spectral Decomposition on the self-similarity matrix, to spot anomalies in the data. Our experiments show that this modular method achieves SOTA results on the MSL dataset and performs quite competitively on the SMAP and SWAT$^*$ datasets. Notably, THEMIS exceeds models trained specifically for anomaly detection, presenting hyperparameter robustness and interpretability by default. This paper advocates for pretrained representations from foundation models for performing efficient and adaptable anomaly detection for time series data.</li>
</ul>

<h3>Title: SPEAR: Soft Prompt Enhanced Anomaly Recognition for Time Series Data</h3>
<ul>
<li><strong>Authors: </strong>Hanzhe Wei, Jiajun Wu, Jialin Yang, Henry Leung, Steve Drew</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.03962">https://arxiv.org/abs/2510.03962</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.03962">https://arxiv.org/pdf/2510.03962</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.03962]] SPEAR: Soft Prompt Enhanced Anomaly Recognition for Time Series Data(https://arxiv.org/abs/2510.03962)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Time series anomaly detection plays a crucial role in a wide range of fields, such as healthcare and internet traffic monitoring. The emergence of large language models (LLMs) offers new opportunities for detecting anomalies in the ubiquitous time series data. Traditional approaches struggle with variable-length time series sequences and context-based anomalies. We propose Soft Prompt Enhanced Anomaly Recognition (SPEAR), a novel approach to leverage LLMs for anomaly detection with soft prompts and quantization. Our methodology involves quantizing and transforming the time series data into input embeddings and combining them with learnable soft prompt embeddings. These combined embeddings are then fed into a frozen LLM. The soft prompts are updated iteratively based on a cross-entropy loss, allowing the model to adapt to time series anomaly detection. The use of soft prompts helps adapt LLMs effectively to time series tasks, while quantization ensures optimal handling of sequences, as LLMs are designed to handle discrete sequences. Our experimental results demonstrate that soft prompts effectively increase LLMs' performance in downstream tasks regarding time series anomaly detection.</li>
</ul>

<h3>Title: Principled and Tractable RL for Reasoning with Diffusion Language Models</h3>
<ul>
<li><strong>Authors: </strong>Anthony Zhan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04019">https://arxiv.org/abs/2510.04019</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04019">https://arxiv.org/pdf/2510.04019</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04019]] Principled and Tractable RL for Reasoning with Diffusion Language Models(https://arxiv.org/abs/2510.04019)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion large language models (dLLMs) are a new paradigm of non-autoregressive language models that are trained to predict multiple tokens in parallel and generate text via iterative unmasking. Recent works have successfully pretrained dLLMs to parity with autoregressive LLMs at the 8B scale, but dLLMs have yet to benefit from modern post-training techniques, e.g. reinforcement learning (RL), that have proven effective for autoregressive models. Crucially, algorithms designed for traditional LLMs aren't directly compatible with diffusion frameworks due to inherent differences in modeling assumptions. Moreover, existing attempts at dLLM post-training with RL rely on heuristic-based objectives with no theoretical grounding. In this work, we present Amortized Group Relative Policy Optimization (AGRPO), a principled on-policy RL algorithm designed specifically for dLLMs. AGRPO uses Monte Carlo sampling to compute an unbiased policy gradient estimate, making it the first tractable, faithful adaptation of policy gradient methods for dLLMs. We demonstrate AGRPO's effectiveness on different math/reasoning tasks, a common setting for RL with LLMs, achieving up to +7.6% absolute gain on GSM8K and 3.8x performance on the Countdown task over the baseline LLaDA-8B-Instruct model and 1.3x performance gains over comparable RL methods such as diffu-GRPO. Furthermore, these gains persist across different numbers of sampling steps at inference time, achieving better tradeoffs between compute and performance. Our results demonstrate that online RL algorithms can be extended to diffusion LLMs in principled ways, maintaining both theoretical soundness and practical effectiveness.</li>
</ul>

<h3>Title: Spatiotemporal Forecasting as Planning: A Model-Based Reinforcement Learning Approach with Generative World Models</h3>
<ul>
<li><strong>Authors: </strong>Hao Wu, Yuan Gao, Xingjian Shi, Shuaipeng Li, Fan Xu, Fan Zhang, Zhihong Zhu, Weiyan Wang, Xiao Luo, Kun Wang, Xian Wu, Xiaomeng Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04020">https://arxiv.org/abs/2510.04020</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04020">https://arxiv.org/pdf/2510.04020</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04020]] Spatiotemporal Forecasting as Planning: A Model-Based Reinforcement Learning Approach with Generative World Models(https://arxiv.org/abs/2510.04020)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>To address the dual challenges of inherent stochasticity and non-differentiable metrics in physical spatiotemporal forecasting, we propose Spatiotemporal Forecasting as Planning (SFP), a new paradigm grounded in Model-Based Reinforcement Learning. SFP constructs a novel Generative World Model to simulate diverse, high-fidelity future states, enabling an "imagination-based" environmental simulation. Within this framework, a base forecasting model acts as an agent, guided by a beam search-based planning algorithm that leverages non-differentiable domain metrics as reward signals to explore high-return future sequences. These identified high-reward candidates then serve as pseudo-labels to continuously optimize the agent's policy through iterative self-training, significantly reducing prediction error and demonstrating exceptional performance on critical domain metrics like capturing extreme events.</li>
</ul>

<h3>Title: Prompt-to-Prompt: Text-Based Image Editing Via Cross-Attention Mechanisms -- The Research of Hyperparameters and Novel Mechanisms to Enhance Existing Frameworks</h3>
<ul>
<li><strong>Authors: </strong>Linn Bieske, Carla Lorente</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04034">https://arxiv.org/abs/2510.04034</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04034">https://arxiv.org/pdf/2510.04034</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04034]] Prompt-to-Prompt: Text-Based Image Editing Via Cross-Attention Mechanisms -- The Research of Hyperparameters and Novel Mechanisms to Enhance Existing Frameworks(https://arxiv.org/abs/2510.04034)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in image editing have shifted from manual pixel manipulation to employing deep learning methods like stable diffusion models, which now leverage cross-attention mechanisms for text-driven control. This transition has simplified the editing process but also introduced variability in results, such as inconsistent hair color changes. Our research aims to enhance the precision and reliability of prompt-to-prompt image editing frameworks by exploring and optimizing hyperparameters. We present a comprehensive study of the "word swap" method, develop an "attention re-weight method" for better adaptability, and propose the "CL P2P" framework to address existing limitations like cycle inconsistency. This work contributes to understanding and improving the interaction between hyperparameter settings and the architectural choices of neural network models, specifically their attention mechanisms, which significantly influence the composition and quality of the generated images.</li>
</ul>

<h3>Title: Adaptive kernel-density approach for imbalanced binary classification</h3>
<ul>
<li><strong>Authors: </strong>Kotaro J. Nishimura, Yuichi Sakumura, Kazushi Ikeda</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04046">https://arxiv.org/abs/2510.04046</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04046">https://arxiv.org/pdf/2510.04046</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04046]] Adaptive kernel-density approach for imbalanced binary classification(https://arxiv.org/abs/2510.04046)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Class imbalance is a common challenge in real-world binary classification tasks, often leading to predictions biased toward the majority class and reduced recognition of the minority class. This issue is particularly critical in domains such as medical diagnosis and anomaly detection, where correct classification of minority classes is essential. Conventional methods often fail to deliver satisfactory performance when the imbalance ratio is extremely severe. To address this challenge, we propose a novel approach called Kernel-density-Oriented Threshold Adjustment with Regional Optimization (KOTARO), which extends the framework of kernel density estimation (KDE) by adaptively adjusting decision boundaries according to local sample density. In KOTARO, the bandwidth of Gaussian basis functions is dynamically tuned based on the estimated density around each sample, thereby enhancing the classifier's ability to capture minority regions. We validated the effectiveness of KOTARO through experiments on both synthetic and real-world imbalanced datasets. The results demonstrated that KOTARO outperformed conventional methods, particularly under conditions of severe imbalance, highlighting its potential as a promising solution for a wide range of imbalanced classification problems</li>
</ul>

<h3>Title: Variational Diffusion Unlearning: A Variational Inference Framework for Unlearning in Diffusion Models under Data Constraints</h3>
<ul>
<li><strong>Authors: </strong>Subhodip Panda, MS Varun, Shreyans Jain, Sarthak Kumar Maharana, Prathosh A.P</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04058">https://arxiv.org/abs/2510.04058</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04058">https://arxiv.org/pdf/2510.04058</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04058]] Variational Diffusion Unlearning: A Variational Inference Framework for Unlearning in Diffusion Models under Data Constraints(https://arxiv.org/abs/2510.04058)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>For a responsible and safe deployment of diffusion models in various domains, regulating the generated outputs from these models is desirable because such models could generate undesired, violent, and obscene outputs. To tackle this problem, recent works use machine unlearning methodology to forget training data points containing these undesired features from pre-trained generative models. However, these methods proved to be ineffective in data-constrained settings where the whole training dataset is inaccessible. Thus, the principal objective of this work is to propose a machine unlearning methodology that can prevent the generation of outputs containing undesired features from a pre-trained diffusion model in such a data-constrained setting. Our proposed method, termed as Variational Diffusion Unlearning (VDU), is a computationally efficient method that only requires access to a subset of training data containing undesired features. Our approach is inspired by the variational inference framework with the objective of minimizing a loss function consisting of two terms: plasticity inducer and stability regularizer. Plasticity inducer reduces the log-likelihood of the undesired training data points, while the stability regularizer, essential for preventing loss of image generation quality, regularizes the model in parameter space. We validate the effectiveness of our method through comprehensive experiments for both class unlearning and feature unlearning. For class unlearning, we unlearn some user-identified classes from MNIST, CIFAR-10, and tinyImageNet datasets from a pre-trained unconditional denoising diffusion probabilistic model (DDPM). Similarly, for feature unlearning, we unlearn the generation of certain high-level features from a pre-trained Stable Diffusion model</li>
</ul>

<h3>Title: Diffusion Low Rank Hybrid Reconstruction for Sparse View Medical Imaging</h3>
<ul>
<li><strong>Authors: </strong>Zongyin Deng, Qing Zhou, Yuhao Fang, Zijian Wang, Yao Lu, Ye Zhang, Chun Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04069">https://arxiv.org/abs/2510.04069</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04069">https://arxiv.org/pdf/2510.04069</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04069]] Diffusion Low Rank Hybrid Reconstruction for Sparse View Medical Imaging(https://arxiv.org/abs/2510.04069)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>This work presents TV-LoRA, a novel method for low-dose sparse-view CT reconstruction that combines a diffusion generative prior (NCSN++ with SDE modeling) and multi-regularization constraints, including anisotropic TV and nuclear norm (LoRA), within an ADMM framework. To address ill-posedness and texture loss under extremely sparse views, TV-LoRA integrates generative and physical constraints, and utilizes a 2D slice-based strategy with FFT acceleration and tensor-parallel optimization for efficient inference. Experiments on AAPM-2016, CTHD, and LIDC datasets with $N_{\mathrm{view}}=8,4,2$ show that TV-LoRA consistently surpasses benchmarks in SSIM, texture recovery, edge clarity, and artifact suppression, demonstrating strong robustness and generalizability. Ablation studies confirm the complementary effects of LoRA regularization and diffusion priors, while the FFT-PCG module provides a speedup. Overall, Diffusion + TV-LoRA achieves high-fidelity, efficient 3D CT reconstruction and broad clinical applicability in low-dose, sparse-sampling scenarios.</li>
</ul>

<h3>Title: What Makes Diffusion Language Models Super Data Learners?</h3>
<ul>
<li><strong>Authors: </strong>Zitian Gao, Haoming Luo, Lynx Chen, Jason Klein Liu, Ran Tao, Joey Zhou, Bryan Dai</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04071">https://arxiv.org/abs/2510.04071</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04071">https://arxiv.org/pdf/2510.04071</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04071]] What Makes Diffusion Language Models Super Data Learners?(https://arxiv.org/abs/2510.04071)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent studies have shown that diffusion language models achieve remarkable data efficiency under limited-data constraints, yet the underlying mechanisms remain unclear. In this work, we perform extensive ablation experiments to disentangle the sources of this efficiency. Our results show that random masking of input tokens plays the dominant role. We further show that similar gains can be obtained through in MLP dropout and weight decay, indicating that stochastic regularization broadly enhances data efficiency in multi-epoch training. Our code is available at this https URL.</li>
</ul>

<h3>Title: Why Cannot Neural Networks Master Extrapolation? Insights from Physical Laws</h3>
<ul>
<li><strong>Authors: </strong>Ramzi Dakhmouche, Hossein Gorji</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA, math.PR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04102">https://arxiv.org/abs/2510.04102</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04102">https://arxiv.org/pdf/2510.04102</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04102]] Why Cannot Neural Networks Master Extrapolation? Insights from Physical Laws(https://arxiv.org/abs/2510.04102)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Motivated by the remarkable success of Foundation Models (FMs) in language modeling, there has been growing interest in developing FMs for time series prediction, given the transformative power such models hold for science and engineering. This culminated in significant success of FMs in short-range forecasting settings. However, extrapolation or long-range forecasting remains elusive for FMs, which struggle to outperform even simple baselines. This contrasts with physical laws which have strong extrapolation properties, and raises the question of the fundamental difference between the structure of neural networks and physical laws. In this work, we identify and formalize a fundamental property characterizing the ability of statistical learning models to predict more accurately outside of their training domain, hence explaining performance deterioration for deep learning models in extrapolation settings. In addition to a theoretical analysis, we present empirical results showcasing the implications of this property on current deep learning architectures. Our results not only clarify the root causes of the extrapolation gap but also suggest directions for designing next-generation forecasting models capable of mastering extrapolation.</li>
</ul>

<h3>Title: Joint Learning of Pose Regression and Denoising Diffusion with Score Scaling Sampling for Category-level 6D Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Seunghyun Lee, Tae-Kyun Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04125">https://arxiv.org/abs/2510.04125</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04125">https://arxiv.org/pdf/2510.04125</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04125]] Joint Learning of Pose Regression and Denoising Diffusion with Score Scaling Sampling for Category-level 6D Pose Estimation(https://arxiv.org/abs/2510.04125)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Latest diffusion models have shown promising results in category-level 6D object pose estimation by modeling the conditional pose distribution with depth image input. The existing methods, however, suffer from slow convergence during training, learning its encoder with the diffusion denoising network in end-to-end fashion, and require an additional network that evaluates sampled pose hypotheses to filter out low-quality pose candidates. In this paper, we propose a novel pipeline that tackles these limitations by two key components. First, the proposed method pretrains the encoder with the direct pose regression head, and jointly learns the networks via the regression head and the denoising diffusion head, significantly accelerating training convergence while achieving higher accuracy. Second, sampling guidance via time-dependent score scaling is proposed s.t. the exploration-exploitation trade-off is effectively taken, eliminating the need for the additional evaluation network. The sampling guidance maintains multi-modal characteristics of symmetric objects at early denoising steps while ensuring high-quality pose generation at final steps. Extensive experiments on multiple benchmarks including REAL275, HouseCat6D, and ROPE, demonstrate that the proposed method, simple yet effective, achieves state-of-the-art accuracies even with single-pose inference, while being more efficient in both training and inference.</li>
</ul>

<h3>Title: Fine Tuning Methods for Low-resource Languages</h3>
<ul>
<li><strong>Authors: </strong>Tim Bakkenes, Daniel Wang, Anton Johansson</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04139">https://arxiv.org/abs/2510.04139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04139">https://arxiv.org/pdf/2510.04139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04139]] Fine Tuning Methods for Low-resource Languages(https://arxiv.org/abs/2510.04139)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rise of Large Language Models has not been inclusive of all cultures. The models are mostly trained on English texts and culture which makes them underperform in other languages and cultural contexts. By developing a generalizable method for preparing culturally relevant datasets and post-training the Gemma 2 model, this project aimed to increase the performance of Gemma 2 for an underrepresented language and showcase how others can do the same to unlock the power of Generative AI in their country and preserve their cultural heritage.</li>
</ul>

<h3>Title: Beyond Next-Token Prediction: A Performance Characterization of Diffusion versus Autoregressive Language Models</h3>
<ul>
<li><strong>Authors: </strong>Minseo Kim, Coleman Hooper, Aditya Tomar, Chenfeng Xu, Mehrdad Farajtabar, Michael W. Mahoney, Kurt Keutzer, Amir Gholami</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04146">https://arxiv.org/abs/2510.04146</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04146">https://arxiv.org/pdf/2510.04146</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04146]] Beyond Next-Token Prediction: A Performance Characterization of Diffusion versus Autoregressive Language Models(https://arxiv.org/abs/2510.04146)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have achieved state-of-the-art performance on a broad range of Natural Language Processing (NLP) tasks, including document processing and coding. Autoregressive Language Models (ARMs), which generate tokens sequentially conditioned on all previous tokens, have been the predominant paradigm for LLMs. However, while these networks have achieved high accuracy across a range of downstream tasks, they exhibit low arithmetic intensity due to the inherent sequential dependency with next-token prediction. Recently, Diffusion Language Models (DLMs) have emerged as a promising alternative architecture. DLMs generate output text in parallel, breaking the limitations of sequential dependency. However, the performance implications of DLMs relative to commonly deployed ARMs are not fully understood. In this work, we present a comprehensive performance study analyzing the performance characteristics of ARMs and DLMs, using both theoretical analysis and profiling data to characterize the trade-offs between these approaches. We illustrate that although DLMs exhibit higher arithmetic intensity compared to ARMs because of their capability to utilize parallelism across sequence lengths, they fail to scale effectively to longer contexts. We then explore DLMs with block-wise decoding, outlining how this approach allows for increased arithmetic intensity, while still scaling well to long contexts (similar to ARMs). We also show interesting trade-offs for batched inference, where we find that ARMs exhibit superior throughput, as they benefit more from parallelism across sequences in the batch. Finally, we highlight opportunities for accelerating DLM inference, and, in particular, highlight the importance of reducing the number of sampling steps for allowing open-source DLMs to provide improved latency relative to ARMs.</li>
</ul>

<h3>Title: Self Speculative Decoding for Diffusion Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yifeng Gao, Ziang Ji, Yuxuan Wang, Biqing Qi, Hanlin Xu, Linfeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04147">https://arxiv.org/abs/2510.04147</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04147">https://arxiv.org/pdf/2510.04147</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04147]] Self Speculative Decoding for Diffusion Large Language Models(https://arxiv.org/abs/2510.04147)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion-based Large Language Models (dLLMs) have emerged as a competitive alternative to autoregressive models, offering unique advantages through bidirectional attention and parallel generation paradigms. However, the generation results of current parallel decoding methods deviate from stepwise decoding, introducing potential performance degradation, which limits their practical deployment. To address this problem, we propose \textbf{S}elf \textbf{S}peculative \textbf{D}ecoding (SSD), a lossless inference acceleration method that leverages the dLLM itself as both speculative decoding drafter and verifier without auxiliary modules. SSD introduces a self-drafting mechanism where the model generates predictions for multiple positions, then verifies them through hierarchical verification trees in a single forward pass. Unlike traditional speculative decoding that requires separate draft models, SSD eliminates model redundancy and memory overhead by exploiting the dLLM's inherent parallel prediction capability for multiple positions. This self-speculative approach allows the model to progressively verify and accept multiple tokens in a single forward pass. Our experiments demonstrate that SSD achieves up to 3.46$\times$ speedup while keeping the output identical to stepwise decoding on open source models such as LLaDA and Dream. Code will be made publicly available on GitHub.</li>
</ul>

<h3>Title: ObCLIP: Oblivious CLoud-Device Hybrid Image Generation with Privacy Preservation</h3>
<ul>
<li><strong>Authors: </strong>Haoqi Wu, Wei Dai, Ming Xu, Li Wang, Qiang Yan</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04153">https://arxiv.org/abs/2510.04153</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04153">https://arxiv.org/pdf/2510.04153</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04153]] ObCLIP: Oblivious CLoud-Device Hybrid Image Generation with Privacy Preservation(https://arxiv.org/abs/2510.04153)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion Models have gained significant popularity due to their remarkable capabilities in image generation, albeit at the cost of intensive computation requirement. Meanwhile, despite their widespread deployment in inference services such as Midjourney, concerns about the potential leakage of sensitive information in uploaded user prompts have arisen. Existing solutions either lack rigorous privacy guarantees or fail to strike an effective balance between utility and efficiency. To bridge this gap, we propose ObCLIP, a plug-and-play safeguard that enables oblivious cloud-device hybrid generation. By oblivious, each input prompt is transformed into a set of semantically similar candidate prompts that differ only in sensitive attributes (e.g., gender, ethnicity). The cloud server processes all candidate prompts without knowing which one is the real one, thus preventing any prompt leakage. To mitigate server cost, only a small portion of denoising steps is performed upon the large cloud model. The intermediate latents are then sent back to the client, which selects the targeted latent and completes the remaining denoising using a small device model. Additionally, we analyze and incorporate several cache-based accelerations that leverage temporal and batch redundancy, effectively reducing computation cost with minimal utility degradation. Extensive experiments across multiple datasets demonstrate that ObCLIP provides rigorous privacy and comparable utility to cloud models with slightly increased server cost.</li>
</ul>

<h3>Title: BLADE: Bias-Linked Adaptive DEbiasing</h3>
<ul>
<li><strong>Authors: </strong>Piyush Arora, Navlika Singh, Vasubhya Diwan, Pratik Mazumder</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04174">https://arxiv.org/abs/2510.04174</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04174">https://arxiv.org/pdf/2510.04174</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04174]] BLADE: Bias-Linked Adaptive DEbiasing(https://arxiv.org/abs/2510.04174)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Neural networks have revolutionized numerous fields, yet they remain vulnerable to a critical flaw: the tendency to learn implicit biases, spurious correlations between certain attributes and target labels in training data. These biases are often more prevalent and easier to learn, causing models to rely on superficial patterns rather than task-relevant features necessary for generalization. Existing methods typically rely on strong assumptions, such as prior knowledge of these biases or access to bias-conflicting samples, i.e., samples that contradict spurious correlations and counterbalance bias-aligned samples, samples that conform to these spurious correlations. However, such assumptions are often impractical in real-world settings. We propose BLADE ({B}ias-{L}inked {A}daptive {DE}biasing), a generative debiasing framework that requires no prior knowledge of bias or bias-conflicting samples. BLADE first trains a generative model to translate images across bias domains while preserving task-relevant features. Then, it adaptively refines each image with its synthetic counterpart based on the image's susceptibility to bias. To encourage robust representations, BLADE aligns an image with its bias-translated synthetic counterpart that shares task-relevant features but differs in bias, while misaligning it with samples sharing the same bias. We evaluate BLADE on multiple benchmark datasets and show that it significantly outperforms state-of-the-art methods. Notably, it exceeds the closest baseline by an absolute margin of around 18% on the corrupted CIFAR-10 dataset under the worst group setting, establishing a new benchmark in bias mitigation and demonstrating its potential for developing more robust deep learning models without explicit supervision.</li>
</ul>

<h3>Title: Let Features Decide Their Own Solvers: Hybrid Feature Caching for Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Shikang Zheng, Guantao Chen, Qinming Zhou, Yuqi Lin, Lixuan He, Chang Zou, Peiliang Cai, Jiacheng Liu, Linfeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04188">https://arxiv.org/abs/2510.04188</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04188">https://arxiv.org/pdf/2510.04188</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04188]] Let Features Decide Their Own Solvers: Hybrid Feature Caching for Diffusion Transformers(https://arxiv.org/abs/2510.04188)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion Transformers offer state-of-the-art fidelity in image and video synthesis, but their iterative sampling process remains a major bottleneck due to the high cost of transformer forward passes at each timestep. To mitigate this, feature caching has emerged as a training-free acceleration technique that reuses or forecasts hidden representations. However, existing methods often apply a uniform caching strategy across all feature dimensions, ignoring their heterogeneous dynamic behaviors. Therefore, we adopt a new perspective by modeling hidden feature evolution as a mixture of ODEs across dimensions, and introduce HyCa, a Hybrid ODE solver inspired caching framework that applies dimension-wise caching strategies. HyCa achieves near-lossless acceleration across diverse domains and models, including 5.55 times speedup on FLUX, 5.56 times speedup on HunyuanVideo, 6.24 times speedup on Qwen-Image and Qwen-Image-Edit without retraining.</li>
</ul>

<h3>Title: World-To-Image: Grounding Text-to-Image Generation with Agent-Driven World Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Moo Hyun Son, Jintaek Oh, Sun Bin Mun, Jaechul Roh, Sehyun Choi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04201">https://arxiv.org/abs/2510.04201</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04201">https://arxiv.org/pdf/2510.04201</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04201]] World-To-Image: Grounding Text-to-Image Generation with Agent-Driven World Knowledge(https://arxiv.org/abs/2510.04201)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>While text-to-image (T2I) models can synthesize high-quality images, their performance degrades significantly when prompted with novel or out-of-distribution (OOD) entities due to inherent knowledge cutoffs. We introduce World-To-Image, a novel framework that bridges this gap by empowering T2I generation with agent-driven world knowledge. We design an agent that dynamically searches the web to retrieve images for concepts unknown to the base model. This information is then used to perform multimodal prompt optimization, steering powerful generative backbones toward an accurate synthesis. Critically, our evaluation goes beyond traditional metrics, utilizing modern assessments like LLMGrader and ImageReward to measure true semantic fidelity. Our experiments show that World-To-Image substantially outperforms state-of-the-art methods in both semantic alignment and visual aesthetics, achieving +8.1% improvement in accuracy-to-prompt on our curated NICE benchmark. Our framework achieves these results with high efficiency in less than three iterations, paving the way for T2I systems that can better reflect the ever-changing real world. Our demo code is available here\footnote{this https URL}.</li>
</ul>

<h3>Title: MASC: Boosting Autoregressive Image Generation with a Manifold-Aligned Semantic Clustering</h3>
<ul>
<li><strong>Authors: </strong>Lixuan He, Shikang Zheng, Linfeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04220">https://arxiv.org/abs/2510.04220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04220">https://arxiv.org/pdf/2510.04220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04220]] MASC: Boosting Autoregressive Image Generation with a Manifold-Aligned Semantic Clustering(https://arxiv.org/abs/2510.04220)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Autoregressive (AR) models have shown great promise in image generation, yet they face a fundamental inefficiency stemming from their core component: a vast, unstructured vocabulary of visual tokens. This conventional approach treats tokens as a flat vocabulary, disregarding the intrinsic structure of the token embedding space where proximity often correlates with semantic similarity. This oversight results in a highly complex prediction task, which hinders training efficiency and limits final generation quality. To resolve this, we propose Manifold-Aligned Semantic Clustering (MASC), a principled framework that constructs a hierarchical semantic tree directly from the codebook's intrinsic structure. MASC employs a novel geometry-aware distance metric and a density-driven agglomerative construction to model the underlying manifold of the token embeddings. By transforming the flat, high-dimensional prediction task into a structured, hierarchical one, MASC introduces a beneficial inductive bias that significantly simplifies the learning problem for the AR model. MASC is designed as a plug-and-play module, and our extensive experiments validate its effectiveness: it accelerates training by up to 57% and significantly improves generation quality, reducing the FID of LlamaGen-XL from 2.87 to 2.58. MASC elevates existing AR frameworks to be highly competitive with state-of-the-art methods, establishing that structuring the prediction space is as crucial as architectural innovation for scalable generative modeling.</li>
</ul>

<h3>Title: Scaling Sequence-to-Sequence Generative Neural Rendering</h3>
<ul>
<li><strong>Authors: </strong>Shikun Liu, Kam Woh Ng, Wonbong Jang, Jiadong Guo, Junlin Han, Haozhe Liu, Yiannis Douratsos, Juan C. Pérez, Zijian Zhou, Chi Phung, Tao Xiang, Juan-Manuel Pérez-Rúa</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04236">https://arxiv.org/abs/2510.04236</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04236">https://arxiv.org/pdf/2510.04236</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04236]] Scaling Sequence-to-Sequence Generative Neural Rendering(https://arxiv.org/abs/2510.04236)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present Kaleido, a family of generative models designed for photorealistic, unified object- and scene-level neural rendering. Kaleido operates on the principle that 3D can be regarded as a specialised sub-domain of video, expressed purely as a sequence-to-sequence image synthesis task. Through a systemic study of scaling sequence-to-sequence generative neural rendering, we introduce key architectural innovations that enable our model to: i) perform generative view synthesis without explicit 3D representations; ii) generate any number of 6-DoF target views conditioned on any number of reference views via a masked autoregressive framework; and iii) seamlessly unify 3D and video modelling within a single decoder-only rectified flow transformer. Within this unified framework, Kaleido leverages large-scale video data for pre-training, which significantly improves spatial consistency and reduces reliance on scarce, camera-labelled 3D datasets -- all without any architectural modifications. Kaleido sets a new state-of-the-art on a range of view synthesis benchmarks. Its zero-shot performance substantially outperforms other generative methods in few-view settings, and, for the first time, matches the quality of per-scene optimisation methods in many-view settings.</li>
</ul>

<h3>Title: Diffusion-Assisted Distillation for Self-Supervised Graph Representation Learning with MLPs</h3>
<ul>
<li><strong>Authors: </strong>Seong Jin Ahn, Myoung-Ho Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04241">https://arxiv.org/abs/2510.04241</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04241">https://arxiv.org/pdf/2510.04241</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04241]] Diffusion-Assisted Distillation for Self-Supervised Graph Representation Learning with MLPs(https://arxiv.org/abs/2510.04241)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, self-supervised</a></li>
<li><strong>Abstract: </strong>For large-scale applications, there is growing interest in replacing Graph Neural Networks (GNNs) with lightweight Multi-Layer Perceptrons (MLPs) via knowledge distillation. However, distilling GNNs for self-supervised graph representation learning into MLPs is more challenging. This is because the performance of self-supervised learning is more related to the model's inductive bias than supervised learning. This motivates us to design a new distillation method to bridge a huge capacity gap between GNNs and MLPs in self-supervised graph representation learning. In this paper, we propose \textbf{D}iffusion-\textbf{A}ssisted \textbf{D}istillation for \textbf{S}elf-supervised \textbf{G}raph representation learning with \textbf{M}LPs (DAD-SGM). The proposed method employs a denoising diffusion model as a teacher assistant to better distill the knowledge from the teacher GNN into the student MLP. This approach enhances the generalizability and robustness of MLPs in self-supervised graph representation learning. Extensive experiments demonstrate that DAD-SGM effectively distills the knowledge of self-supervised GNNs compared to state-of-the-art GNN-to-MLP distillation methods. Our implementation is available at this https URL.</li>
</ul>

<h3>Title: ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation</h3>
<ul>
<li><strong>Authors: </strong>Jay Zhangjie Wu, Xuanchi Ren, Tianchang Shen, Tianshi Cao, Kai He, Yifan Lu, Ruiyuan Gao, Enze Xie, Shiyi Lan, Jose M. Alvarez, Jun Gao, Sanja Fidler, Zian Wang, Huan Ling</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04290">https://arxiv.org/abs/2510.04290</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04290">https://arxiv.org/pdf/2510.04290</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04290]] ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation(https://arxiv.org/abs/2510.04290)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, in-context</a></li>
<li><strong>Abstract: </strong>Recent advances in large generative models have significantly advanced image editing and in-context image generation, yet a critical gap remains in ensuring physical consistency, where edited objects must remain coherent. This capability is especially vital for world simulation related tasks. In this paper, we present ChronoEdit, a framework that reframes image editing as a video generation problem. First, ChronoEdit treats the input and edited images as the first and last frames of a video, allowing it to leverage large pretrained video generative models that capture not only object appearance but also the implicit physics of motion and interaction through learned temporal consistency. Second, ChronoEdit introduces a temporal reasoning stage that explicitly performs editing at inference time. Under this setting, the target frame is jointly denoised with reasoning tokens to imagine a plausible editing trajectory that constrains the solution space to physically viable transformations. The reasoning tokens are then dropped after a few steps to avoid the high computational cost of rendering a full video. To validate ChronoEdit, we introduce PBench-Edit, a new benchmark of image-prompt pairs for contexts that require physical consistency, and demonstrate that ChronoEdit surpasses state-of-the-art baselines in both visual fidelity and physical plausibility. Code and models for both the 14B and 2B variants of ChronoEdit will be released on the project page: this https URL</li>
</ul>

<h3>Title: FoilDiff: A Hybrid Transformer Backbone for Diffusion-based Modelling of 2D Airfoil Flow Fields</h3>
<ul>
<li><strong>Authors: </strong>Kenechukwu Ogbuagu, Sepehr Maleki, Giuseppe Bruni, Senthil Krishnababu</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.flu-dyn</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04325">https://arxiv.org/abs/2510.04325</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04325">https://arxiv.org/pdf/2510.04325</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04325]] FoilDiff: A Hybrid Transformer Backbone for Diffusion-based Modelling of 2D Airfoil Flow Fields(https://arxiv.org/abs/2510.04325)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The accurate prediction of flow fields around airfoils is crucial for aerodynamic design and optimisation. Computational Fluid Dynamics (CFD) models are effective but computationally expensive, thus inspiring the development of surrogate models to enable quicker predictions. These surrogate models can be based on deep learning architectures, such as Convolutional Neural Networks (CNNs), Graph Neural Networks (GNNs), and Diffusion Models (DMs). Diffusion models have shown significant promise in predicting complex flow fields. In this work, we propose FoilDiff, a diffusion-based surrogate model with a hybrid-backbone denoising network. This hybrid design combines the power of convolutional feature extraction and transformer-based global attention to generate more adaptable and accurate representations of flow structures. FoilDiff takes advantage of Denoising Diffusion Implicit Model (DDIM) sampling to optimise the efficiency of the sampling process at no additional cost to model generalisation. We used encoded representations of Reynolds number, angle of attack, and airfoil geometry to define the input space for generalisation across a wide range of aerodynamic conditions. When evaluated against state-of-the-art models, FoilDiff shows significant performance improvements, with mean prediction errors reducing by up to 85\% on the same datasets. The results have demonstrated that FoilDiff can provide both more accurate predictions and better-calibrated predictive uncertainty than existing diffusion-based models.</li>
</ul>

<h3>Title: DoRAN: Stabilizing Weight-Decomposed Low-Rank Adaptation via Noise Injection and Auxiliary Networks</h3>
<ul>
<li><strong>Authors: </strong>Nghiem T. Diep, Hien Dang, Tuan Truong, Tan Dinh, Huy Nguyen, Nhat Ho</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04331">https://arxiv.org/abs/2510.04331</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04331">https://arxiv.org/pdf/2510.04331</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04331]] DoRAN: Stabilizing Weight-Decomposed Low-Rank Adaptation via Noise Injection and Auxiliary Networks(https://arxiv.org/abs/2510.04331)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Parameter-efficient fine-tuning (PEFT) methods have become the standard paradigm for adapting large-scale models. Among these techniques, Weight-Decomposed Low-Rank Adaptation (DoRA) has been shown to improve both the learning capacity and training stability of the vanilla Low-Rank Adaptation (LoRA) method by explicitly decomposing pre-trained weights into magnitude and directional components. In this work, we propose DoRAN, a new variant of DoRA designed to further stabilize training and boost the sample efficiency of DoRA. Our approach includes two key stages: (i) injecting noise into the denominator of DoRA's weight decomposition, which serves as an adaptive regularizer to mitigate instabilities; and (ii) replacing static low-rank matrices with auxiliary networks that generate them dynamically, enabling parameter coupling across layers and yielding better sample efficiency in both theory and practice. Comprehensive experiments on vision and language benchmarks show that DoRAN consistently outperforms LoRA, DoRA, and other PEFT baselines. These results underscore the effectiveness of combining stabilization through noise-based regularization with network-based parameter generation, offering a promising direction for robust and efficient fine-tuning of foundation models.</li>
</ul>

<h3>Title: Evaluation of Clinical Trials Reporting Quality using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mathieu Laï-king, Patrick Paroubek</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04338">https://arxiv.org/abs/2510.04338</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04338">https://arxiv.org/pdf/2510.04338</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04338]] Evaluation of Clinical Trials Reporting Quality using Large Language Models(https://arxiv.org/abs/2510.04338)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Reporting quality is an important topic in clinical trial research articles, as it can impact clinical decisions. In this article, we test the ability of large language models to assess the reporting quality of this type of article using the Consolidated Standards of Reporting Trials (CONSORT). We create CONSORT-QA, an evaluation corpus from two studies on abstract reporting quality with CONSORT-abstract standards. We then evaluate the ability of different large generative language models (from the general domain or adapted to the biomedical domain) to correctly assess CONSORT criteria with different known prompting methods, including Chain-of-thought. Our best combination of model and prompting method achieves 85% accuracy. Using Chain-of-thought adds valuable information on the model's reasoning for completing the task.</li>
</ul>

<h3>Title: Critical appraisal of artificial intelligence for rare-event recognition: principles and pharmacovigilance case studies</h3>
<ul>
<li><strong>Authors: </strong>G. Niklas Noren, Eva-Lisa Meldau, Johan Ellenius</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04341">https://arxiv.org/abs/2510.04341</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04341">https://arxiv.org/pdf/2510.04341</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04341]] Critical appraisal of artificial intelligence for rare-event recognition: principles and pharmacovigilance case studies(https://arxiv.org/abs/2510.04341)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Many high-stakes AI applications target low-prevalence events, where apparent accuracy can conceal limited real-world value. Relevant AI models range from expert-defined rules and traditional machine learning to generative LLMs constrained for classification. We outline key considerations for critical appraisal of AI in rare-event recognition, including problem framing and test set design, prevalence-aware statistical evaluation, robustness assessment, and integration into human workflows. In addition, we propose an approach to structured case-level examination (SCLE), to complement statistical performance evaluation, and a comprehensive checklist to guide procurement or development of AI models for rare-event recognition. We instantiate the framework in pharmacovigilance, drawing on three studies: rule-based retrieval of pregnancy-related reports; duplicate detection combining machine learning with probabilistic record linkage; and automated redaction of person names using an LLM. We highlight pitfalls specific to the rare-event setting including optimism from unrealistic class balance and lack of difficult positive controls in test sets - and show how cost-sensitive targets align model performance with operational value. While grounded in pharmacovigilance practice, the principles generalize to domains where positives are scarce and error costs may be asymmetric.</li>
</ul>

<h3>Title: Unmasking Backdoors: An Explainable Defense via Gradient-Attention Anomaly Scoring for Pre-trained Language Models</h3>
<ul>
<li><strong>Authors: </strong>Anindya Sundar Das, Kangjie Chen, Monowar Bhuyan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04347">https://arxiv.org/abs/2510.04347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04347">https://arxiv.org/pdf/2510.04347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04347]] Unmasking Backdoors: An Explainable Defense via Gradient-Attention Anomaly Scoring for Pre-trained Language Models(https://arxiv.org/abs/2510.04347)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Pre-trained language models have achieved remarkable success across a wide range of natural language processing (NLP) tasks, particularly when fine-tuned on large, domain-relevant datasets. However, they remain vulnerable to backdoor attacks, where adversaries embed malicious behaviors using trigger patterns in the training data. These triggers remain dormant during normal usage, but, when activated, can cause targeted misclassifications. In this work, we investigate the internal behavior of backdoored pre-trained encoder-based language models, focusing on the consistent shift in attention and gradient attribution when processing poisoned inputs; where the trigger token dominates both attention and gradient signals, overriding the surrounding context. We propose an inference-time defense that constructs anomaly scores by combining token-level attention and gradient information. Extensive experiments on text classification tasks across diverse backdoor attack scenarios demonstrate that our method significantly reduces attack success rates compared to existing baselines. Furthermore, we provide an interpretability-driven analysis of the scoring mechanism, shedding light on trigger localization and the robustness of the proposed defense.</li>
</ul>

<h3>Title: Diffusion^2: Dual Diffusion Model with Uncertainty-Aware Adaptive Noise for Momentary Trajectory Prediction</h3>
<ul>
<li><strong>Authors: </strong>Yuhao Luo, Yuang Zhang, Kehua Chen, Xinyu Zheng, Shucheng Zhang, Sikai Chen, Yinhai Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04365">https://arxiv.org/abs/2510.04365</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04365">https://arxiv.org/pdf/2510.04365</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04365]] Diffusion^2: Dual Diffusion Model with Uncertainty-Aware Adaptive Noise for Momentary Trajectory Prediction(https://arxiv.org/abs/2510.04365)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Accurate pedestrian trajectory prediction is crucial for ensuring safety and efficiency in autonomous driving and human-robot interaction scenarios. Earlier studies primarily utilized sufficient observational data to predict future trajectories. However, in real-world scenarios, such as pedestrians suddenly emerging from blind spots, sufficient observational data is often unavailable (i.e. momentary trajectory), making accurate prediction challenging and increasing the risk of traffic accidents. Therefore, advancing research on pedestrian trajectory prediction under extreme scenarios is critical for enhancing traffic safety. In this work, we propose a novel framework termed Diffusion^2, tailored for momentary trajectory prediction. Diffusion^2 consists of two sequentially connected diffusion models: one for backward prediction, which generates unobserved historical trajectories, and the other for forward prediction, which forecasts future trajectories. Given that the generated unobserved historical trajectories may introduce additional noise, we propose a dual-head parameterization mechanism to estimate their aleatoric uncertainty and design a temporally adaptive noise module that dynamically modulates the noise scale in the forward diffusion process. Empirically, Diffusion^2 sets a new state-of-the-art in momentary trajectory prediction on ETH/UCY and Stanford Drone datasets.</li>
</ul>

<h3>Title: CodeFormer++: Blind Face Restoration Using Deformable Registration and Deep Metric Learning</h3>
<ul>
<li><strong>Authors: </strong>Venkata Bharath Reddy Reddem, Akshay P Sarashetti, Ranjith Merugu, Amit Satish Unde</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04410">https://arxiv.org/abs/2510.04410</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04410">https://arxiv.org/pdf/2510.04410</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04410]] CodeFormer++: Blind Face Restoration Using Deformable Registration and Deep Metric Learning(https://arxiv.org/abs/2510.04410)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Blind face restoration (BFR) has attracted increasing attention with the rise of generative methods. Most existing approaches integrate generative priors into the restoration pro- cess, aiming to jointly address facial detail generation and identity preservation. However, these methods often suffer from a trade-off between visual quality and identity fidelity, leading to either identity distortion or suboptimal degradation removal. In this paper, we present CodeFormer++, a novel framework that maximizes the utility of generative priors for high-quality face restoration while preserving identity. We decompose BFR into three sub-tasks: (i) identity- preserving face restoration, (ii) high-quality face generation, and (iii) dynamic fusion of identity features with realistic texture details. Our method makes three key contributions: (1) a learning-based deformable face registration module that semantically aligns generated and restored faces; (2) a texture guided restoration network to dynamically extract and transfer the texture of generated face to boost the quality of identity-preserving restored face; and (3) the integration of deep metric learning for BFR with the generation of informative positive and hard negative samples to better fuse identity- preserving and generative features. Extensive experiments on real-world and synthetic datasets demonstrate that, the pro- posed CodeFormer++ achieves superior performance in terms of both visual fidelity and identity consistency.</li>
</ul>

<h3>Title: Fractional Heat Kernel for Semi-Supervised Graph Learning with Small Training Sample Size</h3>
<ul>
<li><strong>Authors: </strong>Farid Bozorgnia, Vyacheslav Kungurtsev, Shirali Kadyrov, Mohsen Yousefnezhad</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04440">https://arxiv.org/abs/2510.04440</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04440">https://arxiv.org/pdf/2510.04440</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04440]] Fractional Heat Kernel for Semi-Supervised Graph Learning with Small Training Sample Size(https://arxiv.org/abs/2510.04440)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this work, we introduce novel algorithms for label propagation and self-training using fractional heat kernel dynamics with a source term. We motivate the methodology through the classical correspondence of information theory with the physics of parabolic evolution equations. We integrate the fractional heat kernel into Graph Neural Network architectures such as Graph Convolutional Networks and Graph Attention, enhancing their expressiveness through adaptive, multi-hop diffusion. By applying Chebyshev polynomial approximations, large graphs become computationally feasible. Motivating variational formulations demonstrate that by extending the classical diffusion model to fractional powers of the Laplacian, nonlocal interactions deliver more globally diffusing labels. The particular balance between supervision of known labels and diffusion across the graph is particularly advantageous in the case where only a small number of labeled training examples are present. We demonstrate the effectiveness of this approach on standard datasets.</li>
</ul>

<h3>Title: REAR: Rethinking Visual Autoregressive Models via Generator-Tokenizer Consistency Regularization</h3>
<ul>
<li><strong>Authors: </strong>Qiyuan He, Yicong Li, Haotian Ye, Jinghao Wang, Xinyao Liao, Pheng-Ann Heng, Stefano Ermon, James Zou, Angela Yao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04450">https://arxiv.org/abs/2510.04450</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04450">https://arxiv.org/pdf/2510.04450</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04450]] REAR: Rethinking Visual Autoregressive Models via Generator-Tokenizer Consistency Regularization(https://arxiv.org/abs/2510.04450)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Visual autoregressive (AR) generation offers a promising path toward unifying vision and language models, yet its performance remains suboptimal against diffusion models. Prior work often attributes this gap to tokenizer limitations and rasterization ordering. In this work, we identify a core bottleneck from the perspective of generator-tokenizer inconsistency, i.e., the AR-generated tokens may not be well-decoded by the tokenizer. To address this, we propose reAR, a simple training strategy introducing a token-wise regularization objective: when predicting the next token, the causal transformer is also trained to recover the visual embedding of the current token and predict the embedding of the target token under a noisy context. It requires no changes to the tokenizer, generation order, inference pipeline, or external models. Despite its simplicity, reAR substantially improves performance. On ImageNet, it reduces gFID from 3.02 to 1.86 and improves IS to 316.9 using a standard rasterization-based tokenizer. When applied to advanced tokenizers, it achieves a gFID of 1.42 with only 177M parameters, matching the performance with larger state-of-the-art diffusion models (675M).</li>
</ul>

<h3>Title: GenQuest: An LLM-based Text Adventure Game for Language Learners</h3>
<ul>
<li><strong>Authors: </strong>Qiao Wang, Adnan Labib, Robert Swier, Michael Hofmeyr, Zheng Yuan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04498">https://arxiv.org/abs/2510.04498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04498">https://arxiv.org/pdf/2510.04498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04498]] GenQuest: An LLM-based Text Adventure Game for Language Learners(https://arxiv.org/abs/2510.04498)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, in-context</a></li>
<li><strong>Abstract: </strong>GenQuest is a generative text adventure game that leverages Large Language Models (LLMs) to facilitate second language learning through immersive, interactive storytelling. The system engages English as a Foreign Language (EFL) learners in a collaborative "choose-your-own-adventure" style narrative, dynamically generated in response to learner choices. Game mechanics such as branching decision points and story milestones are incorporated to maintain narrative coherence while allowing learner-driven plot development. Key pedagogical features include content generation tailored to each learner's proficiency level, and a vocabulary assistant that provides in-context explanations of learner-queried text strings, ranging from words and phrases to sentences. Findings from a pilot study with university EFL students in China indicate promising vocabulary gains and positive user perceptions. Also discussed are suggestions from participants regarding the narrative length and quality, and the request for multi-modal content such as illustrations.</li>
</ul>

<h3>Title: Asynchronous Denoising Diffusion Models for Aligning Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Zijing Hu, Yunze Tong, Fengda Zhang, Junkun Yuan, Jun Xiao, Kun Kuang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04504">https://arxiv.org/abs/2510.04504</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04504">https://arxiv.org/pdf/2510.04504</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04504]] Asynchronous Denoising Diffusion Models for Aligning Text-to-Image Generation(https://arxiv.org/abs/2510.04504)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have achieved impressive results in generating high-quality images. Yet, they often struggle to faithfully align the generated images with the input prompts. This limitation arises from synchronous denoising, where all pixels simultaneously evolve from random noise to clear images. As a result, during generation, the prompt-related regions can only reference the unrelated regions at the same noise level, failing to obtain clear context and ultimately impairing text-to-image alignment. To address this issue, we propose asynchronous diffusion models -- a novel framework that allocates distinct timesteps to different pixels and reformulates the pixel-wise denoising process. By dynamically modulating the timestep schedules of individual pixels, prompt-related regions are denoised more gradually than unrelated regions, thereby allowing them to leverage clearer inter-pixel context. Consequently, these prompt-related regions achieve better alignment in the final images. Extensive experiments demonstrate that our asynchronous diffusion models can significantly improve text-to-image alignment across diverse prompts. The code repository for this work is available at this https URL.</li>
</ul>

<h3>Title: GRACE: Generative Representation Learning via Contrastive Policy Optimization</h3>
<ul>
<li><strong>Authors: </strong>Jiashuo Sun, Shixuan Liu, Zhaochen Su, Xianrui Zhong, Pengcheng Jiang, Bowen Jin, Peiran Li, Weijia Shi, Jiawei Han</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04506">https://arxiv.org/abs/2510.04506</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04506">https://arxiv.org/pdf/2510.04506</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04506]] GRACE: Generative Representation Learning via Contrastive Policy Optimization(https://arxiv.org/abs/2510.04506)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Prevailing methods for training Large Language Models (LLMs) as text encoders rely on contrastive losses that treat the model as a black box function, discarding its generative and reasoning capabilities in favor of static embeddings. We introduce GRACE (Generative Representation Learning via Contrastive Policy Optimization), a novel framework that reimagines contrastive signals not as losses to be minimized, but as rewards that guide a generative policy. In GRACE, the LLM acts as a policy that produces explicit, human-interpretable rationales--structured natural language explanations of its semantic understanding. These rationales are then encoded into high-quality embeddings via mean pooling. Using policy gradient optimization, we train the model with a multi-component reward function that maximizes similarity between query positive pairs and minimizes similarity with negatives. This transforms the LLM from an opaque encoder into an interpretable agent whose reasoning process is transparent and inspectable. On MTEB benchmark, GRACE yields broad cross category gains: averaged over four backbones, the supervised setting improves overall score by 11.5% over base models, and the unsupervised variant adds 6.9%, while preserving general capabilities. This work treats contrastive objectives as rewards over rationales, unifying representation learning with generation to produce stronger embeddings and transparent rationales. The model, data and code are available at this https URL.</li>
</ul>

<h3>Title: Toward a Unified Geometry Understanding: Riemannian Diffusion Framework for Graph Generation and Prediction</h3>
<ul>
<li><strong>Authors: </strong>Yisen Gao, Xingcheng Fu, Qingyun Sun, Jianxin Li, Xianxian Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04522">https://arxiv.org/abs/2510.04522</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04522">https://arxiv.org/pdf/2510.04522</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04522]] Toward a Unified Geometry Understanding: Riemannian Diffusion Framework for Graph Generation and Prediction(https://arxiv.org/abs/2510.04522)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Graph diffusion models have made significant progress in learning structured graph data and have demonstrated strong potential for predictive tasks. Existing approaches typically embed node, edge, and graph-level features into a unified latent space, modeling prediction tasks including classification and regression as a form of conditional generation. However, due to the non-Euclidean nature of graph data, features of different curvatures are entangled in the same latent space without releasing their geometric potential. To address this issue, we aim to construt an ideal Riemannian diffusion model to capture distinct manifold signatures of complex graph data and learn their distribution. This goal faces two challenges: numerical instability caused by exponential mapping during the encoding proces and manifold deviation during diffusion generation. To address these challenges, we propose GeoMancer: a novel Riemannian graph diffusion framework for both generation and prediction tasks. To mitigate numerical instability, we replace exponential mapping with an isometric-invariant Riemannian gyrokernel approach and decouple multi-level features onto their respective task-specific manifolds to learn optimal representations. To address manifold deviation, we introduce a manifold-constrained diffusion method and a self-guided strategy for unconditional generation, ensuring that the generated data remains aligned with the manifold signature. Extensive experiments validate the effectiveness of our approach, demonstrating superior performance across a variety of tasks.</li>
</ul>

<h3>Title: Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in Masked Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Satoshi Hayakawa, Yuhta Takida, Masaaki Imaizumi, Hiromi Wakaki, Yuki Mitsufuji</a></li>
<li><strong>Subjects: </strong>cs.LG, math.PR, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04525">https://arxiv.org/abs/2510.04525</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04525">https://arxiv.org/pdf/2510.04525</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04525]] Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in Masked Diffusion(https://arxiv.org/abs/2510.04525)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Masked diffusion models have shown promising performance in generating high-quality samples in a wide range of domains, but accelerating their sampling process remains relatively underexplored. To investigate efficient samplers for masked diffusion, this paper theoretically analyzes the MaskGIT sampler for image modeling, revealing its implicit temperature sampling mechanism. Through this analysis, we introduce the "moment sampler," an asymptotically equivalent but more tractable and interpretable alternative to MaskGIT, which employs a "choose-then-sample" approach by selecting unmasking positions before sampling tokens. In addition, we improve the efficiency of choose-then-sample algorithms through two key innovations: a partial caching technique for transformers that approximates longer sampling trajectories without proportional computational cost, and a hybrid approach formalizing the exploration-exploitation trade-off in adaptive unmasking. Experiments in image and text domains demonstrate our theory as well as the efficiency of our proposed methods, advancing both theoretical understanding and practical implementation of masked diffusion samplers.</li>
</ul>

<h3>Title: TAG:Tangential Amplifying Guidance for Hallucination-Resistant Diffusion Sampling</h3>
<ul>
<li><strong>Authors: </strong>Hyunmin Cho, Donghoon Ahn, Susung Hong, Jee Eun Kim, Seungryong Kim, Kyong Hwan Jin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04533">https://arxiv.org/abs/2510.04533</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04533">https://arxiv.org/pdf/2510.04533</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04533]] TAG:Tangential Amplifying Guidance for Hallucination-Resistant Diffusion Sampling(https://arxiv.org/abs/2510.04533)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent diffusion models achieve the state-of-the-art performance in image generation, but often suffer from semantic inconsistencies or hallucinations. While various inference-time guidance methods can enhance generation, they often operate indirectly by relying on external signals or architectural modifications, which introduces additional computational overhead. In this paper, we propose Tangential Amplifying Guidance (TAG), a more efficient and direct guidance method that operates solely on trajectory signals without modifying the underlying diffusion model. TAG leverages an intermediate sample as a projection basis and amplifies the tangential components of the estimated scores with respect to this basis to correct the sampling trajectory. We formalize this guidance process by leveraging a first-order Taylor expansion, which demonstrates that amplifying the tangential component steers the state toward higher-probability regions, thereby reducing inconsistencies and enhancing sample quality. TAG is a plug-and-play, architecture-agnostic module that improves diffusion sampling fidelity with minimal computational addition, offering a new perspective on diffusion guidance.</li>
</ul>

<h3>Title: Post-training quantization of vision encoders needs prefixing registers</h3>
<ul>
<li><strong>Authors: </strong>Seunghyeon Kim, Jinho Kim, Taesun Yeom, Wonpyo Park, Kyuyeun Kim, Jaeho Lee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04547">https://arxiv.org/abs/2510.04547</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04547">https://arxiv.org/pdf/2510.04547</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04547]] Post-training quantization of vision encoders needs prefixing registers(https://arxiv.org/abs/2510.04547)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Transformer-based vision encoders -- such as CLIP -- are central to multimodal intelligence, powering applications from autonomous web agents to robotic control. Since these applications often demand real-time processing of massive visual data, reducing the inference cost of vision encoders is critical. Post-training quantization offers a practical path, but remains challenging even at 8-bit precision due to massive-scale activations (i.e., outliers). In this work, we propose $\textit{RegCache}$, a training-free algorithm to mitigate outliers in vision encoders, enabling quantization with significantly smaller accuracy drops. The proposed RegCache introduces outlier-prone yet semantically meaningless prefix tokens to the target vision encoder, which prevents other tokens from having outliers. Notably, we observe that outliers in vision encoders behave differently from those in language models, motivating two technical innovations: middle-layer prefixing and token deletion. Experiments show that our method consistently improves the accuracy of quantized models across both text-supervised and self-supervised vision encoders.</li>
</ul>

<h3>Title: GILT: An LLM-Free, Tuning-Free Graph Foundational Model for In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Weishuo Ma, Yanbo Wang, Xiyuan Wang, Lei Zou, Muhan Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04567">https://arxiv.org/abs/2510.04567</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04567">https://arxiv.org/pdf/2510.04567</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04567]] GILT: An LLM-Free, Tuning-Free Graph Foundational Model for In-Context Learning(https://arxiv.org/abs/2510.04567)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) are powerful tools for precessing relational data but often struggle to generalize to unseen graphs, giving rise to the development of Graph Foundational Models (GFMs). However, current GFMs are challenged by the extreme heterogeneity of graph data, where each graph can possess a unique feature space, label set, and topology. To address this, two main paradigms have emerged. The first leverages Large Language Models (LLMs), but is fundamentally text-dependent, thus struggles to handle the numerical features in vast graphs. The second pre-trains a structure-based model, but the adaptation to new tasks typically requires a costly, per-graph tuning stage, creating a critical efficiency bottleneck. In this work, we move beyond these limitations and introduce \textbf{G}raph \textbf{I}n-context \textbf{L}earning \textbf{T}ransformer (GILT), a framework built on an LLM-free and tuning-free architecture. GILT introduces a novel token-based framework for in-context learning (ICL) on graphs, reframing classification tasks spanning node, edge and graph levels in a unified framework. This mechanism is the key to handling heterogeneity, as it is designed to operate on generic numerical features. Further, its ability to understand class semantics dynamically from the context enables tuning-free adaptation. Comprehensive experiments show that GILT achieves stronger few-shot performance with significantly less time than LLM-based or tuning-based baselines, validating the effectiveness of our approach.</li>
</ul>

<h3>Title: LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Haoqiang Kang, Yizhe Zhang, Nikki Lijing Kuang, Nicklas Majamaki, Navdeep Jaitly, Yi-An Ma, Lianhui Qin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04573">https://arxiv.org/abs/2510.04573</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04573">https://arxiv.org/pdf/2510.04573</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04573]] LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning(https://arxiv.org/abs/2510.04573)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) demonstrate their reasoning ability through chain-of-thought (CoT) generation. However, LLM's autoregressive decoding may limit the ability to revisit and refine earlier tokens in a holistic manner, which can also lead to inefficient exploration for diverse solutions. In this paper, we propose LaDiR (Latent Diffusion Reasoner), a novel reasoning framework that unifies the expressiveness of continuous latent representation with the iterative refinement capabilities of latent diffusion models for an existing LLM. We first construct a structured latent reasoning space using a Variational Autoencoder (VAE) that encodes text reasoning steps into blocks of thought tokens, preserving semantic information and interpretability while offering compact but expressive representations. Subsequently, we utilize a latent diffusion model that learns to denoise a block of latent thought tokens with a blockwise bidirectional attention mask, enabling longer horizon and iterative refinement with adaptive test-time compute. This design allows efficient parallel generation of diverse reasoning trajectories, allowing the model to plan and revise the reasoning process holistically. We conduct evaluations on a suite of mathematical reasoning and planning benchmarks. Empirical results show that LaDiR consistently improves accuracy, diversity, and interpretability over existing autoregressive, diffusion-based, and latent reasoning methods, revealing a new paradigm for text reasoning with latent diffusion.</li>
</ul>

<h3>Title: SONA: Learning Conditional, Unconditional, and Mismatching-Aware Discriminator</h3>
<ul>
<li><strong>Authors: </strong>Yuhta Takida, Satoshi Hayakawa, Takashi Shibuya, Masaaki Imaizumi, Naoki Murata, Bac Nguyen, Toshimitsu Uesaka, Chieh-Hsin Lai, Yuki Mitsufuji</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04576">https://arxiv.org/abs/2510.04576</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04576">https://arxiv.org/pdf/2510.04576</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04576]] SONA: Learning Conditional, Unconditional, and Mismatching-Aware Discriminator(https://arxiv.org/abs/2510.04576)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Deep generative models have made significant advances in generating complex content, yet conditional generation remains a fundamental challenge. Existing conditional generative adversarial networks often struggle to balance the dual objectives of assessing authenticity and conditional alignment of input samples within their conditional discriminators. To address this, we propose a novel discriminator design that integrates three key capabilities: unconditional discrimination, matching-aware supervision to enhance alignment sensitivity, and adaptive weighting to dynamically balance all objectives. Specifically, we introduce Sum of Naturalness and Alignment (SONA), which employs separate projections for naturalness (authenticity) and alignment in the final layer with an inductive bias, supported by dedicated objective functions and an adaptive weighting mechanism. Extensive experiments on class-conditional generation tasks show that \ours achieves superior sample quality and conditional alignment compared to state-of-the-art methods. Furthermore, we demonstrate its effectiveness in text-to-image generation, confirming the versatility and robustness of our approach.</li>
</ul>

<h3>Title: Improved probabilistic regression using diffusion models</h3>
<ul>
<li><strong>Authors: </strong>Carlo Kneissl, Christopher Bülte, Philipp Scholl, Gitta Kutyniok</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04583">https://arxiv.org/abs/2510.04583</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04583">https://arxiv.org/pdf/2510.04583</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04583]] Improved probabilistic regression using diffusion models(https://arxiv.org/abs/2510.04583)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Probabilistic regression models the entire predictive distribution of a response variable, offering richer insights than classical point estimates and directly allowing for uncertainty quantification. While diffusion-based generative models have shown remarkable success in generating complex, high-dimensional data, their usage in general regression tasks often lacks uncertainty-related evaluation and remains limited to domain-specific applications. We propose a novel diffusion-based framework for probabilistic regression that learns predictive distributions in a nonparametric way. More specifically, we propose to model the full distribution of the diffusion noise, enabling adaptation to diverse tasks and enhanced uncertainty quantification. We investigate different noise parameterizations, analyze their trade-offs, and evaluate our framework across a broad range of regression tasks, covering low- and high-dimensional settings. For several experiments, our approach shows superior performance against existing baselines, while delivering calibrated uncertainty estimates, demonstrating its versatility as a tool for probabilistic prediction.</li>
</ul>

<h3>Title: Pathology-CoT: Learning Visual Chain-of-Thought Agent from Expert Whole Slide Image Diagnosis Behavior</h3>
<ul>
<li><strong>Authors: </strong>Sheng Wang, Ruiming Wu, Charles Herndon, Yihang Liu, Shunsuke Koga, Jeanne Shen, Zhi Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04587">https://arxiv.org/abs/2510.04587</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04587">https://arxiv.org/pdf/2510.04587</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04587]] Pathology-CoT: Learning Visual Chain-of-Thought Agent from Expert Whole Slide Image Diagnosis Behavior(https://arxiv.org/abs/2510.04587)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Diagnosing a whole-slide image is an interactive, multi-stage process involving changes in magnification and movement between fields. Although recent pathology foundation models are strong, practical agentic systems that decide what field to examine next, adjust magnification, and deliver explainable diagnoses are still lacking. The blocker is data: scalable, clinically aligned supervision of expert viewing behavior that is tacit and experience-based, not written in textbooks or online, and therefore absent from large language model training. We introduce the AI Session Recorder, which works with standard WSI viewers to unobtrusively record routine navigation and convert the viewer logs into standardized behavioral commands (inspect or peek at discrete magnifications) and bounding boxes. A lightweight human-in-the-loop review turns AI-drafted rationales into the Pathology-CoT dataset, a form of paired "where to look" and "why it matters" supervision produced at roughly six times lower labeling time. Using this behavioral data, we build Pathologist-o3, a two-stage agent that first proposes regions of interest and then performs behavior-guided reasoning. On gastrointestinal lymph-node metastasis detection, it achieved 84.5% precision, 100.0% recall, and 75.4% accuracy, exceeding the state-of-the-art OpenAI o3 model and generalizing across backbones. To our knowledge, this constitutes one of the first behavior-grounded agentic systems in pathology. Turning everyday viewer logs into scalable, expert-validated supervision, our framework makes agentic pathology practical and establishes a path to human-aligned, upgradeable clinical AI.</li>
</ul>

<h3>Title: Evaluating LLMs for Demographic-Targeted Social Bias Detection: A Comprehensive Benchmark Study</h3>
<ul>
<li><strong>Authors: </strong>Ayan Majumdar, Feihao Chen, Jinghui Li, Xiaozhen Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04641">https://arxiv.org/abs/2510.04641</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04641">https://arxiv.org/pdf/2510.04641</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04641]] Evaluating LLMs for Demographic-Targeted Social Bias Detection: A Comprehensive Benchmark Study(https://arxiv.org/abs/2510.04641)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large-scale web-scraped text corpora used to train general-purpose AI models often contain harmful demographic-targeted social biases, creating a regulatory need for data auditing and developing scalable bias-detection methods. Although prior work has investigated biases in text datasets and related detection methods, these studies remain narrow in scope. They typically focus on a single content type (e.g., hate speech), cover limited demographic axes, overlook biases affecting multiple demographics simultaneously, and analyze limited techniques. Consequently, practitioners lack a holistic understanding of the strengths and limitations of recent large language models (LLMs) for automated bias detection. In this study, we present a comprehensive evaluation framework aimed at English texts to assess the ability of LLMs in detecting demographic-targeted social biases. To align with regulatory requirements, we frame bias detection as a multi-label task using a demographic-focused taxonomy. We then conduct a systematic evaluation with models across scales and techniques, including prompting, in-context learning, and fine-tuning. Using twelve datasets spanning diverse content types and demographics, our study demonstrates the promise of fine-tuned smaller models for scalable detection. However, our analyses also expose persistent gaps across demographic axes and multi-demographic targeted biases, underscoring the need for more effective and scalable auditing frameworks.</li>
</ul>

<h3>Title: ConceptSplit: Decoupled Multi-Concept Personalization of Diffusion Models via Token-wise Adaptation and Attention Disentanglement</h3>
<ul>
<li><strong>Authors: </strong>Habin Lim, Yeongseob Won, Juwon Seo, Gyeong-Moon Park</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04668">https://arxiv.org/abs/2510.04668</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04668">https://arxiv.org/pdf/2510.04668</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04668]] ConceptSplit: Decoupled Multi-Concept Personalization of Diffusion Models via Token-wise Adaptation and Attention Disentanglement(https://arxiv.org/abs/2510.04668)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In recent years, multi-concept personalization for text-to-image (T2I) diffusion models to represent several subjects in an image has gained much more attention. The main challenge of this task is "concept mixing", where multiple learned concepts interfere or blend undesirably in the output image. To address this issue, in this paper, we present ConceptSplit, a novel framework to split the individual concepts through training and inference. Our framework comprises two key components. First, we introduce Token-wise Value Adaptation (ToVA), a merging-free training method that focuses exclusively on adapting the value projection in cross-attention. Based on our empirical analysis, we found that modifying the key projection, a common approach in existing methods, can disrupt the attention mechanism and lead to concept mixing. Second, we propose Latent Optimization for Disentangled Attention (LODA), which alleviates attention entanglement during inference by optimizing the input latent. Through extensive qualitative and quantitative experiments, we demonstrate that ConceptSplit achieves robust multi-concept personalization, mitigating unintended concept interference. Code is available at this https URL</li>
</ul>

<h3>Title: Label-Efficient Cross-Modality Generalization for Liver Segmentation in Multi-Phase MRI</h3>
<ul>
<li><strong>Authors: </strong>Quang-Khai Bui-Tran, Minh-Toan Dinh, Thanh-Huy Nguyen, Ba-Thinh Lam, Mai-Anh Vu, Ulas Bagci</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04705">https://arxiv.org/abs/2510.04705</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04705">https://arxiv.org/pdf/2510.04705</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04705]] Label-Efficient Cross-Modality Generalization for Liver Segmentation in Multi-Phase MRI(https://arxiv.org/abs/2510.04705)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Accurate liver segmentation in multi-phase MRI is vital for liver fibrosis assessment, yet labeled data is often scarce and unevenly distributed across imaging modalities and vendor systems. We propose a label-efficient segmentation approach that promotes cross-modality generalization under real-world conditions, where GED4 hepatobiliary-phase annotations are limited, non-contrast sequences (T1WI, T2WI, DWI) are unlabeled, and spatial misalignment and missing phases are common. Our method integrates a foundation-scale 3D segmentation backbone adapted via fine-tuning, co-training with cross pseudo supervision to leverage unlabeled volumes, and a standardized preprocessing pipeline. Without requiring spatial registration, the model learns to generalize across MRI phases and vendors, demonstrating robust segmentation performance in both labeled and unlabeled domains. Our results exhibit the effectiveness of our proposed label-efficient baseline for liver segmentation in multi-phase, multi-vendor MRI and highlight the potential of combining foundation model adaptation with co-training for real-world clinical imaging tasks.</li>
</ul>

<h3>Title: ID-Consistent, Precise Expression Generation with Blendshape-Guided Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Foivos Paraperas Papantoniou, Stefanos Zafeiriou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04706">https://arxiv.org/abs/2510.04706</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04706">https://arxiv.org/pdf/2510.04706</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04706]] ID-Consistent, Precise Expression Generation with Blendshape-Guided Diffusion(https://arxiv.org/abs/2510.04706)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model, generative</a></li>
<li><strong>Abstract: </strong>Human-centric generative models designed for AI-driven storytelling must bring together two core capabilities: identity consistency and precise control over human performance. While recent diffusion-based approaches have made significant progress in maintaining facial identity, achieving fine-grained expression control without compromising identity remains challenging. In this work, we present a diffusion-based framework that faithfully reimagines any subject under any particular facial expression. Building on an ID-consistent face foundation model, we adopt a compositional design featuring an expression cross-attention module guided by FLAME blendshape parameters for explicit control. Trained on a diverse mixture of image and video data rich in expressive variation, our adapter generalizes beyond basic emotions to subtle micro-expressions and expressive transitions, overlooked by prior works. In addition, a pluggable Reference Adapter enables expression editing in real images by transferring the appearance from a reference frame during synthesis. Extensive quantitative and qualitative evaluations show that our model outperforms existing methods in tailored and identity-consistent expression generation. Code and models can be found at this https URL.</li>
</ul>

<h3>Title: ViTs: Teaching Machines to See Time Series Anomalies Like Human Experts</h3>
<ul>
<li><strong>Authors: </strong>Zexin Wang, Changhua Pei, Yang Liu, Hengyue Jiang, Quan Zhou, Haotian Si, Hang Cui, Jianhui Li, Gaogang Xie, Jingjing Li, Dan Pei</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04710">https://arxiv.org/abs/2510.04710</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04710">https://arxiv.org/pdf/2510.04710</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04710]] ViTs: Teaching Machines to See Time Series Anomalies Like Human Experts(https://arxiv.org/abs/2510.04710)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, anomaly</a></li>
<li><strong>Abstract: </strong>Web service administrators must ensure the stability of multiple systems by promptly detecting anomalies in Key Performance Indicators (KPIs). Achieving the goal of "train once, infer across scenarios" remains a fundamental challenge for time series anomaly detection models. Beyond improving zero-shot generalization, such models must also flexibly handle sequences of varying lengths during inference, ranging from one hour to one week, without retraining. Conventional approaches rely on sliding-window encoding and self-supervised learning, which restrict inference to fixed-length inputs. Large Language Models (LLMs) have demonstrated remarkable zero-shot capabilities across general domains. However, when applied to time series data, they face inherent limitations due to context length. To address this issue, we propose ViTs, a Vision-Language Model (VLM)-based framework that converts time series curves into visual representations. By rescaling time series images, temporal dependencies are preserved while maintaining a consistent input size, thereby enabling efficient processing of arbitrarily long sequences without context constraints. Training VLMs for this purpose introduces unique challenges, primarily due to the scarcity of aligned time series image-text data. To overcome this, we employ an evolutionary algorithm to automatically generate thousands of high-quality image-text pairs and design a three-stage training pipeline consisting of: (1) time series knowledge injection, (2) anomaly detection enhancement, and (3) anomaly reasoning refinement. Extensive experiments demonstrate that ViTs substantially enhance the ability of VLMs to understand and detect anomalies in time series data. All datasets and code will be publicly released at: this https URL.</li>
</ul>

<h3>Title: ReactDiff: Fundamental Multiple Appropriate Facial Reaction Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Luo Cheng, Song Siyang, Yan Siyuan, Yu Zhen, Ge Zongyuan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.HC, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04712">https://arxiv.org/abs/2510.04712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04712">https://arxiv.org/pdf/2510.04712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04712]] ReactDiff: Fundamental Multiple Appropriate Facial Reaction Diffusion Model(https://arxiv.org/abs/2510.04712)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The automatic generation of diverse and human-like facial reactions in dyadic dialogue remains a critical challenge for human-computer interaction systems. Existing methods fail to model the stochasticity and dynamics inherent in real human reactions. To address this, we propose ReactDiff, a novel temporal diffusion framework for generating diverse facial reactions that are appropriate for responding to any given dialogue context. Our key insight is that plausible human reactions demonstrate smoothness, and coherence over time, and conform to constraints imposed by human facial anatomy. To achieve this, ReactDiff incorporates two vital priors (spatio-temporal facial kinematics) into the diffusion process: i) temporal facial behavioral kinematics and ii) facial action unit dependencies. These two constraints guide the model toward realistic human reaction manifolds, avoiding visually unrealistic jitters, unstable transitions, unnatural expressions, and other artifacts. Extensive experiments on the REACT2024 dataset demonstrate that our approach not only achieves state-of-the-art reaction quality but also excels in diversity and reaction appropriateness.</li>
</ul>

<h3>Title: Anomaly-Aware YOLO: A Frugal yet Robust Approach to Infrared Small Target Detection</h3>
<ul>
<li><strong>Authors: </strong>Alina Ciocarlan, Sylvie Le Hégarat-Mascle, Sidonie Lefebvre</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04741">https://arxiv.org/abs/2510.04741</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04741">https://arxiv.org/pdf/2510.04741</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04741]] Anomaly-Aware YOLO: A Frugal yet Robust Approach to Infrared Small Target Detection(https://arxiv.org/abs/2510.04741)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Infrared Small Target Detection (IRSTD) is a challenging task in defense applications, where complex backgrounds and tiny target sizes often result in numerous false alarms using conventional object detectors. To overcome this limitation, we propose Anomaly-Aware YOLO (AA-YOLO), which integrates a statistical anomaly detection test into its detection head. By treating small targets as unexpected patterns against the background, AA-YOLO effectively controls the false alarm rate. Our approach not only achieves competitive performance on several IRSTD benchmarks, but also demonstrates remarkable robustness in scenarios with limited training data, noise, and domain shifts. Furthermore, since only the detection head is modified, our design is highly generic and has been successfully applied across various YOLO backbones, including lightweight models. It also provides promising results when integrated into an instance segmentation YOLO. This versatility makes AA-YOLO an attractive solution for real-world deployments where resources are constrained. The code will be publicly released.</li>
</ul>

<h3>Title: ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs</h3>
<ul>
<li><strong>Authors: </strong>Wonjun Kang, Kevin Galim, Seunghyuk Oh, Minjae Lee, Yuchen Zeng, Shuibai Zhang, Coleman Hooper, Yuezhou Hu, Hyung Il Koo, Nam Ik Cho, Kangwook Lee</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04767">https://arxiv.org/abs/2510.04767</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04767">https://arxiv.org/pdf/2510.04767</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04767]] ParallelBench: Understanding the Trade-offs of Parallel Decoding in Diffusion LLMs(https://arxiv.org/abs/2510.04767)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>While most autoregressive LLMs are constrained to one-by-one decoding, diffusion LLMs (dLLMs) have attracted growing interest for their potential to dramatically accelerate inference through parallel decoding. Despite this promise, the conditional independence assumption in dLLMs causes parallel decoding to ignore token dependencies, inevitably degrading generation quality when these dependencies are strong. However, existing works largely overlook these inherent challenges, and evaluations on standard benchmarks (e.g., math and coding) are not sufficient to capture the quality degradation caused by parallel decoding. To address this gap, we first provide an information-theoretic analysis of parallel decoding. We then conduct case studies on analytically tractable synthetic list operations from both data distribution and decoding strategy perspectives, offering quantitative insights that highlight the fundamental limitations of parallel decoding. Building on these insights, we propose ParallelBench, the first benchmark specifically designed for dLLMs, featuring realistic tasks that are trivial for humans and autoregressive LLMs yet exceptionally challenging for dLLMs under parallel decoding. Using ParallelBench, we systematically analyze both dLLMs and autoregressive LLMs, revealing that: (i) dLLMs under parallel decoding can suffer dramatic quality degradation in real-world scenarios, and (ii) current parallel decoding strategies struggle to adapt their degree of parallelism based on task difficulty, thus failing to achieve meaningful speedup without compromising quality. Our findings underscore the pressing need for innovative decoding methods that can overcome the current speed-quality trade-off. We release our benchmark to help accelerate the development of truly efficient dLLMs.</li>
</ul>

<h3>Title: Federated Learning for Surgical Vision in Appendicitis Classification: Results of the FedSurg EndoVis 2024 Challenge</h3>
<ul>
<li><strong>Authors: </strong>Max Kirchner, Hanna Hoffmann, Alexander C. Jenke, Oliver L. Saldanha, Kevin Pfeiffer, Weam Kanjo, Julia Alekseenko, Claas de Boer, Santhi Raj Kolamuri, Lorenzo Mazza, Nicolas Padoy, Sophia Bano, Annika Reinke, Lena Maier-Hein, Danail Stoyanov, Jakob N. Kather, Fiona R. Kolbinger, Sebastian Bodenstedt, Stefanie Speidel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04772">https://arxiv.org/abs/2510.04772</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04772">https://arxiv.org/pdf/2510.04772</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04772]] Federated Learning for Surgical Vision in Appendicitis Classification: Results of the FedSurg EndoVis 2024 Challenge(https://arxiv.org/abs/2510.04772)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Purpose: The FedSurg challenge was designed to benchmark the state of the art in federated learning for surgical video classification. Its goal was to assess how well current methods generalize to unseen clinical centers and adapt through local fine-tuning while enabling collaborative model development without sharing patient data. Methods: Participants developed strategies to classify inflammation stages in appendicitis using a preliminary version of the multi-center Appendix300 video dataset. The challenge evaluated two tasks: generalization to an unseen center and center-specific adaptation after fine-tuning. Submitted approaches included foundation models with linear probing, metric learning with triplet loss, and various FL aggregation schemes (FedAvg, FedMedian, FedSAM). Performance was assessed using F1-score and Expected Cost, with ranking robustness evaluated via bootstrapping and statistical testing. Results: In the generalization task, performance across centers was limited. In the adaptation task, all teams improved after fine-tuning, though ranking stability was low. The ViViT-based submission achieved the strongest overall performance. The challenge highlighted limitations in generalization, sensitivity to class imbalance, and difficulties in hyperparameter tuning in decentralized training, while spatiotemporal modeling and context-aware preprocessing emerged as promising strategies. Conclusion: The FedSurg Challenge establishes the first benchmark for evaluating FL strategies in surgical video classification. Findings highlight the trade-off between local personalization and global robustness, and underscore the importance of architecture choice, preprocessing, and loss design. This benchmarking offers a reference point for future development of imbalance-aware, adaptive, and robust FL methods in clinical surgical AI.</li>
</ul>

<h3>Title: A Comparative Study of Vision Transformers and CNNs for Few-Shot Rigid Transformation and Fundamental Matrix Estimation</h3>
<ul>
<li><strong>Authors: </strong>Alon Kaya, Igal Bilik, Inna Stainvas</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04794">https://arxiv.org/abs/2510.04794</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04794">https://arxiv.org/pdf/2510.04794</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04794]] A Comparative Study of Vision Transformers and CNNs for Few-Shot Rigid Transformation and Fundamental Matrix Estimation(https://arxiv.org/abs/2510.04794)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Vision-transformers (ViTs) and large-scale convolution-neural-networks (CNNs) have reshaped computer vision through pretrained feature representations that enable strong transfer learning for diverse tasks. However, their efficiency as backbone architectures for geometric estimation tasks involving image deformations in low-data regimes remains an open question. This work considers two such tasks: 1) estimating 2D rigid transformations between pairs of images and 2) predicting the fundamental matrix for stereo image pairs, an important problem in various applications, such as autonomous mobility, robotics, and 3D scene reconstruction. Addressing this intriguing question, this work systematically compares large-scale CNNs (ResNet, EfficientNet, CLIP-ResNet) with ViT-based foundation models (CLIP-ViT variants and DINO) in various data size settings, including few-shot scenarios. These pretrained models are optimized for classification or contrastive learning, encouraging them to focus mostly on high-level semantics. The considered tasks require balancing local and global features differently, challenging the straightforward adoption of these models as the backbone. Empirical comparative analysis shows that, similar to training from scratch, ViTs outperform CNNs during refinement in large downstream-data scenarios. However, in small data scenarios, the inductive bias and smaller capacity of CNNs improve their performance, allowing them to match that of a ViT. Moreover, ViTs exhibit stronger generalization in cross-domain evaluation where the data distribution changes. These results emphasize the importance of carefully selecting model architectures for refinement, motivating future research towards hybrid architectures that balance local and global representations.</li>
</ul>

<h3>Title: DiT-VTON: Diffusion Transformer Framework for Unified Multi-Category Virtual Try-On and Virtual Try-All with Integrated Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Qi Li, Shuwen Qiu, Julien Han, Xingzi Xu, Mehmet Saygin Seyfioglu, Kee Kiat Koo, Karim Bouyarmane</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04797">https://arxiv.org/abs/2510.04797</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04797">https://arxiv.org/pdf/2510.04797</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04797]] DiT-VTON: Diffusion Transformer Framework for Unified Multi-Category Virtual Try-On and Virtual Try-All with Integrated Image Editing(https://arxiv.org/abs/2510.04797)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, in-context</a></li>
<li><strong>Abstract: </strong>The rapid growth of e-commerce has intensified the demand for Virtual Try-On (VTO) technologies, enabling customers to realistically visualize products overlaid on their own images. Despite recent advances, existing VTO models face challenges with fine-grained detail preservation, robustness to real-world imagery, efficient sampling, image editing capabilities, and generalization across diverse product categories. In this paper, we present DiT-VTON, a novel VTO framework that leverages a Diffusion Transformer (DiT), renowned for its performance on text-conditioned image generation, adapted here for the image-conditioned VTO task. We systematically explore multiple DiT configurations, including in-context token concatenation, channel concatenation, and ControlNet integration, to determine the best setup for VTO image conditioning. To enhance robustness, we train the model on an expanded dataset encompassing varied backgrounds, unstructured references, and non-garment categories, demonstrating the benefits of data scaling for VTO adaptability. DiT-VTON also redefines the VTO task beyond garment try-on, offering a versatile Virtual Try-All (VTA) solution capable of handling a wide range of product categories and supporting advanced image editing functionalities such as pose preservation, localized editing, texture transfer, and object-level customization. Experimental results show that our model surpasses state-of-the-art methods on VITON-HD, achieving superior detail preservation and robustness without reliance on additional condition encoders. It also outperforms models with VTA and image editing capabilities on a diverse dataset spanning thousands of product categories.</li>
</ul>

<h3>Title: Did you just see that? Arbitrary view synthesis for egocentric replay of operating room workflows from ambient sensors</h3>
<ul>
<li><strong>Authors: </strong>Han Zhang, Lalithkumar Seenivasan, Jose L. Porras, Roger D. Soberanis-Mukul, Hao Ding, Hongchao Shu, Benjamin D. Killeen, Ankita Ghosh, Lonny Yarmus, Masaru Ishii, Angela Christine Argento, Mathias Unberath</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04802">https://arxiv.org/abs/2510.04802</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04802">https://arxiv.org/pdf/2510.04802</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04802]] Did you just see that? Arbitrary view synthesis for egocentric replay of operating room workflows from ambient sensors(https://arxiv.org/abs/2510.04802)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Observing surgical practice has historically relied on fixed vantage points or recollections, leaving the egocentric visual perspectives that guide clinical decisions undocumented. Fixed-camera video can capture surgical workflows at the room-scale, but cannot reconstruct what each team member actually saw. Thus, these videos only provide limited insights into how decisions that affect surgical safety, training, and workflow optimization are made. Here we introduce EgoSurg, the first framework to reconstruct the dynamic, egocentric replays for any operating room (OR) staff directly from wall-mounted fixed-camera video, and thus, without intervention to clinical workflow. EgoSurg couples geometry-driven neural rendering with diffusion-based view enhancement, enabling high-visual fidelity synthesis of arbitrary and egocentric viewpoints at any moment. In evaluation across multi-site surgical cases and controlled studies, EgoSurg reconstructs person-specific visual fields and arbitrary viewpoints with high visual quality and fidelity. By transforming existing OR camera infrastructure into a navigable dynamic 3D record, EgoSurg establishes a new foundation for immersive surgical data science, enabling surgical practice to be visualized, experienced, and analyzed from every angle.</li>
</ul>

<h3>Title: Synthesising Counterfactual Explanations via Label-Conditional Gaussian Mixture Variational Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Junqi Jiang, Francesco Leofante, Antonio Rago, Francesca Toni</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04855">https://arxiv.org/abs/2510.04855</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04855">https://arxiv.org/pdf/2510.04855</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04855]] Synthesising Counterfactual Explanations via Label-Conditional Gaussian Mixture Variational Autoencoders(https://arxiv.org/abs/2510.04855)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Counterfactual explanations (CEs) provide recourse recommendations for individuals affected by algorithmic decisions. A key challenge is generating CEs that are robust against various perturbation types (e.g. input and model perturbations) while simultaneously satisfying other desirable properties. These include plausibility, ensuring CEs reside on the data manifold, and diversity, providing multiple distinct recourse options for single inputs. Existing methods, however, mostly struggle to address these multifaceted requirements in a unified, model-agnostic manner. We address these limitations by proposing a novel generative framework. First, we introduce the Label-conditional Gaussian Mixture Variational Autoencoder (L-GMVAE), a model trained to learn a structured latent space where each class label is represented by a set of Gaussian components with diverse, prototypical centroids. Building on this, we present LAPACE (LAtent PAth Counterfactual Explanations), a model-agnostic algorithm that synthesises entire paths of CE points by interpolating from inputs' latent representations to those learned latent centroids. This approach inherently ensures robustness to input changes, as all paths for a given target class converge to the same fixed centroids. Furthermore, the generated paths provide a spectrum of recourse options, allowing users to navigate the trade-off between proximity and plausibility while also encouraging robustness against model changes. In addition, user-specified actionability constraints can also be easily incorporated via lightweight gradient optimisation through the L-GMVAE's decoder. Comprehensive experiments show that LAPACE is computationally efficient and achieves competitive performance across eight quantitative metrics.</li>
</ul>

<h3>Title: Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the Rails</h3>
<ul>
<li><strong>Authors: </strong>Siwei Han, Jiaqi Liu, Yaofeng Su, Wenbo Duan, Xinyuan Liu, Cihang Xie, Mohit Bansal, Mingyu Ding, Linjun Zhang, Huaxiu Yao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04860">https://arxiv.org/abs/2510.04860</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04860">https://arxiv.org/pdf/2510.04860</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04860]] Alignment Tipping Process: How Self-Evolution Pushes LLM Agents Off the Rails(https://arxiv.org/abs/2510.04860)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>As Large Language Model (LLM) agents increasingly gain self-evolutionary capabilities to adapt and refine their strategies through real-world interaction, their long-term reliability becomes a critical concern. We identify the Alignment Tipping Process (ATP), a critical post-deployment risk unique to self-evolving LLM agents. Unlike training-time failures, ATP arises when continual interaction drives agents to abandon alignment constraints established during training in favor of reinforced, self-interested strategies. We formalize and analyze ATP through two complementary paradigms: Self-Interested Exploration, where repeated high-reward deviations induce individual behavioral drift, and Imitative Strategy Diffusion, where deviant behaviors spread across multi-agent systems. Building on these paradigms, we construct controllable testbeds and benchmark Qwen3-8B and Llama-3.1-8B-Instruct. Our experiments show that alignment benefits erode rapidly under self-evolution, with initially aligned models converging toward unaligned states. In multi-agent settings, successful violations diffuse quickly, leading to collective misalignment. Moreover, current reinforcement learning-based alignment methods provide only fragile defenses against alignment tipping. Together, these findings demonstrate that alignment of LLM agents is not a static property but a fragile and dynamic one, vulnerable to feedback-driven decay during deployment. Our data and code are available at this https URL.</li>
</ul>

<h3>Title: A Clinical-grade Universal Foundation Model for Intraoperative Pathology</h3>
<ul>
<li><strong>Authors: </strong>Zihan Zhao, Fengtao Zhou, Ronggang Li, Bing Chu, Xinke Zhang, Xueyi Zheng, Ke Zheng, Xiaobo Wen, Jiabo Ma, Yihui Wang, Jiewei Chen, Chengyou Zheng, Jiangyu Zhang, Yongqin Wen, Jiajia Meng, Ziqi Zeng, Xiaoqing Li, Jing Li, Dan Xie, Yaping Ye, Yu Wang, Hao Chen, Muyan Cai</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04861">https://arxiv.org/abs/2510.04861</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04861">https://arxiv.org/pdf/2510.04861</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04861]] A Clinical-grade Universal Foundation Model for Intraoperative Pathology(https://arxiv.org/abs/2510.04861)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Intraoperative pathology is pivotal to precision surgery, yet its clinical impact is constrained by diagnostic complexity and the limited availability of high-quality frozen-section data. While computational pathology has made significant strides, the lack of large-scale, prospective validation has impeded its routine adoption in surgical workflows. Here, we introduce CRISP, a clinical-grade foundation model developed on over 100,000 frozen sections from eight medical centers, specifically designed to provide Clinical-grade Robust Intraoperative Support for Pathology (CRISP). CRISP was comprehensively evaluated on more than 15,000 intraoperative slides across nearly 100 retrospective diagnostic tasks, including benign-malignant discrimination, key intraoperative decision-making, and pan-cancer detection, etc. The model demonstrated robust generalization across diverse institutions, tumor types, and anatomical sites-including previously unseen sites and rare cancers. In a prospective cohort of over 2,000 patients, CRISP sustained high diagnostic accuracy under real-world conditions, directly informing surgical decisions in 92.6% of cases. Human-AI collaboration further reduced diagnostic workload by 35%, avoided 105 ancillary tests and enhanced detection of micrometastases with 87.5% accuracy. Together, these findings position CRISP as a clinical-grade paradigm for AI-driven intraoperative pathology, bridging computational advances with surgical precision and accelerating the translation of artificial intelligence into routine clinical practice.</li>
</ul>

<h3>Title: BenthiCat: An opti-acoustic dataset for advancing benthic classification and habitat mapping</h3>
<ul>
<li><strong>Authors: </strong>Hayat Rajani, Valerio Franchi, Borja Martinez-Clavel Valles, Raimon Ramos, Rafael Garcia, Nuno Gracias</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04876">https://arxiv.org/abs/2510.04876</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04876">https://arxiv.org/pdf/2510.04876</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04876]] BenthiCat: An opti-acoustic dataset for advancing benthic classification and habitat mapping(https://arxiv.org/abs/2510.04876)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Benthic habitat mapping is fundamental for understanding marine ecosystems, guiding conservation efforts, and supporting sustainable resource management. Yet, the scarcity of large, annotated datasets limits the development and benchmarking of machine learning models in this domain. This paper introduces a thorough multi-modal dataset, comprising about a million side-scan sonar (SSS) tiles collected along the coast of Catalonia (Spain), complemented by bathymetric maps and a set of co-registered optical images from targeted surveys using an autonomous underwater vehicle (AUV). Approximately \num{36000} of the SSS tiles have been manually annotated with segmentation masks to enable supervised fine-tuning of classification models. All the raw sensor data, together with mosaics, are also released to support further exploration and algorithm development. To address challenges in multi-sensor data fusion for AUVs, we spatially associate optical images with corresponding SSS tiles, facilitating self-supervised, cross-modal representation learning. Accompanying open-source preprocessing and annotation tools are provided to enhance accessibility and encourage research. This resource aims to establish a standardized benchmark for underwater habitat mapping, promoting advancements in autonomous seafloor classification and multi-sensor integration.</li>
</ul>

<h3>Title: Flow-Matching Based Refiner for Molecular Conformer Generation</h3>
<ul>
<li><strong>Authors: </strong>Xiangyang Xu, Hongyang Gao</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04878">https://arxiv.org/abs/2510.04878</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04878">https://arxiv.org/pdf/2510.04878</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04878]] Flow-Matching Based Refiner for Molecular Conformer Generation(https://arxiv.org/abs/2510.04878)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Low-energy molecular conformers generation (MCG) is a foundational yet challenging problem in drug discovery. Denoising-based methods include diffusion and flow-matching methods that learn mappings from a simple base distribution to the molecular conformer distribution. However, these approaches often suffer from error accumulation during sampling, especially in the low SNR steps, which are hard to train. To address these challenges, we propose a flow-matching refiner for the MCG task. The proposed method initializes sampling from mixed-quality outputs produced by upstream denoising models and reschedules the noise scale to bypass the low-SNR phase, thereby improving sample quality. On the GEOM-QM9 and GEOM-Drugs benchmark datasets, the generator-refiner pipeline improves quality with fewer total denoising steps while preserving diversity.</li>
</ul>

<h3>Title: How Different from the Past? Spatio-Temporal Time Series Forecasting with Self-Supervised Deviation Learning</h3>
<ul>
<li><strong>Authors: </strong>Haotian Gao, Zheng Dong, Jiawei Yong, Shintaro Fukushima, Kenjiro Taura, Renhe Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04908">https://arxiv.org/abs/2510.04908</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04908">https://arxiv.org/pdf/2510.04908</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04908]] How Different from the Past? Spatio-Temporal Time Series Forecasting with Self-Supervised Deviation Learning(https://arxiv.org/abs/2510.04908)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Spatio-temporal forecasting is essential for real-world applications such as traffic management and urban computing. Although recent methods have shown improved accuracy, they often fail to account for dynamic deviations between current inputs and historical patterns. These deviations contain critical signals that can significantly affect model performance. To fill this gap, we propose ST-SSDL, a Spatio-Temporal time series forecasting framework that incorporates a Self-Supervised Deviation Learning scheme to capture and utilize such deviations. ST-SSDL anchors each input to its historical average and discretizes the latent space using learnable prototypes that represent typical spatio-temporal patterns. Two auxiliary objectives are proposed to refine this structure: a contrastive loss that enhances inter-prototype discriminability and a deviation loss that regularizes the distance consistency between input representations and corresponding prototypes to quantify deviation. Optimized jointly with the forecasting objective, these components guide the model to organize its hidden space and improve generalization across diverse input conditions. Experiments on six benchmark datasets show that ST-SSDL consistently outperforms state-of-the-art baselines across multiple metrics. Visualizations further demonstrate its ability to adaptively respond to varying levels of deviation in complex spatio-temporal scenarios. Our code and datasets are available at this https URL.</li>
</ul>

<h3>Title: A Semantics-Aware Hierarchical Self-Supervised Approach to Classification of Remote Sensing Images</h3>
<ul>
<li><strong>Authors: </strong>Giulio Weikmann, Gianmarco Perantoni, Lorenzo Bruzzone</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04916">https://arxiv.org/abs/2510.04916</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04916">https://arxiv.org/pdf/2510.04916</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04916]] A Semantics-Aware Hierarchical Self-Supervised Approach to Classification of Remote Sensing Images(https://arxiv.org/abs/2510.04916)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Deep learning has become increasingly important in remote sensing image classification due to its ability to extract semantic information from complex data. Classification tasks often include predefined label hierarchies that represent the semantic relationships among classes. However, these hierarchies are frequently overlooked, and most approaches focus only on fine-grained classification schemes. In this paper, we present a novel Semantics-Aware Hierarchical Consensus (SAHC) method for learning hierarchical features and relationships by integrating hierarchy-specific classification heads within a deep network architecture, each specialized in different degrees of class granularity. The proposed approach employs trainable hierarchy matrices, which guide the network through the learning of the hierarchical structure in a self-supervised manner. Furthermore, we introduce a hierarchical consensus mechanism to ensure consistent probability distributions across different hierarchical levels. This mechanism acts as a weighted ensemble being able to effectively leverage the inherent structure of the hierarchical classification task. The proposed SAHC method is evaluated on three benchmark datasets with different degrees of hierarchical complexity on different tasks, using distinct backbone architectures to effectively emphasize its adaptability. Experimental results show both the effectiveness of the proposed approach in guiding network learning and the robustness of the hierarchical consensus for remote sensing image classification tasks.</li>
</ul>

<h3>Title: Federated Self-Supervised Learning for Automatic Modulation Classification under Non-IID and Class-Imbalanced Data</h3>
<ul>
<li><strong>Authors: </strong>Usman Akram, Yiyue Chen, Haris Vikalo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04927">https://arxiv.org/abs/2510.04927</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04927">https://arxiv.org/pdf/2510.04927</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04927]] Federated Self-Supervised Learning for Automatic Modulation Classification under Non-IID and Class-Imbalanced Data(https://arxiv.org/abs/2510.04927)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Training automatic modulation classification (AMC) models on centrally aggregated data raises privacy concerns, incurs communication overhead, and often fails to confer robustness to channel shifts. Federated learning (FL) avoids central aggregation by training on distributed clients but remains sensitive to class imbalance, non-IID client distributions, and limited labeled samples. We propose FedSSL-AMC, which trains a causal, time-dilated CNN with triplet-loss self-supervision on unlabeled I/Q sequences across clients, followed by per-client SVMs on small labeled sets. We establish convergence of the federated representation learning procedure and a separability guarantee for the downstream classifier under feature noise. Experiments on synthetic and over-the-air datasets show consistent gains over supervised FL baselines under heterogeneous SNR, carrier-frequency offsets, and non-IID label partitions.</li>
</ul>

<h3>Title: Bidirectional Mammogram View Translation with Column-Aware and Implicit 3D Conditional Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Xin Li, Kaixiang Yang, Qiang Li, Zhiwei Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04947">https://arxiv.org/abs/2510.04947</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04947">https://arxiv.org/pdf/2510.04947</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04947]] Bidirectional Mammogram View Translation with Column-Aware and Implicit 3D Conditional Diffusion(https://arxiv.org/abs/2510.04947)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Dual-view mammography, including craniocaudal (CC) and mediolateral oblique (MLO) projections, offers complementary anatomical views crucial for breast cancer diagnosis. However, in real-world clinical workflows, one view may be missing, corrupted, or degraded due to acquisition errors or compression artifacts, limiting the effectiveness of downstream analysis. View-to-view translation can help recover missing views and improve lesion alignment. Unlike natural images, this task in mammography is highly challenging due to large non-rigid deformations and severe tissue overlap in X-ray projections, which obscure pixel-level correspondences. In this paper, we propose Column-Aware and Implicit 3D Diffusion (CA3D-Diff), a novel bidirectional mammogram view translation framework based on conditional diffusion model. To address cross-view structural misalignment, we first design a column-aware cross-attention mechanism that leverages the geometric property that anatomically corresponding regions tend to lie in similar column positions across views. A Gaussian-decayed bias is applied to emphasize local column-wise correlations while suppressing distant mismatches. Furthermore, we introduce an implicit 3D structure reconstruction module that back-projects noisy 2D latents into a coarse 3D feature volume based on breast-view projection geometry. The reconstructed 3D structure is refined and injected into the denoising UNet to guide cross-view generation with enhanced anatomical awareness. Extensive experiments demonstrate that CA3D-Diff achieves superior performance in bidirectional tasks, outperforming state-of-the-art methods in visual fidelity and structural consistency. Furthermore, the synthesized views effectively improve single-view malignancy classification in screening settings, demonstrating the practical value of our method in real-world diagnostics.</li>
</ul>

<h3>Title: SSDD: Single-Step Diffusion Decoder for Efficient Image Tokenization</h3>
<ul>
<li><strong>Authors: </strong>Théophane Vallaeys, Jakob Verbeek, Matthieu Cord</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04961">https://arxiv.org/abs/2510.04961</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04961">https://arxiv.org/pdf/2510.04961</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04961]] SSDD: Single-Step Diffusion Decoder for Efficient Image Tokenization(https://arxiv.org/abs/2510.04961)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Tokenizers are a key component of state-of-the-art generative image models, extracting the most important features from the signal while reducing data dimension and redundancy. Most current tokenizers are based on KL-regularized variational autoencoders (KL-VAE), trained with reconstruction, perceptual and adversarial losses. Diffusion decoders have been proposed as a more principled alternative to model the distribution over images conditioned on the latent. However, matching the performance of KL-VAE still requires adversarial losses, as well as a higher decoding time due to iterative sampling. To address these limitations, we introduce a new pixel diffusion decoder architecture for improved scaling and training stability, benefiting from transformer components and GAN-free training. We use distillation to replicate the performance of the diffusion decoder in an efficient single-step decoder. This makes SSDD the first diffusion decoder optimized for single-step reconstruction trained without adversarial losses, reaching higher reconstruction quality and faster sampling than KL-VAE. In particular, SSDD improves reconstruction FID from $0.87$ to $0.50$ with $1.4\times$ higher throughput and preserve generation quality of DiTs with $3.8\times$ faster sampling. As such, SSDD can be used as a drop-in replacement for KL-VAE, and for building higher-quality and faster generative models.</li>
</ul>

<h3>Title: ActiveMark: on watermarking of visual foundation models via massive activations</h3>
<ul>
<li><strong>Authors: </strong>Anna Chistyakova, Mikhail Pautov</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04966">https://arxiv.org/abs/2510.04966</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04966">https://arxiv.org/pdf/2510.04966</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04966]] ActiveMark: on watermarking of visual foundation models via massive activations(https://arxiv.org/abs/2510.04966)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Being trained on large and vast datasets, visual foundation models (VFMs) can be fine-tuned for diverse downstream tasks, achieving remarkable performance and efficiency in various computer vision applications. The high computation cost of data collection and training motivates the owners of some VFMs to distribute them alongside the license to protect their intellectual property rights. However, a dishonest user of the protected model's copy may illegally redistribute it, for example, to make a profit. As a consequence, the development of reliable ownership verification tools is of great importance today, since such methods can be used to differentiate between a redistributed copy of the protected model and an independent model. In this paper, we propose an approach to ownership verification of visual foundation models by fine-tuning a small set of expressive layers of a VFM along with a small encoder-decoder network to embed digital watermarks into an internal representation of a hold-out set of input images. Importantly, the watermarks embedded remain detectable in the functional copies of the protected model, obtained, for example, by fine-tuning the VFM for a particular downstream task. Theoretically and experimentally, we demonstrate that the proposed method yields a low probability of false detection of a non-watermarked model and a low probability of false misdetection of a watermarked model.</li>
</ul>

<h3>Title: StructuralDecompose: A Modular Framework for Robust Time Series Decomposition in R</h3>
<ul>
<li><strong>Authors: </strong>Allen Daniel Sunny</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.04974">https://arxiv.org/abs/2510.04974</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.04974">https://arxiv.org/pdf/2510.04974</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.04974]] StructuralDecompose: A Modular Framework for Robust Time Series Decomposition in R(https://arxiv.org/abs/2510.04974)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>We present StructuralDecompose, an R package for modular and interpretable time series decomposition. Unlike existing approaches that treat decomposition as a monolithic process, StructuralDecompose separates the analysis into distinct components: changepoint detection, anomaly detection, smoothing, and decomposition. This design provides flexibility and robust- ness, allowing users to tailor methods to specific time series characteristics. We demonstrate the package on simulated and real-world datasets, benchmark its performance against state-of-the- art tools such as Rbeast and autostsm, and discuss its role in interpretable machine learning workflows.</li>
</ul>

<h3>Title: Exploring the Efficacy of Modified Transfer Learning in Identifying Parkinson's Disease Through Drawn Image Patterns</h3>
<ul>
<li><strong>Authors: </strong>Nabil Daiyan, Md Rakibul Haque</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05015">https://arxiv.org/abs/2510.05015</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05015">https://arxiv.org/pdf/2510.05015</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05015]] Exploring the Efficacy of Modified Transfer Learning in Identifying Parkinson's Disease Through Drawn Image Patterns(https://arxiv.org/abs/2510.05015)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Parkinson's disease (PD) is a progressive neurodegenerative condition characterized by the death of dopaminergic neurons, leading to various movement disorder symptoms. Early diagnosis of PD is crucial to prevent adverse effects, yet traditional diagnostic methods are often cumbersome and costly. In this study, a machine learning-based approach is proposed using hand-drawn spiral and wave images as potential biomarkers for PD detection. Our methodology leverages convolutional neural networks (CNNs), transfer learning, and attention mechanisms to improve model performance and resilience against overfitting. To enhance the diversity and richness of both spiral and wave categories, the training dataset undergoes augmentation to increase the number of images. The proposed architecture comprises three phases: utilizing pre-trained CNNs, incorporating custom convolutional layers, and ensemble voting. Employing hard voting further enhances performance by aggregating predictions from multiple models. Experimental results show promising accuracy rates. For spiral images, weighted average precision, recall, and F1-score are 90%, and for wave images, they are 96.67%. After combining the predictions through ensemble hard voting, the overall accuracy is 93.3%. These findings underscore the potential of machine learning in early PD diagnosis, offering a non-invasive and cost-effective solution to improve patient outcomes.</li>
</ul>

<h3>Title: Graph-Aware Diffusion for Signal Generation</h3>
<ul>
<li><strong>Authors: </strong>Sergio Rozada, Vimal K. B., Andrea Cavallo, Antonio G. Marques, Hadi Jamali-Rad, Elvin Isufi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05036">https://arxiv.org/abs/2510.05036</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05036">https://arxiv.org/pdf/2510.05036</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05036]] Graph-Aware Diffusion for Signal Generation(https://arxiv.org/abs/2510.05036)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We study the problem of generating graph signals from unknown distributions defined over given graphs, relevant to domains such as recommender systems or sensor networks. Our approach builds on generative diffusion models, which are well established in vision and graph generation but remain underexplored for graph signals. Existing methods lack generality, either ignoring the graph structure in the forward process or designing graph-aware mechanisms tailored to specific domains. We adopt a forward process that incorporates the graph through the heat equation. Rather than relying on the standard formulation, we consider a time-warped coefficient to mitigate the exponential decay of the drift term, yielding a graph-aware generative diffusion model (GAD). We analyze its forward dynamics, proving convergence to a Gaussian Markov random field with covariance parametrized by the graph Laplacian, and interpret the backward dynamics as a sequence of graph-signal denoising problems. Finally, we demonstrate the advantages of GAD on synthetic data, real traffic speed measurements, and a temperature sensor network.</li>
</ul>

<h3>Title: Test-Time Scaling in Diffusion LLMs via Hidden Semi-Autoregressive Experts</h3>
<ul>
<li><strong>Authors: </strong>Jihoon Lee, Hoyeon Moon, Kevin Zhai, Arun Kumar Chithanar, Anit Kumar Sahu, Soummya Kar, Chul Lee, Souradip Chakraborty, Amrit Singh Bedi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05040">https://arxiv.org/abs/2510.05040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05040">https://arxiv.org/pdf/2510.05040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05040]] Test-Time Scaling in Diffusion LLMs via Hidden Semi-Autoregressive Experts(https://arxiv.org/abs/2510.05040)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion-based large language models (dLLMs) are trained flexibly to model extreme dependence in the data distribution; however, how to best utilize this information at inference time remains an open problem. In this work, we uncover an interesting property of these models: dLLMs trained on textual data implicitly learn a mixture of semi-autoregressive experts, where different generation orders reveal different specialized behaviors. We show that committing to any single, fixed inference time schedule, a common practice, collapses performance by failing to leverage this latent ensemble. To address this, we introduce HEX (Hidden semiautoregressive EXperts for test-time scaling), a training-free inference method that ensembles across heterogeneous block schedules. By doing a majority vote over diverse block-sized generation paths, HEX robustly avoids failure modes associated with any single fixed schedule. On reasoning benchmarks such as GSM8K, it boosts accuracy by up to 3.56X (from 24.72% to 88.10%), outperforming top-K margin inference and specialized fine-tuned methods like GRPO, without additional training. HEX even yields significant gains on MATH benchmark from 16.40% to 40.00%, scientific reasoning on ARC-C from 54.18% to 87.80%, and TruthfulQA from 28.36% to 57.46%. Our results establish a new paradigm for test-time scaling in diffusion-based LLMs (dLLMs), revealing that the sequence in which masking is performed plays a critical role in determining performance during inference.</li>
</ul>

<h3>Title: SegMASt3R: Geometry Grounded Segment Matching</h3>
<ul>
<li><strong>Authors: </strong>Rohit Jayanti, Swayam Agrawal, Vansh Garg, Siddharth Tourani, Muhammad Haris Khan, Sourav Garg, Madhava Krishna</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05051">https://arxiv.org/abs/2510.05051</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05051">https://arxiv.org/pdf/2510.05051</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05051]] SegMASt3R: Geometry Grounded Segment Matching(https://arxiv.org/abs/2510.05051)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Segment matching is an important intermediate task in computer vision that establishes correspondences between semantically or geometrically coherent regions across images. Unlike keypoint matching, which focuses on localized features, segment matching captures structured regions, offering greater robustness to occlusions, lighting variations, and viewpoint changes. In this paper, we leverage the spatial understanding of 3D foundation models to tackle wide-baseline segment matching, a challenging setting involving extreme viewpoint shifts. We propose an architecture that uses the inductive bias of these 3D foundation models to match segments across image pairs with up to 180 degree view-point change. Extensive experiments show that our approach outperforms state-of-the-art methods, including the SAM2 video propagator and local feature matching methods, by upto 30% on the AUPRC metric, on ScanNet++ and Replica datasets. We further demonstrate benefits of the proposed model on relevant downstream tasks, including 3D instance segmentation and image-goal navigation. Project Page: this https URL</li>
</ul>

<h3>Title: TeachLM: Post-Training LLMs for Education Using Authentic Learning Data</h3>
<ul>
<li><strong>Authors: </strong>Janos Perczel, Jin Chow, Dorottya Demszky</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05087">https://arxiv.org/abs/2510.05087</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05087">https://arxiv.org/pdf/2510.05087</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05087]] TeachLM: Post-Training LLMs for Education Using Authentic Learning Data(https://arxiv.org/abs/2510.05087)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The promise of generative AI to revolutionize education is constrained by the pedagogical limits of large language models (LLMs). A major issue is the lack of access to high-quality training data that reflect the learning of actual students. Prompt engineering has emerged as a stopgap, but the ability of prompts to encode complex pedagogical strategies in rule-based natural language is inherently limited. To address this gap we introduce TeachLM - an LLM optimized for teaching through parameter-efficient fine-tuning of state-of-the-art models. TeachLM is trained on a dataset comprised of 100,000 hours of one-on-one, longitudinal student-tutor interactions maintained by Polygence, which underwent a rigorous anonymization process to protect privacy. We use parameter-efficient fine-tuning to develop an authentic student model that enables the generation of high-fidelity synthetic student-tutor dialogues. Building on this capability, we propose a novel multi-turn evaluation protocol that leverages synthetic dialogue generation to provide fast, scalable, and reproducible assessments of the dialogical capabilities of LLMs. Our evaluations demonstrate that fine-tuning on authentic learning data significantly improves conversational and pedagogical performance - doubling student talk time, improving questioning style, increasing dialogue turns by 50%, and greater personalization of instruction.</li>
</ul>

<h3>Title: Finish First, Perfect Later: Test-Time Token-Level Cross-Validation for Diffusion Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Runchu Tian, Junxia Cui, Xueqiang Xu, Feng Yao, Jingbo Shang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05090">https://arxiv.org/abs/2510.05090</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05090">https://arxiv.org/pdf/2510.05090</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05090]] Finish First, Perfect Later: Test-Time Token-Level Cross-Validation for Diffusion Large Language Models(https://arxiv.org/abs/2510.05090)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion large language models (dLLMs) have recently emerged as a promising alternative to autoregressive (AR) models, offering advantages such as accelerated parallel decoding and bidirectional context modeling. However, the vanilla decoding strategy in discrete dLLMs suffers from a critical limitation: once a token is accepted, it can no longer be revised in subsequent steps. As a result, early mistakes persist across iterations, harming both intermediate predictions and final output quality. To address this issue, we propose Tolerator (Token-Level Cross-Validation Refinement), a training-free decoding strategy that leverages cross-validation among predicted tokens. Unlike existing methods that follow a single progressive unmasking procedure, Tolerator introduces a two-stage process: (i) sequence fill-up and (ii) iterative refinement by remasking and decoding a subset of tokens while treating the remaining as context. This design enables previously accepted tokens to be reconsidered and corrected when necessary, leading to more reliable diffusion decoding outputs. We evaluate Tolerator on five standard benchmarks covering language understanding, code generation, and mathematics. Experiments show that our method achieves consistent improvements over the baselines under the same computational budget. These findings suggest that decoding algorithms are crucial to realizing the full potential of diffusion large language models. Code and data are publicly available.</li>
</ul>

<h3>Title: Character Mixing for Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Tingting Liao, Chongjian Ge, Guangyi Liu, Hao Li, Yi Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.05093">https://arxiv.org/abs/2510.05093</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.05093">https://arxiv.org/pdf/2510.05093</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.05093]] Character Mixing for Video Generation(https://arxiv.org/abs/2510.05093)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Imagine Mr. Bean stepping into Tom and Jerry--can we generate videos where characters interact naturally across different worlds? We study inter-character interaction in text-to-video generation, where the key challenge is to preserve each character's identity and behaviors while enabling coherent cross-context interaction. This is difficult because characters may never have coexisted and because mixing styles often causes style delusion, where realistic characters appear cartoonish or vice versa. We introduce a framework that tackles these issues with Cross-Character Embedding (CCE), which learns identity and behavioral logic across multimodal sources, and Cross-Character Augmentation (CCA), which enriches training with synthetic co-existence and mixed-style data. Together, these techniques allow natural interactions between previously uncoexistent characters without losing stylistic fidelity. Experiments on a curated benchmark of cartoons and live-action series with 10 characters show clear improvements in identity preservation, interaction quality, and robustness to style delusion, enabling new forms of generative this http URL results and videos are available on our project page: this https URL.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
