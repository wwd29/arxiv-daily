<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-11-12</h1>
<h3>Title: Knowledge-Guided Textual Reasoning for Explainable Video Anomaly Detection via LLMs</h3>
<ul>
<li><strong>Authors: </strong>Hari Lee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.07429">https://arxiv.org/abs/2511.07429</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.07429">https://arxiv.org/pdf/2511.07429</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.07429]] Knowledge-Guided Textual Reasoning for Explainable Video Anomaly Detection via LLMs(https://arxiv.org/abs/2511.07429)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>We introduce Text-based Explainable Video Anomaly Detection (TbVAD), a language-driven framework for weakly supervised video anomaly detection that performs anomaly detection and explanation entirely within the textual domain. Unlike conventional WSVAD models that rely on explicit visual features, TbVAD represents video semantics through language, enabling interpretable and knowledge-grounded reasoning. The framework operates in three stages: (1) transforming video content into fine-grained captions using a vision-language model, (2) constructing structured knowledge by organizing the captions into four semantic slots (action, object, context, environment), and (3) generating slot-wise explanations that reveal which semantic factors contribute most to the anomaly decision. We evaluate TbVAD on two public benchmarks, UCF-Crime and XD-Violence, demonstrating that textual knowledge reasoning provides interpretable and reliable anomaly detection for real-world surveillance scenarios.</li>
</ul>

<h3>Title: Motif 2 12.7B technical report</h3>
<ul>
<li><strong>Authors: </strong>Junghwan Lim, Sungmin Lee, Dongseok Kim, Taehyun Kim, Eunhwan Park, Jeesoo Lee, Jeongdoo Lee, Junhyeok Lee, Wai Ting Cheung, Dahye Choi, Jaeheui Her, Jaeyeon Huh, Hanbin Jung, Changjin Kang, Beomgyu Kim, Minjae Kim, Taewhan Kim, Youngrok Kim, Hyukjin Kweon, Haesol Lee, Kungyu Lee, Dongpin Oh, Yeongjae Park, Bokki Ryu, Dongjoo Weon</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.07464">https://arxiv.org/abs/2511.07464</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.07464">https://arxiv.org/pdf/2511.07464</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.07464]] Motif 2 12.7B technical report(https://arxiv.org/abs/2511.07464)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>We introduce Motif-2-12.7B, a new open-weight foundation model that pushes the efficiency frontier of large language models by combining architectural innovation with system-level optimization. Designed for scalable language understanding and robust instruction generalization under constrained compute budgets, Motif-2-12.7B builds upon Motif-2.6B with the integration of Grouped Differential Attention (GDA), which improves representational efficiency by disentangling signal and noise-control attention pathways. The model is pre-trained on 5.5 trillion tokens spanning diverse linguistic, mathematical, scientific, and programming domains using a curriculum-driven data scheduler that gradually changes the data composition ratio. The training system leverages the MuonClip optimizer alongside custom high-performance kernels, including fused PolyNorm activations and the Parallel Muon algorithm, yielding significant throughput and memory efficiency gains in large-scale distributed environments. Post-training employs a three-stage supervised fine-tuning pipeline that successively enhances general instruction adherence, compositional understanding, and linguistic precision. Motif-2-12.7B demonstrates competitive performance across diverse benchmarks, showing that thoughtful architectural scaling and optimized training design can rival the capabilities of much larger models.</li>
</ul>

<h3>Title: Towards Personalized Quantum Federated Learning for Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Ratun Rahman, Sina Shaham, Dinh C. Nguyen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.07471">https://arxiv.org/abs/2511.07471</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.07471">https://arxiv.org/pdf/2511.07471</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.07471]] Towards Personalized Quantum Federated Learning for Anomaly Detection(https://arxiv.org/abs/2511.07471)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Anomaly detection has a significant impact on applications such as video surveillance, medical diagnostics, and industrial monitoring, where anomalies frequently depend on context and anomaly-labeled data are limited. Quantum federated learning (QFL) overcomes these concerns by distributing model training among several quantum clients, consequently eliminating the requirement for centralized quantum storage and processing. However, in real-life quantum networks, clients frequently differ in terms of hardware capabilities, circuit designs, noise levels, and how classical data is encoded or preprocessed into quantum states. These differences create inherent heterogeneity across clients - not just in their data distributions, but also in their quantum processing behaviors. As a result, training a single global model becomes ineffective, especially when clients handle imbalanced or non-identically distributed (non-IID) data. To address this, we propose a new framework called personalized quantum federated learning (PQFL) for anomaly detection. PQFL enhances local model training at quantum clients using parameterized quantum circuits and classical optimizers, while introducing a quantum-centric personalization strategy that adapts each client's model to its own hardware characteristics and data representation. Extensive experiments show that PQFL significantly improves anomaly detection accuracy under diverse and realistic conditions. Compared to state-of-the-art methods, PQFL reduces false errors by up to 23%, and achieves gains of 24.2% in AUROC and 20.5% in AUPR, highlighting its effectiveness and scalability in practical quantum federated settings.</li>
</ul>

<h3>Title: Counterfactual Forecasting of Human Behavior using Generative AI and Causal Graphs</h3>
<ul>
<li><strong>Authors: </strong>Dharmateja Priyadarshi Uddandarao, Ravi Kiran Vadlamani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE, stat.ME</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.07484">https://arxiv.org/abs/2511.07484</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.07484">https://arxiv.org/pdf/2511.07484</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.07484]] Counterfactual Forecasting of Human Behavior using Generative AI and Causal Graphs(https://arxiv.org/abs/2511.07484)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This study presents a novel framework for counterfactual user behavior forecasting that combines structural causal models with transformer-based generative artificial intelligence. To model fictitious situations, the method creates causal graphs that map the connections between user interactions, adoption metrics, and product features. The framework generates realistic behavioral trajectories under counterfactual conditions by using generative models that are conditioned on causal variables. Tested on datasets from web interactions, mobile applications, and e-commerce, the methodology outperforms conventional forecasting and uplift modeling techniques. Product teams can effectively simulate and assess possible interventions prior to deployment thanks to the framework improved interpretability through causal path visualization.</li>
</ul>

<h3>Title: Provably Efficient Sample Complexity for Robust CMDP</h3>
<ul>
<li><strong>Authors: </strong>Sourav Ganguly, Arnob Ghosh</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.07486">https://arxiv.org/abs/2511.07486</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.07486">https://arxiv.org/pdf/2511.07486</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.07486]] Provably Efficient Sample Complexity for Robust CMDP(https://arxiv.org/abs/2511.07486)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We study the problem of learning policies that maximize cumulative reward while satisfying safety constraints, even when the real environment differs from a simulator or nominal model. We focus on robust constrained Markov decision processes (RCMDPs), where the agent must maximize reward while ensuring cumulative utility exceeds a threshold under the worst-case dynamics within an uncertainty set. While recent works have established finite-time iteration complexity guarantees for RCMDPs using policy optimization, their sample complexity guarantees remain largely unexplored. In this paper, we first show that Markovian policies may fail to be optimal even under rectangular uncertainty sets unlike the {\em unconstrained} robust MDP. To address this, we introduce an augmented state space that incorporates the remaining utility budget into the state representation. Building on this formulation, we propose a novel Robust constrained Value iteration (RCVI) algorithm with a sample complexity of $\mathcal{\tilde{O}}(|S||A|H^5/\epsilon^2)$ achieving at most $\epsilon$ violation using a generative model where $|S|$ and $|A|$ denote the sizes of the state and action spaces, respectively, and $H$ is the episode length. To the best of our knowledge, this is the {\em first sample complexity guarantee} for RCMDP. Empirical results further validate the effectiveness of our approach.</li>
</ul>

<h3>Title: Laplacian Score Sharpening for Mitigating Hallucination in Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Barath Chandran.C, Srinivas Anumasa, Dianbo Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.07496">https://arxiv.org/abs/2511.07496</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.07496">https://arxiv.org/pdf/2511.07496</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.07496]] Laplacian Score Sharpening for Mitigating Hallucination in Diffusion Models(https://arxiv.org/abs/2511.07496)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models, though successful, are known to suffer from hallucinations that create incoherent or unrealistic samples. Recent works have attributed this to the phenomenon of mode interpolation and score smoothening, but they lack a method to prevent their generation during sampling. In this paper, we propose a post-hoc adjustment to the score function during inference that leverages the Laplacian (or sharpness) of the score to reduce mode interpolation hallucination in unconditional diffusion models across 1D, 2D, and high-dimensional image data. We derive an efficient Laplacian approximation for higher dimensions using a finite-difference variant of the Hutchinson trace estimator. We show that this correction significantly reduces the rate of hallucinated samples across toy 1D/2D distributions and a high- dimensional image dataset. Furthermore, our analysis explores the relationship between the Laplacian and uncertainty in the score.</li>
</ul>

<h3>Title: Toward the Frontiers of Reliable Diffusion Sampling via Adversarial Sinkhorn Attention Guidance</h3>
<ul>
<li><strong>Authors: </strong>Kwanyoung Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.07499">https://arxiv.org/abs/2511.07499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.07499">https://arxiv.org/pdf/2511.07499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.07499]] Toward the Frontiers of Reliable Diffusion Sampling via Adversarial Sinkhorn Attention Guidance(https://arxiv.org/abs/2511.07499)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated strong generative performance when using guidance methods such as classifier-free guidance (CFG), which enhance output quality by modifying the sampling trajectory. These methods typically improve a target output by intentionally degrading another, often the unconditional output, using heuristic perturbation functions such as identity mixing or blurred conditions. However, these approaches lack a principled foundation and rely on manually designed distortions. In this work, we propose Adversarial Sinkhorn Attention Guidance (ASAG), a novel method that reinterprets attention scores in diffusion models through the lens of optimal transport and intentionally disrupt the transport cost via Sinkhorn algorithm. Instead of naively corrupting the attention mechanism, ASAG injects an adversarial cost within self-attention layers to reduce pixel-wise similarity between queries and keys. This deliberate degradation weakens misleading attention alignments and leads to improved conditional and unconditional sample quality. ASAG shows consistent improvements in text-to-image diffusion, and enhances controllability and fidelity in downstream applications such as IP-Adapter and ControlNet. The method is lightweight, plug-and-play, and improves reliability without requiring any model retraining.</li>
</ul>

<h3>Title: Biologically-Informed Hybrid Membership Inference Attacks on Generative Genomic Models</h3>
<ul>
<li><strong>Authors: </strong>Asia Belfiore, Jonathan Passerat-Palmbach, Dmitrii Usynin</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.07503">https://arxiv.org/abs/2511.07503</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.07503">https://arxiv.org/pdf/2511.07503</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.07503]] Biologically-Informed Hybrid Membership Inference Attacks on Generative Genomic Models(https://arxiv.org/abs/2511.07503)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The increased availability of genetic data has transformed genomics research, but raised many privacy concerns regarding its handling due to its sensitive nature. This work explores the use of language models (LMs) for the generation of synthetic genetic mutation profiles, leveraging differential privacy (DP) for the protection of sensitive genetic data. We empirically evaluate the privacy guarantees of our DP modes by introducing a novel Biologically-Informed Hybrid Membership Inference Attack (biHMIA), which combines traditional black box MIA with contextual genomics metrics for enhanced attack power. Our experiments show that both small and large transformer GPT-like models are viable synthetic variant generators for small-scale genomics, and that our hybrid attack leads, on average, to higher adversarial success compared to traditional metric-based MIAs.</li>
</ul>

<h3>Title: A Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain</h3>
<ul>
<li><strong>Authors: </strong>Yining Lu, Wenyi Tang, Max Johnson, Taeho Jung, Meng Jiang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.07577">https://arxiv.org/abs/2511.07577</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.07577">https://arxiv.org/pdf/2511.07577</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.07577]] A Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain(https://arxiv.org/abs/2511.07577)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Existing retrieval-augmented generation (RAG) systems typically use a centralized architecture, causing a high cost of data collection, integration, and management, as well as privacy concerns. There is a great need for a decentralized RAG system that enables foundation models to utilize information directly from data owners who maintain full control over their sources. However, decentralization brings a challenge: the numerous independent data sources vary significantly in reliability, which can diminish retrieval accuracy and response quality. To address this, our decentralized RAG system has a novel reliability scoring mechanism that dynamically evaluates each source based on the quality of responses it contributes to generate and prioritizes high-quality sources during retrieval. To ensure transparency and trust, the scoring process is securely managed through blockchain-based smart contracts, creating verifiable and tamper-proof reliability records without relying on a central authority. We evaluate our decentralized system with two Llama models (3B and 8B) in two simulated environments where six data sources have different levels of reliability. Our system achieves a +10.7\% performance improvement over its centralized counterpart in the real world-like unreliable data environments. Notably, it approaches the upper-bound performance of centralized systems under ideally reliable data environments. The decentralized infrastructure enables secure and trustworthy scoring management, achieving approximately 56\% marginal cost savings through batched update operations. Our code and system are open-sourced at this http URL.</li>
</ul>

<h3>Title: FlowFeat: Pixel-Dense Embedding of Motion Profiles</h3>
<ul>
<li><strong>Authors: </strong>Nikita Araslanov, Anna Sonnweber, Daniel Cremers</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.07696">https://arxiv.org/abs/2511.07696</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.07696">https://arxiv.org/pdf/2511.07696</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.07696]] FlowFeat: Pixel-Dense Embedding of Motion Profiles(https://arxiv.org/abs/2511.07696)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Dense and versatile image representations underpin the success of virtually all computer vision applications. However, state-of-the-art networks, such as transformers, produce low-resolution feature grids, which are suboptimal for dense prediction tasks. To address this limitation, we present FlowFeat, a high-resolution and multi-task feature representation. The key ingredient behind FlowFeat is a novel distillation technique that embeds a distribution of plausible apparent motions, or motion profiles. By leveraging optical flow networks and diverse video data, we develop an effective self-supervised training framework that statistically approximates the apparent motion. With its remarkable level of spatial detail, FlowFeat encodes a compelling degree of geometric and semantic cues while exhibiting high temporal consistency. Empirically, FlowFeat significantly enhances the representational power of five state-of-the-art encoders and alternative upsampling strategies across three dense tasks: video object segmentation, monocular depth estimation and semantic segmentation. Training FlowFeat is computationally inexpensive and robust to inaccurate flow estimation, remaining highly effective even when using unsupervised flow networks. Our work takes a step forward towards reliable and versatile dense image representations.</li>
</ul>

<h3>Title: Diffusion Guided Adversarial State Perturbations in Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Xiaolin Sun, Feidi Liu, Zhengming Ding, ZiZhan Zheng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.07701">https://arxiv.org/abs/2511.07701</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.07701">https://arxiv.org/pdf/2511.07701</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.07701]] Diffusion Guided Adversarial State Perturbations in Reinforcement Learning(https://arxiv.org/abs/2511.07701)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) systems, while achieving remarkable success across various domains, are vulnerable to adversarial attacks. This is especially a concern in vision-based environments where minor manipulations of high-dimensional image inputs can easily mislead the agent's behavior. To this end, various defenses have been proposed recently, with state-of-the-art approaches achieving robust performance even under large state perturbations. However, after closer investigation, we found that the effectiveness of the current defenses is due to a fundamental weakness of the existing $l_p$ norm-constrained attacks, which can barely alter the semantics of image input even under a relatively large perturbation budget. In this work, we propose SHIFT, a novel policy-agnostic diffusion-based state perturbation attack to go beyond this limitation. Our attack is able to generate perturbed states that are semantically different from the true states while remaining realistic and history-aligned to avoid detection. Evaluations show that our attack effectively breaks existing defenses, including the most sophisticated ones, significantly outperforming existing attacks while being more perceptually stealthy. The results highlight the vulnerability of RL agents to semantics-aware adversarial perturbations, indicating the importance of developing more robust policies.</li>
</ul>

<h3>Title: VectorSynth: Fine-Grained Satellite Image Synthesis with Structured Semantics</h3>
<ul>
<li><strong>Authors: </strong>Daniel Cher, Brian Wei, Srikumar Sastry, Nathan Jacobs</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.07744">https://arxiv.org/abs/2511.07744</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.07744">https://arxiv.org/pdf/2511.07744</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.07744]] VectorSynth: Fine-Grained Satellite Image Synthesis with Structured Semantics(https://arxiv.org/abs/2511.07744)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce VectorSynth, a diffusion-based framework for pixel-accurate satellite image synthesis conditioned on polygonal geographic annotations with semantic attributes. Unlike prior text- or layout-conditioned models, VectorSynth learns dense cross-modal correspondences that align imagery and semantic vector geometry, enabling fine-grained, spatially grounded edits. A vision language alignment module produces pixel-level embeddings from polygon semantics; these embeddings guide a conditional image generation framework to respect both spatial extents and semantic cues. VectorSynth supports interactive workflows that mix language prompts with geometry-aware conditioning, allowing rapid what-if simulations, spatial edits, and map-informed content generation. For training and evaluation, we assemble a collection of satellite scenes paired with pixel-registered polygon annotations spanning diverse urban scenes with both built and natural features. We observe strong improvements over prior methods in semantic fidelity and structural realism, and show that our trained vision language model demonstrates fine-grained spatial grounding. The code and data are available at this https URL.</li>
</ul>

<h3>Title: Back to the Future: The Role of Past and Future Context Predictability in Incremental Language Production</h3>
<ul>
<li><strong>Authors: </strong>Shiva Upadhye, Richard Futrell</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.07752">https://arxiv.org/abs/2511.07752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.07752">https://arxiv.org/pdf/2511.07752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.07752]] Back to the Future: The Role of Past and Future Context Predictability in Incremental Language Production(https://arxiv.org/abs/2511.07752)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Contextual predictability shapes both the form and choice of words in online language production. The effects of the predictability of a word given its previous context are generally well-understood in both production and comprehension, but studies of naturalistic production have also revealed a poorly-understood backward predictability effect of a word given its future context, which may be related to future planning. Here, in two studies of naturalistic speech corpora, we investigate backward predictability effects using improved measures and more powerful language models, introducing a new principled and conceptually motivated information-theoretic predictability measure that integrates predictability from both the future and the past context. Our first study revisits classic predictability effects on word duration. Our second study investigates substitution errors within a generative framework that independently models the effects of lexical, contextual, and communicative factors on word choice, while predicting the actual words that surface as speech errors. We find that our proposed conceptually-motivated alternative to backward predictability yields qualitatively similar effects across both studies. Through a fine-grained analysis of substitution errors, we further show that different kinds of errors are suggestive of how speakers prioritize form, meaning, and context-based information during lexical planning. Together, these findings illuminate the functional roles of past and future context in how speakers encode and choose words, offering a bridge between contextual predictability effects and the mechanisms of sentence planning.</li>
</ul>

<h3>Title: Beyond Randomness: Understand the Order of the Noise in Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Song Yan, Min Li, Bi Xinliang, Jian Yang, Yusen Zhang, Guanye Xiong, Yunwei Lan, Tao Zhang, Wei Zhai, Zheng-Jun Zha</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.07756">https://arxiv.org/abs/2511.07756</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.07756">https://arxiv.org/pdf/2511.07756</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.07756]] Beyond Randomness: Understand the Order of the Noise in Diffusion(https://arxiv.org/abs/2511.07756)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In text-driven content generation (T2C) diffusion model, semantic of generated content is mostly attributed to the process of text embedding and attention mechanism interaction. The initial noise of the generation process is typically characterized as a random element that contributes to the diversity of the generated content. Contrary to this view, this paper reveals that beneath the random surface of noise lies strong analyzable patterns. Specifically, this paper first conducts a comprehensive analysis of the impact of random noise on the model's generation. We found that noise not only contains rich semantic information, but also allows for the erasure of unwanted semantics from it in an extremely simple way based on information theory, and using the equivalence between the generation process of diffusion model and semantic injection to inject semantics into the cleaned noise. Then, we mathematically decipher these observations and propose a simple but efficient training-free and universal two-step "Semantic Erasure-Injection" process to modulate the initial noise in T2C diffusion model. Experimental results demonstrate that our method is consistently effective across various T2C models based on both DiT and UNet architectures and presents a novel perspective for optimizing the generation of diffusion model, providing a universal tool for consistent generation.</li>
</ul>

<h3>Title: HybridGuard: Enhancing Minority-Class Intrusion Detection in Dew-Enabled Edge-of-Things Networks</h3>
<ul>
<li><strong>Authors: </strong>Binayak Kara, Ujjwal Sahua, Ciza Thomas, Jyoti Prakash Sahoo</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.07793">https://arxiv.org/abs/2511.07793</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.07793">https://arxiv.org/pdf/2511.07793</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.07793]] HybridGuard: Enhancing Minority-Class Intrusion Detection in Dew-Enabled Edge-of-Things Networks(https://arxiv.org/abs/2511.07793)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, anomaly</a></li>
<li><strong>Abstract: </strong>Securing Dew-Enabled Edge-of-Things (EoT) networks against sophisticated intrusions is a critical challenge. This paper presents HybridGuard, a framework that integrates machine learning and deep learning to improve intrusion detection. HybridGuard addresses data imbalance through mutual information based feature selection, ensuring that the most relevant features are used to improve detection performance, especially for minority attack classes. The framework leverages Wasserstein Conditional Generative Adversarial Networks with Gradient Penalty (WCGAN-GP) to further reduce class imbalance and enhance detection precision. It adopts a two-phase architecture called DualNetShield to support advanced traffic analysis and anomaly detection, improving the granular identification of threats in complex EoT environments. HybridGuard is evaluated on the UNSW-NB15, CIC-IDS-2017, and IOTID20 datasets, where it demonstrates strong performance across diverse attack scenarios and outperforms existing solutions in adapting to evolving cybersecurity threats. This approach establishes HybridGuard as an effective tool for protecting EoT networks against modern intrusions.</li>
</ul>

<h3>Title: PC-Diffusion: Aligning Diffusion Models with Human Preferences via Preference Classifier</h3>
<ul>
<li><strong>Authors: </strong>Shaomeng Wang, He Wang, Xiaolu Wei, Longquan Dai, Jinhui Tang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.07806">https://arxiv.org/abs/2511.07806</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.07806">https://arxiv.org/pdf/2511.07806</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.07806]] PC-Diffusion: Aligning Diffusion Models with Human Preferences via Preference Classifier(https://arxiv.org/abs/2511.07806)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have achieved remarkable success in conditional image generation, yet their outputs often remain misaligned with human preferences. To address this, recent work has applied Direct Preference Optimization (DPO) to diffusion models, yielding significant improvements.~However, DPO-like methods exhibit two key limitations: 1) High computational cost,due to the entire model fine-tuning; 2) Sensitivity to reference model quality}, due to its tendency to introduce instability and bias. To overcome these limitations, we propose a novel framework for human preference alignment in diffusion models (PC-Diffusion), using a lightweight, trainable Preference Classifier that directly models the relative preference between samples. By restricting preference learning to this classifier, PC-Diffusion decouples preference alignment from the generative model, eliminating the need for entire model fine-tuning and reference model reliance.~We further provide theoretical guarantees for PC-Diffusion:1) PC-Diffusion ensures that the preference-guided distributions are consistently propagated across timesteps. 2)The training objective of the preference classifier is equivalent to DPO, but does not require a reference model.3) The proposed preference-guided correction can progressively steer generation toward preference-aligned regions.~Empirical results show that PC-Diffusion achieves comparable preference consistency to DPO while significantly reducing training costs and enabling efficient and stable preference-guided generation.</li>
</ul>

<h3>Title: DI3CL: Contrastive Learning With Dynamic Instances and Contour Consistency for SAR Land-Cover Classification Foundation Model</h3>
<ul>
<li><strong>Authors: </strong>Zhongle Ren, Hui Ding, Kai Wang, Biao Hou, Xingyu Luo, Weibin Li, Licheng Jiao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.07808">https://arxiv.org/abs/2511.07808</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.07808">https://arxiv.org/pdf/2511.07808</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.07808]] DI3CL: Contrastive Learning With Dynamic Instances and Contour Consistency for SAR Land-Cover Classification Foundation Model(https://arxiv.org/abs/2511.07808)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Although significant advances have been achieved in SAR land-cover classification, recent methods remain predominantly focused on supervised learning, which relies heavily on extensive labeled datasets. This dependency not only limits scalability and generalization but also restricts adaptability to diverse application scenarios. In this paper, a general-purpose foundation model for SAR land-cover classification is developed, serving as a robust cornerstone to accelerate the development and deployment of various downstream models. Specifically, a Dynamic Instance and Contour Consistency Contrastive Learning (DI3CL) pre-training framework is presented, which incorporates a Dynamic Instance (DI) module and a Contour Consistency (CC) module. DI module enhances global contextual awareness by enforcing local consistency across different views of the same region. CC module leverages shallow feature maps to guide the model to focus on the geometric contours of SAR land-cover objects, thereby improving structural discrimination. Additionally, to enhance robustness and generalization during pre-training, a large-scale and diverse dataset named SARSense, comprising 460,532 SAR images, is constructed to enable the model to capture comprehensive and representative features. To evaluate the generalization capability of our foundation model, we conducted extensive experiments across a variety of SAR land-cover classification tasks, including SAR land-cover mapping, water body detection, and road extraction. The results consistently demonstrate that the proposed DI3CL outperforms existing methods. Our code and pre-trained weights are publicly available at: this https URL.</li>
</ul>

<h3>Title: Cancer-Net PCa-MultiSeg: Multimodal Enhancement of Prostate Cancer Lesion Segmentation Using Synthetic Correlated Diffusion Imaging</h3>
<ul>
<li><strong>Authors: </strong>Jarett Dewbury, Chi-en Amy Tai, Alexander Wong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.07816">https://arxiv.org/abs/2511.07816</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.07816">https://arxiv.org/pdf/2511.07816</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.07816]] Cancer-Net PCa-MultiSeg: Multimodal Enhancement of Prostate Cancer Lesion Segmentation Using Synthetic Correlated Diffusion Imaging(https://arxiv.org/abs/2511.07816)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Current deep learning approaches for prostate cancer lesion segmentation achieve limited performance, with Dice scores of 0.32 or lower in large patient cohorts. To address this limitation, we investigate synthetic correlated diffusion imaging (CDI$^s$) as an enhancement to standard diffusion-based protocols. We conduct a comprehensive evaluation across six state-of-the-art segmentation architectures using 200 patients with co-registered CDI$^s$, diffusion-weighted imaging (DWI) and apparent diffusion coefficient (ADC) sequences. We demonstrate that CDI$^s$ integration reliably enhances or preserves segmentation performance in 94% of evaluated configurations, with individual architectures achieving up to 72.5% statistically significant relative improvement over baseline modalities. CDI$^s$ + DWI emerges as the safest enhancement pathway, achieving significant improvements in half of evaluated architectures with zero instances of degradation. Since CDI$^s$ derives from existing DWI acquisitions without requiring additional scan time or architectural modifications, it enables immediate deployment in clinical workflows. Our results establish validated integration pathways for CDI$^s$ as a practical drop-in enhancement for PCa lesion segmentation tasks across diverse deep learning architectures.</li>
</ul>

<h3>Title: Visual Bridge: Universal Visual Perception Representations Generating</h3>
<ul>
<li><strong>Authors: </strong>Yilin Gao, Shuguang Dou, Junzhou Li, Zhiheng Yu, Yin Li, Dongsheng Jiang, Shugong Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.07877">https://arxiv.org/abs/2511.07877</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.07877">https://arxiv.org/pdf/2511.07877</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.07877]] Visual Bridge: Universal Visual Perception Representations Generating(https://arxiv.org/abs/2511.07877)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>Recent advances in diffusion models have achieved remarkable success in isolated computer vision tasks such as text-to-image generation, depth estimation, and optical flow. However, these models are often restricted by a ``single-task-single-model'' paradigm, severely limiting their generalizability and scalability in multi-task scenarios. Motivated by the cross-domain generalization ability of large language models, we propose a universal visual perception framework based on flow matching that can generate diverse visual representations across multiple tasks. Our approach formulates the process as a universal flow-matching problem from image patch tokens to task-specific representations rather than an independent generation or regression problem. By leveraging a strong self-supervised foundation model as the anchor and introducing a multi-scale, circular task embedding mechanism, our method learns a universal velocity field to bridge the gap between heterogeneous tasks, supporting efficient and flexible representation transfer. Extensive experiments on classification, detection, segmentation, depth estimation, and image-text retrieval demonstrate that our model achieves competitive performance in both zero-shot and fine-tuned settings, outperforming prior generalist and several specialist models. Ablation studies further validate the robustness, scalability, and generalization of our framework. Our work marks a significant step towards general-purpose visual perception, providing a solid foundation for future research in universal vision modeling.</li>
</ul>

<h3>Title: Rectified Noise: A Generative Model Using Positive-incentive Noise</h3>
<ul>
<li><strong>Authors: </strong>Zhenyu Gu, Yanchen Xu, Sida Huang, Yubin Guo, Hongyuan Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.07911">https://arxiv.org/abs/2511.07911</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.07911">https://arxiv.org/pdf/2511.07911</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.07911]] Rectified Noise: A Generative Model Using Positive-incentive Noise(https://arxiv.org/abs/2511.07911)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Rectified Flow (RF) has been widely used as an effective generative model. Although RF is primarily based on probability flow Ordinary Differential Equations (ODE), recent studies have shown that injecting noise through reverse-time Stochastic Differential Equations (SDE) for sampling can achieve superior generative performance. Inspired by Positive-incentive Noise ($\pi$-noise), we propose an innovative generative algorithm to train $\pi$-noise generators, namely Rectified Noise ($\Delta$RN), which improves the generative performance by injecting $\pi$-noise into the velocity field of pre-trained RF models. After introducing the Rectified Noise pipeline, pre-trained RF models can be efficiently transformed into $\pi$-noise generators. We validate Rectified Noise by conducting extensive experiments across various model architectures on different datasets. Notably, we find that: (1) RF models using Rectified Noise reduce FID from \textbf{10.16 to 9.05} on ImageNet-1k. (2) The models of $\pi$-noise generators achieve improved performance with only \textbf{0.39\%} additional training parameters.</li>
</ul>

<h3>Title: Feedback Descent: Open-Ended Text Optimization via Pairwise Comparison</h3>
<ul>
<li><strong>Authors: </strong>Yoonho Lee, Joseph Boen, Chelsea Finn</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.07919">https://arxiv.org/abs/2511.07919</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.07919">https://arxiv.org/pdf/2511.07919</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.07919]] Feedback Descent: Open-Ended Text Optimization via Pairwise Comparison(https://arxiv.org/abs/2511.07919)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>We introduce \textit{Feedback Descent}, a framework that optimizes text artifacts -- prompts, code, and molecules -- through structured textual feedback, rather than relying solely on scalar rewards. By preserving detailed critiques instead of compressing them to binary preferences, Feedback Descent widens the information bottleneck in preference learning, enabling directed optimization in text space rather than weight space. We show that in-context learning can transform structured feedback into gradient-like directional information, enabling targeted edits. Unlike prior approaches that collapse judgments into single bits, our evaluators pair each comparison with textual feedback, which functions as high-bandwidth supervision. The iteration loop is done purely at inference time, without modifying any model weights, and is task-agnostic. We evaluate Feedback Descent on three diverse domains and find that it outperforms state-of-the-art prompt optimization (GEPA), reinforcement learning methods (GRPO, REINVENT), and even specialized graph-based molecular optimizers. In the DOCKSTRING molecule discovery benchmark, Feedback Descent identifies novel drug-like molecules surpassing the $99.9$th percentile of a database with more than $260{,}000$ compounds across six protein targets.</li>
</ul>

<h3>Title: IBMA: An Imputation-Based Mixup Augmentation Using Self-Supervised Learning for Time Series Data</h3>
<ul>
<li><strong>Authors: </strong>Dang Nha Nguyen, Hai Dang Nguyen, Khoa Tho Anh Nguyen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.07930">https://arxiv.org/abs/2511.07930</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.07930">https://arxiv.org/pdf/2511.07930</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.07930]] IBMA: An Imputation-Based Mixup Augmentation Using Self-Supervised Learning for Time Series Data(https://arxiv.org/abs/2511.07930)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Data augmentation in time series forecasting plays a crucial role in enhancing model performance by introducing variability while maintaining the underlying temporal patterns. However, time series data offers fewer augmentation strategies compared to fields such as image or text, with advanced techniques like Mixup rarely being used. In this work, we propose a novel approach, Imputation-Based Mixup Augmentation (IBMA), which combines Imputation-Augmented data with Mixup augmentation to bolster model generalization and improve forecasting performance. We evaluate the effectiveness of this method across several forecasting models, including DLinear (MLP), TimesNet (CNN), and iTrainformer (Transformer), these models represent some of the most recent advances in time series forecasting. Our experiments, conducted on four datasets (ETTh1, ETTh2, ETTm1, ETTm2) and compared against eight other augmentation techniques, demonstrate that IBMA consistently enhances performance, achieving 22 improvements out of 24 instances, with 10 of those being the best performances, particularly with iTrainformer imputation.</li>
</ul>

<h3>Title: Laytrol: Preserving Pretrained Knowledge in Layout Control for Multimodal Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Sida Huang, Siqi Huang, Ping Luo, Hongyuan Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.07934">https://arxiv.org/abs/2511.07934</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.07934">https://arxiv.org/pdf/2511.07934</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.07934]] Laytrol: Preserving Pretrained Knowledge in Layout Control for Multimodal Diffusion Transformers(https://arxiv.org/abs/2511.07934)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>With the development of diffusion models, enhancing spatial controllability in text-to-image generation has become a vital challenge. As a representative task for addressing this challenge, layout-to-image generation aims to generate images that are spatially consistent with the given layout condition. Existing layout-to-image methods typically introduce the layout condition by integrating adapter modules into the base generative model. However, the generated images often exhibit low visual quality and stylistic inconsistency with the base model, indicating a loss of pretrained knowledge. To alleviate this issue, we construct the Layout Synthesis (LaySyn) dataset, which leverages images synthesized by the base model itself to mitigate the distribution shift from the pretraining data. Moreover, we propose the Layout Control (Laytrol) Network, in which parameters are inherited from MM-DiT to preserve the pretrained knowledge of the base model. To effectively activate the copied parameters and avoid disturbance from unstable control conditions, we adopt a dedicated initialization scheme for Laytrol. In this scheme, the layout encoder is initialized as a pure text encoder to ensure that its output tokens remain within the data domain of MM-DiT. Meanwhile, the outputs of the layout control network are initialized to zero. In addition, we apply Object-level Rotary Position Embedding to the layout tokens to provide coarse positional information. Qualitative and quantitative experiments demonstrate the effectiveness of our method.</li>
</ul>

<h3>Title: DiffRegCD: Integrated Registration and Change Detection with Diffusion Features</h3>
<ul>
<li><strong>Authors: </strong>Seyedehnanita Madani, Rama Chellappa, Vishal M. Patel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.07935">https://arxiv.org/abs/2511.07935</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.07935">https://arxiv.org/pdf/2511.07935</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.07935]] DiffRegCD: Integrated Registration and Change Detection with Diffusion Features(https://arxiv.org/abs/2511.07935)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Change detection (CD) is fundamental to computer vision and remote sensing, supporting applications in environmental monitoring, disaster response, and urban development. Most CD models assume co-registered inputs, yet real-world imagery often exhibits parallax, viewpoint shifts, and long temporal gaps that cause severe misalignment. Traditional two stage methods that first register and then detect, as well as recent joint frameworks (e.g., BiFA, ChangeRD), still struggle under large displacements, relying on regression only flow, global homographies, or synthetic perturbations. We present DiffRegCD, an integrated framework that unifies dense registration and change detection in a single model. DiffRegCD reformulates correspondence estimation as a Gaussian smoothed classification task, achieving sub-pixel accuracy and stable training. It leverages frozen multi-scale features from a pretrained denoising diffusion model, ensuring robustness to illumination and viewpoint variation. Supervision is provided through controlled affine perturbations applied to standard CD datasets, yielding paired ground truth for both flow and change detection without pseudo labels. Extensive experiments on aerial (LEVIR-CD, DSIFN-CD, WHU-CD, SYSU-CD) and ground level (VL-CMU-CD) datasets show that DiffRegCD consistently surpasses recent baselines and remains reliable under wide temporal and geometric variation, establishing diffusion features and classification based correspondence as a strong foundation for unified change detection.</li>
</ul>

<h3>Title: Continual Unlearning for Text-to-Image Diffusion Models: A Regularization Perspective</h3>
<ul>
<li><strong>Authors: </strong>Justin Lee, Zheda Mai, Jinsu Yoo, Chongyu Fan, Cheng Zhang, Wei-Lun Chao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.07970">https://arxiv.org/abs/2511.07970</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.07970">https://arxiv.org/pdf/2511.07970</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.07970]] Continual Unlearning for Text-to-Image Diffusion Models: A Regularization Perspective(https://arxiv.org/abs/2511.07970)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Machine unlearning--the ability to remove designated concepts from a pre-trained model--has advanced rapidly, particularly for text-to-image diffusion models. However, existing methods typically assume that unlearning requests arrive all at once, whereas in practice they often arrive sequentially. We present the first systematic study of continual unlearning in text-to-image diffusion models and show that popular unlearning methods suffer from rapid utility collapse: after only a few requests, models forget retained knowledge and generate degraded images. We trace this failure to cumulative parameter drift from the pre-training weights and argue that regularization is crucial to addressing it. To this end, we study a suite of add-on regularizers that (1) mitigate drift and (2) remain compatible with existing unlearning methods. Beyond generic regularizers, we show that semantic awareness is essential for preserving concepts close to the unlearning target, and propose a gradient-projection method that constrains parameter drift orthogonal to their subspace. This substantially improves continual unlearning performance and is complementary to other regularizers for further gains. Taken together, our study establishes continual unlearning as a fundamental challenge in text-to-image generation and provides insights, baselines, and open directions for advancing safe and accountable generative AI.</li>
</ul>

<h3>Title: Morphing Through Time: Diffusion-Based Bridging of Temporal Gaps for Robust Alignment in Change Detection</h3>
<ul>
<li><strong>Authors: </strong>Seyedehanita Madani, Vishal M. Patel</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.07976">https://arxiv.org/abs/2511.07976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.07976">https://arxiv.org/pdf/2511.07976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.07976]] Morphing Through Time: Diffusion-Based Bridging of Temporal Gaps for Robust Alignment in Change Detection(https://arxiv.org/abs/2511.07976)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Remote sensing change detection is often challenged by spatial misalignment between bi-temporal images, especially when acquisitions are separated by long seasonal or multi-year gaps. While modern convolutional and transformer-based models perform well on aligned data, their reliance on precise co-registration limits their robustness in real-world conditions. Existing joint registration-detection frameworks typically require retraining and transfer poorly across domains. We introduce a modular pipeline that improves spatial and temporal robustness without altering existing change detection networks. The framework integrates diffusion-based semantic morphing, dense registration, and residual flow refinement. A diffusion module synthesizes intermediate morphing frames that bridge large appearance gaps, enabling RoMa to estimate stepwise correspondences between consecutive frames. The composed flow is then refined through a lightweight U-Net to produce a high-fidelity warp that co-registers the original image pair. Extensive experiments on LEVIR-CD, WHU-CD, and DSIFN-CD show consistent gains in both registration accuracy and downstream change detection across multiple backbones, demonstrating the generality and effectiveness of the proposed approach.</li>
</ul>

<h3>Title: Multi-modal Deepfake Detection and Localization with FPN-Transformer</h3>
<ul>
<li><strong>Authors: </strong>Chende Zheng, Ruiqi Suo, Zhoulin Ji, Jingyi Deng, Fangbin Yi, Chenhao Lin, Chao Shen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.08031">https://arxiv.org/abs/2511.08031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.08031">https://arxiv.org/pdf/2511.08031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.08031]] Multi-modal Deepfake Detection and Localization with FPN-Transformer(https://arxiv.org/abs/2511.08031)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, self-supervised, generative</a></li>
<li><strong>Abstract: </strong>The rapid advancement of generative adversarial networks (GANs) and diffusion models has enabled the creation of highly realistic deepfake content, posing significant threats to digital trust across audio-visual domains. While unimodal detection methods have shown progress in identifying synthetic media, their inability to leverage cross-modal correlations and precisely localize forged segments limits their practicality against sophisticated, fine-grained manipulations. To address this, we introduce a multi-modal deepfake detection and localization framework based on a Feature Pyramid-Transformer (FPN-Transformer), addressing critical gaps in cross-modal generalization and temporal boundary regression. The proposed approach utilizes pre-trained self-supervised models (WavLM for audio, CLIP for video) to extract hierarchical temporal features. A multi-scale feature pyramid is constructed through R-TLM blocks with localized attention mechanisms, enabling joint analysis of cross-context temporal dependencies. The dual-branch prediction head simultaneously predicts forgery probabilities and refines temporal offsets of manipulated segments, achieving frame-level localization precision. We evaluate our approach on the test set of the IJCAI'25 DDL-AV benchmark, showing a good performance with a final score of 0.7535 for cross-modal deepfake detection and localization in challenging environments. Experimental results confirm the effectiveness of our approach and provide a novel way for generalized deepfake detection. Our code is available at this https URL</li>
</ul>

<h3>Title: WEDepth: Efficient Adaptation of World Knowledge for Monocular Depth Estimation</h3>
<ul>
<li><strong>Authors: </strong>Gongshu Wang, Zhirui Wang, Kan Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.08036">https://arxiv.org/abs/2511.08036</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.08036">https://arxiv.org/pdf/2511.08036</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.08036]] WEDepth: Efficient Adaptation of World Knowledge for Monocular Depth Estimation(https://arxiv.org/abs/2511.08036)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model</a></li>
<li><strong>Abstract: </strong>Monocular depth estimation (MDE) has widely applicable but remains highly challenging due to the inherently ill-posed nature of reconstructing 3D scenes from single 2D images. Modern Vision Foundation Models (VFMs), pre-trained on large-scale diverse datasets, exhibit remarkable world understanding capabilities that benefit for various vision tasks. Recent studies have demonstrated significant improvements in MDE through fine-tuning these VFMs. Inspired by these developments, we propose WEDepth, a novel approach that adapts VFMs for MDE without modi-fying their structures and pretrained weights, while effec-tively eliciting and leveraging their inherent priors. Our method employs the VFM as a multi-level feature en-hancer, systematically injecting prior knowledge at differ-ent representation levels. Experiments on NYU-Depth v2 and KITTI datasets show that WEDepth establishes new state-of-the-art (SOTA) performance, achieving competi-tive results compared to both diffusion-based approaches (which require multiple forward passes) and methods pre-trained on relative depth. Furthermore, we demonstrate our method exhibits strong zero-shot transfer capability across diverse scenarios.</li>
</ul>

<h3>Title: Taming Identity Consistency and Prompt Diversity in Diffusion Models via Latent Concatenation and Masked Conditional Flow Matching</h3>
<ul>
<li><strong>Authors: </strong>Aditi Singhania, Arushi Jain, Krutik Malani, Riddhi Dhawan, Souymodip Chakraborty, Vineet Batra, Ankit Phogat</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.08061">https://arxiv.org/abs/2511.08061</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.08061">https://arxiv.org/pdf/2511.08061</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.08061]] Taming Identity Consistency and Prompt Diversity in Diffusion Models via Latent Concatenation and Masked Conditional Flow Matching(https://arxiv.org/abs/2511.08061)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Subject-driven image generation aims to synthesize novel depictions of a specific subject across diverse contexts while preserving its core identity features. Achieving both strong identity consistency and high prompt diversity presents a fundamental trade-off. We propose a LoRA fine-tuned diffusion model employing a latent concatenation strategy, which jointly processes reference and target images, combined with a masked Conditional Flow Matching (CFM) objective. This approach enables robust identity preservation without architectural modifications. To facilitate large-scale training, we introduce a two-stage Distilled Data Curation Framework: the first stage leverages data restoration and VLM-based filtering to create a compact, high-quality seed dataset from diverse sources; the second stage utilizes these curated examples for parameter-efficient fine-tuning, thus scaling the generation capability across various subjects and contexts. Finally, for filtering and quality assessment, we present CHARIS, a fine-grained evaluation framework that performs attribute-level comparisons along five key axes: identity consistency, prompt adherence, region-wise color fidelity, visual quality, and transformation diversity.</li>
</ul>

<h3>Title: CLIP is All You Need for Human-like Semantic Representations in Stable Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Cameron Braunstein, Mariya Toneva, Eddy Ilg</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.08075">https://arxiv.org/abs/2511.08075</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.08075">https://arxiv.org/pdf/2511.08075</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.08075]] CLIP is All You Need for Human-like Semantic Representations in Stable Diffusion(https://arxiv.org/abs/2511.08075)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Latent diffusion models such as Stable Diffusion achieve state-of-the-art results on text-to-image generation tasks. However, the extent to which these models have a semantic understanding of the images they generate is not well understood. In this work, we investigate whether the internal representations used by these models during text-to-image generation contain semantic information that is meaningful to humans. To do so, we perform probing on Stable Diffusion with simple regression layers that predict semantic attributes for objects and evaluate these predictions against human annotations. Surprisingly, we find that this success can actually be attributed to the text encoding occurring in CLIP rather than the reverse diffusion process. We demonstrate that groups of specific semantic attributes have markedly different decoding accuracy than the average, and are thus represented to different degrees. Finally, we show that attributes become more difficult to disambiguate from one another during the inverse diffusion process, further demonstrating the strongest semantic representation of object attributes in CLIP. We conclude that the separately trained CLIP vision-language model is what determines the human-like semantic representation, and that the diffusion process instead takes the role of a visual decoder.</li>
</ul>

<h3>Title: Beyond the Pixels: VLM-based Evaluation of Identity Preservation in Reference-Guided Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Aditi Singhania, Krutik Malani, Riddhi Dhawan, Arushi Jain, Garv Tandon, Nippun Sharma, Souymodip Chakraborty, Vineet Batra, Ankit Phogat</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.08087">https://arxiv.org/abs/2511.08087</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.08087">https://arxiv.org/pdf/2511.08087</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.08087]] Beyond the Pixels: VLM-based Evaluation of Identity Preservation in Reference-Guided Synthesis(https://arxiv.org/abs/2511.08087)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Evaluating identity preservation in generative models remains a critical yet unresolved challenge. Existing metrics rely on global embeddings or coarse VLM prompting, failing to capture fine-grained identity changes and providing limited diagnostic insight. We introduce Beyond the Pixels, a hierarchical evaluation framework that decomposes identity assessment into feature-level transformations. Our approach guides VLMs through structured reasoning by (1) hierarchically decomposing subjects into (type, style) -> attribute -> feature decision tree, and (2) prompting for concrete transformations rather than abstract similarity scores. This decomposition grounds VLM analysis in verifiable visual evidence, reducing hallucinations and improving consistency. We validate our framework across four state-of-the-art generative models, demonstrating strong alignment with human judgments in measuring identity consistency. Additionally, we introduce a new benchmark specifically designed to stress-test generative models. It comprises 1,078 image-prompt pairs spanning diverse subject types, including underrepresented categories such as anthropomorphic and animated characters, and captures an average of six to seven transformation axes per prompt.</li>
</ul>

<h3>Title: StableMorph: High-Quality Face Morph Generation with Stable Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Wassim Kabbani, Kiran Raja, Raghavendra Ramachandra, Christoph Busch</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.08090">https://arxiv.org/abs/2511.08090</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.08090">https://arxiv.org/pdf/2511.08090</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.08090]] StableMorph: High-Quality Face Morph Generation with Stable Diffusion(https://arxiv.org/abs/2511.08090)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Face morphing attacks threaten the integrity of biometric identity systems by enabling multiple individuals to share a single identity. To develop and evaluate effective morphing attack detection (MAD) systems, we need access to high-quality, realistic morphed images that reflect the challenges posed in real-world scenarios. However, existing morph generation methods often produce images that are blurry, riddled with artifacts, or poorly constructed making them easy to detect and not representative of the most dangerous attacks. In this work, we introduce StableMorph, a novel approach that generates highly realistic, artifact-free morphed face images using modern diffusion-based image synthesis. Unlike prior methods, StableMorph produces full-head images with sharp details, avoids common visual flaws, and offers unmatched control over visual attributes. Through extensive evaluation, we show that StableMorph images not only rival or exceed the quality of genuine face images but also maintain a strong ability to fool face recognition systems posing a greater challenge to existing MAD solutions and setting a new standard for morph quality in research and operational testing. StableMorph improves the evaluation of biometric security by creating more realistic and effective attacks and supports the development of more robust detection systems.</li>
</ul>

<h3>Title: Still Not There: Can LLMs Outperform Smaller Task-Specific Seq2Seq Models on the Poetry-to-Prose Conversion Task?</h3>
<ul>
<li><strong>Authors: </strong>Kunal Kingkar Das, Manoj Balaji Jagadeeshan, Nallani Chakravartula Sahith, Jivnesh Sandhan, Pawan Goyal</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.08145">https://arxiv.org/abs/2511.08145</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.08145">https://arxiv.org/pdf/2511.08145</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.08145]] Still Not There: Can LLMs Outperform Smaller Task-Specific Seq2Seq Models on the Poetry-to-Prose Conversion Task?(https://arxiv.org/abs/2511.08145)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are increasingly treated as universal, general-purpose solutions across NLP tasks, particularly in English. But does this assumption hold for low-resource, morphologically rich languages such as Sanskrit? We address this question by comparing instruction-tuned and in-context-prompted LLMs with smaller task-specific encoder-decoder models on the Sanskrit poetry-to-prose conversion task. This task is intrinsically challenging: Sanskrit verse exhibits free word order combined with rigid metrical constraints, and its conversion to canonical prose (anvaya) requires multi-step reasoning involving compound segmentation, dependency resolution, and syntactic linearisation. This makes it an ideal testbed to evaluate whether LLMs can surpass specialised models. For LLMs, we apply instruction fine-tuning on general-purpose models and design in-context learning templates grounded in Paninian grammar and classical commentary heuristics. For task-specific modelling, we fully fine-tune a ByT5-Sanskrit Seq2Seq model. Our experiments show that domain-specific fine-tuning of ByT5-Sanskrit significantly outperforms all instruction-driven LLM approaches. Human evaluation strongly corroborates this result, with scores exhibiting high correlation with Kendall's Tau scores. Additionally, our prompting strategies provide an alternative to fine-tuning when domain-specific verse corpora are unavailable, and the task-specific Seq2Seq model demonstrates robust generalisation on out-of-domain evaluations.</li>
</ul>

<h3>Title: LandSegmenter: Towards a Flexible Foundation Model for Land Use and Land Cover Mapping</h3>
<ul>
<li><strong>Authors: </strong>Chenying Liu, Wei Huang, Xiao Xiang Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.08156">https://arxiv.org/abs/2511.08156</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.08156">https://arxiv.org/pdf/2511.08156</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.08156]] LandSegmenter: Towards a Flexible Foundation Model for Land Use and Land Cover Mapping(https://arxiv.org/abs/2511.08156)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Land Use and Land Cover (LULC) mapping is a fundamental task in Earth Observation (EO). However, current LULC models are typically developed for a specific modality and a fixed class taxonomy, limiting their generability and broader applicability. Recent advances in foundation models (FMs) offer promising opportunities for building universal models. Yet, task-agnostic FMs often require fine-tuning for downstream applications, whereas task-specific FMs rely on massive amounts of labeled data for training, which is costly and impractical in the remote sensing (RS) domain. To address these challenges, we propose LandSegmenter, an LULC FM framework that resolves three-stage challenges at the input, model, and output levels. From the input side, to alleviate the heavy demand on labeled data for FM training, we introduce LAnd Segment (LAS), a large-scale, multi-modal, multi-source dataset built primarily with globally sampled weak labels from existing LULC products. LAS provides a scalable, cost-effective alternative to manual annotation, enabling large-scale FM training across diverse LULC domains. For model architecture, LandSegmenter integrates an RS-specific adapter for cross-modal feature extraction and a text encoder for semantic awareness enhancement. At the output stage, we introduce a class-wise confidence-guided fusion strategy to mitigate semantic omissions and further improve LandSegmenter's zero-shot performance. We evaluate LandSegmenter on six precisely annotated LULC datasets spanning diverse modalities and class taxonomies. Extensive transfer learning and zero-shot experiments demonstrate that LandSegmenter achieves competitive or superior performance, particularly in zero-shot settings when transferred to unseen datasets. These results highlight the efficacy of our proposed framework and the utility of weak supervision for building task-specific FMs.</li>
</ul>

<h3>Title: KPLM-STA: Physically-Accurate Shadow Synthesis for Human Relighting via Keypoint-Based Light Modeling</h3>
<ul>
<li><strong>Authors: </strong>Xinhui Yin, Qifei Li, Yilin Guo, Hongxia Xie, Xiaoli Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.08169">https://arxiv.org/abs/2511.08169</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.08169">https://arxiv.org/pdf/2511.08169</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.08169]] KPLM-STA: Physically-Accurate Shadow Synthesis for Human Relighting via Keypoint-Based Light Modeling(https://arxiv.org/abs/2511.08169)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Image composition aims to seamlessly integrate a foreground object into a background, where generating realistic and geometrically accurate shadows remains a persistent challenge. While recent diffusion-based methods have outperformed GAN-based approaches, existing techniques, such as the diffusion-based relighting framework IC-Light, still fall short in producing shadows with both high appearance realism and geometric precision, especially in composite images. To address these limitations, we propose a novel shadow generation framework based on a Keypoints Linear Model (KPLM) and a Shadow Triangle Algorithm (STA). KPLM models articulated human bodies using nine keypoints and one bounding block, enabling physically plausible shadow projection and dynamic shading across joints, thereby enhancing visual realism. STA further improves geometric accuracy by computing shadow angles, lengths, and spatial positions through explicit geometric formulations. Extensive experiments demonstrate that our method achieves state-of-the-art performance on shadow realism benchmarks, particularly under complex human poses, and generalizes effectively to multi-directional relighting scenarios such as those supported by IC-Light.</li>
</ul>

<h3>Title: VLMDiff: Leveraging Vision-Language Models for Multi-Class Anomaly Detection with Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Samet Hicsonmez, Abd El Rahman Shabayek, Djamila Aouada</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.08173">https://arxiv.org/abs/2511.08173</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.08173">https://arxiv.org/pdf/2511.08173</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.08173]] VLMDiff: Leveraging Vision-Language Models for Multi-Class Anomaly Detection with Diffusion(https://arxiv.org/abs/2511.08173)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, anomaly</a></li>
<li><strong>Abstract: </strong>Detecting visual anomalies in diverse, multi-class real-world images is a significant challenge. We introduce \ours, a novel unsupervised multi-class visual anomaly detection framework. It integrates a Latent Diffusion Model (LDM) with a Vision-Language Model (VLM) for enhanced anomaly localization and detection. Specifically, a pre-trained VLM with a simple prompt extracts detailed image descriptions, serving as additional conditioning for LDM training. Current diffusion-based methods rely on synthetic noise generation, limiting their generalization and requiring per-class model training, which hinders scalability. \ours, however, leverages VLMs to obtain normal captions without manual annotations or additional training. These descriptions condition the diffusion model, learning a robust normal image feature representation for multi-class anomaly detection. Our method achieves competitive performance, improving the pixel-level Per-Region-Overlap (PRO) metric by up to 25 points on the Real-IAD dataset and 8 points on the COCO-AD dataset, outperforming state-of-the-art diffusion-based approaches. Code is available at this https URL.</li>
</ul>

<h3>Title: WarpGAN: Warping-Guided 3D GAN Inversion with Style-Based Novel View Inpainting</h3>
<ul>
<li><strong>Authors: </strong>Kaitao Huang, Yan Yan, Jing-Hao Xue, Hanzi Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.08178">https://arxiv.org/abs/2511.08178</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.08178">https://arxiv.org/pdf/2511.08178</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.08178]] WarpGAN: Warping-Guided 3D GAN Inversion with Style-Based Novel View Inpainting(https://arxiv.org/abs/2511.08178)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>3D GAN inversion projects a single image into the latent space of a pre-trained 3D GAN to achieve single-shot novel view synthesis, which requires visible regions with high fidelity and occluded regions with realism and multi-view consistency. However, existing methods focus on the reconstruction of visible regions, while the generation of occluded regions relies only on the generative prior of 3D GAN. As a result, the generated occluded regions often exhibit poor quality due to the information loss caused by the low bit-rate latent code. To address this, we introduce the warping-and-inpainting strategy to incorporate image inpainting into 3D GAN inversion and propose a novel 3D GAN inversion method, WarpGAN. Specifically, we first employ a 3D GAN inversion encoder to project the single-view image into a latent code that serves as the input to 3D GAN. Then, we perform warping to a novel view using the depth map generated by 3D GAN. Finally, we develop a novel SVINet, which leverages the symmetry prior and multi-view image correspondence w.r.t. the same latent code to perform inpainting of occluded regions in the warped image. Quantitative and qualitative experiments demonstrate that our method consistently outperforms several state-of-the-art methods.</li>
</ul>

<h3>Title: Twist and Compute: The Cost of Pose in 3D Generative Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Kyle Fogarty, Jack Foster, Boqiao Zhang, Jing Yang, Cengiz ztireli</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.08203">https://arxiv.org/abs/2511.08203</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.08203">https://arxiv.org/pdf/2511.08203</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.08203]] Twist and Compute: The Cost of Pose in 3D Generative Diffusion(https://arxiv.org/abs/2511.08203)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Despite their impressive results, large-scale image-to-3D generative models remain opaque in their inductive biases. We identify a significant limitation in image-conditioned 3D generative models: a strong canonical view bias. Through controlled experiments using simple 2D rotations, we show that the state-of-the-art Hunyuan3D 2.0 model can struggle to generalize across viewpoints, with performance degrading under rotated inputs. We show that this failure can be mitigated by a lightweight CNN that detects and corrects input orientation, restoring model performance without modifying the generative backbone. Our findings raise an important open question: Is scale enough, or should we pursue modular, symmetry-aware designs?</li>
</ul>

<h3>Title: Evaluating Gemini LLM in Food Image-Based Recipe and Nutrition Description with EfficientNet-B4 Visual Backbone</h3>
<ul>
<li><strong>Authors: </strong>Rizal Khoirul Anam</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.08215">https://arxiv.org/abs/2511.08215</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.08215">https://arxiv.org/pdf/2511.08215</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.08215]] Evaluating Gemini LLM in Food Image-Based Recipe and Nutrition Description with EfficientNet-B4 Visual Backbone(https://arxiv.org/abs/2511.08215)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The proliferation of digital food applications necessitates robust methods for automated nutritional analysis and culinary guidance. This paper presents a comprehensive comparative evaluation of a decoupled, multimodal pipeline for food recognition. We evaluate a system integrating a specialized visual backbone (EfficientNet-B4) with a powerful generative large language model (Google's Gemini LLM). The core objective is to evaluate the trade-offs between visual classification accuracy, model efficiency, and the quality of generative output (nutritional data and recipes). We benchmark this pipeline against alternative vision backbones (VGG-16, ResNet-50, YOLOv8) and a lightweight LLM (Gemma). We introduce a formalization for "Semantic Error Propagation" (SEP) to analyze how classification inaccuracies from the visual module cascade into the generative output. Our analysis is grounded in a new Custom Chinese Food Dataset (CCFD) developed to address cultural bias in public datasets. Experimental results demonstrate that while EfficientNet-B4 (89.0\% Top-1 Acc.) provides the best balance of accuracy and efficiency, and Gemini (9.2/10 Factual Accuracy) provides superior generative quality, the system's overall utility is fundamentally bottlenecked by the visual front-end's perceptive accuracy. We conduct a detailed per-class analysis, identifying high semantic similarity as the most critical failure mode.</li>
</ul>

<h3>Title: Remodeling Semantic Relationships in Vision-Language Fine-Tuning</h3>
<ul>
<li><strong>Authors: </strong>Xiangyang Wu, Liu Liu, Baosheng Yu, Jiayan Qiu, Zhenwei Shi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.08238">https://arxiv.org/abs/2511.08238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.08238">https://arxiv.org/pdf/2511.08238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.08238]] Remodeling Semantic Relationships in Vision-Language Fine-Tuning(https://arxiv.org/abs/2511.08238)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Vision-language fine-tuning has emerged as an efficient paradigm for constructing multimodal foundation models. While textual context often highlights semantic relationships within an image, existing fine-tuning methods typically overlook this information when aligning vision and language, thus leading to suboptimal performance. Toward solving this problem, we propose a method that can improve multimodal alignment and fusion based on both semantics and this http URL, we first extract multilevel semantic features from different vision encoder to capture more visual cues of the relationships. Then, we learn to project the vision features to group related semantics, among which are more likely to have relationships. Finally, we fuse the visual features with the textual by using inheritable cross-attention, where we globally remove the redundant visual relationships by discarding visual-language feature pairs with low correlation. We evaluate our proposed method on eight foundation models and two downstream tasks, visual question answering and image captioning, and show that it outperforms all existing methods.</li>
</ul>

<h3>Title: Prompt Tuning for Natural Language to SQL with Embedding Fine-Tuning and RAG</h3>
<ul>
<li><strong>Authors: </strong>Jisoo Jang, Tien-Cuong Bui, Yunjun Choi, Wen-Syan Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.08245">https://arxiv.org/abs/2511.08245</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.08245">https://arxiv.org/pdf/2511.08245</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.08245]] Prompt Tuning for Natural Language to SQL with Embedding Fine-Tuning and RAG(https://arxiv.org/abs/2511.08245)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper introduces an Error Correction through Prompt Tuning for NL-to-SQL, leveraging the latest advancements in generative pre-training-based LLMs and RAG. Our work addresses the crucial need for efficient and accurate translation of natural language queries into SQL expressions in various settings with the growing use of natural language interfaces. We explore the evolution of NLIDBs from early rule-based systems to advanced neural network-driven approaches. Drawing inspiration from the medical diagnostic process, we propose a novel framework integrating an error correction mechanism that diagnoses error types, identifies their causes, provides fixing instructions, and applies these corrections to SQL queries. This approach is further enriched by embedding fine-tuning and RAG, which harnesses external knowledge bases for improved accuracy and transparency. Through comprehensive experiments, we demonstrate that our framework achieves a significant 12 percent accuracy improvement over existing baselines, highlighting its potential to revolutionize data access and handling in contemporary data-driven environments.</li>
</ul>

<h3>Title: NERVE: Neighbourhood & Entropy-guided Random-walk for training free open-Vocabulary sEgmentation</h3>
<ul>
<li><strong>Authors: </strong>Kunal Mahatha, Jose Dolz, Christian Desrosiers</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.08248">https://arxiv.org/abs/2511.08248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.08248">https://arxiv.org/pdf/2511.08248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.08248]] NERVE: Neighbourhood & Entropy-guided Random-walk for training free open-Vocabulary sEgmentation(https://arxiv.org/abs/2511.08248)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Despite recent advances in Open-Vocabulary Semantic Segmentation (OVSS), existing training-free methods face several limitations: use of computationally expensive affinity refinement strategies, ineffective fusion of transformer attention maps due to equal weighting or reliance on fixed-size Gaussian kernels to reinforce local spatial smoothness, enforcing isotropic neighborhoods. We propose a strong baseline for training-free OVSS termed as NERVE (Neighbourhood \& Entropy-guided Random-walk for open-Vocabulary sEgmentation), which uniquely integrates global and fine-grained local information, exploiting the neighbourhood structure from the self-attention layer of a stable diffusion model. We also introduce a stochastic random walk for refining the affinity rather than relying on fixed-size Gaussian kernels for local context. This spatial diffusion process encourages propagation across connected and semantically related areas, enabling it to effectively delineate objects with arbitrary shapes. Whereas most existing approaches treat self-attention maps from different transformer heads or layers equally, our method uses entropy-based uncertainty to select the most relevant maps. Notably, our method does not require any conventional post-processing techniques like Conditional Random Fields (CRF) or Pixel-Adaptive Mask Refinement (PAMR). Experiments are performed on 7 popular semantic segmentation benchmarks, yielding an overall state-of-the-art zero-shot segmentation performance, providing an effective approach to open-vocabulary semantic segmentation.</li>
</ul>

<h3>Title: Top2Ground: A Height-Aware Dual Conditioning Diffusion Model for Robust Aerial-to-Ground View Generation</h3>
<ul>
<li><strong>Authors: </strong>Jae Joong Lee, Bedrich Benes</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.08258">https://arxiv.org/abs/2511.08258</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.08258">https://arxiv.org/pdf/2511.08258</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.08258]] Top2Ground: A Height-Aware Dual Conditioning Diffusion Model for Robust Aerial-to-Ground View Generation(https://arxiv.org/abs/2511.08258)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Generating ground-level images from aerial views is a challenging task due to extreme viewpoint disparity, occlusions, and a limited field of view. We introduce Top2Ground, a novel diffusion-based method that directly generates photorealistic ground-view images from aerial input images without relying on intermediate representations such as depth maps or 3D voxels. Specifically, we condition the denoising process on a joint representation of VAE-encoded spatial features (derived from aerial RGB images and an estimated height map) and CLIP-based semantic embeddings. This design ensures the generation is both geometrically constrained by the scene's 3D structure and semantically consistent with its content. We evaluate Top2Ground on three diverse datasets: CVUSA, CVACT, and the Auto Arborist. Our approach shows 7.3% average improvement in SSIM across three benchmark datasets, showing Top2Ground can robustly handle both wide and narrow fields of view, highlighting its strong generalization capabilities.</li>
</ul>

<h3>Title: SynWeather: Weather Observation Data Synthesis across Multiple Regions and Variables via a General Diffusion Transformer</h3>
<ul>
<li><strong>Authors: </strong>Kaiyi Xu, Junchao Gong, Zhiwang Zhou, Zhangrui Li, Yuandong Pu, Yihao Liu, Ben Fei, Fenghua Ling, Wenlong Zhang, Lei Bei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.08291">https://arxiv.org/abs/2511.08291</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.08291">https://arxiv.org/pdf/2511.08291</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.08291]] SynWeather: Weather Observation Data Synthesis across Multiple Regions and Variables via a General Diffusion Transformer(https://arxiv.org/abs/2511.08291)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>With the advancement of meteorological instruments, abundant data has become available. Current approaches are typically focus on single-variable, single-region tasks and primarily rely on deterministic modeling. This limits unified synthesis across variables and regions, overlooks cross-variable complementarity and often leads to over-smoothed results. To address above challenges, we introduce SynWeather, the first dataset designed for Unified Multi-region and Multi-variable Weather Observation Data Synthesis. SynWeather covers four representative regions: the Continental United States, Europe, East Asia, and Tropical Cyclone regions, as well as provides high-resolution observations of key weather variables, including Composite Radar Reflectivity, Hourly Precipitation, Visible Light, and Microwave Brightness Temperature. In addition, we introduce SynWeatherDiff, a general and probabilistic weather synthesis model built upon the Diffusion Transformer framework to address the over-smoothed problem. Experiments on the SynWeather dataset demonstrate the effectiveness of our network compared with both task-specific and general models.</li>
</ul>

<h3>Title: Test-time Diverse Reasoning by Riemannian Activation Steering</h3>
<ul>
<li><strong>Authors: </strong>Ly Tran Ho Khanh, Dongxuan Zhu, Man-Chung Yue, Viet Anh Nguyen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.08305">https://arxiv.org/abs/2511.08305</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.08305">https://arxiv.org/pdf/2511.08305</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.08305]] Test-time Diverse Reasoning by Riemannian Activation Steering(https://arxiv.org/abs/2511.08305)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Best-of-$N$ reasoning improves the accuracy of language models in solving complex tasks by sampling multiple candidate solutions and then selecting the best one based on some criteria. A critical bottleneck for this strategy is the output diversity limit, which occurs when the model generates similar outputs despite stochastic sampling, and hence recites the same error. To address this lack of variance in reasoning paths, we propose a novel unsupervised activation steering strategy that simultaneously optimizes the steering vectors for multiple reasoning trajectories at test time. At any synchronization anchor along the batch generation process, we find the steering vectors that maximize the total volume spanned by all possible intervened activation subsets. We demonstrate that these steering vectors can be determined by solving a Riemannian optimization problem over the product of spheres with a log-determinant objective function. We then use a Riemannian block-coordinate descent algorithm with a well-tuned learning rate to obtain a stationary point of the problem, and we apply these steering vectors until the generation process reaches the subsequent synchronization anchor. Empirical evaluations on popular mathematical benchmarks demonstrate that our test-time Riemannian activation steering strategy outperforms vanilla sampling techniques in terms of generative diversity and solution accuracy.</li>
</ul>

<h3>Title: Empowering DINO Representations for Underwater Instance Segmentation via Aligner and Prompter</h3>
<ul>
<li><strong>Authors: </strong>Zhiyang Chen, Chen Zhang, Hao Fang, Runmin Cong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.08334">https://arxiv.org/abs/2511.08334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.08334">https://arxiv.org/pdf/2511.08334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.08334]] Empowering DINO Representations for Underwater Instance Segmentation via Aligner and Prompter(https://arxiv.org/abs/2511.08334)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Underwater instance segmentation (UIS), integrating pixel-level understanding and instance-level discrimination, is a pivotal technology in marine resource exploration and ecological protection. In recent years, large-scale pretrained visual foundation models, exemplified by DINO, have advanced rapidly and demonstrated remarkable performance on complex downstream tasks. In this paper, we demonstrate that DINO can serve as an effective feature learner for UIS, and we introduce DiveSeg, a novel framework built upon two insightful components: (1) The AquaStyle Aligner, designed to embed underwater color style features into the DINO fine-tuning process, facilitating better adaptation to the underwater domain. (2) The ObjectPrior Prompter, which incorporates binary segmentation-based prompts to deliver object-level priors, provides essential guidance for instance segmentation task that requires both object- and instance-level reasoning. We conduct thorough experiments on the popular UIIS and USIS10K datasets, and the results show that DiveSeg achieves the state-of-the-art performance. Code: this https URL.</li>
</ul>

<h3>Title: HN-MVTS: HyperNetwork-based Multivariate Time Series Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Andrey Savchenko, Oleg Kachan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.08340">https://arxiv.org/abs/2511.08340</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.08340">https://arxiv.org/pdf/2511.08340</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.08340]] HN-MVTS: HyperNetwork-based Multivariate Time Series Forecasting(https://arxiv.org/abs/2511.08340)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Accurate forecasting of multivariate time series data remains a formidable challenge, particularly due to the growing complexity of temporal dependencies in real-world scenarios. While neural network-based models have achieved notable success in this domain, complex channel-dependent models often suffer from performance degradation compared to channel-independent models that do not consider the relationship between components but provide high robustness due to small capacity. In this work, we propose HN-MVTS, a novel architecture that integrates a hypernetwork-based generative prior with an arbitrary neural network forecasting model. The input of this hypernetwork is a learnable embedding matrix of time series components. To restrict the number of new parameters, the hypernetwork learns to generate the weights of the last layer of the target forecasting networks, serving as a data-adaptive regularizer that improves generalization and long-range predictive accuracy. The hypernetwork is used only during the training, so it does not increase the inference time compared to the base forecasting model. Extensive experiments on eight benchmark datasets demonstrate that application of HN-MVTS to the state-of-the-art models (DLinear, PatchTST, TSMixer, etc.) typically improves their performance. Our findings suggest that hypernetwork-driven parameterization offers a promising direction for enhancing existing forecasting techniques in complex scenarios.</li>
</ul>

<h3>Title: Towards Open-Set Myoelectric Gesture Recognition via Dual-Perspective Inconsistency Learning</h3>
<ul>
<li><strong>Authors: </strong>Chen Liu, Can Han, Weishi Xu, Yaqi Wang, Dahong Qian</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.08344">https://arxiv.org/abs/2511.08344</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.08344">https://arxiv.org/pdf/2511.08344</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.08344]] Towards Open-Set Myoelectric Gesture Recognition via Dual-Perspective Inconsistency Learning(https://arxiv.org/abs/2511.08344)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Surface electromyography (sEMG)-based gesture recognition plays a critical role in human-machine interaction (HMI), particularly for rehabilitation and prosthetic control. However, sEMG-based systems often suffer from the scarcity of informative training data, leading to overfitting and poor generalization in deep learning models. Data augmentation offers a promising approach to increasing the size and diversity of training data, where faithfulness and diversity are two critical factors to effectiveness. However, promoting untargeted diversity can result in redundant samples with limited utility. To address these challenges, we propose a novel diffusion-based data augmentation approach, Sparse-Aware Semantic-Guided Diffusion Augmentation (SASG-DA). To enhance generation faithfulness, we introduce the Semantic Representation Guidance (SRG) mechanism by leveraging fine-grained, task-aware semantic representations as generation conditions. To enable flexible and diverse sample generation, we propose a Gaussian Modeling Semantic Modeling (GMSS) strategy, which models the semantic representation distribution and allows stochastic sampling to produce both faithful and diverse samples. To enhance targeted diversity, we further introduce a Sparse-Aware Semantic Sampling strategy to explicitly explore underrepresented regions, improving distribution coverage and sample utility. Extensive experiments on benchmark sEMG datasets, Ninapro DB2, DB4, and DB7, demonstrate that SASG-DA significantly outperforms existing augmentation methods. Overall, our proposed data augmentation approach effectively mitigates overfitting and improves recognition performance and generalization by offering both faithful and diverse samples.</li>
</ul>

<h3>Title: OmniAID: Decoupling Semantic and Artifacts for Universal AI-Generated Image Detection in the Wild</h3>
<ul>
<li><strong>Authors: </strong>Yuncheng Guo, Junyan Ye, Chenjue Zhang, Hengrui Kang, Haohuan Fu, Conghui He, Weijia Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.08423">https://arxiv.org/abs/2511.08423</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.08423">https://arxiv.org/pdf/2511.08423</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.08423]] OmniAID: Decoupling Semantic and Artifacts for Universal AI-Generated Image Detection in the Wild(https://arxiv.org/abs/2511.08423)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>A truly universal AI-Generated Image (AIGI) detector must simultaneously generalize across diverse generative models and varied semantic content. Current state-of-the-art methods learn a single, entangled forgery representation--conflating content-dependent flaws with content-agnostic artifacts--and are further constrained by outdated benchmarks. To overcome these limitations, we propose OmniAID, a novel framework centered on a decoupled Mixture-of-Experts (MoE) architecture. The core of our method is a hybrid expert system engineered to decouple: (1) semantic flaws across distinct content domains, and (2) these content-dependent flaws from content-agnostic universal artifacts. This system employs a set of Routable Specialized Semantic Experts, each for a distinct domain (e.g., human, animal), complemented by a Fixed Universal Artifact Expert. This architecture is trained using a bespoke two-stage strategy: we first train the experts independently with domain-specific hard-sampling to ensure specialization, and subsequently train a lightweight gating network for effective input routing. By explicitly decoupling "what is generated" (content-specific flaws) from "how it is generated" (universal artifacts), OmniAID achieves robust generalization. To address outdated benchmarks and validate real-world applicability, we introduce Mirage, a new large-scale, contemporary dataset. Extensive experiments, using both traditional benchmarks and our Mirage dataset, demonstrate our model surpasses existing monolithic detectors, establishing a new, robust standard for AIGI authentication against modern, in-the-wild threats.</li>
</ul>

<h3>Title: HardFlow: Hard-Constrained Sampling for Flow-Matching Models via Trajectory Optimization</h3>
<ul>
<li><strong>Authors: </strong>Zeyang Li, Kaveh Alim, Navid Azizan</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.08425">https://arxiv.org/abs/2511.08425</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.08425">https://arxiv.org/pdf/2511.08425</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.08425]] HardFlow: Hard-Constrained Sampling for Flow-Matching Models via Trajectory Optimization(https://arxiv.org/abs/2511.08425)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion and flow-matching have emerged as powerful methodologies for generative modeling, with remarkable success in capturing complex data distributions and enabling flexible guidance at inference time. Many downstream applications, however, demand enforcing hard constraints on generated samples (for example, robot trajectories must avoid obstacles), a requirement that goes beyond simple guidance. Prevailing projection-based approaches constrain the entire sampling path to the constraint manifold, which is overly restrictive and degrades sample quality. In this paper, we introduce a novel framework that reformulates hard-constrained sampling as a trajectory optimization problem. Our key insight is to leverage numerical optimal control to steer the sampling trajectory so that constraints are satisfied precisely at the terminal time. By exploiting the underlying structure of flow-matching models and adopting techniques from model predictive control, we transform this otherwise complex constrained optimization problem into a tractable surrogate that can be solved efficiently and effectively. Furthermore, this trajectory optimization perspective offers significant flexibility beyond mere constraint satisfaction, allowing for the inclusion of integral costs to minimize distribution shift and terminal objectives to further enhance sample quality, all within a unified framework. We provide a control-theoretic analysis of our method, establishing bounds on the approximation error between our tractable surrogate and the ideal formulation. Extensive experiments across diverse domains, including robotics (planning), partial differential equations (boundary control), and vision (text-guided image editing), demonstrate that our algorithm, which we name $\textit{HardFlow}$, substantially outperforms existing methods in both constraint satisfaction and sample quality.</li>
</ul>

<h3>Title: Cross-pyramid consistency regularization for semi-supervised medical image segmentation</h3>
<ul>
<li><strong>Authors: </strong>Matus Bojko, Maros Kollar, Marek Jakab, Wanda Benesova</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.08435">https://arxiv.org/abs/2511.08435</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.08435">https://arxiv.org/pdf/2511.08435</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.08435]] Cross-pyramid consistency regularization for semi-supervised medical image segmentation(https://arxiv.org/abs/2511.08435)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Semi-supervised learning (SSL) enables training of powerful models with the assumption of limited, carefully labelled data and a large amount of unlabeled data to support the learning. In this paper, we propose a hybrid consistency learning approach to effectively exploit unlabeled data for semi-supervised medical image segmentation by leveraging Cross-Pyramid Consistency Regularization (CPCR) between two decoders. First, we design a hybrid Dual Branch Pyramid Network (DBPNet), consisting of an encoder and two decoders that differ slightly, each producing a pyramid of perturbed auxiliary predictions across multiple resolution scales. Second, we present a learning strategy for this network named CPCR that combines existing consistency learning and uncertainty minimization approaches on the main output predictions of decoders with our novel regularization term. More specifically, in this term, we extend the soft-labeling setting to pyramid predictions across decoders to support knowledge distillation in deep hierarchical features. Experimental results show that DBPNet with CPCR outperforms five state-of-the-art self-supervised learning methods and has comparable performance with recent ones on a public benchmark dataset.</li>
</ul>

<h3>Title: One Model for All: Universal Pre-training for EEG based Emotion Recognition across Heterogeneous Datasets and Paradigms</h3>
<ul>
<li><strong>Authors: </strong>Xiang Li, You Li, Yazhou Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.08444">https://arxiv.org/abs/2511.08444</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.08444">https://arxiv.org/pdf/2511.08444</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.08444]] One Model for All: Universal Pre-training for EEG based Emotion Recognition across Heterogeneous Datasets and Paradigms(https://arxiv.org/abs/2511.08444)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>EEG-based emotion recognition is hampered by profound dataset heterogeneity (channel/subject variability), hindering generalizable models. Existing approaches struggle to transfer knowledge effectively. We propose 'One Model for All', a universal pre-training framework for EEG analysis across disparate datasets. Our paradigm decouples learning into two stages: (1) Univariate pre-training via self-supervised contrastive learning on individual channels, enabled by a Unified Channel Schema (UCS) that leverages the channel union (e.g., SEED-62ch, DEAP-32ch); (2) Multivariate fine-tuning with a novel 'ART' (Adaptive Resampling Transformer) and 'GAT' (Graph Attention Network) architecture to capture complex spatio-temporal dependencies. Experiments show universal pre-training is an essential stabilizer, preventing collapse on SEED (vs. scratch) and yielding substantial gains on DEAP (+7.65%) and DREAMER (+3.55%). Our framework achieves new SOTA performance on all within-subject benchmarks: SEED (99.27%), DEAP (93.69%), and DREAMER (93.93%). We also show SOTA cross-dataset transfer, achieving 94.08% (intersection) and 93.05% (UCS) on the unseen DREAMER dataset, with the former surpassing the within-domain pre-training benchmark. Ablation studies validate our architecture: the GAT module is critical, yielding a +22.19% gain over GCN on the high-noise DEAP dataset, and its removal causes a catastrophic -16.44% performance drop. This work paves the way for more universal, scalable, and effective pre-trained models for diverse EEG analysis tasks.</li>
</ul>

<h3>Title: Clustering Guided Residual Neural Networks for Multi-Tx Localization in Molecular Communications</h3>
<ul>
<li><strong>Authors: </strong>Ali Sonmez, Erencem Ozbey, Efe Feyzi Mantaroglu, H. Birkan Yilmaz</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.08513">https://arxiv.org/abs/2511.08513</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.08513">https://arxiv.org/pdf/2511.08513</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.08513]] Clustering Guided Residual Neural Networks for Multi-Tx Localization in Molecular Communications(https://arxiv.org/abs/2511.08513)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Transmitter localization in Molecular Communication via Diffusion is a critical topic with many applications. However, accurate localization of multiple transmitters is a challenging problem due to the stochastic nature of diffusion and overlapping molecule distributions at the receiver surface. To address these issues, we introduce clustering-based centroid correction methods that enhance robustness against density variations, and outliers. In addition, we propose two clusteringguided Residual Neural Networks, namely AngleNN for direction refinement and SizeNN for cluster size estimation. Experimental results show that both approaches provide significant improvements with reducing localization error between 69% (2-Tx) and 43% (4-Tx) compared to the K-means.</li>
</ul>

<h3>Title: LeJEPA: Provable and Scalable Self-Supervised Learning Without the Heuristics</h3>
<ul>
<li><strong>Authors: </strong>Randall Balestriero, Yann LeCun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2511.08544">https://arxiv.org/abs/2511.08544</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2511.08544">https://arxiv.org/pdf/2511.08544</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2511.08544]] LeJEPA: Provable and Scalable Self-Supervised Learning Without the Heuristics(https://arxiv.org/abs/2511.08544)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Learning manipulable representations of the world and its dynamics is central to AI. Joint-Embedding Predictive Architectures (JEPAs) offer a promising blueprint, but lack of practical guidance and theory has led to ad-hoc R&D. We present a comprehensive theory of JEPAs and instantiate it in {\bf LeJEPA}, a lean, scalable, and theoretically grounded training objective. First, we identify the isotropic Gaussian as the optimal distribution that JEPAs' embeddings should follow to minimize downstream prediction risk. Second, we introduce a novel objective--{\bf Sketched Isotropic Gaussian Regularization} (SIGReg)--to constrain embeddings to reach that ideal distribution. Combining the JEPA predictive loss with SIGReg yields LeJEPA with numerous theoretical and practical benefits: (i) single trade-off hyperparameter, (ii) linear time and memory complexity, (iii) stability across hyper-parameters, architectures (ResNets, ViTs, ConvNets) and domains, (iv) heuristics-free, e.g., no stop-gradient, no teacher-student, no hyper-parameter schedulers, and (v) distributed training-friendly implementation requiring only $\approx$50 lines of code. Our empirical validation covers 10+ datasets, 60+ architectures, all with varying scales and domains. As an example, using imagenet-1k for pretraining and linear evaluation with frozen backbone, LeJEPA reaches 79\% with a ViT-H/14. We hope that the simplicity and theory-friendly ecosystem offered by LeJEPA will reestablish self-supervised pre-training as a core pillar of AI research (\href{git@github.com:rbalestr-lab/lejepa.git}{GitHub repo}).</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
