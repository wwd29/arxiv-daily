<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-01-19</h1>
<h3>Title: CRD: Collaborative Representation Distance for Practical Anomaly  Detection</h3>
<ul>
<li><strong>Authors: </strong>Chao Han, Yudong Yan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09443">https://arxiv.org/abs/2401.09443</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09443">https://arxiv.org/pdf/2401.09443</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09443]] CRD: Collaborative Representation Distance for Practical Anomaly  Detection(https://arxiv.org/abs/2401.09443)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Visual defect detection plays an important role in intelligent industry. Patch based methods consider visual images as a collection of image patches according to positions, which have stronger discriminative ability for small defects in products, e.g. scratches on pills. However, the nearest neighbor search for the query image and the stored patches will occupy $O(n)$ complexity in terms of time and space requirements, posing strict challenges for deployment in edge environments. In this paper, we propose an alternative approach to the distance calculation of image patches via collaborative representation models. Starting from the nearest neighbor distance with $L_0$ constraint, we relax the constraint to $L_2$ constraint and solve the distance quickly in close-formed without actually accessing the original stored collection of image patches. Furthermore, we point out that the main computational burden of this close-formed solution can be pre-computed by high-performance server before deployment. Consequently, the distance calculation on edge devices only requires a simple matrix multiplication, which is extremely lightweight and GPU-friendly. Performance on real industrial scenarios demonstrates that compared to the existing state-of-the-art methods, this distance achieves several hundred times improvement in computational efficiency with slight performance drop, while greatly reducing memory overhead.</li>
</ul>

<h3>Title: Uncertainty-Aware Hardware Trojan Detection Using Multimodal Deep  Learning</h3>
<ul>
<li><strong>Authors: </strong>Rahul Vishwakarma, Amin Rezaei</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09479">https://arxiv.org/abs/2401.09479</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09479">https://arxiv.org/pdf/2401.09479</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09479]] Uncertainty-Aware Hardware Trojan Detection Using Multimodal Deep  Learning(https://arxiv.org/abs/2401.09479)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The risk of hardware Trojans being inserted at various stages of chip production has increased in a zero-trust fabless era. To counter this, various machine learning solutions have been developed for the detection of hardware Trojans. While most of the focus has been on either a statistical or deep learning approach, the limited number of Trojan-infected benchmarks affects the detection accuracy and restricts the possibility of detecting zero-day Trojans. To close the gap, we first employ generative adversarial networks to amplify our data in two alternative representation modalities, a graph and a tabular, ensuring that the dataset is distributed in a representative manner. Further, we propose a multimodal deep learning approach to detect hardware Trojans and evaluate the results from both early fusion and late fusion strategies. We also estimate the uncertainty quantification metrics of each prediction for risk-aware decision-making. The outcomes not only confirms the efficacy of our proposed hardware Trojan detection method but also opens a new door for future studies employing multimodality and uncertainty quantification to address other hardware security challenges.</li>
</ul>

<h3>Title: PUPAE: Intuitive and Actionable Explanations for Time Series Anomalies</h3>
<ul>
<li><strong>Authors: </strong>Audrey Der, Chin-Chia Michael Yeh, Yan Zheng, Junpeng Wang, Zhongfang Zhuang, Liang Wang, Wei Zhang, Eamonn J. Keogh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09489">https://arxiv.org/abs/2401.09489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09489">https://arxiv.org/pdf/2401.09489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09489]] PUPAE: Intuitive and Actionable Explanations for Time Series Anomalies(https://arxiv.org/abs/2401.09489)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>In recent years there has been significant progress in time series anomaly detection. However, after detecting an (perhaps tentative) anomaly, can we explain it? Such explanations would be useful to triage anomalies. For example, in an oil refinery, should we respond to an anomaly by dispatching a hydraulic engineer, or an intern to replace the battery on a sensor? There have been some parallel efforts to explain anomalies, however many proposed techniques produce explanations that are indirect, and often seem more complex than the anomaly they seek to explain. Our review of the literature/checklists/user-manuals used by frontline practitioners in various domains reveals an interesting near-universal commonality. Most practitioners discuss, explain and report anomalies in the following format: The anomaly would be like normal data A, if not for the corruption B. The reader will appreciate that is a type of counterfactual explanation. In this work we introduce a domain agnostic counterfactual explanation technique to produce explanations for time series anomalies. As we will show, our method can produce both visual and text-based explanations that are objectively correct, intuitive and in many circumstances, directly actionable.</li>
</ul>

<h3>Title: IPR-NeRF: Ownership Verification meets Neural Radiance Field</h3>
<ul>
<li><strong>Authors: </strong>Win Kent Ong, Kam Woh Ng, Chee Seng Chan, Yi Zhe Song, Tao Xiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09495">https://arxiv.org/abs/2401.09495</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09495">https://arxiv.org/pdf/2401.09495</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09495]] IPR-NeRF: Ownership Verification meets Neural Radiance Field(https://arxiv.org/abs/2401.09495)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Neural Radiance Field (NeRF) models have gained significant attention in the computer vision community in the recent past with state-of-the-art visual quality and produced impressive demonstrations. Since then, technopreneurs have sought to leverage NeRF models into a profitable business. Therefore, NeRF models make it worth the risk of plagiarizers illegally copying, re-distributing, or misusing those models. This paper proposes a comprehensive intellectual property (IP) protection framework for the NeRF model in both black-box and white-box settings, namely IPR-NeRF. In the black-box setting, a diffusion-based solution is introduced to embed and extract the watermark via a two-stage optimization process. In the white-box setting, a designated digital signature is embedded into the weights of the NeRF model by adopting the sign loss objective. Our extensive experiments demonstrate that not only does our approach maintain the fidelity (\ie, the rendering quality) of IPR-NeRF models, but it is also robust against both ambiguity and removal attacks compared to prior arts.</li>
</ul>

<h3>Title: Dimensional Neuroimaging Endophenotypes: Neurobiological Representations  of Disease Heterogeneity Through Machine Learning</h3>
<ul>
<li><strong>Authors: </strong>Junhao Wen, Mathilde Antoniades, Zhijian Yang, Gyujoon Hwang, Ioanna Skampardoni, Rongguang Wang, Christos Davatzikos</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.IV, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09517">https://arxiv.org/abs/2401.09517</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09517">https://arxiv.org/pdf/2401.09517</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09517]] Dimensional Neuroimaging Endophenotypes: Neurobiological Representations  of Disease Heterogeneity Through Machine Learning(https://arxiv.org/abs/2401.09517)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Machine learning has been increasingly used to obtain individualized neuroimaging signatures for disease diagnosis, prognosis, and response to treatment in neuropsychiatric and neurodegenerative disorders. Therefore, it has contributed to a better understanding of disease heterogeneity by identifying disease subtypes that present significant differences in various brain phenotypic measures. In this review, we first present a systematic literature overview of studies using machine learning and multimodal MRI to unravel disease heterogeneity in various neuropsychiatric and neurodegenerative disorders, including Alzheimer disease, schizophrenia, major depressive disorder, autism spectrum disorder, multiple sclerosis, as well as their potential in transdiagnostic settings. Subsequently, we summarize relevant machine learning methodologies and discuss an emerging paradigm which we call dimensional neuroimaging endophenotype (DNE). DNE dissects the neurobiological heterogeneity of neuropsychiatric and neurodegenerative disorders into a low dimensional yet informative, quantitative brain phenotypic representation, serving as a robust intermediate phenotype (i.e., endophenotype) largely reflecting underlying genetics and etiology. Finally, we discuss the potential clinical implications of the current findings and envision future research avenues.</li>
</ul>

<h3>Title: Efficient generative adversarial networks using linear  additive-attention Transformers</h3>
<ul>
<li><strong>Authors: </strong>Emilio Morales-Juarez, Gibran Fuentes-Pineda</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09596">https://arxiv.org/abs/2401.09596</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09596">https://arxiv.org/pdf/2401.09596</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09596]] Efficient generative adversarial networks using linear  additive-attention Transformers(https://arxiv.org/abs/2401.09596)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Although the capacity of deep generative models for image generation, such as Diffusion Models (DMs) and Generative Adversarial Networks (GANs), has dramatically improved in recent years, much of their success can be attributed to computationally expensive architectures. This has limited their adoption and use to research laboratories and companies with large resources, while significantly raising the carbon footprint for training, fine-tuning, and inference. In this work, we present LadaGAN, an efficient generative adversarial network that is built upon a novel Transformer block named Ladaformer. The main component of this block is a linear additive-attention mechanism that computes a single attention vector per head instead of the quadratic dot-product attention. We employ Ladaformer in both the generator and discriminator, which reduces the computational complexity and overcomes the training instabilities often associated with Transformer GANs. LadaGAN consistently outperforms existing convolutional and Transformer GANs on benchmark datasets at different resolutions while being significantly more efficient. Moreover, LadaGAN shows competitive performance compared to state-of-the-art multi-step generative models (e.g. DMs) using orders of magnitude less computational resources.</li>
</ul>

<h3>Title: HCVP: Leveraging Hierarchical Contrastive Visual Prompt for Domain  Generalization</h3>
<ul>
<li><strong>Authors: </strong>Guanglin Zhou, Zhongyi Han, Shiming Chen, Biwei Huang, Liming Zhu, Tongliang Liu, Lina Yao, Kun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09716">https://arxiv.org/abs/2401.09716</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09716">https://arxiv.org/pdf/2401.09716</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09716]] HCVP: Leveraging Hierarchical Contrastive Visual Prompt for Domain  Generalization(https://arxiv.org/abs/2401.09716)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Domain Generalization (DG) endeavors to create machine learning models that excel in unseen scenarios by learning invariant features. In DG, the prevalent practice of constraining models to a fixed structure or uniform parameterization to encapsulate invariant features can inadvertently blend specific aspects. Such an approach struggles with nuanced differentiation of inter-domain variations and may exhibit bias towards certain domains, hindering the precise learning of domain-invariant features. Recognizing this, we introduce a novel method designed to supplement the model with domain-level and task-specific characteristics. This approach aims to guide the model in more effectively separating invariant features from specific characteristics, thereby boosting the generalization. Building on the emerging trend of visual prompts in the DG paradigm, our work introduces the novel \textbf{H}ierarchical \textbf{C}ontrastive \textbf{V}isual \textbf{P}rompt (HCVP) methodology. This represents a significant advancement in the field, setting itself apart with a unique generative approach to prompts, alongside an explicit model structure and specialized loss functions. Differing from traditional visual prompts that are often shared across entire datasets, HCVP utilizes a hierarchical prompt generation network enhanced by prompt contrastive learning. These generative prompts are instance-dependent, catering to the unique characteristics inherent to different domains and tasks. Additionally, we devise a prompt modulation network that serves as a bridge, effectively incorporating the generated visual prompts into the vision transformer backbone. Experiments conducted on five DG datasets demonstrate the effectiveness of HCVP, outperforming both established DG algorithms and adaptation protocols.</li>
</ul>

<h3>Title: Image Translation as Diffusion Visual Programmers</h3>
<ul>
<li><strong>Authors: </strong>Cheng Han, James C. Liang, Qifan Wang, Majid Rabbani, Sohail Dianat, Raghuveer Rao, Ying Nian Wu, Dongfang Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09742">https://arxiv.org/abs/2401.09742</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09742">https://arxiv.org/pdf/2401.09742</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09742]] Image Translation as Diffusion Visual Programmers(https://arxiv.org/abs/2401.09742)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, in-context</a></li>
<li><strong>Abstract: </strong>We introduce the novel Diffusion Visual Programmer (DVP), a neuro-symbolic image translation framework. Our proposed DVP seamlessly embeds a condition-flexible diffusion model within the GPT architecture, orchestrating a coherent sequence of visual programs (i.e., computer vision models) for various pro-symbolic steps, which span RoI identification, style transfer, and position manipulation, facilitating transparent and controllable image translation processes. Extensive experiments demonstrate DVP's remarkable performance, surpassing concurrent arts. This success can be attributed to several key features of DVP: First, DVP achieves condition-flexible translation via instance normalization, enabling the model to eliminate sensitivity caused by the manual guidance and optimally focus on textual descriptions for high-quality content generation. Second, the framework enhances in-context reasoning by deciphering intricate high-dimensional concepts in feature spaces into more accessible low-dimensional symbols (e.g., [Prompt], [RoI object]), allowing for localized, context-free editing while maintaining overall coherence. Last but not least, DVP improves systemic controllability and explainability by offering explicit symbolic representations at each programming stage, empowering users to intuitively interpret and modify results. Our research marks a substantial step towards harmonizing artificial image translation processes with cognitive intelligence, promising broader applications.</li>
</ul>

<h3>Title: CLIP Model for Images to Textual Prompts Based on Top-k Neighbors</h3>
<ul>
<li><strong>Authors: </strong>Xin Zhang, Xin Zhang, YeMing Cai, Tianzhi Jia</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09763">https://arxiv.org/abs/2401.09763</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09763">https://arxiv.org/pdf/2401.09763</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09763]] CLIP Model for Images to Textual Prompts Based on Top-k Neighbors(https://arxiv.org/abs/2401.09763)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Text-to-image synthesis, a subfield of multimodal generation, has gained significant attention in recent years. We propose a cost-effective approach for image-to-prompt generation that leverages generative models to generate textual prompts without the need for large amounts of annotated data. We divide our method into two stages: online stage and offline stage. We use a combination of the CLIP model and K-nearest neighbors (KNN) algorithm. The proposed system consists of two main parts: an offline task and an online task. Our method owns the highest metric 0.612 among these models, which is 0.013, 0.055, 0.011 higher than Clip, Clip + KNN(top 10) respectively.</li>
</ul>

<h3>Title: Leveraging Biases in Large Language Models: "bias-kNN'' for Effective  Few-Shot Learning</h3>
<ul>
<li><strong>Authors: </strong>Yong Zhang, Hanzhang Li, Zhitao Li, Ning Cheng, Ming Li, Jing Xiao, Jianzong Wang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09783">https://arxiv.org/abs/2401.09783</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09783">https://arxiv.org/pdf/2401.09783</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09783]] Leveraging Biases in Large Language Models: "bias-kNN'' for Effective  Few-Shot Learning(https://arxiv.org/abs/2401.09783)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown significant promise in various applications, including zero-shot and few-shot learning. However, their performance can be hampered by inherent biases. Instead of traditionally sought methods that aim to minimize or correct these biases, this study introduces a novel methodology named ``bias-kNN''. This approach capitalizes on the biased outputs, harnessing them as primary features for kNN and supplementing with gold labels. Our comprehensive evaluations, spanning diverse domain text classification datasets and different GPT-2 model sizes, indicate the adaptability and efficacy of the ``bias-kNN'' method. Remarkably, this approach not only outperforms conventional in-context learning in few-shot scenarios but also demonstrates robustness across a spectrum of samples, templates and verbalizers. This study, therefore, presents a unique perspective on harnessing biases, transforming them into assets for enhanced model performance.</li>
</ul>

<h3>Title: PatchAD: Patch-based MLP-Mixer for Time Series Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Zhijie Zhong, Zhiwen Yu, Yiyuan Yang, Weizheng Wang, Kaixiang Yang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09793">https://arxiv.org/abs/2401.09793</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09793">https://arxiv.org/pdf/2401.09793</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09793]] PatchAD: Patch-based MLP-Mixer for Time Series Anomaly Detection(https://arxiv.org/abs/2401.09793)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Anomaly detection stands as a crucial aspect of time series analysis, aiming to identify abnormal events in time series samples. The central challenge of this task lies in effectively learning the representations of normal and abnormal patterns in a label-lacking scenario. Previous research mostly relied on reconstruction-based approaches, restricting the representational abilities of the models. In addition, most of the current deep learning-based methods are not lightweight enough, which prompts us to design a more efficient framework for anomaly detection. In this study, we introduce PatchAD, a novel multi-scale patch-based MLP-Mixer architecture that leverages contrastive learning for representational extraction and anomaly detection. Specifically, PatchAD is composed of four distinct MLP Mixers, exclusively utilizing the MLP architecture for high efficiency and lightweight architecture. Additionally, we also innovatively crafted a dual project constraint module to mitigate potential model degradation. Comprehensive experiments demonstrate that PatchAD achieves state-of-the-art results across multiple real-world multivariate time series datasets. Our code is publicly available.\footnote{\url{https://github.com/EmorZz1G/PatchAD}}</li>
</ul>

<h3>Title: Wavelet-Guided Acceleration of Text Inversion in Diffusion-Based Image  Editing</h3>
<ul>
<li><strong>Authors: </strong>Gwanhyeong Koo, Sunjae Yoon, Chang D. Yoo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09794">https://arxiv.org/abs/2401.09794</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09794">https://arxiv.org/pdf/2401.09794</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09794]] Wavelet-Guided Acceleration of Text Inversion in Diffusion-Based Image  Editing(https://arxiv.org/abs/2401.09794)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In the field of image editing, Null-text Inversion (NTI) enables fine-grained editing while preserving the structure of the original image by optimizing null embeddings during the DDIM sampling process. However, the NTI process is time-consuming, taking more than two minutes per image. To address this, we introduce an innovative method that maintains the principles of the NTI while accelerating the image editing process. We propose the WaveOpt-Estimator, which determines the text optimization endpoint based on frequency characteristics. Utilizing wavelet transform analysis to identify the image's frequency characteristics, we can limit text optimization to specific timesteps during the DDIM sampling process. By adopting the Negative-Prompt Inversion (NPI) concept, a target prompt representing the original image serves as the initial text value for optimization. This approach maintains performance comparable to NTI while reducing the average editing time by over 80% compared to the NTI method. Our method presents a promising approach for efficient, high-quality image editing based on diffusion models.</li>
</ul>

<h3>Title: Boosting Few-Shot Semantic Segmentation Via Segment Anything Model</h3>
<ul>
<li><strong>Authors: </strong>Chen-Bin Feng, Qi Lai, Kangdao Liu, Houcheng Su, Chi-Man Vong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09826">https://arxiv.org/abs/2401.09826</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09826">https://arxiv.org/pdf/2401.09826</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09826]] Boosting Few-Shot Semantic Segmentation Via Segment Anything Model(https://arxiv.org/abs/2401.09826)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>In semantic segmentation, accurate prediction masks are crucial for downstream tasks such as medical image analysis and image editing. Due to the lack of annotated data, few-shot semantic segmentation (FSS) performs poorly in predicting masks with precise contours. Recently, we have noticed that the large foundation model segment anything model (SAM) performs well in processing detailed features. Inspired by SAM, we propose FSS-SAM to boost FSS methods by addressing the issue of inaccurate contour. The FSS-SAM is training-free. It works as a post-processing tool for any FSS methods and can improve the accuracy of predicted masks. Specifically, we use predicted masks from FSS methods to generate prompts and then use SAM to predict new masks. To avoid predicting wrong masks with SAM, we propose a prediction result selection (PRS) algorithm. The algorithm can remarkably decrease wrong predictions. Experiment results on public datasets show that our method is superior to base FSS methods in both quantitative and qualitative aspects.</li>
</ul>

<h3>Title: Exploring Latent Cross-Channel Embedding for Accurate 3D Human Pose  Reconstruction in a Diffusion Framework</h3>
<ul>
<li><strong>Authors: </strong>Junkun Jiang, Jie Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09836">https://arxiv.org/abs/2401.09836</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09836">https://arxiv.org/pdf/2401.09836</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09836]] Exploring Latent Cross-Channel Embedding for Accurate 3D Human Pose  Reconstruction in a Diffusion Framework(https://arxiv.org/abs/2401.09836)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Monocular 3D human pose estimation poses significant challenges due to the inherent depth ambiguities that arise during the reprojection process from 2D to 3D. Conventional approaches that rely on estimating an over-fit projection matrix struggle to effectively address these challenges and often result in noisy outputs. Recent advancements in diffusion models have shown promise in incorporating structural priors to address reprojection ambiguities. However, there is still ample room for improvement as these methods often overlook the exploration of correlation between the 2D and 3D joint-level features. In this study, we propose a novel cross-channel embedding framework that aims to fully explore the correlation between joint-level features of 3D coordinates and their 2D projections. In addition, we introduce a context guidance mechanism to facilitate the propagation of joint graph attention across latent channels during the iterative diffusion process. To evaluate the effectiveness of our proposed method, we conduct experiments on two benchmark datasets, namely Human3.6M and MPI-INF-3DHP. Our results demonstrate a significant improvement in terms of reconstruction accuracy compared to state-of-the-art methods. The code for our method will be made available online for further reference.</li>
</ul>

<h3>Title: GA-SmaAt-GNet: Generative Adversarial Small Attention GNet for Extreme  Precipitation Nowcasting</h3>
<ul>
<li><strong>Authors: </strong>Eloy Reulen, Siamak Mehrkanoon</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.ao-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09881">https://arxiv.org/abs/2401.09881</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09881">https://arxiv.org/pdf/2401.09881</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09881]] GA-SmaAt-GNet: Generative Adversarial Small Attention GNet for Extreme  Precipitation Nowcasting(https://arxiv.org/abs/2401.09881)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In recent years, data-driven modeling approaches have gained considerable traction in various meteorological applications, particularly in the realm of weather forecasting. However, these approaches often encounter challenges when dealing with extreme weather conditions. In light of this, we propose GA-SmaAt-GNet, a novel generative adversarial architecture that makes use of two methodologies aimed at enhancing the performance of deep learning models for extreme precipitation nowcasting. Firstly, it uses a novel SmaAt-GNet built upon the successful SmaAt-UNet architecture as generator. This network incorporates precipitation masks (binarized precipitation maps) as an additional data source, leveraging valuable information for improved predictions. Additionally, GA-SmaAt-GNet utilizes an attention-augmented discriminator inspired by the well-established Pix2Pix architecture. Furthermore, we assess the performance of GA-SmaAt-GNet using real-life precipitation dataset from the Netherlands. Our experimental results reveal a notable improvement in both overall performance and for extreme precipitation events. Furthermore, we conduct uncertainty analysis on the proposed GA-SmaAt-GNet model as well as on the precipitation dataset, providing additional insights into the predictive capabilities of the model. Finally, we offer further insights into the predictions of our proposed model using Grad-CAM. This visual explanation technique generates activation heatmaps, illustrating areas of the input that are more activated for various parts of the network.</li>
</ul>

<h3>Title: Question-Answer Cross Language Image Matching for Weakly Supervised  Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Songhe Deng, Wei Zhuo, Jinheng Xie, Linlin Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09883">https://arxiv.org/abs/2401.09883</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09883">https://arxiv.org/pdf/2401.09883</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09883]] Question-Answer Cross Language Image Matching for Weakly Supervised  Semantic Segmentation(https://arxiv.org/abs/2401.09883)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Class Activation Map (CAM) has emerged as a popular tool for weakly supervised semantic segmentation (WSSS), allowing the localization of object regions in an image using only image-level labels. However, existing CAM methods suffer from under-activation of target object regions and false-activation of background regions due to the fact that a lack of detailed supervision can hinder the model's ability to understand the image as a whole. In this paper, we propose a novel Question-Answer Cross-Language-Image Matching framework for WSSS (QA-CLIMS), leveraging the vision-language foundation model to maximize the text-based understanding of images and guide the generation of activation maps. First, a series of carefully designed questions are posed to the VQA (Visual Question Answering) model with Question-Answer Prompt Engineering (QAPE) to generate a corpus of both foreground target objects and backgrounds that are adaptive to query images. We then employ contrastive learning in a Region Image Text Contrastive (RITC) network to compare the obtained foreground and background regions with the generated corpus. Our approach exploits the rich textual information from the open vocabulary as additional supervision, enabling the model to generate high-quality CAMs with a more complete object region and reduce false-activation of background regions. We conduct extensive analysis to validate the proposed method and show that our approach performs state-of-the-art on both PASCAL VOC 2012 and MS COCO datasets. Code is available at: https://github.com/CVI-SZU/QA-CLIMS</li>
</ul>

<h3>Title: BlenDA: Domain Adaptive Object Detection through diffusion-based  blending</h3>
<ul>
<li><strong>Authors: </strong>Tzuhsuan Huang, Chen-Che Huang, Chung-Hao Ku, Jun-Cheng Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09921">https://arxiv.org/abs/2401.09921</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09921">https://arxiv.org/pdf/2401.09921</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09921]] BlenDA: Domain Adaptive Object Detection through diffusion-based  blending(https://arxiv.org/abs/2401.09921)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Unsupervised domain adaptation (UDA) aims to transfer a model learned using labeled data from the source domain to unlabeled data in the target domain. To address the large domain gap issue between the source and target domains, we propose a novel regularization method for domain adaptive object detection, BlenDA, by generating the pseudo samples of the intermediate domains and their corresponding soft domain labels for adaptation training. The intermediate samples are generated by dynamically blending the source images with their corresponding translated images using an off-the-shelf pre-trained text-to-image diffusion model which takes the text label of the target domain as input and has demonstrated superior image-to-image translation quality. Based on experimental results from two adaptation benchmarks, our proposed approach can significantly enhance the performance of the state-of-the-art domain adaptive object detector, Adversarial Query Transformer (AQT). Particularly, in the Cityscapes to Foggy Cityscapes adaptation, we achieve an impressive 53.4% mAP on the Foggy Cityscapes dataset, surpassing the previous state-of-the-art by 1.5%. It is worth noting that our proposed method is also applicable to various paradigms of domain adaptive object detection. The code is available at:https://github.com/aiiu-lab/BlenDA</li>
</ul>

<h3>Title: CustomVideo: Customizing Text-to-Video Generation with Multiple Subjects</h3>
<ul>
<li><strong>Authors: </strong>Zhao Wang, Aoxue Li, Enze Xie, Lingting Zhu, Yong Guo, Qi Dou, Zhenguo Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.09962">https://arxiv.org/abs/2401.09962</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.09962">https://arxiv.org/pdf/2401.09962</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.09962]] CustomVideo: Customizing Text-to-Video Generation with Multiple Subjects(https://arxiv.org/abs/2401.09962)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Customized text-to-video generation aims to generate high-quality videos guided by text prompts and subject references. Current approaches designed for single subjects suffer from tackling multiple subjects, which is a more challenging and practical scenario. In this work, we aim to promote multi-subject guided text-to-video customization. We propose CustomVideo, a novel framework that can generate identity-preserving videos with the guidance of multiple subjects. To be specific, firstly, we encourage the co-occurrence of multiple subjects via composing them in a single image. Further, upon a basic text-to-video diffusion model, we design a simple yet effective attention control strategy to disentangle different subjects in the latent space of diffusion model. Moreover, to help the model focus on the specific object area, we segment the object from given reference images and provide a corresponding object mask for attention learning. Also, we collect a multi-subject text-to-video generation dataset as a comprehensive benchmark, with 69 individual subjects and 57 meaningful pairs. Extensive qualitative, quantitative, and user study results demonstrate the superiority of our method, compared with the previous state-of-the-art approaches.</li>
</ul>

<h3>Title: Gender Bias in Machine Translation and The Era of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Eva Vanmassenhove</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10016">https://arxiv.org/abs/2401.10016</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10016">https://arxiv.org/pdf/2401.10016</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10016]] Gender Bias in Machine Translation and The Era of Large Language Models(https://arxiv.org/abs/2401.10016)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This chapter examines the role of Machine Translation in perpetuating gender bias, highlighting the challenges posed by cross-linguistic settings and statistical dependencies. A comprehensive overview of relevant existing work related to gender bias in both conventional Neural Machine Translation approaches and Generative Pretrained Transformer models employed as Machine Translation systems is provided. Through an experiment using ChatGPT (based on GPT-3.5) in an English-Italian translation context, we further assess ChatGPT's current capacity to address gender bias. The findings emphasize the ongoing need for advancements in mitigating bias in Machine Translation systems and underscore the importance of fostering fairness and inclusivity in language technologies.</li>
</ul>

<h3>Title: DiffusionGPT: LLM-Driven Text-to-Image Generation System</h3>
<ul>
<li><strong>Authors: </strong>Jie Qin, Jie Wu, Weifeng Chen, Yuxi Ren, Huixia Li, Hefeng Wu, Xuefeng Xiao, Rui Wang, Shilei Wen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10061">https://arxiv.org/abs/2401.10061</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10061">https://arxiv.org/pdf/2401.10061</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10061]] DiffusionGPT: LLM-Driven Text-to-Image Generation System(https://arxiv.org/abs/2401.10061)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have opened up new avenues for the field of image generation, resulting in the proliferation of high-quality models shared on open-source platforms. However, a major challenge persists in current text-to-image systems are often unable to handle diverse inputs, or are limited to single model results. Current unified attempts often fall into two orthogonal aspects: i) parse Diverse Prompts in input stage; ii) activate expert model to output. To combine the best of both worlds, we propose DiffusionGPT, which leverages Large Language Models (LLM) to offer a unified generation system capable of seamlessly accommodating various types of prompts and integrating domain-expert models. DiffusionGPT constructs domain-specific Trees for various generative models based on prior knowledge. When provided with an input, the LLM parses the prompt and employs the Trees-of-Thought to guide the selection of an appropriate model, thereby relaxing input constraints and ensuring exceptional performance across diverse domains. Moreover, we introduce Advantage Databases, where the Tree-of-Thought is enriched with human feedback, aligning the model selection process with human preferences. Through extensive experiments and comparisons, we demonstrate the effectiveness of DiffusionGPT, showcasing its potential for pushing the boundaries of image synthesis in diverse domains.</li>
</ul>

<h3>Title: A locally statistical active contour model for SAR image segmentation  can be solved by denoising algorithms</h3>
<ul>
<li><strong>Authors: </strong>Guangming Liu, Quanying Sun, Jing Liang, Qi Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10083">https://arxiv.org/abs/2401.10083</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10083">https://arxiv.org/pdf/2401.10083</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10083]] A locally statistical active contour model for SAR image segmentation  can be solved by denoising algorithms(https://arxiv.org/abs/2401.10083)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this paper, we propose a novel locally statistical variational active contour model based on I-divergence-TV denoising model, which hybrides geodesic active contour (GAC) model with active contours without edges (ACWE) model, and can be used to segment images corrupted by multiplicative gamma noise. By adding a diffusion term into the level set evolution (LSE) equation of the proposed model, we construct a reaction-diffusion (RD) equation, which can gradually regularize the level set function (LSF) to be piecewise constant in each segment domain and gain the stable solution. We further transform the proposed model into classic ROF model by adding a proximity term. Inspired by a fast denoising algorithm proposed by Jia-Zhao recently, we propose two fast fixed point algorithms to solve SAR image segmentation question. Experimental results for real SAR images show that the proposed image segmentation model can efficiently stop the contours at weak or blurred edges, and can automatically detect the exterior and interior boundaries of images with multiplicative gamma noise. The proposed FPRD1/FPRD2 models are about 1/2 (or less than) of the time required for the SBRD model based on the Split Bregman technique.</li>
</ul>

<h3>Title: Motion-Zero: Zero-Shot Moving Object Control Framework for  Diffusion-Based Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Changgu Chen, Junwei Shu, Lianggangxu Chen, Gaoqi He, Changbo Wang, Yang Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10150">https://arxiv.org/abs/2401.10150</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10150">https://arxiv.org/pdf/2401.10150</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10150]] Motion-Zero: Zero-Shot Moving Object Control Framework for  Diffusion-Based Video Generation(https://arxiv.org/abs/2401.10150)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent large-scale pre-trained diffusion models have demonstrated a powerful generative ability to produce high-quality videos from detailed text descriptions. However, exerting control over the motion of objects in videos generated by any video diffusion model is a challenging problem. In this paper, we propose a novel zero-shot moving object trajectory control framework, Motion-Zero, to enable a bounding-box-trajectories-controlled text-to-video diffusion model.To this end, an initial noise prior module is designed to provide a position-based prior to improve the stability of the appearance of the moving object and the accuracy of position. In addition, based on the attention map of the U-net, spatial constraints are directly applied to the denoising process of diffusion models, which further ensures the positional and spatial consistency of moving objects during the inference. Furthermore, temporal consistency is guaranteed with a proposed shift temporal attention mechanism. Our method can be flexibly applied to various state-of-the-art video diffusion models without any training process. Extensive experiments demonstrate our proposed method can control the motion trajectories of objects and generate high-quality videos.</li>
</ul>

<h3>Title: VMamba: Visual State Space Model</h3>
<ul>
<li><strong>Authors: </strong>Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi Xie, Yaowei Wang, Qixiang Ye, Yunfan Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10166">https://arxiv.org/abs/2401.10166</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10166">https://arxiv.org/pdf/2401.10166</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10166]] VMamba: Visual State Space Model(https://arxiv.org/abs/2401.10166)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) stand as the two most popular foundation models for visual representation learning. While CNNs exhibit remarkable scalability with linear complexity w.r.t. image resolution, ViTs surpass them in fitting capabilities despite contending with quadratic complexity. A closer inspection reveals that ViTs achieve superior visual modeling performance through the incorporation of global receptive fields and dynamic weights. This observation motivates us to propose a novel architecture that inherits these components while enhancing computational efficiency. To this end, we draw inspiration from the recently introduced state space model and propose the Visual State Space Model (VMamba), which achieves linear complexity without sacrificing global receptive fields. To address the encountered direction-sensitive issue, we introduce the Cross-Scan Module (CSM) to traverse the spatial domain and convert any non-causal visual image into order patch sequences. Extensive experimental results substantiate that VMamba not only demonstrates promising capabilities across various visual perception tasks, but also exhibits more pronounced advantages over established benchmarks as the image resolution increases. Source code has been available at https://github.com/MzeroMiko/VMamba.</li>
</ul>

<h3>Title: Beyond Reference-Based Metrics: Analyzing Behaviors of Open LLMs on  Data-to-Text Generation</h3>
<ul>
<li><strong>Authors: </strong>Zdeněk Kasner, Ondřej Dušek</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10186">https://arxiv.org/abs/2401.10186</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10186">https://arxiv.org/pdf/2401.10186</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10186]] Beyond Reference-Based Metrics: Analyzing Behaviors of Open LLMs on  Data-to-Text Generation(https://arxiv.org/abs/2401.10186)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>We investigate to which extent open large language models (LLMs) can generate coherent and relevant text from structured data. To prevent bias from benchmarks leaked into LLM training data, we collect Quintd-1: an ad-hoc benchmark for five data-to-text (D2T) generation tasks, consisting of structured data records in standard formats gathered from public APIs. We leverage reference-free evaluation metrics and LLMs' in-context learning capabilities, allowing us to test the models with no human-written references. Our evaluation focuses on annotating semantic accuracy errors on token-level, combining human annotators and a metric based on GPT-4. Our systematic examination of the models' behavior across domains and tasks suggests that state-of-the-art open LLMs with 7B parameters can generate fluent and coherent text from various standard data formats in zero-shot settings. However, we also show that semantic accuracy of the outputs remains a major issue: on our benchmark, 80% of outputs of open LLMs contain a semantic error according to human annotators (91% according to GPT-4). Our code, data, and model outputs are available at https://d2t-llm.github.io.</li>
</ul>

<h3>Title: MM-Interleaved: Interleaved Image-Text Generative Modeling via  Multi-modal Feature Synchronizer</h3>
<ul>
<li><strong>Authors: </strong>Changyao Tian, Xizhou Zhu, Yuwen Xiong, Weiyun Wang, Zhe Chen, Wenhai Wang, Yuntao Chen, Lewei Lu, Tong Lu, Jie Zhou, Hongsheng Li, Yu Qiao, Jifeng Dai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10208">https://arxiv.org/abs/2401.10208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10208">https://arxiv.org/pdf/2401.10208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10208]] MM-Interleaved: Interleaved Image-Text Generative Modeling via  Multi-modal Feature Synchronizer(https://arxiv.org/abs/2401.10208)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Developing generative models for interleaved image-text data has both research and practical value. It requires models to understand the interleaved sequences and subsequently generate images and text. However, existing attempts are limited by the issue that the fixed number of visual tokens cannot efficiently capture image details, which is particularly problematic in the multi-image scenarios. To address this, this paper presents MM-Interleaved, an end-to-end generative model for interleaved image-text data. It introduces a multi-scale and multi-image feature synchronizer module, allowing direct access to fine-grained image features in the previous context during the generation process. MM-Interleaved is end-to-end pre-trained on both paired and interleaved image-text corpora. It is further enhanced through a supervised fine-tuning phase, wherein the model improves its ability to follow complex multi-modal instructions. Experiments demonstrate the versatility of MM-Interleaved in recognizing visual details following multi-modal instructions and generating consistent images following both textual and visual conditions. Code and models are available at \url{https://github.com/OpenGVLab/MM-Interleaved}.</li>
</ul>

<h3>Title: AutoFT: Robust Fine-Tuning by Optimizing Hyperparameters on OOD Data</h3>
<ul>
<li><strong>Authors: </strong>Caroline Choi, Yoonho Lee, Annie Chen, Allan Zhou, Aditi Raghunathan, Chelsea Finn</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10220">https://arxiv.org/abs/2401.10220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10220">https://arxiv.org/pdf/2401.10220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10220]] AutoFT: Robust Fine-Tuning by Optimizing Hyperparameters on OOD Data(https://arxiv.org/abs/2401.10220)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Foundation models encode rich representations that can be adapted to a desired task by fine-tuning on task-specific data. However, fine-tuning a model on one particular data distribution often compromises the model's original performance on other distributions. Current methods for robust fine-tuning utilize hand-crafted regularization techniques to constrain the fine-tuning process towards the base foundation model. Yet, it is hard to precisely specify what characteristics of the foundation model to retain during fine-tuning, as this depends on how the pre-training, fine-tuning, and evaluation data distributions relate to each other. We propose AutoFT, a data-driven approach for guiding foundation model fine-tuning. AutoFT optimizes fine-tuning hyperparameters to maximize performance on a small out-of-distribution (OOD) validation set. To guide fine-tuning in a granular way, AutoFT searches a highly expressive hyperparameter space that includes weight coefficients for many different losses, in addition to learning rate and weight decay values. We evaluate AutoFT on nine natural distribution shifts which include domain shifts and subpopulation shifts. Our experiments show that AutoFT significantly improves generalization to new OOD data, outperforming existing robust fine-tuning methods. Notably, AutoFT achieves new state-of-the-art performance on the WILDS-iWildCam and WILDS-FMoW benchmarks, outperforming the previous best methods by $6.0\%$ and $1.5\%$, respectively.</li>
</ul>

<h3>Title: Supervised Fine-tuning in turn Improves Visual Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Xiaohu Jiang, Yixiao Ge, Yuying Ge, Chun Yuan, Ying Shan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10222">https://arxiv.org/abs/2401.10222</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10222">https://arxiv.org/pdf/2401.10222</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10222]] Supervised Fine-tuning in turn Improves Visual Foundation Models(https://arxiv.org/abs/2401.10222)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Image-text training like CLIP has dominated the pretraining of vision foundation models in recent years. Subsequent efforts have been made to introduce region-level visual learning into CLIP's pretraining but face scalability challenges due to the lack of large-scale region-level datasets. Drawing inspiration from supervised fine-tuning (SFT) in natural language processing such as instruction tuning, we explore the potential of fine-grained SFT in enhancing the generation of vision foundation models after their pretraining. Thus a two-stage method ViSFT (Vision SFT) is proposed to unleash the fine-grained knowledge of vision foundation models. In ViSFT, the vision foundation model is enhanced by performing visual joint learning on some in-domain tasks and then tested on out-of-domain benchmarks. With updating using ViSFT on 8 V100 GPUs in less than 2 days, a vision transformer with over 4.4B parameters shows improvements across various out-of-domain benchmarks including vision and vision-linguistic scenarios.</li>
</ul>

<h3>Title: Towards Language-Driven Video Inpainting via Multimodal Large Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Jianzong Wu, Xiangtai Li, Chenyang Si, Shangchen Zhou, Jingkang Yang, Jiangning Zhang, Yining Li, Kai Chen, Yunhai Tong, Ziwei Liu, Chen Change Loy</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10226">https://arxiv.org/abs/2401.10226</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10226">https://arxiv.org/pdf/2401.10226</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10226]] Towards Language-Driven Video Inpainting via Multimodal Large Language  Models(https://arxiv.org/abs/2401.10226)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce a new task -- language-driven video inpainting, which uses natural language instructions to guide the inpainting process. This approach overcomes the limitations of traditional video inpainting methods that depend on manually labeled binary masks, a process often tedious and labor-intensive. We present the Remove Objects from Videos by Instructions (ROVI) dataset, containing 5,650 videos and 9,091 inpainting results, to support training and evaluation for this task. We also propose a novel diffusion-based language-driven video inpainting framework, the first end-to-end baseline for this task, integrating Multimodal Large Language Models to understand and execute complex language-based inpainting requests effectively. Our comprehensive results showcase the dataset's versatility and the model's effectiveness in various language-instructed inpainting scenarios. We will make datasets, code, and models publicly available.</li>
</ul>

<h3>Title: A Simple Latent Diffusion Approach for Panoptic Segmentation and Mask  Inpainting</h3>
<ul>
<li><strong>Authors: </strong>Wouter Van Gansbeke, Bert De Brabandere</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10227">https://arxiv.org/abs/2401.10227</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10227">https://arxiv.org/pdf/2401.10227</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10227]] A Simple Latent Diffusion Approach for Panoptic Segmentation and Mask  Inpainting(https://arxiv.org/abs/2401.10227)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Panoptic and instance segmentation networks are often trained with specialized object detection modules, complex loss functions, and ad-hoc post-processing steps to handle the permutation-invariance of the instance masks. This work builds upon Stable Diffusion and proposes a latent diffusion approach for panoptic segmentation, resulting in a simple architecture which omits these complexities. Our training process consists of two steps: (1) training a shallow autoencoder to project the segmentation masks to latent space; (2) training a diffusion model to allow image-conditioned sampling in latent space. The use of a generative model unlocks the exploration of mask completion or inpainting, which has applications in interactive segmentation. The experimental validation yields promising results for both panoptic segmentation and mask inpainting. While not setting a new state-of-the-art, our model's simplicity, generality, and mask completion capability are desirable properties.</li>
</ul>

<h3>Title: RAP-SAM: Towards Real-Time All-Purpose Segment Anything</h3>
<ul>
<li><strong>Authors: </strong>Shilin Xu, Haobo Yuan, Qingyu Shi, Lu Qi, Jingbo Wang, Yibo Yang, Yining Li, Kai Chen, Yunhai Tong, Bernard Ghanem, Xiangtai Li, Ming-Hsuan Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10228">https://arxiv.org/abs/2401.10228</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10228">https://arxiv.org/pdf/2401.10228</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10228]] RAP-SAM: Towards Real-Time All-Purpose Segment Anything(https://arxiv.org/abs/2401.10228)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Advanced by transformer architecture, vision foundation models (VFMs) achieve remarkable progress in performance and generalization ability. Segment Anything Model (SAM) is one remarkable model that can achieve generalized segmentation. However, most VFMs cannot run in realtime, which makes it difficult to transfer them into several products. On the other hand, current real-time segmentation mainly has one purpose, such as semantic segmentation on the driving scene. We argue that diverse outputs are needed for real applications. Thus, this work explores a new real-time segmentation setting, named all-purpose segmentation in real-time, to transfer VFMs in real-time deployment. It contains three different tasks, including interactive segmentation, panoptic segmentation, and video segmentation. We aim to use one model to achieve the above tasks in real-time. We first benchmark several strong baselines. Then, we present Real-Time All Purpose SAM (RAP-SAM). It contains an efficient encoder and an efficient decoupled decoder to perform prompt-driven decoding. Moreover, we further explore different training strategies and tuning methods to boost co-training performance further. Our code and model are available at https://github.com/xushilin1/RAP-SAM/.</li>
</ul>

<h3>Title: ParaHome: Parameterizing Everyday Home Activities Towards 3D Generative  Modeling of Human-Object Interactions</h3>
<ul>
<li><strong>Authors: </strong>Jeonghwan Kim, Jisoo Kim, Jeonghyeon Na, Hanbyul Joo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2401.10232">https://arxiv.org/abs/2401.10232</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2401.10232">https://arxiv.org/pdf/2401.10232</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2401.10232]] ParaHome: Parameterizing Everyday Home Activities Towards 3D Generative  Modeling of Human-Object Interactions(https://arxiv.org/abs/2401.10232)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>To enable machines to learn how humans interact with the physical world in our daily activities, it is crucial to provide rich data that encompasses the 3D motion of humans as well as the motion of objects in a learnable 3D representation. Ideally, this data should be collected in a natural setup, capturing the authentic dynamic 3D signals during human-object interactions. To address this challenge, we introduce the ParaHome system, designed to capture and parameterize dynamic 3D movements of humans and objects within a common home environment. Our system consists of a multi-view setup with 70 synchronized RGB cameras, as well as wearable motion capture devices equipped with an IMU-based body suit and hand motion capture gloves. By leveraging the ParaHome system, we collect a novel large-scale dataset of human-object interaction. Notably, our dataset offers key advancement over existing datasets in three main aspects: (1) capturing 3D body and dexterous hand manipulation motion alongside 3D object movement within a contextual home environment during natural activities; (2) encompassing human interaction with multiple objects in various episodic scenarios with corresponding descriptions in texts; (3) including articulated objects with multiple parts expressed with parameterized articulations. Building upon our dataset, we introduce new research tasks aimed at building a generative model for learning and synthesizing human-object interactions in a real-world room setting.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
