<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-04-10</h1>
<h3>Title: Well2Flow: Reconstruction of reservoir states from sparse wells using score-based generative models</h3>
<ul>
<li><strong>Authors: </strong>Shiqin Zeng, Haoyun Li, Abhinav Prakash Gahlot, Felix J. Herrmann</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06305">https://arxiv.org/abs/2504.06305</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06305">https://arxiv.org/pdf/2504.06305</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06305]] Well2Flow: Reconstruction of reservoir states from sparse wells using score-based generative models(https://arxiv.org/abs/2504.06305)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This study investigates the use of score-based generative models for reservoir simulation, with a focus on reconstructing spatially varying permeability and saturation fields in saline aquifers, inferred from sparse observations at two well locations. By modeling the joint distribution of permeability and saturation derived from high-fidelity reservoir simulations, the proposed neural network is trained to learn the complex spatiotemporal dynamics governing multiphase fluid flow in porous media. During inference, the framework effectively reconstructs both permeability and saturation fields by conditioning on sparse vertical profiles extracted from well log data. This approach introduces a novel methodology for incorporating physical constraints and well log guidance into generative models, significantly enhancing the accuracy and physical plausibility of the reconstructed subsurface states. Furthermore, the framework demonstrates strong generalization capabilities across varying geological scenarios, highlighting its potential for practical deployment in data-scarce reservoir management tasks.</li>
</ul>

<h3>Title: Optimizing Large Language Models: Metrics, Energy Efficiency, and Case Study Insights</h3>
<ul>
<li><strong>Authors: </strong>Tahniat Khan, Soroor Motie, Sedef Akinli Kocak, Shaina Raza</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06307">https://arxiv.org/abs/2504.06307</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06307">https://arxiv.org/pdf/2504.06307</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06307]] Optimizing Large Language Models: Metrics, Energy Efficiency, and Case Study Insights(https://arxiv.org/abs/2504.06307)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid adoption of large language models (LLMs) has led to significant energy consumption and carbon emissions, posing a critical challenge to the sustainability of generative AI technologies. This paper explores the integration of energy-efficient optimization techniques in the deployment of LLMs to address these environmental concerns. We present a case study and framework that demonstrate how strategic quantization and local inference techniques can substantially lower the carbon footprints of LLMs without compromising their operational effectiveness. Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45\% post quantization, making them particularly suitable for resource-constrained environments. The findings provide actionable insights for achieving sustainability in AI while maintaining high levels of accuracy and responsiveness.</li>
</ul>

<h3>Title: DMol: A Schedule-Driven Diffusion Model for Highly Efficient and Versatile Molecule Generation</h3>
<ul>
<li><strong>Authors: </strong>Peizhi Niu, Yu-Hsiang Wang, Vishal Rana, Chetan Rupakheti, Abhishek Pandey, Olgica Milenkovic</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06312">https://arxiv.org/abs/2504.06312</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06312">https://arxiv.org/pdf/2504.06312</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06312]] DMol: A Schedule-Driven Diffusion Model for Highly Efficient and Versatile Molecule Generation(https://arxiv.org/abs/2504.06312)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce a new graph diffusion model for small molecule generation, \emph{DMol}, which outperforms the state-of-the-art DiGress model in terms of validity by roughly $1.5\%$ across all benchmarking datasets while reducing the number of diffusion steps by at least $10$-fold, and the running time to roughly one half. The performance improvements are a result of a careful change in the objective function and a ``graph noise" scheduling approach which, at each diffusion step, allows one to only change a subset of nodes of varying size in the molecule graph. Another relevant property of the method is that it can be easily combined with junction-tree-like graph representations that arise by compressing a collection of relevant ring structures into supernodes. Unlike classical junction-tree techniques that involve VAEs and require complicated reconstruction steps, compressed DMol directly performs graph diffusion on a graph that compresses only a carefully selected set of frequent carbon rings into supernodes, which results in straightforward sample generation. This compressed DMol method offers additional validity improvements over generic DMol of roughly $2\%$, increases the novelty of the method, and further improves the running time due to reductions in the graph size.</li>
</ul>

<h3>Title: Hybrid Temporal Differential Consistency Autoencoder for Efficient and Sustainable Anomaly Detection in Cyber-Physical Systems</h3>
<ul>
<li><strong>Authors: </strong>Michael Somma</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06320">https://arxiv.org/abs/2504.06320</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06320">https://arxiv.org/pdf/2504.06320</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06320]] Hybrid Temporal Differential Consistency Autoencoder for Efficient and Sustainable Anomaly Detection in Cyber-Physical Systems(https://arxiv.org/abs/2504.06320)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Cyberattacks on critical infrastructure, particularly water distribution systems, have increased due to rapid digitalization and the integration of IoT devices and industrial control systems (ICS). These cyber-physical systems (CPS) introduce new vulnerabilities, requiring robust and automated intrusion detection systems (IDS) to mitigate potential threats. This study addresses key challenges in anomaly detection by leveraging time correlations in sensor data, integrating physical principles into machine learning models, and optimizing computational efficiency for edge applications. We build upon the concept of temporal differential consistency (TDC) loss to capture the dynamics of the system, ensuring meaningful relationships between dynamic states. Expanding on this foundation, we propose a hybrid autoencoder-based approach, referred to as hybrid TDC-AE, which extends TDC by incorporating both deterministic nodes and conventional statistical nodes. This hybrid structure enables the model to account for non-deterministic processes. Our approach achieves state-of-the-art classification performance while improving time to detect anomalies by 3%, outperforming the BATADAL challenge leader without requiring domain-specific knowledge, making it broadly applicable. Additionally, it maintains the computational efficiency of conventional autoencoders while reducing the number of fully connected layers, resulting in a more sustainable and efficient solution. The method demonstrates how leveraging physics-inspired consistency principles enhances anomaly detection and strengthens the resilience of cyber-physical systems.</li>
</ul>

<h3>Title: Analyzing the Impact of Low-Rank Adaptation for Cross-Domain Few-Shot Object Detection in Aerial Images</h3>
<ul>
<li><strong>Authors: </strong>Hicham Talaoubrid, Anissa Mokraoui, Ismail Ben Ayed, Axel Prouvost, Sonimith Hang, Monit Korn, RÃ©mi Harvey</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06330">https://arxiv.org/abs/2504.06330</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06330">https://arxiv.org/pdf/2504.06330</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06330]] Analyzing the Impact of Low-Rank Adaptation for Cross-Domain Few-Shot Object Detection in Aerial Images(https://arxiv.org/abs/2504.06330)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper investigates the application of Low-Rank Adaptation (LoRA) to small models for cross-domain few-shot object detection in aerial images. Originally designed for large-scale models, LoRA helps mitigate overfitting, making it a promising approach for resource-constrained settings. We integrate LoRA into DiffusionDet, and evaluate its performance on the DOTA and DIOR datasets. Our results show that LoRA applied after an initial fine-tuning slightly improves performance in low-shot settings (e.g., 1-shot and 5-shot), while full fine-tuning remains more effective in higher-shot configurations. These findings highlight LoRA's potential for efficient adaptation in aerial object detection, encouraging further research into parameter-efficient fine-tuning strategies for few-shot learning. Our code is available here: this https URL.</li>
</ul>

<h3>Title: Unifying Autoregressive and Diffusion-Based Sequence Generation</h3>
<ul>
<li><strong>Authors: </strong>Nima Fathi, Torsten Scholak, Pierre-AndrÃ© NoÃ«l</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06416">https://arxiv.org/abs/2504.06416</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06416">https://arxiv.org/pdf/2504.06416</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06416]] Unifying Autoregressive and Diffusion-Based Sequence Generation(https://arxiv.org/abs/2504.06416)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present significant extensions to diffusion-based sequence generation models, blurring the line with autoregressive language models. We introduce hyperschedules, which assign distinct noise schedules to individual token positions, generalizing both autoregressive models (e.g., GPT) and conventional diffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two hybrid token-wise noising processes that interpolate between absorbing and uniform processes, enabling the model to fix past mistakes, and we introduce a novel inference algorithm that leverages this new feature in a simplified context inspired from MDLM. To support efficient training and inference, we design attention masks compatible with KV-caching. Our methods achieve state-of-the-art perplexity and generate diverse, high-quality sequences across standard benchmarks, suggesting a promising path for autoregressive diffusion-based sequence generation.</li>
</ul>

<h3>Title: Releasing Differentially Private Event Logs Using Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Frederik Wangelik, Majid Rafiei, Mahsa Pourbafrani, Wil M.P. van der Aalst</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06418">https://arxiv.org/abs/2504.06418</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06418">https://arxiv.org/pdf/2504.06418</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06418]] Releasing Differentially Private Event Logs Using Generative Models(https://arxiv.org/abs/2504.06418)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In recent years, the industry has been witnessing an extended usage of process mining and automated event data analysis. Consequently, there is a rising significance in addressing privacy apprehensions related to the inclusion of sensitive and private information within event data utilized by process mining algorithms. State-of-the-art research mainly focuses on providing quantifiable privacy guarantees, e.g., via differential privacy, for trace variants that are used by the main process mining techniques, e.g., process discovery. However, privacy preservation techniques designed for the release of trace variants are still insufficient to meet all the demands of industry-scale utilization. Moreover, ensuring privacy guarantees in situations characterized by a high occurrence of infrequent trace variants remains a challenging endeavor. In this paper, we introduce two novel approaches for releasing differentially private trace variants based on trained generative models. With TraVaG, we leverage \textit{Generative Adversarial Networks} (GANs) to sample from a privatized implicit variant distribution. Our second method employs \textit{Denoising Diffusion Probabilistic Models} that reconstruct artificial trace variants from noise via trained Markov chains. Both methods offer industry-scale benefits and elevate the degree of privacy assurances, particularly in scenarios featuring a substantial prevalence of infrequent variants. Also, they overcome the shortcomings of conventional privacy preservation techniques, such as bounding the length of variants and introducing fake variants. Experimental results on real-life event data demonstrate that our approaches surpass state-of-the-art techniques in terms of privacy guarantees and utility preservation.</li>
</ul>

<h3>Title: D-Feat Occlusions: Diffusion Features for Robustness to Partial Visual Occlusions in Object Recognition</h3>
<ul>
<li><strong>Authors: </strong>Rupayan Mallick, Sibo Dong, Nataniel Ruiz, Sarah Adel Bargal</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06432">https://arxiv.org/abs/2504.06432</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06432">https://arxiv.org/pdf/2504.06432</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06432]] D-Feat Occlusions: Diffusion Features for Robustness to Partial Visual Occlusions in Object Recognition(https://arxiv.org/abs/2504.06432)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Applications of diffusion models for visual tasks have been quite noteworthy. This paper targets making classification models more robust to occlusions for the task of object recognition by proposing a pipeline that utilizes a frozen diffusion model. Diffusion features have demonstrated success in image generation and image completion while understanding image context. Occlusion can be posed as an image completion problem by deeming the pixels of the occluder to be `missing.' We hypothesize that such features can help hallucinate object visual features behind occluding objects, and hence we propose using them to enable models to become more occlusion robust. We design experiments to include input-based augmentations as well as feature-based augmentations. Input-based augmentations involve finetuning on images where the occluder pixels are inpainted, and feature-based augmentations involve augmenting classification features with intermediate diffusion features. We demonstrate that our proposed use of diffusion-based features results in models that are more robust to partial object occlusions for both Transformers and ConvNets on ImageNet with simulated occlusions. We also propose a dataset that encompasses real-world occlusions and demonstrate that our method is more robust to partial object occlusions.</li>
</ul>

<h3>Title: A Cross-Domain Few-Shot Learning Method Based on Domain Knowledge Mapping</h3>
<ul>
<li><strong>Authors: </strong>Jiajun Chen, Hongpeng Yin, Yifu Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06608">https://arxiv.org/abs/2504.06608</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06608">https://arxiv.org/pdf/2504.06608</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06608]] A Cross-Domain Few-Shot Learning Method Based on Domain Knowledge Mapping(https://arxiv.org/abs/2504.06608)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>In task-based few-shot learning paradigms, it is commonly assumed that different tasks are independently and identically distributed (i.i.d.). However, in real-world scenarios, the distribution encountered in few-shot learning can significantly differ from the distribution of existing data. Thus, how to effectively leverage existing data knowledge to enable models to quickly adapt to class variations under non-i.i.d. assumptions has emerged as a key research challenge. To address this challenge, this paper proposes a new cross-domain few-shot learning approach based on domain knowledge mapping, applied consistently throughout the pre-training, training, and testing phases. In the pre-training phase, our method integrates self-supervised and supervised losses by maximizing mutual information, thereby mitigating mode collapse. During the training phase, the domain knowledge mapping layer collaborates with a domain classifier to learn both domain mapping capabilities and the ability to assess domain adaptation difficulty. Finally, this approach is applied during the testing phase, rapidly adapting to domain variations through meta-training tasks on support sets, consequently enhancing the model's capability to transfer domain knowledge effectively. Experimental validation conducted across six datasets from diverse domains demonstrates the effectiveness of the proposed method.</li>
</ul>

<h3>Title: AMAD: AutoMasked Attention for Unsupervised Multivariate Time Series Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Tiange Huang, Yongjun Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06643">https://arxiv.org/abs/2504.06643</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06643">https://arxiv.org/pdf/2504.06643</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06643]] AMAD: AutoMasked Attention for Unsupervised Multivariate Time Series Anomaly Detection(https://arxiv.org/abs/2504.06643)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Unsupervised multivariate time series anomaly detection (UMTSAD) plays a critical role in various domains, including finance, networks, and sensor systems. In recent years, due to the outstanding performance of deep learning in general sequential tasks, many models have been specialized for deep UMTSAD tasks and have achieved impressive results, particularly those based on the Transformer and self-attention mechanisms. However, the sequence anomaly association assumptions underlying these models are often limited to specific predefined patterns and scenarios, such as concentrated or peak anomaly patterns. These limitations hinder their ability to generalize to diverse anomaly situations, especially where the lack of labels poses significant challenges. To address these issues, we propose AMAD, which integrates \textbf{A}uto\textbf{M}asked Attention for UMTS\textbf{AD} scenarios. AMAD introduces a novel structure based on the AutoMask mechanism and an attention mixup module, forming a simple yet generalized anomaly association representation framework. This framework is further enhanced by a Max-Min training strategy and a Local-Global contrastive learning approach. By combining multi-scale feature extraction with automatic relative association modeling, AMAD provides a robust and adaptable solution to UMTSAD challenges. Extensive experimental results demonstrate that the proposed model achieving competitive performance results compared to SOTA benchmarks across a variety of datasets.</li>
</ul>

<h3>Title: RAGME: Retrieval Augmented Video Generation for Enhanced Motion Realism</h3>
<ul>
<li><strong>Authors: </strong>Elia Peruzzo, Dejia Xu, Xingqian Xu, Humphrey Shi, Nicu Sebe</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06672">https://arxiv.org/abs/2504.06672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06672">https://arxiv.org/pdf/2504.06672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06672]] RAGME: Retrieval Augmented Video Generation for Enhanced Motion Realism(https://arxiv.org/abs/2504.06672)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Video generation is experiencing rapid growth, driven by advances in diffusion models and the development of better and larger datasets. However, producing high-quality videos remains challenging due to the high-dimensional data and the complexity of the task. Recent efforts have primarily focused on enhancing visual quality and addressing temporal inconsistencies, such as flickering. Despite progress in these areas, the generated videos often fall short in terms of motion complexity and physical plausibility, with many outputs either appearing static or exhibiting unrealistic motion. In this work, we propose a framework to improve the realism of motion in generated videos, exploring a complementary direction to much of the existing literature. Specifically, we advocate for the incorporation of a retrieval mechanism during the generation phase. The retrieved videos act as grounding signals, providing the model with demonstrations of how the objects move. Our pipeline is designed to apply to any text-to-video diffusion model, conditioning a pretrained model on the retrieved samples with minimal fine-tuning. We demonstrate the superiority of our approach through established metrics, recently proposed benchmarks, and qualitative results, and we highlight additional applications of the framework.</li>
</ul>

<h3>Title: Probability Density Geodesics in Image Diffusion Latent Space</h3>
<ul>
<li><strong>Authors: </strong>Qingtao Yu, Jaskirat Singh, Zhaoyuan Yang, Peter Henry Tu, Jing Zhang, Hongdong Li, Richard Hartley, Dylan Campbell</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06675">https://arxiv.org/abs/2504.06675</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06675">https://arxiv.org/pdf/2504.06675</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06675]] Probability Density Geodesics in Image Diffusion Latent Space(https://arxiv.org/abs/2504.06675)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models indirectly estimate the probability density over a data space, which can be used to study its structure. In this work, we show that geodesics can be computed in diffusion latent space, where the norm induced by the spatially-varying inner product is inversely proportional to the probability density. In this formulation, a path that traverses a high density (that is, probable) region of image latent space is shorter than the equivalent path through a low density region. We present algorithms for solving the associated initial and boundary value problems and show how to compute the probability density along the path and the geodesic distance between two points. Using these techniques, we analyze how closely video clips approximate geodesics in a pre-trained image diffusion space. Finally, we demonstrate how these techniques can be applied to training-free image sequence interpolation and extrapolation, given a pre-trained image diffusion model.</li>
</ul>

<h3>Title: Masked Scene Modeling: Narrowing the Gap Between Supervised and Self-Supervised Learning in 3D Scene Understanding</h3>
<ul>
<li><strong>Authors: </strong>Pedro Hermosilla, Christian Stippel, Leon Sick</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06719">https://arxiv.org/abs/2504.06719</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06719">https://arxiv.org/pdf/2504.06719</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06719]] Masked Scene Modeling: Narrowing the Gap Between Supervised and Self-Supervised Learning in 3D Scene Understanding(https://arxiv.org/abs/2504.06719)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-supervised learning has transformed 2D computer vision by enabling models trained on large, unannotated datasets to provide versatile off-the-shelf features that perform similarly to models trained with labels. However, in 3D scene understanding, self-supervised methods are typically only used as a weight initialization step for task-specific fine-tuning, limiting their utility for general-purpose feature extraction. This paper addresses this shortcoming by proposing a robust evaluation protocol specifically designed to assess the quality of self-supervised features for 3D scene understanding. Our protocol uses multi-resolution feature sampling of hierarchical models to create rich point-level representations that capture the semantic capabilities of the model and, hence, are suitable for evaluation with linear probing and nearest-neighbor methods. Furthermore, we introduce the first self-supervised model that performs similarly to supervised models when only off-the-shelf features are used in a linear probing setup. In particular, our model is trained natively in 3D with a novel self-supervised approach based on a Masked Scene Modeling objective, which reconstructs deep features of masked patches in a bottom-up manner and is specifically tailored to hierarchical 3D models. Our experiments not only demonstrate that our method achieves competitive performance to supervised models, but also surpasses existing self-supervised approaches by a large margin. The model and training code can be found at our Github repository (this https URL).</li>
</ul>

<h3>Title: Plastic tensor networks for interpretable generative modeling</h3>
<ul>
<li><strong>Authors: </strong>Katsuya O. Akamatsu, Kenji Harada, Tsuyoshi Okubo, Naoki Kawashima</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.stat-mech</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06722">https://arxiv.org/abs/2504.06722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06722">https://arxiv.org/pdf/2504.06722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06722]] Plastic tensor networks for interpretable generative modeling(https://arxiv.org/abs/2504.06722)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>A structural optimization scheme for a single-layer nonnegative adaptive tensor tree (NATT) that models a target probability distribution is proposed. The NATT scheme, by construction, has the advantage that it is interpretable as a probabilistic graphical model. We consider the NATT scheme and a recently proposed Born machine adaptive tensor tree (BMATT) optimization scheme and demonstrate their effectiveness on a variety of generative modeling tasks where the objective is to infer the hidden structure of a provided dataset. Our results show that in terms of minimizing the negative log-likelihood, the single-layer scheme has model performance comparable to the Born machine scheme, though not better. The tasks include deducing the structure of binary bitwise operations, learning the internal structure of random Bayesian networks given only visible sites, and a real-world example related to hierarchical clustering where a cladogram is constructed from mitochondrial DNA sequences. In doing so, we also show the importance of the choice of network topology and the versatility of a least-mutual information criterion in selecting a candidate structure for a tensor tree, as well as discuss aspects of these tensor tree generative models including their information content and interpretability.</li>
</ul>

<h3>Title: MultiADS: Defect-aware Supervision for Multi-type Anomaly Detection and Segmentation in Zero-Shot Learning</h3>
<ul>
<li><strong>Authors: </strong>Ylli Sadikaj, Hongkuan Zhou, Lavdim Halilaj, Stefan Schmid, Steffen Staab, Claudia Plant</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06740">https://arxiv.org/abs/2504.06740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06740">https://arxiv.org/pdf/2504.06740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06740]] MultiADS: Defect-aware Supervision for Multi-type Anomaly Detection and Segmentation in Zero-Shot Learning(https://arxiv.org/abs/2504.06740)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Precise optical inspection in industrial applications is crucial for minimizing scrap rates and reducing the associated costs. Besides merely detecting if a product is anomalous or not, it is crucial to know the distinct type of defect, such as a bent, cut, or scratch. The ability to recognize the "exact" defect type enables automated treatments of the anomalies in modern production lines. Current methods are limited to solely detecting whether a product is defective or not without providing any insights on the defect type, nevertheless detecting and identifying multiple defects. We propose MultiADS, a zero-shot learning approach, able to perform Multi-type Anomaly Detection and Segmentation. The architecture of MultiADS comprises CLIP and extra linear layers to align the visual- and textual representation in a joint feature space. To the best of our knowledge, our proposal, is the first approach to perform a multi-type anomaly segmentation task in zero-shot learning. Contrary to the other baselines, our approach i) generates specific anomaly masks for each distinct defect type, ii) learns to distinguish defect types, and iii) simultaneously identifies multiple defect types present in an anomalous product. Additionally, our approach outperforms zero/few-shot learning SoTA methods on image-level and pixel-level anomaly detection and segmentation tasks on five commonly used datasets: MVTec-AD, Visa, MPDD, MAD and Real-IAD.</li>
</ul>

<h3>Title: Compass Control: Multi Object Orientation Control for Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Rishbuh Parihar, Vaibhav Agrawal, Sachidanand VS, R. Venkatesh Babu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06752">https://arxiv.org/abs/2504.06752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06752">https://arxiv.org/pdf/2504.06752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06752]] Compass Control: Multi Object Orientation Control for Text-to-Image Generation(https://arxiv.org/abs/2504.06752)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Existing approaches for controlling text-to-image diffusion models, while powerful, do not allow for explicit 3D object-centric control, such as precise control of object orientation. In this work, we address the problem of multi-object orientation control in text-to-image diffusion models. This enables the generation of diverse multi-object scenes with precise orientation control for each object. The key idea is to condition the diffusion model with a set of orientation-aware \textbf{compass} tokens, one for each object, along with text tokens. A light-weight encoder network predicts these compass tokens taking object orientation as the input. The model is trained on a synthetic dataset of procedurally generated scenes, each containing one or two 3D assets on a plain background. However, direct training this framework results in poor orientation control as well as leads to entanglement among objects. To mitigate this, we intervene in the generation process and constrain the cross-attention maps of each compass token to its corresponding object regions. The trained model is able to achieve precise orientation control for a) complex objects not seen during training and b) multi-object scenes with more than two objects, indicating strong generalization capabilities. Further, when combined with personalization methods, our method precisely controls the orientation of the new object in diverse contexts. Our method achieves state-of-the-art orientation control and text alignment, quantified with extensive evaluations and a user study.</li>
</ul>

<h3>Title: DyDiT++: Dynamic Diffusion Transformers for Efficient Visual Generation</h3>
<ul>
<li><strong>Authors: </strong>Wangbo Zhao, Yizeng Han, Jiasheng Tang, Kai Wang, Hao Luo, Yibing Song, Gao Huang, Fan Wang, Yang You</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06803">https://arxiv.org/abs/2504.06803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06803">https://arxiv.org/pdf/2504.06803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06803]] DyDiT++: Dynamic Diffusion Transformers for Efficient Visual Generation(https://arxiv.org/abs/2504.06803)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion Transformer (DiT), an emerging diffusion model for visual generation, has demonstrated superior performance but suffers from substantial computational costs. Our investigations reveal that these costs primarily stem from the \emph{static} inference paradigm, which inevitably introduces redundant computation in certain \emph{diffusion timesteps} and \emph{spatial regions}. To overcome this inefficiency, we propose \textbf{Dy}namic \textbf{Di}ffusion \textbf{T}ransformer (DyDiT), an architecture that \emph{dynamically} adjusts its computation along both \emph{timestep} and \emph{spatial} dimensions. Specifically, we introduce a \emph{Timestep-wise Dynamic Width} (TDW) approach that adapts model width conditioned on the generation timesteps. In addition, we design a \emph{Spatial-wise Dynamic Token} (SDT) strategy to avoid redundant computation at unnecessary spatial locations. TDW and SDT can be seamlessly integrated into DiT and significantly accelerates the generation process. Building on these designs, we further enhance DyDiT in three key aspects. First, DyDiT is integrated seamlessly with flow matching-based generation, enhancing its versatility. Furthermore, we enhance DyDiT to tackle more complex visual generation tasks, including video generation and text-to-image generation, thereby broadening its real-world applications. Finally, to address the high cost of full fine-tuning and democratize technology access, we investigate the feasibility of training DyDiT in a parameter-efficient manner and introduce timestep-based dynamic LoRA (TD-LoRA). Extensive experiments on diverse visual generation models, including DiT, SiT, Latte, and FLUX, demonstrate the effectiveness of DyDiT.</li>
</ul>

<h3>Title: A Graph Diffusion Algorithm for Lexical Similarity Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Karol Mikula, Mariana SarkociovÃ¡ RemeÅ¡Ã­kovÃ¡</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06816">https://arxiv.org/abs/2504.06816</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06816">https://arxiv.org/pdf/2504.06816</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06816]] A Graph Diffusion Algorithm for Lexical Similarity Evaluation(https://arxiv.org/abs/2504.06816)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this paper, we present an algorithm for evaluating lexical similarity between a given language and several reference language clusters. As an input, we have a list of concepts and the corresponding translations in all considered languages. Moreover, each reference language is assigned to one of $c$ language clusters. For each of the concepts, the algorithm computes the distance between each pair of translations. Based on these distances, it constructs a weighted directed graph, where every vertex represents a language. After, it solves a graph diffusion equation with a Dirichlet boundary condition, where the unknown is a map from the vertex set to $\mathbb{R}^c$. The resulting coordinates are values from the interval $[0,1]$ and they can be interpreted as probabilities of belonging to each of the clusters or as a lexical similarity distribution with respect to the reference clusters. The distances between translations are calculated using phonetic transcriptions and a modification of the Damerau-Levenshtein distance. The algorithm can be useful in analyzing relationships between languages spoken in multilingual territories with a lot of mutual influences. We demonstrate this by presenting a case study regarding various European languages.</li>
</ul>

<h3>Title: IAAO: Interactive Affordance Learning for Articulated Objects in 3D Environments</h3>
<ul>
<li><strong>Authors: </strong>Can Zhang, Gim Hee Lee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06827">https://arxiv.org/abs/2504.06827</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06827">https://arxiv.org/pdf/2504.06827</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06827]] IAAO: Interactive Affordance Learning for Articulated Objects in 3D Environments(https://arxiv.org/abs/2504.06827)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>This work presents IAAO, a novel framework that builds an explicit 3D model for intelligent agents to gain understanding of articulated objects in their environment through interaction. Unlike prior methods that rely on task-specific networks and assumptions about movable parts, our IAAO leverages large foundation models to estimate interactive affordances and part articulations in three stages. We first build hierarchical features and label fields for each object state using 3D Gaussian Splatting (3DGS) by distilling mask features and view-consistent labels from multi-view images. We then perform object- and part-level queries on the 3D Gaussian primitives to identify static and articulated elements, estimating global transformations and local articulation parameters along with affordances. Finally, scenes from different states are merged and refined based on the estimated transformations, enabling robust affordance-based interaction and manipulation of objects. Experimental results demonstrate the effectiveness of our method.</li>
</ul>

<h3>Title: Classifying the Unknown: In-Context Learning for Open-Vocabulary Text and Symbol Recognition</h3>
<ul>
<li><strong>Authors: </strong>Tom Simon, William Mocaer, Pierrick Tranouez, Clement Chatelain, Thierry Paquet</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06841">https://arxiv.org/abs/2504.06841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06841">https://arxiv.org/pdf/2504.06841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06841]] Classifying the Unknown: In-Context Learning for Open-Vocabulary Text and Symbol Recognition(https://arxiv.org/abs/2504.06841)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>We introduce Rosetta, a multimodal model that leverages Multimodal In-Context Learning (MICL) to classify sequences of novel script patterns in documents by leveraging minimal examples, thus eliminating the need for explicit retraining. To enhance contextual learning, we designed a dataset generation process that ensures varying degrees of contextual informativeness, improving the model's adaptability in leveraging context across different scenarios. A key strength of our method is the use of a Context-Aware Tokenizer (CAT), which enables open-vocabulary classification. This allows the model to classify text and symbol patterns across an unlimited range of classes, extending its classification capabilities beyond the scope of its training alphabet of patterns. As a result, it unlocks applications such as the recognition of new alphabets and languages. Experiments on synthetic datasets demonstrate the potential of Rosetta to successfully classify Out-Of-Distribution visual patterns and diverse sets of alphabets and scripts, including but not limited to Chinese, Greek, Russian, French, Spanish, and Japanese.</li>
</ul>

<h3>Title: CasTex: Cascaded Text-to-Texture Synthesis via Explicit Texture Maps and Physically-Based Shading</h3>
<ul>
<li><strong>Authors: </strong>Mishan Aliev, Dmitry Baranchuk, Kirill Struminsky</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06856">https://arxiv.org/abs/2504.06856</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06856">https://arxiv.org/pdf/2504.06856</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06856]] CasTex: Cascaded Text-to-Texture Synthesis via Explicit Texture Maps and Physically-Based Shading(https://arxiv.org/abs/2504.06856)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This work investigates text-to-texture synthesis using diffusion models to generate physically-based texture maps. We aim to achieve realistic model appearances under varying lighting conditions. A prominent solution for the task is score distillation sampling. It allows recovering a complex texture using gradient guidance given a differentiable rasterization and shading pipeline. However, in practice, the aforementioned solution in conjunction with the widespread latent diffusion models produces severe visual artifacts and requires additional regularization such as implicit texture parameterization. As a more direct alternative, we propose an approach using cascaded diffusion models for texture synthesis (CasTex). In our setup, score distillation sampling yields high-quality textures out-of-the box. In particular, we were able to omit implicit texture parameterization in favor of an explicit parameterization to improve the procedure. In the experiments, we show that our approach significantly outperforms state-of-the-art optimization-based solutions on public texture synthesis benchmarks.</li>
</ul>

<h3>Title: EIDT-V: Exploiting Intersections in Diffusion Trajectories for Model-Agnostic, Zero-Shot, Training-Free Text-to-Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Diljeet Jagpal, Xi Chen, Vinay P. Namboodiri</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06861">https://arxiv.org/abs/2504.06861</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06861">https://arxiv.org/pdf/2504.06861</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06861]] EIDT-V: Exploiting Intersections in Diffusion Trajectories for Model-Agnostic, Zero-Shot, Training-Free Text-to-Video Generation(https://arxiv.org/abs/2504.06861)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, in-context</a></li>
<li><strong>Abstract: </strong>Zero-shot, training-free, image-based text-to-video generation is an emerging area that aims to generate videos using existing image-based diffusion models. Current methods in this space require specific architectural changes to image generation models, which limit their adaptability and scalability. In contrast to such methods, we provide a model-agnostic approach. We use intersections in diffusion trajectories, working only with the latent values. We could not obtain localized frame-wise coherence and diversity using only the intersection of trajectories. Thus, we instead use a grid-based approach. An in-context trained LLM is used to generate coherent frame-wise prompts; another is used to identify differences between frames. Based on these, we obtain a CLIP-based attention mask that controls the timing of switching the prompts for each grid cell. Earlier switching results in higher variance, while later switching results in more coherence. Therefore, our approach can ensure appropriate control between coherence and variance for the frames. Our approach results in state-of-the-art performance while being more flexible when working with diverse image-generation models. The empirical analysis using quantitative metrics and user studies confirms our model's superior temporal consistency, visual fidelity and user satisfaction, thus providing a novel way to obtain training-free, image-based text-to-video generation.</li>
</ul>

<h3>Title: ColorizeDiffusion v2: Enhancing Reference-based Sketch Colorization Through Separating Utilities</h3>
<ul>
<li><strong>Authors: </strong>Dingkun Yan, Xinrui Wang, Yusuke Iwasawa, Yutaka Matsuo, Suguru Saito, Jiaxian Guo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06895">https://arxiv.org/abs/2504.06895</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06895">https://arxiv.org/pdf/2504.06895</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06895]] ColorizeDiffusion v2: Enhancing Reference-based Sketch Colorization Through Separating Utilities(https://arxiv.org/abs/2504.06895)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Reference-based sketch colorization methods have garnered significant attention due to their potential applications in the animation production industry. However, most existing methods are trained with image triplets of sketch, reference, and ground truth that are semantically and spatially well-aligned, while real-world references and sketches often exhibit substantial misalignment. This mismatch in data distribution between training and inference leads to overfitting, consequently resulting in spatial artifacts and significant degradation in overall colorization quality, limiting potential applications of current methods for general purposes. To address this limitation, we conduct an in-depth analysis of the \textbf{carrier}, defined as the latent representation facilitating information transfer from reference to sketch. Based on this analysis, we propose a novel workflow that dynamically adapts the carrier to optimize distinct aspects of colorization. Specifically, for spatially misaligned artifacts, we introduce a split cross-attention mechanism with spatial masks, enabling region-specific reference injection within the diffusion process. To mitigate semantic neglect of sketches, we employ dedicated background and style encoders to transfer detailed reference information in the latent feature space, achieving enhanced spatial control and richer detail synthesis. Furthermore, we propose character-mask merging and background bleaching as preprocessing steps to improve foreground-background integration and background generation. Extensive qualitative and quantitative evaluations, including a user study, demonstrate the superior performance of our proposed method compared to existing approaches. An ablation study further validates the efficacy of each proposed component.</li>
</ul>

<h3>Title: MedSegFactory: Text-Guided Generation of Medical Image-Mask Pairs</h3>
<ul>
<li><strong>Authors: </strong>Jiawei Mao, Yuhan Wang, Yucheng Tang, Daguang Xu, Kang Wang, Yang Yang, Zongwei Zhou, Yuyin Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06897">https://arxiv.org/abs/2504.06897</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06897">https://arxiv.org/pdf/2504.06897</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06897]] MedSegFactory: Text-Guided Generation of Medical Image-Mask Pairs(https://arxiv.org/abs/2504.06897)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper presents MedSegFactory, a versatile medical synthesis framework that generates high-quality paired medical images and segmentation masks across modalities and tasks. It aims to serve as an unlimited data repository, supplying image-mask pairs to enhance existing segmentation tools. The core of MedSegFactory is a dual-stream diffusion model, where one stream synthesizes medical images and the other generates corresponding segmentation masks. To ensure precise alignment between image-mask pairs, we introduce Joint Cross-Attention (JCA), enabling a collaborative denoising paradigm by dynamic cross-conditioning between streams. This bidirectional interaction allows both representations to guide each other's generation, enhancing consistency between generated pairs. MedSegFactory unlocks on-demand generation of paired medical images and segmentation masks through user-defined prompts that specify the target labels, imaging modalities, anatomical regions, and pathological conditions, facilitating scalable and high-quality data generation. This new paradigm of medical image synthesis enables seamless integration into diverse medical imaging workflows, enhancing both efficiency and accuracy. Extensive experiments show that MedSegFactory generates data of superior quality and usability, achieving competitive or state-of-the-art performance in 2D and 3D segmentation tasks while addressing data scarcity and regulatory constraints.</li>
</ul>

<h3>Title: UKBOB: One Billion MRI Labeled Masks for Generalizable 3D Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Emmanuelle Bourigault, Amir Jamaludin, Abdullah Hamdi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06908">https://arxiv.org/abs/2504.06908</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06908">https://arxiv.org/pdf/2504.06908</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06908]] UKBOB: One Billion MRI Labeled Masks for Generalizable 3D Medical Image Segmentation(https://arxiv.org/abs/2504.06908)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>In medical imaging, the primary challenge is collecting large-scale labeled data due to privacy concerns, logistics, and high labeling costs. In this work, we present the UK Biobank Organs and Bones (UKBOB), the largest labeled dataset of body organs, comprising 51,761 MRI 3D samples (equivalent to 17.9 million 2D images) and more than 1.37 billion 2D segmentation masks of 72 organs, all based on the UK Biobank MRI dataset. We utilize automatic labeling, introduce an automated label cleaning pipeline with organ-specific filters, and manually annotate a subset of 300 MRIs with 11 abdominal classes to validate the quality (referred to as UKBOB-manual). This approach allows for scaling up the dataset collection while maintaining confidence in the labels. We further confirm the validity of the labels by demonstrating zero-shot generalization of trained models on the filtered UKBOB to other small labeled datasets from similar domains (e.g., abdominal MRI). To further mitigate the effect of noisy labels, we propose a novel method called Entropy Test-time Adaptation (ETTA) to refine the segmentation output. We use UKBOB to train a foundation model, Swin-BOB, for 3D medical image segmentation based on the Swin-UNetr architecture, achieving state-of-the-art results in several benchmarks in 3D medical imaging, including the BRATS brain MRI tumor challenge (with a 0.4% improvement) and the BTCV abdominal CT scan benchmark (with a 1.3% improvement). The pre-trained models and the code are available at this https URL , and the filtered labels will be made available with the UK Biobank.</li>
</ul>

<h3>Title: The Importance of Being Discrete: Measuring the Impact of Discretization in End-to-End Differentially Private Synthetic Data</h3>
<ul>
<li><strong>Authors: </strong>Georgi Ganev, Meenatchi Sundaram Muthu Selva Annamalai, Sofiane Mahiou, Emiliano De Cristofaro</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06923">https://arxiv.org/abs/2504.06923</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06923">https://arxiv.org/pdf/2504.06923</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06923]] The Importance of Being Discrete: Measuring the Impact of Discretization in End-to-End Differentially Private Synthetic Data(https://arxiv.org/abs/2504.06923)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Differentially Private (DP) generative marginal models are often used in the wild to release synthetic tabular datasets in lieu of sensitive data while providing formal privacy guarantees. These models approximate low-dimensional marginals or query workloads; crucially, they require the training data to be pre-discretized, i.e., continuous values need to first be partitioned into bins. However, as the range of values (or their domain) is often inferred directly from the training data, with the number of bins and bin edges typically defined arbitrarily, this approach can ultimately break end-to-end DP guarantees and may not always yield optimal utility. In this paper, we present an extensive measurement study of four discretization strategies in the context of DP marginal generative models. More precisely, we design DP versions of three discretizers (uniform, quantile, and k-means) and reimplement the PrivTree algorithm. We find that optimizing both the choice of discretizer and bin count can improve utility, on average, by almost 30% across six DP marginal models, compared to the default strategy and number of bins, with PrivTree being the best-performing discretizer in the majority of cases. We demonstrate that, while DP generative models with non-private discretization remain vulnerable to membership inference attacks, applying DP during discretization effectively mitigates this risk. Finally, we propose an optimized approach for automatically selecting the optimal number of bins, achieving high utility while reducing both privacy budget consumption and computational overhead.</li>
</ul>

<h3>Title: PathSegDiff: Pathology Segmentation using Diffusion model representations</h3>
<ul>
<li><strong>Authors: </strong>Sachin Kumar Danisetty, Alexandros Graikos, Srikar Yellapragada, Dimitris Samaras</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06950">https://arxiv.org/abs/2504.06950</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06950">https://arxiv.org/pdf/2504.06950</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06950]] PathSegDiff: Pathology Segmentation using Diffusion model representations(https://arxiv.org/abs/2504.06950)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, self-supervised</a></li>
<li><strong>Abstract: </strong>Image segmentation is crucial in many computational pathology pipelines, including accurate disease diagnosis, subtyping, outcome, and survivability prediction. The common approach for training a segmentation model relies on a pre-trained feature extractor and a dataset of paired image and mask annotations. These are used to train a lightweight prediction model that translates features into per-pixel classes. The choice of the feature extractor is central to the performance of the final segmentation model, and recent literature has focused on finding tasks to pre-train the feature extractor. In this paper, we propose PathSegDiff, a novel approach for histopathology image segmentation that leverages Latent Diffusion Models (LDMs) as pre-trained featured extractors. Our method utilizes a pathology-specific LDM, guided by a self-supervised encoder, to extract rich semantic information from H\&E stained histopathology images. We employ a simple, fully convolutional network to process the features extracted from the LDM and generate segmentation masks. Our experiments demonstrate significant improvements over traditional methods on the BCSS and GlaS datasets, highlighting the effectiveness of domain-specific diffusion pre-training in capturing intricate tissue structures and enhancing segmentation accuracy in histopathology images.</li>
</ul>

<h3>Title: Efficient Self-Supervised Learning for Earth Observation via Dynamic Dataset Curation</h3>
<ul>
<li><strong>Authors: </strong>Thomas Kerdreux, Alexandre Tuel, Quentin Febvre, Alexis Mouche, Bertrand Chapron</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06962">https://arxiv.org/abs/2504.06962</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06962">https://arxiv.org/pdf/2504.06962</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06962]] Efficient Self-Supervised Learning for Earth Observation via Dynamic Dataset Curation(https://arxiv.org/abs/2504.06962)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>Self-supervised learning (SSL) has enabled the development of vision foundation models for Earth Observation (EO), demonstrating strong transferability across diverse remote sensing tasks. While prior work has focused on network architectures and training strategies, the role of dataset curation, especially in balancing and diversifying pre-training datasets, remains underexplored. In EO, this challenge is amplified by the redundancy and heavy-tailed distributions common in satellite imagery, which can lead to biased representations and inefficient training. In this work, we propose a dynamic dataset pruning strategy designed to improve SSL pre-training by maximizing dataset diversity and balance. Our method iteratively refines the training set without requiring a pre-existing feature extractor, making it well-suited for domains where curated datasets are limited or unavailable. We demonstrate our approach on the Sentinel-1 Wave Mode (WV) Synthetic Aperture Radar (SAR) archive, a challenging dataset dominated by ocean observations. We train models from scratch on the entire Sentinel-1 WV archive spanning 10 years. Across three downstream tasks, our results show that dynamic pruning improves both computational efficiency and representation quality, leading to stronger transferability. We also release the weights of Nereus-SAR-1, the first model in the Nereus family, a series of foundation models for ocean observation and analysis using SAR imagery, at this http URL.</li>
</ul>

<h3>Title: Free Random Projection for In-Context Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Tomohiro Hayase, BenoÃ®t Collins, Nakamasa Inoue</a></li>
<li><strong>Subjects: </strong>cs.LG, math.PR, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.06983">https://arxiv.org/abs/2504.06983</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.06983">https://arxiv.org/pdf/2504.06983</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.06983]] Free Random Projection for In-Context Reinforcement Learning(https://arxiv.org/abs/2504.06983)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Hierarchical inductive biases are hypothesized to promote generalizable policies in reinforcement learning, as demonstrated by explicit hyperbolic latent representations and architectures. Therefore, a more flexible approach is to have these biases emerge naturally from the algorithm. We introduce Free Random Projection, an input mapping grounded in free probability theory that constructs random orthogonal matrices where hierarchical structure arises inherently. The free random projection integrates seamlessly into existing in-context reinforcement learning frameworks by encoding hierarchical organization within the input space without requiring explicit architectural modifications. Empirical results on multi-environment benchmarks show that free random projection consistently outperforms the standard random projection, leading to improvements in generalization. Furthermore, analyses within linearly solvable Markov decision processes and investigations of the spectrum of kernel random matrices reveal the theoretical underpinnings of free random projection's enhanced performance, highlighting its capacity for effective adaptation in hierarchically structured state spaces.</li>
</ul>

<h3>Title: Latent Diffusion U-Net Representations Contain Positional Embeddings and Anomalies</h3>
<ul>
<li><strong>Authors: </strong>Jonas Loos, Lorenz Linhardt</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07008">https://arxiv.org/abs/2504.07008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07008">https://arxiv.org/pdf/2504.07008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07008]] Latent Diffusion U-Net Representations Contain Positional Embeddings and Anomalies(https://arxiv.org/abs/2504.07008)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated remarkable capabilities in synthesizing realistic images, spurring interest in using their representations for various downstream tasks. To better understand the robustness of these representations, we analyze popular Stable Diffusion models using representational similarity and norms. Our findings reveal three phenomena: (1) the presence of a learned positional embedding in intermediate representations, (2) high-similarity corner artifacts, and (3) anomalous high-norm artifacts. These findings underscore the need to further investigate the properties of diffusion model representations before considering them for downstream tasks that require robust features. Project page: this https URL</li>
</ul>

<h3>Title: Evaluating Retrieval Augmented Generative Models for Document Queries in Transportation Safety</h3>
<ul>
<li><strong>Authors: </strong>Chad Melton, Alex Sorokine, Steve Peterson</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07022">https://arxiv.org/abs/2504.07022</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07022">https://arxiv.org/pdf/2504.07022</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07022]] Evaluating Retrieval Augmented Generative Models for Document Queries in Transportation Safety(https://arxiv.org/abs/2504.07022)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Applications of generative Large Language Models LLMs are rapidly expanding across various domains, promising significant improvements in workflow efficiency and information retrieval. However, their implementation in specialized, high-stakes domains such as hazardous materials transportation is challenging due to accuracy and reliability concerns. This study evaluates the performance of three fine-tuned generative models, ChatGPT, Google's Vertex AI, and ORNL Retrieval Augmented Generation augmented LLaMA 2 and LLaMA in retrieving regulatory information essential for hazardous material transportation compliance in the United States. Utilizing approximately 40 publicly available federal and state regulatory documents, we developed 100 realistic queries relevant to route planning and permitting requirements. Responses were qualitatively rated based on accuracy, detail, and relevance, complemented by quantitative assessments of semantic similarity between model outputs. Results demonstrated that the RAG-augmented LLaMA models significantly outperformed Vertex AI and ChatGPT, providing more detailed and generally accurate information, despite occasional inconsistencies. This research introduces the first known application of RAG in transportation safety, emphasizing the need for domain-specific fine-tuning and rigorous evaluation methodologies to ensure reliability and minimize the risk of inaccuracies in high-stakes environments.</li>
</ul>

<h3>Title: Teaching pathology foundation models to accurately predict gene expression with parameter efficient knowledge transfer</h3>
<ul>
<li><strong>Authors: </strong>Shi Pan, Jianan Chen, Maria Secrier</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07061">https://arxiv.org/abs/2504.07061</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07061">https://arxiv.org/pdf/2504.07061</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07061]] Teaching pathology foundation models to accurately predict gene expression with parameter efficient knowledge transfer(https://arxiv.org/abs/2504.07061)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Gene expression profiling provides critical insights into cellular heterogeneity, biological processes and disease mechanisms. There has been an increasing interest in computational approaches that can predict gene expression directly from digitalized histopathology images. While image foundation models have shown promise in a variety of pathology downstream analysis, their performances on gene-expression prediction are still limited. Explicitly incorporating information from the transcriptomic models can help image models to address domain shift, yet the fine-tuning and alignment of foundation models can be expensive. In the work, we propose Parameter Efficient Knowledge trAnsfer (PEKA), a novel framework that leverages Block-Affine Adaptation and integrates knowledge distillation and structure alignment losses for cross-modal knowledge transfer. We evaluated PEKA for gene expression prediction using multiple spatial transcriptomics datasets (comprising 206,123 image tiles with matched gene expression profiles) that encompassed various types of tissue. PEKA achieved at least 5\% performance improvement over baseline foundation models while also outperforming alternative parameter-efficient fine-tuning strategies. We will release the code, datasets and aligned models after peer-review to facilitate broader adoption and further development for parameter efficient model alignment.</li>
</ul>

<h3>Title: Detecting AI-generated Artwork</h3>
<ul>
<li><strong>Authors: </strong>Meien Li, Mark Stamp</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07078">https://arxiv.org/abs/2504.07078</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07078">https://arxiv.org/pdf/2504.07078</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07078]] Detecting AI-generated Artwork(https://arxiv.org/abs/2504.07078)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The high efficiency and quality of artwork generated by Artificial Intelligence (AI) has created new concerns and challenges for human artists. In particular, recent improvements in generative AI have made it difficult for people to distinguish between human-generated and AI-generated art. In this research, we consider the potential utility of various types of Machine Learning (ML) and Deep Learning (DL) models in distinguishing AI-generated artwork from human-generated artwork. We focus on three challenging artistic styles, namely, baroque, cubism, and expressionism. The learning models we test are Logistic Regression (LR), Support Vector Machine (SVM), Multilayer Perceptron (MLP), and Convolutional Neural Network (CNN). Our best experimental results yield a multiclass accuracy of 0.8208 over six classes, and an impressive accuracy of 0.9758 for the binary classification problem of distinguishing AI-generated from human-generated art.</li>
</ul>

<h3>Title: Identifying Unknown Stochastic Dynamics via Finite expression methods</h3>
<ul>
<li><strong>Authors: </strong>Senwei Liang, Chunmei Wang, Xingjian Xu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07085">https://arxiv.org/abs/2504.07085</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07085">https://arxiv.org/pdf/2504.07085</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07085]] Identifying Unknown Stochastic Dynamics via Finite expression methods(https://arxiv.org/abs/2504.07085)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Modeling stochastic differential equations (SDEs) is crucial for understanding complex dynamical systems in various scientific fields. Recent methods often employ neural network-based models, which typically represent SDEs through a combination of deterministic and stochastic terms. However, these models usually lack interpretability and have difficulty generalizing beyond their training domain. This paper introduces the Finite Expression Method (FEX), a symbolic learning approach designed to derive interpretable mathematical representations of the deterministic component of SDEs. For the stochastic component, we integrate FEX with advanced generative modeling techniques to provide a comprehensive representation of SDEs. The numerical experiments on linear, nonlinear, and multidimensional SDEs demonstrate that FEX generalizes well beyond the training domain and delivers more accurate long-term predictions compared to neural network-based methods. The symbolic expressions identified by FEX not only improve prediction accuracy but also offer valuable scientific insights into the underlying dynamics of the systems, paving the way for new scientific discoveries.</li>
</ul>

<h3>Title: Are We Done with Object-Centric Learning?</h3>
<ul>
<li><strong>Authors: </strong>Alexander Rubinstein, Ameya Prabhu, Matthias Bethge, Seong Joon Oh</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2504.07092">https://arxiv.org/abs/2504.07092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2504.07092">https://arxiv.org/pdf/2504.07092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2504.07092]] Are We Done with Object-Centric Learning?(https://arxiv.org/abs/2504.07092)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Object-centric learning (OCL) seeks to learn representations that only encode an object, isolated from other objects or background cues in a scene. This approach underpins various aims, including out-of-distribution (OOD) generalization, sample-efficient composition, and modeling of structured environments. Most research has focused on developing unsupervised mechanisms that separate objects into discrete slots in the representation space, evaluated using unsupervised object discovery. However, with recent sample-efficient segmentation models, we can separate objects in the pixel space and encode them independently. This achieves remarkable zero-shot performance on OOD object discovery benchmarks, is scalable to foundation models, and can handle a variable number of slots out-of-the-box. Hence, the goal of OCL methods to obtain object-centric representations has been largely achieved. Despite this progress, a key question remains: How does the ability to separate objects within a scene contribute to broader OCL objectives, such as OOD generalization? We address this by investigating the OOD generalization challenge caused by spurious background cues through the lens of OCL. We propose a novel, training-free probe called $\textbf{Object-Centric Classification with Applied Masks (OCCAM)}$, demonstrating that segmentation-based encoding of individual objects significantly outperforms slot-based OCL methods. However, challenges in real-world applications remain. We provide the toolbox for the OCL community to use scalable object-centric representations, and focus on practical applications and fundamental questions, such as understanding object perception in human cognition. Our code is available $\href{this https URL}{here}$.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
