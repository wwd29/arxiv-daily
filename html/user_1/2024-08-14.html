<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-08-14</h1>
<h3>Title: Biomedical Event Extraction via Structure-aware Generation</h3>
<ul>
<li><strong>Authors: </strong>Haohan Yuan, Siu Cheung Hui, Haopeng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.06583">https://arxiv.org/abs/2408.06583</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.06583">https://arxiv.org/pdf/2408.06583</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.06583]] Biomedical Event Extraction via Structure-aware Generation(https://arxiv.org/abs/2408.06583)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Biomedical Event Extraction (BEE) is a critical task that involves modeling complex relationships between fine-grained entities in biomedical text data. However, most existing BEE models rely on classification methods that neglect the label semantics and argument dependency structure within the data. To address these limitations, we propose GenBEE, a generative model enhanced with a structure-aware prefix for biomedical event extraction. GenBEE constructs event prompts that leverage knowledge distilled from large language models (LLMs), thereby incorporating both label semantics and argument dependency relationships. Additionally, GenBEE introduces a structural prefix learning module that generates structure-aware prefixes with structural prompts, enriching the generation process with structural features. Extensive experiments on three benchmark datasets demonstrate the effectiveness of GenBEE and it achieves state-of-the-art performance on the MLEE and GE11 datasets. Furthermore, our analysis shows that the structural prefixes effectively bridge the gap between structural prompts and the representation space of generative models, enabling better integration of event structural information.</li>
</ul>

<h3>Title: ViMo: Generating Motions from Casual Videos</h3>
<ul>
<li><strong>Authors: </strong>Liangdong Qiu, Chengxing Yu, Yanran Li, Zhao Wang, Haibin Huang, Chongyang Ma, Di Zhang, Pengfei Wan, Xiaoguang Han</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.06614">https://arxiv.org/abs/2408.06614</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.06614">https://arxiv.org/pdf/2408.06614</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.06614]] ViMo: Generating Motions from Casual Videos(https://arxiv.org/abs/2408.06614)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Although humans have the innate ability to imagine multiple possible actions from videos, it remains an extraordinary challenge for computers due to the intricate camera movements and montages. Most existing motion generation methods predominantly rely on manually collected motion datasets, usually tediously sourced from motion capture (Mocap) systems or Multi-View cameras, unavoidably resulting in a limited size that severely undermines their generalizability. Inspired by recent advance of diffusion models, we probe a simple and effective way to capture motions from videos and propose a novel Video-to-Motion-Generation framework (ViMo) which could leverage the immense trove of untapped video content to produce abundant and diverse 3D human motions. Distinct from prior work, our videos could be more causal, including complicated camera movements and occlusions. Striking experimental results demonstrate the proposed model could generate natural motions even for videos where rapid movements, varying perspectives, or frequent occlusions might exist. We also show this work could enable three important downstream applications, such as generating dancing motions according to arbitrary music and source video style. Extensive experimental results prove that our model offers an effective and scalable way to generate diversity and realistic motions. Code and demos will be public soon.</li>
</ul>

<h3>Title: Unveiling the Flaws: A Critical Analysis of Initialization Effect on Time Series Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Alex Koran, Hadi Hojjati, Narges Armanfard</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.06620">https://arxiv.org/abs/2408.06620</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.06620">https://arxiv.org/pdf/2408.06620</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.06620]] Unveiling the Flaws: A Critical Analysis of Initialization Effect on Time Series Anomaly Detection(https://arxiv.org/abs/2408.06620)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Deep learning for time-series anomaly detection (TSAD) has gained significant attention over the past decade. Despite the reported improvements in several papers, the practical application of these models remains limited. Recent studies have cast doubt on these models, attributing their results to flawed evaluation techniques. However, the impact of initialization has largely been overlooked. This paper provides a critical analysis of the initialization effects on TSAD model performance. Our extensive experiments reveal that TSAD models are highly sensitive to hyperparameters such as window size, seed number, and normalization. This sensitivity often leads to significant variability in performance, which can be exploited to artificially inflate the reported efficacy of these models. We demonstrate that even minor changes in initialization parameters can result in performance variations that overshadow the claimed improvements from novel model architectures. Our findings highlight the need for rigorous evaluation protocols and transparent reporting of preprocessing steps to ensure the reliability and fairness of anomaly detection methods. This paper calls for a more cautious interpretation of TSAD advancements and encourages the development of more robust and transparent evaluation practices to advance the field and its practical applications.</li>
</ul>

<h3>Title: Towards Robust and Cost-Efficient Knowledge Unlearning for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sungmin Cha, Sungjun Cho, Dasol Hwang, Moontae Lee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.06621">https://arxiv.org/abs/2408.06621</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.06621">https://arxiv.org/pdf/2408.06621</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.06621]] Towards Robust and Cost-Efficient Knowledge Unlearning for Large Language Models(https://arxiv.org/abs/2408.06621)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated strong reasoning and memorization capabilities via pretraining on massive textual corpora. However, training LLMs on human-written text entails significant risk of privacy and copyright violations, which demands an efficient machine unlearning framework to remove knowledge of sensitive data without retraining the model from scratch. While Gradient Ascent (GA) is widely used for unlearning by reducing the likelihood of generating unwanted information, the unboundedness of increasing the cross-entropy loss causes not only unstable optimization, but also catastrophic forgetting of knowledge that needs to be retained. We also discover its joint application under low-rank adaptation results in significantly suboptimal computational cost vs. generative performance trade-offs. In light of this limitation, we propose two novel techniques for robust and cost-efficient unlearning on LLMs. We first design an Inverted Hinge loss that suppresses unwanted tokens by increasing the probability of the next most likely token, thereby retaining fluency and structure in language generation. We also propose to initialize low-rank adapter weights based on Fisher-weighted low-rank approximation, which induces faster unlearning and better knowledge retention by allowing model updates to be focused on parameters that are important in generating textual data we wish to remove.</li>
</ul>

<h3>Title: Hybrid SD: Edge-Cloud Collaborative Inference for Stable Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Chenqian Yan, Songwei Liu, Hongjian Liu, Xurui Peng, Xiaojian Wang, Fangming Chen, Lean Fu, Xing Mei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.06646">https://arxiv.org/abs/2408.06646</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.06646">https://arxiv.org/pdf/2408.06646</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.06646]] Hybrid SD: Edge-Cloud Collaborative Inference for Stable Diffusion Models(https://arxiv.org/abs/2408.06646)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Stable Diffusion Models (SDMs) have shown remarkable proficiency in image synthesis. However, their broad application is impeded by their large model sizes and intensive computational requirements, which typically require expensive cloud servers for deployment. On the flip side, while there are many compact models tailored for edge devices that can reduce these demands, they often compromise on semantic integrity and visual quality when compared to full-sized SDMs. To bridge this gap, we introduce Hybrid SD, an innovative, training-free SDMs inference framework designed for edge-cloud collaborative inference. Hybrid SD distributes the early steps of the diffusion process to the large models deployed on cloud servers, enhancing semantic planning. Furthermore, small efficient models deployed on edge devices can be integrated for refining visual details in the later stages. Acknowledging the diversity of edge devices with differing computational and storage capacities, we employ structural pruning to the SDMs U-Net and train a lightweight VAE. Empirical evaluations demonstrate that our compressed models achieve state-of-the-art parameter efficiency (225.8M) on edge devices with competitive image quality. Additionally, Hybrid SD reduces the cloud cost by 66% with edge-cloud collaborative inference.</li>
</ul>

<h3>Title: RW-NSGCN: A Robust Approach to Structural Attacks via Negative Sampling</h3>
<ul>
<li><strong>Authors: </strong>Shuqi He, Jun Zhuang, Ding Wang, Jun Song</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.06665">https://arxiv.org/abs/2408.06665</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.06665">https://arxiv.org/pdf/2408.06665</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.06665]] RW-NSGCN: A Robust Approach to Structural Attacks via Negative Sampling(https://arxiv.org/abs/2408.06665)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Node classification using Graph Neural Networks (GNNs) has been widely applied in various practical scenarios, such as predicting user interests and detecting communities in social networks. However, recent studies have shown that graph-structured networks often contain potential noise and attacks, in the form of topological perturbations and weight disturbances, which can lead to decreased classification performance in GNNs. To improve the robustness of the model, we propose a novel method: Random Walk Negative Sampling Graph Convolutional Network (RW-NSGCN). Specifically, RW-NSGCN integrates the Random Walk with Restart (RWR) and PageRank (PGR) algorithms for negative sampling and employs a Determinantal Point Process (DPP)-based GCN for convolution operations. RWR leverages both global and local information to manage noise and local variations, while PGR assesses node importance to stabilize the topological structure. The DPP-based GCN ensures diversity among negative samples and aggregates their features to produce robust node embeddings, thereby improving classification performance. Experimental results demonstrate that the RW-NSGCN model effectively addresses network topology attacks and weight instability, increasing the accuracy of anomaly detection and overall stability. In terms of classification accuracy, RW-NSGCN significantly outperforms existing methods, showing greater resilience across various scenarios and effectively mitigating the impact of such vulnerabilities.</li>
</ul>

<h3>Title: Leveraging Priors via Diffusion Bridge for Time Series Generation</h3>
<ul>
<li><strong>Authors: </strong>Jinseong Park, Seungyun Lee, Woojin Jeong, Yujin Choi, Jaewook Lee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.06672">https://arxiv.org/abs/2408.06672</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.06672">https://arxiv.org/pdf/2408.06672</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.06672]] Leveraging Priors via Diffusion Bridge for Time Series Generation(https://arxiv.org/abs/2408.06672)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Time series generation is widely used in real-world applications such as simulation, data augmentation, and hypothesis test techniques. Recently, diffusion models have emerged as the de facto approach for time series generation, emphasizing diverse synthesis scenarios based on historical or correlated time series data streams. Since time series have unique characteristics, such as fixed time order and data scaling, standard Gaussian prior might be ill-suited for general time series generation. In this paper, we exploit the usage of diverse prior distributions for synthesis. Then, we propose TimeBridge, a framework that enables flexible synthesis by leveraging diffusion bridges to learn the transport between chosen prior and data distributions. Our model covers a wide range of scenarios in time series diffusion models, which leverages (i) data- and time-dependent priors for unconditional synthesis, and (ii) data-scale preserving synthesis with a constraint as a prior for conditional generation. Experimentally, our model achieves state-of-the-art performance in both unconditional and conditional time series generation tasks.</li>
</ul>

<h3>Title: Masked Image Modeling: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Vlad Hondru, Florinel Alin Croitoru, Shervin Minaee, Radu Tudor Ionescu, Nicu Sebe</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.06687">https://arxiv.org/abs/2408.06687</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.06687">https://arxiv.org/pdf/2408.06687</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.06687]] Masked Image Modeling: A Survey(https://arxiv.org/abs/2408.06687)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>In this work, we survey recent studies on masked image modeling (MIM), an approach that emerged as a powerful self-supervised learning technique in computer vision. The MIM task involves masking some information, e.g. pixels, patches, or even latent representations, and training a model, usually an autoencoder, to predicting the missing information by using the context available in the visible part of the input. We identify and formalize two categories of approaches on how to implement MIM as a pretext task, one based on reconstruction and one based on contrastive learning. Then, we construct a taxonomy and review the most prominent papers in recent years. We complement the manually constructed taxonomy with a dendrogram obtained by applying a hierarchical clustering algorithm. We further identify relevant clusters via manually inspecting the resulting dendrogram. Our review also includes datasets that are commonly used in MIM research. We aggregate the performance results of various masked image modeling methods on the most popular datasets, to facilitate the comparison of competing methods. Finally, we identify research gaps and propose several interesting directions of future work.</li>
</ul>

<h3>Title: DC3DO: Diffusion Classifier for 3D Objects</h3>
<ul>
<li><strong>Authors: </strong>Nursena Koprucu, Meher Shashwat Nigam, Shicheng Xu (Luke), Biruk Abere, Gabriele Dominici, Andrew Rodriguez, Sharvaree Vadgam, Berfin Inal, Alberto Tono</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.06693">https://arxiv.org/abs/2408.06693</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.06693">https://arxiv.org/pdf/2408.06693</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.06693]] DC3DO: Diffusion Classifier for 3D Objects(https://arxiv.org/abs/2408.06693)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Inspired by Geoffrey Hinton emphasis on generative modeling, To recognize shapes, first learn to generate them, we explore the use of 3D diffusion models for object classification. Leveraging the density estimates from these models, our approach, the Diffusion Classifier for 3D Objects (DC3DO), enables zero-shot classification of 3D shapes without additional training. On average, our method achieves a 12.5 percent improvement compared to its multiview counterparts, demonstrating superior multimodal reasoning over discriminative approaches. DC3DO employs a class-conditional diffusion model trained on ShapeNet, and we run inferences on point clouds of chairs and cars. This work highlights the potential of generative models in 3D object classification.</li>
</ul>

<h3>Title: Towards Cross-Domain Single Blood Cell Image Classification via Large-Scale LoRA-based Segment Anything Model</h3>
<ul>
<li><strong>Authors: </strong>Yongcheng Li, Lingcong Cai, Ying Lu, Yupeng Zhang, Jingyan Jiang, Genan Dai, Bowen Zhang, Jingzhou Cao, Xiangzhong Zhang, Xiaomao Fan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.06716">https://arxiv.org/abs/2408.06716</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.06716">https://arxiv.org/pdf/2408.06716</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.06716]] Towards Cross-Domain Single Blood Cell Image Classification via Large-Scale LoRA-based Segment Anything Model(https://arxiv.org/abs/2408.06716)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Accurate classification of blood cells plays a vital role in hematological analysis as it aids physicians in diagnosing various medical conditions. In this study, we present a novel approach for classifying blood cell images known as BC-SAM. BC-SAM leverages the large-scale foundation model of Segment Anything Model (SAM) and incorporates a fine-tuning technique using LoRA, allowing it to extract general image embeddings from blood cell images. To enhance the applicability of BC-SAM across different blood cell image datasets, we introduce an unsupervised cross-domain autoencoder that focuses on learning intrinsic features while suppressing artifacts in the images. To assess the performance of BC-SAM, we employ four widely used machine learning classifiers (Random Forest, Support Vector Machine, Artificial Neural Network, and XGBoost) to construct blood cell classification models and compare them against existing state-of-the-art methods. Experimental results conducted on two publicly available blood cell datasets (Matek-19 and Acevedo-20) demonstrate that our proposed BC-SAM achieves a new state-of-the-art result, surpassing the baseline methods with a significant improvement. The source code of this paper is available at this https URL.</li>
</ul>

<h3>Title: DiffLoRA: Generating Personalized Low-Rank Adaptation Weights with Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Yujia Wu, Yiming Shi, Jiwei Wei, Chengwei Sun, Yuyang Zhou, Yang Yang, Heng Tao Shen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.06740">https://arxiv.org/abs/2408.06740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.06740">https://arxiv.org/pdf/2408.06740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.06740]] DiffLoRA: Generating Personalized Low-Rank Adaptation Weights with Diffusion(https://arxiv.org/abs/2408.06740)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Personalized text-to-image generation has gained significant attention for its capability to generate high-fidelity portraits of specific identities conditioned on user-defined prompts. Existing methods typically involve test-time fine-tuning or instead incorporating an additional pre-trained branch. However, these approaches struggle to simultaneously address the demands of efficiency, identity fidelity, and preserving the model's original generative capabilities. In this paper, we propose DiffLoRA, a novel approach that leverages diffusion models as a hypernetwork to predict personalized low-rank adaptation (LoRA) weights based on the reference images. By integrating these LoRA weights into the text-to-image model, DiffLoRA achieves personalization during inference without further training. Additionally, we propose an identity-oriented LoRA weight construction pipeline to facilitate the training of DiffLoRA. By utilizing the dataset produced by this pipeline, our DiffLoRA consistently generates high-performance and accurate LoRA weights. Extensive evaluations demonstrate the effectiveness of our method, achieving both time efficiency and maintaining identity fidelity throughout the personalization process.</li>
</ul>

<h3>Title: Improving Synthetic Image Detection Towards Generalization: An Image Transformation Perspective</h3>
<ul>
<li><strong>Authors: </strong>Ouxiang Li, Jiayin Cai, Yanbin Hao, Xiaolong Jiang, Yao Hu, Fuli Feng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.06741">https://arxiv.org/abs/2408.06741</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.06741">https://arxiv.org/pdf/2408.06741</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.06741]] Improving Synthetic Image Detection Towards Generalization: An Image Transformation Perspective(https://arxiv.org/abs/2408.06741)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>With recent generative models facilitating photo-realistic image synthesis, the proliferation of synthetic images has also engendered certain negative impacts on social platforms, thereby raising an urgent imperative to develop effective detectors. Current synthetic image detection (SID) pipelines are primarily dedicated to crafting universal artifact features, accompanied by an oversight about SID training paradigm. In this paper, we re-examine the SID problem and identify two prevalent biases in current training paradigms, i.e., weakened artifact features and overfitted artifact features. Meanwhile, we discover that the imaging mechanism of synthetic images contributes to heightened local correlations among pixels, suggesting that detectors should be equipped with local awareness. In this light, we propose SAFE, a lightweight and effective detector with three simple image transformations. Firstly, for weakened artifact features, we substitute the down-sampling operator with the crop operator in image pre-processing to help circumvent artifact distortion. Secondly, for overfitted artifact features, we include ColorJitter and RandomRotation as additional data augmentations, to help alleviate irrelevant biases from color discrepancies and semantic differences in limited training samples. Thirdly, for local awareness, we propose a patch-based random masking strategy tailored for SID, forcing the detector to focus on local regions at training. Comparative experiments are conducted on an open-world dataset, comprising synthetic images generated by 26 distinct generative models. Our pipeline achieves a new state-of-the-art performance, with remarkable improvements of 4.5% in accuracy and 2.9% in average precision against existing methods.</li>
</ul>

<h3>Title: Oracle Bone Script Similiar Character Screening Approach Based on Simsiam Contrastive Learning and Supervised Learning</h3>
<ul>
<li><strong>Authors: </strong>Xinying Weng, Yifan Li, Shuaidong Hao, Jialiang Hou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.06811">https://arxiv.org/abs/2408.06811</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.06811">https://arxiv.org/pdf/2408.06811</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.06811]] Oracle Bone Script Similiar Character Screening Approach Based on Simsiam Contrastive Learning and Supervised Learning(https://arxiv.org/abs/2408.06811)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>This project proposes a new method that uses fuzzy comprehensive evaluation method to integrate ResNet-50 self-supervised and RepVGG supervised learning. The source image dataset HWOBC oracle is taken as input, the target image is selected, and finally the most similar image is output in turn without any manual intervention. The same feature encoding method is not used for images of different modalities. Before the model training, the image data is preprocessed, and the image is enhanced by random rotation processing, self-square graph equalization theory algorithm, and gamma transform, which effectively enhances the key feature learning. Finally, the fuzzy comprehensive evaluation method is used to combine the results of supervised training and unsupervised training, which can better solve the "most similar" problem that is difficult to quantify. At present, there are many unknown oracle-bone inscriptions waiting for us to crack. Contacting with the glyphs can provide new ideas for cracking.</li>
</ul>

<h3>Title: Membership Inference Attack Against Masked Image Modeling</h3>
<ul>
<li><strong>Authors: </strong>Zheng Li, Xinlei He, Ning Yu, Yang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.06825">https://arxiv.org/abs/2408.06825</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.06825">https://arxiv.org/pdf/2408.06825</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.06825]] Membership Inference Attack Against Masked Image Modeling(https://arxiv.org/abs/2408.06825)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Masked Image Modeling (MIM) has achieved significant success in the realm of self-supervised learning (SSL) for visual recognition. The image encoder pre-trained through MIM, involving the masking and subsequent reconstruction of input images, attains state-of-the-art performance in various downstream vision tasks. However, most existing works focus on improving the performance of this http URL this work, we take a different angle by studying the pre-training data privacy of MIM. Specifically, we propose the first membership inference attack against image encoders pre-trained by MIM, which aims to determine whether an image is part of the MIM pre-training dataset. The key design is to simulate the pre-training paradigm of MIM, i.e., image masking and subsequent reconstruction, and then obtain reconstruction errors. These reconstruction errors can serve as membership signals for achieving attack goals, as the encoder is more capable of reconstructing the input image in its training set with lower errors. Extensive evaluations are conducted on three model architectures and three benchmark datasets. Empirical results show that our attack outperforms baseline methods. Additionally, we undertake intricate ablation studies to analyze multiple factors that could influence the performance of the attack.</li>
</ul>

<h3>Title: Low-Bitwidth Floating Point Quantization for Efficient High-Quality Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Cheng Chen, Christina Giannoula, Andreas Moshovos</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.06995">https://arxiv.org/abs/2408.06995</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.06995">https://arxiv.org/pdf/2408.06995</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.06995]] Low-Bitwidth Floating Point Quantization for Efficient High-Quality Diffusion Models(https://arxiv.org/abs/2408.06995)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models are emerging models that generate images by iteratively denoising random Gaussian noise using deep neural networks. These models typically exhibit high computational and memory demands, necessitating effective post-training quantization for high-performance inference. Recent works propose low-bitwidth (e.g., 8-bit or 4-bit) quantization for diffusion models, however 4-bit integer quantization typically results in low-quality images. We observe that on several widely used hardware platforms, there is little or no difference in compute capability between floating-point and integer arithmetic operations of the same bitwidth (e.g., 8-bit or 4-bit). Therefore, we propose an effective floating-point quantization method for diffusion models that provides better image quality compared to integer quantization methods. We employ a floating-point quantization method that was effective for other processing tasks, specifically computer vision and natural language tasks, and tailor it for diffusion models by integrating weight rounding learning during the mapping of the full-precision values to the quantized values in the quantization process. We comprehensively study integer and floating-point quantization methods in state-of-the-art diffusion models. Our floating-point quantization method not only generates higher-quality images than that of integer quantization methods, but also shows no noticeable degradation compared to full-precision models (32-bit floating-point), when both weights and activations are quantized to 8-bit floating-point values, while has minimal degradation with 4-bit weights and 8-bit activations.</li>
</ul>

<h3>Title: Generative AI for automatic topic labelling</h3>
<ul>
<li><strong>Authors: </strong>Diego Kozlowski, Carolina Pradier, Pierre Benz</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.07003">https://arxiv.org/abs/2408.07003</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.07003">https://arxiv.org/pdf/2408.07003</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.07003]] Generative AI for automatic topic labelling(https://arxiv.org/abs/2408.07003)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Topic Modeling has become a prominent tool for the study of scientific fields, as they allow for a large scale interpretation of research trends. Nevertheless, the output of these models is structured as a list of keywords which requires a manual interpretation for the labelling. This paper proposes to assess the reliability of three LLMs, namely flan, GPT-4o, and GPT-4 mini for topic labelling. Drawing on previous research leveraging BERTopic, we generate topics from a dataset of all the scientific articles (n=34,797) authored by all biology professors in Switzerland (n=465) between 2008 and 2020, as recorded in the Web of Science database. We assess the output of the three models both quantitatively and qualitatively and find that, first, both GPT models are capable of accurately and precisely label topics from the models' output keywords. Second, 3-word labels are preferable to grasp the complexity of research topics.</li>
</ul>

<h3>Title: Imagen 3</h3>
<ul>
<li><strong>Authors: </strong>Imagen-Team-Google: Jason Baldridge, Jakob Bauer, Mukul Bhutani, Nicole Brichtova, Andrew Bunner, Kelvin Chan, Yichang Chen, Sander Dieleman, Yuqing Du, Zach Eaton-Rosen, Hongliang Fei, Nando de Freitas, Yilin Gao, Evgeny Gladchenko, Sergio Gómez Colmenarejo, Mandy Guo, Alex Haig, Will Hawkins, Hexiang Hu, Huilian Huang, Tobenna Peter Igwe, Christos Kaplanis, Siavash Khodadadeh, Yelin Kim, Ksenia Konyushkova, Karol Langner, Eric Lau, Shixin Luo, Soňa Mokrá, Henna Nandwani, Yasumasa Onoe, Aäron van den Oord, Zarana Parekh, Jordi Pont-Tuset, Hang Qi, Rui Qian, Deepak Ramachandran, Poorva Rane, Abdullah Rashwan, Ali Razavi, Robert Riachi, Hansa Srinivasan, Srivatsan Srinivasan, Robin Strudel, Benigno Uria, Oliver Wang, Su Wang, Austin Waters, Chris Wolff, Auriel Wright, Zhisheng Xiao, Hao Xiong, Keyang Xu, Marc van Zee, Junlin Zhang, Katie Zhang, Wenlei Zhou, Konrad Zolna, Ola Aboubakar, Canfer Akbulut, Oscar Akerlund, Isabela Albuquerque, Nina Anderson, Marco Andreetto, Lora Aroyo, Ben Bariach, David Barker, Sherry Ben, Dana Berman, Courtney Biles, Irina Blok, Pankil Botadra, Jenny Brennan, Karla Brown, John Buckley, Rudy Bunel, Elie Bursztein, Christina Butterfield, Ben Caine, Viral Carpenter, Norman Casagrande, Ming-Wei Chang, Solomon Chang, Shamik Chaudhuri, Tony Chen, John Choi, Dmitry Churbanau, Nathan Clement, Matan Cohen, Forrester Cole, Mikhail Dektiarev, Vincent Du, Praneet Dutta, Tom Eccles, Ndidi Elue, Ashley Feden, Shlomi Fruchter, Frankie Garcia, Roopal Garg</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.07009">https://arxiv.org/abs/2408.07009</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.07009">https://arxiv.org/pdf/2408.07009</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.07009]] Imagen 3(https://arxiv.org/abs/2408.07009)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce Imagen 3, a latent diffusion model that generates high quality images from text prompts. We describe our quality and responsibility evaluations. Imagen 3 is preferred over other state-of-the-art (SOTA) models at the time of evaluation. In addition, we discuss issues around safety and representation, as well as methods we used to minimize the potential harm of our models.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
