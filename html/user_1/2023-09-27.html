<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>diffusion</h2>
<h3>Title: COCO-Counterfactuals: Automatically Constructed Counterfactual Examples for Image-Text Pairs. (arXiv:2309.14356v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14356">http://arxiv.org/abs/2309.14356</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14356]] COCO-Counterfactuals: Automatically Constructed Counterfactual Examples for Image-Text Pairs(http://arxiv.org/abs/2309.14356)</code></li>
<li>Summary: <p>Counterfactual examples have proven to be valuable in the field of natural
language processing (NLP) for both evaluating and improving the robustness of
language models to spurious correlations in datasets. Despite their
demonstrated utility for NLP, multimodal counterfactual examples have been
relatively unexplored due to the difficulty of creating paired image-text data
with minimal counterfactual changes. To address this challenge, we introduce a
scalable framework for automatic generation of counterfactual examples using
text-to-image diffusion models. We use our framework to create
COCO-Counterfactuals, a multimodal counterfactual dataset of paired image and
text captions based on the MS-COCO dataset. We validate the quality of
COCO-Counterfactuals through human evaluations and show that existing
multimodal models are challenged by our counterfactual image-text pairs.
Additionally, we demonstrate the usefulness of COCO-Counterfactuals for
improving out-of-domain generalization of multimodal vision-language models via
training data augmentation.
</p></li>
</ul>

<h3>Title: Domain-Guided Conditional Diffusion Model for Unsupervised Domain Adaptation. (arXiv:2309.14360v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14360">http://arxiv.org/abs/2309.14360</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14360]] Domain-Guided Conditional Diffusion Model for Unsupervised Domain Adaptation(http://arxiv.org/abs/2309.14360)</code></li>
<li>Summary: <p>Limited transferability hinders the performance of deep learning models when
applied to new application scenarios. Recently, Unsupervised Domain Adaptation
(UDA) has achieved significant progress in addressing this issue via learning
domain-invariant features. However, the performance of existing UDA methods is
constrained by the large domain shift and limited target domain data. To
alleviate these issues, we propose DomAin-guided Conditional Diffusion Model
(DACDM) to generate high-fidelity and diversity samples for the target domain.
In the proposed DACDM, by introducing class information, the labels of
generated samples can be controlled, and a domain classifier is further
introduced in DACDM to guide the generated samples for the target domain. The
generated samples help existing UDA methods transfer from the source domain to
the target domain more easily, thus improving the transfer performance.
Extensive experiments on various benchmarks demonstrate that DACDM brings a
large improvement to the performance of existing UDA methods.
</p></li>
</ul>

<h3>Title: Free-Bloom: Zero-Shot Text-to-Video Generator with LLM Director and LDM Animator. (arXiv:2309.14494v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14494">http://arxiv.org/abs/2309.14494</a></li>
<li>Code URL: https://github.com/soolab/free-bloom</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14494]] Free-Bloom: Zero-Shot Text-to-Video Generator with LLM Director and LDM Animator(http://arxiv.org/abs/2309.14494)</code></li>
<li>Summary: <p>Text-to-video is a rapidly growing research area that aims to generate a
semantic, identical, and temporal coherence sequence of frames that accurately
align with the input text prompt. This study focuses on zero-shot text-to-video
generation considering the data- and cost-efficient. To generate a
semantic-coherent video, exhibiting a rich portrayal of temporal semantics such
as the whole process of flower blooming rather than a set of "moving images",
we propose a novel Free-Bloom pipeline that harnesses large language models
(LLMs) as the director to generate a semantic-coherence prompt sequence, while
pre-trained latent diffusion models (LDMs) as the animator to generate the high
fidelity frames. Furthermore, to ensure temporal and identical coherence while
maintaining semantic coherence, we propose a series of annotative modifications
to adapting LDMs in the reverse process, including joint noise sampling,
step-aware attention shift, and dual-path interpolation. Without any video data
and training requirements, Free-Bloom generates vivid and high-quality videos,
awe-inspiring in generating complex scenes with semantic meaningful frame
sequences. In addition, Free-Bloom is naturally compatible with LDMs-based
extensions.
</p></li>
</ul>

<h3>Title: Bootstrap Diffusion Model Curve Estimation for High Resolution Low-Light Image Enhancement. (arXiv:2309.14709v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14709">http://arxiv.org/abs/2309.14709</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14709]] Bootstrap Diffusion Model Curve Estimation for High Resolution Low-Light Image Enhancement(http://arxiv.org/abs/2309.14709)</code></li>
<li>Summary: <p>Learning-based methods have attracted a lot of research attention and led to
significant improvements in low-light image enhancement. However, most of them
still suffer from two main problems: expensive computational cost in high
resolution images and unsatisfactory performance in simultaneous enhancement
and denoising. To address these problems, we propose BDCE, a bootstrap
diffusion model that exploits the learning of the distribution of the curve
parameters instead of the normal-light image itself. Specifically, we adopt the
curve estimation method to handle the high-resolution images, where the curve
parameters are estimated by our bootstrap diffusion model. In addition, a
denoise module is applied in each iteration of curve adjustment to denoise the
intermediate enhanced result of each iteration. We evaluate BDCE on commonly
used benchmark datasets, and extensive experiments show that it achieves
state-of-the-art qualitative and quantitative performance.
</p></li>
</ul>

<h3>Title: Text-image guided Diffusion Model for generating Deepfake celebrity interactions. (arXiv:2309.14751v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14751">http://arxiv.org/abs/2309.14751</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14751]] Text-image guided Diffusion Model for generating Deepfake celebrity interactions(http://arxiv.org/abs/2309.14751)</code></li>
<li>Summary: <p>Deepfake images are fast becoming a serious concern due to their realism.
Diffusion models have recently demonstrated highly realistic visual content
generation, which makes them an excellent potential tool for Deepfake
generation. To curb their exploitation for Deepfakes, it is imperative to first
explore the extent to which diffusion models can be used to generate realistic
content that is controllable with convenient prompts. This paper devises and
explores a novel method in that regard. Our technique alters the popular stable
diffusion model to generate a controllable high-quality Deepfake image with
text and image prompts. In addition, the original stable model lacks severely
in generating quality images that contain multiple persons. The modified
diffusion model is able to address this problem, it add input anchor image's
latent at the beginning of inferencing rather than Gaussian random latent as
input. Hence, we focus on generating forged content for celebrity interactions,
which may be used to spread rumors. We also apply Dreambooth to enhance the
realism of our fake images. Dreambooth trains the pairing of center words and
specific features to produce more refined and personalized output images. Our
results show that with the devised scheme, it is possible to create fake visual
content with alarming realism, such that the content can serve as believable
evidence of meetings between powerful political figures.
</p></li>
</ul>

<h3>Title: On quantifying and improving realism of images generated with diffusion. (arXiv:2309.14756v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14756">http://arxiv.org/abs/2309.14756</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14756]] On quantifying and improving realism of images generated with diffusion(http://arxiv.org/abs/2309.14756)</code></li>
<li>Summary: <p>Recent advances in diffusion models have led to a quantum leap in the quality
of generative visual content. However, quantification of realism of the content
is still challenging. Existing evaluation metrics, such as Inception Score and
Fr\'echet inception distance, fall short on benchmarking diffusion models due
to the versatility of the generated images. Moreover, they are not designed to
quantify realism of an individual image. This restricts their application in
forensic image analysis, which is becoming increasingly important in the
emerging era of generative models. To address that, we first propose a metric,
called Image Realism Score (IRS), computed from five statistical measures of a
given image. This non-learning based metric not only efficiently quantifies
realism of the generated images, it is readily usable as a measure to classify
a given image as real or fake. We experimentally establish the model- and
data-agnostic nature of the proposed IRS by successfully detecting fake images
generated by Stable Diffusion Model (SDM), Dalle2, Midjourney and BigGAN.
</p>
<p>We further leverage this attribute of our metric to minimize an IRS-augmented
generative loss of SDM, and demonstrate a convenient yet considerable quality
improvement of the SDM-generated content with our modification. Our efforts
have also led to Gen-100 dataset, which provides 1,000 samples for 100 classes
generated by four high-quality models. We will release the dataset and code.
</p></li>
</ul>

<h3>Title: Navigating Text-To-Image Customization:From LyCORIS Fine-Tuning to Model Evaluation. (arXiv:2309.14859v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14859">http://arxiv.org/abs/2309.14859</a></li>
<li>Code URL: https://github.com/kohakublueleaf/lycoris</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14859]] Navigating Text-To-Image Customization:From LyCORIS Fine-Tuning to Model Evaluation(http://arxiv.org/abs/2309.14859)</code></li>
<li>Summary: <p>Text-to-image generative models have garnered immense attention for their
ability to produce high-fidelity images from text prompts. Among these, Stable
Diffusion distinguishes itself as a leading open-source model in this
fast-growing field. However, the intricacies of fine-tuning these models pose
multiple challenges from new methodology integration to systematic evaluation.
Addressing these issues, this paper introduces LyCORIS (Lora beYond
Conventional methods, Other Rank adaptation Implementations for Stable
diffusion) [https://github.com/KohakuBlueleaf/LyCORIS], an open-source library
that offers a wide selection of fine-tuning methodologies for Stable Diffusion.
Furthermore, we present a thorough framework for the systematic assessment of
varied fine-tuning techniques. This framework employs a diverse suite of
metrics and delves into multiple facets of fine-tuning, including
hyperparameter adjustments and the evaluation with different prompt types
across various concept categories. Through this comprehensive approach, our
work provides essential insights into the nuanced effects of fine-tuning
parameters, bridging the gap between state-of-the-art research and practical
application.
</p></li>
</ul>

<h3>Title: ITEM3D: Illumination-Aware Directional Texture Editing for 3D Models. (arXiv:2309.14872v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14872">http://arxiv.org/abs/2309.14872</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14872]] ITEM3D: Illumination-Aware Directional Texture Editing for 3D Models(http://arxiv.org/abs/2309.14872)</code></li>
<li>Summary: <p>Texture editing is a crucial task in 3D modeling that allows users to
automatically manipulate the surface materials of 3D models. However, the
inherent complexity of 3D models and the ambiguous text description lead to the
challenge in this task. To address this challenge, we propose ITEM3D, an
illumination-aware model for automatic 3D object editing according to the text
prompts. Leveraging the diffusion models and the differentiable rendering,
ITEM3D takes the rendered images as the bridge of text and 3D representation,
and further optimizes the disentangled texture and environment map. Previous
methods adopt the absolute editing direction namely score distillation sampling
(SDS) as the optimization objective, which unfortunately results in the noisy
appearance and text inconsistency. To solve the problem caused by the ambiguous
text, we introduce a relative editing direction, an optimization objective
defined by the noise difference between the source and target texts, to release
the semantic ambiguity between the texts and images. Additionally, we gradually
adjust the direction during optimization to further address the unexpected
deviation in the texture domain. Qualitative and quantitative experiments show
that our ITEM3D outperforms the state-of-the-art methods on various 3D objects.
We also perform text-guided relighting to show explicit control over lighting.
</p></li>
</ul>

<h3>Title: FEC: Three Finetuning-free Methods to Enhance Consistency for Real Image Editing. (arXiv:2309.14934v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14934">http://arxiv.org/abs/2309.14934</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14934]] FEC: Three Finetuning-free Methods to Enhance Consistency for Real Image Editing(http://arxiv.org/abs/2309.14934)</code></li>
<li>Summary: <p>Text-conditional image editing is a very useful task that has recently
emerged with immeasurable potential. Most current real image editing methods
first need to complete the reconstruction of the image, and then editing is
carried out by various methods based on the reconstruction. Most methods use
DDIM Inversion for reconstruction, however, DDIM Inversion often fails to
guarantee reconstruction performance, i.e., it fails to produce results that
preserve the original image content. To address the problem of reconstruction
failure, we propose FEC, which consists of three sampling methods, each
designed for different editing types and settings. Our three methods of FEC
achieve two important goals in image editing task: 1) ensuring successful
reconstruction, i.e., sampling to get a generated result that preserves the
texture and features of the original real image. 2) these sampling methods can
be paired with many editing methods and greatly improve the performance of
these editing methods to accomplish various editing tasks. In addition, none of
our sampling methods require fine-tuning of the diffusion model or
time-consuming training on large-scale datasets. Hence the cost of time as well
as the use of computer memory and computation can be significantly reduced.
</p></li>
</ul>

<h3>Title: LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion Models. (arXiv:2309.15103v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15103">http://arxiv.org/abs/2309.15103</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15103]] LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion Models(http://arxiv.org/abs/2309.15103)</code></li>
<li>Summary: <p>This work aims to learn a high-quality text-to-video (T2V) generative model
by leveraging a pre-trained text-to-image (T2I) model as a basis. It is a
highly desirable yet challenging task to simultaneously a) accomplish the
synthesis of visually realistic and temporally coherent videos while b)
preserving the strong creative generation nature of the pre-trained T2I model.
To this end, we propose LaVie, an integrated video generation framework that
operates on cascaded video latent diffusion models, comprising a base T2V
model, a temporal interpolation model, and a video super-resolution model. Our
key insights are two-fold: 1) We reveal that the incorporation of simple
temporal self-attentions, coupled with rotary positional encoding, adequately
captures the temporal correlations inherent in video data. 2) Additionally, we
validate that the process of joint image-video fine-tuning plays a pivotal role
in producing high-quality and creative outcomes. To enhance the performance of
LaVie, we contribute a comprehensive and diverse video dataset named Vimeo25M,
consisting of 25 million text-video pairs that prioritize quality, diversity,
and aesthetic appeal. Extensive experiments demonstrate that LaVie achieves
state-of-the-art performance both quantitatively and qualitatively.
Furthermore, we showcase the versatility of pre-trained LaVie models in various
long video generation and personalized video synthesis applications.
</p></li>
</ul>

<h3>Title: Generating Visual Scenes from Touch. (arXiv:2309.15117v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15117">http://arxiv.org/abs/2309.15117</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15117]] Generating Visual Scenes from Touch(http://arxiv.org/abs/2309.15117)</code></li>
<li>Summary: <p>An emerging line of work has sought to generate plausible imagery from touch.
Existing approaches, however, tackle only narrow aspects of the visuo-tactile
synthesis problem, and lag significantly behind the quality of cross-modal
synthesis methods in other domains. We draw on recent advances in latent
diffusion to create a model for synthesizing images from tactile signals (and
vice versa) and apply it to a number of visuo-tactile synthesis tasks. Using
this model, we significantly outperform prior work on the tactile-driven
stylization problem, i.e., manipulating an image to match a touch signal, and
we are the first to successfully generate images from touch without additional
sources of information about the scene. We also successfully use our model to
address two novel synthesis problems: generating images that do not contain the
touch sensor or the hand holding it, and estimating an image's shading from its
reflectance and touch.
</p></li>
</ul>

<h3>Title: Multiple Noises in Diffusion Model for Semi-Supervised Multi-Domain Translation. (arXiv:2309.14394v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14394">http://arxiv.org/abs/2309.14394</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14394]] Multiple Noises in Diffusion Model for Semi-Supervised Multi-Domain Translation(http://arxiv.org/abs/2309.14394)</code></li>
<li>Summary: <p>Domain-to-domain translation involves generating a target domain sample given
a condition in the source domain. Most existing methods focus on fixed input
and output domains, i.e. they only work for specific configurations (i.e. for
two domains, either $D_1\rightarrow{}D_2$ or $D_2\rightarrow{}D_1$). This paper
proposes Multi-Domain Diffusion (MDD), a conditional diffusion framework for
multi-domain translation in a semi-supervised context. Unlike previous methods,
MDD does not require defining input and output domains, allowing translation
between any partition of domains within a set (such as $(D_1,
D_2)\rightarrow{}D_3$, $D_2\rightarrow{}(D_1, D_3)$, $D_3\rightarrow{}D_1$,
etc. for 3 domains), without the need to train separate models for each domain
configuration. The key idea behind MDD is to leverage the noise formulation of
diffusion models by incorporating one noise level per domain, which allows
missing domains to be modeled with noise in a natural way. This transforms the
training task from a simple reconstruction task to a domain translation task,
where the model relies on less noisy domains to reconstruct more noisy domains.
We present results on a multi-domain (with more than two domains) synthetic
image translation dataset with challenging semantic domain inversion.
</p></li>
</ul>

<h3>Title: Efficient Post-training Quantization with FP8 Formats. (arXiv:2309.14592v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14592">http://arxiv.org/abs/2309.14592</a></li>
<li>Code URL: https://github.com/intel/neural-compressor</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14592]] Efficient Post-training Quantization with FP8 Formats(http://arxiv.org/abs/2309.14592)</code></li>
<li>Summary: <p>Recent advances in deep learning methods such as LLMs and Diffusion models
have created a need for improved quantization methods that can meet the
computational demands of these modern architectures while maintaining accuracy.
Towards this goal, we study the advantages of FP8 data formats for
post-training quantization across 75 unique network architectures covering a
wide range of tasks, including machine translation, language modeling, text
generation, image classification, generation, and segmentation. We examine
three different FP8 representations (E5M2, E4M3, and E3M4) to study the effects
of varying degrees of trade-off between dynamic range and precision on model
accuracy. Based on our extensive study, we developed a quantization workflow
that generalizes across different network architectures. Our empirical results
show that FP8 formats outperform INT8 in multiple aspects, including workload
coverage (92.64% vs. 65.87%), model accuracy and suitability for a broader
range of operations. Furthermore, our findings suggest that E4M3 is better
suited for NLP models, whereas E3M4 performs marginally better than E4M3 on
computer vision tasks. The code is publicly available on Intel Neural
Compressor: https://github.com/intel/neural-compressor.
</p></li>
</ul>

<h2>self-supervised</h2>
<h3>Title: Pre-training-free Image Manipulation Localization through Non-Mutually Exclusive Contrastive Learning. (arXiv:2309.14900v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14900">http://arxiv.org/abs/2309.14900</a></li>
<li>Code URL: https://github.com/knightzjz/ncl-iml</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14900]] Pre-training-free Image Manipulation Localization through Non-Mutually Exclusive Contrastive Learning(http://arxiv.org/abs/2309.14900)</code></li>
<li>Summary: <p>Deep Image Manipulation Localization (IML) models suffer from training data
insufficiency and thus heavily rely on pre-training. We argue that contrastive
learning is more suitable to tackle the data insufficiency problem for IML.
Crafting mutually exclusive positives and negatives is the prerequisite for
contrastive learning. However, when adopting contrastive learning in IML, we
encounter three categories of image patches: tampered, authentic, and contour
patches. Tampered and authentic patches are naturally mutually exclusive, but
contour patches containing both tampered and authentic pixels are non-mutually
exclusive to them. Simply abnegating these contour patches results in a drastic
performance loss since contour patches are decisive to the learning outcomes.
Hence, we propose the Non-mutually exclusive Contrastive Learning (NCL)
framework to rescue conventional contrastive learning from the above dilemma.
In NCL, to cope with the non-mutually exclusivity, we first establish a pivot
structure with dual branches to constantly switch the role of contour patches
between positives and negatives while training. Then, we devise a
pivot-consistent loss to avoid spatial corruption caused by the role-switching
process. In this manner, NCL both inherits the self-supervised merits to
address the data insufficiency and retains a high manipulation localization
accuracy. Extensive experiments verify that our NCL achieves state-of-the-art
performance on all five benchmarks without any pre-training and is more robust
on unseen real-life samples. The code is available at:
https://github.com/Knightzjz/NCL-IML.
</p></li>
</ul>

<h3>Title: Doduo: Learning Dense Visual Correspondence from Unsupervised Semantic-Aware Flow. (arXiv:2309.15110v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15110">http://arxiv.org/abs/2309.15110</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15110]] Doduo: Learning Dense Visual Correspondence from Unsupervised Semantic-Aware Flow(http://arxiv.org/abs/2309.15110)</code></li>
<li>Summary: <p>Dense visual correspondence plays a vital role in robotic perception. This
work focuses on establishing the dense correspondence between a pair of images
that captures dynamic scenes undergoing substantial transformations. We
introduce Doduo to learn general dense visual correspondence from in-the-wild
images and videos without ground truth supervision. Given a pair of images, it
estimates the dense flow field encoding the displacement of each pixel in one
image to its corresponding pixel in the other image. Doduo uses flow-based
warping to acquire supervisory signals for the training. Incorporating semantic
priors with self-supervised flow training, Doduo produces accurate dense
correspondence robust to the dynamic changes of the scenes. Trained on an
in-the-wild video dataset, Doduo illustrates superior performance on
point-level correspondence estimation over existing self-supervised
correspondence learning baselines. We also apply Doduo to articulation
estimation and zero-shot goal-conditioned manipulation, underlining its
practical applications in robotics. Code and additional visualizations are
available at https://ut-austin-rpl.github.io/Doduo
</p></li>
</ul>

<h2>foundation model</h2>
<h3>Title: Assessment of IBM and NASA's geospatial foundation model in flood inundation mapping. (arXiv:2309.14500v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14500">http://arxiv.org/abs/2309.14500</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14500]] Assessment of IBM and NASA's geospatial foundation model in flood inundation mapping(http://arxiv.org/abs/2309.14500)</code></li>
<li>Summary: <p>Vision foundation models are a new frontier in GeoAI research because of
their potential to enable powerful image analysis by learning and extracting
important image features from vast amounts of geospatial data. This paper
evaluates the performance of the first-of-its-kind geospatial foundation model,
IBM-NASA's Prithvi, to support a crucial geospatial analysis task: flood
inundation mapping. This model is compared with popular convolutional neural
network and vision transformer-based architectures in terms of mapping accuracy
for flooded areas. A benchmark dataset, Sen1Floods11, is used in the
experiments, and the models' predictability, generalizability, and
transferability are evaluated based on both a test dataset and a dataset that
is completely unseen by the model. Results show the impressive transferability
of the Prithvi model, highlighting its performance advantages in segmenting
flooded areas in previously unseen regions. The findings also suggest areas for
improvement for the Prithvi model in terms of adopting multi-scale
representation learning, developing more end-to-end pipelines for high-level
image analysis tasks, and offering more flexibility in terms of input data
bands.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Generative Escher Meshes. (arXiv:2309.14564v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14564">http://arxiv.org/abs/2309.14564</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14564]] Generative Escher Meshes(http://arxiv.org/abs/2309.14564)</code></li>
<li>Summary: <p>This paper proposes a fully-automatic, text-guided generative method for
producing periodic, repeating, tile-able 2D art, such as the one seen on
floors, mosaics, ceramics, and the work of M.C. Escher. In contrast to the
standard concept of a seamless texture, i.e., square images that are seamless
when tiled, our method generates non-square tilings which comprise solely of
repeating copies of the same object. It achieves this by optimizing both
geometry and color of a 2D mesh, in order to generate a non-square tile in the
shape and appearance of the desired object, with close to no additional
background details. We enable geometric optimization of tilings by our key
technical contribution: an unconstrained, differentiable parameterization of
the space of all possible tileable shapes for a given symmetry group. Namely,
we prove that modifying the laplacian used in a 2D mesh-mapping technique -
Orbifold Tutte Embedding - can achieve all possible tiling configurations for a
chosen planar symmetry group. We thus consider both the mesh's tile-shape and
its texture as optimizable parameters, rendering the textured mesh via a
differentiable renderer. We leverage a trained image diffusion model to define
a loss on the resulting image, thereby updating the mesh's parameters based on
its appearance matching the text prompt. We show our method is able to produce
plausible, appealing results, with non-trivial tiles, for a variety of
different periodic tiling patterns.
</p></li>
</ul>

<h3>Title: A Comparative Study of Population-Graph Construction Methods and Graph Neural Networks for Brain Age Regression. (arXiv:2309.14816v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14816">http://arxiv.org/abs/2309.14816</a></li>
<li>Code URL: https://github.com/bintsi/brain-age-population-graphs</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14816]] A Comparative Study of Population-Graph Construction Methods and Graph Neural Networks for Brain Age Regression(http://arxiv.org/abs/2309.14816)</code></li>
<li>Summary: <p>The difference between the chronological and biological brain age of a
subject can be an important biomarker for neurodegenerative diseases, thus
brain age estimation can be crucial in clinical settings. One way to
incorporate multimodal information into this estimation is through population
graphs, which combine various types of imaging data and capture the
associations among individuals within a population. In medical imaging,
population graphs have demonstrated promising results, mostly for
classification tasks. In most cases, the graph structure is pre-defined and
remains static during training. However, extracting population graphs is a
non-trivial task and can significantly impact the performance of Graph Neural
Networks (GNNs), which are sensitive to the graph structure. In this work, we
highlight the importance of a meaningful graph construction and experiment with
different population-graph construction methods and their effect on GNN
performance on brain age estimation. We use the homophily metric and graph
visualizations to gain valuable quantitative and qualitative insights on the
extracted graph structures. For the experimental evaluation, we leverage the UK
Biobank dataset, which offers many imaging and non-imaging phenotypes. Our
results indicate that architectures highly sensitive to the graph structure,
such as Graph Convolutional Network (GCN) and Graph Attention Network (GAT),
struggle with low homophily graphs, while other architectures, such as
GraphSage and Chebyshev, are more robust across different homophily ratios. We
conclude that static graph construction approaches are potentially insufficient
for the task of brain age estimation and make recommendations for alternative
research directions.
</p></li>
</ul>

<h3>Title: Locality-preserving Directions for Interpreting the Latent Space of Satellite Image GANs. (arXiv:2309.14883v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14883">http://arxiv.org/abs/2309.14883</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14883]] Locality-preserving Directions for Interpreting the Latent Space of Satellite Image GANs(http://arxiv.org/abs/2309.14883)</code></li>
<li>Summary: <p>We present a locality-aware method for interpreting the latent space of
wavelet-based Generative Adversarial Networks (GANs), that can well capture the
large spatial and spectral variability that is characteristic to satellite
imagery. By focusing on preserving locality, the proposed method is able to
decompose the weight-space of pre-trained GANs and recover interpretable
directions that correspond to high-level semantic concepts (such as
urbanization, structure density, flora presence) - that can subsequently be
used for guided synthesis of satellite imagery. In contrast to typically used
approaches that focus on capturing the variability of the weight-space in a
reduced dimensionality space (i.e., based on Principal Component Analysis,
PCA), we show that preserving locality leads to vectors with different angles,
that are more robust to artifacts and can better preserve class information.
Via a set of quantitative and qualitative examples, we further show that the
proposed approach can outperform both baseline geometric augmentations, as well
as global, PCA-based approaches for data synthesis in the context of data
augmentation for satellite scene classification.
</p></li>
</ul>

<h3>Title: Machine-assisted mixed methods: augmenting humanities and social sciences with artificial intelligence. (arXiv:2309.14379v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14379">http://arxiv.org/abs/2309.14379</a></li>
<li>Code URL: https://github.com/andreskarjus/machineassistedmixedmethods</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14379]] Machine-assisted mixed methods: augmenting humanities and social sciences with artificial intelligence(http://arxiv.org/abs/2309.14379)</code></li>
<li>Summary: <p>The increasing capacities of large language models (LLMs) present an
unprecedented opportunity to scale up data analytics in the humanities and
social sciences, augmenting and automating qualitative analytic tasks
previously typically allocated to human labor. This contribution proposes a
systematic mixed methods framework to harness qualitative analytic expertise,
machine scalability, and rigorous quantification, with attention to
transparency and replicability. 16 machine-assisted case studies are showcased
as proof of concept. Tasks include linguistic and discourse analysis, lexical
semantic change detection, interview analysis, historical event cause inference
and text mining, detection of political stance, text and idea reuse, genre
composition in literature and film; social network inference, automated
lexicography, missing metadata augmentation, and multimodal visual cultural
analytics. In contrast to the focus on English in the emerging LLM
applicability literature, many examples here deal with scenarios involving
smaller languages and historical texts prone to digitization distortions. In
all but the most difficult tasks requiring expert knowledge, generative LLMs
can demonstrably serve as viable research instruments. LLM (and human)
annotations may contain errors and variation, but the agreement rate can and
should be accounted for in subsequent statistical modeling; a bootstrapping
approach is discussed. The replications among the case studies illustrate how
tasks previously requiring potentially months of team effort and complex
computational pipelines, can now be accomplished by an LLM-assisted scholar in
a fraction of the time. Importantly, this approach is not intended to replace,
but to augment researcher knowledge and skills. With these opportunities in
sight, qualitative expertise and the ability to pose insightful questions have
arguably never been more critical.
</p></li>
</ul>

<h3>Title: When Automated Assessment Meets Automated Content Generation: Examining Text Quality in the Era of GPTs. (arXiv:2309.14488v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14488">http://arxiv.org/abs/2309.14488</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14488]] When Automated Assessment Meets Automated Content Generation: Examining Text Quality in the Era of GPTs(http://arxiv.org/abs/2309.14488)</code></li>
<li>Summary: <p>The use of machine learning (ML) models to assess and score textual data has
become increasingly pervasive in an array of contexts including natural
language processing, information retrieval, search and recommendation, and
credibility assessment of online content. A significant disruption at the
intersection of ML and text are text-generating large-language models such as
generative pre-trained transformers (GPTs). We empirically assess the
differences in how ML-based scoring models trained on human content assess the
quality of content generated by humans versus GPTs. To do so, we propose an
analysis framework that encompasses essay scoring ML-models, human and
ML-generated essays, and a statistical model that parsimoniously considers the
impact of type of respondent, prompt genre, and the ML model used for
assessment model. A rich testbed is utilized that encompasses 18,460
human-generated and GPT-based essays. Results of our benchmark analysis reveal
that transformer pretrained language models (PLMs) more accurately score human
essay quality as compared to CNN/RNN and feature-based ML methods.
Interestingly, we find that the transformer PLMs tend to score GPT-generated
text 10-15\% higher on average, relative to human-authored documents.
Conversely, traditional deep learning and feature-based ML models score human
text considerably higher. Further analysis reveals that although the
transformer PLMs are exclusively fine-tuned on human text, they more
prominently attend to certain tokens appearing only in GPT-generated text,
possibly due to familiarity/overlap in pre-training. Our framework and results
have implications for text classification settings where automated scoring of
text is likely to be disrupted by generative AI.
</p></li>
</ul>

<h3>Title: Introducing DictaLM -- A Large Generative Language Model for Modern Hebrew. (arXiv:2309.14568v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14568">http://arxiv.org/abs/2309.14568</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14568]] Introducing DictaLM -- A Large Generative Language Model for Modern Hebrew(http://arxiv.org/abs/2309.14568)</code></li>
<li>Summary: <p>We present DictaLM, a large-scale language model tailored for Modern Hebrew.
Boasting 7B parameters, this model is predominantly trained on Hebrew-centric
data. As a commitment to promoting research and development in the Hebrew
language, we release both the foundation model and the instruct-tuned model
under a Creative Commons license. Concurrently, we introduce DictaLM-Rab,
another foundation model geared towards Rabbinic/Historical Hebrew. These
foundation models serve as ideal starting points for fine-tuning various
Hebrew-specific tasks, such as instruction, Q&amp;A, sentiment analysis, and more.
This release represents a preliminary step, offering an initial Hebrew LLM
model for the Hebrew NLP community to experiment with.
</p></li>
</ul>

<h3>Title: Fine-tuning and aligning question answering models for complex information extraction tasks. (arXiv:2309.14805v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14805">http://arxiv.org/abs/2309.14805</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14805]] Fine-tuning and aligning question answering models for complex information extraction tasks(http://arxiv.org/abs/2309.14805)</code></li>
<li>Summary: <p>The emergence of Large Language Models (LLMs) has boosted performance and
possibilities in various NLP tasks. While the usage of generative AI models
like ChatGPT opens up new opportunities for several business use cases, their
current tendency to hallucinate fake content strongly limits their
applicability to document analysis, such as information retrieval from
documents. In contrast, extractive language models like question answering (QA)
or passage retrieval models guarantee query results to be found within the
boundaries of an according context document, which makes them candidates for
more reliable information extraction in productive environments of companies.
In this work we propose an approach that uses and integrates extractive QA
models for improved feature extraction of German business documents such as
insurance reports or medical leaflets into a document analysis solution. We
further show that fine-tuning existing German QA models boosts performance for
tailored extraction tasks of complex linguistic features like damage cause
explanations or descriptions of medication appearance, even with using only a
small set of annotated data. Finally, we discuss the relevance of scoring
metrics for evaluating information extraction tasks and deduce a combined
metric from Levenshtein distance, F1-Score, Exact Match and ROUGE-L to mimic
the assessment criteria from human experts.
</p></li>
</ul>

<h3>Title: Automating question generation from educational text. (arXiv:2309.15004v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.15004">http://arxiv.org/abs/2309.15004</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.15004]] Automating question generation from educational text(http://arxiv.org/abs/2309.15004)</code></li>
<li>Summary: <p>The use of question-based activities (QBAs) is wide-spread in education,
traditionally forming an integral part of the learning and assessment process.
In this paper, we design and evaluate an automated question generation tool for
formative and summative assessment in schools. We present an expert survey of
one hundred and four teachers, demonstrating the need for automated generation
of QBAs, as a tool that can significantly reduce the workload of teachers and
facilitate personalized learning experiences. Leveraging the recent
advancements in generative AI, we then present a modular framework employing
transformer based language models for automatic generation of multiple-choice
questions (MCQs) from textual content. The presented solution, with distinct
modules for question generation, correct answer prediction, and distractor
formulation, enables us to evaluate different language models and generation
techniques. Finally, we perform an extensive quantitative and qualitative
evaluation, demonstrating trade-offs in the use of different techniques and
models.
</p></li>
</ul>

<h3>Title: DECORAIT -- DECentralized Opt-in/out Registry for AI Training. (arXiv:2309.14400v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14400">http://arxiv.org/abs/2309.14400</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14400]] DECORAIT -- DECentralized Opt-in/out Registry for AI Training(http://arxiv.org/abs/2309.14400)</code></li>
<li>Summary: <p>We present DECORAIT; a decentralized registry through which content creators
may assert their right to opt in or out of AI training as well as receive
reward for their contributions. Generative AI (GenAI) enables images to be
synthesized using AI models trained on vast amounts of data scraped from public
sources. Model and content creators who may wish to share their work openly
without sanctioning its use for training are thus presented with a data
governance challenge. Further, establishing the provenance of GenAI training
data is important to creatives to ensure fair recognition and reward for their
such use. We report a prototype of DECORAIT, which explores hierarchical
clustering and a combination of on/off-chain storage to create a scalable
decentralized registry to trace the provenance of GenAI training data in order
to determine training consent and reward creatives who contribute that data.
DECORAIT combines distributed ledger technology (DLT) with visual
fingerprinting, leveraging the emerging C2PA (Coalition for Content Provenance
and Authenticity) standard to create a secure, open registry through which
creatives may express consent and data ownership for GenAI.
</p></li>
</ul>

<h3>Title: Sampling - Variational Auto Encoder - Ensemble: In the Quest of Explainable Artificial Intelligence. (arXiv:2309.14385v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14385">http://arxiv.org/abs/2309.14385</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14385]] Sampling - Variational Auto Encoder - Ensemble: In the Quest of Explainable Artificial Intelligence(http://arxiv.org/abs/2309.14385)</code></li>
<li>Summary: <p>Explainable Artificial Intelligence (XAI) models have recently attracted a
great deal of interest from a variety of application sectors. Despite
significant developments in this area, there are still no standardized methods
or approaches for understanding AI model outputs. A systematic and cohesive
framework is also increasingly necessary to incorporate new techniques like
discriminative and generative models to close the gap. This paper contributes
to the discourse on XAI by presenting an empirical evaluation based on a novel
framework: Sampling - Variational Auto Encoder (VAE) - Ensemble Anomaly
Detection (SVEAD). It is a hybrid architecture where VAE combined with ensemble
stacking and SHapley Additive exPlanations are used for imbalanced
classification. The finding reveals that combining ensemble stacking, VAE, and
SHAP can. not only lead to better model performance but also provide an easily
explainable framework. This work has used SHAP combined with Permutation
Importance and Individual Conditional Expectations to create a powerful
interpretability of the model. The finding has an important implication in the
real world, where the need for XAI is paramount to boost confidence in AI
applications.
</p></li>
</ul>

<h2>anomaly</h2>
<h3>Title: Divide and Conquer in Video Anomaly Detection: A Comprehensive Review and New Approach. (arXiv:2309.14622v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14622">http://arxiv.org/abs/2309.14622</a></li>
<li>Code URL: https://github.com/XiaoJian923/Divide-and-Conquer</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14622]] Divide and Conquer in Video Anomaly Detection: A Comprehensive Review and New Approach(http://arxiv.org/abs/2309.14622)</code></li>
<li>Summary: <p>Video anomaly detection is a complex task, and the principle of "divide and
conquer" is often regarded as an effective approach to tackling intricate
issues. It's noteworthy that recent methods in video anomaly detection have
revealed the application of the divide and conquer philosophy (albeit with
distinct perspectives from traditional usage), yielding impressive outcomes.
This paper systematically reviews these literatures from six dimensions, aiming
to enhance the use of the divide and conquer strategy in video anomaly
detection. Furthermore, based on the insights gained from this review, a novel
approach is presented, which integrates human skeletal frameworks with video
data analysis techniques. This method achieves state-of-the-art performance on
the ShanghaiTech dataset, surpassing all existing advanced methods.
</p></li>
</ul>

<h3>Title: LogGPT: Log Anomaly Detection via GPT. (arXiv:2309.14482v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14482">http://arxiv.org/abs/2309.14482</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14482]] LogGPT: Log Anomaly Detection via GPT(http://arxiv.org/abs/2309.14482)</code></li>
<li>Summary: <p>Detecting system anomalies based on log data is important for ensuring the
security and reliability of computer systems. Recently, deep learning models
have been widely used for log anomaly detection. The core idea is to model the
log sequences as natural language and adopt deep sequential models, such as
LSTM or Transformer, to encode the normal patterns in log sequences via
language modeling. However, there is a gap between language modeling and
anomaly detection as the objective of training a sequential model via a
language modeling loss is not directly related to anomaly detection. To fill up
the gap, we propose LogGPT, a novel framework that employs GPT for log anomaly
detection. LogGPT is first trained to predict the next log entry based on the
preceding sequence. To further enhance the performance of LogGPT, we propose a
novel reinforcement learning strategy to finetune the model specifically for
the log anomaly detection task. The experimental results on three datasets show
that LogGPT significantly outperforms existing state-of-the-art approaches.
</p></li>
</ul>

<h3>Title: Detach-ROCKET: Sequential feature selection for time series classification with random convolutional kernels. (arXiv:2309.14518v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14518">http://arxiv.org/abs/2309.14518</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14518]] Detach-ROCKET: Sequential feature selection for time series classification with random convolutional kernels(http://arxiv.org/abs/2309.14518)</code></li>
<li>Summary: <p>Time series classification is essential in many fields, such as medicine,
finance, environmental science, and manufacturing, enabling tasks like disease
diagnosis, anomaly detection, and stock price prediction. Machine learning
models like Recurrent Neural Networks and InceptionTime, while successful in
numerous applications, can face scalability limitations due to intensive
training requirements. To address this, random convolutional kernel models such
as Rocket and its derivatives have emerged, simplifying training and achieving
state-of-the-art performance by utilizing a large number of randomly generated
features from time series data. However, due to their random nature, most of
the generated features are redundant or non-informative, adding unnecessary
computational load and compromising generalization. Here, we introduce
Sequential Feature Detachment (SFD) as a method to identify and prune these
non-essential features. SFD uses model coefficients to estimate feature
importance and, unlike previous algorithms, can handle large feature sets
without the need for complex hyperparameter tuning. Testing on the UCR archive
demonstrates that SFD can produce models with $10\%$ of the original features
while improving $0.2\%$ the accuracy on the test set. We also present an
end-to-end procedure for determining an optimal balance between the number of
features and model accuracy, called Detach-ROCKET. When applied to the largest
binary UCR dataset, Detach-ROCKET is capable of reduce model size by $98.9\%$
and increases test accuracy by $0.6\%$.
</p></li>
</ul>

<h2>in-context</h2>
<h3>Title: Boosting In-Context Learning with Factual Knowledge. (arXiv:2309.14771v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14771">http://arxiv.org/abs/2309.14771</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14771]] Boosting In-Context Learning with Factual Knowledge(http://arxiv.org/abs/2309.14771)</code></li>
<li>Summary: <p>In-Context Learning (ICL) over Large language models (LLMs) aims at solving
previously unseen tasks by conditioning on a few training examples, eliminating
the need for parameter updates and achieving competitive performance. In this
paper, we demonstrate that factual knowledge is imperative for the performance
of ICL in three core facets, i.e., the inherent knowledge learned in LLMs, the
factual knowledge derived from the selected in-context examples, and the
knowledge biases in LLMs for output generation. To unleash the power of LLMs in
few-shot learning scenarios, we introduce a novel Knowledgeable In-Context
Tuning (KICT) framework to further improve the performance of ICL: 1) injecting
factual knowledge to LLMs during continual self-supervised pre-training, 2)
judiciously selecting the examples with high knowledge relevance, and 3)
calibrating the prediction results based on prior knowledge. We evaluate the
proposed approaches on auto-regressive LLMs (e.g., GPT-style models) over
multiple text classification and question answering tasks. Experimental results
demonstrate that KICT substantially outperforms strong baselines, and improves
by more than 13% and 7% of accuracy on text classification and question
answering tasks, respectively.
</p></li>
</ul>

<h3>Title: Are Human-generated Demonstrations Necessary for In-context Learning?. (arXiv:2309.14681v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.14681">http://arxiv.org/abs/2309.14681</a></li>
<li>Code URL: https://github.com/ruili33/sec</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.14681]] Are Human-generated Demonstrations Necessary for In-context Learning?(http://arxiv.org/abs/2309.14681)</code></li>
<li>Summary: <p>Despite the promising few-shot ability of large language models (LLMs), the
standard paradigm of In-context Learning (ICL) suffers the disadvantages of
susceptibility to selected demonstrations and the intricacy to generate these
demonstrations. In this paper, we raise the fundamental question that whether
human-generated demonstrations are necessary for ICL. To answer this question,
we propose self-contemplation prompting strategy (SEC), a paradigm free from
human-crafted demonstrations. The key point of SEC is that, instead of using
hand-crafted examples as demonstrations in ICL, SEC asks LLMs to first create
demonstrations on their own, based on which the final output is generated. SEC
is a flexible framework and can be adapted to both the vanilla ICL and the
chain-of-thought (CoT), but with greater ease: as the manual-generation process
of both examples and rationale can be saved. Extensive experiments in
arithmetic reasoning, commonsense reasoning, multi-task language understanding,
and code generation benchmarks, show that SEC, which does not require
hand-crafted demonstrations, significantly outperforms the zero-shot learning
strategy, and achieves comparable results to ICL with hand-crafted
demonstrations. This demonstrates that, for many tasks, contemporary LLMs
possess a sufficient level of competence to exclusively depend on their own
capacity for decision making, removing the need for external training data.
Code is available at https://github.com/ruili33/SEC.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
