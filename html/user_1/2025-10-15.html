<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-10-15</h1>
<h3>Title: GAR: Generative Adversarial Reinforcement Learning for Formal Theorem Proving</h3>
<ul>
<li><strong>Authors: </strong>Ruida Wang, Jiarui Yao, Rui Pan, Shizhe Diao, Tong Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11769">https://arxiv.org/abs/2510.11769</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11769">https://arxiv.org/pdf/2510.11769</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11769]] GAR: Generative Adversarial Reinforcement Learning for Formal Theorem Proving(https://arxiv.org/abs/2510.11769)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Solving math problems through verifiable languages such as Lean has significantly impacted both the mathematics and computer science communities. Current state-of-the-art models are often trained with expensive online Reinforcement Learning (RL) or expert iteration. However, these approaches rely on fixed problem sets, which causes inefficient training and limits the model to tackle complex problems. To overcome these limitations, we propose GAR: Generative Adversarial Reinforcement learning, a comprehensive RL training framework that jointly trains the problem composer and solver in an adversarial loop. GAR introduces an implicit curriculum learning mechanism, which aligns task difficulty with the prover's evolving capability. It thereby improves the training efficiency and enables stronger performance of proving advanced theorems. Experiments show that with GAR training, Goedel-Prover-V2-8B and DeepSeek-Prover-V2-7B achieve an average relative improvement in pass@32 of 4.20% on MiniF2F-Test benchmark, while DeepSeek-Prover-V2's pass@32 on ProofNet-Test increases from 22.58% to 25.81%. Beyond formal proving, GAR establishes a general RL paradigm for co-evolution of problem generation and solving under verifiable environments.</li>
</ul>

<h3>Title: Combining Euclidean and Hyperbolic Representations for Node-level Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Simone Mungari, Ettore Ritacco, Pietro Sabatino</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11827">https://arxiv.org/abs/2510.11827</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11827">https://arxiv.org/pdf/2510.11827</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11827]] Combining Euclidean and Hyperbolic Representations for Node-level Anomaly Detection(https://arxiv.org/abs/2510.11827)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Node-level anomaly detection (NAD) is challenging due to diverse structural patterns and feature distributions. As such, NAD is a critical task with several applications which range from fraud detection, cybersecurity, to recommendation systems. We introduce Janus, a framework that jointly leverages Euclidean and Hyperbolic Graph Neural Networks to capture complementary aspects of node representations. Each node is described by two views, composed by the original features and structural features derived from random walks and degrees, then embedded into Euclidean and Hyperbolic spaces. A multi Graph-Autoencoder framework, equipped with a contrastive learning objective as regularization term, aligns the embeddings across the Euclidean and Hyperbolic spaces, highlighting nodes whose views are difficult to reconcile and are thus likely anomalous. Experiments on four real-world datasets show that Janus consistently outperforms shallow and deep baselines, empirically demonstrating that combining multiple geometric representations provides a robust and effective approach for identifying subtle and complex anomalies in graphs.</li>
</ul>

<h3>Title: Schrödinger bridge for generative AI: Soft-constrained formulation and convergence analysis</h3>
<ul>
<li><strong>Authors: </strong>Jin Ma, Ying Tan, Renyuan Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, math.DS, math.OC, q-fin.MF</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11829">https://arxiv.org/abs/2510.11829</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11829">https://arxiv.org/pdf/2510.11829</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11829]] Schrödinger bridge for generative AI: Soft-constrained formulation and convergence analysis(https://arxiv.org/abs/2510.11829)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative AI can be framed as the problem of learning a model that maps simple reference measures into complex data distributions, and it has recently found a strong connection to the classical theory of the Schrödinger bridge problems (SBPs) due partly to their common nature of interpolating between prescribed marginals via entropy-regularized stochastic dynamics. However, the classical SBP enforces hard terminal constraints, which often leads to instability in practical implementations, especially in high-dimensional or data-scarce regimes. To address this challenge, we follow the idea of the so-called soft-constrained Schrödinger bridge problem (SCSBP), in which the terminal constraint is replaced by a general penalty function. This relaxation leads to a more flexible stochastic control formulation of McKean-Vlasov type. We establish the existence of optimal solutions for all penalty levels and prove that, as the penalty grows, both the controls and value functions converge to those of the classical SBP at a linear rate. Our analysis builds on Doob's h-transform representations, the stability results of Schrödinger potentials, Gamma-convergence, and a novel fixed-point argument that couples an optimization problem over the space of measures with an auxiliary entropic optimal transport problem. These results not only provide the first quantitative convergence guarantees for soft-constrained bridges but also shed light on how penalty regularization enables robust generative modeling, fine-tuning, and transfer learning.</li>
</ul>

<h3>Title: Don't Walk the Line: Boundary Guidance for Filtered Generation</h3>
<ul>
<li><strong>Authors: </strong>Sarah Ball, Andreas Haupt</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11834">https://arxiv.org/abs/2510.11834</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11834">https://arxiv.org/pdf/2510.11834</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11834]] Don't Walk the Line: Boundary Guidance for Filtered Generation(https://arxiv.org/abs/2510.11834)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative models are increasingly paired with safety classifiers that filter harmful or undesirable outputs. A common strategy is to fine-tune the generator to reduce the probability of being filtered, but this can be suboptimal: it often pushes the model toward producing samples near the classifier's decision boundary, increasing both false positives and false negatives. We propose Boundary Guidance, a reinforcement learning fine-tuning method that explicitly steers generation away from the classifier's margin. On a benchmark of jailbreak and ambiguous prompts, Boundary Guidance improves both the safety and the utility of outputs, as judged by LLM-as-a-Judge evaluations. Comprehensive ablations across model scales and reward designs demonstrate the robustness of our approach.</li>
</ul>

<h3>Title: Data or Language Supervision: What Makes CLIP Better than DINO?</h3>
<ul>
<li><strong>Authors: </strong>Yiming Liu, Yuhui Zhang, Dhruba Ghosh, Ludwig Schmidt, Serena Yeung-Levy</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11835">https://arxiv.org/abs/2510.11835</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11835">https://arxiv.org/pdf/2510.11835</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11835]] Data or Language Supervision: What Makes CLIP Better than DINO?(https://arxiv.org/abs/2510.11835)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>CLIP outperforms self-supervised models like DINO as vision encoders for vision-language models (VLMs), but it remains unclear whether this advantage stems from CLIP's language supervision or its much larger training data. To disentangle these factors, we pre-train CLIP and DINO under controlled settings -- using the same architecture, dataset, and training configuration -- achieving similar ImageNet accuracy. Embedding analysis shows that CLIP captures high-level semantics (e.g., object categories, text), while DINO is more responsive to low-level features like colors and styles. When integrated into VLMs and evaluated on 20 VQA benchmarks, CLIP excels at text-intensive tasks, while DINO slightly outperforms on vision-centric ones. Variants of language supervision (e.g., sigmoid loss, pre-trained language encoders) yield limited gains. Our findings provide scientific insights into vision encoder design and its impact on VLM performance.</li>
</ul>

<h3>Title: WaveletDiff: Multilevel Wavelet Diffusion For Time Series Generation</h3>
<ul>
<li><strong>Authors: </strong>Yu-Hsiang Wang, Olgica Milenkovic</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11839">https://arxiv.org/abs/2510.11839</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11839">https://arxiv.org/pdf/2510.11839</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11839]] WaveletDiff: Multilevel Wavelet Diffusion For Time Series Generation(https://arxiv.org/abs/2510.11839)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Time series are ubiquitous in many applications that involve forecasting, classification and causal inference tasks, such as healthcare, finance, audio signal processing and climate sciences. Still, large, high-quality time series datasets remain scarce. Synthetic generation can address this limitation; however, current models confined either to the time or frequency domains struggle to reproduce the inherently multi-scaled structure of real-world time series. We introduce WaveletDiff, a novel framework that trains diffusion models directly on wavelet coefficients to exploit the inherent multi-resolution structure of time series data. The model combines dedicated transformers for each decomposition level with cross-level attention mechanisms that enable selective information exchange between temporal and frequency scales through adaptive gating. It also incorporates energy preservation constraints for individual levels based on Parseval's theorem to preserve spectral fidelity throughout the diffusion process. Comprehensive tests across six real-world datasets from energy, finance, and neuroscience domains demonstrate that WaveletDiff consistently outperforms state-of-the-art time-domain and frequency-domain generative methods on both short and long time series across five diverse performance metrics. For example, WaveletDiff achieves discriminative scores and Context-FID scores that are $3\times$ smaller on average than the second-best baseline across all datasets.</li>
</ul>

<h3>Title: MammoDINO: Anatomically Aware Self-Supervision for Mammographic Images</h3>
<ul>
<li><strong>Authors: </strong>Sicheng Zhou, Lei Wu, Cao Xiao, Parminder Bhatia, Taha Kass-Hout</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11883">https://arxiv.org/abs/2510.11883</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11883">https://arxiv.org/pdf/2510.11883</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11883]] MammoDINO: Anatomically Aware Self-Supervision for Mammographic Images(https://arxiv.org/abs/2510.11883)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-supervised learning (SSL) has transformed vision encoder training in general domains but remains underutilized in medical imaging due to limited data and domain specific biases. We present MammoDINO, a novel SSL framework for mammography, pretrained on 1.4 million mammographic images. To capture clinically meaningful features, we introduce a breast tissue aware data augmentation sampler for both image-level and patch-level supervision and a cross-slice contrastive learning objective that leverages 3D digital breast tomosynthesis (DBT) structure into 2D pretraining. MammoDINO achieves state-of-the-art performance on multiple breast cancer screening tasks and generalizes well across five benchmark datasets. It offers a scalable, annotation-free foundation for multipurpose computer-aided diagnosis (CAD) tools for mammogram, helping reduce radiologists' workload and improve diagnostic efficiency in breast cancer screening.</li>
</ul>

<h3>Title: GRAVITY: A Framework for Personalized Text Generation via Profile-Grounded Synthetic Preferences</h3>
<ul>
<li><strong>Authors: </strong>Priyanka Dey, Daniele Rosa, Wenqing Zheng, Daniel Barcklow, Jieyu Zhao, Emilio Ferrara</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11952">https://arxiv.org/abs/2510.11952</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11952">https://arxiv.org/pdf/2510.11952</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11952]] GRAVITY: A Framework for Personalized Text Generation via Profile-Grounded Synthetic Preferences(https://arxiv.org/abs/2510.11952)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Personalization in LLMs often relies on costly human feedback or interaction logs, limiting scalability and neglecting deeper user attributes. To reduce the reliance on human annotations, we introduce GRAVITY (Generative Response with Aligned Values, Interests, and Traits of You), a framework for generating synthetic, profile-grounded preference data that captures users' interests, values, beliefs, and personality traits. By integrating demographic, cultural, and psychological frameworks -- including Hofstede's cultural dimensions, Schwartz's basic values, the World Values Survey, and Big Five OCEAN traits -- GRAVITY synthesizes preference pairs to guide personalized content generation. We evaluate GRAVITY on book descriptions for 400 Amazon users, comparing it to prompt-based conditioning, standard fine-tuning, and naive synthetic pair generation. Profile-grounded synthetic data consistently improves generation, especially across multiple cultures (USA, Brazil, Japan, India), achieving over 4% higher preference gains across baselines, with user studies showing that GRAVITY outputs are preferred over 86% of the time. Our results show that scenario-grounded synthetic data can capture richer user variation, reduce reliance on costly annotation, and produce more engaging, user-centered content, offering a scalable path for LLM personalization.</li>
</ul>

<h3>Title: Y-shaped Generative Flows</h3>
<ul>
<li><strong>Authors: </strong>Arip Asadulaev, Semyon Semenov, Abduragim Shtanchaev, Eric Moulines, Fakhri Karray, Martin Takac</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11955">https://arxiv.org/abs/2510.11955</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11955">https://arxiv.org/pdf/2510.11955</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11955]] Y-shaped Generative Flows(https://arxiv.org/abs/2510.11955)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Modern continuous-time generative models often induce V-shaped transport: each sample travels independently along nearly straight trajectories from prior to data, overlooking shared structure. We introduce Y-shaped generative flows, which move probability mass together along shared pathways before branching to target-specific endpoints. Our formulation is based on novel velocity-powered transport cost with a sublinear exponent (between zero and one). this concave dependence rewards joint and fast mass movement. Practically, we instantiate the idea in a scalable neural ODE training objective. On synthetic, image, and biology datasets, Y-flows recover hierarchy-aware structure, improve distributional metrics over strong flow-based baselines, and reach targets with fewer integration steps.</li>
</ul>

<h3>Title: MosaicDiff: Training-free Structural Pruning for Diffusion Model Acceleration Reflecting Pretraining Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Bowei Guo, Shengkun Tang, Cong Zeng, Zhiqiang Shen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.11962">https://arxiv.org/abs/2510.11962</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.11962">https://arxiv.org/pdf/2510.11962</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.11962]] MosaicDiff: Training-free Structural Pruning for Diffusion Model Acceleration Reflecting Pretraining Dynamics(https://arxiv.org/abs/2510.11962)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models are renowned for their generative capabilities, yet their pretraining processes exhibit distinct phases of learning speed that have been entirely overlooked in prior post-training acceleration efforts in the community. In this study, we introduce a novel framework called MosaicDiff that aligns diffusion pretraining dynamics with post-training sampling acceleration via trajectory-aware structural pruning. Our approach leverages the observation that the middle, fast-learning stage of diffusion pretraining requires more conservative pruning to preserve critical model features, while the early and later, slow-learning stages benefit from a more aggressive pruning strategy. This adaptive pruning mechanism is the first to explicitly mirror the inherent learning speed variations of diffusion pretraining, thereby harmonizing the model's inner training dynamics with its accelerated sampling process. Extensive experiments on DiT and SDXL demonstrate that our method achieves significant speed-ups in sampling without compromising output quality, outperforming previous state-of-the-art methods by large margins, also providing a new viewpoint for more efficient and robust training-free diffusion acceleration.</li>
</ul>

<h3>Title: Mamaba Can Learn Low-Dimensional Targets In-Context via Test-Time Feature Learning</h3>
<ul>
<li><strong>Authors: </strong>Junsoo Oh, Wei Huang, Taiji Suzuki</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12026">https://arxiv.org/abs/2510.12026</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12026">https://arxiv.org/pdf/2510.12026</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12026]] Mamaba Can Learn Low-Dimensional Targets In-Context via Test-Time Feature Learning(https://arxiv.org/abs/2510.12026)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Mamba, a recently proposed linear-time sequence model, has attracted significant attention for its computational efficiency and strong empirical performance. However, a rigorous theoretical understanding of its underlying mechanisms remains limited. In this work, we provide a theoretical analysis of Mamba's in-context learning (ICL) capability by focusing on tasks defined by low-dimensional nonlinear target functions. Specifically, we study in-context learning of a single-index model $y \approx g_*(\langle \boldsymbol{\beta}, \boldsymbol{x} \rangle)$, which depends on only a single relevant direction $\boldsymbol{\beta}$, referred to as feature. We prove that Mamba, pretrained by gradient-based methods, can achieve efficient ICL via test-time feature learning, extracting the relevant direction directly from context examples. Consequently, we establish a test-time sample complexity that improves upon linear Transformers -- analyzed to behave like kernel methods -- and is comparable to nonlinear Transformers, which have been shown to surpass the Correlational Statistical Query (CSQ) lower bound and achieve near information-theoretically optimal rate in previous works. Our analysis reveals the crucial role of the nonlinear gating mechanism in Mamba for feature extraction, highlighting it as the fundamental driver behind Mamba's ability to achieve both computational efficiency and high performance.</li>
</ul>

<h3>Title: Your VAR Model is Secretly an Efficient and Explainable Generative Classifier</h3>
<ul>
<li><strong>Authors: </strong>Yi-Chung Chen, David I. Inouye, Jing Gao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12060">https://arxiv.org/abs/2510.12060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12060">https://arxiv.org/pdf/2510.12060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12060]] Your VAR Model is Secretly an Efficient and Explainable Generative Classifier(https://arxiv.org/abs/2510.12060)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generative classifiers, which leverage conditional generative models for classification, have recently demonstrated desirable properties such as robustness to distribution shifts. However, recent progress in this area has been largely driven by diffusion-based models, whose substantial computational cost severely limits scalability. This exclusive focus on diffusion-based methods has also constrained our understanding of generative classifiers. In this work, we propose a novel generative classifier built on recent advances in visual autoregressive (VAR) modeling, which offers a new perspective for studying generative classifiers. To further enhance its performance, we introduce the Adaptive VAR Classifier$^+$ (A-VARC$^+$), which achieves a superior trade-off between accuracy and inference speed, thereby significantly improving practical applicability. Moreover, we show that the VAR-based method exhibits fundamentally different properties from diffusion-based methods. In particular, due to its tractable likelihood, the VAR-based classifier enables visual explainability via token-wise mutual information and demonstrates inherent resistance to catastrophic forgetting in class-incremental learning tasks.</li>
</ul>

<h3>Title: VIDMP3: Video Editing by Representing Motion with Pose and Position Priors</h3>
<ul>
<li><strong>Authors: </strong>Sandeep Mishra, Oindrila Saha, Alan C. Bovik</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12069">https://arxiv.org/abs/2510.12069</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12069">https://arxiv.org/pdf/2510.12069</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12069]] VIDMP3: Video Editing by Representing Motion with Pose and Position Priors(https://arxiv.org/abs/2510.12069)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Motion-preserved video editing is crucial for creators, particularly in scenarios that demand flexibility in both the structure and semantics of swapped objects. Despite its potential, this area remains underexplored. Existing diffusion-based editing methods excel in structure-preserving tasks, using dense guidance signals to ensure content integrity. While some recent methods attempt to address structure-variable editing, they often suffer from issues such as temporal inconsistency, subject identity drift, and the need for human intervention. To address these challenges, we introduce VidMP3, a novel approach that leverages pose and position priors to learn a generalized motion representation from source videos. Our method enables the generation of new videos that maintain the original motion while allowing for structural and semantic flexibility. Both qualitative and quantitative evaluations demonstrate the superiority of our approach over existing methods. The code will be made publicly available at this https URL.</li>
</ul>

<h3>Title: A Review on Domain Adaption and Generative Adversarial Networks(GANs)</h3>
<ul>
<li><strong>Authors: </strong>Aashish Dhawan, Divyanshu Mudgal</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12075">https://arxiv.org/abs/2510.12075</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12075">https://arxiv.org/pdf/2510.12075</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12075]] A Review on Domain Adaption and Generative Adversarial Networks(GANs)(https://arxiv.org/abs/2510.12075)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The major challenge in today's computer vision scenario is the availability of good quality labeled data. In a field of study like image classification, where data is of utmost importance, we need to find more reliable methods which can overcome the scarcity of data to produce results comparable to previous benchmark results. In most cases, obtaining labeled data is very difficult because of the high cost of human labor and in some cases impossible. The purpose of this paper is to discuss Domain Adaptation and various methods to implement it. The main idea is to use a model trained on a particular dataset to predict on data from a different domain of the same kind, for example - a model trained on paintings of airplanes predicting on real images of airplanes</li>
</ul>

<h3>Title: Elevating Medical Image Security: A Cryptographic Framework Integrating Hyperchaotic Map and GRU</h3>
<ul>
<li><strong>Authors: </strong>Weixuan Li, Guang Yu, Quanjun Li, Junhua Zhou, Jiajun Chen, Yihang Dong, Mengqian Wang, Zimeng Li, Changwei Gong, Lin Tang, Xuhang Chen</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12084">https://arxiv.org/abs/2510.12084</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12084">https://arxiv.org/pdf/2510.12084</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12084]] Elevating Medical Image Security: A Cryptographic Framework Integrating Hyperchaotic Map and GRU(https://arxiv.org/abs/2510.12084)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Chaotic systems play a key role in modern image encryption due to their sensitivity to initial conditions, ergodicity, and complex dynamics. However, many existing chaos-based encryption methods suffer from vulnerabilities, such as inadequate permutation and diffusion, and suboptimal pseudorandom properties. This paper presents Kun-IE, a novel encryption framework designed to address these issues. The framework features two key contributions: the development of the 2D Sin-Cos Pi Hyperchaotic Map (2D-SCPHM), which offers a broader chaotic range and superior pseudorandom sequence generation, and the introduction of Kun-SCAN, a novel permutation strategy that significantly reduces pixel correlations, enhancing resistance to statistical attacks. Kun-IE is flexible and supports encryption for images of any size. Experimental results and security analyses demonstrate its robustness against various cryptanalytic attacks, making it a strong solution for secure image communication. The code is available at this \href{this https URL}{link}.</li>
</ul>

<h3>Title: GraphShaper: Geometry-aware Alignment for Improving Transfer Learning in Text-Attributed Graphs</h3>
<ul>
<li><strong>Authors: </strong>Heng Zhang, Tianyi Zhang, Yuling Shi, Xiaodong Gu, Yaomin Shen, Haochen You, Zijian Zhang, Yilei Yuan, Jin Huang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12085">https://arxiv.org/abs/2510.12085</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12085">https://arxiv.org/pdf/2510.12085</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12085]] GraphShaper: Geometry-aware Alignment for Improving Transfer Learning in Text-Attributed Graphs(https://arxiv.org/abs/2510.12085)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Graph foundation models represent a transformative paradigm for learning transferable representations across diverse graph domains. Recent methods leverage large language models to unify graph and text modalities into a shared representation space using contrastive learning. However, systematic evaluations reveal significant performance degradation at structural boundaries where distinct topological patterns converge, with accuracy losses exceeding 20 percentage points. This issue arises from a key limitation: current methods assume all graph structures can be encoded within a single Euclidean space. In reality, tree structures require hyperbolic geometry to preserve hierarchical branching, while cyclic patterns depend on spherical geometry for closure properties. At structural boundaries, nodes experience conflicting geometric constraints that uniform encoding spaces cannot resolve. This raises a crucial challenge: \textbf{Can alignment frameworks be designed to respect the intrinsic geometric diversity of graph structures?} We introduce \textbf{GraphShaper}, a geometry-aware framework that enhances graph encoding through multi-geometric specialization. Our approach employs expert networks tailored to different geometric spaces, dynamically computing fusion weights to adaptively integrate geometric properties based on local structural characteristics. This adaptive fusion preserves structural integrity before alignment with text embeddings. Extensive experiments demonstrate that GraphShaper achieves 9.47\% accuracy improvements on citation networks and 7.63\% on social networks in zero-shot settings.</li>
</ul>

<h3>Title: Playmate2: Training-Free Multi-Character Audio-Driven Animation via Diffusion Transformer with Reward Feedback</h3>
<ul>
<li><strong>Authors: </strong>Xingpei Ma, Shenneng Huang, Jiaran Cai, Yuansheng Guan, Shen Zheng, Hanfeng Zhao, Qiang Zhang, Shunsi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12089">https://arxiv.org/abs/2510.12089</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12089">https://arxiv.org/pdf/2510.12089</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12089]] Playmate2: Training-Free Multi-Character Audio-Driven Animation via Diffusion Transformer with Reward Feedback(https://arxiv.org/abs/2510.12089)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model</a></li>
<li><strong>Abstract: </strong>Recent advances in diffusion models have significantly improved audio-driven human video generation, surpassing traditional methods in both quality and controllability. However, existing approaches still face challenges in lip-sync accuracy, temporal coherence for long video generation, and multi-character animation. In this work, we propose a diffusion transformer (DiT)-based framework for generating lifelike talking videos of arbitrary length, and introduce a training-free method for multi-character audio-driven animation. First, we employ a LoRA-based training strategy combined with a position shift inference approach, which enables efficient long video generation while preserving the capabilities of the foundation model. Moreover, we combine partial parameter updates with reward feedback to enhance both lip synchronization and natural body motion. Finally, we propose a training-free approach, Mask Classifier-Free Guidance (Mask-CFG), for multi-character animation, which requires no specialized datasets or model modifications and supports audio-driven animation for three or more characters. Experimental results demonstrate that our method outperforms existing state-of-the-art approaches, achieving high-quality, temporally coherent, and multi-character audio-driven video generation in a simple, efficient, and cost-effective manner.</li>
</ul>

<h3>Title: G4Splat: Geometry-Guided Gaussian Splatting with Generative Prior</h3>
<ul>
<li><strong>Authors: </strong>Junfeng Ni, Yixin Chen, Zhifei Yang, Yu Liu, Ruijie Lu, Song-Chun Zhu, Siyuan Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12099">https://arxiv.org/abs/2510.12099</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12099">https://arxiv.org/pdf/2510.12099</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12099]] G4Splat: Geometry-Guided Gaussian Splatting with Generative Prior(https://arxiv.org/abs/2510.12099)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Despite recent advances in leveraging generative prior from pre-trained diffusion models for 3D scene reconstruction, existing methods still face two critical limitations. First, due to the lack of reliable geometric supervision, they struggle to produce high-quality reconstructions even in observed regions, let alone in unobserved areas. Second, they lack effective mechanisms to mitigate multi-view inconsistencies in the generated images, leading to severe shape-appearance ambiguities and degraded scene geometry. In this paper, we identify accurate geometry as the fundamental prerequisite for effectively exploiting generative models to enhance 3D scene reconstruction. We first propose to leverage the prevalence of planar structures to derive accurate metric-scale depth maps, providing reliable supervision in both observed and unobserved regions. Furthermore, we incorporate this geometry guidance throughout the generative pipeline to improve visibility mask estimation, guide novel view selection, and enhance multi-view consistency when inpainting with video diffusion models, resulting in accurate and consistent scene completion. Extensive experiments on Replica, ScanNet++, and DeepBlending show that our method consistently outperforms existing baselines in both geometry and appearance reconstruction, particularly for unobserved regions. Moreover, our method naturally supports single-view inputs and unposed videos, with strong generalizability in both indoor and outdoor scenarios with practical real-world applicability. The project page is available at this https URL.</li>
</ul>

<h3>Title: Self-Supervised Selective-Guided Diffusion Model for Old-Photo Face Restoration</h3>
<ul>
<li><strong>Authors: </strong>Wenjie Li, Xiangyi Wang, Heng Guo, Guangwei Gao, Zhanyu Ma</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12114">https://arxiv.org/abs/2510.12114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12114">https://arxiv.org/pdf/2510.12114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12114]] Self-Supervised Selective-Guided Diffusion Model for Old-Photo Face Restoration(https://arxiv.org/abs/2510.12114)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, self-supervised</a></li>
<li><strong>Abstract: </strong>Old-photo face restoration poses significant challenges due to compounded degradations such as breakage, fading, and severe blur. Existing pre-trained diffusion-guided methods either rely on explicit degradation priors or global statistical guidance, which struggle with localized artifacts or face color. We propose Self-Supervised Selective-Guided Diffusion (SSDiff), which leverages pseudo-reference faces generated by a pre-trained diffusion model under weak guidance. These pseudo-labels exhibit structurally aligned contours and natural colors, enabling region-specific restoration via staged supervision: structural guidance applied throughout the denoising process and color refinement in later steps, aligned with the coarse-to-fine nature of diffusion. By incorporating face parsing maps and scratch masks, our method selectively restores breakage regions while avoiding identity mismatch. We further construct VintageFace, a 300-image benchmark of real old face photos with varying degradation levels. SSDiff outperforms existing GAN-based and diffusion-based methods in perceptual quality, fidelity, and regional controllability. Code link: this https URL.</li>
</ul>

<h3>Title: Self-Verifying Reflection Helps Transformers with CoT Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Zhongwei Yu, Wannian Xia, Xue Yan, Bo Xu, Haifeng Zhang, Yali Du, Jun Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12157">https://arxiv.org/abs/2510.12157</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12157">https://arxiv.org/pdf/2510.12157</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12157]] Self-Verifying Reflection Helps Transformers with CoT Reasoning(https://arxiv.org/abs/2510.12157)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Advanced large language models (LLMs) frequently reflect in reasoning chain-of-thoughts (CoTs), where they self-verify the correctness of current solutions and explore alternatives. However, given recent findings that LLMs detect limited errors in CoTs, how reflection contributes to empirical improvements remains unclear. To analyze this issue, in this paper, we present a minimalistic reasoning framework to support basic self-verifying reflection for small transformers without natural language, which ensures analytic clarity and reduces the cost of comprehensive experiments. Theoretically, we prove that self-verifying reflection guarantees improvements if verification errors are properly bounded. Experimentally, we show that tiny transformers, with only a few million parameters, benefit from self-verification in both training and reflective execution, reaching remarkable LLM-level performance in integer multiplication and Sudoku. Similar to LLM results, we find that reinforcement learning (RL) improves in-distribution performance and incentivizes frequent reflection for tiny transformers, yet RL mainly optimizes shallow statistical patterns without faithfully reducing verification errors. In conclusion, integrating generative transformers with discriminative verification inherently facilitates CoT reasoning, regardless of scaling and natural language.</li>
</ul>

<h3>Title: DPL: Spatial-Conditioned Diffusion Prototype Enhancement for One-Shot Medical Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Ziyuan Gao, Philippe Morel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12159">https://arxiv.org/abs/2510.12159</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12159">https://arxiv.org/pdf/2510.12159</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12159]] DPL: Spatial-Conditioned Diffusion Prototype Enhancement for One-Shot Medical Segmentation(https://arxiv.org/abs/2510.12159)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>One-shot medical image segmentation faces fundamental challenges in prototype representation due to limited annotated data and significant anatomical variability across patients. Traditional prototype-based methods rely on deterministic averaging of support features, creating brittle representations that fail to capture intra-class diversity essential for robust generalization. This work introduces Diffusion Prototype Learning (DPL), a novel framework that reformulates prototype construction through diffusion-based feature space exploration. DPL models one-shot prototypes as learnable probability distributions, enabling controlled generation of diverse yet semantically coherent prototype variants from minimal labeled data. The framework operates through three core innovations: (1) a diffusion-based prototype enhancement module that transforms single support prototypes into diverse variant sets via forward-reverse diffusion processes, (2) a spatial-aware conditioning mechanism that leverages geometric properties derived from prototype feature statistics, and (3) a conservative fusion strategy that preserves prototype fidelity while maximizing representational diversity. DPL ensures training-inference consistency by using the same diffusion enhancement and fusion pipeline in both phases. This process generates enhanced prototypes that serve as the final representations for similarity calculations, while the diffusion process itself acts as a regularizer. Extensive experiments on abdominal MRI and CT datasets demonstrate significant improvements respectively, establishing new state-of-the-art performance in one-shot medical image segmentation.</li>
</ul>

<h3>Title: The Impact of Synthetic Data on Object Detection Model Performance: A Comparative Analysis with Real-World Data</h3>
<ul>
<li><strong>Authors: </strong>Muammer Bay, Timo von Marcard, Dren Fazlija</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12208">https://arxiv.org/abs/2510.12208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12208">https://arxiv.org/pdf/2510.12208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12208]] The Impact of Synthetic Data on Object Detection Model Performance: A Comparative Analysis with Real-World Data(https://arxiv.org/abs/2510.12208)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in generative AI, particularly in computer vision (CV), offer new opportunities to optimize workflows across industries, including logistics and manufacturing. However, many AI applications are limited by a lack of expertise and resources, which forces a reliance on general-purpose models. Success with these models often requires domain-specific data for fine-tuning, which can be costly and inefficient. Thus, using synthetic data for fine-tuning is a popular, cost-effective alternative to gathering real-world data. This work investigates the impact of synthetic data on the performance of object detection models, compared to models trained on real-world data only, specifically within the domain of warehouse logistics. To this end, we examined the impact of synthetic data generated using the NVIDIA Omniverse Replicator tool on the effectiveness of object detection models in real-world scenarios. It comprises experiments focused on pallet detection in a warehouse setting, utilizing both real and various synthetic dataset generation strategies. Our findings provide valuable insights into the practical applications of synthetic image data in computer vision, suggesting that a balanced integration of synthetic and real data can lead to robust and efficient object detection models.</li>
</ul>

<h3>Title: Hierarchical Koopman Diffusion: Fast Generation with Interpretable Diffusion Trajectory</h3>
<ul>
<li><strong>Authors: </strong>Hanru Bai, Weiyang Ding, Difan Zou</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12220">https://arxiv.org/abs/2510.12220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12220">https://arxiv.org/pdf/2510.12220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12220]] Hierarchical Koopman Diffusion: Fast Generation with Interpretable Diffusion Trajectory(https://arxiv.org/abs/2510.12220)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have achieved impressive success in high-fidelity image generation but suffer from slow sampling due to their inherently iterative denoising process. While recent one-step methods accelerate inference by learning direct noise-to-image mappings, they sacrifice the interpretability and fine-grained control intrinsic to diffusion dynamics, key advantages that enable applications like editable generation. To resolve this dichotomy, we introduce \textbf{Hierarchical Koopman Diffusion}, a novel framework that achieves both one-step sampling and interpretable generative trajectories. Grounded in Koopman operator theory, our method lifts the nonlinear diffusion dynamics into a latent space where evolution is governed by globally linear operators, enabling closed-form trajectory solutions. This formulation not only eliminates iterative sampling but also provides full access to intermediate states, allowing manual intervention during generation. To model the multi-scale nature of images, we design a hierarchical architecture that disentangles generative dynamics across spatial resolutions via scale-specific Koopman subspaces, capturing coarse-to-fine details systematically. We empirically show that the Hierarchical Koopman Diffusion not only achieves competitive one-step generation performance but also provides a principled mechanism for interpreting and manipulating the generative process through spectral analysis. Our framework bridges the gap between fast sampling and interpretability in diffusion models, paving the way for explainable image synthesis in generative modeling.</li>
</ul>

<h3>Title: BIGFix: Bidirectional Image Generation with Token Fixing</h3>
<ul>
<li><strong>Authors: </strong>Victor Besnier, David Hurych, Andrei Bursuc, Eduardo Valle</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12231">https://arxiv.org/abs/2510.12231</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12231">https://arxiv.org/pdf/2510.12231</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12231]] BIGFix: Bidirectional Image Generation with Token Fixing(https://arxiv.org/abs/2510.12231)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in image and video generation have raised significant interest from both academia and industry. A key challenge in this field is improving inference efficiency, as model size and the number of inference steps directly impact the commercial viability of generative models while also posing fundamental scientific challenges. A promising direction involves combining auto-regressive sequential token modeling with multi-token prediction per step, reducing inference time by up to an order of magnitude. However, predicting multiple tokens in parallel can introduce structural inconsistencies due to token incompatibilities, as capturing complex joint dependencies during training remains challenging. Traditionally, once tokens are sampled, there is no mechanism to backtrack and refine erroneous predictions. We propose a method for self-correcting image generation by iteratively refining sampled tokens. We achieve this with a novel training scheme that injects random tokens in the context, improving robustness and enabling token fixing during sampling. Our method preserves the efficiency benefits of parallel token prediction while significantly enhancing generation quality. We evaluate our approach on image generation using the ImageNet-256 and CIFAR-10 datasets, as well as on video generation with UCF-101 and NuScenes, demonstrating substantial improvements across both modalities.</li>
</ul>

<h3>Title: Ivan-ISTD: Rethinking Cross-domain Heteroscedastic Noise Perturbations in Infrared Small Target Detection</h3>
<ul>
<li><strong>Authors: </strong>Yuehui Li, Yahao Lu, Haoyuan Wu, Sen Zhang, Liang Lin, Yukai Shi</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12241">https://arxiv.org/abs/2510.12241</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12241">https://arxiv.org/pdf/2510.12241</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12241]] Ivan-ISTD: Rethinking Cross-domain Heteroscedastic Noise Perturbations in Infrared Small Target Detection(https://arxiv.org/abs/2510.12241)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>In the multimedia domain, Infrared Small Target Detection (ISTD) plays a important role in drone-based multi-modality sensing. To address the dual challenges of cross-domain shift and heteroscedastic noise perturbations in ISTD, we propose a doubly wavelet-guided Invariance learning framework(Ivan-ISTD). In the first stage, we generate training samples aligned with the target domain using Wavelet-guided Cross-domain Synthesis. This wavelet-guided alignment machine accurately separates the target background through multi-frequency wavelet filtering. In the second stage, we introduce Real-domain Noise Invariance Learning, which extracts real noise characteristics from the target domain to build a dynamic noise library. The model learns noise invariance through self-supervised loss, thereby overcoming the limitations of distribution bias in traditional artificial noise modeling. Finally, we create the Dynamic-ISTD Benchmark, a cross-domain dynamic degradation dataset that simulates the distribution shifts encountered in real-world applications. Additionally, we validate the versatility of our method using other real-world datasets. Experimental results demonstrate that our approach outperforms existing state-of-the-art methods in terms of many quantitative metrics. In particular, Ivan-ISTD demonstrates excellent robustness in cross-domain scenarios. The code for this work can be found at: this https URL.</li>
</ul>

<h3>Title: Diffusion Models for Reinforcement Learning: Foundations, Taxonomy, and Development</h3>
<ul>
<li><strong>Authors: </strong>Changfu Xu, Jianxiong Guo, Yuzhu Liang, Haiyang Huang, Haodong Zou, Xi Zheng, Shui Yu, Xiaowen Chu, Jiannong Cao, Tian Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12253">https://arxiv.org/abs/2510.12253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12253">https://arxiv.org/pdf/2510.12253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12253]] Diffusion Models for Reinforcement Learning: Foundations, Taxonomy, and Development(https://arxiv.org/abs/2510.12253)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion Models (DMs), as a leading class of generative models, offer key advantages for reinforcement learning (RL), including multi-modal expressiveness, stable training, and trajectory-level planning. This survey delivers a comprehensive and up-to-date synthesis of diffusion-based RL. We first provide an overview of RL, highlighting its challenges, and then introduce the fundamental concepts of DMs, investigating how they are integrated into RL frameworks to address key challenges in this research field. We establish a dual-axis taxonomy that organizes the field along two orthogonal dimensions: a function-oriented taxonomy that clarifies the roles DMs play within the RL pipeline, and a technique-oriented taxonomy that situates implementations across online versus offline learning regimes. We also provide a comprehensive examination of this progression from single-agent to multi-agent domains, thereby forming several frameworks for DM-RL integration and highlighting their practical utility. Furthermore, we outline several categories of successful applications of diffusion-based RL across diverse domains, discuss open research issues of current methodologies, and highlight key directions for future research to advance the field. Finally, we summarize the survey to identify promising future development directions. We are actively maintaining a GitHub repository (this https URL) for papers and other related resources to apply DMs for RL.</li>
</ul>

<h3>Title: Hybrid Gaussian Splatting for Novel Urban View Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Mohamed Omran, Farhad Zanjani, Davide Abati, Jens Petersen, Amirhossein Habibian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12308">https://arxiv.org/abs/2510.12308</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12308">https://arxiv.org/pdf/2510.12308</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12308]] Hybrid Gaussian Splatting for Novel Urban View Synthesis(https://arxiv.org/abs/2510.12308)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>This paper describes the Qualcomm AI Research solution to the RealADSim-NVS challenge, hosted at the RealADSim Workshop at ICCV 2025. The challenge concerns novel view synthesis in street scenes, and participants are required to generate, starting from car-centric frames captured during some training traversals, renders of the same urban environment as viewed from a different traversal (e.g. different street lane or car direction). Our solution is inspired by hybrid methods in scene generation and generative simulators merging gaussian splatting and diffusion models, and it is composed of two stages: First, we fit a 3D reconstruction of the scene and render novel views as seen from the target cameras. Then, we enhance the resulting frames with a dedicated single-step diffusion model. We discuss specific choices made in the initialization of gaussian primitives as well as the finetuning of the enhancer model and its training data curation. We report the performance of our model design and we ablate its components in terms of novel view quality as measured by PSNR, SSIM and LPIPS. On the public leaderboard reporting test results, our proposal reaches an aggregated score of 0.432, achieving the second place overall.</li>
</ul>

<h3>Title: MoBiLE: Efficient Mixture-of-Experts Inference on Consumer GPU with Mixture of Big Little Experts</h3>
<ul>
<li><strong>Authors: </strong>Yushu Zhao, Yubin Qin, Yang Wang, Xiaolong Yang, Huiming Han, Shaojun Wei, Yang Hu, Shouyi Yin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12357">https://arxiv.org/abs/2510.12357</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12357">https://arxiv.org/pdf/2510.12357</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12357]] MoBiLE: Efficient Mixture-of-Experts Inference on Consumer GPU with Mixture of Big Little Experts(https://arxiv.org/abs/2510.12357)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Mixture-of-Experts (MoE) models have recently demonstrated exceptional performance across a diverse range of applications. The principle of sparse activation in MoE models facilitates an offloading strategy, wherein active experts are maintained in GPU HBM, while inactive experts are stored in CPU DRAM. The efficacy of this approach, however, is fundamentally constrained by the limited bandwidth of the CPU-GPU interconnect. To mitigate this bottleneck, existing approaches have employed prefetching to accelerate MoE inference. These methods attempt to predict and prefetch the required experts using specially trained modules. Nevertheless, such techniques are often encumbered by significant training overhead and have shown diminished effectiveness on recent MoE models with fine-grained expert segmentation. In this paper, we propose MoBiLE, a plug-and-play offloading-based MoE inference framework with \textit{mixture of big-little experts}. It reduces the number of experts for unimportant tokens to half for acceleration while maintaining full experts for important tokens to guarantee model quality. Further, a dedicated fallback and prefetching mechanism is designed for switching between little and big experts to improve memory efficiency. We evaluate MoBiLE on four typical modern MoE architectures and challenging generative tasks. Our results show that MoBiLE achieves a speedup of 1.60x to 1.72x compared to the baseline on a consumer GPU system, with negligible degradation in accuracy.</li>
</ul>

<h3>Title: Scene Coordinate Reconstruction Priors</h3>
<ul>
<li><strong>Authors: </strong>Wenjing Bian, Axel Barroso-Laguna, Tommaso Cavallari, Victor Adrian Prisacariu, Eric Brachmann</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12387">https://arxiv.org/abs/2510.12387</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12387">https://arxiv.org/pdf/2510.12387</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12387]] Scene Coordinate Reconstruction Priors(https://arxiv.org/abs/2510.12387)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Scene coordinate regression (SCR) models have proven to be powerful implicit scene representations for 3D vision, enabling visual relocalization and structure-from-motion. SCR models are trained specifically for one scene. If training images imply insufficient multi-view constraints SCR models degenerate. We present a probabilistic reinterpretation of training SCR models, which allows us to infuse high-level reconstruction priors. We investigate multiple such priors, ranging from simple priors over the distribution of reconstructed depth values to learned priors over plausible scene coordinate configurations. For the latter, we train a 3D point cloud diffusion model on a large corpus of indoor scans. Our priors push predicted 3D scene points towards plausible geometry at each training step to increase their likelihood. On three indoor datasets our priors help learning better scene representations, resulting in more coherent scene point clouds, higher registration rates and better camera poses, with a positive effect on down-stream tasks such as novel view synthesis and camera relocalization.</li>
</ul>

<h3>Title: Enhanced Pre-training of Graph Neural Networks for Million-Scale Heterogeneous Graphs</h3>
<ul>
<li><strong>Authors: </strong>Shengyin Sun, Chen Ma, Jiehao Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12401">https://arxiv.org/abs/2510.12401</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12401">https://arxiv.org/pdf/2510.12401</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12401]] Enhanced Pre-training of Graph Neural Networks for Million-Scale Heterogeneous Graphs(https://arxiv.org/abs/2510.12401)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>In recent years, graph neural networks (GNNs) have facilitated the development of graph data mining. However, training GNNs requires sufficient labeled task-specific data, which is expensive and sometimes unavailable. To be less dependent on labeled data, recent studies propose to pre-train GNNs in a self-supervised manner and then apply the pre-trained GNNs to downstream tasks with limited labeled data. However, most existing methods are designed solely for homogeneous graphs (real-world graphs are mostly heterogeneous) and do not consider semantic mismatch (the semantic difference between the original data and the ideal data containing more transferable semantic information). In this paper, we propose an effective framework to pre-train GNNs on the large-scale heterogeneous graph. We first design a structure-aware pre-training task, which aims to capture structural properties in heterogeneous graphs. Then, we design a semantic-aware pre-training task to tackle the mismatch. Specifically, we construct a perturbation subspace composed of semantic neighbors to help deal with the semantic mismatch. Semantic neighbors make the model focus more on the general knowledge in the semantic space, which in turn assists the model in learning knowledge with better transferability. Finally, extensive experiments are conducted on real-world large-scale heterogeneous graphs to demonstrate the superiority of the proposed method over state-of-the-art baselines. Code available at this https URL.</li>
</ul>

<h3>Title: Continuous Uniqueness and Novelty Metrics for Generative Modeling of Inorganic Crystals</h3>
<ul>
<li><strong>Authors: </strong>Masahiro Negishi, Hyunsoo Park, Kinga O. Mastej, Aron Walsh</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12405">https://arxiv.org/abs/2510.12405</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12405">https://arxiv.org/pdf/2510.12405</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12405]] Continuous Uniqueness and Novelty Metrics for Generative Modeling of Inorganic Crystals(https://arxiv.org/abs/2510.12405)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>To address pressing scientific challenges such as climate change, increasingly sophisticated generative artificial intelligence models are being developed that can efficiently sample the large chemical space of possible functional materials. These models can quickly sample new chemical compositions paired with crystal structures. They are typically evaluated using uniqueness and novelty metrics, which depend on a chosen crystal distance function. However, the most prevalent distance function has four limitations: it fails to quantify the degree of similarity between compounds, cannot distinguish compositional difference and structural difference, lacks Lipschitz continuity against shifts in atomic coordinates, and results in a uniqueness metric that is not invariant against the permutation of generated samples. In this work, we propose using two continuous distance functions to evaluate uniqueness and novelty, which theoretically overcome these limitations. Our experiments show that these distances reveal insights missed by traditional distance functions, providing a more reliable basis for evaluating and comparing generative models for inorganic crystals.</li>
</ul>

<h3>Title: Low-Field Magnetic Resonance Image Quality Enhancement using a Conditional Flow Matching Model</h3>
<ul>
<li><strong>Authors: </strong>Huu Tien Nguyen, Ahmed Karam Eldaly</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12408">https://arxiv.org/abs/2510.12408</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12408">https://arxiv.org/pdf/2510.12408</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12408]] Low-Field Magnetic Resonance Image Quality Enhancement using a Conditional Flow Matching Model(https://arxiv.org/abs/2510.12408)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper introduces a novel framework for image quality transfer based on conditional flow matching (CFM). Unlike conventional generative models that rely on iterative sampling or adversarial objectives, CFM learns a continuous flow between a noise distribution and target data distributions through the direct regression of an optimal velocity field. We evaluate this approach in the context of low-field magnetic resonance imaging (LF-MRI), a rapidly emerging modality that offers affordable and portable scanning but suffers from inherently low signal-to-noise ratio and reduced diagnostic quality. Our framework is designed to reconstruct high-field-like MR images from their corresponding low-field inputs, thereby bridging the quality gap without requiring expensive infrastructure. Experiments demonstrate that CFM not only achieves state-of-the-art performance, but also generalizes robustly to both in-distribution and out-of-distribution data. Importantly, it does so while utilizing significantly fewer parameters than competing deep learning methods. These results underline the potential of CFM as a powerful and scalable tool for MRI reconstruction, particularly in resource-limited clinical environments.</li>
</ul>

<h3>Title: Targeted Pooled Latent-Space Steganalysis Applied to Generative Steganography, with a Fix</h3>
<ul>
<li><strong>Authors: </strong>Etienne Levecque (LIST3N), Aurélien Noirault (CRIStAL), Tomáš Pevný (CTU), Jan Butora (CRIStAL), Patrick Bas (CRIStAL), Rémi Cogranne (LIST3N)</a></li>
<li><strong>Subjects: </strong>cs.CR, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12414">https://arxiv.org/abs/2510.12414</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12414">https://arxiv.org/pdf/2510.12414</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12414]] Targeted Pooled Latent-Space Steganalysis Applied to Generative Steganography, with a Fix(https://arxiv.org/abs/2510.12414)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Steganographic schemes dedicated to generated images modify the seed vector in the latent space to embed a message, whereas most steganalysis methods attempt to detect the embedding in the image space. This paper proposes to perform steganalysis in the latent space by modeling the statistical distribution of the norm of the latent vector. Specifically, we analyze the practical security of a scheme proposed by Hu et. al. for latent diffusion models, which is both robust and practically undetectable when steganalysis is performed on generated images. We show that after embedding, the Stego (latent) vector is distributed on a hypersphere while the Cover vector is i.i.d. Gaussian. By going from the image space to the latent space, we show that it is possible to model the norm of the vector in the latent space under the Cover or Stego hypothesis as Gaussian distributions with different variances. A Likelihood Ratio Test is then derived to perform pooled steganalysis. The impact of the potential knowledge of the prompt and the number of diffusion steps, is also studied. Additionally, we also show how, by randomly sampling the norm of the latent vector before generation, the initial Stego scheme becomes undetectable in the latent space.</li>
</ul>

<h3>Title: Time-Correlated Video Bridge Matching</h3>
<ul>
<li><strong>Authors: </strong>Viacheslav Vasilev, Arseny Ivanov, Nikita Gushchin, Maria Kovaleva, Alexander Korotin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12453">https://arxiv.org/abs/2510.12453</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12453">https://arxiv.org/pdf/2510.12453</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12453]] Time-Correlated Video Bridge Matching(https://arxiv.org/abs/2510.12453)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models excel in noise-to-data generation tasks, providing a mapping from a Gaussian distribution to a more complex data distribution. However they struggle to model translations between complex distributions, limiting their effectiveness in data-to-data tasks. While Bridge Matching (BM) models address this by finding the translation between data distributions, their application to time-correlated data sequences remains unexplored. This is a critical limitation for video generation and manipulation tasks, where maintaining temporal coherence is particularly important. To address this gap, we propose Time-Correlated Video Bridge Matching (TCVBM), a framework that extends BM to time-correlated data sequences in the video domain. TCVBM explicitly models inter-sequence dependencies within the diffusion bridge, directly incorporating temporal correlations into the sampling process. We compare our approach to classical methods based on bridge matching and diffusion models for three video-related tasks: frame interpolation, image-to-video generation, and video super-resolution. TCVBM achieves superior performance across multiple quantitative metrics, demonstrating enhanced generation quality and reconstruction fidelity.</li>
</ul>

<h3>Title: Attack-Specialized Deep Learning with Ensemble Fusion for Network Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Nisith Dissanayake (1), Uthayasanker Thayasivam (1) ((1) University of Moratuwa)</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12455">https://arxiv.org/abs/2510.12455</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12455">https://arxiv.org/pdf/2510.12455</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12455]] Attack-Specialized Deep Learning with Ensemble Fusion for Network Anomaly Detection(https://arxiv.org/abs/2510.12455)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>The growing scale and sophistication of cyberattacks pose critical challenges to network security, particularly in detecting diverse intrusion types within imbalanced datasets. Traditional intrusion detection systems (IDS) often struggle to maintain high accuracy across both frequent and rare attacks, leading to increased false negatives for minority classes. To address this, we propose a hybrid anomaly detection framework that integrates specialized deep learning models with an ensemble meta-classifier. Each model is trained to detect a specific attack category, enabling tailored learning of class-specific patterns, while their collective outputs are fused by a Random Forest meta-classifier to improve overall decision reliability. The framework is evaluated on the NSL-KDD benchmark, demonstrating superior performance in handling class imbalance compared to conventional monolithic models. Results show significant improvements in precision, recall, and F1-score across all attack categories, including rare classes such as User to Root (U2R). The proposed system achieves near-perfect detection rates with minimal false alarms, highlighting its robustness and generalizability. This work advances the design of intrusion detection systems by combining specialization with ensemble learning, providing an effective and scalable solution for safeguarding modern networks.</li>
</ul>

<h3>Title: CrossAD: Time Series Anomaly Detection with Cross-scale Associations and Cross-window Modeling</h3>
<ul>
<li><strong>Authors: </strong>Beibu Li, Qichao Shentu, Yang Shu, Hui Zhang, Ming Li, Ning Jin, Bin Yang, Chenjuan Guo</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12489">https://arxiv.org/abs/2510.12489</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12489">https://arxiv.org/pdf/2510.12489</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12489]] CrossAD: Time Series Anomaly Detection with Cross-scale Associations and Cross-window Modeling(https://arxiv.org/abs/2510.12489)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Time series anomaly detection plays a crucial role in a wide range of real-world applications. Given that time series data can exhibit different patterns at different sampling granularities, multi-scale modeling has proven beneficial for uncovering latent anomaly patterns that may not be apparent at a single scale. However, existing methods often model multi-scale information independently or rely on simple feature fusion strategies, neglecting the dynamic changes in cross-scale associations that occur during anomalies. Moreover, most approaches perform multi-scale modeling based on fixed sliding windows, which limits their ability to capture comprehensive contextual information. In this work, we propose CrossAD, a novel framework for time series Anomaly Detection that takes Cross-scale associations and Cross-window modeling into account. We propose a cross-scale reconstruction that reconstructs fine-grained series from coarser series, explicitly capturing cross-scale associations. Furthermore, we design a query library and incorporate global multi-scale context to overcome the limitations imposed by fixed window sizes. Extensive experiments conducted on multiple real-world datasets using nine evaluation metrics validate the effectiveness of CrossAD, demonstrating state-of-the-art performance in anomaly detection.</li>
</ul>

<h3>Title: Mitigating the Noise Shift for Denoising Generative Models via Noise Awareness Guidance</h3>
<ul>
<li><strong>Authors: </strong>Jincheng Zhong, Boyuan Jiang, Xin Tao, Pengfei Wan, Kun Gai, Mingsheng Long</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12497">https://arxiv.org/abs/2510.12497</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12497">https://arxiv.org/pdf/2510.12497</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12497]] Mitigating the Noise Shift for Denoising Generative Models via Noise Awareness Guidance(https://arxiv.org/abs/2510.12497)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Existing denoising generative models rely on solving discretized reverse-time SDEs or ODEs. In this paper, we identify a long-overlooked yet pervasive issue in this family of models: a misalignment between the pre-defined noise level and the actual noise level encoded in intermediate states during sampling. We refer to this misalignment as noise shift. Through empirical analysis, we demonstrate that noise shift is widespread in modern diffusion models and exhibits a systematic bias, leading to sub-optimal generation due to both out-of-distribution generalization and inaccurate denoising updates. To address this problem, we propose Noise Awareness Guidance (NAG), a simple yet effective correction method that explicitly steers sampling trajectories to remain consistent with the pre-defined noise schedule. We further introduce a classifier-free variant of NAG, which jointly trains a noise-conditional and a noise-unconditional model via noise-condition dropout, thereby eliminating the need for external classifiers. Extensive experiments, including ImageNet generation and various supervised fine-tuning tasks, show that NAG consistently mitigates noise shift and substantially improves the generation quality of mainstream diffusion models.</li>
</ul>

<h3>Title: Voronoi-Assisted Diffusion for Computing Unsigned Distance Fields from Unoriented Points</h3>
<ul>
<li><strong>Authors: </strong>Jiayi Kong, Chen Zong, Junkai Deng, Xuhui Chen, Fei Hou, Shiqing Xin, Junhui Hou, Chen Qian, Ying He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12524">https://arxiv.org/abs/2510.12524</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12524">https://arxiv.org/pdf/2510.12524</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12524]] Voronoi-Assisted Diffusion for Computing Unsigned Distance Fields from Unoriented Points(https://arxiv.org/abs/2510.12524)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Unsigned Distance Fields (UDFs) provide a flexible representation for 3D shapes with arbitrary topology, including open and closed surfaces, orientable and non-orientable geometries, and non-manifold structures. While recent neural approaches have shown promise in learning UDFs, they often suffer from numerical instability, high computational cost, and limited controllability. We present a lightweight, network-free method, Voronoi-Assisted Diffusion (VAD), for computing UDFs directly from unoriented point clouds. Our approach begins by assigning bi-directional normals to input points, guided by two Voronoi-based geometric criteria encoded in an energy function for optimal alignment. The aligned normals are then diffused to form an approximate UDF gradient field, which is subsequently integrated to recover the final UDF. Experiments demonstrate that VAD robustly handles watertight and open surfaces, as well as complex non-manifold and non-orientable geometries, while remaining computationally efficient and stable.</li>
</ul>

<h3>Title: Unconditional Human Motion and Shape Generation via Balanced Score-Based Diffusion</h3>
<ul>
<li><strong>Authors: </strong>David Björkstrand, Tiesheng Wang, Lars Bretzner, Josephine Sullivan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12537">https://arxiv.org/abs/2510.12537</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12537">https://arxiv.org/pdf/2510.12537</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12537]] Unconditional Human Motion and Shape Generation via Balanced Score-Based Diffusion(https://arxiv.org/abs/2510.12537)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent work has explored a range of model families for human motion generation, including Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and diffusion-based models. Despite their differences, many methods rely on over-parameterized input features and auxiliary losses to improve empirical results. These strategies should not be strictly necessary for diffusion models to match the human motion distribution. We show that on par with state-of-the-art results in unconditional human motion generation are achievable with a score-based diffusion model using only careful feature-space normalization and analytically derived weightings for the standard L2 score-matching loss, while generating both motion and shape directly, thereby avoiding slow post hoc shape recovery from joints. We build the method step by step, with a clear theoretical motivation for each component, and provide targeted ablations demonstrating the effectiveness of each proposed addition in isolation.</li>
</ul>

<h3>Title: Unlocking Zero-Shot Plant Segmentation with Pl@ntNet Intelligence</h3>
<ul>
<li><strong>Authors: </strong>Simon Ravé, Jean-Christophe Lombardo, Pejman Rasti, Alexis Joly, David Rousseau</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12579">https://arxiv.org/abs/2510.12579</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12579">https://arxiv.org/pdf/2510.12579</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12579]] Unlocking Zero-Shot Plant Segmentation with Pl@ntNet Intelligence(https://arxiv.org/abs/2510.12579)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>We present a zero-shot segmentation approach for agricultural imagery that leverages Plantnet, a large-scale plant classification model, in conjunction with its DinoV2 backbone and the Segment Anything Model (SAM). Rather than collecting and annotating new datasets, our method exploits Plantnet's specialized plant representations to identify plant regions and produce coarse segmentation masks. These masks are then refined by SAM to yield detailed segmentations. We evaluate on four publicly available datasets of various complexity in terms of contrast including some where the limited size of the training data and complex field conditions often hinder purely supervised methods. Our results show consistent performance gains when using Plantnet-fine-tuned DinoV2 over the base DinoV2 model, as measured by the Jaccard Index (IoU). These findings highlight the potential of combining foundation models with specialized plant-centric models to alleviate the annotation bottleneck and enable effective segmentation in diverse agricultural scenarios.</li>
</ul>

<h3>Title: LayerSync: Self-aligning Intermediate Layers</h3>
<ul>
<li><strong>Authors: </strong>Yasaman Haghighi, Bastien van Delft, Mariam Hassan, Alexandre Alahi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12581">https://arxiv.org/abs/2510.12581</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12581">https://arxiv.org/pdf/2510.12581</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12581]] LayerSync: Self-aligning Intermediate Layers(https://arxiv.org/abs/2510.12581)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We propose LayerSync, a domain-agnostic approach for improving the generation quality and the training efficiency of diffusion models. Prior studies have highlighted the connection between the quality of generation and the representations learned by diffusion models, showing that external guidance on model intermediate representations accelerates training. We reconceptualize this paradigm by regularizing diffusion models with their own intermediate representations. Building on the observation that representation quality varies across diffusion model layers, we show that the most semantically rich representations can act as an intrinsic guidance for weaker ones, reducing the need for external supervision. Our approach, LayerSync, is a self-sufficient, plug-and-play regularizer term with no overhead on diffusion model training and generalizes beyond the visual domain to other modalities. LayerSync requires no pretrained models nor additional data. We extensively evaluate the method on image generation and demonstrate its applicability to other domains such as audio, video, and motion generation. We show that it consistently improves the generation quality and the training efficiency. For example, we speed up the training of flow-based transformer by over 8.75x on ImageNet dataset and improved the generation quality by 23.6%. The code is available at this https URL.</li>
</ul>

<h3>Title: Advancing End-to-End Pixel Space Generative Modeling via Self-supervised Pre-training</h3>
<ul>
<li><strong>Authors: </strong>Jiachen Lei, Keli Liu, Julius Berner, Haiming Yu, Hongkai Zheng, Jiahong Wu, Xiangxiang Chu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12586">https://arxiv.org/abs/2510.12586</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12586">https://arxiv.org/pdf/2510.12586</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12586]] Advancing End-to-End Pixel Space Generative Modeling via Self-supervised Pre-training(https://arxiv.org/abs/2510.12586)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, self-supervised, generative</a></li>
<li><strong>Abstract: </strong>Pixel-space generative models are often more difficult to train and generally underperform compared to their latent-space counterparts, leaving a persistent performance and efficiency gap. In this paper, we introduce a novel two-stage training framework that closes this gap for pixel-space diffusion and consistency models. In the first stage, we pre-train encoders to capture meaningful semantics from clean images while aligning them with points along the same deterministic sampling trajectory, which evolves points from the prior to the data distribution. In the second stage, we integrate the encoder with a randomly initialized decoder and fine-tune the complete model end-to-end for both diffusion and consistency models. Our training framework demonstrates strong empirical performance on ImageNet dataset. Specifically, our diffusion model reaches an FID of 2.04 on ImageNet-256 and 2.35 on ImageNet-512 with 75 number of function evaluations (NFE), surpassing prior pixel-space methods by a large margin in both generation quality and efficiency while rivaling leading VAE-based models at comparable training cost. Furthermore, on ImageNet-256, our consistency model achieves an impressive FID of 8.82 in a single sampling step, significantly surpassing its latent-space counterpart. To the best of our knowledge, this marks the first successful training of a consistency model directly on high-resolution images without relying on pre-trained VAEs or diffusion models.</li>
</ul>

<h3>Title: Towards Fast Coarse-graining and Equation Discovery with Foundation Inference Models</h3>
<ul>
<li><strong>Authors: </strong>Manuel Hinz, Maximilian Mauel, Patrick Seifner, David Berghaus, Kostadin Cvejoski, Ramses J. Sanchez</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12618">https://arxiv.org/abs/2510.12618</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12618">https://arxiv.org/pdf/2510.12618</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12618]] Towards Fast Coarse-graining and Equation Discovery with Foundation Inference Models(https://arxiv.org/abs/2510.12618)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>High-dimensional recordings of dynamical processes are often characterized by a much smaller set of effective variables, evolving on low-dimensional manifolds. Identifying these latent dynamics requires solving two intertwined problems: discovering appropriate coarse-grained variables and simultaneously fitting the governing equations. Most machine learning approaches tackle these tasks jointly by training autoencoders together with models that enforce dynamical consistency. We propose to decouple the two problems by leveraging the recently introduced Foundation Inference Models (FIMs). FIMs are pretrained models that estimate the infinitesimal generators of dynamical systems (e.g., the drift and diffusion of a stochastic differential equation) in zero-shot mode. By amortizing the inference of the dynamics through a FIM with frozen weights, and training only the encoder-decoder map, we define a simple, simulation-consistent loss that stabilizes representation learning. A proof of concept on a stochastic double-well system with semicircle diffusion, embedded into synthetic video data, illustrates the potential of this approach for fast and reusable coarse-graining pipelines.</li>
</ul>

<h3>Title: Learning-To-Measure: In-context Active Feature Acquisition</h3>
<ul>
<li><strong>Authors: </strong>Yuta Kobayashi, Zilin Jing, Jiayu Yao, Hongseok Namkoong, Shalmali Joshi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12624">https://arxiv.org/abs/2510.12624</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12624">https://arxiv.org/pdf/2510.12624</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12624]] Learning-To-Measure: In-context Active Feature Acquisition(https://arxiv.org/abs/2510.12624)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Active feature acquisition (AFA) is a sequential decision-making problem where the goal is to improve model performance for test instances by adaptively selecting which features to acquire. In practice, AFA methods often learn from retrospective data with systematic missingness in the features and limited task-specific labels. Most prior work addresses acquisition for a single predetermined task, limiting scalability. To address this limitation, we formalize the meta-AFA problem, where the goal is to learn acquisition policies across various tasks. We introduce Learning-to-Measure (L2M), which consists of i) reliable uncertainty quantification over unseen tasks, and ii) an uncertainty-guided greedy feature acquisition agent that maximizes conditional mutual information. We demonstrate a sequence-modeling or autoregressive pre-training approach that underpins reliable uncertainty quantification for tasks with arbitrary missingness. L2M operates directly on datasets with retrospective missingness and performs the meta-AFA task in-context, eliminating per-task retraining. Across synthetic and real-world tabular benchmarks, L2M matches or surpasses task-specific baselines, particularly under scarce labels and high missingness.</li>
</ul>

<h3>Title: On Foundation Models for Temporal Point Processes to Accelerate Scientific Discovery</h3>
<ul>
<li><strong>Authors: </strong>David Berghaus, Patrick Seifner, Kostadin Cvejoski, Ramses J. Sanchez</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12640">https://arxiv.org/abs/2510.12640</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12640">https://arxiv.org/pdf/2510.12640</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12640]] On Foundation Models for Temporal Point Processes to Accelerate Scientific Discovery(https://arxiv.org/abs/2510.12640)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Many scientific fields, from medicine to seismology, rely on analyzing sequences of events over time to understand complex systems. Traditionally, machine learning models must be built and trained from scratch for each new dataset, which is a slow and costly process. We introduce a new approach: a single, powerful model that learns the underlying patterns of event data in context. We trained this "foundation model" on millions of simulated event sequences, teaching it a general-purpose understanding of how events can unfold. As a result, our model can analyze new scientific data instantly, without retraining, simply by looking at a few examples from the dataset. It can also be quickly fine-tuned for even higher accuracy. This approach makes sophisticated event analysis more accessible and accelerates the pace of scientific discovery.</li>
</ul>

<h3>Title: Towards Foundation Inference Models that Learn ODEs In-Context</h3>
<ul>
<li><strong>Authors: </strong>Maximilian Mauel, Manuel Hinz, Patrick Seifner, David Berghaus, Ramses J. Sanchez</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12650">https://arxiv.org/abs/2510.12650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12650">https://arxiv.org/pdf/2510.12650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12650]] Towards Foundation Inference Models that Learn ODEs In-Context(https://arxiv.org/abs/2510.12650)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Ordinary differential equations (ODEs) describe dynamical systems evolving deterministically in continuous time. Accurate data-driven modeling of systems as ODEs, a central problem across the natural sciences, remains challenging, especially if the data is sparse or noisy. We introduce FIM-ODE (Foundation Inference Model for ODEs), a pretrained neural model designed to estimate ODEs zero-shot (i.e., in context) from sparse and noisy observations. Trained on synthetic data, the model utilizes a flexible neural operator for robust ODE inference, even from corrupted data. We empirically verify that FIM-ODE provides accurate estimates, on par with a neural state-of-the-art method, and qualitatively compare the structure of their estimated vector fields.</li>
</ul>

<h3>Title: On the Use of Hierarchical Vision Foundation Models for Low-Cost Human Mesh Recovery and Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Shuhei Tarashima, Yushan Wang, Norio Tagawa</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12660">https://arxiv.org/abs/2510.12660</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12660">https://arxiv.org/pdf/2510.12660</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12660]] On the Use of Hierarchical Vision Foundation Models for Low-Cost Human Mesh Recovery and Pose Estimation(https://arxiv.org/abs/2510.12660)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>In this work, we aim to develop simple and efficient models for human mesh recovery (HMR) and its predecessor task, human pose estimation (HPE). State-of-the-art HMR methods, such as HMR2.0 and its successors, rely on large, non-hierarchical vision transformers as encoders, which are inherited from the corresponding HPE models like ViTPose. To establish baselines across varying computational budgets, we first construct three lightweight HMR2.0 variants by adapting the corresponding ViTPose models. In addition, we propose leveraging the early stages of hierarchical vision foundation models (VFMs), including Swin Transformer, GroupMixFormer, and VMamba, as encoders. This design is motivated by the observation that intermediate stages of hierarchical VFMs produce feature maps with resolutions comparable to or higher than those of non-hierarchical counterparts. We conduct a comprehensive evaluation of 27 hierarchical-VFM-based HMR and HPE models, demonstrating that using only the first two or three stages achieves performance on par with full-stage models. Moreover, we show that the resulting truncated models exhibit better trade-offs between accuracy and computational efficiency compared to existing lightweight alternatives.</li>
</ul>

<h3>Title: CoRA: Covariate-Aware Adaptation of Time Series Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Guo Qin, Zhi Chen, Yong Liu, Zhiyuan Shi, Haixuan Liu, Xiangdong Huang, Jianmin Wang, Mingsheng Long</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12681">https://arxiv.org/abs/2510.12681</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12681">https://arxiv.org/pdf/2510.12681</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12681]] CoRA: Covariate-Aware Adaptation of Time Series Foundation Models(https://arxiv.org/abs/2510.12681)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Time Series Foundation Models (TSFMs) have shown significant impact through their model capacity, scalability, and zero-shot generalization. However, due to the heterogeneity of inter-variate dependencies and the backbone scalability on large-scale multivariate datasets, most TSFMs are typically pre-trained on univariate time series. This limitation renders them oblivious to crucial information from diverse covariates in real-world forecasting tasks. To further enhance the performance of TSFMs, we propose a general covariate-aware adaptation (CoRA) framework for TSFMs. It leverages pre-trained backbones of foundation models while effectively incorporating exogenous covariates from various modalities, including time series, language, and images, to improve the quality of predictions. Technically, CoRA maintains the equivalence of initialization and parameter consistency during adaptation. With preserved backbones of foundation models as frozen feature extractors, the outcome embeddings from foundation models are empirically demonstrated more informative than raw data. Further, CoRA employs a novel Granger Causality Embedding (GCE) to automatically evaluate covariates regarding their causal predictability with respect to the target variate. We incorporate these weighted embeddings with a zero-initialized condition-injection mechanism, avoiding catastrophic forgetting of pre-trained foundation models and gradually integrates exogenous information. Extensive experiments show that CoRA of TSFMs surpasses state-of-the-art covariate-aware deep forecasters with full or few-shot training samples, achieving 31.1% MSE reduction on covariate-aware forecasting. Compared to other adaptation methods, CoRA exhibits strong compatibility with various advanced TSFMs and extends the scope of covariates to other modalities, presenting a practical paradigm for the application of TSFMs.</li>
</ul>

<h3>Title: DiffEM: Learning from Corrupted Data with Diffusion Models via Expectation Maximization</h3>
<ul>
<li><strong>Authors: </strong>Danial Hosseintabar, Fan Chen, Giannis Daras, Antonio Torralba, Constantinos Daskalakis</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12691">https://arxiv.org/abs/2510.12691</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12691">https://arxiv.org/pdf/2510.12691</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12691]] DiffEM: Learning from Corrupted Data with Diffusion Models via Expectation Maximization(https://arxiv.org/abs/2510.12691)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged as powerful generative priors for high-dimensional inverse problems, yet learning them when only corrupted or noisy observations are available remains challenging. In this work, we propose a new method for training diffusion models with Expectation-Maximization (EM) from corrupted data. Our proposed method, DiffEM, utilizes conditional diffusion models to reconstruct clean data from observations in the E-step, and then uses the reconstructed data to refine the conditional diffusion model in the M-step. Theoretically, we provide monotonic convergence guarantees for the DiffEM iteration, assuming appropriate statistical conditions. We demonstrate the effectiveness of our approach through experiments on various image reconstruction tasks.</li>
</ul>

<h3>Title: Hybrid Explanation-Guided Learning for Transformer-Based Chest X-Ray Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Shelley Zixin Shu, Haozhe Luo, Alexander Poellinger, Mauricio Reyes</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12704">https://arxiv.org/abs/2510.12704</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12704">https://arxiv.org/pdf/2510.12704</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12704]] Hybrid Explanation-Guided Learning for Transformer-Based Chest X-Ray Diagnosis(https://arxiv.org/abs/2510.12704)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Transformer-based deep learning models have demonstrated exceptional performance in medical imaging by leveraging attention mechanisms for feature representation and interpretability. However, these models are prone to learning spurious correlations, leading to biases and limited generalization. While human-AI attention alignment can mitigate these issues, it often depends on costly manual supervision. In this work, we propose a Hybrid Explanation-Guided Learning (H-EGL) framework that combines self-supervised and human-guided constraints to enhance attention alignment and improve generalization. The self-supervised component of H-EGL leverages class-distinctive attention without relying on restrictive priors, promoting robustness and flexibility. We validate our approach on chest X-ray classification using the Vision Transformer (ViT), where H-EGL outperforms two state-of-the-art Explanation-Guided Learning (EGL) methods, demonstrating superior classification accuracy and generalization capability. Additionally, it produces attention maps that are better aligned with human expertise.</li>
</ul>

<h3>Title: Multitask finetuning and acceleration of chemical pretrained models for small molecule drug property prediction</h3>
<ul>
<li><strong>Authors: </strong>Matthew Adrian, Yunsie Chung, Kevin Boyd, Saee Paliwal, Srimukh Prasad Veccham, Alan C. Cheng</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12719">https://arxiv.org/abs/2510.12719</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12719">https://arxiv.org/pdf/2510.12719</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12719]] Multitask finetuning and acceleration of chemical pretrained models for small molecule drug property prediction(https://arxiv.org/abs/2510.12719)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>Chemical pretrained models, sometimes referred to as foundation models, are receiving considerable interest for drug discovery applications. The general chemical knowledge extracted from self-supervised training has the potential to improve predictions for critical drug discovery endpoints, including on-target potency and ADMET properties. Multi-task learning has previously been successfully leveraged to improve predictive models. Here, we show that enabling multitasking in finetuning of chemical pretrained graph neural network models such as Kinetic GROVER Multi-Task (KERMT), an enhanced version of the GROVER model, and Knowledge-guided Pre-training of Graph Transformer (KGPT) significantly improves performance over non-pretrained graph neural network models. Surprisingly, we find that the performance improvement from finetuning KERMT in a multitask manner is most significant at larger data sizes. Additionally, we publish two multitask ADMET data splits to enable more accurate benchmarking of multitask deep learning methods for drug property prediction. Finally, we provide an accelerated implementation of the KERMT model on GitHub, unlocking large-scale pretraining, finetuning, and inference in industrial drug discovery workflows.</li>
</ul>

<h3>Title: CARVQ: Corrective Adaptor with Group Residual Vector Quantization for LLM Embedding Compression</h3>
<ul>
<li><strong>Authors: </strong>Dayin Gou, Sanghyun Byun, Nilesh Malpeddi, Gabrielle De Micheli, Prathamesh Vaste, Jacob Song, Woo Seong Chung</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12721">https://arxiv.org/abs/2510.12721</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12721">https://arxiv.org/pdf/2510.12721</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12721]] CARVQ: Corrective Adaptor with Group Residual Vector Quantization for LLM Embedding Compression(https://arxiv.org/abs/2510.12721)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) typically rely on a large number of parameters for token embedding, leading to substantial storage requirements and memory footprints. In particular, LLMs deployed on edge devices are memory-bound, and reducing the memory footprint by compressing the embedding layer not only frees up the memory bandwidth but also speeds up inference. To address this, we introduce CARVQ, a post-training novel Corrective Adaptor combined with group Residual Vector Quantization. CARVQ relies on the composition of both linear and non-linear maps and mimics the original model embedding to compress to approximately 1.6 bits without requiring specialized hardware to support lower-bit storage. We test our method on pre-trained LLMs such as LLaMA-3.2-1B, LLaMA-3.2-3B, LLaMA-3.2-3B-Instruct, LLaMA-3.1-8B, Qwen2.5-7B, Qwen2.5-Math-7B and Phi-4, evaluating on common generative, discriminative, math and reasoning tasks. We show that in most cases, CARVQ can achieve lower average bitwidth-per-parameter while maintaining reasonable perplexity and accuracy compared to scalar quantization. Our contributions include a novel compression technique that is compatible with state-of-the-art transformer quantization methods and can be seamlessly integrated into any hardware supporting 4-bit memory to reduce the model's memory footprint in memory-constrained devices. This work demonstrates a crucial step toward the efficient deployment of LLMs on edge devices.</li>
</ul>

<h3>Title: Personalized Federated Fine-Tuning of Vision Foundation Models for Healthcare</h3>
<ul>
<li><strong>Authors: </strong>Adam Tupper, Christian Gagné</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12741">https://arxiv.org/abs/2510.12741</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12741">https://arxiv.org/pdf/2510.12741</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12741]] Personalized Federated Fine-Tuning of Vision Foundation Models for Healthcare(https://arxiv.org/abs/2510.12741)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Foundation models open up new possibilities for the use of AI in healthcare. However, even when pre-trained on health data, they still need to be fine-tuned for specific downstream tasks. Furthermore, although foundation models reduce the amount of training data required to achieve good performance, obtaining sufficient data is still a challenge. This is due, in part, to restrictions on sharing and aggregating data from different sources to protect patients' privacy. One possible solution to this is to fine-tune foundation models via federated learning across multiple participating clients (i.e., hospitals, clinics, etc.). In this work, we propose a new personalized federated fine-tuning method that learns orthogonal LoRA adapters to disentangle general and client-specific knowledge, enabling each client to fully exploit both their own data and the data of others. Our preliminary results on real-world federated medical imaging tasks demonstrate that our approach is competitive against current federated fine-tuning methods.</li>
</ul>

<h3>Title: FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Junhao Zhuang, Shi Guo, Xin Cai, Xiaohui Li, Yihao Liu, Chun Yuan, Tianfan Xue</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12747">https://arxiv.org/abs/2510.12747</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12747">https://arxiv.org/pdf/2510.12747</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12747]] FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution(https://arxiv.org/abs/2510.12747)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have recently advanced video restoration, but applying them to real-world video super-resolution (VSR) remains challenging due to high latency, prohibitive computation, and poor generalization to ultra-high resolutions. Our goal in this work is to make diffusion-based VSR practical by achieving efficiency, scalability, and real-time performance. To this end, we propose FlashVSR, the first diffusion-based one-step streaming framework towards real-time VSR. FlashVSR runs at approximately 17 FPS for 768x1408 videos on a single A100 GPU by combining three complementary innovations: (i) a train-friendly three-stage distillation pipeline that enables streaming super-resolution, (ii) locality-constrained sparse attention that cuts redundant computation while bridging the train-test resolution gap, and (iii) a tiny conditional decoder that accelerates reconstruction without sacrificing quality. To support large-scale training, we also construct VSR-120K, a new dataset with 120k videos and 180k images. Extensive experiments show that FlashVSR scales reliably to ultra-high resolutions and achieves state-of-the-art performance with up to 12x speedup over prior one-step diffusion VSR models. We will release the code, pretrained models, and dataset to foster future research in efficient diffusion-based VSR.</li>
</ul>

<h3>Title: MVP4D: Multi-View Portrait Video Diffusion for Animatable 4D Avatars</h3>
<ul>
<li><strong>Authors: </strong>Felix Taubner, Ruihang Zhang, Mathieu Tuli, Sherwin Bahmani, David B. Lindell</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12785">https://arxiv.org/abs/2510.12785</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12785">https://arxiv.org/pdf/2510.12785</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12785]] MVP4D: Multi-View Portrait Video Diffusion for Animatable 4D Avatars(https://arxiv.org/abs/2510.12785)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Digital human avatars aim to simulate the dynamic appearance of humans in virtual environments, enabling immersive experiences across gaming, film, virtual reality, and more. However, the conventional process for creating and animating photorealistic human avatars is expensive and time-consuming, requiring large camera capture rigs and significant manual effort from professional 3D artists. With the advent of capable image and video generation models, recent methods enable automatic rendering of realistic animated avatars from a single casually captured reference image of a target subject. While these techniques significantly lower barriers to avatar creation and offer compelling realism, they lack constraints provided by multi-view information or an explicit 3D representation. So, image quality and realism degrade when rendered from viewpoints that deviate strongly from the reference image. Here, we build a video model that generates animatable multi-view videos of digital humans based on a single reference image and target expressions. Our model, MVP4D, is based on a state-of-the-art pre-trained video diffusion model and generates hundreds of frames simultaneously from viewpoints varying by up to 360 degrees around a target subject. We show how to distill the outputs of this model into a 4D avatar that can be rendered in real-time. Our approach significantly improves the realism, temporal consistency, and 3D consistency of generated avatars compared to previous methods.</li>
</ul>

<h3>Title: UniFusion: Vision-Language Model as Unified Encoder in Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Kevin Li, Manuel Brack, Sudeep Katakol, Hareesh Ravi, Ajinkya Kale</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12789">https://arxiv.org/abs/2510.12789</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12789">https://arxiv.org/pdf/2510.12789</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12789]] UniFusion: Vision-Language Model as Unified Encoder in Image Generation(https://arxiv.org/abs/2510.12789)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Although recent advances in visual generation have been remarkable, most existing architectures still depend on distinct encoders for images and text. This separation constrains diffusion models' ability to perform cross-modal reasoning and knowledge transfer. Prior attempts to bridge this gap often use the last layer information from VLM, employ multiple visual encoders, or train large unified models jointly for text and image generation, which demands substantial computational resources and large-scale data, limiting its this http URL present UniFusion, a diffusion-based generative model conditioned on a frozen large vision-language model (VLM) that serves as a unified multimodal encoder. At the core of UniFusion is the Layerwise Attention Pooling (LAP) mechanism that extracts both high level semantics and low level details from text and visual tokens of a frozen VLM to condition a diffusion generative model. We demonstrate that LAP outperforms other shallow fusion architectures on text-image alignment for generation and faithful transfer of visual information from VLM to the diffusion model which is key for editing. We propose VLM-Enabled Rewriting Injection with Flexibile Inference (VERIFI), which conditions a diffusion transformer (DiT) only on the text tokens generated by the VLM during in-model prompt rewriting. VERIFI combines the alignment of the conditioning distribution with the VLM's reasoning capabilities for increased capabilities and flexibility at inference. In addition, finetuning on editing task not only improves text-image alignment for generation, indicative of cross-modality knowledge transfer, but also exhibits tremendous generalization capabilities. Our model when trained on single image editing, zero-shot generalizes to multiple image references further motivating the unified encoder design of UniFusion.</li>
</ul>

<h3>Title: DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Yingyan Li, Shuyao Shang, Weisong Liu, Bing Zhan, Haochen Wang, Yuqi Wang, Yuntao Chen, Xiaoman Wang, Yasong An, Chufeng Tang, Lu Hou, Lue Fan, Zhaoxiang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2510.12796">https://arxiv.org/abs/2510.12796</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2510.12796">https://arxiv.org/pdf/2510.12796</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2510.12796]] DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving(https://arxiv.org/abs/2510.12796)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, self-supervised</a></li>
<li><strong>Abstract: </strong>Scaling Vision-Language-Action (VLA) models on large-scale data offers a promising path to achieving a more generalized driving intelligence. However, VLA models are limited by a ``supervision deficit'': the vast model capacity is supervised by sparse, low-dimensional actions, leaving much of their representational power underutilized. To remedy this, we propose \textbf{DriveVLA-W0}, a training paradigm that employs world modeling to predict future images. This task generates a dense, self-supervised signal that compels the model to learn the underlying dynamics of the driving environment. We showcase the paradigm's versatility by instantiating it for two dominant VLA archetypes: an autoregressive world model for VLAs that use discrete visual tokens, and a diffusion world model for those operating on continuous visual features. Building on the rich representations learned from world modeling, we introduce a lightweight action expert to address the inference latency for real-time deployment. Extensive experiments on the NAVSIM v1/v2 benchmark and a 680x larger in-house dataset demonstrate that DriveVLA-W0 significantly outperforms BEV and VLA baselines. Crucially, it amplifies the data scaling law, showing that performance gains accelerate as the training dataset size increases.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
