<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-02-16</h1>
<h3>Title: Rolling Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>David Ruhe, Jonathan Heek, Tim Salimans, Emiel Hoogeboom</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09470">https://arxiv.org/abs/2402.09470</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09470">https://arxiv.org/pdf/2402.09470</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09470]] Rolling Diffusion Models(https://arxiv.org/abs/2402.09470)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have recently been increasingly applied to temporal data such as video, fluid mechanics simulations, or climate data. These methods generally treat subsequent frames equally regarding the amount of noise in the diffusion process. This paper explores Rolling Diffusion: a new approach that uses a sliding window denoising process. It ensures that the diffusion process progressively corrupts through time by assigning more noise to frames that appear later in a sequence, reflecting greater uncertainty about the future as the generation process unfolds. Empirically, we show that when the temporal dynamics are complex, Rolling Diffusion is superior to standard diffusion. In particular, this result is demonstrated in a video prediction task using the Kinetics-600 video dataset and in a chaotic fluid dynamics forecasting experiment.</li>
</ul>

<h3>Title: Reducing Texture Bias of Deep Neural Networks via Edge Enhancing  Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Edgar Heinert, Matthias Rottmann, Kira Maag, Karsten Kahl</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09530">https://arxiv.org/abs/2402.09530</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09530">https://arxiv.org/pdf/2402.09530</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09530]] Reducing Texture Bias of Deep Neural Networks via Edge Enhancing  Diffusion(https://arxiv.org/abs/2402.09530)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Convolutional neural networks (CNNs) for image processing tend to focus on localized texture patterns, commonly referred to as texture bias. While most of the previous works in the literature focus on the task of image classification, we go beyond this and study the texture bias of CNNs in semantic segmentation. In this work, we propose to train CNNs on pre-processed images with less texture to reduce the texture bias. Therein, the challenge is to suppress image texture while preserving shape information. To this end, we utilize edge enhancing diffusion (EED), an anisotropic image diffusion method initially introduced for image compression, to create texture reduced duplicates of existing datasets. Extensive numerical studies are performed with both CNNs and vision transformer models trained on original data and EED-processed data from the Cityscapes dataset and the CARLA driving simulator. We observe strong texture-dependence of CNNs and moderate texture-dependence of transformers. Training CNNs on EED-processed images enables the models to become completely ignorant with respect to texture, demonstrating resilience with respect to texture re-introduction to any degree. Additionally we analyze the performance reduction in depth on a level of connected components in the semantic segmentation and study the influence of EED pre-processing on domain generalization as well as adversarial robustness.</li>
</ul>

<h3>Title: WERank: Towards Rank Degradation Prevention for Self-Supervised Learning  Using Weight Regularization</h3>
<ul>
<li><strong>Authors: </strong>Ali Saheb Pasand, Reza Moravej, Mahdi Biparva, Ali Ghodsi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09586">https://arxiv.org/abs/2402.09586</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09586">https://arxiv.org/pdf/2402.09586</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09586]] WERank: Towards Rank Degradation Prevention for Self-Supervised Learning  Using Weight Regularization(https://arxiv.org/abs/2402.09586)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>A common phenomena confining the representation quality in Self-Supervised Learning (SSL) is dimensional collapse (also known as rank degeneration), where the learned representations are mapped to a low dimensional subspace of the representation space. The State-of-the-Art SSL methods have shown to suffer from dimensional collapse and fall behind maintaining full rank. Recent approaches to prevent this problem have proposed using contrastive losses, regularization techniques, or architectural tricks. We propose WERank, a new regularizer on the weight parameters of the network to prevent rank degeneration at different layers of the network. We provide empirical evidence and mathematical justification to demonstrate the effectiveness of the proposed regularization method in preventing dimensional collapse. We verify the impact of WERank on graph SSL where dimensional collapse is more pronounced due to the lack of proper data augmentation. We empirically demonstrate that WERank is effective in helping BYOL to achieve higher rank during SSL pre-training and consequently downstream accuracy during evaluation probing. Ablation studies and experimental analysis shed lights on the underlying factors behind the performance gains of the proposed approach.</li>
</ul>

<h3>Title: DeepATLAS: One-Shot Localization for Biomedical Data</h3>
<ul>
<li><strong>Authors: </strong>Peter D. Chang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09587">https://arxiv.org/abs/2402.09587</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09587">https://arxiv.org/pdf/2402.09587</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09587]] DeepATLAS: One-Shot Localization for Biomedical Data(https://arxiv.org/abs/2402.09587)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>This paper introduces the DeepATLAS foundational model for localization tasks in the domain of high-dimensional biomedical data. Upon convergence of the proposed self-supervised objective, a pretrained model maps an input to an anatomically-consistent embedding from which any point or set of points (e.g., boxes or segmentations) may be identified in a one-shot or few-shot approach. As a representative benchmark, a DeepATLAS model pretrained on a comprehensive cohort of 51,000+ unlabeled 3D computed tomography exams yields high one-shot segmentation performance on over 50 anatomic structures across four different external test sets, either matching or exceeding the performance of a standard supervised learning model. Further improvements in accuracy can be achieved by adding a small amount of labeled data using either a semisupervised or more conventional fine-tuning strategy.</li>
</ul>

<h3>Title: Scalable Graph Self-Supervised Learning</h3>
<ul>
<li><strong>Authors: </strong>Ali Saheb Pasand, Reza Moravej, Mahdi Biparva, Raika Karimi, Ali Ghodsi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09603">https://arxiv.org/abs/2402.09603</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09603">https://arxiv.org/pdf/2402.09603</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09603]] Scalable Graph Self-Supervised Learning(https://arxiv.org/abs/2402.09603)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>In regularization Self-Supervised Learning (SSL) methods for graphs, computational complexity increases with the number of nodes in graphs and embedding dimensions. To mitigate the scalability of non-contrastive graph SSL, we propose a novel approach to reduce the cost of computing the covariance matrix for the pre-training loss function with volume-maximization terms. Our work focuses on reducing the cost associated with the loss computation via graph node or dimension sampling. We provide theoretical insight into why dimension sampling would result in accurate loss computations and support it with mathematical derivation of the novel approach. We develop our experimental setup on the node-level graph prediction tasks, where SSL pre-training has shown to be difficult due to the large size of real world graphs. Our experiments demonstrate that the cost associated with the loss computation can be reduced via node or dimension sampling without lowering the downstream performance. Our results demonstrate that sampling mostly results in improved downstream performance. Ablation studies and experimental analysis are provided to untangle the role of the different factors in the experimental setup.</li>
</ul>

<h3>Title: Towards Privacy-Aware Sign Language Translation at Scale</h3>
<ul>
<li><strong>Authors: </strong>Phillip Rust, Bowen Shi, Skyler Wang, Necati Cihan Camgöz, Jean Maillard</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09611">https://arxiv.org/abs/2402.09611</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09611">https://arxiv.org/pdf/2402.09611</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09611]] Towards Privacy-Aware Sign Language Translation at Scale(https://arxiv.org/abs/2402.09611)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>A major impediment to the advancement of sign language translation (SLT) is data scarcity. Much of the sign language data currently available on the web cannot be used for training supervised models due to the lack of aligned captions. Furthermore, scaling SLT using large-scale web-scraped datasets bears privacy risks due to the presence of biometric information, which the responsible development of SLT technologies should account for. In this work, we propose a two-stage framework for privacy-aware SLT at scale that addresses both of these issues. We introduce SSVP-SLT, which leverages self-supervised video pretraining on anonymized and unannotated videos, followed by supervised SLT finetuning on a curated parallel dataset. SSVP-SLT achieves state-of-the-art finetuned and zero-shot gloss-free SLT performance on the How2Sign dataset, outperforming the strongest respective baselines by over 3 BLEU-4. Based on controlled experiments, we further discuss the advantages and limitations of self-supervised pretraining and anonymization via facial obfuscation for SLT.</li>
</ul>

<h3>Title: Probabilistic Reasoning in Generative Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Aliakbar Nafar, Kristen Brent Venable, Parisa Kordjamshidi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09614">https://arxiv.org/abs/2402.09614</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09614">https://arxiv.org/pdf/2402.09614</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09614]] Probabilistic Reasoning in Generative Large Language Models(https://arxiv.org/abs/2402.09614)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper considers the challenges that Large Language Models (LLMs) face when reasoning over text that includes information involving uncertainty explicitly quantified via probability values. This type of reasoning is relevant to a variety of contexts ranging from everyday conversations to medical decision-making. Despite improvements in the mathematical reasoning capabilities of LLMs, they still exhibit significant difficulties when it comes to probabilistic reasoning. To deal with this problem, we first introduce the Bayesian Linguistic Inference Dataset (BLInD), a new dataset specifically designed to test the probabilistic reasoning capabilities of LLMs. We then leverage this new dataset to thoroughly illustrate the specific limitations of LLMs for tasks involving probabilistic reasoning and present several strategies that map the problem to different formal representations, including Python code, probabilistic inference algorithms, and probabilistic logical programming. We conclude by providing an evaluation of our methods on BLInD and on an adaptation of a causal reasoning question-answering dataset, which further shows their practical effectiveness.</li>
</ul>

<h3>Title: Seed Optimization with Frozen Generator for Superior Zero-shot Low-light  Enhancement</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Gu, Yi Jin, Ben Wang, Zhixiang Wei, Xiaoxiao Ma, Pengyang Ling, Haoxuan Wang, Huaian Chen, Enhong Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09694">https://arxiv.org/abs/2402.09694</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09694">https://arxiv.org/pdf/2402.09694</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09694]] Seed Optimization with Frozen Generator for Superior Zero-shot Low-light  Enhancement(https://arxiv.org/abs/2402.09694)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In this work, we observe that the generators, which are pre-trained on massive natural images, inherently hold the promising potential for superior low-light image enhancement against varying scenarios.Specifically, we embed a pre-trained generator to Retinex model to produce reflectance maps with enhanced detail and vividness, thereby recovering features degraded by low-light conditions.Taking one step further, we introduce a novel optimization strategy, which backpropagates the gradients to the input seeds rather than the parameters of the low-light enhancement model, thus intactly retaining the generative knowledge learned from natural images and achieving faster convergence speed. Benefiting from the pre-trained knowledge and seed-optimization strategy, the low-light enhancement model can significantly regularize the realness and fidelity of the enhanced result, thus rapidly generating high-quality images without training on any low-light dataset. Extensive experiments on various benchmarks demonstrate the superiority of the proposed method over numerous state-of-the-art methods qualitatively and quantitatively.</li>
</ul>

<h3>Title: Diffusion Model with Cross Attention as an Inductive Bias for  Disentanglement</h3>
<ul>
<li><strong>Authors: </strong>Tao Yang, Cuiling Lan, Yan Lu, Nanning zheng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09712">https://arxiv.org/abs/2402.09712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09712">https://arxiv.org/pdf/2402.09712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09712]] Diffusion Model with Cross Attention as an Inductive Bias for  Disentanglement(https://arxiv.org/abs/2402.09712)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Disentangled representation learning strives to extract the intrinsic factors within observed data. Factorizing these representations in an unsupervised manner is notably challenging and usually requires tailored loss functions or specific structural designs. In this paper, we introduce a new perspective and framework, demonstrating that diffusion models with cross-attention can serve as a powerful inductive bias to facilitate the learning of disentangled representations. We propose to encode an image to a set of concept tokens and treat them as the condition of the latent diffusion for image reconstruction, where cross-attention over the concept tokens is used to bridge the interaction between the encoder and diffusion. Without any additional regularization, this framework achieves superior disentanglement performance on the benchmark datasets, surpassing all previous methods with intricate designs. We have conducted comprehensive ablation studies and visualization analysis, shedding light on the functioning of this model. This is the first work to reveal the potent disentanglement capability of diffusion models with cross-attention, requiring no complex designs. We anticipate that our findings will inspire more investigation on exploring diffusion for disentangled representation learning towards more sophisticated data analysis and understanding.</li>
</ul>

<h3>Title: Visually Dehallucinative Instruction Generation: Know What You Don't  Know</h3>
<ul>
<li><strong>Authors: </strong>Sungguk Cha, Jusung Lee, Younghyun Lee, Cheoljong Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09717">https://arxiv.org/abs/2402.09717</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09717">https://arxiv.org/pdf/2402.09717</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09717]] Visually Dehallucinative Instruction Generation: Know What You Don't  Know(https://arxiv.org/abs/2402.09717)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>"When did the emperor Napoleon invented iPhone?" Such hallucination-inducing question is well known challenge in generative language modeling. In this study, we present an innovative concept of visual hallucination, referred to as "I Know (IK)" hallucination, to address scenarios where "I Don't Know" is the desired response. To effectively tackle this issue, we propose the VQAv2-IDK benchmark, the subset of VQAv2 comprising unanswerable image-question pairs as determined by human annotators. Stepping further, we present the visually dehallucinative instruction generation method for IK hallucination and introduce the IDK-Instructions visual instruction database. Our experiments show that current methods struggle with IK hallucination. Yet, our approach effectively reduces these hallucinations, proving its versatility across different frameworks and datasets.</li>
</ul>

<h3>Title: AbuseGPT: Abuse of Generative AI ChatBots to Create Smishing Campaigns</h3>
<ul>
<li><strong>Authors: </strong>Ashfak Md Shibli, Mir Mehedi A. Pritom, Maanak Gupta</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09728">https://arxiv.org/abs/2402.09728</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09728">https://arxiv.org/pdf/2402.09728</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09728]] AbuseGPT: Abuse of Generative AI ChatBots to Create Smishing Campaigns(https://arxiv.org/abs/2402.09728)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>SMS phishing, also known as "smishing", is a growing threat that tricks users into disclosing private information or clicking into URLs with malicious content through fraudulent mobile text messages. In recent past, we have also observed a rapid advancement of conversational generative AI chatbot services (e.g., OpenAI's ChatGPT, Google's BARD), which are powered by pre-trained large language models (LLMs). These AI chatbots certainly have a lot of utilities but it is not systematically understood how they can play a role in creating threats and attacks. In this paper, we propose AbuseGPT method to show how the existing generative AI-based chatbot services can be exploited by attackers in real world to create smishing texts and eventually lead to craftier smishing campaigns. To the best of our knowledge, there is no pre-existing work that evidently shows the impacts of these generative text-based models on creating SMS phishing. Thus, we believe this study is the first of its kind to shed light on this emerging cybersecurity threat. We have found strong empirical evidences to show that attackers can exploit ethical standards in the existing generative AI-based chatbot services by crafting prompt injection attacks to create newer smishing campaigns. We also discuss some future research directions and guidelines to protect the abuse of generative AI-based services and safeguard users from smishing attacks.</li>
</ul>

<h3>Title: DFORM: Diffeomorphic vector field alignment for assessing dynamics  across learned models</h3>
<ul>
<li><strong>Authors: </strong>Ruiqi Chen, Giacomo Vedovati, Todd Braver, ShiNung Ching</a></li>
<li><strong>Subjects: </strong>cs.LG, eess.SY, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09735">https://arxiv.org/abs/2402.09735</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09735">https://arxiv.org/pdf/2402.09735</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09735]] DFORM: Diffeomorphic vector field alignment for assessing dynamics  across learned models(https://arxiv.org/abs/2402.09735)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Dynamical system models such as Recurrent Neural Networks (RNNs) have become increasingly popular as hypothesis-generating tools in scientific research. Evaluating the dynamics in such networks is key to understanding their learned generative mechanisms. However, comparison of learned dynamics across models is challenging due to their inherent nonlinearity and because a priori there is no enforced equivalence of their coordinate systems. Here, we propose the DFORM (Diffeomorphic vector field alignment for comparing dynamics across learned models) framework. DFORM learns a nonlinear coordinate transformation which provides a continuous, maximally one-to-one mapping between the trajectories of learned models, thus approximating a diffeomorphism between them. The mismatch between DFORM-transformed vector fields defines the orbital similarity between two models, thus providing a generalization of the concepts of smooth orbital and topological equivalence. As an example, we apply DFORM to models trained on a canonical neuroscience task, showing that learned dynamics may be functionally similar, despite overt differences in attractor landscapes.</li>
</ul>

<h3>Title: QuRating: Selecting High-Quality Data for Training Language Models</h3>
<ul>
<li><strong>Authors: </strong>Alexander Wettig, Aatmik Gupta, Saumya Malik, Danqi Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09739">https://arxiv.org/abs/2402.09739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09739">https://arxiv.org/pdf/2402.09739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09739]] QuRating: Selecting High-Quality Data for Training Language Models(https://arxiv.org/abs/2402.09739)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Selecting high-quality pre-training data is important for creating capable language models, but existing methods rely on simple heuristics. We introduce QuRating, a method for selecting pre-training data that captures the abstract qualities of texts which humans intuitively perceive. In this paper, we investigate four qualities - writing style, required expertise, facts & trivia, and educational value. We find that LLMs are able to discern these qualities and observe that they are better at making pairwise judgments of texts than at rating the quality of a text directly. We train a QuRater model to learn scalar ratings from pairwise judgments, and use it to annotate a 260B training corpus with quality ratings for each of the four criteria. In our experiments, we select 30B tokens according to the different quality ratings and train 1.3B-parameter language models on the selected data. We find that it is important to balance quality and diversity, as selecting only the highest-rated documents leads to poor results. When we sample using quality ratings as logits over documents, our models achieve lower perplexity and stronger in-context learning performance than baselines. Beyond data selection, we use the quality ratings to construct a training curriculum which improves performance without changing the training dataset. We extensively analyze the quality ratings and discuss their characteristics, biases, and wider implications.</li>
</ul>

<h3>Title: Grounding Language Model with Chunking-Free In-Context Retrieval</h3>
<ul>
<li><strong>Authors: </strong>Hongjin Qian, Zheng Liu, Kelong Mao, Yujia Zhou, Zhicheng Dou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09760">https://arxiv.org/abs/2402.09760</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09760">https://arxiv.org/pdf/2402.09760</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09760]] Grounding Language Model with Chunking-Free In-Context Retrieval(https://arxiv.org/abs/2402.09760)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>This paper presents a novel Chunking-Free In-Context (CFIC) retrieval approach, specifically tailored for Retrieval-Augmented Generation (RAG) systems. Traditional RAG systems often struggle with grounding responses using precise evidence text due to the challenges of processing lengthy documents and filtering out irrelevant content. Commonly employed solutions, such as document chunking and adapting language models to handle longer contexts, have their limitations. These methods either disrupt the semantic coherence of the text or fail to effectively address the issues of noise and inaccuracy in evidence retrieval. CFIC addresses these challenges by circumventing the conventional chunking process. It utilizes the encoded hidden states of documents for in-context retrieval, employing auto-aggressive decoding to accurately identify the specific evidence text required for user queries, eliminating the need for chunking. CFIC is further enhanced by incorporating two decoding strategies, namely Constrained Sentence Prefix Decoding and Skip Decoding. These strategies not only improve the efficiency of the retrieval process but also ensure that the fidelity of the generated grounding text evidence is maintained. Our evaluations of CFIC on a range of open QA datasets demonstrate its superiority in retrieving relevant and accurate evidence, offering a significant improvement over traditional methods. By doing away with the need for document chunking, CFIC presents a more streamlined, effective, and efficient retrieval solution, making it a valuable advancement in the field of RAG systems.</li>
</ul>

<h3>Title: Examining Pathological Bias in a Generative Adversarial Network  Discriminator: A Case Study on a StyleGAN3 Model</h3>
<ul>
<li><strong>Authors: </strong>Alvin Grissom II, Ryan F. Lei, Jeova Farias Sales Rocha Neto, Bailey Lin, Ryan Trotter</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09786">https://arxiv.org/abs/2402.09786</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09786">https://arxiv.org/pdf/2402.09786</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09786]] Examining Pathological Bias in a Generative Adversarial Network  Discriminator: A Case Study on a StyleGAN3 Model(https://arxiv.org/abs/2402.09786)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative adversarial networks generate photorealistic faces that are often indistinguishable by humans from real faces. We find that the discriminator in the pre-trained StyleGAN3 model, a popular GAN network, systematically stratifies scores by both image- and face-level qualities and that this disproportionately affects images across gender, race, and other categories. We examine the discriminator's bias for color and luminance across axes perceived race and gender; we then examine axes common in research on stereotyping in social psychology.</li>
</ul>

<h3>Title: DreamMatcher: Appearance Matching Self-Attention for  Semantically-Consistent Text-to-Image Personalization</h3>
<ul>
<li><strong>Authors: </strong>Jisu Nam, Heesu Kim, DongJae Lee, Siyoon Jin, Seungryong Kim, Seunggyu Chang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09812">https://arxiv.org/abs/2402.09812</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09812">https://arxiv.org/pdf/2402.09812</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09812]] DreamMatcher: Appearance Matching Self-Attention for  Semantically-Consistent Text-to-Image Personalization(https://arxiv.org/abs/2402.09812)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The objective of text-to-image (T2I) personalization is to customize a diffusion model to a user-provided reference concept, generating diverse images of the concept aligned with the target prompts. Conventional methods representing the reference concepts using unique text embeddings often fail to accurately mimic the appearance of the reference. To address this, one solution may be explicitly conditioning the reference images into the target denoising process, known as key-value replacement. However, prior works are constrained to local editing since they disrupt the structure path of the pre-trained T2I model. To overcome this, we propose a novel plug-in method, called DreamMatcher, which reformulates T2I personalization as semantic matching. Specifically, DreamMatcher replaces the target values with reference values aligned by semantic matching, while leaving the structure path unchanged to preserve the versatile capability of pre-trained T2I models for generating diverse structures. We also introduce a semantic-consistent masking strategy to isolate the personalized concept from irrelevant regions introduced by the target prompts. Compatible with existing T2I models, DreamMatcher shows significant improvements in complex scenarios. Intensive analyses demonstrate the effectiveness of our approach.</li>
</ul>

<h3>Title: Mind the Modality Gap: Towards a Remote Sensing Vision-Language Model  via Cross-modal Alignment</h3>
<ul>
<li><strong>Authors: </strong>Angelos Zavras, Dimitrios Michail, Begüm Demir, Ioannis Papoutsis</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09816">https://arxiv.org/abs/2402.09816</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09816">https://arxiv.org/pdf/2402.09816</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09816]] Mind the Modality Gap: Towards a Remote Sensing Vision-Language Model  via Cross-modal Alignment(https://arxiv.org/abs/2402.09816)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Deep Learning (DL) is undergoing a paradigm shift with the emergence of foundation models, aptly named by their crucial, yet incomplete nature. In this work, we focus on Contrastive Language-Image Pre-training (CLIP), an open-vocabulary foundation model, which achieves high accuracy across many image classification tasks and is often competitive with a fully supervised baseline without being explicitly trained. Nevertheless, there are still domains where zero-shot CLIP performance is far from optimal, such as Remote Sensing (RS) and medical imagery. These domains do not only exhibit fundamentally different distributions compared to natural images, but also commonly rely on complementary modalities, beyond RGB, to derive meaningful insights. To this end, we propose a methodology for the purpose of aligning distinct RS imagery modalities with the visual and textual modalities of CLIP. Our two-stage procedure, comprises of robust fine-tuning CLIP in order to deal with the distribution shift, accompanied by the cross-modal alignment of a RS modality encoder, in an effort to extend the zero-shot capabilities of CLIP. We ultimately demonstrate our method on the tasks of RS imagery classification and cross-modal retrieval. We empirically show that both robust fine-tuning and cross-modal alignment translate to significant performance gains, across several RS benchmark datasets. Notably, these enhancements are achieved without the reliance on textual descriptions, without introducing any task-specific parameters, without training from scratch and without catastrophic forgetting.</li>
</ul>

<h3>Title: Utilizing GANs for Fraud Detection: Model Training with Synthetic  Transaction Data</h3>
<ul>
<li><strong>Authors: </strong>Mengran Zhu, Yulu Gong, Yafei Xiang, Hanyi Yu, Shuning Huo</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09830">https://arxiv.org/abs/2402.09830</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09830">https://arxiv.org/pdf/2402.09830</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09830]] Utilizing GANs for Fraud Detection: Model Training with Synthetic  Transaction Data(https://arxiv.org/abs/2402.09830)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, anomaly</a></li>
<li><strong>Abstract: </strong>Anomaly detection is a critical challenge across various research domains, aiming to identify instances that deviate from normal data distributions. This paper explores the application of Generative Adversarial Networks (GANs) in fraud detection, comparing their advantages with traditional methods. GANs, a type of Artificial Neural Network (ANN), have shown promise in modeling complex data distributions, making them effective tools for anomaly detection. The paper systematically describes the principles of GANs and their derivative models, emphasizing their application in fraud detection across different datasets. And by building a collection of adversarial verification graphs, we will effectively prevent fraud caused by bots or automated systems and ensure that the users in the transaction are real. The objective of the experiment is to design and implement a fake face verification code and fraud detection system based on Generative Adversarial network (GANs) algorithm to enhance the security of the transaction process.The study demonstrates the potential of GANs in enhancing transaction security through deep learning techniques.</li>
</ul>

<h3>Title: Social Reward: Evaluating and Enhancing Generative AI through  Million-User Feedback from an Online Creative Community</h3>
<ul>
<li><strong>Authors: </strong>Arman Isajanyan, Artur Shatveryan, David Kocharyan, Zhangyang Wang, Humphrey Shi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09872">https://arxiv.org/abs/2402.09872</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09872">https://arxiv.org/pdf/2402.09872</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09872]] Social Reward: Evaluating and Enhancing Generative AI through  Million-User Feedback from an Online Creative Community(https://arxiv.org/abs/2402.09872)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Social reward as a form of community recognition provides a strong source of motivation for users of online platforms to engage and contribute with content. The recent progress of text-conditioned image synthesis has ushered in a collaborative era where AI empowers users to craft original visual artworks seeking community validation. Nevertheless, assessing these models in the context of collective community preference introduces distinct challenges. Existing evaluation methods predominantly center on limited size user studies guided by image quality and prompt alignment. This work pioneers a paradigm shift, unveiling Social Reward - an innovative reward modeling framework that leverages implicit feedback from social network users engaged in creative editing of generated images. We embark on an extensive journey of dataset curation and refinement, drawing from Picsart: an online visual creation and editing platform, yielding a first million-user-scale dataset of implicit human preferences for user-generated visual art named Picsart Image-Social. Our analysis exposes the shortcomings of current metrics in modeling community creative preference of text-to-image models' outputs, compelling us to introduce a novel predictive model explicitly tailored to address these limitations. Rigorous quantitative experiments and user study show that our Social Reward model aligns better with social popularity than existing metrics. Furthermore, we utilize Social Reward to fine-tune text-to-image models, yielding images that are more favored by not only Social Reward, but also other established metrics. These findings highlight the relevance and effectiveness of Social Reward in assessing community appreciation for AI-generated artworks, establishing a closer alignment with users' creative goals: creating popular visual art. Codes can be accessed at https://github.com/Picsart-AI-Research/Social-Reward</li>
</ul>

<h3>Title: Lester: rotoscope animation through video object segmentation and  tracking</h3>
<ul>
<li><strong>Authors: </strong>Ruben Tous</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09883">https://arxiv.org/abs/2402.09883</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09883">https://arxiv.org/pdf/2402.09883</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09883]] Lester: rotoscope animation through video object segmentation and  tracking(https://arxiv.org/abs/2402.09883)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This article introduces Lester, a novel method to automatically synthetise retro-style 2D animations from videos. The method approaches the challenge mainly as an object segmentation and tracking problem. Video frames are processed with the Segment Anything Model (SAM) and the resulting masks are tracked through subsequent frames with DeAOT, a method of hierarchical propagation for semi-supervised video object segmentation. The geometry of the masks' contours is simplified with the Douglas-Peucker algorithm. Finally, facial traits, pixelation and a basic shadow effect can be optionally added. The results show that the method exhibits an excellent temporal consistency and can correctly process videos with different poses and appearances, dynamic shots, partial shots and diverse backgrounds. The proposed method provides a more simple and deterministic approach than diffusion models based video-to-video translation pipelines, which suffer from temporal consistency problems and do not cope well with pixelated and schematic outputs. The method is also much most practical than techniques based on 3D human pose estimation, which require custom handcrafted 3D models and are very limited with respect to the type of scenes they can process.</li>
</ul>

<h3>Title: Generative Representational Instruction Tuning</h3>
<ul>
<li><strong>Authors: </strong>Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, Douwe Kiela</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09906">https://arxiv.org/abs/2402.09906</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09906">https://arxiv.org/pdf/2402.09906</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09906]] Generative Representational Instruction Tuning(https://arxiv.org/abs/2402.09906)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>All text-based language problems can be reduced to either generation or embedding. Current models only perform well at one or the other. We introduce generative representational instruction tuning (GRIT) whereby a large language model is trained to handle both generative and embedding tasks by distinguishing between them through instructions. Compared to other open models, our resulting GritLM 7B sets a new state of the art on the Massive Text Embedding Benchmark (MTEB) and outperforms all models up to its size on a range of generative tasks. By scaling up further, GritLM 8x7B outperforms all open generative language models that we tried while still being among the best embedding models. Notably, we find that GRIT matches training on only generative or embedding data, thus we can unify both at no performance loss. Among other benefits, the unification via GRIT speeds up Retrieval-Augmented Generation (RAG) by > 60% for long documents, by no longer requiring separate retrieval and generation models. Models, code, etc. are freely available at https://github.com/ContextualAI/gritlm.</li>
</ul>

<h3>Title: Crafting a Good Prompt or Providing Exemplary Dialogues? A Study of  In-Context Learning for Persona-based Dialogue Generation</h3>
<ul>
<li><strong>Authors: </strong>Jiashu Pu, Yajing Wan, Yuru Zhang, Jing Chen, Ling Cheng, Qian Shao, Yongzhu Chang, Tangjie Lv, Rongsheng Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09954">https://arxiv.org/abs/2402.09954</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09954">https://arxiv.org/pdf/2402.09954</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09954]] Crafting a Good Prompt or Providing Exemplary Dialogues? A Study of  In-Context Learning for Persona-based Dialogue Generation(https://arxiv.org/abs/2402.09954)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Previous in-context learning (ICL) research has focused on tasks such as classification, machine translation, text2table, etc., while studies on whether ICL can improve human-like dialogue generation are scarce. Our work fills this gap by systematically investigating the ICL capabilities of large language models (LLMs) in persona-based dialogue generation, conducting extensive experiments on high-quality real human Chinese dialogue datasets. From experimental results, we draw three conclusions: 1) adjusting prompt instructions is the most direct, effective, and economical way to improve generation quality; 2) randomly retrieving demonstrations (demos) achieves the best results, possibly due to the greater diversity and the amount of effective information; counter-intuitively, retrieving demos with a context identical to the query performs the worst; 3) even when we destroy the multi-turn associations and single-turn semantics in the demos, increasing the number of demos still improves dialogue performance, proving that LLMs can learn from corrupted dialogue demos. Previous explanations of the ICL mechanism, such as $n$-gram induction head, cannot fully account for this phenomenon.</li>
</ul>

<h3>Title: Textual Localization: Decomposing Multi-concept Images for  Subject-Driven Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Junjie Shentu, Matthew Watson, Noura Al Moubayed</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09966">https://arxiv.org/abs/2402.09966</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09966">https://arxiv.org/pdf/2402.09966</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09966]] Textual Localization: Decomposing Multi-concept Images for  Subject-Driven Text-to-Image Generation(https://arxiv.org/abs/2402.09966)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Subject-driven text-to-image diffusion models empower users to tailor the model to new concepts absent in the pre-training dataset using a few sample images. However, prevalent subject-driven models primarily rely on single-concept input images, facing challenges in specifying the target concept when dealing with multi-concept input images. To this end, we introduce a textual localized text-to-image model (Texual Localization) to handle multi-concept input images. During fine-tuning, our method incorporates a novel cross-attention guidance to decompose multiple concepts, establishing distinct connections between the visual representation of the target concept and the identifier token in the text prompt. Experimental results reveal that our method outperforms or performs comparably to the baseline models in terms of image fidelity and image-text alignment on multi-concept input images. In comparison to Custom Diffusion, our method with hard guidance achieves CLIP-I scores that are 7.04%, 8.13% higher and CLIP-T scores that are 2.22%, 5.85% higher in single-concept and multi-concept generation, respectively. Notably, our method generates cross-attention maps consistent with the target concept in the generated images, a capability absent in existing models.</li>
</ul>

<h3>Title: Accelerating Parallel Sampling of Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Zhiwei Tang, Jiasheng Tang, Hao Luo, Fan Wang, Tsung-Hui Chang</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.09970">https://arxiv.org/abs/2402.09970</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.09970">https://arxiv.org/pdf/2402.09970</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.09970]] Accelerating Parallel Sampling of Diffusion Models(https://arxiv.org/abs/2402.09970)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models have emerged as state-of-the-art generative models for image generation. However, sampling from diffusion models is usually time-consuming due to the inherent autoregressive nature of their sampling process. In this work, we propose a novel approach that accelerates the sampling of diffusion models by parallelizing the autoregressive process. Specifically, we reformulate the sampling process as solving a system of triangular nonlinear equations through fixed-point iteration. With this innovative formulation, we explore several systematic techniques to further reduce the iteration steps required by the solving process. Applying these techniques, we introduce ParaTAA, a universal and training-free parallel sampling algorithm that can leverage extra computational and memory resources to increase the sampling speed. Our experiments demonstrate that ParaTAA can decrease the inference steps required by common sequential sampling algorithms such as DDIM and DDPM by a factor of 4~14 times. Notably, when applying ParaTAA with 100 steps DDIM for Stable Diffusion, a widely-used text-to-image diffusion model, it can produce the same images as the sequential sampling in only 7 inference steps.</li>
</ul>

<h3>Title: MM-Point: Multi-View Information-Enhanced Multi-Modal Self-Supervised 3D  Point Cloud Understanding</h3>
<ul>
<li><strong>Authors: </strong>Hai-Tao Yu, Mofei Song</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10002">https://arxiv.org/abs/2402.10002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10002">https://arxiv.org/pdf/2402.10002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10002]] MM-Point: Multi-View Information-Enhanced Multi-Modal Self-Supervised 3D  Point Cloud Understanding(https://arxiv.org/abs/2402.10002)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>In perception, multiple sensory information is integrated to map visual information from 2D views onto 3D objects, which is beneficial for understanding in 3D environments. But in terms of a single 2D view rendered from different angles, only limited partial information can be provided.The richness and value of Multi-view 2D information can provide superior self-supervised signals for 3D objects. In this paper, we propose a novel self-supervised point cloud representation learning method, MM-Point, which is driven by intra-modal and inter-modal similarity objectives. The core of MM-Point lies in the Multi-modal interaction and transmission between 3D objects and multiple 2D views at the same time. In order to more effectively simultaneously perform the consistent cross-modal objective of 2D multi-view information based on contrastive learning, we further propose Multi-MLP and Multi-level Augmentation strategies. Through carefully designed transformation strategies, we further learn Multi-level invariance in 2D Multi-views. MM-Point demonstrates state-of-the-art (SOTA) performance in various downstream tasks. For instance, it achieves a peak accuracy of 92.4% on the synthetic dataset ModelNet40, and a top accuracy of 87.8% on the real-world dataset ScanObjectNN, comparable to fully supervised methods. Additionally, we demonstrate its effectiveness in tasks such as few-shot classification, 3D part segmentation and 3D semantic segmentation.</li>
</ul>

<h3>Title: Self-Augmented In-Context Learning for Unsupervised Word Translation</h3>
<ul>
<li><strong>Authors: </strong>Yaoyiran Li, Anna Korhonen, Ivan Vulić</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10024">https://arxiv.org/abs/2402.10024</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10024">https://arxiv.org/pdf/2402.10024</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10024]] Self-Augmented In-Context Learning for Unsupervised Word Translation(https://arxiv.org/abs/2402.10024)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Recent work has shown that, while large language models (LLMs) demonstrate strong word translation or bilingual lexicon induction (BLI) capabilities in few-shot setups, they still cannot match the performance of 'traditional' mapping-based approaches in the unsupervised scenario where no seed translation pairs are available, especially for lower-resource languages. To address this challenge with LLMs, we propose self-augmented in-context learning (SAIL) for unsupervised BLI: starting from a zero-shot prompt, SAIL iteratively induces a set of high-confidence word translation pairs for in-context learning (ICL) from an LLM, which it then reapplies to the same LLM in the ICL fashion. Our method shows substantial gains over zero-shot prompting of LLMs on two established BLI benchmarks spanning a wide range of language pairs, also outperforming mapping-based baselines across the board. In addition to achieving state-of-the-art unsupervised BLI performance, we also conduct comprehensive analyses on SAIL and discuss its limitations.</li>
</ul>

<h3>Title: Diffusion Models Meet Contextual Bandits with Large Action Spaces</h3>
<ul>
<li><strong>Authors: </strong>Imad Aouali</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10028">https://arxiv.org/abs/2402.10028</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10028">https://arxiv.org/pdf/2402.10028</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10028]] Diffusion Models Meet Contextual Bandits with Large Action Spaces(https://arxiv.org/abs/2402.10028)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Efficient exploration is a key challenge in contextual bandits due to the large size of their action space, where uninformed exploration can result in computational and statistical inefficiencies. Fortunately, the rewards of actions are often correlated and this can be leveraged to explore them efficiently. In this work, we capture such correlations using pre-trained diffusion models; upon which we design diffusion Thompson sampling (dTS). Both theoretical and algorithmic foundations are developed for dTS, and empirical evaluation also shows its favorable performance.</li>
</ul>

<h3>Title: Review of the Learning-based Camera and Lidar Simulation Methods for  Autonomous Driving Systems</h3>
<ul>
<li><strong>Authors: </strong>Hamed Haghighi, Xiaomeng Wang, Hao Jing, Mehrdad Dianati</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10079">https://arxiv.org/abs/2402.10079</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10079">https://arxiv.org/pdf/2402.10079</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10079]] Review of the Learning-based Camera and Lidar Simulation Methods for  Autonomous Driving Systems(https://arxiv.org/abs/2402.10079)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Perception sensors, particularly camera and Lidar, are key elements of Autonomous Driving Systems (ADS) that enable them to comprehend their surroundings for informed driving and control decisions. Therefore, developing realistic camera and Lidar simulation methods, also known as camera and Lidar models, is of paramount importance to effectively conduct simulation-based testing for ADS. Moreover, the rise of deep learning-based perception models has propelled the prevalence of perception sensor models as valuable tools for synthesising diverse training datasets. The traditional sensor simulation methods rely on computationally expensive physics-based algorithms, specifically in complex systems such as ADS. Hence, the current potential resides in learning-based models, driven by the success of deep generative models in synthesising high-dimensional data. This paper reviews the current state-of-the-art in learning-based sensor simulation methods and validation approaches, focusing on two main types of perception sensors: cameras and Lidars. This review covers two categories of learning-based approaches, namely raw-data-based and object-based models. Raw-data-based methods are explained concerning the employed learning strategy, while object-based models are categorised based on the type of error considered. Finally, the paper illustrates commonly used validation techniques for evaluating perception sensor models and highlights the existing research gaps in the area.</li>
</ul>

<h3>Title: Classification Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Shahar Yadin, Noam Elata, Tomer Michaeli</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10095">https://arxiv.org/abs/2402.10095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10095">https://arxiv.org/pdf/2402.10095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10095]] Classification Diffusion Models(https://arxiv.org/abs/2402.10095)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>A prominent family of methods for learning data distributions relies on density ratio estimation (DRE), where a model is trained to $\textit{classify}$ between data samples and samples from some reference distribution. These techniques are successful in simple low-dimensional settings but fail to achieve good results on complex high-dimensional data, like images. A different family of methods for learning distributions is that of denoising diffusion models (DDMs), in which a model is trained to $\textit{denoise}$ data samples. These approaches achieve state-of-the-art results in image, video, and audio generation. In this work, we present $\textit{Classification Diffusion Models}$ (CDMs), a generative technique that adopts the denoising-based formalism of DDMs while making use of a classifier that predicts the amount of noise added to a clean signal, similarly to DRE methods. Our approach is based on the observation that an MSE-optimal denoiser for white Gaussian noise can be expressed in terms of the gradient of a cross-entropy-optimal classifier for predicting the noise level. As we illustrate, CDM achieves better denoising results compared to DDM, and leads to at least comparable FID in image generation. CDM is also capable of highly efficient one-step exact likelihood estimation, achieving state-of-the-art results among methods that use a single step. Code is available on the project's webpage in https://shaharYadin.github.io/CDM/ .</li>
</ul>

<h3>Title: Quantized Embedding Vectors for Controllable Diffusion Language Models</h3>
<ul>
<li><strong>Authors: </strong>Cheng Kang, Xinye Chen, Yong Hu, Daniel Novak</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10107">https://arxiv.org/abs/2402.10107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10107">https://arxiv.org/pdf/2402.10107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10107]] Quantized Embedding Vectors for Controllable Diffusion Language Models(https://arxiv.org/abs/2402.10107)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Improving the controllability, portability, and inference speed of diffusion language models (DLMs) is a key challenge in natural language generation. While recent research has shown significant success in complex text generation with language models, the memory and computational power are still very demanding and fall short of expectations, which naturally results in low portability and instability for the models. To mitigate these issues, numerous well-established methods were proposed for neural network quantization. To further enhance their portability of independent deployment as well as improve their stability evaluated by language perplexity, we propose a novel approach called the Quantized Embedding Controllable Diffusion Language Model (QE-CDLM). QE-CDLM builds upon the recent successful controllable DLMs by remodeling the task-specific embedding space via quantization. This leads to a gradient-based controller for the generation tasks, and more stable intermediate latent variables are obtained, which naturally brings in an accelerated convergence as well as better controllability. Additionally, the adaption fine-tuning method is employed to reduce tunable weights. Experimental results on five challenging fine-grained control tasks demonstrate that QE-CDLM compares favorably to existing methods in terms of quality and feasibility, achieving better perplexity and lightweight fine-tuning.</li>
</ul>

<h3>Title: Tracking Changing Probabilities via Dynamic Learners</h3>
<ul>
<li><strong>Authors: </strong>Omid Madani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10142">https://arxiv.org/abs/2402.10142</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10142">https://arxiv.org/pdf/2402.10142</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10142]] Tracking Changing Probabilities via Dynamic Learners(https://arxiv.org/abs/2402.10142)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Consider a predictor, a learner, whose input is a stream of discrete items. The predictor's task, at every time point, is probabilistic multiclass prediction, i.e., to predict which item may occur next by outputting zero or more candidate items, each with a probability, after which the actual item is revealed and the predictor learns from this observation. To output probabilities, the predictor keeps track of the proportions of the items it has seen. The predictor has constant (limited) space and we seek efficient prediction and update techniques: The stream is unbounded, the set of items is unknown to the predictor and their totality can also grow unbounded. Moreover, there is non-stationarity: the underlying frequencies of items may change, substantially, from time to time. For instance, new items may start appearing and a few currently frequent items may cease to occur again. The predictor, being space-bounded, need only provide probabilities for those items with (currently) sufficiently high frequency, i.e., the salient items. This problem is motivated in the setting of prediction games, a self-supervised learning regime where concepts serve as both the predictors and the predictands, and the set of concepts grows over time, resulting in non-stationarities as new concepts are generated and used. We develop moving average techniques designed to respond to such non-stationarities in a timely manner, and explore their properties. One is a simple technique based on queuing of count snapshots, and another is a combination of queuing together with an extended version of sparse EMA. The latter combination supports predictand-specific dynamic learning rates. We find that this flexibility allows for a more accurate and timely convergence.</li>
</ul>

<h3>Title: $f$-MICL: Understanding and Generalizing InfoNCE-based Contrastive  Learning</h3>
<ul>
<li><strong>Authors: </strong>Yiwei Lu, Guojun Zhang, Sun Sun, Hongyu Guo, Yaoliang Yu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10150">https://arxiv.org/abs/2402.10150</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10150">https://arxiv.org/pdf/2402.10150</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10150]] $f$-MICL: Understanding and Generalizing InfoNCE-based Contrastive  Learning(https://arxiv.org/abs/2402.10150)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>In self-supervised contrastive learning, a widely-adopted objective function is InfoNCE, which uses the heuristic cosine similarity for the representation comparison, and is closely related to maximizing the Kullback-Leibler (KL)-based mutual information. In this paper, we aim at answering two intriguing questions: (1) Can we go beyond the KL-based objective? (2) Besides the popular cosine similarity, can we design a better similarity function? We provide answers to both questions by generalizing the KL-based mutual information to the $f$-Mutual Information in Contrastive Learning ($f$-MICL) using the $f$-divergences. To answer the first question, we provide a wide range of $f$-MICL objectives which share the nice properties of InfoNCE (e.g., alignment and uniformity), and meanwhile result in similar or even superior performance. For the second question, assuming that the joint feature distribution is proportional to the Gaussian kernel, we derive an $f$-Gaussian similarity with better interpretability and empirical performance. Finally, we identify close relationships between the $f$-MICL objective and several popular InfoNCE-based objectives. Using benchmark tasks from both vision and natural language, we empirically evaluate $f$-MICL with different $f$-divergences on various architectures (SimCLR, MoCo, and MoCo v3) and datasets. We observe that $f$-MICL generally outperforms the benchmarks and the best-performing $f$-divergence is task and dataset dependent.</li>
</ul>

<h3>Title: Uncertainty Decomposition and Quantification for In-Context Learning of  Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Chen Ling, Xujiang Zhao, Wei Cheng, Yanchi Liu, Yiyou Sun, Xuchao Zhang, Mika Oishi, Takao Osaki, Katsushi Matsuda, Jie Ji, Guangji Bai, Liang Zhao, Haifeng Chen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10189">https://arxiv.org/abs/2402.10189</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10189">https://arxiv.org/pdf/2402.10189</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10189]] Uncertainty Decomposition and Quantification for In-Context Learning of  Large Language Models(https://arxiv.org/abs/2402.10189)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>In-context learning has emerged as a groundbreaking ability of Large Language Models (LLMs) and revolutionized various fields by providing a few task-relevant demonstrations in the prompt. However, trustworthy issues with LLM's response, such as hallucination, have also been actively discussed. Existing works have been devoted to quantifying the uncertainty in LLM's response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning. In this work, we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model's configurations (epistemic uncertainty). We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties. The proposed method offers an unsupervised way to understand the prediction of in-context learning in a plug-and-play fashion. Extensive experiments are conducted to demonstrate the effectiveness of the decomposition. The code and data are available at: \url{https://github.com/lingchen0331/UQ_ICL}.</li>
</ul>

<h3>Title: Bridging Associative Memory and Probabilistic Modeling</h3>
<ul>
<li><strong>Authors: </strong>Rylan Schaeffer, Nika Zahedi, Mikail Khona, Dhruv Pai, Sang Truong, Yilun Du, Mitchell Ostrow, Sarthak Chandra, Andres Carranza, Ila Rani Fiete, Andrey Gromov, Sanmi Koyejo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10202">https://arxiv.org/abs/2402.10202</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10202">https://arxiv.org/pdf/2402.10202</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10202]] Bridging Associative Memory and Probabilistic Modeling(https://arxiv.org/abs/2402.10202)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Associative memory and probabilistic modeling are two fundamental topics in artificial intelligence. The first studies recurrent neural networks designed to denoise, complete and retrieve data, whereas the second studies learning and sampling from probability distributions. Based on the observation that associative memory's energy functions can be seen as probabilistic modeling's negative log likelihoods, we build a bridge between the two that enables useful flow of ideas in both directions. We showcase four examples: First, we propose new energy-based models that flexibly adapt their energy functions to new in-context datasets, an approach we term \textit{in-context learning of energy functions}. Second, we propose two new associative memory models: one that dynamically creates new memories as necessitated by the training data using Bayesian nonparametrics, and another that explicitly computes proportional memory assignments using the evidence lower bound. Third, using tools from associative memory, we analytically and numerically characterize the memory capacity of Gaussian kernel density estimators, a widespread tool in probababilistic modeling. Fourth, we study a widespread implementation choice in transformers -- normalization followed by self attention -- to show it performs clustering on the hypersphere. Altogether, this work urges further exchange of useful ideas between these two continents of artificial intelligence.</li>
</ul>

<h3>Title: Rewards-in-Context: Multi-objective Alignment of Foundation Models with  Dynamic Preference Adjustment</h3>
<ul>
<li><strong>Authors: </strong>Rui Yang, Xiaoman Pan, Feng Luo, Shuang Qiu, Han Zhong, Dong Yu, Jianshu Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10207">https://arxiv.org/abs/2402.10207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10207">https://arxiv.org/pdf/2402.10207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10207]] Rewards-in-Context: Multi-objective Alignment of Foundation Models with  Dynamic Preference Adjustment(https://arxiv.org/abs/2402.10207)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model, in-context</a></li>
<li><strong>Abstract: </strong>We consider the problem of multi-objective alignment of foundation models with human preferences, which is a critical step towards helpful and harmless AI systems. However, it is generally costly and unstable to fine-tune large foundation models using reinforcement learning (RL), and the multi-dimensionality, heterogeneity, and conflicting nature of human preferences further complicate the alignment process. In this paper, we introduce Rewards-in-Context (RiC), which conditions the response of a foundation model on multiple rewards in its prompt context and applies supervised fine-tuning for alignment. The salient features of RiC are simplicity and adaptivity, as it only requires supervised fine-tuning of a single foundation model and supports dynamic adjustment for user preferences during inference time. Inspired by the analytical solution of an abstracted convex optimization problem, our dynamic inference-time adjustment method approaches the Pareto-optimal solution for multiple objectives. Empirical evidence demonstrates the efficacy of our method in aligning both Large Language Models (LLMs) and diffusion models to accommodate diverse rewards with only around $10\%$ GPU hours compared with multi-objective RL baseline.</li>
</ul>

<h3>Title: Recovering the Pre-Fine-Tuning Weights of Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Eliahu Horwitz, Jonathan Kahana, Yedid Hoshen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CR, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10208">https://arxiv.org/abs/2402.10208</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10208">https://arxiv.org/pdf/2402.10208</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10208]] Recovering the Pre-Fine-Tuning Weights of Generative Models(https://arxiv.org/abs/2402.10208)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The dominant paradigm in generative modeling consists of two steps: i) pre-training on a large-scale but unsafe dataset, ii) aligning the pre-trained model with human values via fine-tuning. This practice is considered safe, as no current method can recover the unsafe, pre-fine-tuning model weights. In this paper, we demonstrate that this assumption is often false. Concretely, we present Spectral DeTuning, a method that can recover the weights of the pre-fine-tuning model using a few low-rank (LoRA) fine-tuned models. In contrast to previous attacks that attempt to recover pre-fine-tuning capabilities, our method aims to recover the exact pre-fine-tuning weights. Our approach exploits this new vulnerability against large-scale models such as a personalized Stable Diffusion and an aligned Mistral.</li>
</ul>

<h3>Title: Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Huizhuo Yuan, Zixiang Chen, Kaixuan Ji, Quanquan Gu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.10210">https://arxiv.org/abs/2402.10210</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.10210">https://arxiv.org/pdf/2402.10210</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.10210]] Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation(https://arxiv.org/abs/2402.10210)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Fine-tuning Diffusion Models remains an underexplored frontier in generative artificial intelligence (GenAI), especially when compared with the remarkable progress made in fine-tuning Large Language Models (LLMs). While cutting-edge diffusion models such as Stable Diffusion (SD) and SDXL rely on supervised fine-tuning, their performance inevitably plateaus after seeing a certain volume of data. Recently, reinforcement learning (RL) has been employed to fine-tune diffusion models with human preference data, but it requires at least two images ("winner" and "loser" images) for each text prompt. In this paper, we introduce an innovative technique called self-play fine-tuning for diffusion models (SPIN-Diffusion), where the diffusion model engages in competition with its earlier versions, facilitating an iterative self-improvement process. Our approach offers an alternative to conventional supervised fine-tuning and RL strategies, significantly improving both model performance and alignment. Our experiments on the Pick-a-Pic dataset reveal that SPIN-Diffusion outperforms the existing supervised fine-tuning method in aspects of human preference alignment and visual appeal right from its first iteration. By the second iteration, it exceeds the performance of RLHF-based methods across all metrics, achieving these results with less data.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
