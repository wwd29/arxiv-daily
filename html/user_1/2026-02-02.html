<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2026-02-02</h1>
<h3>Title: MemeChain: A Multimodal Cross-Chain Dataset for Meme Coin Forensics and Risk Analysis</h3>
<ul>
<li><strong>Authors: </strong>Alberto Maria Mongardini, Alessandro Mei</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22185">https://arxiv.org/abs/2601.22185</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22185">https://arxiv.org/pdf/2601.22185</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22185]] MemeChain: A Multimodal Cross-Chain Dataset for Meme Coin Forensics and Risk Analysis(https://arxiv.org/abs/2601.22185)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>The meme coin ecosystem has grown into one of the most active yet least observable segments of the cryptocurrency market, characterized by extreme churn, minimal project commitment, and widespread fraudulent behavior. While countless meme coins are deployed across multiple blockchains, they rely heavily on off-chain web and social infrastructure to signal legitimacy. These very signals are largely absent from existing datasets, which are often limited to single-chain data or lack the multimodal artifacts required for comprehensive risk modeling. To address this gap, we introduce MemeChain, a large-scale, open-source, cross-chain dataset comprising 34,988 meme coins across Ethereum, BNB Smart Chain, Solana, and Base. MemeChain integrates on-chain data with off-chain artifacts, including website HTML source code, token logos, and linked social media accounts, enabling multimodal and forensic study of meme coin projects. Analysis of the dataset shows that visual branding is frequently omitted in low-effort deployments, and many projects lack a functional website. Moreover, we quantify the ecosystem's extreme volatility, identifying 1,801 tokens (5.15%) that cease all trading activity within just 24 hours of launch. By providing unified cross-chain coverage and rich off-chain context, MemeChain serves as a foundational resource for research in financial forensics, multimodal anomaly detection, and automated scam prevention in the meme coin ecosystem.</li>
</ul>

<h3>Title: Neural Signals Generate Clinical Notes in the Wild</h3>
<ul>
<li><strong>Authors: </strong>Jathurshan Pradeepkumar, Zheng Chen, Jimeng Sun</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22197">https://arxiv.org/abs/2601.22197</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22197">https://arxiv.org/pdf/2601.22197</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22197]] Neural Signals Generate Clinical Notes in the Wild(https://arxiv.org/abs/2601.22197)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Generating clinical reports that summarize abnormal patterns, diagnostic findings, and clinical interpretations from long-term EEG recordings remains labor-intensive. We curate a large-scale clinical EEG dataset with $9{,}922$ reports paired with approximately $11{,}000$ hours of EEG recordings from $9{,}048$ patients. We therefore develop CELM, the first clinical EEG-to-Language foundation model capable of summarizing long-duration, variable-length EEG recordings and performing end-to-end clinical report generation at multiple scales, including recording description, background activity, epileptiform abnormalities, events/seizures, and impressions. Experimental results show that, with patient history supervision, our method achieves $70\%$--$95\%$ average relative improvements in standard generation metrics (e.g., ROUGE-1 and METEOR) from $0.2$--$0.3$ to $0.4$--$0.6$. In the zero-shot setting without patient history, CELM attains generation scores in the range of $0.43$--$0.52$, compared to baselines of $0.17$--$0.26$. CELM integrates pretrained EEG foundation models with language models to enable scalable multimodal learning. We release our model and benchmark construction pipeline at [URL].</li>
</ul>

<h3>Title: Latent Spherical Flow Policy for Reinforcement Learning with Combinatorial Actions</h3>
<ul>
<li><strong>Authors: </strong>Lingkai Kong, Anagha Satish, Hezi Jiang, Akseli Kangaslahti, Andrew Ma, Wenbo Chen, Mingxiao Song, Lily Xu, Milind Tambe</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22211">https://arxiv.org/abs/2601.22211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22211">https://arxiv.org/pdf/2601.22211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22211]] Latent Spherical Flow Policy for Reinforcement Learning with Combinatorial Actions(https://arxiv.org/abs/2601.22211)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Reinforcement learning (RL) with combinatorial action spaces remains challenging because feasible action sets are exponentially large and governed by complex feasibility constraints, making direct policy parameterization impractical. Existing approaches embed task-specific value functions into constrained optimization programs or learn deterministic structured policies, sacrificing generality and policy expressiveness. We propose a solver-induced \emph{latent spherical flow policy} that brings the expressiveness of modern generative policies to combinatorial RL while guaranteeing feasibility by design. Our method, LSFlow, learns a \emph{stochastic} policy in a compact continuous latent space via spherical flow matching, and delegates feasibility to a combinatorial optimization solver that maps each latent sample to a valid structured action. To improve efficiency, we train the value network directly in the latent space, avoiding repeated solver calls during policy optimization. To address the piecewise-constant and discontinuous value landscape induced by solver-based action selection, we introduce a smoothed Bellman operator that yields stable, well-defined learning targets. Empirically, our approach outperforms state-of-the-art baselines by an average of 20.6\% across a range of challenging combinatorial RL tasks.</li>
</ul>

<h3>Title: A Systematic Literature Review on LLM Defenses Against Prompt Injection and Jailbreaking: Expanding NIST Taxonomy</h3>
<ul>
<li><strong>Authors: </strong>Pedro H. Barcha Correia, Ryan W. Achjian, Diego E. G. Caetano de Oliveira, Ygor Acacio Maria, Victor Takashi Hayashi, Marcos Lopes, Charles Christian Miers, Marcos A. Simplicio Jr</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22240">https://arxiv.org/abs/2601.22240</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22240">https://arxiv.org/pdf/2601.22240</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22240]] A Systematic Literature Review on LLM Defenses Against Prompt Injection and Jailbreaking: Expanding NIST Taxonomy(https://arxiv.org/abs/2601.22240)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid advancement and widespread adoption of generative artificial intelligence (GenAI) and large language models (LLMs) has been accompanied by the emergence of new security vulnerabilities and challenges, such as jailbreaking and other prompt injection attacks. These maliciously crafted inputs can exploit LLMs, causing data leaks, unauthorized actions, or compromised outputs, for instance. As both offensive and defensive prompt injection techniques evolve quickly, a structured understanding of mitigation strategies becomes increasingly important. To address that, this work presents the first systematic literature review on prompt injection mitigation strategies, comprehending 88 studies. Building upon NIST's report on adversarial machine learning, this work contributes to the field through several avenues. First, it identifies studies beyond those documented in NIST's report and other academic reviews and surveys. Second, we propose an extension to NIST taxonomy by introducing additional categories of defenses. Third, by adopting NIST's established terminology and taxonomy as a foundation, we promote consistency and enable future researchers to build upon the standardized taxonomy proposed in this work. Finally, we provide a comprehensive catalog of the reviewed prompt injection defenses, documenting their reported quantitative effectiveness across specific LLMs and attack datasets, while also indicating which solutions are open-source and model-agnostic. This catalog, together with the guidelines presented herein, aims to serve as a practical resource for researchers advancing the field of adversarial machine learning and for developers seeking to implement effective defenses in production systems.</li>
</ul>

<h3>Title: Is Hierarchical Quantization Essential for Optimal Reconstruction?</h3>
<ul>
<li><strong>Authors: </strong>Shirin Reyhanian, Laurenz Wiskott</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22244">https://arxiv.org/abs/2601.22244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22244">https://arxiv.org/pdf/2601.22244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22244]] Is Hierarchical Quantization Essential for Optimal Reconstruction?(https://arxiv.org/abs/2601.22244)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Vector-quantized variational autoencoders (VQ-VAEs) are central to models that rely on high reconstruction fidelity, from neural compression to generative pipelines. Hierarchical extensions, such as VQ-VAE2, are often credited with superior reconstruction performance because they split global and local features across multiple levels. However, since higher levels derive all their information from lower levels, they should not carry additional reconstructive content beyond what the lower-level already encodes. Combined with recent advances in training objectives and quantization mechanisms, this leads us to ask whether a single-level VQ-VAE, with matched representational budget and no codebook collapse, can equal the reconstruction fidelity of its hierarchical counterpart. Although the multi-scale structure of hierarchical models may improve perceptual quality in downstream tasks, the effect of hierarchy on reconstruction accuracy, isolated from codebook utilization and overall representational capacity, remains empirically underexamined. We revisit this question by comparing a two-level VQ-VAE and a capacity-matched single-level model on high-resolution ImageNet images. Consistent with prior observations, we confirm that inadequate codebook utilization limits single-level VQ-VAEs and that overly high-dimensional embeddings destabilize quantization and increase codebook collapse. We show that lightweight interventions such as initialization from data, periodic reset of inactive codebook vectors, and systematic tuning of codebook hyperparameters significantly reduce collapse. Our results demonstrate that when representational budgets are matched, and codebook collapse is mitigated, single-level VQ-VAEs can match the reconstruction fidelity of hierarchical variants, challenging the assumption that hierarchical quantization is inherently superior for high-quality reconstructions.</li>
</ul>

<h3>Title: Tabular Foundation Models Can Do Survival Analysis</h3>
<ul>
<li><strong>Authors: </strong>Da In Kim, Wei Siang Lai, Kelly W. Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22259">https://arxiv.org/abs/2601.22259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22259">https://arxiv.org/pdf/2601.22259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22259]] Tabular Foundation Models Can Do Survival Analysis(https://arxiv.org/abs/2601.22259)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, in-context</a></li>
<li><strong>Abstract: </strong>While tabular foundation models have achieved remarkable success in classification and regression, adapting them to model time-to-event outcomes for survival analysis is non-trivial due to right-censoring, where data observations may end before the event occurs. We develop a classification-based framework that reformulates both static and dynamic survival analysis as a series of binary classification problems by discretizing event times. Censored observations are naturally handled as examples with missing labels at certain time points. This classification formulation enables existing tabular foundation models to perform survival analysis through in-context learning without explicit training. We prove that under standard censoring assumptions, minimizing our binary classification loss recovers the true survival probabilities as the training set size increases. We demonstrate through evaluation across $53$ real-world datasets that off-the-shelf tabular foundation models with this classification formulation outperform classical and deep learning baselines on average over multiple survival metrics.</li>
</ul>

<h3>Title: VMonarch: Efficient Video Diffusion Transformers with Structured Attention</h3>
<ul>
<li><strong>Authors: </strong>Cheng Liang, Haoxian Chen, Liang Hou, Qi Fan, Gangshan Wu, Xin Tao, Limin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22275">https://arxiv.org/abs/2601.22275</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22275">https://arxiv.org/pdf/2601.22275</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22275]] VMonarch: Efficient Video Diffusion Transformers with Structured Attention(https://arxiv.org/abs/2601.22275)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The quadratic complexity of the attention mechanism severely limits the context scalability of Video Diffusion Transformers (DiTs). We find that the highly sparse spatio-temporal attention patterns exhibited in Video DiTs can be naturally represented by the Monarch matrix. It is a class of structured matrices with flexible sparsity, enabling sub-quadratic attention via an alternating minimization algorithm. Accordingly, we propose VMonarch, a novel attention mechanism for Video DiTs that enables efficient computation over the dynamic sparse patterns with structured Monarch matrices. First, we adapt spatio-temporal Monarch factorization to explicitly capture the intra-frame and inter-frame correlations of the video data. Second, we introduce a recomputation strategy to mitigate artifacts arising from instabilities during alternating minimization of Monarch matrices. Third, we propose a novel online entropy algorithm fused into FlashAttention, enabling fast Monarch matrix updates for long sequences. Extensive experiments demonstrate that VMonarch achieves comparable or superior generation quality to full attention on VBench after minimal tuning. It overcomes the attention bottleneck in Video DiTs, reduces attention FLOPs by a factor of 17.5, and achieves a speedup of over 5x in attention computation for long videos, surpassing state-of-the-art sparse attention methods at 90% sparsity.</li>
</ul>

<h3>Title: SurrogateSHAP: Training-Free Contributor Attribution for Text-to-Image (T2I) Models</h3>
<ul>
<li><strong>Authors: </strong>Mingyu Lu, Soham Gadgil, Chris Lin, Chanwoo Kim, Su-In Lee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22276">https://arxiv.org/abs/2601.22276</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22276">https://arxiv.org/pdf/2601.22276</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22276]] SurrogateSHAP: Training-Free Contributor Attribution for Text-to-Image (T2I) Models(https://arxiv.org/abs/2601.22276)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>As Text-to-Image (T2I) diffusion models are increasingly used in real-world creative workflows, a principled framework for valuing contributors who provide a collection of data is essential for fair compensation and sustainable data marketplaces. While the Shapley value offers a theoretically grounded approach to attribution, it faces a dual computational bottleneck: (i) the prohibitive cost of exhaustive model retraining for each sampled subset of players (i.e., data contributors) and (ii) the combinatorial number of subsets needed to estimate marginal contributions due to contributor interactions. To this end, we propose SurrogateSHAP, a retraining-free framework that approximates the expensive retraining game through inference from a pretrained model. To further improve efficiency, we employ a gradient-boosted tree to approximate the utility function and derive Shapley values analytically from the tree-based model. We evaluate SurrogateSHAP across three diverse attribution tasks: (i) image quality for DDPM-CFG on CIFAR-20, (ii) aesthetics for Stable Diffusion on Post-Impressionist artworks, and (iii) product diversity for FLUX.1 on Fashion-Product data. Across settings, SurrogateSHAP outperforms prior methods while substantially reducing computational overhead, consistently identifying influential contributors across multiple utility metrics. Finally, we demonstrate that SurrogateSHAP effectively localizes data sources responsible for spurious correlations in clinical images, providing a scalable path toward auditing safety-critical generative models.</li>
</ul>

<h3>Title: Conformal Prediction for Generative Models via Adaptive Cluster-Based Density Estimation</h3>
<ul>
<li><strong>Authors: </strong>Qidong Yang, Qianyu Julie Zhu, Jonathan Giezendanner, Youssef Marzouk, Stephen Bates, Sherrie Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.ao-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22298">https://arxiv.org/abs/2601.22298</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22298">https://arxiv.org/pdf/2601.22298</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22298]] Conformal Prediction for Generative Models via Adaptive Cluster-Based Density Estimation(https://arxiv.org/abs/2601.22298)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Conditional generative models map input variables to complex, high-dimensional distributions, enabling realistic sample generation in a diverse set of domains. A critical challenge with these models is the absence of calibrated uncertainty, which undermines trust in individual outputs for high-stakes applications. To address this issue, we propose a systematic conformal prediction approach tailored to conditional generative models, leveraging density estimation on model-generated samples. We introduce a novel method called CP4Gen, which utilizes clustering-based density estimation to construct prediction sets that are less sensitive to outliers, more interpretable, and of lower structural complexity than existing methods. Extensive experiments on synthetic datasets and real-world applications, including climate emulation tasks, demonstrate that CP4Gen consistently achieves superior performance in terms of prediction set volume and structural simplicity. Our approach offers practitioners a powerful tool for uncertainty estimation associated with conditional generative models, particularly in scenarios demanding rigorous and interpretable prediction sets.</li>
</ul>

<h3>Title: Coarse-to-Real: Generative Rendering for Populated Dynamic Scenes</h3>
<ul>
<li><strong>Authors: </strong>Gonzalo Gomez-Nogales, Yicong Hong, Chongjian Ge, Marc Comino-Trinidad, Dan Casas, Yi Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22301">https://arxiv.org/abs/2601.22301</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22301">https://arxiv.org/pdf/2601.22301</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22301]] Coarse-to-Real: Generative Rendering for Populated Dynamic Scenes(https://arxiv.org/abs/2601.22301)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Traditional rendering pipelines rely on complex assets, accurate materials and lighting, and substantial computational resources to produce realistic imagery, yet they still face challenges in scalability and realism for populated dynamic scenes. We present C2R (Coarse-to-Real), a generative rendering framework that synthesizes real-style urban crowd videos from coarse 3D simulations. Our approach uses coarse 3D renderings to explicitly control scene layout, camera motion, and human trajectories, while a learned neural renderer generates realistic appearance, lighting, and fine-scale dynamics guided by text prompts. To overcome the lack of paired training data between coarse simulations and real videos, we adopt a two-phase mixed CG-real training strategy that learns a strong generative prior from large-scale real footage and introduces controllability through shared implicit spatio-temporal features across domains. The resulting system supports coarse-to-fine control, generalizes across diverse CG and game inputs, and produces temporally consistent, controllable, and realistic urban scene videos from minimal 3D input. We will release the model and project webpage at this https URL.</li>
</ul>

<h3>Title: SCALAR: Quantifying Structural Hallucination, Consistency, and Reasoning Gaps in Materials Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Can Polat, Erchin Serpedin, Mustafa Kurban, Hasan Kurban</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.mtrl-sci, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22312">https://arxiv.org/abs/2601.22312</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22312">https://arxiv.org/pdf/2601.22312</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22312]] SCALAR: Quantifying Structural Hallucination, Consistency, and Reasoning Gaps in Materials Foundation Models(https://arxiv.org/abs/2601.22312)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Large language models are increasingly applied to materials science reasoning, yet their behavior under physically structured distribution shifts remains poorly understood. We introduce SCALAR (Structural Consistency And Logic Across Regimes), a benchmark for evaluating geometric scale generalization and its connection to structural hallucination, consistency, and reasoning in materials foundation models. Given canonical crystal representations, models must reason about derived nanoparticle structures obtained through supercell expansion and geometric truncation across length scales spanning a few atoms to over 18,000 atoms, totaling $\approx$100,000 structures from DFT-validated unit cells. SCALAR defines three tasks. (i) CIF to property prediction. (ii) A Chain-of-Thought variant with explicit physics-grounded reasoning. (iii) Inverse retrieval identifying crystals from candidates given target properties. Outputs are evaluated via structured metrics capturing numeric error, hallucination, cross-prompt consistency, monotonic reasoning, output validity, and retrieval regret. Experiments across diverse foundation models reveal large, model-dependent shifts under explicit reasoning, often reducing hallucination and error, but frequently destabilizing consistency or validity. These results demonstrate that geometric scale generalization cannot be inferred from accuracy alone. Supplementary materials are available at this https URL.</li>
</ul>

<h3>Title: MixQuant: Pushing the Limits of Block Rotations in Post-Training Quantization</h3>
<ul>
<li><strong>Authors: </strong>Sai Sanjeet, Ian Colbert, Pablo Monteagudo-Lago, Giuseppe Franco, Yaman Umuroglu, Nicholas J. Fraser</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22347">https://arxiv.org/abs/2601.22347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22347">https://arxiv.org/pdf/2601.22347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22347]] MixQuant: Pushing the Limits of Block Rotations in Post-Training Quantization(https://arxiv.org/abs/2601.22347)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent post-training quantization (PTQ) methods have adopted block rotations to diffuse outliers prior to rounding. While this reduces the overhead of full-vector rotations, the effect of block structure on outlier suppression remains poorly understood. To fill this gap, we present the first systematic, non-asymptotic analysis of outlier suppression for block Hadamard rotations. Our analysis reveals that outlier suppression is fundamentally limited by the geometry of the input vector. In particular, post-rotation outliers are deterministically minimized when the pre-rotation $\ell_1$ norm mass is evenly distributed across blocks. Guided by these insights, we introduce MixQuant, a block rotation-aware PTQ framework that redistributes activation mass via permutations prior to rotation. We propose a greedy mass diffusion algorithm to calibrate permutations by equalizing the expected blockwise $\ell_1$ norms. To avoid adding inference overhead, we identify permutation-equivariant regions in transformer architectures to merge the resulting permutations into model weights before deployment. Experiments show that MixQuant consistently improves accuracy across all block sizes, recovering up to 90% of the full-vector rotation perplexity when quantizing Llama3 1B to INT4 with block size 16, compared to 46% without permutations.</li>
</ul>

<h3>Title: Learning Policy Representations for Steerable Behavior Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Beiming Li, Sergio Rozada, Alejandro Ribeiro</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22350">https://arxiv.org/abs/2601.22350</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22350">https://arxiv.org/pdf/2601.22350</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22350]] Learning Policy Representations for Steerable Behavior Synthesis(https://arxiv.org/abs/2601.22350)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Given a Markov decision process (MDP), we seek to learn representations for a range of policies to facilitate behavior steering at test time. As policies of an MDP are uniquely determined by their occupancy measures, we propose modeling policy representations as expectations of state-action feature maps with respect to occupancy measures. We show that these representations can be approximated uniformly for a range of policies using a set-based architecture. Our model encodes a set of state-action samples into a latent embedding, from which we decode both the policy and its value functions corresponding to multiple rewards. We use variational generative approach to induce a smooth latent space, and further shape it with contrastive learning so that latent distances align with differences in value functions. This geometry permits gradient-based optimization directly in the latent space. Leveraging this capability, we solve a novel behavior synthesis task, where policies are steered to satisfy previously unseen value function constraints without additional training.</li>
</ul>

<h3>Title: Context Structure Reshapes the Representational Geometry of Language Models</h3>
<ul>
<li><strong>Authors: </strong>Eghbal A. Hosseini, Yuxuan Li, Yasaman Bahri, Declan Campbell, Andrew Kyle Lampinen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22364">https://arxiv.org/abs/2601.22364</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22364">https://arxiv.org/pdf/2601.22364</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22364]] Context Structure Reshapes the Representational Geometry of Language Models(https://arxiv.org/abs/2601.22364)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have been shown to organize the representations of input sequences into straighter neural trajectories in their deep layers, which has been hypothesized to facilitate next-token prediction via linear extrapolation. Language models can also adapt to diverse tasks and learn new structure in context, and recent work has shown that this in-context learning (ICL) can be reflected in representational changes. Here we bring these two lines of research together to explore whether representation straightening occurs \emph{within} a context during ICL. We measure representational straightening in Gemma 2 models across a diverse set of in-context tasks, and uncover a dichotomy in how LLMs' representations change in context. In continual prediction settings (e.g., natural language, grid world traversal tasks) we observe that increasing context increases the straightness of neural sequence trajectories, which is correlated with improvement in model prediction. Conversely, in structured prediction settings (e.g., few-shot tasks), straightening is inconsistent -- it is only present in phases of the task with explicit structure (e.g., repeating a template), but vanishes elsewhere. These results suggest that ICL is not a monolithic process. Instead, we propose that LLMs function like a Swiss Army knife: depending on task structure, the LLM dynamically selects between strategies, only some of which yield representational straightening.</li>
</ul>

<h3>Title: FIRE: Multi-fidelity Regression with Distribution-conditioned In-context Learning using Tabular Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Rosen Ting-Ying Yu, Nicholas Sung, Faez Ahmed</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22371">https://arxiv.org/abs/2601.22371</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22371">https://arxiv.org/pdf/2601.22371</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22371]] FIRE: Multi-fidelity Regression with Distribution-conditioned In-context Learning using Tabular Foundation Models(https://arxiv.org/abs/2601.22371)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, in-context</a></li>
<li><strong>Abstract: </strong>Multi-fidelity (MF) regression often operates in regimes of extreme data imbalance, where the commonly-used Gaussian-process (GP) surrogates struggle with cubic scaling costs and overfit to sparse high-fidelity observations, limiting efficiency and generalization in real-world applications. We introduce FIRE, a training-free MF framework that couples tabular foundation models (TFMs) to perform zero-shot in-context Bayesian inference via a high-fidelity correction model conditioned on the low-fidelity model's posterior predictive distributions. This cross-fidelity information transfer via distributional summaries captures heteroscedastic errors, enabling robust residual learning without model retraining. Across 31 benchmark problems spanning synthetic and real-world tasks (e.g., DrivAerNet, LCBench), FIRE delivers a stronger performance-time trade-off than seven state-of-the-art GP-based or deep learning MF regression methods, ranking highest in accuracy and uncertainty quantification with runtime advantages. Limitations include context window constraints and dependence on the quality of the pre-trained TFM's.</li>
</ul>

<h3>Title: FlexMap: Generalized HD Map Construction from Flexible Camera Configurations</h3>
<ul>
<li><strong>Authors: </strong>Run Wang, Chaoyi Zhou, Amir Salarpour, Xi Liu, Zhi-Qi Cheng, Feng Luo, Mert D. Pes√©, Siyu Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22376">https://arxiv.org/abs/2601.22376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22376">https://arxiv.org/pdf/2601.22376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22376]] FlexMap: Generalized HD Map Construction from Flexible Camera Configurations(https://arxiv.org/abs/2601.22376)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>High-definition (HD) maps provide essential semantic information of road structures for autonomous driving systems, yet current HD map construction methods require calibrated multi-camera setups and either implicit or explicit 2D-to-BEV transformations, making them fragile when sensors fail or camera configurations vary across vehicle fleets. We introduce FlexMap, unlike prior methods that are fixed to a specific N-camera rig, our approach adapts to variable camera configurations without any architectural changes or per-configuration retraining. Our key innovation eliminates explicit geometric projections by using a geometry-aware foundation model with cross-frame attention to implicitly encode 3D scene understanding in feature space. FlexMap features two core components: a spatial-temporal enhancement module that separates cross-view spatial reasoning from temporal dynamics, and a camera-aware decoder with latent camera tokens, enabling view-adaptive attention without the need for projection matrices. Experiments demonstrate that FlexMap outperforms existing methods across multiple configurations while maintaining robustness to missing views and sensor variations, enabling more practical real-world deployment.</li>
</ul>

<h3>Title: SAIR: Cost-Efficient Multi-Stage ML Pipeline Autoscaling via In-Context Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Jianchang Su, Yifan Zhang, Shengkai Lin, Shizhen Zhao, Yusheng Zheng, Yiwei Yang, Wei Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22397">https://arxiv.org/abs/2601.22397</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22397">https://arxiv.org/pdf/2601.22397</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22397]] SAIR: Cost-Efficient Multi-Stage ML Pipeline Autoscaling via In-Context Reinforcement Learning(https://arxiv.org/abs/2601.22397)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Multi-stage ML inference pipelines are difficult to autoscale due to heterogeneous resources, cross-stage coupling, and dynamic bottleneck migration. We present SAIR, an autoscaling framework that uses an LLM as an in-context reinforcement learning controller, improving its policy online from reward-labeled interaction histories without gradient updates. SAIR combines Pareto-dominance reward shaping with a provable separation margin, surprisal-guided experience retrieval for context efficiency, and fine-grained GPU rate control via user-space CUDA interception. We provide regret analysis decomposing error into retrieval coverage and LLM selection components. On four ML serving pipelines under three workload patterns, SAIR achieves the best or tied-best P99 latency and effective resource cost among deployed baselines, improving P99 by up to 50% and reducing effective cost by up to 97% (under GPU rate-control assumptions), with 86% bottleneck detection accuracy and no offline training.</li>
</ul>

<h3>Title: Score-based Integrated Gradient for Root Cause Explanations of Outliers</h3>
<ul>
<li><strong>Authors: </strong>Phuoc Nguyen, Truyen Tran, Sunil Gupta, Svetha Venkatesh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22399">https://arxiv.org/abs/2601.22399</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22399">https://arxiv.org/pdf/2601.22399</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22399]] Score-based Integrated Gradient for Root Cause Explanations of Outliers(https://arxiv.org/abs/2601.22399)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Identifying the root causes of outliers is a fundamental problem in causal inference and anomaly detection. Traditional approaches based on heuristics or counterfactual reasoning often struggle under uncertainty and high-dimensional dependencies. We introduce SIREN, a novel and scalable method that attributes the root causes of outliers by estimating the score functions of the data likelihood. Attribution is computed via integrated gradients that accumulate score contributions along paths from the outlier toward the normal data distribution. Our method satisfies three of the four classic Shapley value axioms - dummy, efficiency, and linearity - as well as an asymmetry axiom derived from the underlying causal structure. Unlike prior work, SIREN operates directly on the score function, enabling tractable and uncertainty-aware root cause attribution in nonlinear, high-dimensional, and heteroscedastic causal models. Extensive experiments on synthetic random graphs and real-world cloud service and supply chain datasets show that SIREN outperforms state-of-the-art baselines in both attribution accuracy and computational efficiency.</li>
</ul>

<h3>Title: Rethinking Anonymity Claims in Synthetic Data Generation: A Model-Centric Privacy Attack Perspective</h3>
<ul>
<li><strong>Authors: </strong>Georgi Ganev, Emiliano De Cristofaro</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22434">https://arxiv.org/abs/2601.22434</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22434">https://arxiv.org/pdf/2601.22434</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22434]] Rethinking Anonymity Claims in Synthetic Data Generation: A Model-Centric Privacy Attack Perspective(https://arxiv.org/abs/2601.22434)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Training generative machine learning models to produce synthetic tabular data has become a popular approach for enhancing privacy in data sharing. As this typically involves processing sensitive personal information, releasing either the trained model or generated synthetic datasets can still pose privacy risks. Yet, recent research, commercial deployments, and privacy regulations like the General Data Protection Regulation (GDPR) largely assess anonymity at the level of an individual dataset. In this paper, we rethink anonymity claims about synthetic data from a model-centric perspective and argue that meaningful assessments must account for the capabilities and properties of the underlying generative model and be grounded in state-of-the-art privacy attacks. This perspective better reflects real-world products and deployments, where trained models are often readily accessible for interaction or querying. We interpret the GDPR's definitions of personal data and anonymization under such access assumptions to identify the types of identifiability risks that must be mitigated and map them to privacy attacks across different threat settings. We then argue that synthetic data techniques alone do not ensure sufficient anonymization. Finally, we compare the two mechanisms most commonly used alongside synthetic data -- Differential Privacy (DP) and Similarity-based Privacy Metrics (SBPMs) -- and argue that while DP can offer robust protections against identifiability risks, SBPMs lack adequate safeguards. Overall, our work connects regulatory notions of identifiability with model-centric privacy attacks, enabling more responsible and trustworthy regulatory assessment of synthetic data systems by researchers, practitioners, and policymakers.</li>
</ul>

<h3>Title: Weak Diffusion Priors Can Still Achieve Strong Inverse-Problem Performance</h3>
<ul>
<li><strong>Authors: </strong>Jing Jia, Wei Yuan, Sifan Liu, Liyue Shen, Guanyang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, stat.CO, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22443">https://arxiv.org/abs/2601.22443</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22443">https://arxiv.org/pdf/2601.22443</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22443]] Weak Diffusion Priors Can Still Achieve Strong Inverse-Problem Performance(https://arxiv.org/abs/2601.22443)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Can a diffusion model trained on bedrooms recover human faces? Diffusion models are widely used as priors for inverse problems, but standard approaches usually assume a high-fidelity model trained on data that closely match the unknown signal. In practice, one often must use a mismatched or low-fidelity diffusion prior. Surprisingly, these weak priors often perform nearly as well as full-strength, in-domain baselines. We study when and why inverse solvers are robust to weak diffusion priors. Through extensive experiments, we find that weak priors succeed when measurements are highly informative (e.g., many observed pixels), and we identify regimes where they fail. Our theory, based on Bayesian consistency, gives conditions under which high-dimensional measurements make the posterior concentrate near the true signal. These results provide a principled justification on when weak diffusion priors can be used reliably.</li>
</ul>

<h3>Title: Tuning the Implicit Regularizer of Masked Diffusion Language Models: Enhancing Generalization via Insights from $k$-Parity</h3>
<ul>
<li><strong>Authors: </strong>Jianhao Huang, Baharan Mirzasoleiman</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22450">https://arxiv.org/abs/2601.22450</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22450">https://arxiv.org/pdf/2601.22450</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22450]] Tuning the Implicit Regularizer of Masked Diffusion Language Models: Enhancing Generalization via Insights from $k$-Parity(https://arxiv.org/abs/2601.22450)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Masked Diffusion Language Models have recently emerged as a powerful generative paradigm, yet their generalization properties remain understudied compared to their auto-regressive counterparts. In this work, we investigate these properties within the setting of the $k$-parity problem (computing the XOR sum of $k$ relevant bits), where neural networks typically exhibit grokking -- a prolonged plateau of chance-level performance followed by sudden generalization. We theoretically decompose the Masked Diffusion (MD) objective into a Signal regime which drives feature learning, and a Noise regime which serves as an implicit regularizer. By training nanoGPT using MD objective on the $k$-parity problem, we demonstrate that MD objective fundamentally alters the learning landscape, enabling rapid and simultaneous generalization without experiencing grokking. Furthermore, we leverage our theoretical insights to optimize the distribution of the mask probability in the MD objective. Our method significantly improves perplexity for 50M-parameter models and achieves superior results across both pre-training from scratch and supervised fine-tuning. Specifically, we observe performance gains peaking at $8.8\%$ and $5.8\%$, respectively, on 8B-parameter models, confirming the scalability and effectiveness of our framework in large-scale masked diffusion language model regimes.</li>
</ul>

<h3>Title: Temporal Graph Pattern Machine</h3>
<ul>
<li><strong>Authors: </strong>Yijun Ma, Zehong Wang, Weixiang Sun, Yanfang Ye</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22454">https://arxiv.org/abs/2601.22454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22454">https://arxiv.org/pdf/2601.22454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22454]] Temporal Graph Pattern Machine(https://arxiv.org/abs/2601.22454)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Temporal graph learning is pivotal for deciphering dynamic systems, where the core challenge lies in explicitly modeling the underlying evolving patterns that govern network transformation. However, prevailing methods are predominantly task-centric and rely on restrictive assumptions -- such as short-term dependency modeling, static neighborhood semantics, and retrospective time usage. These constraints hinder the discovery of transferable temporal evolution mechanisms. To address this, we propose the Temporal Graph Pattern Machine (TGPM), a foundation framework that shifts the focus toward directly learning generalized evolving patterns. TGPM conceptualizes each interaction as an interaction patch synthesized via temporally-biased random walks, thereby capturing multi-scale structural semantics and long-range dependencies that extend beyond immediate neighborhoods. These patches are processed by a Transformer-based backbone designed to capture global temporal regularities while adapting to context-specific interaction dynamics. To further empower the model, we introduce a suite of self-supervised pre-training tasks -- specifically masked token modeling and next-time prediction -- to explicitly encode the fundamental laws of network evolution. Extensive experiments show that TGPM consistently achieves state-of-the-art performance in both transductive and inductive link prediction, demonstrating exceptional cross-domain transferability.</li>
</ul>

<h3>Title: ScribbleSense: Generative Scribble-Based Texture Editing with Intent Prediction</h3>
<ul>
<li><strong>Authors: </strong>Yudi Zhang, Yeming Geng, Lei Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22455">https://arxiv.org/abs/2601.22455</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22455">https://arxiv.org/pdf/2601.22455</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22455]] ScribbleSense: Generative Scribble-Based Texture Editing with Intent Prediction(https://arxiv.org/abs/2601.22455)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Interactive 3D model texture editing presents enhanced opportunities for creating 3D assets, with freehand drawing style offering the most intuitive experience. However, existing methods primarily support sketch-based interactions for outlining, while the utilization of coarse-grained scribble-based interaction remains limited. Furthermore, current methodologies often encounter challenges due to the abstract nature of scribble instructions, which can result in ambiguous editing intentions and unclear target semantic locations. To address these issues, we propose ScribbleSense, an editing method that combines multimodal large language models (MLLMs) and image generation models to effectively resolve these challenges. We leverage the visual capabilities of MLLMs to predict the editing intent behind the scribbles. Once the semantic intent of the scribble is discerned, we employ globally generated images to extract local texture details, thereby anchoring local semantics and alleviating ambiguities concerning the target semantic locations. Experimental results indicate that our method effectively leverages the strengths of MLLMs, achieving state-of-the-art interactive editing performance for scribble-based texture editing.</li>
</ul>

<h3>Title: EvoEGF-Mol: Evolving Exponential Geodesic Flow for Structure-based Drug Design</h3>
<ul>
<li><strong>Authors: </strong>Yaowei Jin, Junjie Wang, Cheng Cao, Penglei Wang, Duo An, Qian Shi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22466">https://arxiv.org/abs/2601.22466</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22466">https://arxiv.org/pdf/2601.22466</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22466]] EvoEGF-Mol: Evolving Exponential Geodesic Flow for Structure-based Drug Design(https://arxiv.org/abs/2601.22466)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Structure-Based Drug Design (SBDD) aims to discover bioactive ligands. Conventional approaches construct probability paths separately in Euclidean and probabilistic spaces for continuous atomic coordinates and discrete chemical categories, leading to a mismatch with the underlying statistical manifolds. We address this issue from an information-geometric perspective by modeling molecules as composite exponential-family distributions and defining generative flows along exponential geodesics under the Fisher-Rao metric. To avoid the instantaneous trajectory collapse induced by geodesics directly targeting Dirac distributions, we propose Evolving Exponential Geodesic Flow for SBDD (EvoEGF-Mol), which replaces static Dirac targets with dynamically concentrating distributions, ensuring stable training via a progressive-parameter-refinement architecture. Our model approaches a reference-level PoseBusters passing rate (93.4%) on CrossDock, demonstrating remarkable geometric precision and interaction fidelity, while outperforming baselines on real-world MolGenBench tasks by recovering bioactive scaffolds and generating candidates that meet established MedChem filters.</li>
</ul>

<h3>Title: Training-Free Representation Guidance for Diffusion Models with a Representation Alignment Projector</h3>
<ul>
<li><strong>Authors: </strong>Wenqiang Zu, Shenghao Xie, Bo Lei, Lei Ma</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22468">https://arxiv.org/abs/2601.22468</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22468">https://arxiv.org/pdf/2601.22468</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22468]] Training-Free Representation Guidance for Diffusion Models with a Representation Alignment Projector(https://arxiv.org/abs/2601.22468)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent progress in generative modeling has enabled high-quality visual synthesis with diffusion-based frameworks, supporting controllable sampling and large-scale training. Inference-time guidance methods such as classifier-free and representative guidance enhance semantic alignment by modifying sampling dynamics; however, they do not fully exploit unsupervised feature representations. Although such visual representations contain rich semantic structure, their integration during generation is constrained by the absence of ground-truth reference images at inference. This work reveals semantic drift in the early denoising stages of diffusion transformers, where stochasticity results in inconsistent alignment even under identical conditioning. To mitigate this issue, we introduce a guidance scheme using a representation alignment projector that injects representations predicted by a projector into intermediate sampling steps, providing an effective semantic anchor without modifying the model architecture. Experiments on SiTs and REPAs show notable improvements in class-conditional ImageNet synthesis, achieving substantially lower FID scores; for example, REPA-XL/2 improves from 5.9 to 3.3, and the proposed method outperforms representative guidance when applied to SiT models. The approach further yields complementary gains when combined with classifier-free guidance, demonstrating enhanced semantic coherence and visual fidelity. These results establish representation-informed diffusion sampling as a practical strategy for reinforcing semantic preservation and image consistency.</li>
</ul>

<h3>Title: Continual Policy Distillation from Distributed Reinforcement Learning Teachers</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Li, Qijun He, Mingqi Yuan, Wen-Tse Chen, Jeff Schneider, Jiayu Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22475">https://arxiv.org/abs/2601.22475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22475">https://arxiv.org/pdf/2601.22475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22475]] Continual Policy Distillation from Distributed Reinforcement Learning Teachers(https://arxiv.org/abs/2601.22475)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Continual Reinforcement Learning (CRL) aims to develop lifelong learning agents to continuously acquire knowledge across diverse tasks while mitigating catastrophic forgetting. This requires efficiently managing the stability-plasticity dilemma and leveraging prior experience to rapidly generalize to novel tasks. While various enhancement strategies for both aspects have been proposed, achieving scalable performance by directly applying RL to sequential task streams remains challenging. In this paper, we propose a novel teacher-student framework that decouples CRL into two independent processes: training single-task teacher models through distributed RL and continually distilling them into a central generalist model. This design is motivated by the observation that RL excels at solving single tasks, while policy distillation -- a relatively stable supervised learning process -- is well aligned with large foundation models and multi-task learning. Moreover, a mixture-of-experts (MoE) architecture and a replay-based approach are employed to enhance the plasticity and stability of the continual policy distillation process. Extensive experiments on the Meta-World benchmark demonstrate that our framework enables efficient continual RL, recovering over 85% of teacher performance while constraining task-wise forgetting to within 10%.</li>
</ul>

<h3>Title: Elastic Spectral State Space Models for Budgeted Inference</h3>
<ul>
<li><strong>Authors: </strong>Dachuan Song, Xuan Wang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22488">https://arxiv.org/abs/2601.22488</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22488">https://arxiv.org/pdf/2601.22488</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22488]] Elastic Spectral State Space Models for Budgeted Inference(https://arxiv.org/abs/2601.22488)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Foundation models are typically trained at a fixed computational capacity, while real-world applications require deployment across platforms with different resource constraints. Current approaches usually rely on training families of model variants or model distillation, which requires additional training and supports only a pre-selected set of sizes rather than fine-grained adaptation at runtime. In this paper, we propose Elastic Spectral State Space Models (ES-SSM), which require only one-time training at full capacity, but can be directly truncated into arbitrary scales for budgeted, runtime inference without retraining. Our ES-SSM builds on Hankel spectral filtering over a state space model (SSM), coupled with a lightweight input-adaptive gate trained under randomized spectral budgets. Using a shared masked normalization rule over the ordered spectral channels, we encourage predictive capability to concentrate in low-index components, while higher-index components act primarily as refinement. We test our algorithm across long-sequence benchmarks spanning text, logic, retrieval, vision, and audio. We demonstrate that a single ES-SSM model trained once can be truncated to provide competitive performance compared with modern Transformer and SSM baselines at similar parameter scales. Furthermore, by testing under various runtime budgets, we observe smooth and stable budget-performance curves over a wide range of truncation levels.</li>
</ul>

<h3>Title: PromptMAD: Cross-Modal Prompting for Multi-Class Visual Anomaly Localization</h3>
<ul>
<li><strong>Authors: </strong>Duncan McCain, Hossein Kashiani, Fatemeh Afghah</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22492">https://arxiv.org/abs/2601.22492</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22492">https://arxiv.org/pdf/2601.22492</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22492]] PromptMAD: Cross-Modal Prompting for Multi-Class Visual Anomaly Localization(https://arxiv.org/abs/2601.22492)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, anomaly</a></li>
<li><strong>Abstract: </strong>Visual anomaly detection in multi-class settings poses significant challenges due to the diversity of object categories, the scarcity of anomalous examples, and the presence of camouflaged defects. In this paper, we propose PromptMAD, a cross-modal prompting framework for unsupervised visual anomaly detection and localization that integrates semantic guidance through vision-language alignment. By leveraging CLIP-encoded text prompts describing both normal and anomalous class-specific characteristics, our method enriches visual reconstruction with semantic context, improving the detection of subtle and textural anomalies. To further address the challenge of class imbalance at the pixel level, we incorporate Focal loss function, which emphasizes hard-to-detect anomalous regions during training. Our architecture also includes a supervised segmentor that fuses multi-scale convolutional features with Transformer-based spatial attention and diffusion iterative refinement, yielding precise and high-resolution anomaly maps. Extensive experiments on the MVTec-AD dataset demonstrate that our method achieves state-of-the-art pixel-level performance, improving mean AUC to 98.35% and AP to 66.54%, while maintaining efficiency across diverse categories.</li>
</ul>

<h3>Title: Gradual Fine-Tuning for Flow Matching Models</h3>
<ul>
<li><strong>Authors: </strong>Gudrun Thorkelsdottir, Arindam Banerjee</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22495">https://arxiv.org/abs/2601.22495</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22495">https://arxiv.org/pdf/2601.22495</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22495]] Gradual Fine-Tuning for Flow Matching Models(https://arxiv.org/abs/2601.22495)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Fine-tuning flow matching models is a central challenge in settings with limited data, evolving distributions, or strict efficiency demands, where unconstrained fine-tuning can erode the accuracy and efficiency gains learned during pretraining. Prior work has produced theoretical guarantees and empirical advances for reward-based fine-tuning formulations, but these methods often impose restrictions on permissible drift structure or training techniques. In this work, we propose Gradual Fine-Tuning (GFT), a principled framework for fine-tuning flow-based generative models when samples from the target distribution are available. For stochastic flows, GFT defines a temperature-controlled sequence of intermediate objectives that smoothly interpolate between the pretrained and target drifts, approaching the true target as the temperature approaches zero. We prove convergence results for both marginal and conditional GFT objectives, enabling the use of suitable (e.g., optimal transport) couplings during GFT while preserving correctness. Empirically, GFT improves convergence stability and shortens probability paths, resulting in faster inference, while maintaining generation quality comparable to standard fine-tuning. Our results position GFT as a theoretically grounded and practically effective alternative for scalable adaptation of flow matching models under distribution shift.</li>
</ul>

<h3>Title: MIRRORTALK: Forging Personalized Avatars Via Disentangled Style and Hierarchical Motion Control</h3>
<ul>
<li><strong>Authors: </strong>Renjie Lu, Xulong Zhang, Xiaoyang Qu, Jianzong Wang, Shangfei Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22501">https://arxiv.org/abs/2601.22501</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22501">https://arxiv.org/pdf/2601.22501</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22501]] MIRRORTALK: Forging Personalized Avatars Via Disentangled Style and Hierarchical Motion Control(https://arxiv.org/abs/2601.22501)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Synthesizing personalized talking faces that uphold and highlight a speaker's unique style while maintaining lip-sync accuracy remains a significant challenge. A primary limitation of existing approaches is the intrinsic confounding of speaker-specific talking style and semantic content within facial motions, which prevents the faithful transfer of a speaker's unique persona to arbitrary speech. In this paper, we propose MirrorTalk, a generative framework based on a conditional diffusion model, combined with a Semantically-Disentangled Style Encoder (SDSE) that can distill pure style representations from a brief reference video. To effectively utilize this representation, we further introduce a hierarchical modulation strategy within the diffusion process. This mechanism guides the synthesis by dynamically balancing the contributions of audio and style features across distinct facial regions, ensuring both precise lip-sync accuracy and expressive full-face dynamics. Extensive experiments demonstrate that MirrorTalk achieves significant improvements over state-of-the-art methods in terms of lip-sync accuracy and personalization preservation.</li>
</ul>

<h3>Title: DreamVAR: Taming Reinforced Visual Autoregressive Model for High-Fidelity Subject-Driven Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Xin Jiang, Jingwen Chen, Yehao Li, Yingwei Pan, Kezhou Chen, Zechao Li, Ting Yao, Tao Mei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22507">https://arxiv.org/abs/2601.22507</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22507">https://arxiv.org/pdf/2601.22507</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22507]] DreamVAR: Taming Reinforced Visual Autoregressive Model for High-Fidelity Subject-Driven Image Generation(https://arxiv.org/abs/2601.22507)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in subject-driven image generation using diffusion models have attracted considerable attention for their remarkable capabilities in producing high-quality images. Nevertheless, the potential of Visual Autoregressive (VAR) models, despite their unified architecture and efficient inference, remains underexplored. In this work, we present DreamVAR, a novel framework for subject-driven image synthesis built upon a VAR model that employs next-scale prediction. Technically, multi-scale features of the reference subject are first extracted by a visual tokenizer. Instead of interleaving these conditional features with target image tokens across scales, our DreamVAR pre-fills the full subject feature sequence prior to predicting target image tokens. This design simplifies autoregressive dependencies and mitigates the train-test discrepancy in multi-scale conditioning scenario within the VAR paradigm. DreamVAR further incorporates reinforcement learning to jointly enhance semantic alignment and subject consistency. Extensive experiments demonstrate that DreamVAR achieves superior appearance preservation compared to leading diffusion-based methods.</li>
</ul>

<h3>Title: DNA: Uncovering Universal Latent Forgery Knowledge</h3>
<ul>
<li><strong>Authors: </strong>Jingtong Dou, Chuancheng Shi, Yemin Wang, Shiming Guo, Anqi Yi, Wenhua Wu, Li Zhang, Fei Shen, Tat-Seng Chua</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22515">https://arxiv.org/abs/2601.22515</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22515">https://arxiv.org/pdf/2601.22515</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22515]] DNA: Uncovering Universal Latent Forgery Knowledge(https://arxiv.org/abs/2601.22515)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>As generative AI achieves hyper-realism, superficial artifact detection has become obsolete. While prevailing methods rely on resource-intensive fine-tuning of black-box backbones, we propose that forgery detection capability is already encoded within pre-trained models rather than requiring end-to-end retraining. To elicit this intrinsic capability, we propose the discriminative neural anchors (DNA) framework, which employs a coarse-to-fine excavation mechanism. First, by analyzing feature decoupling and attention distribution shifts, we pinpoint critical intermediate layers where the focus of the model logically transitions from global semantics to local anomalies. Subsequently, we introduce a triadic fusion scoring metric paired with a curvature-truncation strategy to strip away semantic redundancy, precisely isolating the forgery-discriminative units (FDUs) inherently imprinted with sensitivity to forgery traces. Moreover, we introduce HIFI-Gen, a high-fidelity synthetic benchmark built upon the very latest models, to address the lag in existing datasets. Experiments demonstrate that by solely relying on these anchors, DNA achieves superior detection performance even under few-shot conditions. Furthermore, it exhibits remarkable robustness across diverse architectures and against unseen generative models, validating that waking up latent neurons is more effective than extensive fine-tuning.</li>
</ul>

<h3>Title: SCOPE-PD: Explainable AI on Subjective and Clinical Objective Measurements of Parkinson's Disease for Precision Decision-Making</h3>
<ul>
<li><strong>Authors: </strong>Md Mezbahul Islam, John Michael Templeton, Masrur Sobhan, Christian Poellabauer, Ananda Mohan Mondal</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22516">https://arxiv.org/abs/2601.22516</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22516">https://arxiv.org/pdf/2601.22516</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22516]] SCOPE-PD: Explainable AI on Subjective and Clinical Objective Measurements of Parkinson's Disease for Precision Decision-Making(https://arxiv.org/abs/2601.22516)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Parkinson's disease (PD) is a chronic and complex neurodegenerative disorder influenced by genetic, clinical, and lifestyle factors. Predicting this disease early is challenging because it depends on traditional diagnostic methods that face issues of subjectivity, which commonly delay diagnosis. Several objective analyses are currently in practice to help overcome the challenges of subjectivity; however, a proper explanation of these analyses is still lacking. While machine learning (ML) has demonstrated potential in supporting PD diagnosis, existing approaches often rely on subjective reports only and lack interpretability for individualized risk estimation. This study proposes SCOPE-PD, an explainable AI-based prediction framework, by integrating subjective and objective assessments to provide personalized health decisions. Subjective and objective clinical assessment data are collected from the Parkinson's Progression Markers Initiative (PPMI) study to construct a multimodal prediction framework. Several ML techniques are applied to these data, and the best ML model is selected to interpret the results. Model interpretability is examined using SHAP-based analysis. The Random Forest algorithm achieves the highest accuracy of 98.66 percent using combined features from both subjective and objective test data. Tremor, bradykinesia, and facial expression are identified as the top three contributing features from the MDS-UPDRS test in the prediction of PD.</li>
</ul>

<h3>Title: Variational Bayesian Flow Network for Graph Generation</h3>
<ul>
<li><strong>Authors: </strong>Yida Xiong, Jiameng Chen, Xiuwen Gong, Jia Wu, Shirui Pan, Wenbin Hu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22524">https://arxiv.org/abs/2601.22524</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22524">https://arxiv.org/pdf/2601.22524</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22524]] Variational Bayesian Flow Network for Graph Generation(https://arxiv.org/abs/2601.22524)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Graph generation aims to sample discrete node and edge attributes while satisfying coupled structural constraints. Diffusion models for graphs often adopt largely factorized forward-noising, and many flow-matching methods start from factorized reference noise and coordinate-wise interpolation, so node-edge coupling is not encoded by the generative geometry and must be recovered implicitly by the core network, which can be brittle after discrete decoding. Bayesian Flow Networks (BFNs) evolve distribution parameters and naturally support discrete generation. But classical BFNs typically rely on factorized beliefs and independent channels, which limit geometric evidence fusion. We propose Variational Bayesian Flow Network (VBFN), which performs a variational lifting to a tractable joint Gaussian variational belief family governed by structured precisions. Each Bayesian update reduces to solving a symmetric positive definite linear system, enabling coupled node and edge updates within a single fusion step. We construct sample-agnostic sparse precisions from a representation-induced dependency graph, thereby avoiding label leakage while enforcing node-edge consistency. On synthetic and molecular graph datasets, VBFN improves fidelity and diversity, and surpasses baseline methods.</li>
</ul>

<h3>Title: $œÅ$-$\texttt{EOS}$: Training-free Bidirectional Variable-Length Control for Masked Diffusion LLMs</h3>
<ul>
<li><strong>Authors: </strong>Jingyi Yang, Yuxian Jiang, Jing Shao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22527">https://arxiv.org/abs/2601.22527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22527">https://arxiv.org/pdf/2601.22527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22527]] $œÅ$-$\texttt{EOS}$: Training-free Bidirectional Variable-Length Control for Masked Diffusion LLMs(https://arxiv.org/abs/2601.22527)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Beyond parallel generation and global context modeling, current masked diffusion large language models (dLLMs) suffer from a fundamental limitation: they require a predefined, fixed generation length, which lacks flexibility and forces an inevitable trade-off between output quality and computational efficiency. To address this, we study the denoising dynamics and find that the implicit density ($\rho$) of end-of-sequence ($\texttt{EOS}$) tokens serves as a reliable signal of generation sufficiency. In particular, the evolving implicit $\texttt{EOS}$ density during denoising reveals whether the current masked space is excessive or insufficient, thereby guiding the adjustment direction for generation length. Building on this insight, we propose $\textbf{$\rho$-$\texttt{EOS}$}$, a training-free, single-stage strategy that enables bidirectional variable-length generation for masked dLLMs. Unlike prior two-stage approaches--which require separate length adjustment and iterative mask insertion phases while supporting only unidirectional expansion--$\textbf{$\rho$-$\texttt{EOS}$}$ achieves bidirectional length adjustment within a unified denoising process by continuously estimating the implicit $\texttt{EOS}$ density: excessively high density triggers $\texttt{MASK}$ token contraction, while insufficient density induces expansion. Extensive experiments on mathematics and code benchmarks demonstrate that $\textbf{$\rho$-$\texttt{EOS}$}$ achieves comparable performance while substantially improving inference efficiency and token utilization.</li>
</ul>

<h3>Title: Towards the Holographic Characteristic of LLMs for Efficient Short-text Generation</h3>
<ul>
<li><strong>Authors: </strong>Shun Qian, Bingquan Liu, Chengjie Sun, Zhen Xu, Baoxun Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22546">https://arxiv.org/abs/2601.22546</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22546">https://arxiv.org/pdf/2601.22546</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22546]] Towards the Holographic Characteristic of LLMs for Efficient Short-text Generation(https://arxiv.org/abs/2601.22546)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>The recent advancements in Large Language Models (LLMs) have attracted interest in exploring their in-context learning abilities and chain-of-thought capabilities. However, there are few studies investigating the specific traits related to the powerful generation capacity of LLMs. This paper aims to delve into the generation characteristics exhibited by LLMs. Through our investigation, we have discovered that language models tend to capture target-side keywords at the beginning of the generation process. We name this phenomenon the Holographic Characteristic of language models. For the purpose of exploring this characteristic and further improving the inference efficiency of language models, we propose a plugin called HOLO, which leverages the Holographic Characteristic to extract target-side keywords from language models within a limited number of generation steps and complements the sentence with a parallel lexically constrained text generation method. To verify the effectiveness of HOLO, we conduct massive experiments on language models of varying architectures and scales in the short-text generation scenario. The results demonstrate that HOLO achieves comparable performance to the baselines in terms of both automatic and human-like evaluation metrics and highlight the potential of the Holographic Characteristic.</li>
</ul>

<h3>Title: VocBulwark: Towards Practical Generative Speech Watermarking via Additional-Parameter Injection</h3>
<ul>
<li><strong>Authors: </strong>Weizhi Liu, Yue Li, Zhaoxia Yin</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22556">https://arxiv.org/abs/2601.22556</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22556">https://arxiv.org/pdf/2601.22556</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22556]] VocBulwark: Towards Practical Generative Speech Watermarking via Additional-Parameter Injection(https://arxiv.org/abs/2601.22556)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generated speech achieves human-level naturalness but escalates security risks of misuse. However, existing watermarking methods fail to reconcile fidelity with robustness, as they rely either on simple superposition in the noise space or on intrusive alterations to model weights. To bridge this gap, we propose VocBulwark, an additional-parameter injection framework that freezes generative model parameters to preserve perceptual quality. Specifically, we design a Temporal Adapter to deeply entangle watermarks with acoustic attributes, synergizing with a Coarse-to-Fine Gated Extractor to resist advanced attacks. Furthermore, we develop an Accuracy-Guided Optimization Curriculum that dynamically orchestrates gradient flow to resolve the optimization conflict between fidelity and robustness. Comprehensive experiments demonstrate that VocBulwark achieves high-capacity and high-fidelity watermarking, offering robust defense against complex practical scenarios, with resilience to Codec regenerations and variable-length manipulations.</li>
</ul>

<h3>Title: Leveraging Data to Say No: Memory Augmented Plug-and-Play Selective Prediction</h3>
<ul>
<li><strong>Authors: </strong>Aditya Sarkar, Yi Li, Jiacheng Cheng, Shlok Mishra, Nuno Vasconcelos</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22570">https://arxiv.org/abs/2601.22570</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22570">https://arxiv.org/pdf/2601.22570</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22570]] Leveraging Data to Say No: Memory Augmented Plug-and-Play Selective Prediction(https://arxiv.org/abs/2601.22570)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Selective prediction aims to endow predictors with a reject option, to avoid low confidence predictions. However, existing literature has primarily focused on closed-set tasks, such as visual question answering with predefined options or fixed-category classification. This paper considers selective prediction for visual language foundation models, addressing a taxonomy of tasks ranging from closed to open set and from finite to unbounded vocabularies, as in image captioning. We seek training-free approaches of low-complexity, applicable to any foundation model and consider methods based on external vision-language model embeddings, like CLIP. This is denoted as Plug-and-Play Selective Prediction (PaPSP). We identify two key challenges: (1) instability of the visual-language representations, leading to high variance in image-text embeddings, and (2) poor calibration of similarity scores. To address these issues, we propose a memory augmented PaPSP (MA-PaPSP) model, which augments PaPSP with a retrieval dataset of image-text pairs. This is leveraged to reduce embedding variance by averaging retrieved nearest-neighbor pairs and is complemented by the use of contrastive normalization to improve score calibration. Through extensive experiments on multiple datasets, we show that MA-PaPSP outperforms PaPSP and other selective prediction baselines for selective captioning, image-text matching, and fine-grained classification. Code is publicly available at this https URL.</li>
</ul>

<h3>Title: PhoStream: Benchmarking Real-World Streaming for Omnimodal Assistants in Mobile Scenarios</h3>
<ul>
<li><strong>Authors: </strong>Xudong Lu, Huankang Guan, Yang Bo, Jinpeng Chen, Xintong Guo, Shuhan Li, Fang Liu, Peiwen Sun, Xueying Li, Wei Zhang, Xue Yang, Rui Liu, Hongsheng Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22575">https://arxiv.org/abs/2601.22575</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22575">https://arxiv.org/pdf/2601.22575</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22575]] PhoStream: Benchmarking Real-World Streaming for Omnimodal Assistants in Mobile Scenarios(https://arxiv.org/abs/2601.22575)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models excel at offline audio-visual understanding, but their ability to serve as mobile assistants in continuous real-world streams remains underexplored. In daily phone use, mobile assistants must track streaming audio-visual inputs and respond at the right time, yet existing benchmarks are often restricted to multiple-choice questions or use shorter videos. In this paper, we introduce PhoStream, the first mobile-centric streaming benchmark that unifies on-screen and off-screen scenarios to evaluate video, audio, and temporal reasoning. PhoStream contains 5,572 open-ended QA pairs from 578 videos across 4 scenarios and 10 capabilities. We build it with an Automated Generative Pipeline backed by rigorous human verification, and evaluate models using a realistic Online Inference Pipeline and LLM-as-a-Judge evaluation for open-ended responses. Experiments reveal a temporal asymmetry in LLM-judged scores (0-100): models perform well on Instant and Backward tasks (Gemini 3 Pro exceeds 80), but drop sharply on Forward tasks (16.40), largely due to early responses before the required visual and audio cues appear. This highlights a fundamental limitation: current MLLMs struggle to decide when to speak, not just what to say. Code and datasets used in this work will be made publicly accessible at this https URL.</li>
</ul>

<h3>Title: FedDis: A Causal Disentanglement Framework for Federated Traffic Prediction</h3>
<ul>
<li><strong>Authors: </strong>Chengyang Zhou, Zijian Zhang, Chunxu Zhang, Hao Miao, Yulin Zhang, Kedi Lyu, Juncheng Hu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22578">https://arxiv.org/abs/2601.22578</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22578">https://arxiv.org/pdf/2601.22578</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22578]] FedDis: A Causal Disentanglement Framework for Federated Traffic Prediction(https://arxiv.org/abs/2601.22578)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Federated learning offers a promising paradigm for privacy-preserving traffic prediction, yet its performance is often challenged by the non-identically and independently distributed (non-IID) nature of decentralized traffic data. Existing federated methods frequently struggle with this data heterogeneity, typically entangling globally shared patterns with client-specific local dynamics within a single representation. In this work, we postulate that this heterogeneity stems from the entanglement of two distinct generative sources: client-specific localized dynamics and cross-client global spatial-temporal patterns. Motivated by this perspective, we introduce FedDis, a novel framework that, to the best of our knowledge, is the first to leverage causal disentanglement for federated spatial-temporal prediction. Architecturally, FedDis comprises a dual-branch design wherein a Personalized Bank learns to capture client-specific factors, while a Global Pattern Bank distills common knowledge. This separation enables robust cross-client knowledge transfer while preserving high adaptability to unique local environments. Crucially, a mutual information minimization objective is employed to enforce informational orthogonality between the two branches, thereby ensuring effective disentanglement. Comprehensive experiments conducted on four real-world benchmark datasets demonstrate that FedDis consistently achieves state-of-the-art performance, promising efficiency, and superior expandability.</li>
</ul>

<h3>Title: Cross-Domain Few-Shot Learning for Hyperspectral Image Classification Based on Mixup Foundation Model</h3>
<ul>
<li><strong>Authors: </strong>Naeem Paeedeh, Mahardhika Pratama, Ary Shiddiqi, Zehong Cao, Mukesh Prasad, Wisnu Jatmiko</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22581">https://arxiv.org/abs/2601.22581</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22581">https://arxiv.org/pdf/2601.22581</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22581]] Cross-Domain Few-Shot Learning for Hyperspectral Image Classification Based on Mixup Foundation Model(https://arxiv.org/abs/2601.22581)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Although cross-domain few-shot learning (CDFSL) for hyper-spectral image (HSI) classification has attracted significant research interest, existing works often rely on an unrealistic data augmentation procedure in the form of external noise to enlarge the sample size, thus greatly simplifying the issue of data scarcity. They involve a large number of parameters for model updates, being prone to the overfitting problem. To the best of our knowledge, none has explored the strength of the foundation model, having strong generalization power to be quickly adapted to downstream tasks. This paper proposes the MIxup FOundation MOdel (MIFOMO) for CDFSL of HSI classifications. MIFOMO is built upon the concept of a remote sensing (RS) foundation model, pre-trained across a large scale of RS problems, thus featuring generalizable features. The notion of coalescent projection (CP) is introduced to quickly adapt the foundation model to downstream tasks while freezing the backbone network. The concept of mixup domain adaptation (MDM) is proposed to address the extreme domain discrepancy problem. Last but not least, the label smoothing concept is implemented to cope with noisy pseudo-label problems. Our rigorous experiments demonstrate the advantage of MIFOMO, where it beats prior arts with up to 14% margin. The source code of MIFOMO is open-sourced in this https URL Paeedeh/MIFOMO for reproducibility and convenient further study.</li>
</ul>

<h3>Title: Rethinking LLM-as-a-Judge: Representation-as-a-Judge with Small Language Models via Semantic Capacity Asymmetry</h3>
<ul>
<li><strong>Authors: </strong>Zhuochun Li, Yong Zhang, Ming Li, Yuelyu Ji, Yiming Zeng, Ning Cheng, Yun Zhu, Yanmeng Wang, Shaojun Wang, Jing Xiao, Daqing He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22588">https://arxiv.org/abs/2601.22588</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22588">https://arxiv.org/pdf/2601.22588</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22588]] Rethinking LLM-as-a-Judge: Representation-as-a-Judge with Small Language Models via Semantic Capacity Asymmetry(https://arxiv.org/abs/2601.22588)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) are widely used as reference-free evaluators via prompting, but this "LLM-as-a-Judge" paradigm is costly, opaque, and sensitive to prompt design. In this work, we investigate whether smaller models can serve as efficient evaluators by leveraging internal representations instead of surface generation. We uncover a consistent empirical pattern: small LMs, despite with weak generative ability, encode rich evaluative signals in their hidden states. This motivates us to propose the Semantic Capacity Asymmetry Hypothesis: evaluation requires significantly less semantic capacity than generation and can be grounded in intermediate representations, suggesting that evaluation does not necessarily need to rely on large-scale generative models but can instead leverage latent features from smaller ones. Our findings motivate a paradigm shift from LLM-as-a-Judge to Representation-as-a-Judge, a decoding-free evaluation strategy that probes internal model structure rather than relying on prompted output. We instantiate this paradigm through INSPECTOR, a probing-based framework that predicts aspect-level evaluation scores from small model representations. Experiments on reasoning benchmarks (GSM8K, MATH, GPQA) show that INSPECTOR substantially outperforms prompting-based small LMs and closely approximates full LLM judges, while offering a more efficient, reliable, and interpretable alternative for scalable evaluation.</li>
</ul>

<h3>Title: Local-Global Multimodal Contrastive Learning for Molecular Property Prediction</h3>
<ul>
<li><strong>Authors: </strong>Xiayu Liu, Zhengyi Lu, Yunhong Liao, Chan Fan, Hou-biao Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22610">https://arxiv.org/abs/2601.22610</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22610">https://arxiv.org/pdf/2601.22610</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22610]] Local-Global Multimodal Contrastive Learning for Molecular Property Prediction(https://arxiv.org/abs/2601.22610)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Accurate molecular property prediction requires integrating complementary information from molecular structure and chemical semantics. In this work, we propose LGM-CL, a local-global multimodal contrastive learning framework that jointly models molecular graphs and textual representations derived from SMILES and chemistry-aware augmented texts. Local functional group information and global molecular topology are captured using AttentiveFP and Graph Transformer encoders, respectively, and aligned through self-supervised contrastive learning. In addition, chemically enriched textual descriptions are contrasted with original SMILES to incorporate physicochemical semantics in a task-agnostic manner. During fine-tuning, molecular fingerprints are further integrated via Dual Cross-attention multimodal fusion. Extensive experiments on MoleculeNet benchmarks demonstrate that LGM-CL achieves consistent and competitive performance across both classification and regression tasks, validating the effectiveness of unified local-global and multimodal representation learning.</li>
</ul>

<h3>Title: Time-Annealed Perturbation Sampling: Diverse Generation for Diffusion Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jingxuan Wu, Zhenglin Wan, Xingrui Yu, Yuzhe Yang, Yiqiao Huang, Ivor Tsang, Yang You</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22629">https://arxiv.org/abs/2601.22629</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22629">https://arxiv.org/pdf/2601.22629</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22629]] Time-Annealed Perturbation Sampling: Diverse Generation for Diffusion Language Models(https://arxiv.org/abs/2601.22629)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion language models (Diffusion-LMs) introduce an explicit temporal dimension into text generation, yet how this structure can be leveraged to control generation diversity for exploring multiple valid semantic or reasoning paths remains underexplored. In this paper, we show that Diffusion-LMs, like diffusion models in image generation, exhibit a temporal division of labor: early denoising steps largely determine the global semantic structure, while later steps focus on local lexical refinement. Building on this insight, we propose Time-Annealed Perturbation Sampling (TAPS), a training-free inference strategy that encourages semantic branching early in the diffusion process while progressively reducing perturbations to preserve fluency and instruction adherence. TAPS is compatible with both non-autoregressive and semi-autoregressive Diffusion backbones, demonstrated on LLaDA and TraDo in our paper, and consistently improves output diversity across creative writing and reasoning benchmarks without compromising generation quality.</li>
</ul>

<h3>Title: LINA: Linear Autoregressive Image Generative Models with Continuous Tokens</h3>
<ul>
<li><strong>Authors: </strong>Jiahao Wang, Ting Pan, Haoge Deng, Dongchen Han, Taiqiang Wu, Xinlong Wang, Ping Luo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22630">https://arxiv.org/abs/2601.22630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22630">https://arxiv.org/pdf/2601.22630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22630]] LINA: Linear Autoregressive Image Generative Models with Continuous Tokens(https://arxiv.org/abs/2601.22630)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Autoregressive models with continuous tokens form a promising paradigm for visual generation, especially for text-to-image (T2I) synthesis, but they suffer from high computational cost. We study how to design compute-efficient linear attention within this framework. Specifically, we conduct a systematic empirical analysis of scaling behavior with respect to parameter counts under different design choices, focusing on (1) normalization paradigms in linear attention (division-based vs. subtraction-based) and (2) depthwise convolution for locality augmentation. Our results show that although subtraction-based normalization is effective for image classification, division-based normalization scales better for linear generative transformers. In addition, incorporating convolution for locality modeling plays a crucial role in autoregressive generation, consistent with findings in diffusion models. We further extend gating mechanisms, commonly used in causal linear attention, to the bidirectional setting and propose a KV gate. By introducing data-independent learnable parameters to the key and value states, the KV gate assigns token-wise memory weights, enabling flexible memory management similar to forget gates in language models. Based on these findings, we present LINA, a simple and compute-efficient T2I model built entirely on linear attention, capable of generating high-fidelity 1024x1024 images from user instructions. LINA achieves competitive performance on both class-conditional and T2I benchmarks, obtaining 2.18 FID on ImageNet (about 1.4B parameters) and 0.74 on GenEval (about 1.5B parameters). A single linear attention module reduces FLOPs by about 61 percent compared to softmax attention. Code and models are available at: this https URL.</li>
</ul>

<h3>Title: GUDA: Counterfactual Group-wise Training Data Attribution for Diffusion Models via Unlearning</h3>
<ul>
<li><strong>Authors: </strong>Naoki Murata, Yuhta Takida, Chieh-Hsin Lai, Toshimitsu Uesaka, Bac Nguyen, Stefano Ermon, Yuki Mitsufuji</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22651">https://arxiv.org/abs/2601.22651</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22651">https://arxiv.org/pdf/2601.22651</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22651]] GUDA: Counterfactual Group-wise Training Data Attribution for Diffusion Models via Unlearning(https://arxiv.org/abs/2601.22651)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Training-data attribution for vision generative models aims to identify which training data influenced a given output. While most methods score individual examples, practitioners often need group-level answers (e.g., artistic styles or object classes). Group-wise attribution is counterfactual: how would a model's behavior on a generated sample change if a group were absent from training? A natural realization of this counterfactual is Leave-One-Group-Out (LOGO) retraining, which retrains the model with each group removed; however, it becomes computationally prohibitive as the number of groups grows. We propose GUDA (Group Unlearning-based Data Attribution) for diffusion models, which approximates each counterfactual model by applying machine unlearning to a shared full-data model instead of training from scratch. GUDA quantifies group influence using differences in a likelihood-based scoring rule (ELBO) between the full model and each unlearned counterfactual. Experiments on CIFAR-10 and artistic style attribution with Stable Diffusion show that GUDA identifies primary contributing groups more reliably than semantic similarity, gradient-based attribution, and instance-level unlearning approaches, while achieving x100 speedup on CIFAR-10 over LOGO retraining.</li>
</ul>

<h3>Title: Unsupervised Synthetic Image Attribution: Alignment and Disentanglement</h3>
<ul>
<li><strong>Authors: </strong>Zongfang Liu, Guangyi Chen, Boyang Sun, Tongliang Liu, Kun Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22663">https://arxiv.org/abs/2601.22663</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22663">https://arxiv.org/pdf/2601.22663</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22663]] Unsupervised Synthetic Image Attribution: Alignment and Disentanglement(https://arxiv.org/abs/2601.22663)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>As the quality of synthetic images improves, identifying the underlying concepts of model-generated images is becoming increasingly crucial for copyright protection and ensuring model transparency. Existing methods achieve this attribution goal by training models using annotated pairs of synthetic images and their original training sources. However, obtaining such paired supervision is challenging, as it requires either well-designed synthetic concepts or precise annotations from millions of training sources. To eliminate the need for costly paired annotations, in this paper, we explore the possibility of unsupervised synthetic image attribution. We propose a simple yet effective unsupervised method called Alignment and Disentanglement. Specifically, we begin by performing basic concept alignment using contrastive self-supervised learning. Next, we enhance the model's attribution ability by promoting representation disentanglement with the Infomax loss. This approach is motivated by an interesting observation: contrastive self-supervised models, such as MoCo and DINO, inherently exhibit the ability to perform simple cross-domain alignment. By formulating this observation as a theoretical assumption on cross-covariance, we provide a theoretical explanation of how alignment and disentanglement can approximate the concept-matching process through a decomposition of the canonical correlation analysis objective. On the real-world benchmarks, AbC, we show that our unsupervised method surprisingly outperforms the supervised methods. As a starting point, we expect our intuitive insights and experimental findings to provide a fresh perspective on this challenging task.</li>
</ul>

<h3>Title: Fire on Motion: Optimizing Video Pass-bands for Efficient Spiking Action Recognition</h3>
<ul>
<li><strong>Authors: </strong>Shuhan Ye, Yuanbin Qian, Yi Yu, Chong Wang, Yuqi Xie, Jiazhen Xu, Kun Wang, Xudong Jiang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22675">https://arxiv.org/abs/2601.22675</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22675">https://arxiv.org/pdf/2601.22675</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22675]] Fire on Motion: Optimizing Video Pass-bands for Efficient Spiking Action Recognition(https://arxiv.org/abs/2601.22675)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Spiking neural networks (SNNs) have gained traction in vision due to their energy efficiency, bio-plausibility, and inherent temporal processing. Yet, despite this temporal capacity, most progress concentrates on static image benchmarks, and SNNs still underperform on dynamic video tasks compared to artificial neural networks (ANNs). In this work, we diagnose a fundamental pass-band mismatch: Standard spiking dynamics behave as a temporal low pass that emphasizes static content while attenuating motion bearing bands, where task relevant information concentrates in dynamic tasks. This phenomenon explains why SNNs can approach ANNs on static tasks yet fall behind on tasks that demand richer temporal this http URL remedy this, we propose the Pass-Bands Optimizer (PBO), a plug-and-play module that optimizes the temporal pass-band toward task-relevant motion bands. PBO introduces only two learnable parameters, and a lightweight consistency constraint that preserves semantics and boundaries, incurring negligible computational overhead and requires no architectural changes. PBO deliberately suppresses static components that contribute little to discrimination, effectively high passing the stream so that spiking activity concentrates on motion bearing content. On UCF101, PBO yields over ten percentage points improvement. On more complex multi-modal action recognition and weakly supervised video anomaly detection, PBO delivers consistent and significant gains, offering a new perspective for SNN based video processing and understanding.</li>
</ul>

<h3>Title: Stabilizing Consistency Training: A Flow Map Analysis and Self-Distillation</h3>
<ul>
<li><strong>Authors: </strong>Youngjoong Kim, Duhoe Kim, Woosung Kim, Jaesik Park</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22679">https://arxiv.org/abs/2601.22679</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22679">https://arxiv.org/pdf/2601.22679</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22679]] Stabilizing Consistency Training: A Flow Map Analysis and Self-Distillation(https://arxiv.org/abs/2601.22679)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Consistency models have been proposed for fast generative modeling, achieving results competitive with diffusion and flow models. However, these methods exhibit inherent instability and limited reproducibility when training from scratch, motivating subsequent work to explain and stabilize these issues. While these efforts have provided valuable insights, the explanations remain fragmented, and the theoretical relationships remain unclear. In this work, we provide a theoretical examination of consistency models by analyzing them from a flow map-based perspective. This joint analysis clarifies how training stability and convergence behavior can give rise to degenerate solutions. Building on these insights, we revisit self-distillation as a practical remedy for certain forms of suboptimal convergence and reformulate it to avoid excessive gradient norms for stable optimization. We further demonstrate that our strategy extends beyond image generation to diffusion-based policy learning, without reliance on a pretrained diffusion model for initialization, thereby illustrating its broader applicability.</li>
</ul>

<h3>Title: Visual Personalization Turing Test</h3>
<ul>
<li><strong>Authors: </strong>Rameen Abdal, James Burgess, Sergey Tulyakov, Kuan-Chieh Jackson Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22680">https://arxiv.org/abs/2601.22680</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22680">https://arxiv.org/pdf/2601.22680</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22680]] Visual Personalization Turing Test(https://arxiv.org/abs/2601.22680)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce the Visual Personalization Turing Test (VPTT), a new paradigm for evaluating contextual visual personalization based on perceptual indistinguishability, rather than identity replication. A model passes the VPTT if its output (image, video, 3D asset, etc.) is indistinguishable to a human or calibrated VLM judge from content a given person might plausibly create or share. To operationalize VPTT, we present the VPTT Framework, integrating a 10k-persona benchmark (VPTT-Bench), a visual retrieval-augmented generator (VPRAG), and the VPTT Score, a text-only metric calibrated against human and VLM judgments. We show high correlation across human, VLM, and VPTT evaluations, validating the VPTT Score as a reliable perceptual proxy. Experiments demonstrate that VPRAG achieves the best alignment-originality balance, offering a scalable and privacy-safe foundation for personalized generative AI.</li>
</ul>

<h3>Title: Do Transformers Have the Ability for Periodicity Generalization?</h3>
<ul>
<li><strong>Authors: </strong>Huanyu Liu, Ge Li, Yihong Dong, Sihan Wu, Peixu Wang, Sihao Cheng, Taozhi Chen, Kechi Zhang, Hao Zhu, Tongxuan Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22690">https://arxiv.org/abs/2601.22690</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22690">https://arxiv.org/pdf/2601.22690</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22690]] Do Transformers Have the Ability for Periodicity Generalization?(https://arxiv.org/abs/2601.22690)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) based on the Transformer have demonstrated strong performance across diverse tasks. However, current models still exhibit substantial limitations in out-of-distribution (OOD) generalization compared with humans. We investigate this gap through periodicity, one of the basic OOD scenarios. Periodicity captures invariance amid variation. Periodicity generalization represents a model's ability to extract periodic patterns from training data and generalize to OOD scenarios. We introduce a unified interpretation of periodicity from the perspective of abstract algebra and reasoning, including both single and composite periodicity, to explain why Transformers struggle to generalize periodicity. Then we construct Coper about composite periodicity, a controllable generative benchmark with two OOD settings, Hollow and Extrapolation. Experiments reveal that periodicity generalization in Transformers is limited, where models can memorize periodic data during training, but cannot generalize to unseen composite periodicity. We release the source code to support future research.</li>
</ul>

<h3>Title: OpenVTON-Bench: A Large-Scale High-Resolution Benchmark for Controllable Virtual Try-On Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Jin Li, Tao Chen, Shuai Jiang, Weijie Wang, Jingwen Luo, Chenhui Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22725">https://arxiv.org/abs/2601.22725</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22725">https://arxiv.org/pdf/2601.22725</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22725]] OpenVTON-Bench: A Large-Scale High-Resolution Benchmark for Controllable Virtual Try-On Evaluation(https://arxiv.org/abs/2601.22725)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in diffusion models have significantly elevated the visual fidelity of Virtual Try-On (VTON) systems, yet reliable evaluation remains a persistent bottleneck. Traditional metrics struggle to quantify fine-grained texture details and semantic consistency, while existing datasets fail to meet commercial standards in scale and diversity. We present OpenVTON-Bench, a large-scale benchmark comprising approximately 100K high-resolution image pairs (up to $1536 \times 1536$). The dataset is constructed using DINOv3-based hierarchical clustering for semantically balanced sampling and Gemini-powered dense captioning, ensuring a uniform distribution across 20 fine-grained garment categories. To support reliable evaluation, we propose a multi-modal protocol that measures VTON quality along five interpretable dimensions: background consistency, identity fidelity, texture fidelity, shape plausibility, and overall realism. The protocol integrates VLM-based semantic reasoning with a novel Multi-Scale Representation Metric based on SAM3 segmentation and morphological erosion, enabling the separation of boundary alignment errors from internal texture artifacts. Experimental results show strong agreement with human judgments (Kendall's $\tau$ of 0.833 vs. 0.611 for SSIM), establishing a robust benchmark for VTON evaluation.</li>
</ul>

<h3>Title: AR-BENCH: Benchmarking Legal Reasoning with Judgment Error Detection, Classification and Correction</h3>
<ul>
<li><strong>Authors: </strong>Yifei Li, Richong Zhang, Wanyu Tu, Zhijie Nie, Haokun Luo, Chuantao Yin, Pengchong Li</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22742">https://arxiv.org/abs/2601.22742</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22742">https://arxiv.org/pdf/2601.22742</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22742]] AR-BENCH: Benchmarking Legal Reasoning with Judgment Error Detection, Classification and Correction(https://arxiv.org/abs/2601.22742)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Legal judgments may contain errors due to the complexity of case circumstances and the abstract nature of legal concepts, while existing appellate review mechanisms face efficiency pressures from a surge in case volumes. Although current legal AI research focuses on tasks like judgment prediction and legal document generation, the task of judgment review differs fundamentally in its objectives and paradigm: it centers on detecting, classifying, and correcting errors after a judgment is issued, constituting anomaly detection rather than prediction or generation. To address this research gap, we introduce a novel task APPELLATE REVIEW, aiming to assess models' diagnostic reasoning and reliability in legal practice. We also construct a novel dataset benchmark AR-BENCH, which comprises 8,700 finely annotated decisions and 34,617 supplementary corpora. By evaluating 14 large language models, we reveal critical limitations in existing models' ability to identify legal application errors, providing empirical evidence for future improvements.</li>
</ul>

<h3>Title: Beauty and the Beast: Imperceptible Perturbations Against Diffusion-Based Face Swapping via Directional Attribute Editing</h3>
<ul>
<li><strong>Authors: </strong>Yilong Huang, Songze Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22744">https://arxiv.org/abs/2601.22744</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22744">https://arxiv.org/pdf/2601.22744</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22744]] Beauty and the Beast: Imperceptible Perturbations Against Diffusion-Based Face Swapping via Directional Attribute Editing(https://arxiv.org/abs/2601.22744)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion-based face swapping achieves state-of-the-art performance, yet it also exacerbates the potential harm of malicious face swapping to violate portraiture right or undermine personal reputation. This has spurred the development of proactive defense methods. However, existing approaches face a core trade-off: large perturbations distort facial structures, while small ones weaken protection effectiveness. To address these issues, we propose FaceDefense, an enhanced proactive defense framework against diffusion-based face swapping. Our method introduces a new diffusion loss to strengthen the defensive efficacy of adversarial examples, and employs a directional facial attribute editing to restore perturbation-induced distortions, thereby enhancing visual imperceptibility. A two-phase alternating optimization strategy is designed to generate final perturbed face images. Extensive experiments show that FaceDefense significantly outperforms existing methods in both imperceptibility and defense effectiveness, achieving a superior trade-off.</li>
</ul>

<h3>Title: OSNIP: Breaking the Privacy-Utility-Efficiency Trilemma in LLM Inference via Obfuscated Semantic Null Space</h3>
<ul>
<li><strong>Authors: </strong>Zhiyuan Cao, Zeyu Ma, Chenhao Yang, Han Zheng, Mingang Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22752">https://arxiv.org/abs/2601.22752</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22752">https://arxiv.org/pdf/2601.22752</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22752]] OSNIP: Breaking the Privacy-Utility-Efficiency Trilemma in LLM Inference via Obfuscated Semantic Null Space(https://arxiv.org/abs/2601.22752)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We propose Obfuscated Semantic Null space Injection for Privacy (OSNIP), a lightweight client-side encryption framework for privacy-preserving LLM inference. Generalizing the geometric intuition of linear kernels to the high-dimensional latent space of LLMs, we formally define the ``Obfuscated Semantic Null Space'', a high-dimensional regime that preserves semantic fidelity while enforcing near-orthogonality to the original embedding. By injecting perturbations that project the original embedding into this space, OSNIP ensures privacy without any post-processing. Furthermore, OSNIP employs a key-dependent stochastic mapping that synthesizes individualized perturbation trajectories unique to each user. Evaluations on 12 generative and classification benchmarks show that OSNIP achieves state-of-the-art performance, sharply reducing attack success rates while maintaining strong model utility under strict security constraints.</li>
</ul>

<h3>Title: Unveiling Scaling Behaviors in Molecular Language Models: Effects of Model Size, Data, and Representation</h3>
<ul>
<li><strong>Authors: </strong>Dong Xu, Qihua Pan, Sisi Yuan, Jianqiang Li, Zexuan Zhu, Junkai Ji</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22757">https://arxiv.org/abs/2601.22757</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22757">https://arxiv.org/pdf/2601.22757</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22757]] Unveiling Scaling Behaviors in Molecular Language Models: Effects of Model Size, Data, and Representation(https://arxiv.org/abs/2601.22757)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Molecular generative models, often employing GPT-style language modeling on molecular string representations, have shown promising capabilities when scaled to large datasets and model sizes. However, it remains unclear and subject to debate whether these models adhere to predictable scaling laws under fixed computational budgets, which is a crucial understanding for optimally allocating resources between model size, data volume, and molecular representation. In this study, we systematically investigate the scaling behavior of molecular language models across both pretraining and downstream tasks. We train 300 models and conduct over 10,000 experiments, rigorously controlling compute budgets while independently varying model size, number of training tokens, and molecular representation. Our results demonstrate clear scaling laws in molecular models for both pretraining and downstream transfer, reveal the substantial impact of molecular representation on performance, and explain previously observed inconsistencies in scaling behavior for molecular generation. Additionally, we publicly release the largest library of molecular language models to date to facilitate future research and development. Code and models are available at this https URL.</li>
</ul>

<h3>Title: Is Training Necessary for Anomaly Detection?</h3>
<ul>
<li><strong>Authors: </strong>Xingwu Zhang, Guanxuan Li, Paul Henderson, Gerardo Aragon-Camarasa, Zijun Long</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22763">https://arxiv.org/abs/2601.22763</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22763">https://arxiv.org/pdf/2601.22763</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22763]] Is Training Necessary for Anomaly Detection?(https://arxiv.org/abs/2601.22763)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Current state-of-the-art multi-class unsupervised anomaly detection (MUAD) methods rely on training encoder-decoder models to reconstruct anomaly-free features. We first show these approaches have an inherent fidelity-stability dilemma in how they detect anomalies via reconstruction residuals. We then abandon the reconstruction paradigm entirely and propose Retrieval-based Anomaly Detection (RAD). RAD is a training-free approach that stores anomaly-free features in a memory and detects anomalies through multi-level retrieval, matching test patches against the memory. Experiments demonstrate that RAD achieves state-of-the-art performance across four established benchmarks (MVTec-AD, VisA, Real-IAD, 3D-ADAM) under both standard and few-shot settings. On MVTec-AD, RAD reaches 96.7\% Pixel AUROC with just a single anomaly-free image compared to 98.5\% of RAD's full-data performance. We further prove that retrieval-based scores theoretically upper-bound reconstruction-residual scores. Collectively, these findings overturn the assumption that MUAD requires task-specific training, showing that state-of-the-art anomaly detection is feasible with memory-based retrieval. Our code is available at this https URL.</li>
</ul>

<h3>Title: Sparse Attention as Compact Kernel Regression</h3>
<ul>
<li><strong>Authors: </strong>Saul Santos, Nuno Gon√ßalves, Daniel C. McNamee, Andr√© F.T Martins</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22766">https://arxiv.org/abs/2601.22766</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22766">https://arxiv.org/pdf/2601.22766</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22766]] Sparse Attention as Compact Kernel Regression(https://arxiv.org/abs/2601.22766)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Recent work has revealed a link between self-attention mechanisms in transformers and test-time kernel regression via the Nadaraya-Watson estimator, with standard softmax attention corresponding to a Gaussian kernel. However, a kernel-theoretic understanding of sparse attention mechanisms is currently missing. In this paper, we establish a formal correspondence between sparse attention and compact (bounded support) kernels. We show that normalized ReLU and sparsemax attention arise from Epanechnikov kernel regression under fixed and adaptive normalizations, respectively. More generally, we demonstrate that widely used kernels in nonparametric density estimation -- including Epanechnikov, biweight, and triweight -- correspond to $\alpha$-entmax attention with $\alpha = 1 + \frac{1}{n}$ for $n \in \mathbb{N}$, while the softmax/Gaussian relationship emerges in the limit $n \to \infty$. This unified perspective explains how sparsity naturally emerges from kernel design and provides principled alternatives to heuristic top-$k$ attention and other associative memory mechanisms. Experiments with a kernel-regression-based variant of transformers -- Memory Mosaics -- show that kernel-based sparse attention achieves competitive performance on language modeling, in-context learning, and length generalization tasks, offering a principled framework for designing attention mechanisms.</li>
</ul>

<h3>Title: Okara: Detection and Attribution of TLS Man-in-the-Middle Vulnerabilities in Android Apps with Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Haoyun Yang, Ronghong Huang, Yong Fang, Beizeng Zhang, Junpu Guo, Zhanyu Wu, Xianghang Mi</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22770">https://arxiv.org/abs/2601.22770</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22770">https://arxiv.org/pdf/2601.22770</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22770]] Okara: Detection and Attribution of TLS Man-in-the-Middle Vulnerabilities in Android Apps with Foundation Models(https://arxiv.org/abs/2601.22770)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Transport Layer Security (TLS) is fundamental to secure online communication, yet vulnerabilities in certificate validation that enable Man-in-the-Middle (MitM) attacks remain a pervasive threat in Android apps. Existing detection tools are hampered by low-coverage UI interaction, costly instrumentation, and a lack of scalable root-cause analysis. We present Okara, a framework that leverages foundation models to automate the detection and deep attribution of TLS MitM Vulnerabilities (TMVs). Okara's detection component, TMV-Hunter, employs foundation model-driven GUI agents to achieve high-coverage app interaction, enabling efficient vulnerability discovery at scale. Deploying TMV-Hunter on 37,349 apps from Google Play and a third-party store revealed 8,374 (22.42%) vulnerable apps. Our measurement shows these vulnerabilities are widespread across all popularity levels, affect critical functionalities like authentication and code delivery, and are highly persistent with a median vulnerable lifespan of over 1,300 days. Okara's attribution component, TMV-ORCA, combines dynamic instrumentation with a novel LLM-based classifier to locate and categorize vulnerable code according to a comprehensive new taxonomy. This analysis attributes 41% of vulnerabilities to third-party libraries and identifies recurring insecure patterns, such as empty trust managers and flawed hostname verification. We have initiated a large-scale responsible disclosure effort and will release our tools and datasets to support further research and mitigation.</li>
</ul>

<h3>Title: Color Matters: Demosaicing-Guided Color Correlation Training for Generalizable AI-Generated Image Detection</h3>
<ul>
<li><strong>Authors: </strong>Nan Zhong, Yiran Xu, Mian Zou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22778">https://arxiv.org/abs/2601.22778</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22778">https://arxiv.org/pdf/2601.22778</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22778]] Color Matters: Demosaicing-Guided Color Correlation Training for Generalizable AI-Generated Image Detection(https://arxiv.org/abs/2601.22778)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, generative</a></li>
<li><strong>Abstract: </strong>As realistic AI-generated images threaten digital authenticity, we address the generalization failure of generative artifact-based detectors by exploiting the intrinsic properties of the camera imaging pipeline. Concretely, we investigate color correlations induced by the color filter array (CFA) and demosaicing, and propose a Demosaicing-guided Color Correlation Training (DCCT) framework for AI-generated image detection. By simulating the CFA sampling pattern, we decompose each color image into a single-channel input (as the condition) and the remaining two channels as the ground-truth targets (for prediction). A self-supervised U-Net is trained to model the conditional distribution of the missing channels from the given one, parameterized via a mixture of logistic functions. Our theoretical analysis reveals that DCCT targets a provable distributional difference in color-correlation features between photographic and AI-generated images. By leveraging these distinct features to construct a binary classifier, DCCT achieves state-of-the-art generalization and robustness, significantly outperforming prior methods across over 20 unseen generators.</li>
</ul>

<h3>Title: Trackly: A Unified SaaS Platform for User Behavior Analytics and Real Time Rule Based Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Md Zahurul Haque, Md. Hafizur Rahman, Yeahyea Sarker</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22800">https://arxiv.org/abs/2601.22800</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22800">https://arxiv.org/pdf/2601.22800</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22800]] Trackly: A Unified SaaS Platform for User Behavior Analytics and Real Time Rule Based Anomaly Detection(https://arxiv.org/abs/2601.22800)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Understanding user behavior is essential for improving digital experiences, optimizing business conversions, and mitigating threats like account takeovers, fraud, and bot attacks. Most platforms separate product analytics and security, creating fragmented visibility and delayed threat detection. Trackly, a scalable SaaS platform, unifies comprehensive user behavior analytics with real time, rule based anomaly detection. It tracks sessions, IP based geo location, device browser fingerprints, and granular events such as page views, add to cart, and checkouts. Suspicious activities logins from new devices or locations, impossible travel (Haversine formula), rapid bot like actions, VPN proxy usage, or multiple accounts per IP are flagged via configurable rules with weighted risk scoring, enabling transparent, explainable decisions. A real time dashboard provides global session maps, DAU MAU, bounce rates, and session durations. Integration is simplified with a lightweight JavaScript SDK and secure REST APIs. Implemented on a multi tenant microservices stack (this http URL Core, MongoDB, RabbitMQ, this http URL), Trackly achieved 98.1% accuracy, 97.7% precision, and 2.25% false positives on synthetic datasets, proving its efficiency for SMEs and ecommerce.</li>
</ul>

<h3>Title: Cascaded Flow Matching for Heterogeneous Tabular Data with Mixed-Type Features</h3>
<ul>
<li><strong>Authors: </strong>Markus Mueller, Kathrin Gruber, Dennis Fok</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22816">https://arxiv.org/abs/2601.22816</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22816">https://arxiv.org/pdf/2601.22816</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22816]] Cascaded Flow Matching for Heterogeneous Tabular Data with Mixed-Type Features(https://arxiv.org/abs/2601.22816)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Advances in generative modeling have recently been adapted to tabular data containing discrete and continuous features. However, generating mixed-type features that combine discrete states with an otherwise continuous distribution in a single feature remains challenging. We advance the state-of-the-art in diffusion models for tabular data with a cascaded approach. We first generate a low-resolution version of a tabular data row, that is, the collection of the purely categorical features and a coarse categorical representation of numerical features. Next, this information is leveraged in the high-resolution flow matching model via a novel guided conditional probability path and data-dependent coupling. The low-resolution representation of numerical features explicitly accounts for discrete outcomes, such as missing or inflated values, and therewith enables a more faithful generation of mixed-type features. We formally prove that this cascade tightens the transport cost bound. The results indicate that our model generates significantly more realistic samples and captures distributional details more accurately, for example, the detection score increases by 40%.</li>
</ul>

<h3>Title: NativeTok: Native Visual Tokenization for Improved Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Bin Wu, Mengqi Huang, Weinan Jia, Zhendong Mao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22837">https://arxiv.org/abs/2601.22837</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22837">https://arxiv.org/pdf/2601.22837</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22837]] NativeTok: Native Visual Tokenization for Improved Image Generation(https://arxiv.org/abs/2601.22837)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>VQ-based image generation typically follows a two-stage pipeline: a tokenizer encodes images into discrete tokens, and a generative model learns their dependencies for reconstruction. However, improved tokenization in the first stage does not necessarily enhance the second-stage generation, as existing methods fail to constrain token dependencies. This mismatch forces the generative model to learn from unordered distributions, leading to bias and weak coherence. To address this, we propose native visual tokenization, which enforces causal dependencies during tokenization. Building on this idea, we introduce NativeTok, a framework that achieves efficient reconstruction while embedding relational constraints within token sequences. NativeTok consists of: (1) a Meta Image Transformer (MIT) for latent image modeling, and (2) a Mixture of Causal Expert Transformer (MoCET), where each lightweight expert block generates a single token conditioned on prior tokens and latent features. We further design a Hierarchical Native Training strategy that updates only new expert blocks, ensuring training efficiency. Extensive experiments demonstrate the effectiveness of NativeTok.</li>
</ul>

<h3>Title: Neural Clothing Tryer: Customized Virtual Try-On via Semantic Enhancement and Controlling Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Zhijing Yang, Weiwei Zhang, Mingliang Yang, Siyuan Peng, Yukai Shi, Junpeng Tan, Tianshui Chen, Liruo Zhong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22838">https://arxiv.org/abs/2601.22838</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22838">https://arxiv.org/pdf/2601.22838</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22838]] Neural Clothing Tryer: Customized Virtual Try-On via Semantic Enhancement and Controlling Diffusion Model(https://arxiv.org/abs/2601.22838)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This work aims to address a novel Customized Virtual Try-ON (Cu-VTON) task, enabling the superimposition of a specified garment onto a model that can be customized in terms of appearance, posture, and additional attributes. Compared with traditional VTON task, it enables users to tailor digital avatars to their individual preferences, thereby enhancing the virtual fitting experience with greater flexibility and engagement. To address this task, we introduce a Neural Clothing Tryer (NCT) framework, which exploits the advanced diffusion models equipped with semantic enhancement and controlling modules to better preserve semantic characterization and textural details of the garment and meanwhile facilitating the flexible editing of the model's postures and appearances. Specifically, NCT introduces a semantic-enhanced module to take semantic descriptions of garments and utilizes a visual-language encoder to learn aligned features across modalities. The aligned features are served as condition input to the diffusion model to enhance the preservation of the garment's semantics. Then, a semantic controlling module is designed to take the garment image, tailored posture image, and semantic description as input to maintain garment details while simultaneously editing model postures, expressions, and various attributes. Extensive experiments on the open available benchmark demonstrate the superior performance of the proposed NCT framework.</li>
</ul>

<h3>Title: How Much of a Model Do We Need? Redundancy and Slimmability in Remote Sensing Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Leonard Hackel, Tom Burgert, Beg√ºm Demir</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22841">https://arxiv.org/abs/2601.22841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22841">https://arxiv.org/pdf/2601.22841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22841]] How Much of a Model Do We Need? Redundancy and Slimmability in Remote Sensing Foundation Models(https://arxiv.org/abs/2601.22841)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Large-scale foundation models (FMs) in remote sensing (RS) are developed based on the paradigms established in computer vision (CV) and have shown promise for various Earth observation applications. However, the direct transfer of scaling assumptions from CV to RS has not been adequately examined. We hypothesize that RS FMs enter an overparameterized regime at substantially smaller scales than their CV counterparts, where increasing parameter count primarily induces redundant representations rather than qualitatively new abstractions. To test this hypothesis, we use post-hoc slimming, where we uniformly reduce the width of pretrained encoder, as a tool to measure representational redundancy across six state-of-the-art RS FMs on four downstream classification tasks. Our findings reveal a significant contrast with those in the CV domain: while a post-hoc slimmed masked autoencoder (MAE) trained on ImageNet retains less than 10% accuracy at 1% FLOPs, RS FMs maintain over 71% relative accuracy at the same budget. This sevenfold difference provides strong empirical support for our hypothesis. We further demonstrate that learned slimmable training can improve both Momentum Contrast (MoCo)- and MAE- based models. In addition, through the explained variance ratio and the feature correlation analysis, we provide mechanistic explanations showing that RS FMs distribute task-relevant information with high redundancy. Our findings establish post-hoc slimmability as both a practical deployment strategy for resource-constrained environments and a diagnostic tool that challenges the prevailing scaling paradigm in RS. Upon acceptance, we will publish all code.</li>
</ul>

<h3>Title: Unconditional flow-based time series generation with equivariance-regularised latent spaces</h3>
<ul>
<li><strong>Authors: </strong>Camilo Carvajal Reyes, Felipe Tobar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22848">https://arxiv.org/abs/2601.22848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22848">https://arxiv.org/pdf/2601.22848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22848]] Unconditional flow-based time series generation with equivariance-regularised latent spaces(https://arxiv.org/abs/2601.22848)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Flow-based models have proven successful for time-series generation, particularly when defined in lower-dimensional latent spaces that enable efficient sampling. However, how to design latent representations with desirable equivariance properties for time-series generative modelling remains underexplored. In this work, we propose a latent flow-matching framework in which equivariance is explicitly encouraged through a simple regularisation of a pre-trained autoencoder. Specifically, we introduce an equivariance loss that enforces consistency between transformed signals and their reconstructions, and use it to fine-tune latent spaces with respect to basic time-series transformations such as translation and amplitude scaling. We show that these equivariance-regularised latent spaces improve generation quality while preserving the computational advantages of latent flow models. Experiments on multiple real-world datasets demonstrate that our approach consistently outperforms existing diffusion-based baselines in standard time-series generation metrics, while achieving orders-of-magnitude faster sampling. These results highlight the practical benefits of incorporating geometric inductive biases into latent generative models for time series.</li>
</ul>

<h3>Title: When Anomalies Depend on Context: Learning Conditional Compatibility for Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Shashank Mishra, Didier Stricker, Jason Rambach</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22868">https://arxiv.org/abs/2601.22868</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22868">https://arxiv.org/pdf/2601.22868</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22868]] When Anomalies Depend on Context: Learning Conditional Compatibility for Anomaly Detection(https://arxiv.org/abs/2601.22868)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Anomaly detection is often formulated under the assumption that abnormality is an intrinsic property of an observation, independent of context. This assumption breaks down in many real-world settings, where the same object or action may be normal or anomalous depending on latent contextual factors (e.g., running on a track versus on a highway). We revisit \emph{contextual anomaly detection}, classically defined as context-dependent abnormality, and operationalize it in the visual domain, where anomaly labels depend on subject--context compatibility rather than intrinsic appearance. To enable systematic study of this setting, we introduce CAAD-3K, a benchmark that isolates contextual anomalies by controlling subject identity while varying context. We further propose a conditional compatibility learning framework that leverages vision--language representations to model subject--context relationships under limited supervision. Our method substantially outperforms existing approaches on CAAD-3K and achieves state-of-the-art performance on MVTec-AD and VisA, demonstrating that modeling context dependence complements traditional structural anomaly detection. Our code and dataset will be publicly released.</li>
</ul>

<h3>Title: Synthetic Time Series Generation via Complex Networks</h3>
<ul>
<li><strong>Authors: </strong>Jaime Vale, Vanessa Freitas Silva, Maria Eduarda Silva, Fernando Silva</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22879">https://arxiv.org/abs/2601.22879</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22879">https://arxiv.org/pdf/2601.22879</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22879]] Synthetic Time Series Generation via Complex Networks(https://arxiv.org/abs/2601.22879)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Time series data are essential for a wide range of applications, particularly in developing robust machine learning models. However, access to high-quality datasets is often limited due to privacy concerns, acquisition costs, and labeling challenges. Synthetic time series generation has emerged as a promising solution to address these constraints. In this work, we present a framework for generating synthetic time series by leveraging complex networks mappings. Specifically, we investigate whether time series transformed into Quantile Graphs (QG) -- and then reconstructed via inverse mapping -- can produce synthetic data that preserve the statistical and structural properties of the original. We evaluate the fidelity and utility of the generated data using both simulated and real-world datasets, and compare our approach against state-of-the-art Generative Adversarial Network (GAN) methods. Results indicate that our quantile graph-based methodology offers a competitive and interpretable alternative for synthetic time series generation.</li>
</ul>

<h3>Title: MoVE: Mixture of Value Embeddings -- A New Axis for Scaling Parametric Memory in Autoregressive Models</h3>
<ul>
<li><strong>Authors: </strong>Yangyan Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22887">https://arxiv.org/abs/2601.22887</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22887">https://arxiv.org/pdf/2601.22887</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22887]] MoVE: Mixture of Value Embeddings -- A New Axis for Scaling Parametric Memory in Autoregressive Models(https://arxiv.org/abs/2601.22887)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Autoregressive sequence modeling stands as the cornerstone of modern Generative AI, powering results across diverse modalities ranging from text generation to image generation. However, a fundamental limitation of this paradigm is the rigid structural coupling of model capacity to computational cost: expanding a model's parametric memory -- its repository of factual knowledge or visual patterns -- traditionally requires deepening or widening the network, which incurs a proportional rise in active FLOPs. In this work, we introduce $\textbf{MoVE (Mixture of Value Embeddings)}$, a mechanism that breaks this coupling and establishes a new axis for scaling capacity. MoVE decouples memory from compute by introducing a global bank of learnable value embeddings shared across all attention layers. For every step in the sequence, the model employs a differentiable soft gating mechanism to dynamically mix retrieved concepts from this bank into the standard value projection. This architecture allows parametric memory to be scaled independently of network depth by simply increasing the number of embedding slots. We validate MoVE through strictly controlled experiments on two representative applications of autoregressive modeling: Text Generation and Image Generation. In both domains, MoVE yields consistent performance improvements over standard and layer-wise memory baselines, enabling the construction of "memory-dense" models that achieve lower perplexity and higher fidelity than their dense counterparts at comparable compute budgets.</li>
</ul>

<h3>Title: DiffuSpeech: Silent Thought, Spoken Answer via Unified Speech-Text Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Lou, Ziming Wu, Yaochen Wang, Yong Liu, Yingxuan Ren, Fuming Lai, Shaobing Lian, Jie Tang, Yang You</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.SD</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22889">https://arxiv.org/abs/2601.22889</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22889">https://arxiv.org/pdf/2601.22889</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22889]] DiffuSpeech: Silent Thought, Spoken Answer via Unified Speech-Text Diffusion(https://arxiv.org/abs/2601.22889)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Current speech language models generate responses directly without explicit reasoning, leading to errors that cannot be corrected once audio is produced. We introduce \textbf{``Silent Thought, Spoken Answer''} -- a paradigm where speech LLMs generate internal text reasoning alongside spoken responses, with thinking traces informing speech quality. To realize this, we present \method{}, the first diffusion-based speech-text language model supporting both understanding and generation, unifying discrete text and tokenized speech under a single masked diffusion framework. Unlike autoregressive approaches, \method{} jointly generates reasoning traces and speech tokens through iterative denoising, with modality-specific masking schedules. We also construct \dataset{}, the first speech QA dataset with paired text reasoning traces, containing 26K samples totaling 319 hours. Experiments show \method{} achieves state-of-the-art speech-to-speech QA accuracy, outperforming the best baseline by up to 9 points, while attaining the best TTS quality among generative models (6.2\% WER) and preserving language understanding (66.2\% MMLU). Ablations confirm that both the diffusion architecture and thinking traces contribute to these gains.</li>
</ul>

<h3>Title: DINO-SAE: DINO Spherical Autoencoder for High-Fidelity Image Reconstruction and Generation</h3>
<ul>
<li><strong>Authors: </strong>Hun Chang, Byunghee Cha, Jong Chul Ye</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22904">https://arxiv.org/abs/2601.22904</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22904">https://arxiv.org/pdf/2601.22904</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22904]] DINO-SAE: DINO Spherical Autoencoder for High-Fidelity Image Reconstruction and Generation(https://arxiv.org/abs/2601.22904)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model, generative</a></li>
<li><strong>Abstract: </strong>Recent studies have explored using pretrained Vision Foundation Models (VFMs) such as DINO for generative autoencoders, showing strong generative performance. Unfortunately, existing approaches often suffer from limited reconstruction fidelity due to the loss of high-frequency details. In this work, we present the DINO Spherical Autoencoder (DINO-SAE), a framework that bridges semantic representation and pixel-level reconstruction. Our key insight is that semantic information in contrastive representations is primarily encoded in the direction of feature vectors, while forcing strict magnitude matching can hinder the encoder from preserving fine-grained details. To address this, we introduce Hierarchical Convolutional Patch Embedding module that enhances local structure and texture preservation, and Cosine Similarity Alignment objective that enforces semantic consistency while allowing flexible feature magnitudes for detail retention. Furthermore, leveraging the observation that SSL-based foundation model representations intrinsically lie on a hypersphere, we employ Riemannian Flow Matching to train a Diffusion Transformer (DiT) directly on this spherical latent manifold. Experiments on ImageNet-1K demonstrate that our approach achieves state-of-the-art reconstruction quality, reaching 0.37 rFID and 26.2 dB PSNR, while maintaining strong semantic alignment to the pretrained VFM. Notably, our Riemannian Flow Matching-based DiT exhibits efficient convergence, achieving a gFID of 3.47 at 80 epochs.</li>
</ul>

<h3>Title: Multi-Cue Anomaly Detection and Localization under Data Contamination</h3>
<ul>
<li><strong>Authors: </strong>Anindya Sundar Das, Monowar Bhuyan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22913">https://arxiv.org/abs/2601.22913</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22913">https://arxiv.org/pdf/2601.22913</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22913]] Multi-Cue Anomaly Detection and Localization under Data Contamination(https://arxiv.org/abs/2601.22913)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Visual anomaly detection in real-world industrial settings faces two major limitations. First, most existing methods are trained on purely normal data or on unlabeled datasets assumed to be predominantly normal, presuming the absence of contamination, an assumption that is rarely satisfied in practice. Second, they assume no access to labeled anomaly samples, limiting the model from learning discriminative characteristics of true anomalies. Therefore, these approaches often struggle to distinguish anomalies from normal instances, resulting in reduced detection and weak localization performance. In real-world applications, where training data are frequently contaminated with anomalies, such methods fail to deliver reliable performance. In this work, we propose a robust anomaly detection framework that integrates limited anomaly supervision into the adaptive deviation learning paradigm. We introduce a composite anomaly score that combines three complementary components: a deviation score capturing statistical irregularity, an entropy-based uncertainty score reflecting predictive inconsistency, and a segmentation-based score highlighting spatial abnormality. This unified scoring mechanism enables accurate detection and supports gradient-based localization, providing intuitive and explainable visual evidence of anomalous regions. Following the few-anomaly paradigm, we incorporate a small set of labeled anomalies during training while simultaneously mitigating the influence of contaminated samples through adaptive instance weighting. Extensive experiments on the MVTec and VisA benchmarks demonstrate that our framework outperforms state-of-the-art baselines and achieves strong detection and localization performance, interpretability, and robustness under various levels of data contamination.</li>
</ul>

<h3>Title: Relaxing Positional Alignment in Masked Diffusion Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mengyu Ye, Ryosuke Takahashi, Keito Kudo, Jun Suzuki</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22947">https://arxiv.org/abs/2601.22947</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22947">https://arxiv.org/pdf/2601.22947</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22947]] Relaxing Positional Alignment in Masked Diffusion Language Models(https://arxiv.org/abs/2601.22947)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Masked diffusion language models (MDLMs) have emerged as a promising alternative to dominant autoregressive approaches. Although they achieve competitive performance on several tasks, a substantial gap remains in open-ended text generation. We hypothesize that one cause of this gap is that strict positional prediction makes MDLM decoding highly sensitive to token misalignment, and we show through controlled interventions that a one-position shift can severely disrupt semantics. This observation suggests that enforcing strict positional supervision during training is misaligned with the irreversible denoising dynamics of MDLM decoding. Motivated by this mismatch, we adopt an alignment-flexible supervision strategy during fine-tuning. Specifically, we introduce a special token <slack> via the connectionist temporal classification objective. We apply this approach to the widely used MDLM model and conduct experiments on five open-ended text generation benchmarks. Our method consistently outperforms the original model and improves robustness to positional shifts, indicating that relaxing strict positional supervision is an important factor in improving generation quality in MDLMs.</li>
</ul>

<h3>Title: Residual Context Diffusion Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yuezhou Hu, Harman Singh, Monishwaran Maheswaran, Haocheng Xi, Coleman Hooper, Jintao Zhang, Aditya Tomar, Michael W. Mahoney, Sewon Min, Mehrdad Farajtabar, Kurt Keutzer, Amir Gholami, Chenfeng Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22954">https://arxiv.org/abs/2601.22954</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22954">https://arxiv.org/pdf/2601.22954</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22954]] Residual Context Diffusion Language Models(https://arxiv.org/abs/2601.22954)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion Large Language Models (dLLMs) have emerged as a promising alternative to purely autoregressive language models because they can decode multiple tokens in parallel. However, state-of-the-art block-wise dLLMs rely on a "remasking" mechanism that decodes only the most confident tokens and discards the rest, effectively wasting computation. We demonstrate that recycling computation from the discarded tokens is beneficial, as these tokens retain contextual information useful for subsequent decoding iterations. In light of this, we propose Residual Context Diffusion (RCD), a module that converts these discarded token representations into contextual residuals and injects them back for the next denoising step. RCD uses a decoupled two-stage training pipeline to bypass the memory bottlenecks associated with backpropagation. We validate our method on both long CoT reasoning (SDAR) and short CoT instruction following (LLaDA) models. We demonstrate that a standard dLLM can be efficiently converted to the RCD paradigm with merely ~1 billion tokens. RCD consistently improves frontier dLLMs by 5-10 points in accuracy with minimal extra computation overhead across a wide range of benchmarks. Notably, on the most challenging AIME tasks, RCD nearly doubles baseline accuracy and attains up to 4-5x fewer denoising steps at equivalent accuracy levels.</li>
</ul>

<h3>Title: Improving Supervised Machine Learning Performance in Optical Quality Control via Generative AI for Dataset Expansion</h3>
<ul>
<li><strong>Authors: </strong>Dennis Sprute, Hanna Senke, Holger Flatt</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22961">https://arxiv.org/abs/2601.22961</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22961">https://arxiv.org/pdf/2601.22961</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22961]] Improving Supervised Machine Learning Performance in Optical Quality Control via Generative AI for Dataset Expansion(https://arxiv.org/abs/2601.22961)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Supervised machine learning algorithms play a crucial role in optical quality control within industrial production. These approaches require representative datasets for effective model training. However, while non-defective components are frequent, defective parts are rare in production, resulting in highly imbalanced datasets that adversely impact model performance. Existing strategies to address this challenge, such as specialized loss functions or traditional data augmentation techniques, have limitations, including the need for careful hyperparameter tuning or the alteration of only simple image features. Therefore, this work explores the potential of generative artificial intelligence (GenAI) as an alternative method for expanding limited datasets and enhancing supervised machine learning performance. Specifically, we investigate Stable Diffusion and CycleGAN as image generation models, focusing on the segmentation of combine harvester components in thermal images for subsequent defect detection. Our results demonstrate that dataset expansion using Stable Diffusion yields the most significant improvement, enhancing segmentation performance by 4.6 %, resulting in a Mean Intersection over Union (Mean IoU) of 84.6 %.</li>
</ul>

<h3>Title: dgMARK: Decoding-Guided Watermarking for Diffusion Language Models</h3>
<ul>
<li><strong>Authors: </strong>Pyo Min Hong, Albert No</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22985">https://arxiv.org/abs/2601.22985</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22985">https://arxiv.org/pdf/2601.22985</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22985]] dgMARK: Decoding-Guided Watermarking for Diffusion Language Models(https://arxiv.org/abs/2601.22985)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We propose dgMARK, a decoding-guided watermarking method for discrete diffusion language models (dLLMs). Unlike autoregressive models, dLLMs can generate tokens in arbitrary order. While an ideal conditional predictor would be invariant to this order, practical dLLMs exhibit strong sensitivity to the unmasking order, creating a new channel for watermarking. dgMARK steers the unmasking order toward positions whose high-reward candidate tokens satisfy a simple parity constraint induced by a binary hash, without explicitly reweighting the model's learned probabilities. The method is plug-and-play with common decoding strategies (e.g., confidence, entropy, and margin-based ordering) and can be strengthened with a one-step lookahead variant. Watermarks are detected via elevated parity-matching statistics, and a sliding-window detector ensures robustness under post-editing operations including insertion, deletion, substitution, and paraphrasing.</li>
</ul>

<h3>Title: Self-Supervised Slice-to-Volume Reconstruction with Gaussian Representations for Fetal MRI</h3>
<ul>
<li><strong>Authors: </strong>Yinsong Wang, Thomas Fletcher, Xinzhe Luo, Aine Travers Dineen, Rhodri Cusack, Chen Qin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.22990">https://arxiv.org/abs/2601.22990</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.22990">https://arxiv.org/pdf/2601.22990</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.22990]] Self-Supervised Slice-to-Volume Reconstruction with Gaussian Representations for Fetal MRI(https://arxiv.org/abs/2601.22990)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Reconstructing 3D fetal MR volumes from motion-corrupted stacks of 2D slices is a crucial and challenging task. Conventional slice-to-volume reconstruction (SVR) methods are time-consuming and require multiple orthogonal stacks for reconstruction. While learning-based SVR approaches have significantly reduced the time required at the inference stage, they heavily rely on ground truth information for training, which is inaccessible in practice. To address these challenges, we propose GaussianSVR, a self-supervised framework for slice-to-volume reconstruction. GaussianSVR represents the target volume using 3D Gaussian representations to achieve high-fidelity reconstruction. It leverages a simulated forward slice acquisition model to enable self-supervised training, alleviating the need for ground-truth volumes. Furthermore, to enhance both accuracy and efficiency, we introduce a multi-resolution training strategy that jointly optimizes Gaussian parameters and spatial transformations across different resolution levels. Experiments show that GaussianSVR outperforms the baseline methods on fetal MR volumetric reconstruction. Code will be available upon acceptance.</li>
</ul>

<h3>Title: Causal Characterization of Measurement and Mechanistic Anomalies</h3>
<ul>
<li><strong>Authors: </strong>Hendrik Suhr, David Kaltenpoth, Jilles Vreeken</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.23026">https://arxiv.org/abs/2601.23026</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.23026">https://arxiv.org/pdf/2601.23026</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.23026]] Causal Characterization of Measurement and Mechanistic Anomalies(https://arxiv.org/abs/2601.23026)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Root cause analysis of anomalies aims to identify those features that cause the deviation from the normal process. Existing methods ignore, however, that anomalies can arise through two fundamentally different processes: measurement errors, where data was generated normally but one or more values were recorded incorrectly, and mechanism shifts, where the causal process generating the data changed. While measurement errors can often be safely corrected, mechanistic anomalies require careful consideration. We define a causal model that explicitly captures both types by treating outliers as latent interventions on latent ("true") and observed ("measured") variables. We show that they are identifiable, and propose a maximum likelihood estimation approach to put this to practice. Experiments show that our method matches state-of-the-art performance in root cause localization, while it additionally enables accurate classification of anomaly types, and remains robust even when the causal DAG is unknown.</li>
</ul>

<h3>Title: One-shot Optimized Steering Vector for Hallucination Mitigation for VLMs</h3>
<ul>
<li><strong>Authors: </strong>Youxu Shi, Suorong Yang, Dong Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.23041">https://arxiv.org/abs/2601.23041</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.23041">https://arxiv.org/pdf/2601.23041</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.23041]] One-shot Optimized Steering Vector for Hallucination Mitigation for VLMs(https://arxiv.org/abs/2601.23041)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Vision Language Models (VLMs) achieve strong performance on multimodal tasks but still suffer from hallucination and safety-related failures that persist even at scale. Steering offers a lightweight technique to improve model performance. However, steering, whether input-dependent or input-independent, achieves a meaningful trade-off between efficiency and effectiveness. In this work, we observe that steering vectors can generalize across inputs when tasks share aligned semantic intent. Based on this insight, we propose \textbf{OSGA} (\textbf{O}ne-shot \textbf{S}teering with \textbf{G}enerative \textbf{A}nchor), an input-independent framework that improves model performance with a single optimization instance. OSGA first selects an informative sample via a variance-based data selection strategy and learns a single steering vector with a contrastive objective with generative anchor regularization. The resulting vector can be universally applied at a certain layer during inference time without modifying model parameters. Experiments across multiple benchmarks show that a single OSGA-optimized steering vector consistently improves hallucination mitigation and safety enhancement with negligible overhead, highlighting one-shot steering as a practical and scalable solution for reliable VLMs.</li>
</ul>

<h3>Title: Adaptive Edge Learning for Density-Aware Graph Generation</h3>
<ul>
<li><strong>Authors: </strong>Seyedeh Ava Razi Razavi, James Sargant, Sheridan Houghten, Renata Dividino</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.23052">https://arxiv.org/abs/2601.23052</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.23052">https://arxiv.org/pdf/2601.23052</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.23052]] Adaptive Edge Learning for Density-Aware Graph Generation(https://arxiv.org/abs/2601.23052)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generating realistic graph-structured data is challenging due to discrete structures, variable sizes, and class-specific connectivity patterns that resist conventional generative modelling. While recent graph generation methods employ generative adversarial network (GAN) frameworks to handle permutation invariance and irregular topologies, they typically rely on random edge sampling with fixed probabilities, limiting their capacity to capture complex structural dependencies between nodes. We propose a density-aware conditional graph generation framework using Wasserstein GANs (WGAN) that replaces random sampling with a learnable distance-based edge predictor. Our approach embeds nodes into a latent space where proximity correlates with edge likelihood, enabling the generator to learn meaningful connectivity patterns. A differentiable edge predictor determines pairwise relationships directly from node embeddings, while a density-aware selection mechanism adaptively controls edge density to match class-specific sparsity distributions observed in real graphs. We train the model using a WGAN with gradient penalty, employing a GCN-based critic to ensure generated graphs exhibit realistic topology and align with target class distributions. Experiments on benchmark datasets demonstrate that our method produces graphs with superior structural coherence and class-consistent connectivity compared to existing baselines. The learned edge predictor captures complex relational patterns beyond simple heuristics, generating graphs whose density and topology closely match real structural distributions. Our results show improved training stability and controllable synthesis, making the framework effective for realistic graph generation and data augmentation. Source code is publicly available at this https URL.</li>
</ul>

<h3>Title: HierLoc: Hyperbolic Entity Embeddings for Hierarchical Visual Geolocation</h3>
<ul>
<li><strong>Authors: </strong>Hari Krishna Gadi, Daniel Matos, Hongyi Luo, Lu Liu, Yongliang Wang, Yanfeng Zhang, Liqiu Meng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.23064">https://arxiv.org/abs/2601.23064</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.23064">https://arxiv.org/pdf/2601.23064</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.23064]] HierLoc: Hyperbolic Entity Embeddings for Hierarchical Visual Geolocation(https://arxiv.org/abs/2601.23064)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Visual geolocalization, the task of predicting where an image was taken, remains challenging due to global scale, visual ambiguity, and the inherently hierarchical structure of geography. Existing paradigms rely on either large-scale retrieval, which requires storing a large number of image embeddings, grid-based classifiers that ignore geographic continuity, or generative models that diffuse over space but struggle with fine detail. We introduce an entity-centric formulation of geolocation that replaces image-to-image retrieval with a compact hierarchy of geographic entities embedded in Hyperbolic space. Images are aligned directly to country, region, subregion, and city entities through Geo-Weighted Hyperbolic contrastive learning by directly incorporating haversine distance into the contrastive objective. This hierarchical design enables interpretable predictions and efficient inference with 240k entity embeddings instead of over 5 million image embeddings on the OSV5M benchmark, on which our method establishes a new state-of-the-art performance. Compared to the current methods in the literature, it reduces mean geodesic error by 19.5\%, while improving the fine-grained subregion accuracy by 43%. These results demonstrate that geometry-aware hierarchical embeddings provide a scalable and conceptually new alternative for global image geolocation.</li>
</ul>

<h3>Title: ExplainerPFN: Towards tabular foundation models for model-free zero-shot feature importance estimations</h3>
<ul>
<li><strong>Authors: </strong>Joao Fonseca, Julia Stoyanovich</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.23068">https://arxiv.org/abs/2601.23068</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.23068">https://arxiv.org/pdf/2601.23068</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.23068]] ExplainerPFN: Towards tabular foundation models for model-free zero-shot feature importance estimations(https://arxiv.org/abs/2601.23068)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Computing the importance of features in supervised classification tasks is critical for model interpretability. Shapley values are a widely used approach for explaining model predictions, but require direct access to the underlying model, an assumption frequently violated in real-world deployments. Further, even when model access is possible, their exact computation may be prohibitively expensive. We investigate whether meaningful Shapley value estimations can be obtained in a zero-shot setting, using only the input data distribution and no evaluations of the target model. To this end, we introduce ExplainerPFN, a tabular foundation model built on TabPFN that is pretrained on synthetic datasets generated from random structural causal models and supervised using exact or near-exact Shapley values. Once trained, ExplainerPFN predicts feature attributions for unseen tabular datasets without model access, gradients, or example explanations. Our contributions are fourfold: (1) we show that few-shot learning-based explanations can achieve high fidelity to SHAP values with as few as two reference observations; (2) we propose ExplainerPFN, the first zero-shot method for estimating Shapley values without access to the underlying model or reference explanations; (3) we provide an open-source implementation of ExplainerPFN, including the full training pipeline and synthetic data generator; and (4) through extensive experiments on real and synthetic datasets, we show that ExplainerPFN achieves performance competitive with few-shot surrogate explainers that rely on 2-10 SHAP examples.</li>
</ul>

<h3>Title: SplineFlow: Flow Matching for Dynamical Systems with B-Spline Interpolants</h3>
<ul>
<li><strong>Authors: </strong>Santanu Subhash Rathod, Pietro Li√≤, Xiao Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.23072">https://arxiv.org/abs/2601.23072</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.23072">https://arxiv.org/pdf/2601.23072</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.23072]] SplineFlow: Flow Matching for Dynamical Systems with B-Spline Interpolants(https://arxiv.org/abs/2601.23072)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Flow matching is a scalable generative framework for characterizing continuous normalizing flows with wide-range applications. However, current state-of-the-art methods are not well-suited for modeling dynamical systems, as they construct conditional paths using linear interpolants that may not capture the underlying state evolution, especially when learning higher-order dynamics from irregular sampled observations. Constructing unified paths that satisfy multi-marginal constraints across observations is challenging, since na√Øve higher-order polynomials tend to be unstable and oscillatory. We introduce SplineFlow, a theoretically grounded flow matching algorithm that jointly models conditional paths across observations via B-spline interpolation. Specifically, SplineFlow exploits the smoothness and stability of B-spline bases to learn the complex underlying dynamics in a structured manner while ensuring the multi-marginal requirements are met. Comprehensive experiments across various deterministic and stochastic dynamical systems of varying complexity, as well as on cellular trajectory inference tasks, demonstrate the strong improvement of SplineFlow over existing baselines. Our code is available at: this https URL.</li>
</ul>

<h3>Title: To See Far, Look Close: Evolutionary Forecasting for Long-term Time Series</h3>
<ul>
<li><strong>Authors: </strong>Jiaming Ma, Siyuan Mu, Ruilin Tang, Haofeng Ma, Qihe Huang, Zhengyang Zhou, Pengkun Wang, Binwu Wang, Yang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.23114">https://arxiv.org/abs/2601.23114</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.23114">https://arxiv.org/pdf/2601.23114</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.23114]] To See Far, Look Close: Evolutionary Forecasting for Long-term Time Series(https://arxiv.org/abs/2601.23114)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, anomaly</a></li>
<li><strong>Abstract: </strong>The prevailing Direct Forecasting (DF) paradigm dominates Long-term Time Series Forecasting (LTSF) by forcing models to predict the entire future horizon in a single forward pass. While efficient, this rigid coupling of output and evaluation horizons necessitates computationally prohibitive re-training for every target horizon. In this work, we uncover a counter-intuitive optimization anomaly: models trained on short horizons-when coupled with our proposed Evolutionary Forecasting (EF) paradigm-significantly outperform those trained directly on long horizons. We attribute this success to the mitigation of a fundamental optimization pathology inherent in DF, where conflicting gradients from distant futures cripple the learning of local dynamics. We establish EF as a unified generative framework, proving that DF is merely a degenerate special case of EF. Extensive experiments demonstrate that a singular EF model surpasses task-specific DF ensembles across standard benchmarks and exhibits robust asymptotic stability in extreme extrapolation. This work propels a paradigm shift in LTSF: moving from passive Static Mapping to autonomous Evolutionary Reasoning.</li>
</ul>

<h3>Title: Securing Time in Energy IoT: A Clock-Dynamics-Aware Spatio-Temporal Graph Attention Network for Clock Drift Attacks and Y2K38 Failures</h3>
<ul>
<li><strong>Authors: </strong>Saeid Jamshidi, Omar Abdul Wahab, Rolando Herrero, Foutse Khomh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.23147">https://arxiv.org/abs/2601.23147</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.23147">https://arxiv.org/pdf/2601.23147</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.23147]] Securing Time in Energy IoT: A Clock-Dynamics-Aware Spatio-Temporal Graph Attention Network for Clock Drift Attacks and Y2K38 Failures(https://arxiv.org/abs/2601.23147)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>The integrity of time in distributed Internet of Things (IoT) devices is crucial for reliable operation in energy cyber-physical systems, such as smart grids and microgrids. However, IoT systems are vulnerable to clock drift, time-synchronization manipulation, and timestamp discontinuities, such as the Year 2038 (Y2K38) Unix overflow, all of which disrupt temporal ordering. Conventional anomaly-detection models, which assume reliable timestamps, fail to capture temporal inconsistencies. This paper introduces STGAT (Spatio-Temporal Graph Attention Network), a framework that models both temporal distortion and inter-device consistency in energy IoT systems. STGAT combines drift-aware temporal embeddings and temporal self-attention to capture corrupted time evolution at individual devices, and uses graph attention to model spatial propagation of timing errors. A curvature-regularized latent representation geometrically separates normal clock evolution from anomalies caused by drift, synchronization offsets, and overflow events. Experimental results on energy IoT telemetry with controlled timing perturbations show that STGAT achieves 95.7% accuracy, outperforming recurrent, transformer, and graph-based baselines with significant improvements (d > 1.8, p < 0.001). Additionally, STGAT reduces detection delay by 26%, achieving a 2.3-time-step delay while maintaining stable performance under overflow, drift, and physical inconsistencies.</li>
</ul>

<h3>Title: Manifold-Aware Perturbations for Constrained Generative Modeling</h3>
<ul>
<li><strong>Authors: </strong>Katherine Keegan, Lars Ruthotto</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.23151">https://arxiv.org/abs/2601.23151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.23151">https://arxiv.org/pdf/2601.23151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.23151]] Manifold-Aware Perturbations for Constrained Generative Modeling(https://arxiv.org/abs/2601.23151)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generative models have enjoyed widespread success in a variety of applications. However, they encounter inherent mathematical limitations in modeling distributions where samples are constrained by equalities, as is frequently the setting in scientific domains. In this work, we develop a computationally cheap, mathematically justified, and highly flexible distributional modification for combating known pitfalls in equality-constrained generative models. We propose perturbing the data distribution in a constraint-aware way such that the new distribution has support matching the ambient space dimension while still implicitly incorporating underlying manifold geometry. Through theoretical analyses and empirical evidence on several representative tasks, we illustrate that our approach consistently enables data distribution recovery and stable sampling with both diffusion models and normalizing flows.</li>
</ul>

<h3>Title: Hi-Light: A Path to high-fidelity, high-resolution video relighting with a Novel Evaluation Paradigm</h3>
<ul>
<li><strong>Authors: </strong>Xiangrui Liu, Haoxiang Li, Yezhou Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.23167">https://arxiv.org/abs/2601.23167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.23167">https://arxiv.org/pdf/2601.23167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.23167]] Hi-Light: A Path to high-fidelity, high-resolution video relighting with a Novel Evaluation Paradigm(https://arxiv.org/abs/2601.23167)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Video relighting offers immense creative potential and commercial value but is hindered by challenges, including the absence of an adequate evaluation metric, severe light flickering, and the degradation of fine-grained details during editing. To overcome these challenges, we introduce Hi-Light, a novel, training-free framework for high-fidelity, high-resolution, robust video relighting. Our approach introduces three technical innovations: lightness prior anchored guided relighting diffusion that stabilises intermediate relit video, a Hybrid Motion-Adaptive Lighting Smoothing Filter that leverages optical flow to ensure temporal stability without introducing motion blur, and a LAB-based Detail Fusion module that preserves high-frequency detail information from the original video. Furthermore, to address the critical gap in evaluation, we propose the Light Stability Score, the first quantitative metric designed to specifically measure lighting consistency. Extensive experiments demonstrate that Hi-Light significantly outperforms state-of-the-art methods in both qualitative and quantitative comparisons, producing stable, highly detailed relit videos.</li>
</ul>

<h3>Title: FourierSampler: Unlocking Non-Autoregressive Potential in Diffusion Language Models via Frequency-Guided Generation</h3>
<ul>
<li><strong>Authors: </strong>Siyang He, Qiqi Wang, Xiaoran Liu, Hongnan Ma, Yiwei Shi, Yuerong Song, Ying Zhu, Tianyi Liang, Zengfeng Huang, Ziwei He, Xipeng Qiu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.23182">https://arxiv.org/abs/2601.23182</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.23182">https://arxiv.org/pdf/2601.23182</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.23182]] FourierSampler: Unlocking Non-Autoregressive Potential in Diffusion Language Models via Frequency-Guided Generation(https://arxiv.org/abs/2601.23182)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Despite the non-autoregressive potential of diffusion language models (dLLMs), existing decoding strategies demonstrate positional bias, failing to fully unlock the potential of arbitrary generation. In this work, we delve into the inherent spectral characteristics of dLLMs and present the first frequency-domain analysis showing that low-frequency components in hidden states primarily encode global structural information and long-range dependencies, while high-frequency components are responsible for characterizing local details. Based on this observation, we propose FourierSampler, which leverages a frequency-domain sliding window mechanism to dynamically guide the model to achieve a "structure-to-detail" generation. FourierSampler outperforms other inference enhancement strategies on LLADA and SDAR, achieving relative improvements of 20.4% on LLaDA1.5-8B and 16.0% on LLaDA-8B-Instruct. It notably surpasses similarly sized autoregressive models like Llama3.1-8B-Instruct.</li>
</ul>

<h3>Title: Deep Search with Hierarchical Meta-Cognitive Monitoring Inspired by Cognitive Neuroscience</h3>
<ul>
<li><strong>Authors: </strong>Zhongxiang Sun, Qipeng Wang, Weijie Yu, Jingxuan Yang, Haolang Lu, Jun Xu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.23188">https://arxiv.org/abs/2601.23188</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.23188">https://arxiv.org/pdf/2601.23188</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.23188]] Deep Search with Hierarchical Meta-Cognitive Monitoring Inspired by Cognitive Neuroscience(https://arxiv.org/abs/2601.23188)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Deep search agents powered by large language models have demonstrated strong capabilities in multi-step retrieval, reasoning, and long-horizon task execution. However, their practical failures often stem from the lack of mechanisms to monitor and regulate reasoning and retrieval states as tasks evolve under uncertainty. Insights from cognitive neuroscience suggest that human metacognition is hierarchically organized, integrating fast anomaly detection with selectively triggered, experience-driven reflection. In this work, we propose Deep Search with Meta-Cognitive Monitoring (DS-MCM), a deep search framework augmented with an explicit hierarchical metacognitive monitoring mechanism. DS-MCM integrates a Fast Consistency Monitor, which performs lightweight checks on the alignment between external evidence and internal reasoning confidence, and a Slow Experience-Driven Monitor, which is selectively activated to guide corrective intervention based on experience memory from historical agent trajectories. By embedding monitoring directly into the reasoning-retrieval loop, DS-MCM determines both when intervention is warranted and how corrective actions should be informed by prior experience. Experiments across multiple deep search benchmarks and backbone models demonstrate that DS-MCM consistently improves performance and robustness.</li>
</ul>

<h3>Title: Med-Scout: Curing MLLMs' Geometric Blindness in Medical Perception via Geometry-Aware RL Post-Training</h3>
<ul>
<li><strong>Authors: </strong>Anglin Liu, Ruichao Chen, Yi Lu, Hongxia Xu, Jintai Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.23220">https://arxiv.org/abs/2601.23220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.23220">https://arxiv.org/pdf/2601.23220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.23220]] Med-Scout: Curing MLLMs' Geometric Blindness in Medical Perception via Geometry-Aware RL Post-Training(https://arxiv.org/abs/2601.23220)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Despite recent Multimodal Large Language Models (MLLMs)' linguistic prowess in medical diagnosis, we find even state-of-the-art MLLMs suffer from a critical perceptual deficit: geometric blindness. This failure to ground outputs in objective geometric constraints leads to plausible yet factually incorrect hallucinations, rooted in training paradigms that prioritize linguistic fluency over geometric fidelity. This paper introduces Med-Scout, a novel framework that "cures" this blindness via Reinforcement Learning (RL) that leverages the intrinsic geometric logic latent within unlabeled medical images. Instead of relying on costly expert annotations, Med-Scout derives verifiable supervision signals through three strategic proxy tasks: Hierarchical Scale Localization, Topological Jigsaw Reconstruction, and Anomaly Consistency Detection. To rigorously quantify this deficit, we present Med-Scout-Bench, a new benchmark specifically designed to evaluate geometric perception. Extensive evaluations show that Med-Scout significantly mitigates geometric blindness, outperforming leading proprietary and open-source MLLMs by over 40% on our benchmark. Furthermore, this enhanced geometric perception generalizes to broader medical understanding, achieving superior results on radiological and comprehensive medical VQA tasks.</li>
</ul>

<h3>Title: Sequence Diffusion Model for Temporal Link Prediction in Continuous-Time Dynamic Graph</h3>
<ul>
<li><strong>Authors: </strong>Nguyen Minh Duc, Viet Cuong Ta</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.23233">https://arxiv.org/abs/2601.23233</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.23233">https://arxiv.org/pdf/2601.23233</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.23233]] Sequence Diffusion Model for Temporal Link Prediction in Continuous-Time Dynamic Graph(https://arxiv.org/abs/2601.23233)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Temporal link prediction in dynamic graphs is a fundamental problem in many real-world systems. Existing temporal graph neural networks mainly focus on learning representations of historical interactions. Despite their strong performance, these models are still purely discriminative, producing point estimates for future links and lacking an explicit mechanism to capture the uncertainty and sequential structure of future temporal interactions. In this paper, we propose SDG, a novel sequence-level diffusion framework that unifies dynamic graph learning with generative denoising. Specifically, SDG injects noise into the entire historical interaction sequence and jointly reconstructs all interaction embeddings through a conditional denoising process, thereby enabling the model to capture more comprehensive interaction distributions. To align the generative process with temporal link prediction, we employ a cross-attention denoising decoder to guide the reconstruction of the destination sequence and optimize the model in an end-to-end manner. Extensive experiments on various temporal graph benchmarks show that SDG consistently achieves state-of-the-art performance in the temporal link prediction task.</li>
</ul>

<h3>Title: How well do generative models solve inverse problems? A benchmark study</h3>
<ul>
<li><strong>Authors: </strong>Patrick Kr√ºger, Patrick Materne, Werner Krebs, Hanno Gottschalk</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.23238">https://arxiv.org/abs/2601.23238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.23238">https://arxiv.org/pdf/2601.23238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.23238]] How well do generative models solve inverse problems? A benchmark study(https://arxiv.org/abs/2601.23238)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative learning generates high dimensional data based on low dimensional conditions, also called prompts. Therefore, generative learning algorithms are eligible for solving (Bayesian) inverse problems. In this article we compare a traditional Bayesian inverse approach based on a forward regression model and a prior sampled with the Markov Chain Monte Carlo method with three state of the art generative learning models, namely conditional Generative Adversarial Networks, Invertible Neural Networks and Conditional Flow Matching. We apply them to a problem of gas turbine combustor design where we map six independent design parameters to three performance labels. We propose several metrics for the evaluation of this inverse design approaches and measure the accuracy of the labels of the generated designs along with the diversity. We also study the performance as a function of the training dataset size. Our benchmark has a clear winner, as Conditional Flow Matching consistently outperforms all competing approaches.</li>
</ul>

<h3>Title: Particle-Guided Diffusion Models for Partial Differential Equations</h3>
<ul>
<li><strong>Authors: </strong>Andrew Millard, Fredrik Lindsten, Zheng Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.23262">https://arxiv.org/abs/2601.23262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.23262">https://arxiv.org/pdf/2601.23262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.23262]] Particle-Guided Diffusion Models for Partial Differential Equations(https://arxiv.org/abs/2601.23262)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We introduce a guided stochastic sampling method that augments sampling from diffusion models with physics-based guidance derived from partial differential equation (PDE) residuals and observational constraints, ensuring generated samples remain physically admissible. We embed this sampling procedure within a new Sequential Monte Carlo (SMC) framework, yielding a scalable generative PDE solver. Across multiple benchmark PDE systems as well as multiphysics and interacting PDE systems, our method produces solution fields with lower numerical error than existing state-of-the-art generative methods.</li>
</ul>

<h3>Title: FOCUS: DLLMs Know How to Tame Their Compute Bound</h3>
<ul>
<li><strong>Authors: </strong>Kaihua Liang, Xin Tan, An Zhong, Hong Xu, Marco Canini</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AR, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.23278">https://arxiv.org/abs/2601.23278</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.23278">https://arxiv.org/pdf/2601.23278</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.23278]] FOCUS: DLLMs Know How to Tame Their Compute Bound(https://arxiv.org/abs/2601.23278)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion Large Language Models (DLLMs) offer a compelling alternative to Auto-Regressive models, but their deployment is constrained by high decoding cost. In this work, we identify a key inefficiency in DLLM decoding: while computation is parallelized over token blocks, only a small subset of tokens is decodable at each diffusion step, causing most compute to be wasted on non-decodable tokens. We further observe a strong correlation between attention-derived token importance and token-wise decoding probability. Based on this insight, we propose FOCUS -- an inference system designed for DLLMs. By dynamically focusing computation on decodable tokens and evicting non-decodable ones on-the-fly, FOCUS increases the effective batch size, alleviating compute limitations and enabling scalable throughput. Empirical evaluations demonstrate that FOCUS achieves up to 3.52$\times$ throughput improvement over the production-grade engine LMDeploy, while preserving or improving generation quality across multiple benchmarks. The FOCUS system is publicly available on GitHub: this https URL.</li>
</ul>

<h3>Title: Decoupled Diffusion Sampling for Inverse Problems on Function Spaces</h3>
<ul>
<li><strong>Authors: </strong>Thomas Y.L. Lin, Jiachen Yao, Lufang Chiang, Julius Berner, Anima Anandkumar</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.23280">https://arxiv.org/abs/2601.23280</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.23280">https://arxiv.org/pdf/2601.23280</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.23280]] Decoupled Diffusion Sampling for Inverse Problems on Function Spaces(https://arxiv.org/abs/2601.23280)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We propose a data-efficient, physics-aware generative framework in function space for inverse PDE problems. Existing plug-and-play diffusion posterior samplers represent physics implicitly through joint coefficient-solution modeling, requiring substantial paired supervision. In contrast, our Decoupled Diffusion Inverse Solver (DDIS) employs a decoupled design: an unconditional diffusion learns the coefficient prior, while a neural operator explicitly models the forward PDE for guidance. This decoupling enables superior data efficiency and effective physics-informed learning, while naturally supporting Decoupled Annealing Posterior Sampling (DAPS) to avoid over-smoothing in Diffusion Posterior Sampling (DPS). Theoretically, we prove that DDIS avoids the guidance attenuation failure of joint models when training data is scarce. Empirically, DDIS achieves state-of-the-art performance under sparse observation, improving $l_2$ error by 11% and spectral error by 54% on average; when data is limited to 1%, DDIS maintains accuracy with 40% advantage in $l_2$ error compared to joint models.</li>
</ul>

<h3>Title: VideoGPA: Distilling Geometry Priors for 3D-Consistent Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Hongyang Du, Junjie Ye, Xiaoyan Cong, Runhao Li, Jingcheng Ni, Aman Agarwal, Zeqi Zhou, Zekun Li, Randall Balestriero, Yue Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2601.23286">https://arxiv.org/abs/2601.23286</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2601.23286">https://arxiv.org/pdf/2601.23286</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2601.23286]] VideoGPA: Distilling Geometry Priors for 3D-Consistent Video Generation(https://arxiv.org/abs/2601.23286)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, self-supervised, foundation model, generative</a></li>
<li><strong>Abstract: </strong>While recent video diffusion models (VDMs) produce visually impressive results, they fundamentally struggle to maintain 3D structural consistency, often resulting in object deformation or spatial drift. We hypothesize that these failures arise because standard denoising objectives lack explicit incentives for geometric coherence. To address this, we introduce VideoGPA (Video Geometric Preference Alignment), a data-efficient self-supervised framework that leverages a geometry foundation model to automatically derive dense preference signals that guide VDMs via Direct Preference Optimization (DPO). This approach effectively steers the generative distribution toward inherent 3D consistency without requiring human annotations. VideoGPA significantly enhances temporal stability, physical plausibility, and motion coherence using minimal preference pairs, consistently outperforming state-of-the-art baselines in extensive experiments.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
