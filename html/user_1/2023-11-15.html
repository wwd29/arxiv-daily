<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>diffusion</h2>
<h3>Title: Finetuning Text-to-Image Diffusion Models for Fairness. (arXiv:2311.07604v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.07604">http://arxiv.org/abs/2311.07604</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.07604]] Finetuning Text-to-Image Diffusion Models for Fairness(http://arxiv.org/abs/2311.07604)</code></li>
<li>Summary: <p>The rapid adoption of text-to-image diffusion models in society underscores
an urgent need to address their biases. Without interventions, these biases
could propagate a distorted worldview and limit opportunities for minority
groups. In this work, we frame fairness as a distributional alignment problem.
Our solution consists of two main technical contributions: (1) a distributional
alignment loss that steers specific characteristics of the generated images
towards a user-defined target distribution, and (2) biased direct finetuning of
diffusion model's sampling process, which leverages a biased gradient to more
effectively optimize losses defined on the generated images. Empirically, our
method markedly reduces gender, racial, and their intersectional biases for
occupational prompts. Gender bias is significantly reduced even when finetuning
just five soft tokens. Crucially, our method supports diverse perspectives of
fairness beyond absolute equality, which is demonstrated by controlling age to
a $75\%$ young and $25\%$ old distribution while simultaneously debiasing
gender and race. Finally, our method is scalable: it can debias multiple
concepts at once by simply including these prompts in the finetuning data. We
hope our work facilitates the social alignment of T2I generative AI. We will
share code and various debiased diffusion model adaptors.
</p></li>
</ul>

<h3>Title: One-2-3-45++: Fast Single Image to 3D Objects with Consistent Multi-View Generation and 3D Diffusion. (arXiv:2311.07885v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.07885">http://arxiv.org/abs/2311.07885</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.07885]] One-2-3-45++: Fast Single Image to 3D Objects with Consistent Multi-View Generation and 3D Diffusion(http://arxiv.org/abs/2311.07885)</code></li>
<li>Summary: <p>Recent advancements in open-world 3D object generation have been remarkable,
with image-to-3D methods offering superior fine-grained control over their
text-to-3D counterparts. However, most existing models fall short in
simultaneously providing rapid generation speeds and high fidelity to input
images - two features essential for practical applications. In this paper, we
present One-2-3-45++, an innovative method that transforms a single image into
a detailed 3D textured mesh in approximately one minute. Our approach aims to
fully harness the extensive knowledge embedded in 2D diffusion models and
priors from valuable yet limited 3D data. This is achieved by initially
finetuning a 2D diffusion model for consistent multi-view image generation,
followed by elevating these images to 3D with the aid of multi-view conditioned
3D native diffusion models. Extensive experimental evaluations demonstrate that
our method can produce high-quality, diverse 3D assets that closely mirror the
original input image. Our project webpage:
https://sudo-ai-3d.github.io/One2345plus_page.
</p></li>
</ul>

<h3>Title: CLAMP: A Contrastive Language And Molecule Pre-training Network. (arXiv:2311.07617v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.07617">http://arxiv.org/abs/2311.07617</a></li>
<li>Code URL: https://github.com/neelr/clamp</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.07617]] CLAMP: A Contrastive Language And Molecule Pre-training Network(http://arxiv.org/abs/2311.07617)</code></li>
<li>Summary: <p>This paper highlights a shift in how to approach material generation. Instead
of material-to-material, we propose a language-to-material generation
architecture that utilizes millions of untapped data points. Using a web
scraper to collect crystal text pairs from open-source research papers, a
contrastive model can be trained using a convolutional graph neural network
encoder and a language encoder. This would allow unsupervised zero-shot
classification which can be trained by taking advantage of linguistic
structure. Without any specific training data, an ~82\% accuracy was achieved
and ~75\% accuracy for photocatalyst prediction with an extremely small
dataset. This novel network could ideally be cross-applied to any reaction that
can be described via text, opening completely new methods to think about 3D
chemical framework generation. In the full experiment diffusion models would
likely be incorporated to fully exploit the latent space.
</p></li>
</ul>

<h3>Title: Brain-Driven Representation Learning Based on Diffusion Model. (arXiv:2311.07925v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.07925">http://arxiv.org/abs/2311.07925</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.07925]] Brain-Driven Representation Learning Based on Diffusion Model(http://arxiv.org/abs/2311.07925)</code></li>
<li>Summary: <p>Interpreting EEG signals linked to spoken language presents a complex
challenge, given the data's intricate temporal and spatial attributes, as well
as the various noise factors. Denoising diffusion probabilistic models (DDPMs),
which have recently gained prominence in diverse areas for their capabilities
in representation learning, are explored in our research as a means to address
this issue. Using DDPMs in conjunction with a conditional autoencoder, our new
approach considerably outperforms traditional machine learning algorithms and
established baseline models in accuracy. Our results highlight the potential of
DDPMs as a sophisticated computational method for the analysis of
speech-related EEG signals. This could lead to significant advances in
brain-computer interfaces tailored for spoken communication.
</p></li>
</ul>

<h3>Title: A Consistent Diffusion-Based Algorithm for Semi-Supervised Graph Learning. (arXiv:2311.07627v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.07627">http://arxiv.org/abs/2311.07627</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.07627]] A Consistent Diffusion-Based Algorithm for Semi-Supervised Graph Learning(http://arxiv.org/abs/2311.07627)</code></li>
<li>Summary: <p>The task of semi-supervised classification aims at assigning labels to all
nodes of a graph based on the labels known for a few nodes, called the seeds.
One of the most popular algorithms relies on the principle of heat diffusion,
where the labels of the seeds are spread by thermoconductance and the
temperature of each node at equilibrium is used as a score function for each
label. In this paper, we prove that this algorithm is not consistent unless the
temperatures of the nodes at equilibrium are centered before scoring. This
crucial step does not only make the algorithm provably consistent on a block
model but brings significant performance gains on real graphs.
</p></li>
</ul>

<h2>self-supervised</h2>
<h3>Title: PECoP: Parameter Efficient Continual Pretraining for Action Quality Assessment. (arXiv:2311.07603v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.07603">http://arxiv.org/abs/2311.07603</a></li>
<li>Code URL: https://github.com/plrbear/pecop</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.07603]] PECoP: Parameter Efficient Continual Pretraining for Action Quality Assessment(http://arxiv.org/abs/2311.07603)</code></li>
<li>Summary: <p>The limited availability of labelled data in Action Quality Assessment (AQA),
has forced previous works to fine-tune their models pretrained on large-scale
domain-general datasets. This common approach results in weak generalisation,
particularly when there is a significant domain shift. We propose a novel,
parameter efficient, continual pretraining framework, PECoP, to reduce such
domain shift via an additional pretraining stage. In PECoP, we introduce
3D-Adapters, inserted into the pretrained model, to learn spatiotemporal,
in-domain information via self-supervised learning where only the adapter
modules' parameters are updated. We demonstrate PECoP's ability to enhance the
performance of recent state-of-the-art methods (MUSDL, CoRe, and TSA) applied
to AQA, leading to considerable improvements on benchmark datasets, JIGSAWS
($\uparrow6.0\%$), MTL-AQA ($\uparrow0.99\%$), and FineDiving
($\uparrow2.54\%$). We also present a new Parkinson's Disease dataset, PD4T, of
real patients performing four various actions, where we surpass
($\uparrow3.56\%$) the state-of-the-art in comparison. Our code, pretrained
models, and the PD4T dataset are available at https://github.com/Plrbear/PECoP.
</p></li>
</ul>

<h3>Title: CSLP-AE: A Contrastive Split-Latent Permutation Autoencoder Framework for Zero-Shot Electroencephalography Signal Conversion. (arXiv:2311.07788v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.07788">http://arxiv.org/abs/2311.07788</a></li>
<li>Code URL: https://github.com/andersxa/cslp-ae</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.07788]] CSLP-AE: A Contrastive Split-Latent Permutation Autoencoder Framework for Zero-Shot Electroencephalography Signal Conversion(http://arxiv.org/abs/2311.07788)</code></li>
<li>Summary: <p>Electroencephalography (EEG) is a prominent non-invasive neuroimaging
technique providing insights into brain function. Unfortunately, EEG data
exhibit a high degree of noise and variability across subjects hampering
generalizable signal extraction. Therefore, a key aim in EEG analysis is to
extract the underlying neural activation (content) as well as to account for
the individual subject variability (style). We hypothesize that the ability to
convert EEG signals between tasks and subjects requires the extraction of
latent representations accounting for content and style. Inspired by recent
advancements in voice conversion technologies, we propose a novel contrastive
split-latent permutation autoencoder (CSLP-AE) framework that directly
optimizes for EEG conversion. Importantly, the latent representations are
guided using contrastive learning to promote the latent splits to explicitly
represent subject (style) and task (content). We contrast CSLP-AE to
conventional supervised, unsupervised (AE), and self-supervised (contrastive
learning) training and find that the proposed approach provides favorable
generalizable characterizations of subject and task. Importantly, the procedure
also enables zero-shot conversion between unseen subjects. While the present
work only considers conversion of EEG, the proposed CSLP-AE provides a general
framework for signal conversion and extraction of content (task activation) and
style (subject variability) components of general interest for the modeling and
analysis of biological signals.
</p></li>
</ul>

<h3>Title: Dual-channel Prototype Network for few-shot Classification of Pathological Images. (arXiv:2311.07871v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.07871">http://arxiv.org/abs/2311.07871</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.07871]] Dual-channel Prototype Network for few-shot Classification of Pathological Images(http://arxiv.org/abs/2311.07871)</code></li>
<li>Summary: <p>In pathology, the rarity of certain diseases and the complexity in annotating
pathological images significantly hinder the creation of extensive,
high-quality datasets. This limitation impedes the progress of deep
learning-assisted diagnostic systems in pathology. Consequently, it becomes
imperative to devise a technology that can discern new disease categories from
a minimal number of annotated examples. Such a technology would substantially
advance deep learning models for rare diseases. Addressing this need, we
introduce the Dual-channel Prototype Network (DCPN), rooted in the few-shot
learning paradigm, to tackle the challenge of classifying pathological images
with limited samples. DCPN augments the Pyramid Vision Transformer (PVT)
framework for few-shot classification via self-supervised learning and
integrates it with convolutional neural networks. This combination forms a
dual-channel architecture that extracts multi-scale, highly precise
pathological features. The approach enhances the versatility of prototype
representations and elevates the efficacy of prototype networks in few-shot
pathological image classification tasks. We evaluated DCPN using three publicly
available pathological datasets, configuring small-sample classification tasks
that mirror varying degrees of clinical scenario domain shifts. Our
experimental findings robustly affirm DCPN's superiority in few-shot
pathological image classification, particularly in tasks within the same
domain, where it achieves the benchmarks of supervised learning.
</p></li>
</ul>

<h3>Title: Bring Your Own KG: Self-Supervised Program Synthesis for Zero-Shot KGQA. (arXiv:2311.07850v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.07850">http://arxiv.org/abs/2311.07850</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.07850]] Bring Your Own KG: Self-Supervised Program Synthesis for Zero-Shot KGQA(http://arxiv.org/abs/2311.07850)</code></li>
<li>Summary: <p>We present BYOKG, a universal question-answering (QA) system that can operate
on any knowledge graph (KG), requires no human-annotated training data, and can
be ready to use within a day -- attributes that are out-of-scope for current
KGQA systems. BYOKG draws inspiration from the remarkable ability of humans to
comprehend information present in an unseen KG through exploration -- starting
at random nodes, inspecting the labels of adjacent nodes and edges, and
combining them with their prior world knowledge. In BYOKG, exploration
leverages an LLM-backed symbolic agent that generates a diverse set of
query-program exemplars, which are then used to ground a retrieval-augmented
reasoning procedure to predict programs for arbitrary questions. BYOKG is
effective over both small- and large-scale graphs, showing dramatic gains in QA
accuracy over a zero-shot baseline of 27.89 and 58.02 F1 on GrailQA and MetaQA,
respectively. On GrailQA, we further show that our unsupervised BYOKG
outperforms a supervised in-context learning method, demonstrating the
effectiveness of exploration. Lastly, we find that performance of BYOKG
reliably improves with continued exploration as well as improvements in the
base LLM, notably outperforming a state-of-the-art fine-tuned model by 7.08 F1
on a sub-sampled zero-shot split of GrailQA.
</p></li>
</ul>

<h3>Title: PEMS: Pre-trained Epidmic Time-series Models. (arXiv:2311.07841v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.07841">http://arxiv.org/abs/2311.07841</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.07841]] PEMS: Pre-trained Epidmic Time-series Models(http://arxiv.org/abs/2311.07841)</code></li>
<li>Summary: <p>Providing accurate and reliable predictions about the future of an epidemic
is an important problem for enabling informed public health decisions. Recent
works have shown that leveraging data-driven solutions that utilize advances in
deep learning methods to learn from past data of an epidemic often outperform
traditional mechanistic models. However, in many cases, the past data is sparse
and may not sufficiently capture the underlying dynamics. While there exists a
large amount of data from past epidemics, leveraging prior knowledge from
time-series data of other diseases is a non-trivial challenge. Motivated by the
success of pre-trained models in language and vision tasks, we tackle the
problem of pre-training epidemic time-series models to learn from multiple
datasets from different diseases and epidemics. We introduce Pre-trained
Epidemic Time-Series Models (PEMS) that learn from diverse time-series datasets
of a variety of diseases by formulating pre-training as a set of
self-supervised learning (SSL) tasks. We tackle various important challenges
specific to pre-training for epidemic time-series such as dealing with
heterogeneous dynamics and efficiently capturing useful patterns from multiple
epidemic datasets by carefully designing the SSL tasks to learn important
priors about the epidemic dynamics that can be leveraged for fine-tuning to
multiple downstream tasks. The resultant PEM outperforms previous
state-of-the-art methods in various downstream time-series tasks across
datasets of varying seasonal patterns, geography, and mechanism of contagion
including the novel Covid-19 pandemic unseen in pre-trained data with better
efficiency using smaller fraction of datasets.
</p></li>
</ul>

<h3>Title: Self-supervised Heterogeneous Graph Variational Autoencoders. (arXiv:2311.07929v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.07929">http://arxiv.org/abs/2311.07929</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.07929]] Self-supervised Heterogeneous Graph Variational Autoencoders(http://arxiv.org/abs/2311.07929)</code></li>
<li>Summary: <p>Heterogeneous Information Networks (HINs), which consist of various types of
nodes and edges, have recently demonstrated excellent performance in graph
mining. However, most existing heterogeneous graph neural networks (HGNNs)
ignore the problems of missing attributes, inaccurate attributes and scarce
labels for nodes, which limits their expressiveness. In this paper, we propose
a generative self-supervised model SHAVA to address these issues
simultaneously. Specifically, SHAVA first initializes all the nodes in the
graph with a low-dimensional representation matrix. After that, based on the
variational graph autoencoder framework, SHAVA learns both node-level and
attribute-level embeddings in the encoder, which can provide fine-grained
semantic information to construct node attributes. In the decoder, SHAVA
reconstructs both links and attributes. Instead of directly reconstructing raw
features for attributed nodes, SHAVA generates the initial low-dimensional
representation matrix for all the nodes, based on which raw features of
attributed nodes are further reconstructed to leverage accurate attributes. In
this way, SHAVA can not only complete informative features for non-attributed
nodes, but rectify inaccurate ones for attributed nodes. Finally, we conduct
extensive experiments to show the superiority of SHAVA in tackling HINs with
missing and inaccurate attributes.
</p></li>
</ul>

<h3>Title: Neural Lattice Reduction: A Self-Supervised Geometric Deep Learning Approach. (arXiv:2311.08170v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08170">http://arxiv.org/abs/2311.08170</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08170]] Neural Lattice Reduction: A Self-Supervised Geometric Deep Learning Approach(http://arxiv.org/abs/2311.08170)</code></li>
<li>Summary: <p>Lattice reduction is a combinatorial optimization problem aimed at finding
the most orthogonal basis in a given lattice. In this work, we address lattice
reduction via deep learning methods. We design a deep neural model outputting
factorized unimodular matrices and train it in a self-supervised manner by
penalizing non-orthogonal lattice bases. We incorporate the symmetries of
lattice reduction into the model by making it invariant and equivariant with
respect to appropriate continuous and discrete groups.
</p></li>
</ul>

<h3>Title: Mobility-Induced Graph Learning for WiFi Positioning. (arXiv:2311.08271v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08271">http://arxiv.org/abs/2311.08271</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08271]] Mobility-Induced Graph Learning for WiFi Positioning(http://arxiv.org/abs/2311.08271)</code></li>
<li>Summary: <p>A smartphone-based user mobility tracking could be effective in finding
his/her location, while the unpredictable error therein due to low
specification of built-in inertial measurement units (IMUs) rejects its
standalone usage but demands the integration to another positioning technique
like WiFi positioning. This paper aims to propose a novel integration technique
using a graph neural network called Mobility-INduced Graph LEarning (MINGLE),
which is designed based on two types of graphs made by capturing different user
mobility features. Specifically, considering sequential measurement points
(MPs) as nodes, a user's regular mobility pattern allows us to connect neighbor
MPs as edges, called time-driven mobility graph (TMG). Second, a user's
relatively straight transition at a constant pace when moving from one position
to another can be captured by connecting the nodes on each path, called a
direction-driven mobility graph (DMG). Then, we can design graph convolution
network (GCN)-based cross-graph learning, where two different GCN models for
TMG and DMG are jointly trained by feeding different input features created by
WiFi RTTs yet sharing their weights. Besides, the loss function includes a
mobility regularization term such that the differences between adjacent
location estimates should be less variant due to the user's stable moving pace.
Noting that the regularization term does not require ground-truth location,
MINGLE can be designed under semi- and self-supervised learning frameworks. The
proposed MINGLE's effectiveness is extensively verified through field
experiments, showing a better positioning accuracy than benchmarks, say root
mean square errors (RMSEs) being 1.398 (m) and 1.073 (m) for self- and
semi-supervised learning cases, respectively.
</p></li>
</ul>

<h2>foundation model</h2>
<h3>Title: Zero-Shot Segmentation of Eye Features Using the Segment Anything Model (SAM). (arXiv:2311.08077v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08077">http://arxiv.org/abs/2311.08077</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08077]] Zero-Shot Segmentation of Eye Features Using the Segment Anything Model (SAM)(http://arxiv.org/abs/2311.08077)</code></li>
<li>Summary: <p>The advent of foundation models signals a new era in artificial intelligence.
The Segment Anything Model (SAM) is the first foundation model for image
segmentation. In this study, we evaluate SAM's ability to segment features from
eye images recorded in virtual reality setups. The increasing requirement for
annotated eye-image datasets presents a significant opportunity for SAM to
redefine the landscape of data annotation in gaze estimation. Our investigation
centers on SAM's zero-shot learning abilities and the effectiveness of prompts
like bounding boxes or point clicks. Our results are consistent with studies in
other domains, demonstrating that SAM's segmentation effectiveness can be
on-par with specialized models depending on the feature, with prompts improving
its performance, evidenced by an IoU of 93.34% for pupil segmentation in one
dataset. Foundation models like SAM could revolutionize gaze estimation by
enabling quick and easy image segmentation, reducing reliance on specialized
models and extensive manual annotation.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: A Data-Free Approach to Mitigate Catastrophic Forgetting in Federated Class Incremental Learning for Vision Tasks. (arXiv:2311.07784v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.07784">http://arxiv.org/abs/2311.07784</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.07784]] A Data-Free Approach to Mitigate Catastrophic Forgetting in Federated Class Incremental Learning for Vision Tasks(http://arxiv.org/abs/2311.07784)</code></li>
<li>Summary: <p>Deep learning models often suffer from forgetting previously learned
information when trained on new data. This problem is exacerbated in federated
learning (FL), where the data is distributed and can change independently for
each user. Many solutions are proposed to resolve this catastrophic forgetting
in a centralized setting. However, they do not apply directly to FL because of
its unique complexities, such as privacy concerns and resource limitations. To
overcome these challenges, this paper presents a framework for
\textbf{federated class incremental learning} that utilizes a generative model
to synthesize samples from past distributions. This data can be later exploited
alongside the training data to mitigate catastrophic forgetting. To preserve
privacy, the generative model is trained on the server using data-free methods
at the end of each task without requesting data from clients. Moreover, our
solution does not demand the users to store old data or models, which gives
them the freedom to join/leave the training at any time. Additionally, we
introduce SuperImageNet, a new regrouping of the ImageNet dataset specifically
tailored for federated continual learning. We demonstrate significant
improvements compared to existing baselines through extensive experiments on
multiple datasets.
</p></li>
</ul>

<h3>Title: Peer is Your Pillar: A Data-unbalanced Conditional GANs for Few-shot Image Generation. (arXiv:2311.08217v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08217">http://arxiv.org/abs/2311.08217</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08217]] Peer is Your Pillar: A Data-unbalanced Conditional GANs for Few-shot Image Generation(http://arxiv.org/abs/2311.08217)</code></li>
<li>Summary: <p>Few-shot image generation aims to train generative models using a small
number of training images. When there are few images available for training
(e.g. 10 images), Learning From Scratch (LFS) methods often generate images
that closely resemble the training data while Transfer Learning (TL) methods
try to improve performance by leveraging prior knowledge from GANs pre-trained
on large-scale datasets. However, current TL methods may not allow for
sufficient control over the degree of knowledge preservation from the source
model, making them unsuitable for setups where the source and target domains
are not closely related. To address this, we propose a novel pipeline called
Peer is your Pillar (PIP), which combines a target few-shot dataset with a peer
dataset to create a data-unbalanced conditional generation. Our approach
includes a class embedding method that separates the class space from the
latent space, and we use a direction loss based on pre-trained CLIP to improve
image diversity. Experiments on various few-shot datasets demonstrate the
advancement of the proposed PIP, especially reduces the training requirements
of few-shot image generation.
</p></li>
</ul>

<h3>Title: AuthentiGPT: Detecting Machine-Generated Text via Black-Box Language Models Denoising. (arXiv:2311.07700v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.07700">http://arxiv.org/abs/2311.07700</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.07700]] AuthentiGPT: Detecting Machine-Generated Text via Black-Box Language Models Denoising(http://arxiv.org/abs/2311.07700)</code></li>
<li>Summary: <p>Large language models (LLMs) have opened up enormous opportunities while
simultaneously posing ethical dilemmas. One of the major concerns is their
ability to create text that closely mimics human writing, which can lead to
potential misuse, such as academic misconduct, disinformation, and fraud. To
address this problem, we present AuthentiGPT, an efficient classifier that
distinguishes between machine-generated and human-written texts. Under the
assumption that human-written text resides outside the distribution of
machine-generated text, AuthentiGPT leverages a black-box LLM to denoise input
text with artificially added noise, and then semantically compares the denoised
text with the original to determine if the content is machine-generated. With
only one trainable parameter, AuthentiGPT eliminates the need for a large
training dataset, watermarking the LLM's output, or computing the
log-likelihood. Importantly, the detection capability of AuthentiGPT can be
easily adapted to any generative language model. With a 0.918 AUROC score on a
domain-specific dataset, AuthentiGPT demonstrates its effectiveness over other
commercial algorithms, highlighting its potential for detecting
machine-generated text in academic settings.
</p></li>
</ul>

<h3>Title: The ART of LLM Refinement: Ask, Refine, and Trust. (arXiv:2311.07961v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.07961">http://arxiv.org/abs/2311.07961</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.07961]] The ART of LLM Refinement: Ask, Refine, and Trust(http://arxiv.org/abs/2311.07961)</code></li>
<li>Summary: <p>In recent years, Large Language Models (LLMs) have demonstrated remarkable
generative abilities, but can they judge the quality of their own generations?
A popular concept, referred to as self-refinement, postulates that LLMs can
detect and correct the errors in their generations when asked to do so.
However, recent empirical evidence points in the opposite direction, suggesting
that LLMs often struggle to accurately identify errors when reasoning is
involved. To address this, we propose a reasoning with refinement objective
called ART: Ask, Refine, and Trust, which asks necessary questions to decide
when an LLM should refine its output, and either affirm or withhold trust in
its refinement by ranking the refinement and the initial prediction. On two
multistep reasoning tasks of mathematical word problems (GSM8K) and question
answering (StrategyQA), ART achieves a performance gain of +5 points over
self-refinement baselines, while using a much smaller model as the decision
maker. We also demonstrate the benefit of using smaller models to make
refinement decisions as a cost-effective alternative to fine-tuning a larger
model.
</p></li>
</ul>

<h3>Title: How good are Large Language Models on African Languages?. (arXiv:2311.07978v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.07978">http://arxiv.org/abs/2311.07978</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.07978]] How good are Large Language Models on African Languages?(http://arxiv.org/abs/2311.07978)</code></li>
<li>Summary: <p>Recent advancements in natural language processing have led to the
proliferation of large language models (LLMs). These models have been shown to
yield good performance, using in-context learning, even on unseen tasks and
languages. Additionally, they have been widely adopted as
language-model-as-a-service commercial APIs like GPT-4 API. However, their
performance on African languages is largely unknown. We present an analysis of
three popular large language models (mT0, LLaMa 2, and GPT-4) on five tasks
(news topic classification, sentiment classification, machine translation,
question answering, and named entity recognition) across 30 African languages,
spanning different language families and geographical regions. Our results
suggest that all LLMs produce below-par performance on African languages, and
there is a large gap in performance compared to high-resource languages like
English most tasks. We find that GPT-4 has an average or impressive performance
on classification tasks but very poor results on generative tasks like machine
translation. Surprisingly, we find that mT0 had the best overall on
cross-lingual QA, better than the state-of-the-art supervised model (i.e.
fine-tuned mT5) and GPT-4 on African languages. Overall, LLaMa 2 records the
worst performance due to its limited multilingual capabilities and
English-centric pre-training corpus. In general, our findings present a
call-to-action to ensure African languages are well represented in large
language models, given their growing popularity.
</p></li>
</ul>

<h3>Title: Align after Pre-train: Improving Multilingual Generative Models with Cross-lingual Alignment. (arXiv:2311.08089v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08089">http://arxiv.org/abs/2311.08089</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08089]] Align after Pre-train: Improving Multilingual Generative Models with Cross-lingual Alignment(http://arxiv.org/abs/2311.08089)</code></li>
<li>Summary: <p>Multilingual generative models obtain remarkable cross-lingual capabilities
through pre-training on large-scale corpora. However, they still exhibit a
performance bias toward high-resource languages, and learn isolated
distributions of sentence representations across languages. To bridge this gap,
we propose a simple yet effective alignment framework exploiting pairs of
translation sentences. It aligns the internal sentence representations across
different languages via multilingual contrastive learning and aligns model
outputs by answering prompts in different languages. Experimental results
demonstrate that even with less than 0.1 {\textperthousand} of pre-training
tokens, our alignment framework significantly boosts the cross-lingual
abilities of generative models and mitigates the performance gap. Further
analysis reveals that it results in a better internal multilingual
representation distribution of multilingual models.
</p></li>
</ul>

<h3>Title: Eval-GCSC: A New Metric for Evaluating ChatGPT's Performance in Chinese Spelling Correction. (arXiv:2311.08219v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08219">http://arxiv.org/abs/2311.08219</a></li>
<li>Code URL: https://github.com/ktlktl/eval-gcsc</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08219]] Eval-GCSC: A New Metric for Evaluating ChatGPT's Performance in Chinese Spelling Correction(http://arxiv.org/abs/2311.08219)</code></li>
<li>Summary: <p>ChatGPT has demonstrated impressive performance in various downstream tasks.
However, in the Chinese Spelling Correction (CSC) task, we observe a
discrepancy: while ChatGPT performs well under human evaluation, it scores
poorly according to traditional metrics. We believe this inconsistency arises
because the traditional metrics are not well-suited for evaluating generative
models. Their overly strict length and phonics constraints may lead to
underestimating ChatGPT's correction capabilities. To better evaluate
generative models in the CSC task, this paper proposes a new evaluation metric:
Eval-GCSC. By incorporating word-level and semantic similarity judgments, it
relaxes the stringent length and phonics constraints. Experimental results show
that Eval-GCSC closely aligns with human evaluations. Under this metric,
ChatGPT's performance is comparable to traditional token-level classification
models (TCM), demonstrating its potential as a CSC tool. The source code and
scripts can be accessed at https://github.com/ktlKTL/Eval-GCSC.
</p></li>
</ul>

<h3>Title: Modeling Complex Disease Trajectories using Deep Generative Models with Semi-Supervised Latent Processes. (arXiv:2311.08149v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.08149">http://arxiv.org/abs/2311.08149</a></li>
<li>Code URL: https://github.com/uzh-dqbm-cmi/eustar_dgm4h</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.08149]] Modeling Complex Disease Trajectories using Deep Generative Models with Semi-Supervised Latent Processes(http://arxiv.org/abs/2311.08149)</code></li>
<li>Summary: <p>In this paper, we propose a deep generative time series approach using latent
temporal processes for modeling and holistically analyzing complex disease
trajectories. We aim to find meaningful temporal latent representations of an
underlying generative process that explain the observed disease trajectories in
an interpretable and comprehensive way. To enhance the interpretability of
these latent temporal processes, we develop a semi-supervised approach for
disentangling the latent space using established medical concepts. By combining
the generative approach with medical knowledge, we leverage the ability to
discover novel aspects of the disease while integrating medical concepts into
the model. We show that the learned temporal latent processes can be utilized
for further data analysis and clinical hypothesis testing, including finding
similar patients and clustering the disease into new sub-types. Moreover, our
method enables personalized online monitoring and prediction of multivariate
time series including uncertainty quantification. We demonstrate the
effectiveness of our approach in modeling systemic sclerosis, showcasing the
potential of our machine learning model to capture complex disease trajectories
and acquire new medical knowledge.
</p></li>
</ul>

<h2>anomaly</h2>
<h3>Title: VegaEdge: Edge AI Confluence Anomaly Detection for Real-Time Highway IoT-Applications. (arXiv:2311.07880v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.07880">http://arxiv.org/abs/2311.07880</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.07880]] VegaEdge: Edge AI Confluence Anomaly Detection for Real-Time Highway IoT-Applications(http://arxiv.org/abs/2311.07880)</code></li>
<li>Summary: <p>Vehicle anomaly detection plays a vital role in highway safety applications
such as accident prevention, rapid response, traffic flow optimization, and
work zone safety. With the surge of the Internet of Things (IoT) in recent
years, there has arisen a pressing demand for Artificial Intelligence (AI)
based anomaly detection methods designed to meet the requirements of IoT
devices. Catering to this futuristic vision, we introduce a lightweight
approach to vehicle anomaly detection by utilizing the power of trajectory
prediction. Our proposed design identifies vehicles deviating from expected
paths, indicating highway risks from different camera-viewing angles from
real-world highway datasets. On top of that, we present VegaEdge - a
sophisticated AI confluence designed for real-time security and surveillance
applications in modern highway settings through edge-centric IoT-embedded
platforms equipped with our anomaly detection approach. Extensive testing
across multiple platforms and traffic scenarios showcases the versatility and
effectiveness of VegaEdge. This work also presents the Carolinas Anomaly
Dataset (CAD), to bridge the existing gap in datasets tailored for highway
anomalies. In real-world scenarios, our anomaly detection approach achieves an
AUC-ROC of 0.94, and our proposed VegaEdge design, on an embedded IoT platform,
processes 738 trajectories per second in a typical highway setting. The dataset
is available at
https://github.com/TeCSAR-UNCC/Carolinas_Dataset#chd-anomaly-test-set .
</p></li>
</ul>

<h2>in-context</h2>
<h3>Title: In-context Learning and Gradient Descent Revisited. (arXiv:2311.07772v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.07772">http://arxiv.org/abs/2311.07772</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.07772]] In-context Learning and Gradient Descent Revisited(http://arxiv.org/abs/2311.07772)</code></li>
<li>Summary: <p>In-context learning (ICL) has shown impressive results in few-shot learning
tasks, yet its underlying mechanism is still not fully understood. Recent works
suggest that ICL can be thought of as a gradient descent (GD) based
optimization process. While promising, these results mainly focus on simplified
settings of ICL and provide only a preliminary evaluation of the similarities
between the two methods. In this work, we revisit the comparison between ICL
and GD-based finetuning and study what properties of ICL an equivalent process
must follow. We highlight a major difference in the flow of information between
ICL and standard finetuning. Namely, ICL can only rely on information from
lower layers at every point, while finetuning depends on loss gradients from
deeper layers. We refer to this discrepancy as Layer Causality and show that a
layer causal variant of the finetuning process aligns with ICL on par with
vanilla finetuning and is even better in most cases across relevant metrics. To
the best of our knowledge, this is the first work to discuss this discrepancy
explicitly and suggest a solution that tackles this problem with minimal
changes.
</p></li>
</ul>

<h3>Title: In-context Learning Generalizes, But Not Always Robustly: The Case of Syntax. (arXiv:2311.07811v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.07811">http://arxiv.org/abs/2311.07811</a></li>
<li>Code URL: https://github.com/aaronmueller/syntax-icl</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.07811]] In-context Learning Generalizes, But Not Always Robustly: The Case of Syntax(http://arxiv.org/abs/2311.07811)</code></li>
<li>Summary: <p>In-context learning (ICL) is now a common method for supervising large
language models (LLMs): given labeled examples in the input context, the LLM
learns to perform the task without weight updates. Despite ICL's prevalence and
utility, we understand little about whether models supervised in this manner
represent the underlying structure of their tasks, rather than superficial
heuristics that only generalize to identically distributed examples. In this
study, we investigate the robustness of LLMs supervised via ICL using the test
case of sensitivity to syntax, which is a prerequisite for robust language
understanding. Our experiments are based on two simple and well-controlled
syntactic transformations tasks, where correct out-of-distribution
generalization requires an accurate syntactic analysis of the input. We further
investigate whether out-of-distribution generalization can be improved via
chain-of-thought prompting, where the model is provided with a sequence of
intermediate computation steps that illustrate how the task ought to be
performed. In experiments with models from the GPT, PaLM, and Llama 2 families,
we find large variance across LMs on this fundamental linguistic phenomenon,
and that the variance is explained more by the composition of the pre-training
corpus and supervision methods than by model size. In particular, we find
evidence that models pre-trained on code generalize better, and benefit to a
greater extent from chain-of-thought prompting.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
