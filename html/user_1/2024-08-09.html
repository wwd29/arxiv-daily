<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-08-09</h1>
<h3>Title: Histopathology image embedding based on foundation models features aggregation for patient treatment response prediction</h3>
<ul>
<li><strong>Authors: </strong>Bilel Guetarni, Feryal Windal, Halim Benhabiles, Mahfoud Chaibi, Romain Dubois, Emmanuelle Leteurtre, Dominique Collard</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.03954">https://arxiv.org/abs/2408.03954</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.03954">https://arxiv.org/pdf/2408.03954</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.03954]] Histopathology image embedding based on foundation models features aggregation for patient treatment response prediction(https://arxiv.org/abs/2408.03954)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>Predicting the response of a patient to a cancer treatment is of high interest. Nonetheless, this task is still challenging from a medical point of view due to the complexity of the interaction between the patient organism and the considered treatment. Recent works on foundation models pre-trained with self-supervised learning on large-scale unlabeled histopathology datasets have opened a new direction towards the development of new methods for cancer diagnosis related tasks. In this article, we propose a novel methodology for predicting Diffuse Large B-Cell Lymphoma patients treatment response from Whole Slide Images. Our method exploits several foundation models as feature extractors to obtain a local representation of the image corresponding to a small region of the tissue, then, a global representation of the image is obtained by aggregating these local representations using attention-based Multiple Instance Learning. Our experimental study conducted on a dataset of 152 patients, shows the promising results of our methodology, notably by highlighting the advantage of using foundation models compared to conventional ImageNet pre-training. Moreover, the obtained results clearly demonstrates the potential of foundation models for characterizing histopathology images and generating more suited semantic representation for this task.</li>
</ul>

<h3>Title: Deep Generative Models for Subgraph Prediction</h3>
<ul>
<li><strong>Authors: </strong>Erfaneh Mahmoudzadeh, Parmis Naddaf, Kiarash Zahirnia, Oliver Schulte</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.04053">https://arxiv.org/abs/2408.04053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.04053">https://arxiv.org/pdf/2408.04053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.04053]] Deep Generative Models for Subgraph Prediction(https://arxiv.org/abs/2408.04053)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs) are important across different domains, such as social network analysis and recommendation systems, due to their ability to model complex relational data. This paper introduces subgraph queries as a new task for deep graph learning. Unlike traditional graph prediction tasks that focus on individual components like link prediction or node classification, subgraph queries jointly predict the components of a target subgraph based on evidence that is represented by an observed subgraph. For instance, a subgraph query can predict a set of target links and/or node labels. To answer subgraph queries, we utilize a probabilistic deep Graph Generative Model. Specifically, we inductively train a Variational Graph Auto-Encoder (VGAE) model, augmented to represent a joint distribution over links, node features and labels. Bayesian optimization is used to tune a weighting for the relative importance of links, node features and labels in a specific domain. We describe a deterministic and a sampling-based inference method for estimating subgraph probabilities from the VGAE generative graph distribution, without retraining, in zero-shot fashion. For evaluation, we apply the inference methods on a range of subgraph queries on six benchmark datasets. We find that inference from a model achieves superior predictive performance, surpassing independent prediction baselines with improvements in AUC scores ranging from 0.06 to 0.2 points, depending on the dataset.</li>
</ul>

<h3>Title: PowerPM: Foundation Model for Power Systems</h3>
<ul>
<li><strong>Authors: </strong>Shihao Tu, Yupeng Zhang, Jing Zhang, Yang Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.04057">https://arxiv.org/abs/2408.04057</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.04057">https://arxiv.org/pdf/2408.04057</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.04057]] PowerPM: Foundation Model for Power Systems(https://arxiv.org/abs/2408.04057)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>The emergence of abundant electricity time series (ETS) data provides ample opportunities for various applications in the power systems, including demand-side management, grid stability, and consumer behavior analysis. Deep learning models have advanced ETS modeling by effectively capturing sequence dependence. Nevertheless, learning a generic representation of ETS data for various applications remains challenging due to the inherently complex hierarchical structure of ETS data. Moreover, ETS data exhibits intricate temporal dependencies and is suscepti ble to the influence of exogenous variables. Furthermore, different instances exhibit diverse electricity consumption behavior. In this paper, we propose a foundation model PowerPM to model ETS data, providing a large-scale, off-the-shelf model for power systems. PowerPM consists of a temporal encoder and a hierarchical encoder. The temporal encoder captures both temporal dependencies in ETS data, considering exogenous variables. The hierarchical encoder models the correlation between hierarchy. Furthermore, PowerPM leverages a novel self-supervised pretraining framework consisting of masked ETS modeling and dual-view contrastive learning, which enable PowerPM to capture temporal dependency within ETS windows and aware the discrepancy across ETS windows, providing two different perspectives to learn generic representation. Our experiments involve five real world scenario datasets, comprising private and public data. Through pre-training on massive ETS data, PowerPM achieves SOTA performance on diverse downstream tasks within the private dataset. Impressively, when transferred to the public datasets, PowerPM maintains its superiority, showcasing its remarkable generalization ability across various tasks and domains. Moreover, ablation studies, few-shot experiments provide additional evidence of the effectiveness of our model.</li>
</ul>

<h3>Title: ArtVLM: Attribute Recognition Through Vision-Based Prefix Language Modeling</h3>
<ul>
<li><strong>Authors: </strong>William Y. Zhu, Keren Ye, Junjie Ke, Jiahui Yu, Leonidas Guibas, Peyman Milanfar, Feng Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.04102">https://arxiv.org/abs/2408.04102</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.04102">https://arxiv.org/pdf/2408.04102</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.04102]] ArtVLM: Attribute Recognition Through Vision-Based Prefix Language Modeling(https://arxiv.org/abs/2408.04102)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recognizing and disentangling visual attributes from objects is a foundation to many computer vision applications. While large vision language representations like CLIP had largely resolved the task of zero-shot object recognition, zero-shot visual attribute recognition remains a challenge because CLIP's contrastively-learned vision-language representation cannot effectively capture object-attribute dependencies. In this paper, we target this weakness and propose a sentence generation-based retrieval formulation for attribute recognition that is novel in 1) explicitly modeling a to-be-measured and retrieved object-attribute relation as a conditional probability graph, which converts the recognition problem into a dependency-sensitive language-modeling problem, and 2) applying a large pretrained Vision-Language Model (VLM) on this reformulation and naturally distilling its knowledge of image-object-attribute relations to use towards attribute recognition. Specifically, for each attribute to be recognized on an image, we measure the visual-conditioned probability of generating a short sentence encoding the attribute's relation to objects on the image. Unlike contrastive retrieval, which measures likelihood by globally aligning elements of the sentence to the image, generative retrieval is sensitive to the order and dependency of objects and attributes in the sentence. We demonstrate through experiments that generative retrieval consistently outperforms contrastive retrieval on two visual reasoning datasets, Visual Attribute in the Wild (VAW), and our newly-proposed Visual Genome Attribute Ranking (VGARank).</li>
</ul>

<h3>Title: Uncertainty-Aware Crime Prediction With Spatial Temporal Multivariate Graph Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Zepu Wang, Xiaobo Ma, Huajie Yang, Weimin Lvu, Peng Sun, Sharath Chandra Guntuku</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.04193">https://arxiv.org/abs/2408.04193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.04193">https://arxiv.org/pdf/2408.04193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.04193]] Uncertainty-Aware Crime Prediction With Spatial Temporal Multivariate Graph Neural Networks(https://arxiv.org/abs/2408.04193)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Crime forecasting is a critical component of urban analysis and essential for stabilizing society today. Unlike other time series forecasting problems, crime incidents are sparse, particularly in small regions and within specific time periods. Traditional spatial-temporal deep learning models often struggle with this sparsity, as they typically cannot effectively handle the non-Gaussian nature of crime data, which is characterized by numerous zeros and over-dispersed patterns. To address these challenges, we introduce a novel approach termed Spatial Temporal Multivariate Zero-Inflated Negative Binomial Graph Neural Networks (STMGNN-ZINB). This framework leverages diffusion and convolution networks to analyze spatial, temporal, and multivariate correlations, enabling the parameterization of probabilistic distributions of crime incidents. By incorporating a Zero-Inflated Negative Binomial model, STMGNN-ZINB effectively manages the sparse nature of crime data, enhancing prediction accuracy and the precision of confidence intervals. Our evaluation on real-world datasets confirms that STMGNN-ZINB outperforms existing models, providing a more reliable tool for predicting and understanding crime dynamics.</li>
</ul>

<h3>Title: Diffusion Guided Language Modeling</h3>
<ul>
<li><strong>Authors: </strong>Justin Lovelace, Varsha Kishore, Yiwei Chen, Kilian Q. Weinberger</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.04220">https://arxiv.org/abs/2408.04220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.04220">https://arxiv.org/pdf/2408.04220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.04220]] Diffusion Guided Language Modeling(https://arxiv.org/abs/2408.04220)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Current language models demonstrate remarkable proficiency in text generation. However, for many applications it is desirable to control attributes, such as sentiment, or toxicity, of the generated language -- ideally tailored towards each specific use case and target audience. For auto-regressive language models, existing guidance methods are prone to decoding errors that cascade during generation and degrade performance. In contrast, text diffusion models can easily be guided with, for example, a simple linear sentiment classifier -- however they do suffer from significantly higher perplexity than auto-regressive alternatives. In this paper we use a guided diffusion model to produce a latent proposal that steers an auto-regressive language model to generate text with desired properties. Our model inherits the unmatched fluency of the auto-regressive approach and the plug-and-play flexibility of diffusion. We show that it outperforms previous plug-and-play guidance methods across a wide range of benchmark data sets. Further, controlling a new attribute in our framework is reduced to training a single logistic regression classifier.</li>
</ul>

<h3>Title: Connective Viewpoints of Signal-to-Noise Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Khanh Doan, Long Tung Vuong, Tuan Nguyen, Anh Tuan Bui, Quyen Tran, Thanh-Toan Do, Dinh Phung, Trung Le</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.04221">https://arxiv.org/abs/2408.04221</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.04221">https://arxiv.org/pdf/2408.04221</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.04221]] Connective Viewpoints of Signal-to-Noise Diffusion Models(https://arxiv.org/abs/2408.04221)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models (DM) have become fundamental components of generative models, excelling across various domains such as image creation, audio generation, and complex data interpolation. Signal-to-Noise diffusion models constitute a diverse family covering most state-of-the-art diffusion models. While there have been several attempts to study Signal-to-Noise (S2N) diffusion models from various perspectives, there remains a need for a comprehensive study connecting different viewpoints and exploring new perspectives. In this study, we offer a comprehensive perspective on noise schedulers, examining their role through the lens of the signal-to-noise ratio (SNR) and its connections to information theory. Building upon this framework, we have developed a generalized backward equation to enhance the performance of the inference process.</li>
</ul>

<h3>Title: Cross-View Meets Diffusion: Aerial Image Synthesis with Geometry and Text Guidance</h3>
<ul>
<li><strong>Authors: </strong>Ahmad Arrabi, Xiaohan Zhang, Waqas Sultan, Chen Chen, Safwan Wshah</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.04224">https://arxiv.org/abs/2408.04224</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.04224">https://arxiv.org/pdf/2408.04224</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.04224]] Cross-View Meets Diffusion: Aerial Image Synthesis with Geometry and Text Guidance(https://arxiv.org/abs/2408.04224)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Aerial imagery analysis is critical for many research fields. However, obtaining frequent high-quality aerial images is not always accessible due to its high effort and cost requirements. One solution is to use the Ground-to-Aerial (G2A) technique to synthesize aerial images from easily collectible ground images. However, G2A is rarely studied, because of its challenges, including but not limited to, the drastic view changes, occlusion, and range of visibility. In this paper, we present a novel Geometric Preserving Ground-to-Aerial (G2A) image synthesis (GPG2A) model that can generate realistic aerial images from ground images. GPG2A consists of two stages. The first stage predicts the Bird's Eye View (BEV) segmentation (referred to as the BEV layout map) from the ground image. The second stage synthesizes the aerial image from the predicted BEV layout map and text descriptions of the ground image. To train our model, we present a new multi-modal cross-view dataset, namely VIGORv2 which is built upon VIGOR with newly collected aerial images, maps, and text descriptions. Our extensive experiments illustrate that GPG2A synthesizes better geometry-preserved aerial images than existing models. We also present two applications, data augmentation for cross-view geo-localization and sketch-based region search, to further verify the effectiveness of our GPG2A. The code and data will be publicly available.</li>
</ul>

<h3>Title: LLDif: Diffusion Models for Low-light Emotion Recognition</h3>
<ul>
<li><strong>Authors: </strong>Zhifeng Wang, Kaihao Zhang, Ramesh Sankaranarayana</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.04235">https://arxiv.org/abs/2408.04235</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.04235">https://arxiv.org/pdf/2408.04235</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.04235]] LLDif: Diffusion Models for Low-light Emotion Recognition(https://arxiv.org/abs/2408.04235)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper introduces LLDif, a novel diffusion-based facial expression recognition (FER) framework tailored for extremely low-light (LL) environments. Images captured under such conditions often suffer from low brightness and significantly reduced contrast, presenting challenges to conventional methods. These challenges include poor image quality that can significantly reduce the accuracy of emotion recognition. LLDif addresses these issues with a novel two-stage training process that combines a Label-aware CLIP (LA-CLIP), an embedding prior network (PNET), and a transformer-based network adept at handling the noise of low-light images. The first stage involves LA-CLIP generating a joint embedding prior distribution (EPD) to guide the LLformer in label recovery. In the second stage, the diffusion model (DM) refines the EPD inference, ultilising the compactness of EPD for precise predictions. Experimental evaluations on various LL-FER datasets have shown that LLDif achieves competitive performance, underscoring its potential to enhance FER applications in challenging lighting conditions.</li>
</ul>

<h3>Title: Cluster-Wide Task Slowdown Detection in Cloud System</h3>
<ul>
<li><strong>Authors: </strong>Feiyi Chen, Yingying Zhang, Lunting Fan, Yuxuan Liang, Guansong Pang, Qingsong Wen, Shuiguang Deng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.04236">https://arxiv.org/abs/2408.04236</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.04236">https://arxiv.org/pdf/2408.04236</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.04236]] Cluster-Wide Task Slowdown Detection in Cloud System(https://arxiv.org/abs/2408.04236)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Slow task detection is a critical problem in cloud operation and maintenance since it is highly related to user experience and can bring substantial liquidated damages. Most anomaly detection methods detect it from a single-task aspect. However, considering millions of concurrent tasks in large-scale cloud computing clusters, it becomes impractical and inefficient. Moreover, single-task slowdowns are very common and do not necessarily indicate a malfunction of a cluster due to its violent fluctuation nature in a virtual environment. Thus, we shift our attention to cluster-wide task slowdowns by utilizing the duration time distribution of tasks across a cluster, so that the computation complexity is not relevant to the number of tasks. The task duration time distribution often exhibits compound periodicity and local exceptional fluctuations over time. Though transformer-based methods are one of the most powerful methods to capture these time series normal variation patterns, we empirically find and theoretically explain the flaw of the standard attention mechanism in reconstructing subperiods with low amplitude when dealing with compound periodicity. To tackle these challenges, we propose SORN (i.e., Skimming Off subperiods in descending amplitude order and Reconstructing Non-slowing fluctuation), which consists of a Skimming Attention mechanism to reconstruct the compound periodicity and a Neural Optimal Transport module to distinguish cluster-wide slowdowns from other exceptional fluctuations. Furthermore, since anomalies in the training set are inevitable in a practical scenario, we propose a picky loss function, which adaptively assigns higher weights to reliable time slots in the training set. Extensive experiments demonstrate that SORN outperforms state-of-the-art methods on multiple real-world industrial datasets.</li>
</ul>

<h3>Title: The Ungrounded Alignment Problem</h3>
<ul>
<li><strong>Authors: </strong>Marc Pickett, Aakash Kumar Nain, Joseph Modayil, Llion Jones</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.04242">https://arxiv.org/abs/2408.04242</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.04242">https://arxiv.org/pdf/2408.04242</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.04242]] The Ungrounded Alignment Problem(https://arxiv.org/abs/2408.04242)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Modern machine learning systems have demonstrated substantial abilities with methods that either embrace or ignore human-provided knowledge, but combining benefits of both styles remains a challenge. One particular challenge involves designing learning systems that exhibit built-in responses to specific abstract stimulus patterns, yet are still plastic enough to be agnostic about the modality and exact form of their inputs. In this paper, we investigate what we call The Ungrounded Alignment Problem, which asks How can we build in predefined knowledge in a system where we don't know how a given stimulus will be grounded? This paper examines a simplified version of the general problem, where an unsupervised learner is presented with a sequence of images for the characters in a text corpus, and this learner is later evaluated on its ability to recognize specific (possibly rare) sequential patterns. Importantly, the learner is given no labels during learning or evaluation, but must map images from an unknown font or permutation to its correct class label. That is, at no point is our learner given labeled images, where an image vector is explicitly associated with a class label. Despite ample work in unsupervised and self-supervised loss functions, all current methods require a labeled fine-tuning phase to map the learned representations to correct classes. Finding this mapping in the absence of labels may seem a fool's errand, but our main result resolves this seeming paradox. We show that leveraging only letter bigram frequencies is sufficient for an unsupervised learner both to reliably associate images to class labels and to reliably identify trigger words in the sequence of inputs. More generally, this method suggests an approach for encoding specific desired innate behaviour in modality-agnostic models.</li>
</ul>

<h3>Title: MU-MAE: Multimodal Masked Autoencoders-Based One-Shot Learning</h3>
<ul>
<li><strong>Authors: </strong>Rex Liu, Xin Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.04243">https://arxiv.org/abs/2408.04243</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.04243">https://arxiv.org/pdf/2408.04243</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.04243]] MU-MAE: Multimodal Masked Autoencoders-Based One-Shot Learning(https://arxiv.org/abs/2408.04243)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>With the exponential growth of multimedia data, leveraging multimodal sensors presents a promising approach for improving accuracy in human activity recognition. Nevertheless, accurately identifying these activities using both video data and wearable sensor data presents challenges due to the labor-intensive data annotation, and reliance on external pretrained models or additional data. To address these challenges, we introduce Multimodal Masked Autoencoders-Based One-Shot Learning (Mu-MAE). Mu-MAE integrates a multimodal masked autoencoder with a synchronized masking strategy tailored for wearable sensors. This masking strategy compels the networks to capture more meaningful spatiotemporal features, which enables effective self-supervised pretraining without the need for external data. Furthermore, Mu-MAE leverages the representation extracted from multimodal masked autoencoders as prior information input to a cross-attention multimodal fusion layer. This fusion layer emphasizes spatiotemporal features requiring attention across different modalities while highlighting differences from other classes, aiding in the classification of various classes in metric-based one-shot learning. Comprehensive evaluations on MMAct one-shot classification show that Mu-MAE outperforms all the evaluated approaches, achieving up to an 80.17% accuracy for five-way one-shot multimodal classification, without the use of additional data.</li>
</ul>

<h3>Title: InstantStyleGaussian: Efficient Art Style Transfer with 3D Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Xin-Yi Yu, Jun-Xin Yu, Li-Bo Zhou, Yan Wei, Lin-Lin Ou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.04249">https://arxiv.org/abs/2408.04249</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.04249">https://arxiv.org/pdf/2408.04249</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.04249]] InstantStyleGaussian: Efficient Art Style Transfer with 3D Gaussian Splatting(https://arxiv.org/abs/2408.04249)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present InstantStyleGaussian, an innovative 3D style transfer method based on the 3D Gaussian Splatting (3DGS) scene representation. By inputting a target style image, it quickly generates new 3D GS scenes. Our approach operates on pre-reconstructed GS scenes, combining diffusion models with an improved iterative dataset update strategy. It utilizes diffusion models to generate target style images, adds these new images to the training dataset, and uses this dataset to iteratively update and optimize the GS scenes. Extensive experimental results demonstrate that our method ensures high-quality stylized scenes while offering significant advantages in style transfer speed and consistency.</li>
</ul>

<h3>Title: Generating Fine-Grained Causality in Climate Time Series Data for Forecasting and Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Dongqi Fu, Yada Zhu, Hanghang Tong, Kommy Weldemariam, Onkar Bhardwaj, Jingrui He</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.04254">https://arxiv.org/abs/2408.04254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.04254">https://arxiv.org/pdf/2408.04254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.04254]] Generating Fine-Grained Causality in Climate Time Series Data for Forecasting and Anomaly Detection(https://arxiv.org/abs/2408.04254)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, anomaly</a></li>
<li><strong>Abstract: </strong>Understanding the causal interaction of time series variables can contribute to time series data analysis for many real-world applications, such as climate forecasting and extreme weather alerts. However, causal relationships are difficult to be fully observed in real-world complex settings, such as spatial-temporal data from deployed sensor networks. Therefore, to capture fine-grained causal relations among spatial-temporal variables for further a more accurate and reliable time series analysis, we first design a conceptual fine-grained causal model named TBN Granger Causality, which adds time-respecting Bayesian Networks to the previous time-lagged Neural Granger Causality to offset the instantaneous effects. Second, we propose an end-to-end deep generative model called TacSas, which discovers TBN Granger Causality in a generative manner to help forecast time series data and detect possible anomalies during the forecast. For evaluations, besides the causality discovery benchmark Lorenz-96, we also test TacSas on climate benchmark ERA5 for climate forecasting and the extreme weather benchmark of NOAA for extreme weather alerts.</li>
</ul>

<h3>Title: Unveiling Hidden Visual Information: A Reconstruction Attack Against Adversarial Visual Information Hiding</h3>
<ul>
<li><strong>Authors: </strong>Jonggyu Jang, Hyeonsu Lyu, Seongjin Hwang, Hyun Jong Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.04261">https://arxiv.org/abs/2408.04261</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.04261">https://arxiv.org/pdf/2408.04261</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.04261]] Unveiling Hidden Visual Information: A Reconstruction Attack Against Adversarial Visual Information Hiding(https://arxiv.org/abs/2408.04261)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper investigates the security vulnerabilities of adversarial-example-based image encryption by executing data reconstruction (DR) attacks on encrypted images. A representative image encryption method is the adversarial visual information hiding (AVIH), which uses type-I adversarial example training to protect gallery datasets used in image recognition tasks. In the AVIH method, the type-I adversarial example approach creates images that appear completely different but are still recognized by machines as the original ones. Additionally, the AVIH method can restore encrypted images to their original forms using a predefined private key generative model. For the best security, assigning a unique key to each image is recommended; however, storage limitations may necessitate some images sharing the same key model. This raises a crucial security question for AVIH: How many images can safely share the same key model without being compromised by a DR attack? To address this question, we introduce a dual-strategy DR attack against the AVIH encryption method by incorporating (1) generative-adversarial loss and (2) augmented identity loss, which prevent DR from overfitting -- an issue akin to that in machine learning. Our numerical results validate this approach through image recognition and re-identification benchmarks, demonstrating that our strategy can significantly enhance the quality of reconstructed images, thereby requiring fewer key-sharing encrypted images. Our source code to reproduce our results will be available soon.</li>
</ul>

<h3>Title: CoBooM: Codebook Guided Bootstrapping for Medical Image Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Azad Singh, Deepak Mishra</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.04262">https://arxiv.org/abs/2408.04262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.04262">https://arxiv.org/pdf/2408.04262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.04262]] CoBooM: Codebook Guided Bootstrapping for Medical Image Representation Learning(https://arxiv.org/abs/2408.04262)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-supervised learning (SSL) has emerged as a promising paradigm for medical image analysis by harnessing unannotated data. Despite their potential, the existing SSL approaches overlook the high anatomical similarity inherent in medical images. This makes it challenging for SSL methods to capture diverse semantic content in medical images consistently. This work introduces a novel and generalized solution that implicitly exploits anatomical similarities by integrating codebooks in SSL. The codebook serves as a concise and informative dictionary of visual patterns, which not only aids in capturing nuanced anatomical details but also facilitates the creation of robust and generalized feature representations. In this context, we propose CoBooM, a novel framework for self-supervised medical image learning by integrating continuous and discrete representations. The continuous component ensures the preservation of fine-grained details, while the discrete aspect facilitates coarse-grained feature extraction through the structured embedding space. To understand the effectiveness of CoBooM, we conduct a comprehensive evaluation of various medical datasets encompassing chest X-rays and fundus images. The experimental results reveal a significant performance gain in classification and segmentation tasks.</li>
</ul>

<h3>Title: Dual-branch PolSAR Image Classification Based on GraphMAE and Local Feature Extraction</h3>
<ul>
<li><strong>Authors: </strong>Yuchen Wang, Ziyi Guo, Haixia Bi, Danfeng Hong, Chen Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.04294">https://arxiv.org/abs/2408.04294</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.04294">https://arxiv.org/pdf/2408.04294</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.04294]] Dual-branch PolSAR Image Classification Based on GraphMAE and Local Feature Extraction(https://arxiv.org/abs/2408.04294)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, generative</a></li>
<li><strong>Abstract: </strong>The annotation of polarimetric synthetic aperture radar (PolSAR) images is a labor-intensive and time-consuming process. Therefore, classifying PolSAR images with limited labels is a challenging task in remote sensing domain. In recent years, self-supervised learning approaches have proven effective in PolSAR image classification with sparse labels. However, we observe a lack of research on generative selfsupervised learning in the studied task. Motivated by this, we propose a dual-branch classification model based on generative self-supervised learning in this paper. The first branch is a superpixel-branch, which learns superpixel-level polarimetric representations using a generative self-supervised graph masked autoencoder. To acquire finer classification results, a convolutional neural networks-based pixel-branch is further incorporated to learn pixel-level features. Classification with fused dual-branch features is finally performed to obtain the predictions. Experimental results on the benchmark Flevoland dataset demonstrate that our approach yields promising classification results.</li>
</ul>

<h3>Title: Self-Supervised Contrastive Graph Clustering Network via Structural Information Fusion</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyang Ji, Yuchen Zhou, Haofu Yang, Shiyue Xu, Jiahao Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.04339">https://arxiv.org/abs/2408.04339</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.04339">https://arxiv.org/pdf/2408.04339</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.04339]] Self-Supervised Contrastive Graph Clustering Network via Structural Information Fusion(https://arxiv.org/abs/2408.04339)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, anomaly</a></li>
<li><strong>Abstract: </strong>Graph clustering, a classical task in graph learning, involves partitioning the nodes of a graph into distinct clusters. This task has applications in various real-world scenarios, such as anomaly detection, social network analysis, and community discovery. Current graph clustering methods commonly rely on module pre-training to obtain a reliable prior distribution for the model, which is then used as the optimization objective. However, these methods often overlook deeper supervised signals, leading to sub-optimal reliability of the prior distribution. To address this issue, we propose a novel deep graph clustering method called CGCN. Our approach introduces contrastive signals and deep structural information into the pre-training process. Specifically, CGCN utilizes a contrastive learning mechanism to foster information interoperability among multiple modules and allows the model to adaptively adjust the degree of information aggregation for different order structures. Our CGCN method has been experimentally validated on multiple real-world graph datasets, showcasing its ability to boost the dependability of prior clustering distributions acquired through pre-training. As a result, we observed notable enhancements in the performance of the model.</li>
</ul>

<h3>Title: AggSS: An Aggregated Self-Supervised Approach for Class-Incremental Learning</h3>
<ul>
<li><strong>Authors: </strong>Jayateja Kalla, Soma Biswas</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.04347">https://arxiv.org/abs/2408.04347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.04347">https://arxiv.org/pdf/2408.04347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.04347]] AggSS: An Aggregated Self-Supervised Approach for Class-Incremental Learning(https://arxiv.org/abs/2408.04347)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>This paper investigates the impact of self-supervised learning, specifically image rotations, on various class-incremental learning paradigms. Here, each image with a predefined rotation is considered as a new class for training. At inference, all image rotation predictions are aggregated for the final prediction, a strategy we term Aggregated Self-Supervision (AggSS). We observe a shift in the deep neural network's attention towards intrinsic object features as it learns through AggSS strategy. This learning approach significantly enhances class-incremental learning by promoting robust feature learning. AggSS serves as a plug-and-play module that can be seamlessly incorporated into any class-incremental learning framework, leveraging its powerful feature learning capabilities to enhance performance across various class-incremental learning approaches. Extensive experiments conducted on standard incremental learning datasets CIFAR-100 and ImageNet-Subset demonstrate the significant role of AggSS in improving performance within these paradigms.</li>
</ul>

<h3>Title: Anomaly Prediction: A Novel Approach with Explicit Delay and Horizon</h3>
<ul>
<li><strong>Authors: </strong>Jiang You, Arben Cela, Ren√© Natowicz, Jacob Ouanounou, Patrick Siarry</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.04377">https://arxiv.org/abs/2408.04377</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.04377">https://arxiv.org/pdf/2408.04377</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.04377]] Anomaly Prediction: A Novel Approach with Explicit Delay and Horizon(https://arxiv.org/abs/2408.04377)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Detecting anomalies in time series data is a critical challenge across various domains. Traditional methods typically focus on identifying anomalies in immediate subsequent steps, often underestimating the significance of temporal dynamics such as delay time and horizons of anomalies, which generally require extensive post-analysis. This paper introduces a novel approach for time series anomaly prediction, incorporating temporal information directly into the prediction results. We propose a new dataset specifically designed to evaluate this approach and conduct comprehensive experiments using several state-of-the-art methods. results demonstrate the efficacy of our approach in providing timely and accurate anomaly predictions, setting a new benchmark for future research in this field.</li>
</ul>

<h3>Title: Deeploy: Enabling Energy-Efficient Deployment of Small Language Models On Heterogeneous Microcontrollers</h3>
<ul>
<li><strong>Authors: </strong>Moritz Scherer, Luka Macan, Victor Jung, Philip Wiese, Luca Bompani, Alessio Burrello, Francesco Conti, Luca Benini</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.04413">https://arxiv.org/abs/2408.04413</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.04413">https://arxiv.org/pdf/2408.04413</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.04413]] Deeploy: Enabling Energy-Efficient Deployment of Small Language Models On Heterogeneous Microcontrollers(https://arxiv.org/abs/2408.04413)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>With the rise of Embodied Foundation Models (EFMs), most notably Small Language Models (SLMs), adapting Transformers for edge applications has become a very active field of research. However, achieving end-to-end deployment of SLMs on microcontroller (MCU)-class chips without high-bandwidth off-chip main memory access is still an open challenge. In this paper, we demonstrate high-efficiency end-to-end SLM deployment on a multicore RISC-V (RV32) MCU augmented with ML instruction extensions and a hardware neural processing unit (NPU). To automate the exploration of the constrained, multi-dimensional memory vs. computation tradeoffs involved in aggressive SLM deployment on heterogeneous (multicore+NPU) resources, we introduce Deeploy, a novel Deep Neural Network (DNN) compiler, which generates highly-optimized C code requiring minimal runtime support. We demonstrate that Deeploy generates end-to-end code for executing SLMs, fully exploiting the RV32 cores' instruction extensions and the NPU: We achieve leading-edge energy and throughput of \SI{490}{\micro\joule \per Token}, at \SI{340}{Token \per \second} for an SLM trained on the TinyStories dataset, running for the first time on an MCU-class device without external memory.</li>
</ul>

<h3>Title: Enhancing Robustness of Retrieval-Augmented Language Models with In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Seong-Il Park, Seung-Woo Choi, Na-Hyun Kim, Jay-Yoon Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.04414">https://arxiv.org/abs/2408.04414</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.04414">https://arxiv.org/pdf/2408.04414</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.04414]] Enhancing Robustness of Retrieval-Augmented Language Models with In-Context Learning(https://arxiv.org/abs/2408.04414)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Retrieval-Augmented Language Models (RALMs) have significantly improved performance in open-domain question answering (QA) by leveraging external knowledge. However, RALMs still struggle with unanswerable queries, where the retrieved contexts do not contain the correct answer, and with conflicting information, where different sources provide contradictory answers due to imperfect retrieval. This study introduces an in-context learning-based approach to enhance the reasoning capabilities of RALMs, making them more robust in imperfect retrieval scenarios. Our method incorporates Machine Reading Comprehension (MRC) demonstrations, referred to as cases, to boost the model's capabilities to identify unanswerabilities and conflicts among the retrieved contexts. Experiments on two open-domain QA datasets show that our approach increases accuracy in identifying unanswerable and conflicting scenarios without requiring additional fine-tuning. This work demonstrates that in-context learning can effectively enhance the robustness of RALMs in open-domain QA tasks.</li>
</ul>

<h3>Title: Detection of Animal Movement from Weather Radar using Self-Supervised Learning</h3>
<ul>
<li><strong>Authors: </strong>Mubin Ul Haque, Joel Janek Dabrowski, Rebecca M. Rogers, Hazel Parry</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.04424">https://arxiv.org/abs/2408.04424</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.04424">https://arxiv.org/pdf/2408.04424</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.04424]] Detection of Animal Movement from Weather Radar using Self-Supervised Learning(https://arxiv.org/abs/2408.04424)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Detecting flying animals (e.g., birds, bats, and insects) using weather radar helps gain insights into animal movement and migration patterns, aids in management efforts (such as biosecurity) and enhances our understanding of the ecosystem.The conventional approach to detecting animals in weather radar involves thresholding: defining and applying thresholds for the radar variables, based on expert opinion. More recently, Deep Learning approaches have been shown to provide improved performance in detection. However, obtaining sufficient labelled weather radar data for flying animals to build learning-based models is time-consuming and labor-intensive. To address the challenge of data labelling, we propose a self-supervised learning method for detecting animal movement. In our proposed method, we pre-train our model on a large dataset with noisy labels produced by a threshold approach. The key advantage is that the pre-trained dataset size is limited only by the number of radar images available. We then fine-tune the model on a small human-labelled dataset. Our experiments on Australian weather radar data for waterbird segmentation show that the proposed method outperforms the current state-of-the art approach by 43.53% in the dice co-efficient statistic.</li>
</ul>

<h3>Title: FedAD-Bench: A Unified Benchmark for Federated Unsupervised Anomaly Detection in Tabular Data</h3>
<ul>
<li><strong>Authors: </strong>Ahmed Anwar, Brian Moser, Dayananda Herurkar, Federico Raue, Vinit Hegiste, Tatjana Legler, Andreas Dengel</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.04442">https://arxiv.org/abs/2408.04442</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.04442">https://arxiv.org/pdf/2408.04442</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.04442]] FedAD-Bench: A Unified Benchmark for Federated Unsupervised Anomaly Detection in Tabular Data(https://arxiv.org/abs/2408.04442)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>The emergence of federated learning (FL) presents a promising approach to leverage decentralized data while preserving privacy. Furthermore, the combination of FL and anomaly detection is particularly compelling because it allows for detecting rare and critical anomalies (usually also rare in locally gathered data) in sensitive data from multiple sources, such as cybersecurity and healthcare. However, benchmarking the performance of anomaly detection methods in FL environments remains an underexplored area. This paper introduces FedAD-Bench, a unified benchmark for evaluating unsupervised anomaly detection algorithms within the context of FL. We systematically analyze and compare the performance of recent deep learning anomaly detection models under federated settings, which were typically assessed solely in centralized settings. FedAD-Bench encompasses diverse datasets and metrics to provide a holistic evaluation. Through extensive experiments, we identify key challenges such as model aggregation inefficiencies and metric unreliability. We present insights into FL's regularization effects, revealing scenarios in which it outperforms centralized approaches due to its inherent ability to mitigate overfitting. Our work aims to establish a standardized benchmark to guide future research and development in federated anomaly detection, promoting reproducibility and fair comparison across studies.</li>
</ul>

<h3>Title: Random Walk Diffusion for Efficient Large-Scale Graph Generation</h3>
<ul>
<li><strong>Authors: </strong>Tobias Bernecker, Ghalia Rehawi, Francesco Paolo Casale, Janine Knauer-Arloth, Annalisa Marsico</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.04461">https://arxiv.org/abs/2408.04461</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.04461">https://arxiv.org/pdf/2408.04461</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.04461]] Random Walk Diffusion for Efficient Large-Scale Graph Generation(https://arxiv.org/abs/2408.04461)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Graph generation addresses the problem of generating new graphs that have a data distribution similar to real-world graphs. While previous diffusion-based graph generation methods have shown promising results, they often struggle to scale to large graphs. In this work, we propose ARROW-Diff (AutoRegressive RandOm Walk Diffusion), a novel random walk-based diffusion approach for efficient large-scale graph generation. Our method encompasses two components in an iterative process of random walk sampling and graph pruning. We demonstrate that ARROW-Diff can scale to large graphs efficiently, surpassing other baseline methods in terms of both generation time and multiple graph statistics, reflecting the high quality of the generated graphs.</li>
</ul>

<h3>Title: NFDI4Health workflow and service for synthetic data generation, assessment and risk management</h3>
<ul>
<li><strong>Authors: </strong>Sobhan Moazemi, Tim Adams, Hwei Geok NG, Lisa K√ºhnel, Julian Schneider, Anatol-Fiete N√§her, Juliane Fluck, Holger Fr√∂hlich</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.04478">https://arxiv.org/abs/2408.04478</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.04478">https://arxiv.org/pdf/2408.04478</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.04478]] NFDI4Health workflow and service for synthetic data generation, assessment and risk management(https://arxiv.org/abs/2408.04478)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Individual health data is crucial for scientific advancements, particularly in developing Artificial Intelligence (AI); however, sharing real patient information is often restricted due to privacy concerns. A promising solution to this challenge is synthetic data generation. This technique creates entirely new datasets that mimic the statistical properties of real data, while preserving confidential patient information. In this paper, we present the workflow and different services developed in the context of Germany's National Data Infrastructure project NFDI4Health. First, two state-of-the-art AI tools (namely, VAMBN and MultiNODEs) for generating synthetic health data are outlined. Further, we introduce SYNDAT (a public web-based tool) which allows users to visualize and assess the quality and risk of synthetic data provided by desired generative models. Additionally, the utility of the proposed methods and the web-based tool is showcased using data from Alzheimer's Disease Neuroimaging Initiative (ADNI) and the Center for Cancer Registry Data of the Robert Koch Institute (RKI).</li>
</ul>

<h3>Title: Depth Any Canopy: Leveraging Depth Foundation Models for Canopy Height Estimation</h3>
<ul>
<li><strong>Authors: </strong>Daniele Rege Cambrin, Isaac Corley, Paolo Garza</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.04523">https://arxiv.org/abs/2408.04523</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.04523">https://arxiv.org/pdf/2408.04523</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.04523]] Depth Any Canopy: Leveraging Depth Foundation Models for Canopy Height Estimation(https://arxiv.org/abs/2408.04523)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Estimating global tree canopy height is crucial for forest conservation and climate change applications. However, capturing high-resolution ground truth canopy height using LiDAR is expensive and not available globally. An efficient alternative is to train a canopy height estimator to operate on single-view remotely sensed imagery. The primary obstacle to this approach is that these methods require significant training data to generalize well globally and across uncommon edge cases. Recent monocular depth estimation foundation models have show strong zero-shot performance even for complex scenes. In this paper we leverage the representations learned by these models to transfer to the remote sensing domain for measuring canopy height. Our findings suggest that our proposed Depth Any Canopy, the result of fine-tuning the Depth Anything v2 model for canopy height estimation, provides a performant and efficient solution, surpassing the current state-of-the-art with superior or comparable performance using only a fraction of the computational resources and parameters. Furthermore, our approach requires less than \$1.30 in compute and results in an estimated carbon footprint of 0.14 kgCO2. Code, experimental results, and model checkpoints are openly available at this https URL.</li>
</ul>

<h3>Title: How Transformers Utilize Multi-Head Attention in In-Context Learning? A Case Study on Sparse Linear Regression</h3>
<ul>
<li><strong>Authors: </strong>Xingwu Chen, Lei Zhao, Difan Zou</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.04532">https://arxiv.org/abs/2408.04532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.04532">https://arxiv.org/pdf/2408.04532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.04532]] How Transformers Utilize Multi-Head Attention in In-Context Learning? A Case Study on Sparse Linear Regression(https://arxiv.org/abs/2408.04532)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Despite the remarkable success of transformer-based models in various real-world tasks, their underlying mechanisms remain poorly understood. Recent studies have suggested that transformers can implement gradient descent as an in-context learner for linear regression problems and have developed various theoretical analyses accordingly. However, these works mostly focus on the expressive power of transformers by designing specific parameter constructions, lacking a comprehensive understanding of their inherent working mechanisms post-training. In this study, we consider a sparse linear regression problem and investigate how a trained multi-head transformer performs in-context learning. We experimentally discover that the utilization of multi-heads exhibits different patterns across layers: multiple heads are utilized and essential in the first layer, while usually only a single head is sufficient for subsequent layers. We provide a theoretical explanation for this observation: the first layer preprocesses the context data, and the following layers execute simple optimization steps based on the preprocessed context. Moreover, we demonstrate that such a preprocess-then-optimize algorithm can significantly outperform naive gradient descent and ridge regression algorithms. Further experimental results support our explanations. Our findings offer insights into the benefits of multi-head attention and contribute to understanding the more intricate mechanisms hidden within trained transformers.</li>
</ul>

<h3>Title: Bias-Aware Low-Rank Adaptation: Mitigating Catastrophic Inheritance of Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yupeng Chang, Yi Chang, Yuan Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.04556">https://arxiv.org/abs/2408.04556</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.04556">https://arxiv.org/pdf/2408.04556</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.04556]] Bias-Aware Low-Rank Adaptation: Mitigating Catastrophic Inheritance of Large Language Models(https://arxiv.org/abs/2408.04556)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have exhibited remarkable proficiency across a diverse array of natural language processing (NLP) tasks. However, adapting LLMs to downstream applications typically necessitates computationally intensive and memory-demanding fine-tuning procedures. To mitigate these burdens, parameter-efficient fine-tuning (PEFT) techniques have emerged as a promising approach to tailor LLMs with minimal computational overhead. While PEFT methods offer substantial advantages, they do not fully address the pervasive issue of bias propagation from pre-training data. In this work, we introduce Bias-Aware Low-Rank Adaptation (BA-LoRA), a novel PEFT method designed to counteract bias inheritance. BA-LoRA incorporates three distinct regularization terms: (1) consistency regularizer, (2) diversity regularizer, and (3) singular vector decomposition regularizer. These regularizers collectively aim to improve the generative models' consistency, diversity, and generalization capabilities during the fine-tuning process. Through extensive experiments on a variety of natural language understanding (NLU) and natural language generation (NLG) tasks, employing prominent LLMs such as LLaMA, Mistral, and Gemma, we demonstrate that BA-LoRA surpasses the performance of LoRA and its state-of-the-art variants. Moreover, our method effectively mitigates the deleterious effects of pre-training bias, leading to more reliable and robust model outputs. The code is available at this https URL.</li>
</ul>

<h3>Title: Sketch2Scene: Automatic Generation of Interactive 3D Game Scenes from User's Casual Sketches</h3>
<ul>
<li><strong>Authors: </strong>Yongzhi Xu, Yonhon Ng, Yifu Wang, Inkyu Sa, Yunfei Duan, Yang Li, Pan Ji, Hongdong Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.04567">https://arxiv.org/abs/2408.04567</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.04567">https://arxiv.org/pdf/2408.04567</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.04567]] Sketch2Scene: Automatic Generation of Interactive 3D Game Scenes from User's Casual Sketches(https://arxiv.org/abs/2408.04567)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>3D Content Generation is at the heart of many computer graphics applications, including video gaming, film-making, virtual and augmented reality, etc. This paper proposes a novel deep-learning based approach for automatically generating interactive and playable 3D game scenes, all from the user's casual prompts such as a hand-drawn sketch. Sketch-based input offers a natural, and convenient way to convey the user's design intention in the content creation process. To circumvent the data-deficient challenge in learning (i.e. the lack of large training data of 3D scenes), our method leverages a pre-trained 2D denoising diffusion model to generate a 2D image of the scene as the conceptual guidance. In this process, we adopt the isometric projection mode to factor out unknown camera poses while obtaining the scene layout. From the generated isometric image, we use a pre-trained image understanding method to segment the image into meaningful parts, such as off-ground objects, trees, and buildings, and extract the 2D scene layout. These segments and layouts are subsequently fed into a procedural content generation (PCG) engine, such as a 3D video game engine like Unity or Unreal, to create the 3D scene. The resulting 3D scene can be seamlessly integrated into a game development environment and is readily playable. Extensive tests demonstrate that our method can efficiently generate high-quality and interactive 3D game scenes with layouts that closely follow the user's intention.</li>
</ul>

<h3>Title: Learning Fine-Grained Grounded Citations for Attributed Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Lei Huang, Xiaocheng Feng, Weitao Ma, Yuxuan Gu, Weihong Zhong, Xiachong Feng, Weijiang Yu, Weihua Peng, Duyu Tang, Dandan Tu, Bing Qin</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.04568">https://arxiv.org/abs/2408.04568</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.04568">https://arxiv.org/pdf/2408.04568</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.04568]] Learning Fine-Grained Grounded Citations for Attributed Large Language Models(https://arxiv.org/abs/2408.04568)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Despite the impressive performance on information-seeking tasks, large language models (LLMs) still struggle with hallucinations. Attributed LLMs, which augment generated text with in-line citations, have shown potential in mitigating hallucinations and improving verifiability. However, current approaches suffer from suboptimal citation quality due to their reliance on in-context learning. Furthermore, the practice of citing only coarse document identifiers makes it challenging for users to perform fine-grained verification. In this work, we introduce FRONT, a training framework designed to teach LLMs to generate Fine-Grained Grounded Citations. By grounding model outputs in fine-grained supporting quotes, these quotes guide the generation of grounded and consistent responses, not only improving citation quality but also facilitating fine-grained verification. Experiments on the ALCE benchmark demonstrate the efficacy of FRONT in generating superior grounded responses and highly supportive citations. With LLaMA-2-7B, the framework significantly outperforms all the baselines, achieving an average of 14.21% improvement in citation quality across all datasets, even surpassing ChatGPT.</li>
</ul>

<h3>Title: SAM2-Adapter: Evaluating & Adapting Segment Anything 2 in Downstream Tasks: Camouflage, Shadow, Medical Image Segmentation, and More</h3>
<ul>
<li><strong>Authors: </strong>Tianrun Chen, Ankang Lu, Lanyun Zhu, Chaotao Ding, Chunan Yu, Deyi Ji, Zejian Li, Lingyun Sun, Papa Mao, Ying Zang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.04579">https://arxiv.org/abs/2408.04579</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.04579">https://arxiv.org/pdf/2408.04579</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.04579]] SAM2-Adapter: Evaluating & Adapting Segment Anything 2 in Downstream Tasks: Camouflage, Shadow, Medical Image Segmentation, and More(https://arxiv.org/abs/2408.04579)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>The advent of large models, also known as foundation models, has significantly transformed the AI research landscape, with models like Segment Anything (SAM) achieving notable success in diverse image segmentation scenarios. Despite its advancements, SAM encountered limitations in handling some complex low-level segmentation tasks like camouflaged object and medical imaging. In response, in 2023, we introduced SAM-Adapter, which demonstrated improved performance on these challenging tasks. Now, with the release of Segment Anything 2 (SAM2), a successor with enhanced architecture and a larger training corpus, we reassess these challenges. This paper introduces SAM2-Adapter, the first adapter designed to overcome the persistent limitations observed in SAM2 and achieve new state-of-the-art (SOTA) results in specific downstream tasks including medical image segmentation, camouflaged (concealed) object detection, and shadow detection. SAM2-Adapter builds on the SAM-Adapter's strengths, offering enhanced generalizability and composability for diverse applications. We present extensive experimental results demonstrating SAM2-Adapter's effectiveness. We show the potential and encourage the research community to leverage the SAM2 model with our SAM2-Adapter for achieving superior segmentation outcomes. Code, pre-trained models, and data processing protocols are available at this http URL</li>
</ul>

<h3>Title: Img-Diff: Contrastive Data Synthesis for Multimodal Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Qirui Jiao, Daoyuan Chen, Yilun Huang, Yaliang Li, Ying Shen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.04594">https://arxiv.org/abs/2408.04594</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.04594">https://arxiv.org/pdf/2408.04594</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.04594]] Img-Diff: Contrastive Data Synthesis for Multimodal Large Language Models(https://arxiv.org/abs/2408.04594)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>High-performance Multimodal Large Language Models (MLLMs) rely heavily on data quality. This study introduces a novel dataset named Img-Diff, designed to enhance fine-grained image recognition in MLLMs by leveraging insights from contrastive learning and image difference captioning. By analyzing object differences between similar images, we challenge models to identify both matching and distinct components. We utilize the Stable-Diffusion-XL model and advanced image editing techniques to create pairs of similar images that highlight object replacements. Our methodology includes a Difference Area Generator for object differences identifying, followed by a Difference Captions Generator for detailed difference descriptions. The result is a relatively small but high-quality dataset of "object replacement" samples. We use the the proposed dataset to fine-tune state-of-the-art (SOTA) MLLMs such as MGM-7B, yielding comprehensive improvements of performance scores over SOTA models that trained with larger-scale datasets, in numerous image difference and Visual Question Answering tasks. For instance, our trained models notably surpass the SOTA models GPT-4V and Gemini on the MMVP benchmark. Besides, we investigate alternative methods for generating image difference data through "object removal" and conduct thorough evaluation to confirm the dataset's diversity, quality, and robustness, presenting several insights on synthesis of such contrastive dataset. To encourage further research and advance the field of multimodal data synthesis and enhancement of MLLMs' fundamental capabilities for image understanding, we release our codes and dataset at this https URL.</li>
</ul>

<h3>Title: Towards High-resolution 3D Anomaly Detection via Group-Level Feature Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Hongze Zhu, Guoyang Xie, Chengbin Hou, Tao Dai, Can Gao, Jinbao Wang, Linlin Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.04604">https://arxiv.org/abs/2408.04604</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.04604">https://arxiv.org/pdf/2408.04604</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.04604]] Towards High-resolution 3D Anomaly Detection via Group-Level Feature Contrastive Learning(https://arxiv.org/abs/2408.04604)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>High-resolution point clouds~(HRPCD) anomaly detection~(AD) plays a critical role in precision machining and high-end equipment manufacturing. Despite considerable 3D-AD methods that have been proposed recently, they still cannot meet the requirements of the HRPCD-AD task. There are several challenges: i) It is difficult to directly capture HRPCD information due to large amounts of points at the sample level; ii) The advanced transformer-based methods usually obtain anisotropic features, leading to degradation of the representation; iii) The proportion of abnormal areas is very small, which makes it difficult to characterize. To address these challenges, we propose a novel group-level feature-based network, called Group3AD, which has a significantly efficient representation ability. First, we design an Intercluster Uniformity Network~(IUN) to present the mapping of different groups in the feature space as several clusters, and obtain a more uniform distribution between clusters representing different parts of the point clouds in the feature space. Then, an Intracluster Alignment Network~(IAN) is designed to encourage groups within the cluster to be distributed tightly in the feature space. In addition, we propose an Adaptive Group-Center Selection~(AGCS) based on geometric information to improve the pixel density of potential anomalous regions during inference. The experimental results verify the effectiveness of our proposed Group3AD, which surpasses Reg3D-AD by the margin of 5\% in terms of object-level AUROC on Real3D-AD. We provide the code and supplementary information on our website: this https URL.</li>
</ul>

<h3>Title: Transformer Explainer: Interactive Learning of Text-Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Aeree Cho, Grace C. Kim, Alexander Karpekov, Alec Helbling, Zijie J. Wang, Seongmin Lee, Benjamin Hoover, Duen Horng Chau</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.HC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.04619">https://arxiv.org/abs/2408.04619</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.04619">https://arxiv.org/pdf/2408.04619</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.04619]] Transformer Explainer: Interactive Learning of Text-Generative Models(https://arxiv.org/abs/2408.04619)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Transformers have revolutionized machine learning, yet their inner workings remain opaque to many. We present Transformer Explainer, an interactive visualization tool designed for non-experts to learn about Transformers through the GPT-2 model. Our tool helps users understand complex Transformer concepts by integrating a model overview and enabling smooth transitions across abstraction levels of mathematical operations and model structures. It runs a live GPT-2 instance locally in the user's browser, empowering users to experiment with their own input and observe in real-time how the internal components and parameters of the Transformer work together to predict the next tokens. Our tool requires no installation or special hardware, broadening the public's education access to modern generative AI techniques. Our open-sourced tool is available at this https URL. A video demo is available at this https URL.</li>
</ul>

<h3>Title: Puppet-Master: Scaling Interactive Video Generation as a Motion Prior for Part-Level Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Ruining Li, Chuanxia Zheng, Christian Rupprecht, Andrea Vedaldi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.04631">https://arxiv.org/abs/2408.04631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.04631">https://arxiv.org/pdf/2408.04631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.04631]] Puppet-Master: Scaling Interactive Video Generation as a Motion Prior for Part-Level Dynamics(https://arxiv.org/abs/2408.04631)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We present Puppet-Master, an interactive video generative model that can serve as a motion prior for part-level dynamics. At test time, given a single image and a sparse set of motion trajectories (i.e., drags), Puppet-Master can synthesize a video depicting realistic part-level motion faithful to the given drag interactions. This is achieved by fine-tuning a large-scale pre-trained video diffusion model, for which we propose a new conditioning architecture to inject the dragging control effectively. More importantly, we introduce the all-to-first attention mechanism, a drop-in replacement for the widely adopted spatial attention modules, which significantly improves generation quality by addressing the appearance and background issues in existing models. Unlike other motion-conditioned video generators that are trained on in-the-wild videos and mostly move an entire object, Puppet-Master is learned from Objaverse-Animation-HQ, a new dataset of curated part-level motion clips. We propose a strategy to automatically filter out sub-optimal animations and augment the synthetic renderings with meaningful motion trajectories. Puppet-Master generalizes well to real images across various categories and outperforms existing methods in a zero-shot manner on a real-world benchmark. See our project page for more results: this http URL.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
