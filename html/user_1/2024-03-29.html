<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-03-29</h1>
<h3>Title: JEP-KD: Joint-Embedding Predictive Architecture Based Knowledge  Distillation for Visual Speech Recognition</h3>
<ul>
<li><strong>Authors: </strong>Chang Sun, Hong Yang, Bo Qin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.LG, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18843">https://arxiv.org/abs/2403.18843</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18843">https://arxiv.org/pdf/2403.18843</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18843]] JEP-KD: Joint-Embedding Predictive Architecture Based Knowledge  Distillation for Visual Speech Recognition(https://arxiv.org/abs/2403.18843)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Visual Speech Recognition (VSR) tasks are generally recognized to have a lower theoretical performance ceiling than Automatic Speech Recognition (ASR), owing to the inherent limitations of conveying semantic information visually. To mitigate this challenge, this paper introduces an advanced knowledge distillation approach using a Joint-Embedding Predictive Architecture (JEPA), named JEP-KD, designed to more effectively utilize audio features during model training. Central to JEP-KD is the inclusion of a generative network within the embedding layer, which enhances the video encoder's capacity for semantic feature extraction and brings it into closer alignment with the audio features from a pre-trained ASR model's encoder. This approach aims to progressively reduce the performance gap between VSR and ASR. Moreover, a comprehensive multimodal, multistage training regimen for the JEP-KD framework is established, bolstering the robustness and efficacy of the training process. Experiment results demonstrate that JEP-KD significantly improves the performance of VSR models and demonstrates versatility across different VSR platforms, indicating its potential for broader application within other multimodal tasks.</li>
</ul>

<h3>Title: A Geometric Explanation of the Likelihood OOD Detection Paradox</h3>
<ul>
<li><strong>Authors: </strong>Hamidreza Kamkari, Brendan Leigh Ross, Jesse C. Cresswell, Anthony L. Caterini, Rahul G. Krishnan, Gabriel Loaiza-Ganem</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18910">https://arxiv.org/abs/2403.18910</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18910">https://arxiv.org/pdf/2403.18910</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18910]] A Geometric Explanation of the Likelihood OOD Detection Paradox(https://arxiv.org/abs/2403.18910)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Likelihood-based deep generative models (DGMs) commonly exhibit a puzzling behaviour: when trained on a relatively complex dataset, they assign higher likelihood values to out-of-distribution (OOD) data from simpler sources. Adding to the mystery, OOD samples are never generated by these DGMs despite having higher likelihoods. This two-pronged paradox has yet to be conclusively explained, making likelihood-based OOD detection unreliable. Our primary observation is that high-likelihood regions will not be generated if they contain minimal probability mass. We demonstrate how this seeming contradiction of large densities yet low probability mass can occur around data confined to low-dimensional manifolds. We also show that this scenario can be identified through local intrinsic dimension (LID) estimation, and propose a method for OOD detection which pairs the likelihoods and LID estimates obtained from a pre-trained DGM. Our method can be applied to normalizing flows and score-based diffusion models, and obtains results which match or surpass state-of-the-art OOD detection benchmarks using the same DGM backbones. Our code is available at https://github.com/layer6ai-labs/dgm_ood_detection.</li>
</ul>

<h3>Title: CPR: Retrieval Augmented Generation for Copyright Protection</h3>
<ul>
<li><strong>Authors: </strong>Aditya Golatkar, Alessandro Achille, Luca Zancato, Yu-Xiang Wang, Ashwin Swaminathan, Stefano Soatto</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18920">https://arxiv.org/abs/2403.18920</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18920">https://arxiv.org/pdf/2403.18920</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18920]] CPR: Retrieval Augmented Generation for Copyright Protection(https://arxiv.org/abs/2403.18920)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Retrieval Augmented Generation (RAG) is emerging as a flexible and robust technique to adapt models to private users data without training, to handle credit attribution, and to allow efficient machine unlearning at scale. However, RAG techniques for image generation may lead to parts of the retrieved samples being copied in the model's output. To reduce risks of leaking private information contained in the retrieved set, we introduce Copy-Protected generation with Retrieval (CPR), a new method for RAG with strong copyright protection guarantees in a mixed-private setting for diffusion models.CPR allows to condition the output of diffusion models on a set of retrieved images, while also guaranteeing that unique identifiable information about those example is not exposed in the generated outputs. In particular, it does so by sampling from a mixture of public (safe) distribution and private (user) distribution by merging their diffusion scores at inference. We prove that CPR satisfies Near Access Freeness (NAF) which bounds the amount of information an attacker may be able to extract from the generated images. We provide two algorithms for copyright protection, CPR-KL and CPR-Choose. Unlike previously proposed rejection-sampling-based NAF methods, our methods enable efficient copyright-protected sampling with a single run of backward diffusion. We show that our method can be applied to any pre-trained conditional diffusion model, such as Stable Diffusion or unCLIP. In particular, we empirically show that applying CPR on top of unCLIP improves quality and text-to-image alignment of the generated results (81.4 to 83.17 on TIFA benchmark), while enabling credit attribution, copy-right protection, and deterministic, constant time, unlearning.</li>
</ul>

<h3>Title: Reshaping Free-Text Radiology Notes Into Structured Reports With  Generative Transformers</h3>
<ul>
<li><strong>Authors: </strong>Laura Bergomi, Tommaso M. Buonocore, Paolo Antonazzo, Lorenzo Alberghi, Riccardo Bellazzi, Lorenzo Preda, Chandra Bortolotto, Enea Parimbelli</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18938">https://arxiv.org/abs/2403.18938</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18938">https://arxiv.org/pdf/2403.18938</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18938]] Reshaping Free-Text Radiology Notes Into Structured Reports With  Generative Transformers(https://arxiv.org/abs/2403.18938)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>BACKGROUND: Radiology reports are typically written in a free-text format, making clinical information difficult to extract and use. Recently the adoption of structured reporting (SR) has been recommended by various medical societies thanks to the advantages it offers, e.g. standardization, completeness and information retrieval. We propose a pipeline to extract information from free-text radiology reports, that fits with the items of the reference SR registry proposed by a national society of interventional and medical radiology, focusing on CT staging of patients with lymphoma. METHODS: Our work aims to leverage the potential of Natural Language Processing (NLP) and Transformer-based models to deal with automatic SR registry filling. With the availability of 174 radiology reports, we investigate a rule-free generative Question Answering approach based on a domain-specific version of T5 (IT5). Two strategies (batch-truncation and ex-post combination) are implemented to comply with the model's context length limitations. Performance is evaluated in terms of strict accuracy, F1, and format accuracy, and compared with the widely used GPT-3.5 Large Language Model. A 5-point Likert scale questionnaire is used to collect human-expert feedback on the similarity between medical annotations and generated answers. RESULTS: The combination of fine-tuning and batch splitting allows IT5 to achieve notable results; it performs on par with GPT-3.5 albeit its size being a thousand times smaller in terms of parameters. Human-based assessment scores show a high correlation (Spearman's correlation coefficients>0.88, p-values<0.001) with AI performance metrics (F1) and confirm the superior ability of LLMs (i.e., GPT-3.5, 175B of parameters) in generating plausible human-like statements.</li>
</ul>

<h3>Title: Self-Supervised Interpretable Sensorimotor Learning via Latent  Functional Modularity</h3>
<ul>
<li><strong>Authors: </strong>Hyunki Seong, David Hyunchul Shim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18947">https://arxiv.org/abs/2403.18947</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18947">https://arxiv.org/pdf/2403.18947</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18947]] Self-Supervised Interpretable Sensorimotor Learning via Latent  Functional Modularity(https://arxiv.org/abs/2403.18947)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>We introduce MoNet, a novel method that combines end-to-end learning with modular network architectures for self-supervised and interpretable sensorimotor learning. MoNet is composed of three functionally distinct neural modules: Perception, Planning, and Control. Leveraging its inherent modularity through a cognition-guided contrastive loss function, MoNet efficiently learns task-specific decision-making processes in latent space, without requiring task-level supervision. Moreover, our method incorporates an online post-hoc explainability approach, which enhances the interpretability of the end-to-end inferences without a trade-off in sensorimotor performance. In real-world indoor environments, MoNet demonstrates effective visual autonomous navigation, surpassing baseline models by 11% to 47% in task specificity analysis. We further delve into the interpretability of our network through the post-hoc analysis of perceptual saliency maps and latent decision vectors. This offers insights into the incorporation of explainable artificial intelligence within the realm of robotic learning, encompassing both perceptual and behavioral perspectives.</li>
</ul>

<h3>Title: TextCraftor: Your Text Encoder Can be Image Quality Controller</h3>
<ul>
<li><strong>Authors: </strong>Yanyu Li, Xian Liu, Anil Kag, Ju Hu, Yerlan Idelbayev, Dhritiman Sagar, Yanzhi Wang, Sergey Tulyakov, Jian Ren</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18978">https://arxiv.org/abs/2403.18978</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18978">https://arxiv.org/pdf/2403.18978</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18978]] TextCraftor: Your Text Encoder Can be Image Quality Controller(https://arxiv.org/abs/2403.18978)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion-based text-to-image generative models, e.g., Stable Diffusion, have revolutionized the field of content generation, enabling significant advancements in areas like image editing and video synthesis. Despite their formidable capabilities, these models are not without their limitations. It is still challenging to synthesize an image that aligns well with the input text, and multiple runs with carefully crafted prompts are required to achieve satisfactory results. To mitigate these limitations, numerous studies have endeavored to fine-tune the pre-trained diffusion models, i.e., UNet, utilizing various technologies. Yet, amidst these efforts, a pivotal question of text-to-image diffusion model training has remained largely unexplored: Is it possible and feasible to fine-tune the text encoder to improve the performance of text-to-image diffusion models? Our findings reveal that, instead of replacing the CLIP text encoder used in Stable Diffusion with other large language models, we can enhance it through our proposed fine-tuning approach, TextCraftor, leading to substantial improvements in quantitative benchmarks and human assessments. Interestingly, our technique also empowers controllable image generation through the interpolation of different text encoders fine-tuned with various rewards. We also demonstrate that TextCraftor is orthogonal to UNet finetuning, and can be combined to further improve generative quality.</li>
</ul>

<h3>Title: Dealing with Imbalanced Classes in Bot-IoT Dataset</h3>
<ul>
<li><strong>Authors: </strong>Jesse Atuhurra, Takanori Hara, Yuanyu Zhang, Masahiro Sasabe, Shoji Kasahara</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.18989">https://arxiv.org/abs/2403.18989</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.18989">https://arxiv.org/pdf/2403.18989</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.18989]] Dealing with Imbalanced Classes in Bot-IoT Dataset(https://arxiv.org/abs/2403.18989)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>With the rapidly spreading usage of Internet of Things (IoT) devices, a network intrusion detection system (NIDS) plays an important role in detecting and protecting various types of attacks in the IoT network. To evaluate the robustness of the NIDS in the IoT network, the existing work proposed a realistic botnet dataset in the IoT network (Bot-IoT dataset) and applied it to machine learning-based anomaly detection. This dataset contains imbalanced normal and attack packets because the number of normal packets is much smaller than that of attack ones. The nature of imbalanced data may make it difficult to identify the minority class correctly. In this thesis, to address the class imbalance problem in the Bot-IoT dataset, we propose a binary classification method with synthetic minority over-sampling techniques (SMOTE). The proposed classifier aims to detect attack packets and overcome the class imbalance problem using the SMOTE algorithm. Through numerical results, we demonstrate the proposed classifier's fundamental characteristics and the impact of imbalanced data on its performance.</li>
</ul>

<h3>Title: Cross--domain Fiber Cluster Shape Analysis for Language Performance  Cognitive Score Prediction</h3>
<ul>
<li><strong>Authors: </strong>Yui Lo, Yuqian Chen, Dongnan Liu, Wan Liu, Leo Zekelman, Fan Zhang, Yogesh Rathi, Nikos Makris, Alexandra J. Golby, Weidong Cai, Lauren J. O'Donnell</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, eess.IV, q-bio.NC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19001">https://arxiv.org/abs/2403.19001</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19001">https://arxiv.org/pdf/2403.19001</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19001]] Cross--domain Fiber Cluster Shape Analysis for Language Performance  Cognitive Score Prediction(https://arxiv.org/abs/2403.19001)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Shape plays an important role in computer graphics, offering informative features to convey an object's morphology and functionality. Shape analysis in brain imaging can help interpret structural and functionality correlations of the human brain. In this work, we investigate the shape of the brain's 3D white matter connections and its potential predictive relationship to human cognitive function. We reconstruct brain connections as sequences of 3D points using diffusion magnetic resonance imaging (dMRI) tractography. To describe each connection, we extract 12 shape descriptors in addition to traditional dMRI connectivity and tissue microstructure features. We introduce a novel framework, Shape--fused Fiber Cluster Transformer (SFFormer), that leverages a multi-head cross-attention feature fusion module to predict subject-specific language performance based on dMRI tractography. We assess the performance of the method on a large dataset including 1065 healthy young adults. The results demonstrate that both the transformer-based SFFormer model and its inter/intra feature fusion with shape, microstructure, and connectivity are informative, and together, they improve the prediction of subject-specific language performance scores. Overall, our results indicate that the shape of the brain's connections is predictive of human language function.</li>
</ul>

<h3>Title: Egocentric Scene-aware Human Trajectory Prediction</h3>
<ul>
<li><strong>Authors: </strong>Weizhuo Wang, C. Karen Liu, Monroe Kennedy III</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19026">https://arxiv.org/abs/2403.19026</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19026">https://arxiv.org/pdf/2403.19026</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19026]] Egocentric Scene-aware Human Trajectory Prediction(https://arxiv.org/abs/2403.19026)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Wearable collaborative robots stand to assist human wearers who need fall prevention assistance or wear exoskeletons. Such a robot needs to be able to predict the ego motion of the wearer based on egocentric vision and the surrounding scene. In this work, we leveraged body-mounted cameras and sensors to anticipate the trajectory of human wearers through complex surroundings. To facilitate research in ego-motion prediction, we have collected a comprehensive walking scene navigation dataset centered on the user's perspective. We present a method to predict human motion conditioning on the surrounding static scene. Our method leverages a diffusion model to produce a distribution of potential future trajectories, taking into account the user's observation of the environment. We introduce a compact representation to encode the user's visual memory of the surroundings, as well as an efficient sample-generating technique to speed up real-time inference of a diffusion model. We ablate our model and compare it to baselines, and results show that our model outperforms existing methods on key metrics of collision avoidance and trajectory mode coverage.</li>
</ul>

<h3>Title: Detecting Generative Parroting through Overfitting Masked Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Saeid Asgari Taghanaki, Joseph Lambourne</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19050">https://arxiv.org/abs/2403.19050</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19050">https://arxiv.org/pdf/2403.19050</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19050]] Detecting Generative Parroting through Overfitting Masked Autoencoders(https://arxiv.org/abs/2403.19050)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The advent of generative AI models has revolutionized digital content creation, yet it introduces challenges in maintaining copyright integrity due to generative parroting, where models mimic their training data too closely. Our research presents a novel approach to tackle this issue by employing an overfitted Masked Autoencoder (MAE) to detect such parroted samples effectively. We establish a detection threshold based on the mean loss across the training dataset, allowing for the precise identification of parroted content in modified datasets. Preliminary evaluations demonstrate promising results, suggesting our method's potential to ensure ethical use and enhance the legal compliance of generative models.</li>
</ul>

<h3>Title: Generative Quanta Color Imaging</h3>
<ul>
<li><strong>Authors: </strong>Vishal Purohit, Junjie Luo, Yiheng Chi, Qi Guo, Stanley H. Chan, Qiang Qiu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19066">https://arxiv.org/abs/2403.19066</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19066">https://arxiv.org/pdf/2403.19066</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19066]] Generative Quanta Color Imaging(https://arxiv.org/abs/2403.19066)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The astonishing development of single-photon cameras has created an unprecedented opportunity for scientific and industrial imaging. However, the high data throughput generated by these 1-bit sensors creates a significant bottleneck for low-power applications. In this paper, we explore the possibility of generating a color image from a single binary frame of a single-photon camera. We evidently find this problem being particularly difficult to standard colorization approaches due to the substantial degree of exposure variation. The core innovation of our paper is an exposure synthesis model framed under a neural ordinary differential equation (Neural ODE) that allows us to generate a continuum of exposures from a single observation. This innovation ensures consistent exposure in binary images that colorizers take on, resulting in notably enhanced colorization. We demonstrate applications of the method in single-image and burst colorization and show superior generative performance over baselines. Project website can be found at https://vishal-s-p.github.io/projects/2023/generative_quanta_color.html.</li>
</ul>

<h3>Title: MVEB: Self-Supervised Learning with Multi-View Entropy Bottleneck</h3>
<ul>
<li><strong>Authors: </strong>Liangjian Wen, Xiasi Wang, Jianzhuang Liu, Zenglin Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19078">https://arxiv.org/abs/2403.19078</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19078">https://arxiv.org/pdf/2403.19078</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19078]] MVEB: Self-Supervised Learning with Multi-View Entropy Bottleneck(https://arxiv.org/abs/2403.19078)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-supervised learning aims to learn representation that can be effectively generalized to downstream tasks. Many self-supervised approaches regard two views of an image as both the input and the self-supervised signals, assuming that either view contains the same task-relevant information and the shared information is (approximately) sufficient for predicting downstream tasks. Recent studies show that discarding superfluous information not shared between the views can improve generalization. Hence, the ideal representation is sufficient for downstream tasks and contains minimal superfluous information, termed minimal sufficient representation. One can learn this representation by maximizing the mutual information between the representation and the supervised view while eliminating superfluous information. Nevertheless, the computation of mutual information is notoriously intractable. In this work, we propose an objective termed multi-view entropy bottleneck (MVEB) to learn minimal sufficient representation effectively. MVEB simplifies the minimal sufficient learning to maximizing both the agreement between the embeddings of two views and the differential entropy of the embedding distribution. Our experiments confirm that MVEB significantly improves performance. For example, it achieves top-1 accuracy of 76.9\% on ImageNet with a vanilla ResNet-50 backbone on linear evaluation. To the best of our knowledge, this is the new state-of-the-art result with ResNet-50.</li>
</ul>

<h3>Title: Automated Black-box Prompt Engineering for Personalized Text-to-Image  Generation</h3>
<ul>
<li><strong>Authors: </strong>Yutong He, Alexander Robey, Naoki Murata, Yiding Jiang, Joshua Williams, George J. Pappas, Hamed Hassani, Yuki Mitsufuji, Ruslan Salakhutdinov, J. Zico Kolter</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19103">https://arxiv.org/abs/2403.19103</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19103">https://arxiv.org/pdf/2403.19103</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19103]] Automated Black-box Prompt Engineering for Personalized Text-to-Image  Generation(https://arxiv.org/abs/2403.19103)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, in-context</a></li>
<li><strong>Abstract: </strong>Prompt engineering is effective for controlling the output of text-to-image (T2I) generative models, but it is also laborious due to the need for manually crafted prompts. This challenge has spurred the development of algorithms for automated prompt generation. However, these methods often struggle with transferability across T2I models, require white-box access to the underlying model, and produce non-intuitive prompts. In this work, we introduce PRISM, an algorithm that automatically identifies human-interpretable and transferable prompts that can effectively generate desired concepts given only black-box access to T2I models. Inspired by large language model (LLM) jailbreaking, PRISM leverages the in-context learning ability of LLMs to iteratively refine the candidate prompts distribution for given reference images. Our experiments demonstrate the versatility and effectiveness of PRISM in generating accurate prompts for objects, styles and images across multiple T2I models, including Stable Diffusion, DALL-E, and Midjourney.</li>
</ul>

<h3>Title: Synthetic Medical Imaging Generation with Generative Adversarial  Networks For Plain Radiographs</h3>
<ul>
<li><strong>Authors: </strong>John R. McNulty, Lee Kho, Alexandria L. Case, Charlie Fornaca, Drew Johnston, David Slater, Joshua M. Abzug, Sybil A. Russell</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19107">https://arxiv.org/abs/2403.19107</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19107">https://arxiv.org/pdf/2403.19107</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19107]] Synthetic Medical Imaging Generation with Generative Adversarial  Networks For Plain Radiographs(https://arxiv.org/abs/2403.19107)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In medical imaging, access to data is commonly limited due to patient privacy restrictions and the issue that it can be difficult to acquire enough data in the case of rare diseases.[1] The purpose of this investigation was to develop a reusable open-source synthetic image generation pipeline, the GAN Image Synthesis Tool (GIST), that is easy to use as well as easy to deploy. The pipeline helps to improve and standardize AI algorithms in the digital health space by generating high quality synthetic image data that is not linked to specific patients. Its image generation capabilities include the ability to generate imaging of pathologies or injuries with low incidence rates. This improvement of digital health AI algorithms could improve diagnostic accuracy, aid in patient care, decrease medicolegal claims, and ultimately decrease the overall cost of healthcare. The pipeline builds on existing Generative Adversarial Networks (GANs) algorithms, and preprocessing and evaluation steps were included for completeness. For this work, we focused on ensuring the pipeline supports radiography, with a focus on synthetic knee and elbow x-ray images. In designing the pipeline, we evaluated the performance of current GAN architectures, studying the performance on available x-ray data. We show that the pipeline is capable of generating high quality and clinically relevant images based on a lay person's evaluation and the Fr\'echet Inception Distance (FID) metric.</li>
</ul>

<h3>Title: Patch Spatio-Temporal Relation Prediction for Video Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Hao Shen, Lu Shi, Wanru Xu, Yigang Cen, Linna Zhang, Gaoyun An</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19111">https://arxiv.org/abs/2403.19111</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19111">https://arxiv.org/pdf/2403.19111</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19111]] Patch Spatio-Temporal Relation Prediction for Video Anomaly Detection(https://arxiv.org/abs/2403.19111)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, anomaly</a></li>
<li><strong>Abstract: </strong>Video Anomaly Detection (VAD), aiming to identify abnormalities within a specific context and timeframe, is crucial for intelligent Video Surveillance Systems. While recent deep learning-based VAD models have shown promising results by generating high-resolution frames, they often lack competence in preserving detailed spatial and temporal coherence in video frames. To tackle this issue, we propose a self-supervised learning approach for VAD through an inter-patch relationship prediction task. Specifically, we introduce a two-branch vision transformer network designed to capture deep visual features of video frames, addressing spatial and temporal dimensions responsible for modeling appearance and motion patterns, respectively. The inter-patch relationship in each dimension is decoupled into inter-patch similarity and the order information of each patch. To mitigate memory consumption, we convert the order information prediction task into a multi-label learning problem, and the inter-patch similarity prediction task into a distance matrix regression problem. Comprehensive experiments demonstrate the effectiveness of our method, surpassing pixel-generation-based methods by a significant margin across three public benchmarks. Additionally, our approach outperforms other self-supervised learning-based methods.</li>
</ul>

<h3>Title: PoCo: A Self-Supervised Approach via Polar Transformation Based  Progressive Contrastive Learning for Ophthalmic Disease Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Jinhong Wang, Tingting Chen, Jintai Chen, Yixuan Wu, Yuyang Xu, Danny Chen, Haochao Ying, Jian Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19124">https://arxiv.org/abs/2403.19124</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19124">https://arxiv.org/pdf/2403.19124</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19124]] PoCo: A Self-Supervised Approach via Polar Transformation Based  Progressive Contrastive Learning for Ophthalmic Disease Diagnosis(https://arxiv.org/abs/2403.19124)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Automatic ophthalmic disease diagnosis on fundus images is important in clinical practice. However, due to complex fundus textures and limited annotated data, developing an effective automatic method for this problem is still challenging. In this paper, we present a self-supervised method via polar transformation based progressive contrastive learning, called PoCo, for ophthalmic disease diagnosis. Specifically, we novelly inject the polar transformation into contrastive learning to 1) promote contrastive learning pre-training to be faster and more stable and 2) naturally capture task-free and rotation-related textures, which provides insights into disease recognition on fundus images. Beneficially, simple normal translation-invariant convolution on transformed images can equivalently replace the complex rotation-invariant and sector convolution on raw images. After that, we develop a progressive contrastive learning method to efficiently utilize large unannotated images and a novel progressive hard negative sampling scheme to gradually reduce the negative sample number for efficient training and performance enhancement. Extensive experiments on three public ophthalmic disease datasets show that our PoCo achieves state-of-the-art performance with good generalization ability, validating that our method can reduce annotation efforts and provide reliable diagnosis. Codes are available at \url{https://github.com/wjh892521292/PoCo}.</li>
</ul>

<h3>Title: OmniParser: A Unified Framework for Text Spotting, Key Information  Extraction and Table Recognition</h3>
<ul>
<li><strong>Authors: </strong>Jianqiang Wan, Sibo Song, Wenwen Yu, Yuliang Liu, Wenqing Cheng, Fei Huang, Xiang Bai, Cong Yao, Zhibo Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19128">https://arxiv.org/abs/2403.19128</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19128">https://arxiv.org/pdf/2403.19128</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19128]] OmniParser: A Unified Framework for Text Spotting, Key Information  Extraction and Table Recognition(https://arxiv.org/abs/2403.19128)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recently, visually-situated text parsing (VsTP) has experienced notable advancements, driven by the increasing demand for automated document understanding and the emergence of Generative Large Language Models (LLMs) capable of processing document-based questions. Various methods have been proposed to address the challenging problem of VsTP. However, due to the diversified targets and heterogeneous schemas, previous works usually design task-specific architectures and objectives for individual tasks, which inadvertently leads to modal isolation and complex workflow. In this paper, we propose a unified paradigm for parsing visually-situated text across diverse scenarios. Specifically, we devise a universal model, called OmniParser, which can simultaneously handle three typical visually-situated text parsing tasks: text spotting, key information extraction, and table recognition. In OmniParser, all tasks share the unified encoder-decoder architecture, the unified objective: point-conditioned text generation, and the unified input & output representation: prompt & structured sequences. Extensive experiments demonstrate that the proposed OmniParser achieves state-of-the-art (SOTA) or highly competitive performances on 7 datasets for the three visually-situated text parsing tasks, despite its unified, concise design. The code is available at https://github.com/AlibabaResearch/AdvancedLiterateMachinery.</li>
</ul>

<h3>Title: QNCD: Quantization Noise Correction for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Huanpeng Chu, Wei Wu, Chengjie Zang, Kun Yuan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19140">https://arxiv.org/abs/2403.19140</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19140">https://arxiv.org/pdf/2403.19140</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19140]] QNCD: Quantization Noise Correction for Diffusion Models(https://arxiv.org/abs/2403.19140)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have revolutionized image synthesis, setting new benchmarks in quality and creativity. However, their widespread adoption is hindered by the intensive computation required during the iterative denoising process. Post-training quantization (PTQ) presents a solution to accelerate sampling, aibeit at the expense of sample quality, extremely in low-bit settings. Addressing this, our study introduces a unified Quantization Noise Correction Scheme (QNCD), aimed at minishing quantization noise throughout the sampling process. We identify two primary quantization challenges: intra and inter quantization noise. Intra quantization noise, mainly exacerbated by embeddings in the resblock module, extends activation quantization ranges, increasing disturbances in each single denosing step. Besides, inter quantization noise stems from cumulative quantization deviations across the entire denoising process, altering data distributions step-by-step. QNCD combats these through embedding-derived feature smoothing for eliminating intra quantization noise and an effective runtime noise estimatiation module for dynamicly filtering inter quantization noise. Extensive experiments demonstrate that our method outperforms previous quantization methods for diffusion models, achieving lossless results in W4A8 and W8A8 quantization settings on ImageNet (LDM-4). Code is available at: https://github.com/huanpengchu/QNCD</li>
</ul>

<h3>Title: MoDiTalker: Motion-Disentangled Diffusion Model for High-Fidelity  Talking Head Generation</h3>
<ul>
<li><strong>Authors: </strong>Seyeon Kim, Siyoon Jin, Jihye Park, Kihong Kim, Jiyoung Kim, Jisu Nam, Seungryong Kim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19144">https://arxiv.org/abs/2403.19144</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19144">https://arxiv.org/pdf/2403.19144</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19144]] MoDiTalker: Motion-Disentangled Diffusion Model for High-Fidelity  Talking Head Generation(https://arxiv.org/abs/2403.19144)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Conventional GAN-based models for talking head generation often suffer from limited quality and unstable training. Recent approaches based on diffusion models aimed to address these limitations and improve fidelity. However, they still face challenges, including extensive sampling times and difficulties in maintaining temporal consistency due to the high stochasticity of diffusion models. To overcome these challenges, we propose a novel motion-disentangled diffusion model for high-quality talking head generation, dubbed MoDiTalker. We introduce the two modules: audio-to-motion (AToM), designed to generate a synchronized lip motion from audio, and motion-to-video (MToV), designed to produce high-quality head video following the generated motion. AToM excels in capturing subtle lip movements by leveraging an audio attention mechanism. In addition, MToV enhances temporal consistency by leveraging an efficient tri-plane representation. Our experiments conducted on standard benchmarks demonstrate that our model achieves superior performance compared to existing models. We also provide comprehensive ablation studies and user study results.</li>
</ul>

<h3>Title: RecDiffusion: Rectangling for Image Stitching with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Tianhao Zhou, Haipeng Li, Ziyi Wang, Ao Luo, Chen-Lin Zhang, Jiajun Li, Bing Zeng, Shuaicheng Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19164">https://arxiv.org/abs/2403.19164</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19164">https://arxiv.org/pdf/2403.19164</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19164]] RecDiffusion: Rectangling for Image Stitching with Diffusion Models(https://arxiv.org/abs/2403.19164)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Image stitching from different captures often results in non-rectangular boundaries, which is often considered unappealing. To solve non-rectangular boundaries, current solutions involve cropping, which discards image content, inpainting, which can introduce unrelated content, or warping, which can distort non-linear features and introduce artifacts. To overcome these issues, we introduce a novel diffusion-based learning framework, \textbf{RecDiffusion}, for image stitching rectangling. This framework combines Motion Diffusion Models (MDM) to generate motion fields, effectively transitioning from the stitched image's irregular borders to a geometrically corrected intermediary. Followed by Content Diffusion Models (CDM) for image detail refinement. Notably, our sampling process utilizes a weighted map to identify regions needing correction during each iteration of CDM. Our RecDiffusion ensures geometric accuracy and overall visual appeal, surpassing all previous methods in both quantitative and qualitative measures when evaluated on public benchmarks. Code is released at https://github.com/lhaippp/RecDiffusion.</li>
</ul>

<h3>Title: Text Data-Centric Image Captioning with Interactive Prompts</h3>
<ul>
<li><strong>Authors: </strong>Yiyu Wang, Hao Luo, Jungang Xu, Yingfei Sun, Fan Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19193">https://arxiv.org/abs/2403.19193</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19193">https://arxiv.org/pdf/2403.19193</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19193]] Text Data-Centric Image Captioning with Interactive Prompts(https://arxiv.org/abs/2403.19193)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Supervised image captioning approaches have made great progress, but it is challenging to collect high-quality human-annotated image-text data. Recently, large-scale vision and language models (e.g., CLIP) and large-scale generative language models (e.g., GPT-2) have shown strong performances in various tasks, which also provide some new solutions for image captioning with web paired data, unpaired data or even text-only data. Among them, the mainstream solution is to project image embeddings into the text embedding space with the assistance of consistent representations between image-text pairs from the CLIP model. However, the current methods still face several challenges in adapting to the diversity of data configurations in a unified solution, accurately estimating image-text embedding bias, and correcting unsatisfactory prediction results in the inference stage. This paper proposes a new Text data-centric approach with Interactive Prompts for image Captioning, named TIPCap. 1) We consider four different settings which gradually reduce the dependence on paired data. 2) We construct a mapping module driven by multivariate Gaussian distribution to mitigate the modality gap, which is applicable to the above four different settings. 3) We propose a prompt interaction module that can incorporate optional prompt information before generating captions. Extensive experiments show that our TIPCap outperforms other weakly or unsupervised image captioning methods and achieves a new state-of-the-art performance on two widely used datasets, i.e., MS-COCO and Flickr30K.</li>
</ul>

<h3>Title: Dual-Personalizing Adapter for Federated Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Yiyuan Yang, Guodong Long, Tao Shen, Jing Jiang, Michael Blumenstein</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19211">https://arxiv.org/abs/2403.19211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19211">https://arxiv.org/pdf/2403.19211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19211]] Dual-Personalizing Adapter for Federated Foundation Models(https://arxiv.org/abs/2403.19211)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Recently, foundation models, particularly large language models (LLMs), have demonstrated an impressive ability to adapt to various tasks by fine-tuning large amounts of instruction data. Notably, federated foundation models emerge as a privacy preservation method to fine-tune models collaboratively under federated learning (FL) settings by leveraging many distributed datasets with non-IID data. To alleviate communication and computation overhead, parameter-efficient methods are introduced for efficiency, and some research adapted personalization methods to federated foundation models for better user preferences alignment. However, a critical gap in existing research is the neglect of test-time distribution shifts in real-world applications. Therefore, to bridge this gap, we propose a new setting, termed test-time personalization, which not only concentrates on the targeted local task but also extends to other tasks that exhibit test-time distribution shifts. To address challenges in this new setting, we explore a simple yet effective solution to learn a comprehensive foundation model. Specifically, a dual-personalizing adapter architecture (FedDPA) is proposed, comprising a global adapter and a local adapter for addressing test-time distribution shifts and personalization, respectively. Additionally, we introduce an instance-wise dynamic weighting mechanism to optimize the balance between the global and local adapters, enhancing overall performance. The effectiveness of the proposed method has been evaluated on benchmark datasets across different NLP tasks.</li>
</ul>

<h3>Title: DreamSalon: A Staged Diffusion Framework for Preserving Identity-Context  in Editable Face Generation</h3>
<ul>
<li><strong>Authors: </strong>Haonan Lin, Mengmeng Wang, Yan Chen, Wenbin An, Yuzhe Yao, Guang Dai, Qianying Wang, Yong Liu, Jingdong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19235">https://arxiv.org/abs/2403.19235</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19235">https://arxiv.org/pdf/2403.19235</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19235]] DreamSalon: A Staged Diffusion Framework for Preserving Identity-Context  in Editable Face Generation(https://arxiv.org/abs/2403.19235)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>While large-scale pre-trained text-to-image models can synthesize diverse and high-quality human-centered images, novel challenges arise with a nuanced task of "identity fine editing": precisely modifying specific features of a subject while maintaining its inherent identity and context. Existing personalization methods either require time-consuming optimization or learning additional encoders, adept in "identity re-contextualization". However, they often struggle with detailed and sensitive tasks like human face editing. To address these challenges, we introduce DreamSalon, a noise-guided, staged-editing framework, uniquely focusing on detailed image manipulations and identity-context preservation. By discerning editing and boosting stages via the frequency and gradient of predicted noises, DreamSalon first performs detailed manipulations on specific features in the editing stage, guided by high-frequency information, and then employs stochastic denoising in the boosting stage to improve image quality. For more precise editing, DreamSalon semantically mixes source and target textual prompts, guided by differences in their embedding covariances, to direct the model's focus on specific manipulation areas. Our experiments demonstrate DreamSalon's ability to efficiently and faithfully edit fine details on human faces, outperforming existing methods both qualitatively and quantitatively.</li>
</ul>

<h3>Title: Genos: General In-Network Unsupervised Intrusion Detection by Rule  Extraction</h3>
<ul>
<li><strong>Authors: </strong>Ruoyu Li, Qing Li, Yu Zhang, Dan Zhao, Xi Xiao, Yong Jiang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19248">https://arxiv.org/abs/2403.19248</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19248">https://arxiv.org/pdf/2403.19248</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19248]] Genos: General In-Network Unsupervised Intrusion Detection by Rule  Extraction(https://arxiv.org/abs/2403.19248)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Anomaly-based network intrusion detection systems (A-NIDS) use unsupervised models to detect unforeseen attacks. However, existing A-NIDS solutions suffer from low throughput, lack of interpretability, and high maintenance costs. Recent in-network intelligence (INI) exploits programmable switches to offer line-rate deployment of NIDS. Nevertheless, current in-network NIDS are either model-specific or only apply to supervised models. In this paper, we propose Genos, a general in-network framework for unsupervised A-NIDS by rule extraction, which consists of a Model Compiler, a Model Interpreter, and a Model Debugger. Specifically, observing benign data are multimodal and usually located in multiple subspaces in the feature space, we utilize a divide-and-conquer approach for model-agnostic rule extraction. In the Model Compiler, we first propose a tree-based clustering algorithm to partition the feature space into subspaces, then design a decision boundary estimation mechanism to approximate the source model in each subspace. The Model Interpreter interprets predictions by important attributes to aid network operators in understanding the predictions. The Model Debugger conducts incremental updating to rectify errors by only fine-tuning rules on affected subspaces, thus reducing maintenance costs. We implement a prototype using physical hardware, and experiments demonstrate its superior performance of 100 Gbps throughput, great interpretability, and trivial updating overhead.</li>
</ul>

<h3>Title: Imperceptible Protection against Style Imitation from Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Namhyuk Ahn, Wonhyuk Ahn, KiYoon Yoo, Daesik Kim, Seung-Hun Nam</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19254">https://arxiv.org/abs/2403.19254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19254">https://arxiv.org/pdf/2403.19254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19254]] Imperceptible Protection against Style Imitation from Diffusion Models(https://arxiv.org/abs/2403.19254)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent progress in diffusion models has profoundly enhanced the fidelity of image generation. However, this has raised concerns about copyright infringements. While prior methods have introduced adversarial perturbations to prevent style imitation, most are accompanied by the degradation of artworks' visual quality. Recognizing the importance of maintaining this, we develop a visually improved protection method that preserves its protection capability. To this end, we create a perceptual map to identify areas most sensitive to human eyes. We then adjust the protection intensity guided by an instance-aware refinement. We also integrate a perceptual constraints bank to further improve the imperceptibility. Results show that our method substantially elevates the quality of the protected image without compromising on protection efficacy.</li>
</ul>

<h3>Title: Ungrammatical-syntax-based In-context Example Selection for Grammatical  Error Correction</h3>
<ul>
<li><strong>Authors: </strong>Chenming Tang, Fanyi Qu, Yunfang Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19283">https://arxiv.org/abs/2403.19283</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19283">https://arxiv.org/pdf/2403.19283</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19283]] Ungrammatical-syntax-based In-context Example Selection for Grammatical  Error Correction(https://arxiv.org/abs/2403.19283)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>In the era of large language models (LLMs), in-context learning (ICL) stands out as an effective prompting strategy that explores LLMs' potency across various tasks. However, applying LLMs to grammatical error correction (GEC) is still a challenging task. In this paper, we propose a novel ungrammatical-syntax-based in-context example selection strategy for GEC. Specifically, we measure similarity of sentences based on their syntactic structures with diverse algorithms, and identify optimal ICL examples sharing the most similar ill-formed syntax to the test input. Additionally, we carry out a two-stage process to further improve the quality of selection results. On benchmark English GEC datasets, empirical results show that our proposed ungrammatical-syntax-based strategies outperform commonly-used word-matching or semantics-based methods with multiple LLMs. This indicates that for a syntax-oriented task like GEC, paying more attention to syntactic information can effectively boost LLMs' performance. Our code will be publicly available after the publication of this paper.</li>
</ul>

<h3>Title: Going Beyond Word Matching: Syntax Improves In-context Example Selection  for Machine Translation</h3>
<ul>
<li><strong>Authors: </strong>Chenming Tang, Zhixiang Wang, Yunfang Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19285">https://arxiv.org/abs/2403.19285</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19285">https://arxiv.org/pdf/2403.19285</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19285]] Going Beyond Word Matching: Syntax Improves In-context Example Selection  for Machine Translation(https://arxiv.org/abs/2403.19285)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) is the trending prompting strategy in the era of large language models (LLMs), where a few examples are demonstrated to evoke LLMs' power for a given task. How to select informative examples remains an open issue. Previous works on in-context example selection for machine translation (MT) focus on superficial word-level features while ignoring deep syntax-level knowledge. In this paper, we propose a syntax-based in-context example selection method for MT, by computing the syntactic similarity between dependency trees using Polynomial Distance. In addition, we propose an ensemble strategy combining examples selected by both word-level and syntax-level criteria. Experimental results between English and 6 common languages indicate that syntax can effectively enhancing ICL for MT, obtaining the highest COMET scores on 11 out of 12 translation directions.</li>
</ul>

<h3>Title: FlowDepth: Decoupling Optical Flow for Self-Supervised Monocular Depth  Estimation</h3>
<ul>
<li><strong>Authors: </strong>Yiyang Sun, Zhiyuan Xu, Xiaonian Wang, Jing Yao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19294">https://arxiv.org/abs/2403.19294</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19294">https://arxiv.org/pdf/2403.19294</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19294]] FlowDepth: Decoupling Optical Flow for Self-Supervised Monocular Depth  Estimation(https://arxiv.org/abs/2403.19294)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-supervised multi-frame methods have currently achieved promising results in depth estimation. However, these methods often suffer from mismatch problems due to the moving objects, which break the static assumption. Additionally, unfairness can occur when calculating photometric errors in high-freq or low-texture regions of the images. To address these issues, existing approaches use additional semantic priori black-box networks to separate moving objects and improve the model only at the loss level. Therefore, we propose FlowDepth, where a Dynamic Motion Flow Module (DMFM) decouples the optical flow by a mechanism-based approach and warps the dynamic regions thus solving the mismatch problem. For the unfairness of photometric errors caused by high-freq and low-texture regions, we use Depth-Cue-Aware Blur (DCABlur) and Cost-Volume sparsity loss respectively at the input and the loss level to solve the problem. Experimental results on the KITTI and Cityscapes datasets show that our method outperforms the state-of-the-art methods.</li>
</ul>

<h3>Title: MATEval: A Multi-Agent Discussion Framework for Advancing Open-Ended  Text Evaluation</h3>
<ul>
<li><strong>Authors: </strong>Yu Li, Shenyu Zhang, Rui Wu, Xiutian Huang, Yongrui Chen, Wenhao Xu, Guilin Qi, Dehai Min</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19305">https://arxiv.org/abs/2403.19305</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19305">https://arxiv.org/pdf/2403.19305</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19305]] MATEval: A Multi-Agent Discussion Framework for Advancing Open-Ended  Text Evaluation(https://arxiv.org/abs/2403.19305)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in generative Large Language Models(LLMs) have been remarkable, however, the quality of the text generated by these models often reveals persistent issues. Evaluating the quality of text generated by these models, especially in open-ended text, has consistently presented a significant challenge. Addressing this, recent work has explored the possibility of using LLMs as evaluators. While using a single LLM as an evaluation agent shows potential, it is filled with significant uncertainty and instability. To address these issues, we propose the MATEval: A "Multi-Agent Text Evaluation framework" where all agents are played by LLMs like GPT-4. The MATEval framework emulates human collaborative discussion methods, integrating multiple agents' interactions to evaluate open-ended text. Our framework incorporates self-reflection and Chain-of-Thought (CoT) strategies, along with feedback mechanisms, enhancing the depth and breadth of the evaluation process and guiding discussions towards consensus, while the framework generates comprehensive evaluation reports, including error localization, error types and scoring. Experimental results show that our framework outperforms existing open-ended text evaluation methods and achieves the highest correlation with human evaluation, which confirms the effectiveness and advancement of our framework in addressing the uncertainties and instabilities in evaluating LLMs-generated text. Furthermore, our framework significantly improves the efficiency of text evaluation and model iteration in industrial scenarios.</li>
</ul>

<h3>Title: Mesh2NeRF: Direct Mesh Supervision for Neural Radiance Field  Representation and Generation</h3>
<ul>
<li><strong>Authors: </strong>Yujin Chen, Yinyu Nie, Benjamin Ummenhofer, Reiner Birkl, Michael Paulitsch, Matthias Mller, Matthias Niener</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19319">https://arxiv.org/abs/2403.19319</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19319">https://arxiv.org/pdf/2403.19319</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19319]] Mesh2NeRF: Direct Mesh Supervision for Neural Radiance Field  Representation and Generation(https://arxiv.org/abs/2403.19319)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present Mesh2NeRF, an approach to derive ground-truth radiance fields from textured meshes for 3D generation tasks. Many 3D generative approaches represent 3D scenes as radiance fields for training. Their ground-truth radiance fields are usually fitted from multi-view renderings from a large-scale synthetic 3D dataset, which often results in artifacts due to occlusions or under-fitting issues. In Mesh2NeRF, we propose an analytic solution to directly obtain ground-truth radiance fields from 3D meshes, characterizing the density field with an occupancy function featuring a defined surface thickness, and determining view-dependent color through a reflection function considering both the mesh and environment lighting. Mesh2NeRF extracts accurate radiance fields which provides direct supervision for training generative NeRFs and single scene representation. We validate the effectiveness of Mesh2NeRF across various tasks, achieving a noteworthy 3.12dB improvement in PSNR for view synthesis in single scene representation on the ABO dataset, a 0.69 PSNR enhancement in the single-view conditional generation of ShapeNet Cars, and notably improved mesh extraction from NeRF in the unconditional generation of Objaverse Mugs.</li>
</ul>

<h3>Title: Burst Super-Resolution with Diffusion Models for Improving Perceptual  Quality</h3>
<ul>
<li><strong>Authors: </strong>Kyotaro Tokoro, Kazutoshi Akita, Norimichi Ukita</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19428">https://arxiv.org/abs/2403.19428</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19428">https://arxiv.org/pdf/2403.19428</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19428]] Burst Super-Resolution with Diffusion Models for Improving Perceptual  Quality(https://arxiv.org/abs/2403.19428)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>While burst LR images are useful for improving the SR image quality compared with a single LR image, prior SR networks accepting the burst LR images are trained in a deterministic manner, which is known to produce a blurry SR image. In addition, it is difficult to perfectly align the burst LR images, making the SR image more blurry. Since such blurry images are perceptually degraded, we aim to reconstruct the sharp high-fidelity boundaries. Such high-fidelity images can be reconstructed by diffusion models. However, prior SR methods using the diffusion model are not properly optimized for the burst SR task. Specifically, the reverse process starting from a random sample is not optimized for image enhancement and restoration methods, including burst SR. In our proposed method, on the other hand, burst LR features are used to reconstruct the initial burst SR image that is fed into an intermediate step in the diffusion model. This reverse process from the intermediate step 1) skips diffusion steps for reconstructing the global structure of the image and 2) focuses on steps for refining detailed textures. Our experimental results demonstrate that our method can improve the scores of the perceptual quality metrics. Code: https://github.com/placerkyo/BSRD</li>
</ul>

<h3>Title: BAMM: Bidirectional Autoregressive Motion Model</h3>
<ul>
<li><strong>Authors: </strong>Ekkasit Pinyoanuntapong, Muhammad Usama Saleem, Pu Wang, Minwoo Lee, Srijan Das, Chen Chen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19435">https://arxiv.org/abs/2403.19435</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19435">https://arxiv.org/pdf/2403.19435</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19435]] BAMM: Bidirectional Autoregressive Motion Model(https://arxiv.org/abs/2403.19435)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generating human motion from text has been dominated by denoising motion models either through diffusion or generative masking process. However, these models face great limitations in usability by requiring prior knowledge of the motion length. Conversely, autoregressive motion models address this limitation by adaptively predicting motion endpoints, at the cost of degraded generation quality and editing capabilities. To address these challenges, we propose Bidirectional Autoregressive Motion Model (BAMM), a novel text-to-motion generation framework. BAMM consists of two key components: (1) a motion tokenizer that transforms 3D human motion into discrete tokens in latent space, and (2) a masked self-attention transformer that autoregressively predicts randomly masked tokens via a hybrid attention masking strategy. By unifying generative masked modeling and autoregressive modeling, BAMM captures rich and bidirectional dependencies among motion tokens, while learning the probabilistic mapping from textual inputs to motion outputs with dynamically-adjusted motion sequence length. This feature enables BAMM to simultaneously achieving high-quality motion generation with enhanced usability and built-in motion editability. Extensive experiments on HumanML3D and KIT-ML datasets demonstrate that BAMM surpasses current state-of-the-art methods in both qualitative and quantitative measures.</li>
</ul>

<h3>Title: SubjectDrive: Scaling Generative Data in Autonomous Driving via Subject  Control</h3>
<ul>
<li><strong>Authors: </strong>Binyuan Huang, Yuqing Wen, Yucheng Zhao, Yaosi Hu, Yingfei Liu, Fan Jia, Weixin Mao, Tiancai Wang, Chi Zhang, Chang Wen Chen, Zhenzhong Chen, Xiangyu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19438">https://arxiv.org/abs/2403.19438</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19438">https://arxiv.org/pdf/2403.19438</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19438]] SubjectDrive: Scaling Generative Data in Autonomous Driving via Subject  Control(https://arxiv.org/abs/2403.19438)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Autonomous driving progress relies on large-scale annotated datasets. In this work, we explore the potential of generative models to produce vast quantities of freely-labeled data for autonomous driving applications and present SubjectDrive, the first model proven to scale generative data production in a way that could continuously improve autonomous driving applications. We investigate the impact of scaling up the quantity of generative data on the performance of downstream perception models and find that enhancing data diversity plays a crucial role in effectively scaling generative data production. Therefore, we have developed a novel model equipped with a subject control mechanism, which allows the generative model to leverage diverse external data sources for producing varied and useful data. Extensive evaluations confirm SubjectDrive's efficacy in generating scalable autonomous driving training data, marking a significant step toward revolutionizing data production methods in this field.</li>
</ul>

<h3>Title: JDocQA: Japanese Document Question Answering Dataset for Generative  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Eri Onami, Shuhei Kurita, Taiki Miyanishi, Taro Watanabe</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19454">https://arxiv.org/abs/2403.19454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19454">https://arxiv.org/pdf/2403.19454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19454]] JDocQA: Japanese Document Question Answering Dataset for Generative  Language Models(https://arxiv.org/abs/2403.19454)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Document question answering is a task of question answering on given documents such as reports, slides, pamphlets, and websites, and it is a truly demanding task as paper and electronic forms of documents are so common in our society. This is known as a quite challenging task because it requires not only text understanding but also understanding of figures and tables, and hence visual question answering (VQA) methods are often examined in addition to textual approaches. We introduce Japanese Document Question Answering (JDocQA), a large-scale document-based QA dataset, essentially requiring both visual and textual information to answer questions, which comprises 5,504 documents in PDF format and annotated 11,600 question-and-answer instances in Japanese. Each QA instance includes references to the document pages and bounding boxes for the answer clues. We incorporate multiple categories of questions and unanswerable questions from the document for realistic question-answering applications. We empirically evaluate the effectiveness of our dataset with text-based large language models (LLMs) and multimodal models. Incorporating unanswerable questions in finetuning may contribute to harnessing the so-called hallucination generation.</li>
</ul>

<h3>Title: Surface-based parcellation and vertex-wise analysis of ultra  high-resolution ex vivo 7 tesla MRI in neurodegenerative diseases</h3>
<ul>
<li><strong>Authors: </strong>Pulkit Khandelwal, Michael Tran Duong, Constanza Fuentes, Amanda Denning, Winifred Trotman, Ranjit Ittyerah, Alejandra Bahena, Theresa Schuck, Marianna Gabrielyan, Karthik Prabhakaran, Daniel Ohm, Gabor Mizsei, John Robinson, Monica Munoz, John Detre, Edward Lee, David Irwin, Corey McMillan, M. Dylan Tisdall, Sandhitsu Das, David Wolk, Paul A. Yushkevich</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19497">https://arxiv.org/abs/2403.19497</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19497">https://arxiv.org/pdf/2403.19497</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19497]] Surface-based parcellation and vertex-wise analysis of ultra  high-resolution ex vivo 7 tesla MRI in neurodegenerative diseases(https://arxiv.org/abs/2403.19497)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Magnetic resonance imaging (MRI) is the standard modality to understand human brain structure and function in vivo (antemortem). Decades of research in human neuroimaging has led to the widespread development of methods and tools to provide automated volume-based segmentations and surface-based parcellations which help localize brain functions to specialized anatomical regions. Recently ex vivo (postmortem) imaging of the brain has opened-up avenues to study brain structure at sub-millimeter ultra high-resolution revealing details not possible to observe with in vivo MRI. Unfortunately, there has been limited methodological development in ex vivo MRI primarily due to lack of datasets and limited centers with such imaging resources. Therefore, in this work, we present one-of-its-kind dataset of 82 ex vivo T2w whole brain hemispheres MRI at 0.3 mm isotropic resolution spanning Alzheimer's disease and related dementias. We adapted and developed a fast and easy-to-use automated surface-based pipeline to parcellate, for the first time, ultra high-resolution ex vivo brain tissue at the native subject space resolution using the Desikan-Killiany-Tourville (DKT) brain atlas. This allows us to perform vertex-wise analysis in the template space and thereby link morphometry measures with pathology measurements derived from histology. We will open-source our dataset docker container, Jupyter notebooks for ready-to-use out-of-the-box set of tools and command line options to advance ex vivo MRI clinical brain imaging research on the project webpage.</li>
</ul>

<h3>Title: Improving Clinical NLP Performance through Language Model-Generated  Synthetic Clinical Data</h3>
<ul>
<li><strong>Authors: </strong>Shan Chen, Jack Gallifant, Marco Guevara, Yanjun Gao, Majid Afshar, Timothy Miller, Dmitriy Dligach, Danielle S. Bitterman</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19511">https://arxiv.org/abs/2403.19511</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19511">https://arxiv.org/pdf/2403.19511</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19511]] Improving Clinical NLP Performance through Language Model-Generated  Synthetic Clinical Data(https://arxiv.org/abs/2403.19511)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative models have been showing potential for producing data in mass. This study explores the enhancement of clinical natural language processing performance by utilizing synthetic data generated from advanced language models. Promising results show feasible applications in such a high-stakes domain.</li>
</ul>

<h3>Title: WaterJudge: Quality-Detection Trade-off when Watermarking Large Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Piotr Molenda, Adian Liusie, Mark J. F. Gales</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19548">https://arxiv.org/abs/2403.19548</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19548">https://arxiv.org/pdf/2403.19548</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19548]] WaterJudge: Quality-Detection Trade-off when Watermarking Large Language  Models(https://arxiv.org/abs/2403.19548)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Watermarking generative-AI systems, such as LLMs, has gained considerable interest, driven by their enhanced capabilities across a wide range of tasks. Although current approaches have demonstrated that small, context-dependent shifts in the word distributions can be used to apply and detect watermarks, there has been little work in analyzing the impact that these perturbations have on the quality of generated texts. Balancing high detectability with minimal performance degradation is crucial in terms of selecting the appropriate watermarking setting; therefore this paper proposes a simple analysis framework where comparative assessment, a flexible NLG evaluation framework, is used to assess the quality degradation caused by a particular watermark setting. We demonstrate that our framework provides easy visualization of the quality-detection trade-off of watermark settings, enabling a simple solution to find an LLM watermark operating point that provides a well-balanced performance. This approach is applied to two different summarization systems and a translation system, enabling cross-model analysis for a task, and cross-task analysis.</li>
</ul>

<h3>Title: The Bad Batches: Enhancing Self-Supervised Learning in Image  Classification Through Representative Batch Curation</h3>
<ul>
<li><strong>Authors: </strong>Ozgu Goksu, Nicolas Pugeault</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19579">https://arxiv.org/abs/2403.19579</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19579">https://arxiv.org/pdf/2403.19579</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19579]] The Bad Batches: Enhancing Self-Supervised Learning in Image  Classification Through Representative Batch Curation(https://arxiv.org/abs/2403.19579)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>The pursuit of learning robust representations without human supervision is a longstanding challenge. The recent advancements in self-supervised contrastive learning approaches have demonstrated high performance across various representation learning challenges. However, current methods depend on the random transformation of training examples, resulting in some cases of unrepresentative positive pairs that can have a large impact on learning. This limitation not only impedes the convergence of the learning process but the robustness of the learnt representation as well as requiring larger batch sizes to improve robustness to such bad batches. This paper attempts to alleviate the influence of false positive and false negative pairs by employing pairwise similarity calculations through the Fr\'echet ResNet Distance (FRD), thereby obtaining robust representations from unlabelled data. The effectiveness of the proposed method is substantiated by empirical results, where a linear classifier trained on self-supervised contrastive representations achieved an impressive 87.74\% top-1 accuracy on STL10 and 99.31\% on the Flower102 dataset. These results emphasize the potential of the proposed approach in pushing the boundaries of the state-of-the-art in self-supervised contrastive learning, particularly for image classification tasks.</li>
</ul>

<h3>Title: Img2Loc: Revisiting Image Geolocalization using Multi-modality  Foundation Models and Image-based Retrieval-Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Zhongliang Zhou, Jielu Zhang, Zihan Guan, Mengxuan Hu, Ni Lao, Lan Mu, Sheng Li, Gengchen Mai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19584">https://arxiv.org/abs/2403.19584</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19584">https://arxiv.org/pdf/2403.19584</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19584]] Img2Loc: Revisiting Image Geolocalization using Multi-modality  Foundation Models and Image-based Retrieval-Augmented Generation(https://arxiv.org/abs/2403.19584)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Geolocating precise locations from images presents a challenging problem in computer vision and information retrieval.Traditional methods typically employ either classification, which dividing the Earth surface into grid cells and classifying images accordingly, or retrieval, which identifying locations by matching images with a database of image-location pairs. However, classification-based approaches are limited by the cell size and cannot yield precise predictions, while retrieval-based systems usually suffer from poor search quality and inadequate coverage of the global landscape at varied scale and aggregation levels. To overcome these drawbacks, we present Img2Loc, a novel system that redefines image geolocalization as a text generation task. This is achieved using cutting-edge large multi-modality models like GPT4V or LLaVA with retrieval augmented generation. Img2Loc first employs CLIP-based representations to generate an image-based coordinate query database. It then uniquely combines query results with images itself, forming elaborate prompts customized for LMMs. When tested on benchmark datasets such as Im2GPS3k and YFCC4k, Img2Loc not only surpasses the performance of previous state-of-the-art models but does so without any model training.</li>
</ul>

<h3>Title: Frame by Familiar Frame: Understanding Replication in Video Diffusion  Models</h3>
<ul>
<li><strong>Authors: </strong>Aimon Rahman, Malsha V. Perera, Vishal M. Patel</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19593">https://arxiv.org/abs/2403.19593</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19593">https://arxiv.org/pdf/2403.19593</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19593]] Frame by Familiar Frame: Understanding Replication in Video Diffusion  Models(https://arxiv.org/abs/2403.19593)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Building on the momentum of image generation diffusion models, there is an increasing interest in video-based diffusion models. However, video generation poses greater challenges due to its higher-dimensional nature, the scarcity of training data, and the complex spatiotemporal relationships involved. Image generation models, due to their extensive data requirements, have already strained computational resources to their limits. There have been instances of these models reproducing elements from the training samples, leading to concerns and even legal disputes over sample replication. Video diffusion models, which operate with even more constrained datasets and are tasked with generating both spatial and temporal content, may be more prone to replicating samples from their training sets. Compounding the issue, these models are often evaluated using metrics that inadvertently reward replication. In our paper, we present a systematic investigation into the phenomenon of sample replication in video diffusion models. We scrutinize various recent diffusion models for video synthesis, assessing their tendency to replicate spatial and temporal content in both unconditional and conditional generation scenarios. Our study identifies strategies that are less likely to lead to replication. Furthermore, we propose new evaluation strategies that take replication into account, offering a more accurate measure of a model's ability to generate the original content.</li>
</ul>

<h3>Title: Enhance Image Classification via Inter-Class Image Mixup with Diffusion  Model</h3>
<ul>
<li><strong>Authors: </strong>Zhicai Wang, Longhui Wei, Tan Wang, Heyu Chen, Yanbin Hao, Xiang Wang, Xiangnan He, Qi Tian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19600">https://arxiv.org/abs/2403.19600</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19600">https://arxiv.org/pdf/2403.19600</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19600]] Enhance Image Classification via Inter-Class Image Mixup with Diffusion  Model(https://arxiv.org/abs/2403.19600)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Text-to-image (T2I) generative models have recently emerged as a powerful tool, enabling the creation of photo-realistic images and giving rise to a multitude of applications. However, the effective integration of T2I models into fundamental image classification tasks remains an open question. A prevalent strategy to bolster image classification performance is through augmenting the training set with synthetic images generated by T2I models. In this study, we scrutinize the shortcomings of both current generative and conventional data augmentation techniques. Our analysis reveals that these methods struggle to produce images that are both faithful (in terms of foreground objects) and diverse (in terms of background contexts) for domain-specific concepts. To tackle this challenge, we introduce an innovative inter-class data augmentation method known as Diff-Mix (https://github.com/Zhicaiwww/Diff-Mix), which enriches the dataset by performing image translations between classes. Our empirical results demonstrate that Diff-Mix achieves a better balance between faithfulness and diversity, leading to a marked improvement in performance across diverse image classification scenarios, including few-shot, conventional, and long-tail classifications for domain-specific datasets.</li>
</ul>

<h3>Title: Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in  Language Models</h3>
<ul>
<li><strong>Authors: </strong>Yucheng Shi, Qiaoyu Tan, Xuansheng Wu, Shaochen Zhong, Kaixiong Zhou, Ninghao Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19631">https://arxiv.org/abs/2403.19631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19631">https://arxiv.org/pdf/2403.19631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19631]] Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in  Language Models(https://arxiv.org/abs/2403.19631)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown proficiency in question-answering tasks but often struggle to integrate real-time knowledge updates, leading to potentially outdated or inaccurate responses. This problem becomes even more challenging when dealing with multi-hop questions since they require LLMs to update and integrate multiple knowledge pieces relevant to the questions. To tackle the problem, we propose the Retrieval-Augmented model Editing (RAE) framework tailored for multi-hop question answering. RAE first retrieves edited facts and then refines the language model through in-context learning. Specifically, our retrieval approach, based on mutual information maximization, leverages the reasoning abilities of LLMs to identify chain facts that na\"ive similarity-based searches might miss. Additionally, our framework incorporates a pruning strategy to eliminate redundant information from the retrieved facts, which enhances the editing accuracy and mitigates the hallucination problem. Our framework is supported by theoretical justification for its fact retrieval efficacy. Finally, comprehensive evaluation across various LLMs validates RAE's ability in providing accurate answers with updated knowledge.</li>
</ul>

<h3>Title: GANTASTIC: GAN-based Transfer of Interpretable Directions for  Disentangled Image Editing in Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Yusuf Dalva, Hidir Yesiltepe, Pinar Yanardag</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19645">https://arxiv.org/abs/2403.19645</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19645">https://arxiv.org/pdf/2403.19645</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19645]] GANTASTIC: GAN-based Transfer of Interpretable Directions for  Disentangled Image Editing in Text-to-Image Diffusion Models(https://arxiv.org/abs/2403.19645)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The rapid advancement in image generation models has predominantly been driven by diffusion models, which have demonstrated unparalleled success in generating high-fidelity, diverse images from textual prompts. Despite their success, diffusion models encounter substantial challenges in the domain of image editing, particularly in executing disentangled edits-changes that target specific attributes of an image while leaving irrelevant parts untouched. In contrast, Generative Adversarial Networks (GANs) have been recognized for their success in disentangled edits through their interpretable latent spaces. We introduce GANTASTIC, a novel framework that takes existing directions from pre-trained GAN models-representative of specific, controllable attributes-and transfers these directions into diffusion-based models. This novel approach not only maintains the generative quality and diversity that diffusion models are known for but also significantly enhances their capability to perform precise, targeted image edits, thereby leveraging the best of both worlds.</li>
</ul>

<h3>Title: MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions</h3>
<ul>
<li><strong>Authors: </strong>Kai Zhang, Yi Luan, Hexiang Hu, Kenton Lee, Siyuan Qiao, Wenhu Chen, Yu Su, Ming-Wei Chang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL, cs.IR, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19651">https://arxiv.org/abs/2403.19651</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19651">https://arxiv.org/pdf/2403.19651</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19651]] MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions(https://arxiv.org/abs/2403.19651)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Image retrieval, i.e., finding desired images given a reference image, inherently encompasses rich, multi-faceted search intents that are difficult to capture solely using image-based measures. Recent work leverages text instructions to allow users to more freely express their search intents. However, existing work primarily focuses on image pairs that are visually similar and/or can be characterized by a small set of pre-defined relations. The core thesis of this paper is that text instructions can enable retrieving images with richer relations beyond visual similarity. To show this, we introduce MagicLens, a series of self-supervised image retrieval models that support open-ended instructions. MagicLens is built on a key novel insight: image pairs that naturally occur on the same web pages contain a wide range of implicit relations (e.g., inside view of), and we can bring those implicit relations explicit by synthesizing instructions via large multimodal models (LMMs) and large language models (LLMs). Trained on 36.7M (query image, instruction, target image) triplets with rich semantic relations mined from the web, MagicLens achieves comparable or better results on eight benchmarks of various image retrieval tasks than prior state-of-the-art (SOTA) methods. Remarkably, it outperforms previous SOTA but with a 50X smaller model size on multiple benchmarks. Additional human analyses on a 1.4M-image unseen corpus further demonstrate the diversity of search intents supported by MagicLens.</li>
</ul>

<h3>Title: InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction</h3>
<ul>
<li><strong>Authors: </strong>Sirui Xu, Ziyin Wang, Yu-Xiong Wang, Liang-Yan Gui</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19652">https://arxiv.org/abs/2403.19652</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19652">https://arxiv.org/pdf/2403.19652</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19652]] InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction(https://arxiv.org/abs/2403.19652)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-conditioned human motion generation has experienced significant advancements with diffusion models trained on extensive motion capture data and corresponding textual annotations. However, extending such success to 3D dynamic human-object interaction (HOI) generation faces notable challenges, primarily due to the lack of large-scale interaction data and comprehensive descriptions that align with these interactions. This paper takes the initiative and showcases the potential of generating human-object interactions without direct training on text-interaction pair data. Our key insight in achieving this is that interaction semantics and dynamics can be decoupled. Being unable to learn interaction semantics through supervised training, we instead leverage pre-trained large models, synergizing knowledge from a large language model and a text-to-motion model. While such knowledge offers high-level control over interaction semantics, it cannot grasp the intricacies of low-level interaction dynamics. To overcome this issue, we further introduce a world model designed to comprehend simple physics, modeling how human actions influence object motion. By integrating these components, our novel framework, InterDreamer, is able to generate text-aligned 3D HOI sequences in a zero-shot manner. We apply InterDreamer to the BEHAVE and CHAIRS datasets, and our comprehensive experimental analysis demonstrates its capability to generate realistic and coherent interaction sequences that seamlessly align with the text directives.</li>
</ul>

<h3>Title: Detecting Image Attribution for Text-to-Image Diffusion Models in RGB  and Beyond</h3>
<ul>
<li><strong>Authors: </strong>Katherine Xu, Lingzhi Zhang, Jianbo Shi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19653">https://arxiv.org/abs/2403.19653</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19653">https://arxiv.org/pdf/2403.19653</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19653]] Detecting Image Attribution for Text-to-Image Diffusion Models in RGB  and Beyond(https://arxiv.org/abs/2403.19653)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Modern text-to-image (T2I) diffusion models can generate images with remarkable realism and creativity. These advancements have sparked research in fake image detection and attribution, yet prior studies have not fully explored the practical and scientific dimensions of this task. In addition to attributing images to 12 state-of-the-art T2I generators, we provide extensive analyses on what inference stage hyperparameters and image modifications are discernible. Our experiments reveal that initialization seeds are highly detectable, along with other subtle variations in the image generation process to some extent. We further investigate what visual traces are leveraged in image attribution by perturbing high-frequency details and employing mid-level representations of image style and structure. Notably, altering high-frequency information causes only slight reductions in accuracy, and training an attributor on style representations outperforms training on RGB images. Our analyses underscore that fake images are detectable and attributable at various levels of visual granularity than previously explored.</li>
</ul>

<h3>Title: RSMamba: Remote Sensing Image Classification with State Space Model</h3>
<ul>
<li><strong>Authors: </strong>Keyan Chen, Bowen Chen, Chenyang Liu, Wenyuan Li, Zhengxia Zou, Zhenwei Shi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19654">https://arxiv.org/abs/2403.19654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19654">https://arxiv.org/pdf/2403.19654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19654]] RSMamba: Remote Sensing Image Classification with State Space Model(https://arxiv.org/abs/2403.19654)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Remote sensing image classification forms the foundation of various understanding tasks, serving a crucial function in remote sensing image interpretation. The recent advancements of Convolutional Neural Networks (CNNs) and Transformers have markedly enhanced classification accuracy. Nonetheless, remote sensing scene classification remains a significant challenge, especially given the complexity and diversity of remote sensing scenarios and the variability of spatiotemporal resolutions. The capacity for whole-image understanding can provide more precise semantic cues for scene discrimination. In this paper, we introduce RSMamba, a novel architecture for remote sensing image classification. RSMamba is based on the State Space Model (SSM) and incorporates an efficient, hardware-aware design known as the Mamba. It integrates the advantages of both a global receptive field and linear modeling complexity. To overcome the limitation of the vanilla Mamba, which can only model causal sequences and is not adaptable to two-dimensional image data, we propose a dynamic multi-path activation mechanism to augment Mamba's capacity to model non-causal data. Notably, RSMamba maintains the inherent modeling mechanism of the vanilla Mamba, yet exhibits superior performance across multiple remote sensing image classification datasets. This indicates that RSMamba holds significant potential to function as the backbone of future visual foundation models. The code will be available at \url{https://github.com/KyanChen/RSMamba}.</li>
</ul>

<h3>Title: GaussianCube: Structuring Gaussian Splatting using Optimal Transport for  3D Generative Modeling</h3>
<ul>
<li><strong>Authors: </strong>Bowen Zhang, Yiji Cheng, Jiaolong Yang, Chunyu Wang, Feng Zhao, Yansong Tang, Dong Chen, Baining Guo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2403.19655">https://arxiv.org/abs/2403.19655</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2403.19655">https://arxiv.org/pdf/2403.19655</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2403.19655]] GaussianCube: Structuring Gaussian Splatting using Optimal Transport for  3D Generative Modeling(https://arxiv.org/abs/2403.19655)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>3D Gaussian Splatting (GS) have achieved considerable improvement over Neural Radiance Fields in terms of 3D fitting fidelity and rendering speed. However, this unstructured representation with scattered Gaussians poses a significant challenge for generative modeling. To address the problem, we introduce GaussianCube, a structured GS representation that is both powerful and efficient for generative modeling. We achieve this by first proposing a modified densification-constrained GS fitting algorithm which can yield high-quality fitting results using a fixed number of free Gaussians, and then re-arranging the Gaussians into a predefined voxel grid via Optimal Transport. The structured grid representation allows us to use standard 3D U-Net as our backbone in diffusion generative modeling without elaborate designs. Extensive experiments conducted on ShapeNet and OmniObject3D show that our model achieves state-of-the-art generation results both qualitatively and quantitatively, underscoring the potential of GaussianCube as a powerful and versatile 3D representation.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
