<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-02-12</h1>
<h3>Title: Cooperative Knowledge Distillation: A Learner Agnostic Approach</h3>
<ul>
<li><strong>Authors: </strong>Michael Livanos, Ian Davidson, Stephen Wong</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05942">https://arxiv.org/abs/2402.05942</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05942">https://arxiv.org/pdf/2402.05942</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05942]] Cooperative Knowledge Distillation: A Learner Agnostic Approach(https://arxiv.org/abs/2402.05942)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Knowledge distillation is a simple but powerful way to transfer knowledge between a teacher model to a student model. Existing work suffers from at least one of the following key limitations in terms of direction and scope of transfer which restrict its use: all knowledge is transferred from teacher to student regardless of whether or not that knowledge is useful, the student is the only one learning in this exchange, and typically distillation transfers knowledge only from a single teacher to a single student. We formulate a novel form of knowledge distillation in which many models can act as both students and teachers which we call cooperative distillation. The models cooperate as follows: a model (the student) identifies specific deficiencies in it's performance and searches for another model (the teacher) who encodes learned knowledge into instructional virtual instances via counterfactual instance generation. Because different models may have different strengths and weaknesses, all models can act as either students or teachers (cooperation) when appropriate and only distill knowledge in areas specific to their strengths (focus). Since counterfactuals as a paradigm are not tied to any specific algorithm, we can use this method to distill knowledge between learners of different architectures, algorithms, and even feature spaces. We demonstrate that our approach not only outperforms baselines such as transfer learning, self-supervised learning, and multiple knowledge distillation algorithms on several datasets, but it can also be used in settings where the aforementioned techniques cannot.</li>
</ul>

<h3>Title: A hybrid IndRNNLSTM approach for real-time anomaly detection in  software-defined networks</h3>
<ul>
<li><strong>Authors: </strong>Sajjad Salem, Salman Asoudeh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05943">https://arxiv.org/abs/2402.05943</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05943">https://arxiv.org/pdf/2402.05943</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05943]] A hybrid IndRNNLSTM approach for real-time anomaly detection in  software-defined networks(https://arxiv.org/abs/2402.05943)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Anomaly detection in SDN using data flow prediction is a difficult task. This problem is included in the category of time series and regression problems. Machine learning approaches are challenging in this field due to the manual selection of features. On the other hand, deep learning approaches have important features due to the automatic selection of features. Meanwhile, RNN-based approaches have been used the most. The LSTM and GRU approaches learn dependent entities well; on the other hand, the IndRNN approach learns non-dependent entities in time series. The proposed approach tried to use a combination of IndRNN and LSTM approaches to learn dependent and non-dependent features. Feature selection approaches also provide a suitable view of features for the models; for this purpose, four feature selection models, Filter, Wrapper, Embedded, and Autoencoder were used. The proposed IndRNNLSTM algorithm, in combination with Embedded, was able to achieve MAE=1.22 and RMSE=9.92 on NSL-KDD data.</li>
</ul>

<h3>Title: Separable Multi-Concept Erasure from Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Mengnan Zhao, Lihe Zhang, Tianhang Zheng, Yuqiu Kong, Baocai Yin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05947">https://arxiv.org/abs/2402.05947</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05947">https://arxiv.org/pdf/2402.05947</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05947]] Separable Multi-Concept Erasure from Diffusion Models(https://arxiv.org/abs/2402.05947)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Large-scale diffusion models, known for their impressive image generation capabilities, have raised concerns among researchers regarding social impacts, such as the imitation of copyrighted artistic styles. In response, existing approaches turn to machine unlearning techniques to eliminate unsafe concepts from pre-trained models. However, these methods compromise the generative performance and neglect the coupling among multi-concept erasures, as well as the concept restoration problem. To address these issues, we propose a Separable Multi-concept Eraser (SepME), which mainly includes two parts: the generation of concept-irrelevant representations and the weight decoupling. The former aims to avoid unlearning substantial information that is irrelevant to forgotten concepts. The latter separates optimizable model weights, making each weight increment correspond to a specific concept erasure without affecting generative performance on other concepts. Specifically, the weight increment for erasing a specified concept is formulated as a linear combination of solutions calculated based on other known undesirable concepts. Extensive experiments indicate the efficacy of our approach in eliminating concepts, preserving model performance, and offering flexibility in the erasure or recovery of various concepts.</li>
</ul>

<h3>Title: Nature-Inspired Local Propagation</h3>
<ul>
<li><strong>Authors: </strong>Alessandro Betti, Marco Gori</a></li>
<li><strong>Subjects: </strong>cs.LG, cond-mat.dis-nn, cs.AI, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05959">https://arxiv.org/abs/2402.05959</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05959">https://arxiv.org/pdf/2402.05959</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05959]] Nature-Inspired Local Propagation(https://arxiv.org/abs/2402.05959)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The spectacular results achieved in machine learning, including the recent advances in generative AI, rely on large data collections. On the opposite, intelligent processes in nature arises without the need for such collections, but simply by online processing of the environmental information. In particular, natural learning processes rely on mechanisms where data representation and learning are intertwined in such a way to respect spatiotemporal locality. This paper shows that such a feature arises from a pre-algorithmic view of learning that is inspired by related studies in Theoretical Physics. We show that the algorithmic interpretation of the derived "laws of learning", which takes the structure of Hamiltonian equations, reduces to Backpropagation when the speed of propagation goes to infinity. This opens the doors to machine learning studies based on full on-line information processing that are based the replacement of Backpropagation with the proposed spatiotemporal local algorithm.</li>
</ul>

<h3>Title: The last Dance : Robust backdoor attack via diffusion models and  bayesian approach</h3>
<ul>
<li><strong>Authors: </strong>Orson Mengara</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.05967">https://arxiv.org/abs/2402.05967</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.05967">https://arxiv.org/pdf/2402.05967</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.05967]] The last Dance : Robust backdoor attack via diffusion models and  bayesian approach(https://arxiv.org/abs/2402.05967)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models are state-of-the-art deep learning generative models that are trained on the principle of learning forward and backward diffusion processes via the progressive addition of noise and denoising. In this paper, we seek to trick audio-based DNN models, such as those in the Hugging Face framework, for example, those that focus on audio, in particular transformer-based artificial intelligence models, which are powerful machine learning models that save time and deliver faster, more efficient results. We demonstrate the feasibility of backdoor attacks (called `BacKBayDiffMod`) on audio transformers derived from Hugging Face, a popular framework in the world of artificial intelligence (AI) research. The backdoor attack developed in this paper is based on poisoning the model's training data by incorporating backdoor diffusion sampling and a Bayesian approach to the distribution of poisoned data.</li>
</ul>

<h3>Title: Animated Stickers: Bringing Stickers to Life with Video Diffusion</h3>
<ul>
<li><strong>Authors: </strong>David Yan, Winnie Zhang, Luxin Zhang, Anmol Kalia, Dingkang Wang, Ankit Ramchandani, Miao Liu, Albert Pumarola, Edgar Schoenfeld, Elliot Blanchard, Krishna Narni, Yaqiao Luo, Lawrence Chen, Guan Pang, Ali Thabet, Peter Vajda, Amy Bearman, Licheng Yu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06088">https://arxiv.org/abs/2402.06088</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06088">https://arxiv.org/pdf/2402.06088</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06088]] Animated Stickers: Bringing Stickers to Life with Video Diffusion(https://arxiv.org/abs/2402.06088)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce animated stickers, a video diffusion model which generates an animation conditioned on a text prompt and static sticker image. Our model is built on top of the state-of-the-art Emu text-to-image model, with the addition of temporal layers to model motion. Due to the domain gap, i.e. differences in visual and motion style, a model which performed well on generating natural videos can no longer generate vivid videos when applied to stickers. To bridge this gap, we employ a two-stage finetuning pipeline: first with weakly in-domain data, followed by human-in-the-loop (HITL) strategy which we term ensemble-of-teachers. It distills the best qualities of multiple teachers into a smaller student model. We show that this strategy allows us to specifically target improvements to motion quality while maintaining the style from the static image. With inference optimizations, our model is able to generate an eight-frame video with high-quality, interesting, and relevant motion in under one second.</li>
</ul>

<h3>Title: CLR-Face: Conditional Latent Refinement for Blind Face Restoration Using  Score-Based Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Maitreya Suin, Rama Chellappa</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06106">https://arxiv.org/abs/2402.06106</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06106">https://arxiv.org/pdf/2402.06106</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06106]] CLR-Face: Conditional Latent Refinement for Blind Face Restoration Using  Score-Based Diffusion Models(https://arxiv.org/abs/2402.06106)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent generative-prior-based methods have shown promising blind face restoration performance. They usually project the degraded images to the latent space and then decode high-quality faces either by single-stage latent optimization or directly from the encoding. Generating fine-grained facial details faithful to inputs remains a challenging problem. Most existing methods produce either overly smooth outputs or alter the identity as they attempt to balance between generation and reconstruction. This may be attributed to the typical trade-off between quality and resolution in the latent space. If the latent space is highly compressed, the decoded output is more robust to degradations but shows worse fidelity. On the other hand, a more flexible latent space can capture intricate facial details better, but is extremely difficult to optimize for highly degraded faces using existing techniques. To address these issues, we introduce a diffusion-based-prior inside a VQGAN architecture that focuses on learning the distribution over uncorrupted latent embeddings. With such knowledge, we iteratively recover the clean embedding conditioning on the degraded counterpart. Furthermore, to ensure the reverse diffusion trajectory does not deviate from the underlying identity, we train a separate Identity Recovery Network and use its output to constrain the reverse diffusion process. Specifically, using a learnable latent mask, we add gradients from a face-recognition network to a subset of latent features that correlates with the finer identity-related details in the pixel space, leaving the other features untouched. Disentanglement between perception and fidelity in the latent space allows us to achieve the best of both worlds. We perform extensive evaluations on multiple real and synthetic datasets to validate the superiority of our approach.</li>
</ul>

<h3>Title: Iterated Denoising Energy Matching for Sampling from Boltzmann Densities</h3>
<ul>
<li><strong>Authors: </strong>Tara Akhound-Sadegh, Jarrid Rector-Brooks, Avishek Joey Bose, Sarthak Mittal, Pablo Lemos, Cheng-Hao Liu, Marcin Sendera, Siamak Ravanbakhsh, Gauthier Gidel, Yoshua Bengio, Nikolay Malkin, Alexander Tong</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06121">https://arxiv.org/abs/2402.06121</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06121">https://arxiv.org/pdf/2402.06121</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06121]] Iterated Denoising Energy Matching for Sampling from Boltzmann Densities(https://arxiv.org/abs/2402.06121)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Efficiently generating statistically independent samples from an unnormalized probability distribution, such as equilibrium samples of many-body systems, is a foundational problem in science. In this paper, we propose Iterated Denoising Energy Matching (iDEM), an iterative algorithm that uses a novel stochastic score matching objective leveraging solely the energy function and its gradient -- and no data samples -- to train a diffusion-based sampler. Specifically, iDEM alternates between (I) sampling regions of high model density from a diffusion-based sampler and (II) using these samples in our stochastic matching objective to further improve the sampler. iDEM is scalable to high dimensions as the inner matching objective, is simulation-free, and requires no MCMC samples. Moreover, by leveraging the fast mode mixing behavior of diffusion, iDEM smooths out the energy landscape enabling efficient exploration and learning of an amortized sampler. We evaluate iDEM on a suite of tasks ranging from standard synthetic energy functions to invariant $n$-body particle systems. We show that the proposed approach achieves state-of-the-art performance on all metrics and trains $2-5\times$ faster, which allows it to be the first method to train using energy on the challenging $55$-particle Lennard-Jones system.</li>
</ul>

<h3>Title: Jointly Learning Representations for Map Entities via Heterogeneous  Graph Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Jiawei Jiang, Yifan Yang, Jingyuan Wang, Junjie Wu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06135">https://arxiv.org/abs/2402.06135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06135">https://arxiv.org/pdf/2402.06135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06135]] Jointly Learning Representations for Map Entities via Heterogeneous  Graph Contrastive Learning(https://arxiv.org/abs/2402.06135)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>The electronic map plays a crucial role in geographic information systems, serving various urban managerial scenarios and daily life services. Developing effective Map Entity Representation Learning (MERL) methods is crucial to extracting embedding information from electronic maps and converting map entities into representation vectors for downstream applications. However, existing MERL methods typically focus on one specific category of map entities, such as POIs, road segments, or land parcels, which is insufficient for real-world diverse map-based applications and might lose latent structural and semantic information interacting between entities of different types. Moreover, using representations generated by separate models for different map entities can introduce inconsistencies. Motivated by this, we propose a novel method named HOME-GCL for learning representations of multiple categories of map entities. Our approach utilizes a heterogeneous map entity graph (HOME graph) that integrates both road segments and land parcels into a unified framework. A HOME encoder with parcel-segment joint feature encoding and heterogeneous graph transformer is then deliberately designed to convert segments and parcels into representation vectors. Moreover, we introduce two types of contrastive learning tasks, namely intra-entity and inter-entity tasks, to train the encoder in a self-supervised manner. Extensive experiments on three large-scale datasets covering road segment-based, land parcel-based, and trajectory-based tasks demonstrate the superiority of our approach. To the best of our knowledge, HOME-GCL is the first attempt to jointly learn representations for road segments and land parcels using a unified model.</li>
</ul>

<h3>Title: HeadStudio: Text to Animatable Head Avatars with 3D Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Zhenglin Zhou, Fan Ma, Hehe Fan, Yi Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06149">https://arxiv.org/abs/2402.06149</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06149">https://arxiv.org/pdf/2402.06149</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06149]] HeadStudio: Text to Animatable Head Avatars with 3D Gaussian Splatting(https://arxiv.org/abs/2402.06149)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Creating digital avatars from textual prompts has long been a desirable yet challenging task. Despite the promising outcomes obtained through 2D diffusion priors in recent works, current methods face challenges in achieving high-quality and animated avatars effectively. In this paper, we present $\textbf{HeadStudio}$, a novel framework that utilizes 3D Gaussian splatting to generate realistic and animated avatars from text prompts. Our method drives 3D Gaussians semantically to create a flexible and achievable appearance through the intermediate FLAME representation. Specifically, we incorporate the FLAME into both 3D representation and score distillation: 1) FLAME-based 3D Gaussian splatting, driving 3D Gaussian points by rigging each point to a FLAME mesh. 2) FLAME-based score distillation sampling, utilizing FLAME-based fine-grained control signal to guide score distillation from the text prompt. Extensive experiments demonstrate the efficacy of HeadStudio in generating animatable avatars from textual prompts, exhibiting visually appealing appearances. The avatars are capable of rendering high-quality real-time ($\geq 40$ fps) novel views at a resolution of 1024. They can be smoothly controlled by real-world speech and video. We hope that HeadStudio can advance digital avatar creation and that the present method can widely be applied across various domains.</li>
</ul>

<h3>Title: Learning Contrastive Feature Representations for Facial Action Unit  Detection</h3>
<ul>
<li><strong>Authors: </strong>Ziqiao Shang, Bin Liu, Fei Teng, Tianrui Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06165">https://arxiv.org/abs/2402.06165</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06165">https://arxiv.org/pdf/2402.06165</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06165]] Learning Contrastive Feature Representations for Facial Action Unit  Detection(https://arxiv.org/abs/2402.06165)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>The predominant approach to facial action unit (AU) detection revolves around a supervised multi-label binary classification problem. Existing methodologies often encode pixel-level information of AUs, thereby imposing substantial demands on model complexity and expressiveness. Moreover, this practice elevates the susceptibility to overfitting due to the presence of noisy AU labels. In the present study, we introduce a contrastive learning framework enhanced by both supervised and self-supervised signals. The objective is to acquire discriminative features, deviating from the conventional pixel-level learning paradigm within the domain of AU detection. To address the challenge posed by noisy AU labels, we augment the supervised signal through the introduction of a self-supervised signal. This augmentation is achieved through positive sample sampling, encompassing three distinct types of positive sample pairs. Furthermore, to mitigate the imbalanced distribution of each AU type, we employ an importance re-weighting strategy tailored for minority AUs. The resulting loss, denoted as AUNCE, is proposed to encapsulate this strategy. Our experimental assessments, conducted on two widely-utilized benchmark datasets (BP4D and DISFA), underscore the superior performance of our approach compared to state-of-the-art methods in the realm of AU detection.</li>
</ul>

<h3>Title: A self-supervised framework for learning whole slide representations</h3>
<ul>
<li><strong>Authors: </strong>Xinhai Hou, Cheng Jiang, Akhil Kondepudi, Yiwei Lyu, Asadur Zaman Chowdury, Honglak Lee, Todd C. Hollon</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06188">https://arxiv.org/abs/2402.06188</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06188">https://arxiv.org/pdf/2402.06188</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06188]] A self-supervised framework for learning whole slide representations(https://arxiv.org/abs/2402.06188)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Whole slide imaging is fundamental to biomedical microscopy and computational pathology. However, whole slide images (WSIs) present a complex computer vision challenge due to their gigapixel size, diverse histopathologic features, spatial heterogeneity, and limited/absent data annotations. These challenges highlight that supervised training alone can result in suboptimal whole slide representations. Self-supervised representation learning can achieve high-quality WSI visual feature learning for downstream diagnostic tasks, such as cancer diagnosis or molecular genetic prediction. Here, we present a general self-supervised whole slide learning (S3L) framework for gigapixel-scale self-supervision of WSIs. S3L combines data transformation strategies from transformer-based vision and language modeling into a single unified framework to generate paired views for self-supervision. S3L leverages the inherent regional heterogeneity, histologic feature variability, and information redundancy within WSIs to learn high-quality whole-slide representations. We benchmark S3L visual representations on two diagnostic tasks for two biomedical microscopy modalities. S3L significantly outperforms WSI baselines for cancer diagnosis and genetic mutation prediction. Additionally, S3L achieves good performance using both in-domain and out-of-distribution patch encoders, demonstrating good flexibility and generalizability.</li>
</ul>

<h3>Title: Masked LoGoNet: Fast and Accurate 3D Image Analysis for Medical Domain</h3>
<ul>
<li><strong>Authors: </strong>Amin Karimi Monsefi, Payam Karisani, Mengxi Zhou, Stacey Choi, Nathan Doble, Heng Ji, Srinivasan Parthasarathy, Rajiv Ramnath</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06190">https://arxiv.org/abs/2402.06190</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06190">https://arxiv.org/pdf/2402.06190</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06190]] Masked LoGoNet: Fast and Accurate 3D Image Analysis for Medical Domain(https://arxiv.org/abs/2402.06190)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Standard modern machine-learning-based imaging methods have faced challenges in medical applications due to the high cost of dataset construction and, thereby, the limited labeled training data available. Additionally, upon deployment, these methods are usually used to process a large volume of data on a daily basis, imposing a high maintenance cost on medical facilities. In this paper, we introduce a new neural network architecture, termed LoGoNet, with a tailored self-supervised learning (SSL) method to mitigate such challenges. LoGoNet integrates a novel feature extractor within a U-shaped architecture, leveraging Large Kernel Attention (LKA) and a dual encoding strategy to capture both long-range and short-range feature dependencies adeptly. This is in contrast to existing methods that rely on increasing network capacity to enhance feature extraction. This combination of novel techniques in our model is especially beneficial in medical image segmentation, given the difficulty of learning intricate and often irregular body organ shapes, such as the spleen. Complementary, we propose a novel SSL method tailored for 3D images to compensate for the lack of large labeled datasets. The method combines masking and contrastive learning techniques within a multi-task learning framework and is compatible with both Vision Transformer (ViT) and CNN-based models. We demonstrate the efficacy of our methods in numerous tasks across two standard datasets (i.e., BTCV and MSD). Benchmark comparisons with eight state-of-the-art models highlight LoGoNet's superior performance in both inference time and accuracy.</li>
</ul>

<h3>Title: The Generative AI Paradox on Evaluation: What It Can Solve, It May Not  Evaluate</h3>
<ul>
<li><strong>Authors: </strong>Juhyun Oh, Eunsu Kim, Inha Cha, Alice Oh</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06204">https://arxiv.org/abs/2402.06204</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06204">https://arxiv.org/pdf/2402.06204</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06204]] The Generative AI Paradox on Evaluation: What It Can Solve, It May Not  Evaluate(https://arxiv.org/abs/2402.06204)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper explores the assumption that Large Language Models (LLMs) skilled in generation tasks are equally adept as evaluators. We assess the performance of three LLMs and one open-source LM in Question-Answering (QA) and evaluation tasks using the TriviaQA (Joshi et al., 2017) dataset. Results indicate a significant disparity, with LLMs exhibiting lower performance in evaluation tasks compared to generation tasks. Intriguingly, we discover instances of unfaithful evaluation where models accurately evaluate answers in areas where they lack competence, underscoring the need to examine the faithfulness and trustworthiness of LLMs as evaluators. This study contributes to the understanding of "the Generative AI Paradox" (West et al., 2023), highlighting a need to explore the correlation between generative excellence and evaluation proficiency, and the necessity to scrutinize the faithfulness aspect in model evaluations.</li>
</ul>

<h3>Title: Anomaly Unveiled: Securing Image Classification against Adversarial  Patch Attacks</h3>
<ul>
<li><strong>Authors: </strong>Nandish Chattopadhyay, Amira Guesmi, Muhammad Shafique</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06249">https://arxiv.org/abs/2402.06249</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06249">https://arxiv.org/pdf/2402.06249</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06249]] Anomaly Unveiled: Securing Image Classification against Adversarial  Patch Attacks(https://arxiv.org/abs/2402.06249)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Adversarial patch attacks pose a significant threat to the practical deployment of deep learning systems. However, existing research primarily focuses on image pre-processing defenses, which often result in reduced classification accuracy for clean images and fail to effectively counter physically feasible attacks. In this paper, we investigate the behavior of adversarial patches as anomalies within the distribution of image information and leverage this insight to develop a robust defense strategy. Our proposed defense mechanism utilizes a clustering-based technique called DBSCAN to isolate anomalous image segments, which is carried out by a three-stage pipeline consisting of Segmenting, Isolating, and Blocking phases to identify and mitigate adversarial noise. Upon identifying adversarial components, we neutralize them by replacing them with the mean pixel value, surpassing alternative replacement options. Our model-agnostic defense mechanism is evaluated across multiple models and datasets, demonstrating its effectiveness in countering various adversarial patch attacks in image classification tasks. Our proposed approach significantly improves accuracy, increasing from 38.8\% without the defense to 67.1\% with the defense against LaVAN and GoogleAp attacks, surpassing prominent state-of-the-art methods such as LGS (53.86\%) and Jujutsu (60\%)</li>
</ul>

<h3>Title: On the Efficacy of Eviction Policy for Key-Value Constrained Generative  Language Model Inference</h3>
<ul>
<li><strong>Authors: </strong>Siyu Ren, Kenny Q. Zhu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06262">https://arxiv.org/abs/2402.06262</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06262">https://arxiv.org/pdf/2402.06262</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06262]] On the Efficacy of Eviction Policy for Key-Value Constrained Generative  Language Model Inference(https://arxiv.org/abs/2402.06262)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Despite the recent success associated with Large Language Models~(LLMs), they are notably cost-prohibitive to deploy in resource-constrained environments due to their excessive memory and computational demands. In addition to model parameters, the key-value cache is also stored in GPU memory, growing linearly with batch size and sequence length. As a remedy, recent works have proposed various eviction policies for maintaining the overhead of key-value cache under a given budget. This paper embarks on the efficacy of existing eviction policies in terms of \textit{importance score calculation} and \textit{eviction scope construction}. We identify the deficiency of prior policies in these two aspects and introduce RoCo, a \underline{r}\underline{o}bust \underline{c}ache \underline{o}mission policy based on temporal attention scores and robustness measures. Extensive experimentation spanning prefilling and auto-regressive decoding stages validates the superiority of RoCo. Finally, we release EasyKV, a versatile software package dedicated to user-friendly key-value constrained generative inference. Code available at \url{https://github.com/DRSY/EasyKV}.</li>
</ul>

<h3>Title: Multisource Semisupervised Adversarial Domain Generalization Network for  Cross-Scene Sea\textendash Land Clutter Classification</h3>
<ul>
<li><strong>Authors: </strong>Xiaoxuan Zhang, Quan Pan, Salvador García</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06315">https://arxiv.org/abs/2402.06315</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06315">https://arxiv.org/pdf/2402.06315</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06315]] Multisource Semisupervised Adversarial Domain Generalization Network for  Cross-Scene Sea\textendash Land Clutter Classification(https://arxiv.org/abs/2402.06315)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Deep learning (DL)-based sea\textendash land clutter classification for sky-wave over-the-horizon-radar (OTHR) has become a novel research topic. In engineering applications, real-time predictions of sea\textendash land clutter with existing distribution discrepancies are crucial. To solve this problem, this article proposes a novel Multisource Semisupervised Adversarial Domain Generalization Network (MSADGN) for cross-scene sea\textendash land clutter classification. MSADGN can extract domain-invariant and domain-specific features from one labeled source domain and multiple unlabeled source domains, and then generalize these features to an arbitrary unseen target domain for real-time prediction of sea\textendash land clutter. Specifically, MSADGN consists of three modules: domain-related pseudolabeling module, domain-invariant module, and domain-specific module. The first module introduces an improved pseudolabel method called domain-related pseudolabel, which is designed to generate reliable pseudolabels to fully exploit unlabeled source domains. The second module utilizes a generative adversarial network (GAN) with a multidiscriminator to extract domain-invariant features, to enhance the model's transferability in the target domain. The third module employs a parallel multiclassifier branch to extract domain-specific features, to enhance the model's discriminability in the target domain. The effectiveness of our method is validated in twelve domain generalizations (DG) scenarios. Meanwhile, we selected 10 state-of-the-art DG methods for comparison. The experimental results demonstrate the superiority of our method.</li>
</ul>

<h3>Title: TimEHR: Image-based Time Series Generation for Electronic Health Records</h3>
<ul>
<li><strong>Authors: </strong>Hojjat Karami, Mary-Anne Hartley, David Atienza, Anisoara Ionescu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06318">https://arxiv.org/abs/2402.06318</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06318">https://arxiv.org/pdf/2402.06318</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06318]] TimEHR: Image-based Time Series Generation for Electronic Health Records(https://arxiv.org/abs/2402.06318)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Time series in Electronic Health Records (EHRs) present unique challenges for generative models, such as irregular sampling, missing values, and high dimensionality. In this paper, we propose a novel generative adversarial network (GAN) model, TimEHR, to generate time series data from EHRs. In particular, TimEHR treats time series as images and is based on two conditional GANs. The first GAN generates missingness patterns, and the second GAN generates time series values based on the missingness pattern. Experimental results on three real-world EHR datasets show that TimEHR outperforms state-of-the-art methods in terms of fidelity, utility, and privacy metrics.</li>
</ul>

<h3>Title: InternLM-Math: Open Math Large Language Models Toward Verifiable  Reasoning</h3>
<ul>
<li><strong>Authors: </strong>Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, Yudong Wang, Zijian Wu, Shuaibin Li, Fengzhe Zhou, Hongwei Liu, Songyang Zhang, Wenwei Zhang, Hang Yan, Xipeng Qiu, Jiayu Wang, Kai Chen, Dahua Lin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06332">https://arxiv.org/abs/2402.06332</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06332">https://arxiv.org/pdf/2402.06332</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06332]] InternLM-Math: Open Math Large Language Models Toward Verifiable  Reasoning(https://arxiv.org/abs/2402.06332)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>The math abilities of large language models can represent their abstract reasoning ability. In this paper, we introduce and open-source our math reasoning LLMs InternLM-Math which is continue pre-trained from InternLM2. We unify chain-of-thought reasoning, reward modeling, formal reasoning, data augmentation, and code interpreter in a unified seq2seq format and supervise our model to be a versatile math reasoner, verifier, prover, and augmenter. These abilities can be used to develop the next math LLMs or self-iteration. InternLM-Math obtains open-sourced state-of-the-art performance under the setting of in-context learning, supervised fine-tuning, and code-assisted reasoning in various informal and formal benchmarks including GSM8K, MATH, Hungary math exam, MathBench-ZH, and MiniF2F. Our pre-trained model achieves 30.3 on the MiniF2F test set without fine-tuning. We further explore how to use LEAN to solve math problems and study its performance under the setting of multi-task learning which shows the possibility of using LEAN as a unified platform for solving and proving in math. Our models, codes, and data are released at \url{https://github.com/InternLM/InternLM-Math}.</li>
</ul>

<h3>Title: TEE4EHR: Transformer Event Encoder for Better Representation Learning in  Electronic Health Records</h3>
<ul>
<li><strong>Authors: </strong>Hojjat Karami, David Atienza, Anisoara Ionescu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06367">https://arxiv.org/abs/2402.06367</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06367">https://arxiv.org/pdf/2402.06367</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06367]] TEE4EHR: Transformer Event Encoder for Better Representation Learning in  Electronic Health Records(https://arxiv.org/abs/2402.06367)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Irregular sampling of time series in electronic health records (EHRs) is one of the main challenges for developing machine learning models. Additionally, the pattern of missing data in certain clinical variables is not at random but depends on the decisions of clinicians and the state of the patient. Point process is a mathematical framework for analyzing event sequence data that is consistent with irregular sampling patterns. Our model, TEE4EHR, is a transformer event encoder (TEE) with point process loss that encodes the pattern of laboratory tests in EHRs. The utility of our TEE has been investigated in a variety of benchmark event sequence datasets. Additionally, we conduct experiments on two real-world EHR databases to provide a more comprehensive evaluation of our model. Firstly, in a self-supervised learning approach, the TEE is jointly learned with an existing attention-based deep neural network which gives superior performance in negative log-likelihood and future event prediction. Besides, we propose an algorithm for aggregating attention weights that can reveal the interaction between the events. Secondly, we transfer and freeze the learned TEE to the downstream task for the outcome prediction, where it outperforms state-of-the-art models for handling irregularly sampled time series. Furthermore, our results demonstrate that our approach can improve representation learning in EHRs and can be useful for clinical prediction tasks.</li>
</ul>

<h3>Title: ImplicitDeepfake: Plausible Face-Swapping through Implicit Deepfake  Generation using NeRF and Gaussian Splatting</h3>
<ul>
<li><strong>Authors: </strong>Georgii Stanishevskii, Jakub Steczkiewicz, Tomasz Szczepanik, Sławomir Tadeja, Jacek Tabor, Przemysław Spurek</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06390">https://arxiv.org/abs/2402.06390</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06390">https://arxiv.org/pdf/2402.06390</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06390]] ImplicitDeepfake: Plausible Face-Swapping through Implicit Deepfake  Generation using NeRF and Gaussian Splatting(https://arxiv.org/abs/2402.06390)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Numerous emerging deep-learning techniques have had a substantial impact on computer graphics. Among the most promising breakthroughs are the recent rise of Neural Radiance Fields (NeRFs) and Gaussian Splatting (GS). NeRFs encode the object's shape and color in neural network weights using a handful of images with known camera positions to generate novel views. In contrast, GS provides accelerated training and inference without a decrease in rendering quality by encoding the object's characteristics in a collection of Gaussian distributions. These two techniques have found many use cases in spatial computing and other domains. On the other hand, the emergence of deepfake methods has sparked considerable controversy. Such techniques can have a form of artificial intelligence-generated videos that closely mimic authentic footage. Using generative models, they can modify facial features, enabling the creation of altered identities or facial expressions that exhibit a remarkably realistic appearance to a real person. Despite these controversies, deepfake can offer a next-generation solution for avatar creation and gaming when of desirable quality. To that end, we show how to combine all these emerging technologies to obtain a more plausible outcome. Our ImplicitDeepfake1 uses the classical deepfake algorithm to modify all training images separately and then train NeRF and GS on modified faces. Such relatively simple strategies can produce plausible 3D deepfake-based avatars.</li>
</ul>

<h3>Title: Trust the Process: Zero-Knowledge Machine Learning to Enhance Trust in  Generative AI Interactions</h3>
<ul>
<li><strong>Authors: </strong>Bianca-Mihaela Ganescu, Jonathan Passerat-Palmbach</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06414">https://arxiv.org/abs/2402.06414</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06414">https://arxiv.org/pdf/2402.06414</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06414]] Trust the Process: Zero-Knowledge Machine Learning to Enhance Trust in  Generative AI Interactions(https://arxiv.org/abs/2402.06414)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative AI, exemplified by models like transformers, has opened up new possibilities in various domains but also raised concerns about fairness, transparency and reliability, especially in fields like medicine and law. This paper emphasizes the urgency of ensuring fairness and quality in these domains through generative AI. It explores using cryptographic techniques, particularly Zero-Knowledge Proofs (ZKPs), to address concerns regarding performance fairness and accuracy while protecting model privacy. Applying ZKPs to Machine Learning models, known as ZKML (Zero-Knowledge Machine Learning), enables independent validation of AI-generated content without revealing sensitive model information, promoting transparency and trust. ZKML enhances AI fairness by providing cryptographic audit trails for model predictions and ensuring uniform performance across users. We introduce snarkGPT, a practical ZKML implementation for transformers, to empower users to verify output accuracy and quality while preserving model privacy. We present a series of empirical results studying snarkGPT's scalability and performance to assess the feasibility and challenges of adopting a ZKML-powered approach to capture quality and performance fairness problems in generative AI models.</li>
</ul>

<h3>Title: Improving 2D-3D Dense Correspondences with Diffusion Models for 6D  Object Pose Estimation</h3>
<ul>
<li><strong>Authors: </strong>Peter Hönig, Stefan Thalhammer, Markus Vincze</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06436">https://arxiv.org/abs/2402.06436</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06436">https://arxiv.org/pdf/2402.06436</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06436]] Improving 2D-3D Dense Correspondences with Diffusion Models for 6D  Object Pose Estimation(https://arxiv.org/abs/2402.06436)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Estimating 2D-3D correspondences between RGB images and 3D space is a fundamental problem in 6D object pose estimation. Recent pose estimators use dense correspondence maps and Point-to-Point algorithms to estimate object poses. The accuracy of pose estimation depends heavily on the quality of the dense correspondence maps and their ability to withstand occlusion, clutter, and challenging material properties. Currently, dense correspondence maps are estimated using image-to-image translation models based on GANs, Autoencoders, or direct regression models. However, recent advancements in image-to-image translation have led to diffusion models being the superior choice when evaluated on benchmarking datasets. In this study, we compare image-to-image translation networks based on GANs and diffusion models for the downstream task of 6D object pose estimation. Our results demonstrate that the diffusion-based image-to-image translation model outperforms the GAN, revealing potential for further improvements in 6D object pose estimation models.</li>
</ul>

<h3>Title: ControlUDA: Controllable Diffusion-assisted Unsupervised Domain  Adaptation for Cross-Weather Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Fengyi Shen, Li Zhou, Kagan Kucukaytekin, Ziyuan Liu, He Wang, Alois Knoll</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06446">https://arxiv.org/abs/2402.06446</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06446">https://arxiv.org/pdf/2402.06446</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06446]] ControlUDA: Controllable Diffusion-assisted Unsupervised Domain  Adaptation for Cross-Weather Semantic Segmentation(https://arxiv.org/abs/2402.06446)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Data generation is recognized as a potent strategy for unsupervised domain adaptation (UDA) pertaining semantic segmentation in adverse weathers. Nevertheless, these adverse weather scenarios encompass multiple possibilities, and high-fidelity data synthesis with controllable weather is under-researched in previous UDA works. The recent strides in large-scale text-to-image diffusion models (DM) have ushered in a novel avenue for research, enabling the generation of realistic images conditioned on semantic labels. This capability proves instrumental for cross-domain data synthesis from source to target domain owing to their shared label space. Thus, source domain labels can be paired with those generated pseudo target data for training UDA. However, from the UDA perspective, there exists several challenges for DM training: (i) ground-truth labels from target domain are missing; (ii) the prompt generator may produce vague or noisy descriptions of images from adverse weathers; (iii) existing arts often struggle to well handle the complex scene structure and geometry of urban scenes when conditioned only on semantic labels. To tackle the above issues, we propose ControlUDA, a diffusion-assisted framework tailored for UDA segmentation under adverse weather conditions. It first leverages target prior from a pre-trained segmentor for tuning the DM, compensating the missing target domain labels; It also contains UDAControlNet, a condition-fused multi-scale and prompt-enhanced network targeted at high-fidelity data generation in adverse weathers. Training UDA with our generated data brings the model performances to a new milestone (72.0 mIoU) on the popular Cityscapes-to-ACDC benchmark for adverse weathers. Furthermore, ControlUDA helps to achieve good model generalizability on unseen data.</li>
</ul>

<h3>Title: Sequential Flow Matching for Generative Modeling</h3>
<ul>
<li><strong>Authors: </strong>Jongmin Yoon, Juho Lee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06461">https://arxiv.org/abs/2402.06461</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06461">https://arxiv.org/pdf/2402.06461</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06461]] Sequential Flow Matching for Generative Modeling(https://arxiv.org/abs/2402.06461)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Straightening the probability flow of the continuous-time generative models, such as diffusion models or flow-based models, is the key to fast sampling through the numerical solvers, existing methods learn a linear path by directly generating the probability path the joint distribution between the noise and data distribution. One key reason for the slow sampling speed of the ODE-based solvers that simulate these generative models is the global truncation error of the ODE solver, caused by the high curvature of the ODE trajectory, which explodes the truncation error of the numerical solvers in the low-NFE regime. To address this challenge, We propose a novel method called SeqRF, a learning technique that straightens the probability flow to reduce the global truncation error and hence enable acceleration of sampling and improve the synthesis quality. In both theoretical and empirical studies, we first observe the straightening property of our SeqRF. Through empirical evaluations via SeqRF over flow-based generative models, We achieve surpassing results on CIFAR-10, CelebA-$64 \times 64$, and LSUN-Church datasets.</li>
</ul>

<h3>Title: BarlowTwins-CXR : Enhancing Chest X-Ray abnormality localization in  heterogeneous data with cross-domain self-supervised learning</h3>
<ul>
<li><strong>Authors: </strong>Haoyue Sheng, Linrui Ma, Jean-Francois Samson, Dianbo Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06499">https://arxiv.org/abs/2402.06499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06499">https://arxiv.org/pdf/2402.06499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06499]] BarlowTwins-CXR : Enhancing Chest X-Ray abnormality localization in  heterogeneous data with cross-domain self-supervised learning(https://arxiv.org/abs/2402.06499)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Background: Chest X-ray imaging-based abnormality localization, essential in diagnosing various diseases, faces significant clinical challenges due to complex interpretations and the growing workload of radiologists. While recent advances in deep learning offer promising solutions, there is still a critical issue of domain inconsistency in cross-domain transfer learning, which hampers the efficiency and accuracy of diagnostic processes. This study aims to address the domain inconsistency problem and improve autonomic abnormality localization performance of heterogeneous chest X-ray image analysis, by developing a self-supervised learning strategy called "BarlwoTwins-CXR". Methods: We utilized two publicly available datasets: the NIH Chest X-ray Dataset and the VinDr-CXR. The BarlowTwins-CXR approach was conducted in a two-stage training process. Initially, self-supervised pre-training was performed using an adjusted Barlow Twins algorithm on the NIH dataset with a Resnet50 backbone pre-trained on ImageNet. This was followed by supervised fine-tuning on the VinDr-CXR dataset using Faster R-CNN with Feature Pyramid Network (FPN). Results: Our experiments showed a significant improvement in model performance with BarlowTwins-CXR. The approach achieved a 3% increase in mAP50 accuracy compared to traditional ImageNet pre-trained models. In addition, the Ablation CAM method revealed enhanced precision in localizing chest abnormalities. Conclusion: BarlowTwins-CXR significantly enhances the efficiency and accuracy of chest X-ray image-based abnormality localization, outperforming traditional transfer learning methods and effectively overcoming domain inconsistency in cross-domain scenarios. Our experiment results demonstrate the potential of using self-supervised learning to improve the generalizability of models in medical settings with limited amounts of heterogeneous data.</li>
</ul>

<h3>Title: Generative Adversarial Bayesian Optimization for Surrogate Objectives</h3>
<ul>
<li><strong>Authors: </strong>Michael S. Yao, Yimeng Zeng, Hamsa Bastani, Jacob Gardner, James C. Gee, Osbert Bastani</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06532">https://arxiv.org/abs/2402.06532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06532">https://arxiv.org/pdf/2402.06532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06532]] Generative Adversarial Bayesian Optimization for Surrogate Objectives(https://arxiv.org/abs/2402.06532)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Offline model-based policy optimization seeks to optimize a learned surrogate objective function without querying the true oracle objective during optimization. However, inaccurate surrogate model predictions are frequently encountered along the optimization trajectory. To address this limitation, we propose generative adversarial Bayesian optimization (GABO) using adaptive source critic regularization, a task-agnostic framework for Bayesian optimization that employs a Lipschitz-bounded source critic model to constrain the optimization trajectory to regions where the surrogate function is reliable. We show that under certain assumptions for the continuous input space prior, our algorithm dynamically adjusts the strength of the source critic regularization. GABO outperforms existing baselines on a number of different offline optimization tasks across a variety of scientific domains. Our code is available at https://github.com/michael-s-yao/gabo</li>
</ul>

<h3>Title: Diffusion-ES: Gradient-free Planning with Diffusion for Autonomous  Driving and Zero-Shot Instruction Following</h3>
<ul>
<li><strong>Authors: </strong>Brian Yang, Huangyuan Su, Nikolaos Gkanatsios, Tsung-Wei Ke, Ayush Jain, Jeff Schneider, Katerina Fragkiadaki</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06559">https://arxiv.org/abs/2402.06559</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06559">https://arxiv.org/pdf/2402.06559</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06559]] Diffusion-ES: Gradient-free Planning with Diffusion for Autonomous  Driving and Zero-Shot Instruction Following(https://arxiv.org/abs/2402.06559)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models excel at modeling complex and multimodal trajectory distributions for decision-making and control. Reward-gradient guided denoising has been recently proposed to generate trajectories that maximize both a differentiable reward function and the likelihood under the data distribution captured by a diffusion model. Reward-gradient guided denoising requires a differentiable reward function fitted to both clean and noised samples, limiting its applicability as a general trajectory optimizer. In this paper, we propose DiffusionES, a method that combines gradient-free optimization with trajectory denoising to optimize black-box non-differentiable objectives while staying in the data manifold. Diffusion-ES samples trajectories during evolutionary search from a diffusion model and scores them using a black-box reward function. It mutates high-scoring trajectories using a truncated diffusion process that applies a small number of noising and denoising steps, allowing for much more efficient exploration of the solution space. We show that DiffusionES achieves state-of-the-art performance on nuPlan, an established closed-loop planning benchmark for autonomous driving. Diffusion-ES outperforms existing sampling-based planners, reactive deterministic or diffusion-based policies, and reward-gradient guidance. Additionally, we show that unlike prior guidance methods, our method can optimize non-differentiable language-shaped reward functions generated by few-shot LLM prompting. When guided by a human teacher that issues instructions to follow, our method can generate novel, highly complex behaviors, such as aggressive lane weaving, which are not present in the training data. This allows us to solve the hardest nuPlan scenarios which are beyond the capabilities of existing trajectory optimization methods and driving policies.</li>
</ul>

<h3>Title: Video Annotator: A framework for efficiently building video classifiers  using vision-language models and active learning</h3>
<ul>
<li><strong>Authors: </strong>Amir Ziai, Aneesh Vartakavi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06560">https://arxiv.org/abs/2402.06560</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06560">https://arxiv.org/pdf/2402.06560</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06560]] Video Annotator: A framework for efficiently building video classifiers  using vision-language models and active learning(https://arxiv.org/abs/2402.06560)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>High-quality and consistent annotations are fundamental to the successful development of robust machine learning models. Traditional data annotation methods are resource-intensive and inefficient, often leading to a reliance on third-party annotators who are not the domain experts. Hard samples, which are usually the most informative for model training, tend to be difficult to label accurately and consistently without business context. These can arise unpredictably during the annotation process, requiring a variable number of iterations and rounds of feedback, leading to unforeseen expenses and time commitments to guarantee quality. We posit that more direct involvement of domain experts, using a human-in-the-loop system, can resolve many of these practical challenges. We propose a novel framework we call Video Annotator (VA) for annotating, managing, and iterating on video classification datasets. Our approach offers a new paradigm for an end-user-centered model development process, enhancing the efficiency, usability, and effectiveness of video classifiers. Uniquely, VA allows for a continuous annotation process, seamlessly integrating data collection and model training. We leverage the zero-shot capabilities of vision-language foundation models combined with active learning techniques, and demonstrate that VA enables the efficient creation of high-quality models. VA achieves a median 6.8 point improvement in Average Precision relative to the most competitive baseline across a wide-ranging assortment of tasks. We release a dataset with 153k labels across 56 video understanding tasks annotated by three professional video editors using VA, and also release code to replicate our experiments at: this http URL</li>
</ul>

<h3>Title: On the Out-Of-Distribution Generalization of Multimodal Large Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Xingxuan Zhang, Jiansheng Li, Wenjing Chu, Junjia Hai, Renzhe Xu, Yuqing Yang, Shikai Guan, Jiazheng Xu, Peng Cui</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06599">https://arxiv.org/abs/2402.06599</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06599">https://arxiv.org/pdf/2402.06599</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06599]] On the Out-Of-Distribution Generalization of Multimodal Large Language  Models(https://arxiv.org/abs/2402.06599)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>We investigate the generalization boundaries of current Multimodal Large Language Models (MLLMs) via comprehensive evaluation under out-of-distribution scenarios and domain-specific tasks. We evaluate their zero-shot generalization across synthetic images, real-world distributional shifts, and specialized datasets like medical and molecular imagery. Empirical results indicate that MLLMs struggle with generalization beyond common training domains, limiting their direct application without adaptation. To understand the cause of unreliable performance, we analyze three hypotheses: semantic misinterpretation, visual feature extraction insufficiency, and mapping deficiency. Results identify mapping deficiency as the primary hurdle. To address this problem, we show that in-context learning (ICL) can significantly enhance MLLMs' generalization, opening new avenues for overcoming generalization barriers. We further explore the robustness of ICL under distribution shifts and show its vulnerability to domain shifts, label shifts, and spurious correlation shifts between in-context examples and test data.</li>
</ul>

<h3>Title: Feedback Loops With Language Models Drive In-Context Reward Hacking</h3>
<ul>
<li><strong>Authors: </strong>Alexander Pan, Erik Jones, Meena Jagadeesan, Jacob Steinhardt</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.06627">https://arxiv.org/abs/2402.06627</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.06627">https://arxiv.org/pdf/2402.06627</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.06627]] Feedback Loops With Language Models Drive In-Context Reward Hacking(https://arxiv.org/abs/2402.06627)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Language models influence the external world: they query APIs that read and write to web pages, generate content that shapes human behavior, and run system commands as autonomous agents. These interactions form feedback loops: LLM outputs affect the world, which in turn affect subsequent LLM outputs. In this work, we show that feedback loops can cause in-context reward hacking (ICRH), where the LLM at test-time optimizes a (potentially implicit) objective but creates negative side effects in the process. For example, consider an LLM agent deployed to increase Twitter engagement; the LLM may retrieve its previous tweets into the context window and make them more controversial, increasing engagement but also toxicity. We identify and study two processes that lead to ICRH: output-refinement and policy-refinement. For these processes, evaluations on static datasets are insufficient -- they miss the feedback effects and thus cannot capture the most harmful behavior. In response, we provide three recommendations for evaluation to capture more instances of ICRH. As AI development accelerates, the effects of feedback loops will proliferate, increasing the need to understand their role in shaping LLM behavior.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
