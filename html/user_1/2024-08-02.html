<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-08-02</h1>
<h3>Title: Replication in Visual Diffusion Models: A Survey and Outlook</h3>
<ul>
<li><strong>Authors: </strong>Wenhao Wang, Yifan Sun, Zongxin Yang, Zhengdong Hu, Zhentao Tan, Yi Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00001">https://arxiv.org/abs/2408.00001</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00001">https://arxiv.org/pdf/2408.00001</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00001]] Replication in Visual Diffusion Models: A Survey and Outlook(https://arxiv.org/abs/2408.00001)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Visual diffusion models have revolutionized the field of creative AI, producing high-quality and diverse content. However, they inevitably memorize training images or videos, subsequently replicating their concepts, content, or styles during inference. This phenomenon raises significant concerns about privacy, security, and copyright within generated outputs. In this survey, we provide the first comprehensive review of replication in visual diffusion models, marking a novel contribution to the field by systematically categorizing the existing studies into unveiling, understanding, and mitigating this phenomenon. Specifically, unveiling mainly refers to the methods used to detect replication instances. Understanding involves analyzing the underlying mechanisms and factors that contribute to this phenomenon. Mitigation focuses on developing strategies to reduce or eliminate replication. Beyond these aspects, we also review papers focusing on its real-world influence. For instance, in the context of healthcare, replication is critically worrying due to privacy concerns related to patient data. Finally, the paper concludes with a discussion of the ongoing challenges, such as the difficulty in detecting and benchmarking replication, and outlines future directions including the development of more robust mitigation techniques. By synthesizing insights from diverse studies, this paper aims to equip researchers and practitioners with a deeper understanding at the intersection between AI technology and social good. We release this project at this https URL.</li>
</ul>

<h3>Title: Localized Gaussian Splatting Editing with Contextual Awareness</h3>
<ul>
<li><strong>Authors: </strong>Hanyuan Xiao, Yingshu Chen, Huajian Huang, Haolin Xiong, Jing Yang, Pratusha Prasad, Yajie Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00083">https://arxiv.org/abs/2408.00083</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00083">https://arxiv.org/pdf/2408.00083</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00083]] Localized Gaussian Splatting Editing with Contextual Awareness(https://arxiv.org/abs/2408.00083)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent text-guided generation of individual 3D object has achieved great success using diffusion priors. However, these methods are not suitable for object insertion and replacement tasks as they do not consider the background, leading to illumination mismatches within the environment. To bridge the gap, we introduce an illumination-aware 3D scene editing pipeline for 3D Gaussian Splatting (3DGS) representation. Our key observation is that inpainting by the state-of-the-art conditional 2D diffusion model is consistent with background in lighting. To leverage the prior knowledge from the well-trained diffusion models for 3D object generation, our approach employs a coarse-to-fine objection optimization pipeline with inpainted views. In the first coarse step, we achieve image-to-3D lifting given an ideal inpainted view. The process employs 3D-aware diffusion prior from a view-conditioned diffusion model, which preserves illumination present in the conditioning image. To acquire an ideal inpainted image, we introduce an Anchor View Proposal (AVP) algorithm to find a single view that best represents the scene illumination in target region. In the second Texture Enhancement step, we introduce a novel Depth-guided Inpainting Score Distillation Sampling (DI-SDS), which enhances geometry and texture details with the inpainting diffusion prior, beyond the scope of the 3D-aware diffusion prior knowledge in the first coarse step. DI-SDS not only provides fine-grained texture enhancement, but also urges optimization to respect scene lighting. Our approach efficiently achieves local editing with global illumination consistency without explicitly modeling light transport. We demonstrate robustness of our method by evaluating editing in real scenes containing explicit highlight and shadows, and compare against the state-of-the-art text-to-3D editing methods.</li>
</ul>

<h3>Title: WAS: Dataset and Methods for Artistic Text Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Xudong Xie, Yuzhe Li, Yang Liu, Zhifei Zhang, Zhaowen Wang, Wei Xiong, Xiang Bai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00106">https://arxiv.org/abs/2408.00106</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00106">https://arxiv.org/pdf/2408.00106</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00106]] WAS: Dataset and Methods for Artistic Text Segmentation(https://arxiv.org/abs/2408.00106)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Accurate text segmentation results are crucial for text-related generative tasks, such as text image generation, text editing, text removal, and text style transfer. Recently, some scene text segmentation methods have made significant progress in segmenting regular text. However, these methods perform poorly in scenarios containing artistic text. Therefore, this paper focuses on the more challenging task of artistic text segmentation and constructs a real artistic text segmentation dataset. One challenge of the task is that the local stroke shapes of artistic text are changeable with diversity and complexity. We propose a decoder with the layer-wise momentum query to prevent the model from ignoring stroke regions of special shapes. Another challenge is the complexity of the global topological structure. We further design a skeleton-assisted head to guide the model to focus on the global structure. Additionally, to enhance the generalization performance of the text segmentation model, we propose a strategy for training data synthesis, based on the large multi-modal model and the diffusion model. Experimental results show that our proposed method and synthetic dataset can significantly enhance the performance of artistic text segmentation and achieve state-of-the-art results on other public datasets.</li>
</ul>

<h3>Title: Distributed In-Context Learning under Non-IID Among Clients</h3>
<ul>
<li><strong>Authors: </strong>Siqi Liang, Sumyeong Ahn, Jiayu Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00144">https://arxiv.org/abs/2408.00144</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00144">https://arxiv.org/pdf/2408.00144</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00144]] Distributed In-Context Learning under Non-IID Among Clients(https://arxiv.org/abs/2408.00144)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Advancements in large language models (LLMs) have shown their effectiveness in multiple complicated natural language reasoning tasks. A key challenge remains in adapting these models efficiently to new or unfamiliar tasks. In-context learning (ICL) provides a promising solution for few-shot adaptation by retrieving a set of data points relevant to a query, called in-context examples (ICE), from a training dataset and providing them during the inference as context. Most existing studies utilize a centralized training dataset, yet many real-world datasets may be distributed among multiple clients, and remote data retrieval can be associated with costs. Especially when the client data are non-identical independent distributions (non-IID), retrieving from clients a proper set of ICEs needed for a test query presents critical challenges. In this paper, we first show that in this challenging setting, test queries will have different preferences among clients because of non-IIDness, and equal contribution often leads to suboptimal performance. We then introduce a novel approach to tackle the distributed non-IID ICL problem when a data usage budget is present. The principle is that each client's proper contribution (budget) should be designed according to the preference of each query for that client. Our approach uses a data-driven manner to allocate a budget for each client, tailored to each test query. Through extensive empirical studies on diverse datasets, our framework demonstrates superior performance relative to competing baselines.</li>
</ul>

<h3>Title: Generative Learning of the Solution of Parametric Partial Differential Equations Using Guided Diffusion Models and Virtual Observations</h3>
<ul>
<li><strong>Authors: </strong>Han Gao, Sebastian Kaltenbach, Petros Koumoutsakos</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.comp-ph, physics.flu-dyn</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00157">https://arxiv.org/abs/2408.00157</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00157">https://arxiv.org/pdf/2408.00157</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00157]] Generative Learning of the Solution of Parametric Partial Differential Equations Using Guided Diffusion Models and Virtual Observations(https://arxiv.org/abs/2408.00157)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We introduce a generative learning framework to model high-dimensional parametric systems using gradient guidance and virtual observations. We consider systems described by Partial Differential Equations (PDEs) discretized with structured or unstructured grids. The framework integrates multi-level information to generate high fidelity time sequences of the system dynamics. We demonstrate the effectiveness and versatility of our framework with two case studies in incompressible, two dimensional, low Reynolds cylinder flow on an unstructured mesh and incompressible turbulent channel flow on a structured mesh, both parameterized by the Reynolds number. Our results illustrate the framework's robustness and ability to generate accurate flow sequences across various parameter settings, significantly reducing computational costs allowing for efficient forecasting and reconstruction of flow dynamics.</li>
</ul>

<h3>Title: Automated Software Vulnerability Static Code Analysis Using Generative Pre-Trained Transformer Models</h3>
<ul>
<li><strong>Authors: </strong>Elijah Pelofske, Vincent Urias, Lorie M. Liebrock</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00197">https://arxiv.org/abs/2408.00197</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00197">https://arxiv.org/pdf/2408.00197</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00197]] Automated Software Vulnerability Static Code Analysis Using Generative Pre-Trained Transformer Models(https://arxiv.org/abs/2408.00197)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative Pre-Trained Transformer models have been shown to be surprisingly effective at a variety of natural language processing tasks -- including generating computer code. We evaluate the effectiveness of open source GPT models for the task of automatic identification of the presence of vulnerable code syntax (specifically targeting C and C++ source code). This task is evaluated on a selection of 36 source code examples from the NIST SARD dataset, which are specifically curated to not contain natural English that indicates the presence, or lack thereof, of a particular vulnerability. The NIST SARD source code dataset contains identified vulnerable lines of source code that are examples of one out of the 839 distinct Common Weakness Enumerations (CWE), allowing for exact quantification of the GPT output classification error rate. A total of 5 GPT models are evaluated, using 10 different inference temperatures and 100 repetitions at each setting, resulting in 5,000 GPT queries per vulnerable source code analyzed. Ultimately, we find that the GPT models that we evaluated are not suitable for fully automated vulnerability scanning because the false positive and false negative rates are too high to likely be useful in practice. However, we do find that the GPT models perform surprisingly well at automated vulnerability detection for some of the test cases, in particular surpassing random sampling, and being able to identify the exact lines of code that are vulnerable albeit at a low success rate. The best performing GPT model result found was Llama-2-70b-chat-hf with inference temperature of 0.1 applied to NIST SARD test case 149165 (which is an example of a buffer overflow vulnerability), which had a binary classification recall score of 1.0 and a precision of 1.0 for correctly and uniquely identifying the vulnerable line of code and the correct CWE number.</li>
</ul>

<h3>Title: A Prior Embedding-Driven Architecture for Long Distance Blind Iris Recognition</h3>
<ul>
<li><strong>Authors: </strong>Qi Xiong, Xinman Zhang, Jun Shen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00210">https://arxiv.org/abs/2408.00210</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00210">https://arxiv.org/pdf/2408.00210</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00210]] A Prior Embedding-Driven Architecture for Long Distance Blind Iris Recognition(https://arxiv.org/abs/2408.00210)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Blind iris images, which result from unknown degradation during the process of iris recognition at long distances, often lead to decreased iris recognition rates. Currently, little existing literature offers a solution to this problem. In response, we propose a prior embedding-driven architecture for long distance blind iris recognition. We first proposed a blind iris image restoration network called Iris-PPRGAN. To effectively restore the texture of the blind iris, Iris-PPRGAN includes a Generative Adversarial Network (GAN) used as a Prior Decoder, and a DNN used as the encoder. To extract iris features more efficiently, we then proposed a robust iris classifier by modifying the bottleneck module of InsightFace, which called Insight-Iris. A low-quality blind iris image is first restored by Iris-PPRGAN, then the restored iris image undergoes recognition via Insight-Iris. Experimental results on the public CASIA-Iris-distance dataset demonstrate that our proposed method significantly superior results to state-of-the-art blind iris restoration methods both quantitatively and qualitatively, Specifically, the recognition rate for long-distance blind iris images reaches 90% after processing with our methods, representing an improvement of approximately ten percentage points compared to images without restoration.</li>
</ul>

<h3>Title: Mobility-Aware Federated Self-supervised Learning in Vehicular Network</h3>
<ul>
<li><strong>Authors: </strong>Xueying Gu, Qiong Wu, Pingyi Fan, Qiang Fan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00256">https://arxiv.org/abs/2408.00256</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00256">https://arxiv.org/pdf/2408.00256</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00256]] Mobility-Aware Federated Self-supervised Learning in Vehicular Network(https://arxiv.org/abs/2408.00256)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL) is an advanced distributed machine learning approach, that protects the privacy of each vehicle by allowing the model to be trained on multiple devices simultaneously without the need to upload all data to a road side unit (RSU). This enables FL to handle scenarios with sensitive or widely distributed data. However, in these fields, it is well known that the labeling costs can be a significant expense, and models relying on labels are not suitable for these rapidly evolving fields especially in vehicular networks, or mobile internet of things (MIoT), where new data emerges constantly. To handle this issue, the self-supervised learning paves the way for training without labels. Additionally, for vehicles with high velocity, owing to blurred images, simple aggregation not only impacts the accuracy of the aggregated model but also reduces the convergence speed of FL. This paper proposes a FL algorithm based on image blur level to aggregation, called FLSimCo, which does not require labels and serves as a pre-training stage for self-supervised learning in the vehicular environment. Simulation results demonstrate that the proposed algorithm exhibits fast and stable convergence.</li>
</ul>

<h3>Title: QUITO: Accelerating Long-Context Reasoning through Query-Guided Context Compression</h3>
<ul>
<li><strong>Authors: </strong>Wenshan Wang, Yihang Wang, Yixing Fan, Huaming Liao, Jiafeng Guo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00274">https://arxiv.org/abs/2408.00274</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00274">https://arxiv.org/pdf/2408.00274</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00274]] QUITO: Accelerating Long-Context Reasoning through Query-Guided Context Compression(https://arxiv.org/abs/2408.00274)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>In-context learning (ICL) capabilities are foundational to the success of large language models (LLMs). Recently, context compression has attracted growing interest since it can largely reduce reasoning complexities and computation costs of LLMs. In this paper, we introduce a novel Query-gUIded aTtention cOmpression (QUITO) method, which leverages attention of the question over the contexts to filter useless information. Specifically, we take a trigger token to calculate the attention distribution of the context in response to the question. Based on the distribution, we propose three different filtering methods to satisfy the budget constraints of the context length. We evaluate the QUITO using two widely-used datasets, namely, NaturalQuestions and ASQA. Experimental results demonstrate that QUITO significantly outperforms established baselines across various datasets and downstream LLMs, underscoring its effectiveness. Our code is available at this https URL.</li>
</ul>

<h3>Title: Navigating Text-to-Image Generative Bias across Indic Languages</h3>
<ul>
<li><strong>Authors: </strong>Surbhi Mittal, Arnav Sudan, Mayank Vatsa, Richa Singh, Tamar Glaser, Tal Hassner</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00283">https://arxiv.org/abs/2408.00283</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00283">https://arxiv.org/pdf/2408.00283</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00283]] Navigating Text-to-Image Generative Bias across Indic Languages(https://arxiv.org/abs/2408.00283)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>This research investigates biases in text-to-image (TTI) models for the Indic languages widely spoken across India. It evaluates and compares the generative performance and cultural relevance of leading TTI models in these languages against their performance in English. Using the proposed IndicTTI benchmark, we comprehensively assess the performance of 30 Indic languages with two open-source diffusion models and two commercial generation APIs. The primary objective of this benchmark is to evaluate the support for Indic languages in these models and identify areas needing improvement. Given the linguistic diversity of 30 languages spoken by over 1.4 billion people, this benchmark aims to provide a detailed and insightful analysis of TTI models' effectiveness within the Indic linguistic landscape. The data and code for the IndicTTI benchmark can be accessed at this https URL.</li>
</ul>

<h3>Title: Bailing-TTS: Chinese Dialectal Speech Synthesis Towards Human-like Spontaneous Representation</h3>
<ul>
<li><strong>Authors: </strong>Xinhan Di, Zihao Chen, Yunming Liang, Junjie Zheng, Yihua Wang, Chaofan Ding</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00284">https://arxiv.org/abs/2408.00284</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00284">https://arxiv.org/pdf/2408.00284</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00284]] Bailing-TTS: Chinese Dialectal Speech Synthesis Towards Human-like Spontaneous Representation(https://arxiv.org/abs/2408.00284)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Large-scale text-to-speech (TTS) models have made significant progress recently.However, they still fall short in the generation of Chinese dialectal speech. Toaddress this, we propose Bailing-TTS, a family of large-scale TTS models capable of generating high-quality Chinese dialectal speech. Bailing-TTS serves as a foundation model for Chinese dialectal speech generation. First, continual semi-supervised learning is proposed to facilitate the alignment of text tokens and speech tokens. Second, the Chinese dialectal representation learning is developed using a specific transformer architecture and multi-stage training processes. With the proposed design of novel network architecture and corresponding strategy, Bailing-TTS is able to generate Chinese dialectal speech from text effectively and efficiently. Experiments demonstrate that Bailing-TTS generates Chinese dialectal speech towards human-like spontaneous representation. Readers are encouraged to listen to demos at \url{this https URL}.</li>
</ul>

<h3>Title: Diff3DETR:Agent-based Diffusion Model for Semi-supervised 3D Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Jiacheng Deng, Jiahao Lu, Tianzhu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00286">https://arxiv.org/abs/2408.00286</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00286">https://arxiv.org/pdf/2408.00286</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00286]] Diff3DETR:Agent-based Diffusion Model for Semi-supervised 3D Object Detection(https://arxiv.org/abs/2408.00286)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>3D object detection is essential for understanding 3D scenes. Contemporary techniques often require extensive annotated training data, yet obtaining point-wise annotations for point clouds is time-consuming and laborious. Recent developments in semi-supervised methods seek to mitigate this problem by employing a teacher-student framework to generate pseudo-labels for unlabeled point clouds. However, these pseudo-labels frequently suffer from insufficient diversity and inferior quality. To overcome these hurdles, we introduce an Agent-based Diffusion Model for Semi-supervised 3D Object Detection (Diff3DETR). Specifically, an agent-based object query generator is designed to produce object queries that effectively adapt to dynamic scenes while striking a balance between sampling locations and content embedding. Additionally, a box-aware denoising module utilizes the DDIM denoising process and the long-range attention in the transformer decoder to refine bounding boxes incrementally. Extensive experiments on ScanNet and SUN RGB-D datasets demonstrate that Diff3DETR outperforms state-of-the-art semi-supervised 3D object detection methods.</li>
</ul>

<h3>Title: Multi-Modal Parameter-Efficient Fine-tuning via Graph Neural Network</h3>
<ul>
<li><strong>Authors: </strong>Bin Cheng, Jiaxuan Lu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00290">https://arxiv.org/abs/2408.00290</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00290">https://arxiv.org/pdf/2408.00290</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00290]] Multi-Modal Parameter-Efficient Fine-tuning via Graph Neural Network(https://arxiv.org/abs/2408.00290)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>With the advent of the era of foundation models, pre-training and fine-tuning have become common paradigms. Recently, parameter-efficient fine-tuning has garnered widespread attention due to its better balance between the number of learnable parameters and performance. However, some current parameter-efficient fine-tuning methods only model a single modality and lack the utilization of structural knowledge in downstream tasks. To address this issue, this paper proposes a multi-modal parameter-efficient fine-tuning method based on graph networks. Each image is fed into a multi-modal large language model (MLLM) to generate a text description. The image and its corresponding text description are then processed by a frozen image encoder and text encoder to generate image features and text features, respectively. A graph is constructed based on the similarity of the multi-modal feature nodes, and knowledge and relationships relevant to these features are extracted from each node. Additionally, Elastic Weight Consolidation (EWC) regularization is incorporated into the loss function to mitigate the problem of forgetting during task learning. The proposed model achieves test accuracies on the OxfordPets, Flowers102, and Food101 datasets that improve by 4.45%, 2.92%, and 0.23%, respectively. The code is available at this https URL.</li>
</ul>

<h3>Title: Towards Flexible Evaluation for Generative Visual Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Huishan Ji, Qingyi Si, Zheng Lin, Weiping Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00300">https://arxiv.org/abs/2408.00300</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00300">https://arxiv.org/pdf/2408.00300</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00300]] Towards Flexible Evaluation for Generative Visual Question Answering(https://arxiv.org/abs/2408.00300)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Throughout rapid development of multimodal large language models, a crucial ingredient is a fair and accurate evaluation of their multimodal comprehension abilities. Although Visual Question Answering (VQA) could serve as a developed test field, limitations of VQA evaluation, like the inflexible pattern of Exact Match, have hindered MLLMs from demonstrating their real capability and discourage rich responses. Therefore, this paper proposes the use of semantics-based evaluators for assessing unconstrained open-ended responses on VQA datasets. As characteristics of VQA have made such evaluation significantly different than the traditional Semantic Textual Similarity (STS) task, to systematically analyze the behaviour and compare the performance of various evaluators including LLM-based ones, we proposes three key properties, i.e., Alignment, Consistency and Generalization, and a corresponding dataset Assessing VQA Evaluators (AVE) to facilitate analysis. In addition, this paper proposes a Semantically Flexible VQA Evaluator (SFVE) with meticulous design based on the unique features of VQA evaluation. Experimental results verify the feasibility of model-based VQA evaluation and effectiveness of the proposed evaluator that surpasses existing semantic evaluators by a large margin. The proposed training scheme generalizes to both the BERT-like encoders and decoder-only LLM.</li>
</ul>

<h3>Title: ADBM: Adversarial diffusion bridge model for reliable adversarial purification</h3>
<ul>
<li><strong>Authors: </strong>Xiao Li, Wenxuan Sun, Huanran Chen, Qiongxiu Li, Yining Liu, Yingzhe He, Jie Shi, Xiaolin Hu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00315">https://arxiv.org/abs/2408.00315</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00315">https://arxiv.org/pdf/2408.00315</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00315]] ADBM: Adversarial diffusion bridge model for reliable adversarial purification(https://arxiv.org/abs/2408.00315)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recently Diffusion-based Purification (DiffPure) has been recognized as an effective defense method against adversarial examples. However, we find DiffPure which directly employs the original pre-trained diffusion models for adversarial purification, to be suboptimal. This is due to an inherent trade-off between noise purification performance and data recovery quality. Additionally, the reliability of existing evaluations for DiffPure is questionable, as they rely on weak adaptive attacks. In this work, we propose a novel Adversarial Diffusion Bridge Model, termed ADBM. ADBM directly constructs a reverse bridge from the diffused adversarial data back to its original clean examples, enhancing the purification capabilities of the original diffusion models. Through theoretical analysis and experimental validation across various scenarios, ADBM has proven to be a superior and robust defense mechanism, offering significant promise for practical applications.</li>
</ul>

<h3>Title: DECIDER: Leveraging Foundation Model Priors for Improved Model Failure Detection and Explanation</h3>
<ul>
<li><strong>Authors: </strong>Rakshith Subramanyam, Kowshik Thopalli, Vivek Narayanaswamy, Jayaraman J.Thiagarajan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00331">https://arxiv.org/abs/2408.00331</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00331">https://arxiv.org/pdf/2408.00331</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00331]] DECIDER: Leveraging Foundation Model Priors for Improved Model Failure Detection and Explanation(https://arxiv.org/abs/2408.00331)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Reliably detecting when a deployed machine learning model is likely to fail on a given input is crucial for ensuring safe operation. In this work, we propose DECIDER (Debiasing Classifiers to Identify Errors Reliably), a novel approach that leverages priors from large language models (LLMs) and vision-language models (VLMs) to detect failures in image classification models. DECIDER utilizes LLMs to specify task-relevant core attributes and constructs a ``debiased'' version of the classifier by aligning its visual features to these core attributes using a VLM, and detects potential failure by measuring disagreement between the original and debiased models. In addition to proactively identifying samples on which the model would fail, DECIDER also provides human-interpretable explanations for failure through a novel attribute-ablation strategy. Through extensive experiments across diverse benchmarks spanning subpopulation shifts (spurious correlations, class imbalance) and covariate shifts (synthetic corruptions, domain shifts), DECIDER consistently achieves state-of-the-art failure detection performance, significantly outperforming baselines in terms of the overall Matthews correlation coefficient as well as failure and success recall. Our codes can be accessed at~\url{this https URL}</li>
</ul>

<h3>Title: Advancing Medical Image Segmentation: Morphology-Driven Learning with Diffusion Transformer</h3>
<ul>
<li><strong>Authors: </strong>Sungmin Kang, Jaeha Song, Jihie Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00347">https://arxiv.org/abs/2408.00347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00347">https://arxiv.org/pdf/2408.00347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00347]] Advancing Medical Image Segmentation: Morphology-Driven Learning with Diffusion Transformer(https://arxiv.org/abs/2408.00347)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, self-supervised</a></li>
<li><strong>Abstract: </strong>Understanding the morphological structure of medical images and precisely segmenting the region of interest or abnormality is an important task that can assist in diagnosis. However, the unique properties of medical imaging make clear segmentation difficult, and the high cost and time-consuming task of labeling leads to a coarse-grained representation of ground truth. Facing with these problems, we propose a novel Diffusion Transformer Segmentation (DTS) model for robust segmentation in the presence of noise. We propose an alternative to the dominant Denoising U-Net encoder through experiments applying a transformer architecture, which captures global dependency through self-attention. Additionally, we propose k-neighbor label smoothing, reverse boundary attention, and self-supervised learning with morphology-driven learning to improve the ability to identify complex structures. Our model, which analyzes the morphological representation of images, shows better results than the previous models in various medical imaging modalities, including CT, MRI, and lesion images.</li>
</ul>

<h3>Title: A Simple Background Augmentation Method for Object Detection with Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Yuhang Li, Xin Dong, Chen Chen, Weiming Zhuang, Lingjuan Lyu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00350">https://arxiv.org/abs/2408.00350</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00350">https://arxiv.org/pdf/2408.00350</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00350]] A Simple Background Augmentation Method for Object Detection with Diffusion Model(https://arxiv.org/abs/2408.00350)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In computer vision, it is well-known that a lack of data diversity will impair model performance. In this study, we address the challenges of enhancing the dataset diversity problem in order to benefit various downstream tasks such as object detection and instance segmentation. We propose a simple yet effective data augmentation approach by leveraging advancements in generative models, specifically text-to-image synthesis technologies like Stable Diffusion. Our method focuses on generating variations of labeled real images, utilizing generative object and background augmentation via inpainting to augment existing training data without the need for additional annotations. We find that background augmentation, in particular, significantly improves the models' robustness and generalization capabilities. We also investigate how to adjust the prompt and mask to ensure the generated content comply with the existing annotations. The efficacy of our augmentation techniques is validated through comprehensive evaluations of the COCO dataset and several other key object detection benchmarks, demonstrating notable enhancements in model performance across diverse scenarios. This approach offers a promising solution to the challenges of dataset enhancement, contributing to the development of more accurate and robust computer vision models.</li>
</ul>

<h3>Title: Autonomous LLM-Enhanced Adversarial Attack for Text-to-Motion</h3>
<ul>
<li><strong>Authors: </strong>Honglei Miao, Fan Ma, Ruijie Quan, Kun Zhan, Yi Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00352">https://arxiv.org/abs/2408.00352</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00352">https://arxiv.org/pdf/2408.00352</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00352]] Autonomous LLM-Enhanced Adversarial Attack for Text-to-Motion(https://arxiv.org/abs/2408.00352)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Human motion generation driven by deep generative models has enabled compelling applications, but the ability of text-to-motion (T2M) models to produce realistic motions from text prompts raises security concerns if exploited maliciously. Despite growing interest in T2M, few methods focus on safeguarding these models against adversarial attacks, with existing work on text-to-image models proving insufficient for the unique motion domain. In the paper, we propose ALERT-Motion, an autonomous framework leveraging large language models (LLMs) to craft targeted adversarial attacks against black-box T2M models. Unlike prior methods modifying prompts through predefined rules, ALERT-Motion uses LLMs' knowledge of human motion to autonomously generate subtle yet powerful adversarial text descriptions. It comprises two key modules: an adaptive dispatching module that constructs an LLM-based agent to iteratively refine and search for adversarial prompts; and a multimodal information contrastive module that extracts semantically relevant motion information to guide the agent's search. Through this LLM-driven approach, ALERT-Motion crafts adversarial prompts querying victim models to produce outputs closely matching targeted motions, while avoiding obvious perturbations. Evaluations across popular T2M models demonstrate ALERT-Motion's superiority over previous methods, achieving higher attack success rates with stealthier adversarial prompts. This pioneering work on T2M adversarial attacks highlights the urgency of developing defensive measures as motion generation technology advances, urging further research into safe and responsible deployment.</li>
</ul>

<h3>Title: High-Precision Self-Supervised Monocular Depth Estimation with Rich-Resource Prior</h3>
<ul>
<li><strong>Authors: </strong>Wencheng Han, Jianbing Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00361">https://arxiv.org/abs/2408.00361</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00361">https://arxiv.org/pdf/2408.00361</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00361]] High-Precision Self-Supervised Monocular Depth Estimation with Rich-Resource Prior(https://arxiv.org/abs/2408.00361)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>In the area of self-supervised monocular depth estimation, models that utilize rich-resource inputs, such as high-resolution and multi-frame inputs, typically achieve better performance than models that use ordinary single image input. However, these rich-resource inputs may not always be available, limiting the applicability of these methods in general scenarios. In this paper, we propose Rich-resource Prior Depth estimator (RPrDepth), which only requires single input image during the inference phase but can still produce highly accurate depth estimations comparable to rich resource based methods. Specifically, we treat rich-resource data as prior information and extract features from it as reference features in an offline manner. When estimating the depth for a single-image image, we search for similar pixels from the rich-resource features and use them as prior information to estimate the depth. Experimental results demonstrate that our model outperform other single-image model and can achieve comparable or even better performance than models with rich-resource inputs, only using low-resolution single-image input.</li>
</ul>

<h3>Title: Few-shot Defect Image Generation based on Consistency Modeling</h3>
<ul>
<li><strong>Authors: </strong>Qingfeng Shi, Jing Wei, Fei Shen, Zhengtao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00372">https://arxiv.org/abs/2408.00372</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00372">https://arxiv.org/pdf/2408.00372</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00372]] Few-shot Defect Image Generation based on Consistency Modeling(https://arxiv.org/abs/2408.00372)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Image generation can solve insufficient labeled data issues in defect detection. Most defect generation methods are only trained on a single product without considering the consistencies among multiple products, leading to poor quality and diversity of generated results. To address these issues, we propose DefectDiffu, a novel text-guided diffusion method to model both intra-product background consistency and inter-product defect consistency across multiple products and modulate the consistency perturbation directions to control product type and defect strength, achieving diversified defect image generation. Firstly, we leverage a text encoder to separately provide consistency prompts for background, defect, and fusion parts of the disentangled integrated architecture, thereby disentangling defects and normal backgrounds. Secondly, we propose the double-free strategy to generate defect images through two-stage perturbation of consistency direction, thereby controlling product type and defect strength by adjusting the perturbation scale. Besides, DefectDiffu can generate defect mask annotations utilizing cross-attention maps from the defect part. Finally, to improve the generation quality of small defects and masks, we propose the adaptive attention-enhance loss to increase the attention to defects. Experimental results demonstrate that DefectDiffu surpasses state-of-the-art methods in terms of generation quality and diversity, thus effectively improving downstream defection performance. Moreover, defect perturbation directions can be transferred among various products to achieve zero-shot defect generation, which is highly beneficial for addressing insufficient data issues. The code are available at this https URL.</li>
</ul>

<h3>Title: On the Limitations and Prospects of Machine Unlearning for Generative AI</h3>
<ul>
<li><strong>Authors: </strong>Shiji Zhou, Lianzhe Wang, Jiangnan Ye, Yongliang Wu, Heng Chang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00376">https://arxiv.org/abs/2408.00376</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00376">https://arxiv.org/pdf/2408.00376</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00376]] On the Limitations and Prospects of Machine Unlearning for Generative AI(https://arxiv.org/abs/2408.00376)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generative AI (GenAI), which aims to synthesize realistic and diverse data samples from latent variables or other data modalities, has achieved remarkable results in various domains, such as natural language, images, audio, and graphs. However, they also pose challenges and risks to data privacy, security, and ethics. Machine unlearning is the process of removing or weakening the influence of specific data samples or features from a trained model, without affecting its performance on other data or tasks. While machine unlearning has shown significant efficacy in traditional machine learning tasks, it is still unclear if it could help GenAI become safer and aligned with human desire. To this end, this position paper provides an in-depth discussion of the machine unlearning approaches for GenAI. Firstly, we formulate the problem of machine unlearning tasks on GenAI and introduce the background. Subsequently, we systematically examine the limitations of machine unlearning on GenAI models by focusing on the two representative branches: LLMs and image generative (diffusion) models. Finally, we provide our prospects mainly from three aspects: benchmark, evaluation metrics, and utility-unlearning trade-off, and conscientiously advocate for the future development of this field.</li>
</ul>

<h3>Title: Enhancing Whole Slide Pathology Foundation Models through Stain Normalization</h3>
<ul>
<li><strong>Authors: </strong>Juseung Yun, Yi Hu, Jinhyung Kim, Jongseong Jang, Soonyoung Lee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00380">https://arxiv.org/abs/2408.00380</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00380">https://arxiv.org/pdf/2408.00380</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00380]] Enhancing Whole Slide Pathology Foundation Models through Stain Normalization(https://arxiv.org/abs/2408.00380)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>Recent advancements in digital pathology have led to the development of numerous foundational models that utilize self-supervised learning on patches extracted from gigapixel whole slide images (WSIs). While this approach leverages vast amounts of unlabeled data, we have discovered a significant issue: features extracted from these self-supervised models tend to cluster by individual WSIs, a phenomenon we term WSI-specific feature collapse. This problem can potentially limit the model's generalization ability and performance on various downstream tasks. To address this issue, we introduce Stain Normalized Pathology Foundational Model, a novel foundational model trained on patches that have undergone stain normalization. Stain normalization helps reduce color variability arising from different laboratories and scanners, enabling the model to learn more consistent features. Stain Normalized Pathology Foundational Model is trained using 285,153,903 patches extracted from a total of 34,795 WSIs, combining data from The Cancer Genome Atlas (TCGA) and the Genotype-Tissue Expression (GTEx) project. Our experiments demonstrate that Stain Normalized Pathology Foundational Model significantly mitigates the feature collapse problem, indicating that the model has learned more generalized features rather than overfitting to individual WSI characteristics. We compared Stain Normalized Pathology Foundational Model with state-of-the-art models across six downstream task datasets, and our results show that \name{} achieves excellent performance relative to the number of WSIs used and the model's parameter count. This suggests that the application of stain normalization has substantially improved the model's efficiency and generalization capabilities.</li>
</ul>

<h3>Title: Deepfake Media Forensics: State of the Art and Challenges Ahead</h3>
<ul>
<li><strong>Authors: </strong>Irene Amerini, Mauro Barni, Sebastiano Battiato, Paolo Bestagini, Giulia Boato, Tania Sari Bonaventura, Vittoria Bruni, Roberto Caldelli, Francesco De Natale, Rocco De Nicola, Luca Guarnera, Sara Mandelli, Gian Luca Marcialis, Marco Micheletto, Andrea Montibeller, Giulia Orru', Alessandro Ortis, Pericle Perazzo, Davide Salvi, Stefano Tubaro, Claudia Melis Tonti, Massimo Villari, Domenico Vitulano</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00388">https://arxiv.org/abs/2408.00388</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00388">https://arxiv.org/pdf/2408.00388</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00388]] Deepfake Media Forensics: State of the Art and Challenges Ahead(https://arxiv.org/abs/2408.00388)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>AI-generated synthetic media, also called Deepfakes, have significantly influenced so many domains, from entertainment to cybersecurity. Generative Adversarial Networks (GANs) and Diffusion Models (DMs) are the main frameworks used to create Deepfakes, producing highly realistic yet fabricated content. While these technologies open up new creative possibilities, they also bring substantial ethical and security risks due to their potential misuse. The rise of such advanced media has led to the development of a cognitive bias known as Impostor Bias, where individuals doubt the authenticity of multimedia due to the awareness of AI's capabilities. As a result, Deepfake detection has become a vital area of research, focusing on identifying subtle inconsistencies and artifacts with machine learning techniques, especially Convolutional Neural Networks (CNNs). Research in forensic Deepfake technology encompasses five main areas: detection, attribution and recognition, passive authentication, detection in realistic scenarios, and active authentication. Each area tackles specific challenges, from tracing the origins of synthetic media and examining its inherent characteristics for authenticity. This paper reviews the primary algorithms that address these challenges, examining their advantages, limitations, and future prospects.</li>
</ul>

<h3>Title: In-Context Example Selection via Similarity Search Improves Low-Resource Machine Translation</h3>
<ul>
<li><strong>Authors: </strong>Armel Zebaze, Benot Sagot, Rachel Bawden</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00397">https://arxiv.org/abs/2408.00397</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00397">https://arxiv.org/pdf/2408.00397</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00397]] In-Context Example Selection via Similarity Search Improves Low-Resource Machine Translation(https://arxiv.org/abs/2408.00397)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, in-context</a></li>
<li><strong>Abstract: </strong>The ability of generative large language models (LLMs) to perform in-context learning has given rise to a large body of research into how best to prompt models for various natural language processing tasks. In this paper, we focus on machine translation (MT), a task that has been shown to benefit from in-context translation examples. However no systematic studies have been published on how best to select examples, and mixed results have been reported on the usefulness of similarity-based selection over random selection. We provide a study covering multiple LLMs and multiple in-context example retrieval strategies, comparing multilingual sentence embeddings. We cover several language directions, representing different levels of language resourcedness (English into French, German, Swahili and Wolof). Contrarily to previously published results, we find that sentence embedding similarity can improve MT, especially for low-resource language directions, and discuss the balance between selection pool diversity and quality. We also highlight potential problems with the evaluation of LLM-based MT and suggest a more appropriate evaluation protocol, adapting the COMET metric to the evaluation of LLMs. Code and outputs are freely available at this https URL.</li>
</ul>

<h3>Title: Towards Reliable Advertising Image Generation Using Human Feedback</h3>
<ul>
<li><strong>Authors: </strong>Zhenbang Du, Wei Feng, Haohan Wang, Yaoyu Li, Jingsen Wang, Jian Li, Zheng Zhang, Jingjing Lv, Xin Zhu, Junsheng Jin, Junjie Shen, Zhangang Lin, Jingping Shao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00418">https://arxiv.org/abs/2408.00418</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00418">https://arxiv.org/pdf/2408.00418</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00418]] Towards Reliable Advertising Image Generation Using Human Feedback(https://arxiv.org/abs/2408.00418)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In the e-commerce realm, compelling advertising images are pivotal for attracting customer attention. While generative models automate image generation, they often produce substandard images that may mislead customers and require significant labor costs to inspect. This paper delves into increasing the rate of available generated images. We first introduce a multi-modal Reliable Feedback Network (RFNet) to automatically inspect the generated images. Combining the RFNet into a recurrent process, Recurrent Generation, results in a higher number of available advertising images. To further enhance production efficiency, we fine-tune diffusion models with an innovative Consistent Condition regularization utilizing the feedback from RFNet (RFFT). This results in a remarkable increase in the available rate of generated images, reducing the number of attempts in Recurrent Generation, and providing a highly efficient production process without sacrificing visual appeal. We also construct a Reliable Feedback 1 Million (RF1M) dataset which comprises over one million generated advertising images annotated by human, which helps to train RFNet to accurately assess the availability of generated images and faithfully reflect the human feedback. Generally speaking, our approach offers a reliable solution for advertising image generation.</li>
</ul>

<h3>Title: Graph Representation Learning via Causal Diffusion for Out-of-Distribution Recommendation</h3>
<ul>
<li><strong>Authors: </strong>Chu Zhao, Enneng Yang, Yuliang Liang, Pengxiang Lan, Yuting Liu, Jianzhe Zhao, Guibing Guo, Xingwei Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.IR, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00490">https://arxiv.org/abs/2408.00490</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00490">https://arxiv.org/pdf/2408.00490</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00490]] Graph Representation Learning via Causal Diffusion for Out-of-Distribution Recommendation(https://arxiv.org/abs/2408.00490)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Graph Neural Networks (GNNs)-based recommendation algorithms typically assume that training and testing data are drawn from independent and identically distributed (IID) spaces. However, this assumption often fails in the presence of out-of-distribution (OOD) data, resulting in significant performance degradation. In this study, we construct a Structural Causal Model (SCM) to analyze interaction data, revealing that environmental confounders (e.g., the COVID-19 pandemic) lead to unstable correlations in GNN-based models, thus impairing their generalization to OOD data. To address this issue, we propose a novel approach, graph representation learning via causal diffusion (CausalDiffRec) for OOD recommendation. This method enhances the model's generalization on OOD data by eliminating environmental confounding factors and learning invariant graph representations. Specifically, we use backdoor adjustment and variational inference to infer the real environmental distribution, thereby eliminating the impact of environmental confounders. This inferred distribution is then used as prior knowledge to guide the representation learning in the reverse phase of the diffusion process to learn the invariant representation. In addition, we provide a theoretical derivation that proves optimizing the objective function of CausalDiffRec can encourage the model to learn environment-invariant graph representations, thereby achieving excellent generalization performance in recommendations under distribution shifts. Our extensive experiments validate the effectiveness of CausalDiffRec in improving the generalization of OOD data, and the average improvement is up to 10.69% on Food, 18.83% on KuaiRec, 22.41% on Yelp2018, and 11.65% on Douban datasets.</li>
</ul>

<h3>Title: How Effective are Self-Supervised Models for Contact Identification in Videos</h3>
<ul>
<li><strong>Authors: </strong>Malitha Gunawardhana, Limalka Sadith, Liel David, Daniel Harari, Muhammad Haris Khan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00498">https://arxiv.org/abs/2408.00498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00498">https://arxiv.org/pdf/2408.00498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00498]] How Effective are Self-Supervised Models for Contact Identification in Videos(https://arxiv.org/abs/2408.00498)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>The exploration of video content via Self-Supervised Learning (SSL) models has unveiled a dynamic field of study, emphasizing both the complex challenges and unique opportunities inherent in this area. Despite the growing body of research, the ability of SSL models to detect physical contacts in videos remains largely unexplored, particularly the effectiveness of methods such as downstream supervision with linear probing or full fine-tuning. This work aims to bridge this gap by employing eight different convolutional neural networks (CNNs) based video SSL models to identify instances of physical contact within video sequences specifically. The Something-Something v2 (SSv2) and Epic-Kitchen (EK-100) datasets were chosen for evaluating these approaches due to the promising results on UCF101 and HMDB51, coupled with their limited prior assessment on SSv2 and EK-100. Additionally, these datasets feature diverse environments and scenarios, essential for testing the robustness and accuracy of video-based models. This approach not only examines the effectiveness of each model in recognizing physical contacts but also explores the performance in the action recognition downstream task. By doing so, valuable insights into the adaptability of SSL models in interpreting complex, dynamic visual information are contributed.</li>
</ul>

<h3>Title: Jailbreaking Text-to-Image Models with LLM-Based Agents</h3>
<ul>
<li><strong>Authors: </strong>Yingkai Dong, Zheng Li, Xiangtao Meng, Ning Yu, Shanqing Guo</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00523">https://arxiv.org/abs/2408.00523</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00523">https://arxiv.org/pdf/2408.00523</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00523]] Jailbreaking Text-to-Image Models with LLM-Based Agents(https://arxiv.org/abs/2408.00523)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, in-context</a></li>
<li><strong>Abstract: </strong>Recent advancements have significantly improved automated task-solving capabilities using autonomous agents powered by large language models (LLMs). However, most LLM-based agents focus on dialogue, programming, or specialized domains, leaving gaps in addressing generative AI safety tasks. These gaps are primarily due to the challenges posed by LLM hallucinations and the lack of clear guidelines. In this paper, we propose Atlas, an advanced LLM-based multi-agent framework that integrates an efficient fuzzing workflow to target generative AI models, specifically focusing on jailbreak attacks against text-to-image (T2I) models with safety filters. Atlas utilizes a vision-language model (VLM) to assess whether a prompt triggers the T2I model's safety filter. It then iteratively collaborates with both LLM and VLM to generate an alternative prompt that bypasses the filter. Atlas also enhances the reasoning abilities of LLMs in attack scenarios by leveraging multi-agent communication, in-context learning (ICL) memory mechanisms, and the chain-of-thought (COT) approach. Our evaluation demonstrates that Atlas successfully jailbreaks several state-of-the-art T2I models in a black-box setting, which are equipped with multi-modal safety filters. In addition, Atlas outperforms existing methods in both query efficiency and the quality of the generated images.</li>
</ul>

<h3>Title: Contrastive Learning with Dynamic Localized Repulsion for Brain Age Prediction on 3D Stiffness Maps</h3>
<ul>
<li><strong>Authors: </strong>Jakob Truble, Lucy Hiscox, Curtis Johnson, Carola-Bibiane Schnlieb, Gabriele Kaminski Schierle, Angelica Aviles-Rivero</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00527">https://arxiv.org/abs/2408.00527</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00527">https://arxiv.org/pdf/2408.00527</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00527]] Contrastive Learning with Dynamic Localized Repulsion for Brain Age Prediction on 3D Stiffness Maps(https://arxiv.org/abs/2408.00527)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, generative</a></li>
<li><strong>Abstract: </strong>In the field of neuroimaging, accurate brain age prediction is pivotal for uncovering the complexities of brain aging and pinpointing early indicators of neurodegenerative conditions. Recent advancements in self-supervised learning, particularly in contrastive learning, have demonstrated greater robustness when dealing with complex datasets. However, current approaches often fall short in generalizing across non-uniformly distributed data, prevalent in medical imaging scenarios. To bridge this gap, we introduce a novel contrastive loss that adapts dynamically during the training process, focusing on the localized neighborhoods of samples. Moreover, we expand beyond traditional structural features by incorporating brain stiffness, a mechanical property previously underexplored yet promising due to its sensitivity to age-related changes. This work presents the first application of self-supervised learning to brain mechanical properties, using compiled stiffness maps from various clinical studies to predict brain age. Our approach, featuring dynamic localized loss, consistently outperforms existing state-of-the-art methods, demonstrating superior performance and laying the way for new directions in brain aging research.</li>
</ul>

<h3>Title: Intermittent Semi-working Mask: A New Masking Paradigm for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Mingcong Lu, Jiangcai Zhu, Wang Hao, Zheng Li, Shusheng Zhang, Kailai Shao, Chao Chen, Nan Li, Feng Wang, Xin Lu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00539">https://arxiv.org/abs/2408.00539</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00539">https://arxiv.org/pdf/2408.00539</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00539]] Intermittent Semi-working Mask: A New Masking Paradigm for LLMs(https://arxiv.org/abs/2408.00539)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Multi-turn dialogues are a key interaction method between humans and Large Language Models (LLMs), as conversations extend over multiple rounds, keeping LLMs' high generation quality and low latency is a challenge. Mainstream LLMs can be grouped into two categories based on masking strategy: causal LLM and prefix LLM. Several works have demonstrated that prefix LLMs tend to outperform causal ones in scenarios that heavily depend on historical context such as multi-turn dialogues or in-context learning, thanks to their bidirectional attention on prefix sequences. However, prefix LLMs have an inherent inefficient training problem in multi-turn dialogue datasets. In addition, the attention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV Cache) across dialogue rounds to reduce generation latency. In this paper, we propose a novel masking scheme called Intermittent Semi-working Mask (ISM) to address these problems. Specifically, we apply alternate bidirectional and unidirectional attention on queries and answers in the dialogue history. In this way, ISM is able to maintain the high quality of prefix LLM and low generation latency of causal LLM, simultaneously. Extensive experiments illustrate that our ISM achieves significant performance.</li>
</ul>

<h3>Title: Privacy-preserving datasets by capturing feature distributions with Conditional VAEs</h3>
<ul>
<li><strong>Authors: </strong>Francesco Di Salvo, David Tafler, Sebastian Doerrich, Christian Ledig</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00639">https://arxiv.org/abs/2408.00639</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00639">https://arxiv.org/pdf/2408.00639</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00639]] Privacy-preserving datasets by capturing feature distributions with Conditional VAEs(https://arxiv.org/abs/2408.00639)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, generative</a></li>
<li><strong>Abstract: </strong>Large and well-annotated datasets are essential for advancing deep learning applications, however often costly or impossible to obtain by a single entity. In many areas, including the medical domain, approaches relying on data sharing have become critical to address those challenges. While effective in increasing dataset size and diversity, data sharing raises significant privacy concerns. Commonly employed anonymization methods based on the k-anonymity paradigm often fail to preserve data diversity, affecting model robustness. This work introduces a novel approach using Conditional Variational Autoencoders (CVAEs) trained on feature vectors extracted from large pre-trained vision foundation models. Foundation models effectively detect and represent complex patterns across diverse domains, allowing the CVAE to faithfully capture the embedding space of a given data distribution to generate (sample) a diverse, privacy-respecting, and potentially unbounded set of synthetic feature vectors. Our method notably outperforms traditional approaches in both medical and natural image domains, exhibiting greater dataset diversity and higher robustness against perturbations while preserving sample privacy. These results underscore the potential of generative models to significantly impact deep learning applications in data-scarce and privacy-sensitive environments. The source code is available at this https URL .</li>
</ul>

<h3>Title: Enhancing Ethereum Fraud Detection via Generative and Contrastive Self-supervision</h3>
<ul>
<li><strong>Authors: </strong>Chenxiang Jin, Jiajun Zhou, Chenxuan Xie, Shanqing Yu, Qi Xuan, Xiaoniu Yang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00641">https://arxiv.org/abs/2408.00641</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00641">https://arxiv.org/pdf/2408.00641</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00641]] Enhancing Ethereum Fraud Detection via Generative and Contrastive Self-supervision(https://arxiv.org/abs/2408.00641)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rampant fraudulent activities on Ethereum hinder the healthy development of the blockchain ecosystem, necessitating the reinforcement of regulations. However, multiple imbalances involving account interaction frequencies and interaction types in the Ethereum transaction environment pose significant challenges to data mining-based fraud detection research. To address this, we first propose the concept of meta-interactions to refine interaction behaviors in Ethereum, and based on this, we present a dual self-supervision enhanced Ethereum fraud detection framework, named Meta-IFD. This framework initially introduces a generative self-supervision mechanism to augment the interaction features of accounts, followed by a contrastive self-supervision mechanism to differentiate various behavior patterns, and ultimately characterizes the behavioral representations of accounts and mines potential fraud risks through multi-view interaction feature learning. Extensive experiments on real Ethereum datasets demonstrate the effectiveness and superiority of our framework in detecting common Ethereum fraud behaviors such as Ponzi schemes and phishing scams. Additionally, the generative module can effectively alleviate the interaction distribution imbalance in Ethereum data, while the contrastive module significantly enhances the framework's ability to distinguish different behavior patterns. The source code will be released on GitHub soon.</li>
</ul>

<h3>Title: You Can't Ignore Either: Unifying Structure and Feature Denoising for Robust Graph Learning</h3>
<ul>
<li><strong>Authors: </strong>Tianmeng Yang, Jiahao Meng, Min Zhou, Yaming Yang, Yujing Wang, Xiangtai Li, Yunhai Tong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00700">https://arxiv.org/abs/2408.00700</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00700">https://arxiv.org/pdf/2408.00700</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00700]] You Can't Ignore Either: Unifying Structure and Feature Denoising for Robust Graph Learning(https://arxiv.org/abs/2408.00700)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Recent research on the robustness of Graph Neural Networks (GNNs) under noises or attacks has attracted great attention due to its importance in real-world applications. Most previous methods explore a single noise source, recovering corrupt node embedding by reliable structures bias or developing structure learning with reliable node features. However, the noises and attacks may come from both structures and features in graphs, making the graph denoising a dilemma and challenging problem. In this paper, we develop a unified graph denoising (UGD) framework to unravel the deadlock between structure and feature denoising. Specifically, a high-order neighborhood proximity evaluation method is proposed to recognize noisy edges, considering features may be perturbed simultaneously. Moreover, we propose to refine noisy features with reconstruction based on a graph auto-encoder. An iterative updating algorithm is further designed to optimize the framework and acquire a clean graph, thus enabling robust graph learning for downstream tasks. Our UGD framework is self-supervised and can be easily implemented as a plug-and-play module. We carry out extensive experiments, which proves the effectiveness and advantages of our method. Code is avalaible at this https URL.</li>
</ul>

<h3>Title: Synthetic dual image generation for reduction of labeling efforts in semantic segmentation of micrographs with a customized metric function</h3>
<ul>
<li><strong>Authors: </strong>Matias Oscar Volman Stern, Dominic Hohs, Andreas Jansche, Timo Bernthaler, Gerhard Schneider</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CE, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00707">https://arxiv.org/abs/2408.00707</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00707">https://arxiv.org/pdf/2408.00707</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00707]] Synthetic dual image generation for reduction of labeling efforts in semantic segmentation of micrographs with a customized metric function(https://arxiv.org/abs/2408.00707)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Training of semantic segmentation models for material analysis requires micrographs and their corresponding masks. It is quite unlikely that perfect masks will be drawn, especially at the edges of objects, and sometimes the amount of data that can be obtained is small, since only a few samples are available. These aspects make it very problematic to train a robust model. We demonstrate a workflow for the improvement of semantic segmentation models of micrographs through the generation of synthetic microstructural images in conjunction with masks. The workflow only requires joining a few micrographs with their respective masks to create the input for a Vector Quantised-Variational AutoEncoder model that includes an embedding space, which is trained such that a generative model (PixelCNN) learns the distribution of each input, transformed into discrete codes, and can be used to sample new codes. The latter will eventually be decoded by VQ-VAE to generate images alongside corresponding masks for semantic segmentation. To evaluate the synthetic data, we have trained U-Net models with different amounts of these synthetic data in conjunction with real data. These models were then evaluated using non-synthetic images only. Additionally, we introduce a customized metric derived from the mean Intersection over Union (mIoU). The proposed metric prevents a few falsely predicted pixels from greatly reducing the value of the mIoU. We have achieved a reduction in sample preparation and acquisition times, as well as the efforts, needed for image processing and labeling tasks, are less when it comes to training semantic segmentation model. The approach could be generalized to various types of image data such that it serves as a user-friendly solution for training models with a small number of real images.</li>
</ul>

<h3>Title: MotionFix: Text-Driven 3D Human Motion Editing</h3>
<ul>
<li><strong>Authors: </strong>Nikos Athanasiou, Alpr Ceske, Markos Diomataris, Michael J. Black, Gl Varol</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00712">https://arxiv.org/abs/2408.00712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00712">https://arxiv.org/pdf/2408.00712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00712]] MotionFix: Text-Driven 3D Human Motion Editing(https://arxiv.org/abs/2408.00712)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The focus of this paper is 3D motion editing. Given a 3D human motion and a textual description of the desired modification, our goal is to generate an edited motion as described by the text. The challenges include the lack of training data and the design of a model that faithfully edits the source motion. In this paper, we address both these challenges. We build a methodology to semi-automatically collect a dataset of triplets in the form of (i) a source motion, (ii) a target motion, and (iii) an edit text, and create the new MotionFix dataset. Having access to such data allows us to train a conditional diffusion model, TMED, that takes both the source motion and the edit text as input. We further build various baselines trained only on text-motion pairs datasets, and show superior performance of our model trained on triplets. We introduce new retrieval-based metrics for motion editing and establish a new benchmark on the evaluation set of MotionFix. Our results are encouraging, paving the way for further research on finegrained motion generation. Code and models will be made publicly available.</li>
</ul>

<h3>Title: SAM 2: Segment Anything in Images and Videos</h3>
<ul>
<li><strong>Authors: </strong>Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rdle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollr, Christoph Feichtenhofer</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00714">https://arxiv.org/abs/2408.00714</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00714">https://arxiv.org/pdf/2408.00714</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00714]] SAM 2: Segment Anything in Images and Videos(https://arxiv.org/abs/2408.00714)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing a version of our model, the dataset and an interactive demo.</li>
</ul>

<h3>Title: TurboEdit: Text-Based Image Editing Using Few-Step Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Gilad Deutch, Rinon Gal, Daniel Garibi, Or Patashnik, Daniel Cohen-Or</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00735">https://arxiv.org/abs/2408.00735</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00735">https://arxiv.org/pdf/2408.00735</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00735]] TurboEdit: Text-Based Image Editing Using Few-Step Diffusion Models(https://arxiv.org/abs/2408.00735)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have opened the path to a wide range of text-based image editing frameworks. However, these typically build on the multi-step nature of the diffusion backwards process, and adapting them to distilled, fast-sampling methods has proven surprisingly challenging. Here, we focus on a popular line of text-based editing frameworks - the ``edit-friendly'' DDPM-noise inversion approach. We analyze its application to fast sampling methods and categorize its failures into two classes: the appearance of visual artifacts, and insufficient editing strength. We trace the artifacts to mismatched noise statistics between inverted noises and the expected noise schedule, and suggest a shifted noise schedule which corrects for this offset. To increase editing strength, we propose a pseudo-guidance approach that efficiently increases the magnitude of edits without introducing new artifacts. All in all, our method enables text-based image editing with as few as three diffusion steps, while providing novel insights into the mechanisms behind popular text-based editing approaches.</li>
</ul>

<h3>Title: Virchow 2: Scaling Self-Supervised Mixed Magnification Models in Pathology</h3>
<ul>
<li><strong>Authors: </strong>Eric Zimmermann, Eugene Vorontsov, Julian Viret, Adam Casson, Michal Zelechowski, George Shaikovski, Neil Tenenholtz, James Hall, Thomas Fuchs, Nicolo Fusi, Siqi Liu, Kristen Severson</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00738">https://arxiv.org/abs/2408.00738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00738">https://arxiv.org/pdf/2408.00738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00738]] Virchow 2: Scaling Self-Supervised Mixed Magnification Models in Pathology(https://arxiv.org/abs/2408.00738)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>Foundation models are rapidly being developed for computational pathology applications. However, it remains an open question which factors are most important for downstream performance with data scale and diversity, model size, and training algorithm all playing a role. In this work, we present the result of scaling both data and model size, surpassing previous studies in both dimensions, and introduce two new models: Virchow 2, a 632M parameter vision transformer, and Virchow 2G, a 1.85B parameter vision transformer, each trained with 3.1M histopathology whole slide images. To support this scale, we propose domain-inspired adaptations to the DINOv2 training algorithm, which is quickly becoming the default method in self-supervised learning for computational pathology. We achieve state of the art performance on twelve tile-level tasks, as compared to the top performing competing models. Our results suggest that data diversity and domain-specific training can outperform models that only scale in the number of parameters, but, on average, performance benefits from domain-tailoring, data scale, and model scale.</li>
</ul>

<h3>Title: Smoothed Energy Guidance: Guiding Diffusion Models with Reduced Energy Curvature of Attention</h3>
<ul>
<li><strong>Authors: </strong>Susung Hong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00760">https://arxiv.org/abs/2408.00760</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00760">https://arxiv.org/pdf/2408.00760</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00760]] Smoothed Energy Guidance: Guiding Diffusion Models with Reduced Energy Curvature of Attention(https://arxiv.org/abs/2408.00760)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Conditional diffusion models have shown remarkable success in visual content generation, producing high-quality samples across various domains, largely due to classifier-free guidance (CFG). Recent attempts to extend guidance to unconditional models have relied on heuristic techniques, resulting in suboptimal generation quality and unintended effects. In this work, we propose Smoothed Energy Guidance (SEG), a novel training- and condition-free approach that leverages the energy-based perspective of the self-attention mechanism to enhance image generation. By defining the energy of self-attention, we introduce a method to reduce the curvature of the energy landscape of attention and use the output as the unconditional prediction. Practically, we control the curvature of the energy landscape by adjusting the Gaussian kernel parameter while keeping the guidance scale parameter fixed. Additionally, we present a query blurring method that is equivalent to blurring the entire attention weights without incurring quadratic complexity in the number of tokens. In our experiments, SEG achieves a Pareto improvement in both quality and the reduction of side effects. The code is available at \url{this https URL}.</li>
</ul>

<h3>Title: UniTalker: Scaling up Audio-Driven 3D Facial Animation through A Unified Model</h3>
<ul>
<li><strong>Authors: </strong>Xiangyu Fan, Jiaqi Li, Zhiqian Lin, Weiye Xiao, Lei Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00762">https://arxiv.org/abs/2408.00762</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00762">https://arxiv.org/pdf/2408.00762</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00762]] UniTalker: Scaling up Audio-Driven 3D Facial Animation through A Unified Model(https://arxiv.org/abs/2408.00762)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Audio-driven 3D facial animation aims to map input audio to realistic facial motion. Despite significant progress, limitations arise from inconsistent 3D annotations, restricting previous models to training on specific annotations and thereby constraining the training scale. In this work, we present UniTalker, a unified model featuring a multi-head architecture designed to effectively leverage datasets with varied annotations. To enhance training stability and ensure consistency among multi-head outputs, we employ three training strategies, namely, PCA, model warm-up, and pivot identity embedding. To expand the training scale and diversity, we assemble A2F-Bench, comprising five publicly available datasets and three newly curated datasets. These datasets contain a wide range of audio domains, covering multilingual speech voices and songs, thereby scaling the training data from commonly employed datasets, typically less than 1 hour, to 18.5 hours. With a single trained UniTalker model, we achieve substantial lip vertex error reductions of 9.2% for BIWI dataset and 13.7% for Vocaset. Additionally, the pre-trained UniTalker exhibits promise as the foundation model for audio-driven facial animation tasks. Fine-tuning the pre-trained UniTalker on seen datasets further enhances performance on each dataset, with an average error reduction of 6.3% on A2F-Bench. Moreover, fine-tuning UniTalker on an unseen dataset with only half the data surpasses prior state-of-the-art models trained on the full dataset. The code and dataset are available at the project page this https URL.</li>
</ul>

<h3>Title: Optimizing Diffusion Models for Joint Trajectory Prediction and Controllable Generation</h3>
<ul>
<li><strong>Authors: </strong>Yixiao Wang, Chen Tang, Lingfeng Sun, Simone Rossi, Yichen Xie, Chensheng Peng, Thomas Hannagan, Stefano Sabatini, Nicola Poerio, Masayoshi Tomizuka, Wei Zhan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2408.00766">https://arxiv.org/abs/2408.00766</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2408.00766">https://arxiv.org/pdf/2408.00766</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2408.00766]] Optimizing Diffusion Models for Joint Trajectory Prediction and Controllable Generation(https://arxiv.org/abs/2408.00766)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models are promising for joint trajectory prediction and controllable generation in autonomous driving, but they face challenges of inefficient inference steps and high computational demands. To tackle these challenges, we introduce Optimal Gaussian Diffusion (OGD) and Estimated Clean Manifold (ECM) Guidance. OGD optimizes the prior distribution for a small diffusion time $T$ and starts the reverse diffusion process from it. ECM directly injects guidance gradients to the estimated clean manifold, eliminating extensive gradient backpropagation throughout the network. Our methodology streamlines the generative process, enabling practical applications with reduced computational overhead. Experimental validation on the large-scale Argoverse 2 dataset demonstrates our approach's superior performance, offering a viable solution for computationally efficient, high-quality joint trajectory prediction and controllable generation for autonomous driving. Our project webpage is at this https URL.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
