<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-12-09</h1>
<h3>Title: Take Package as Language: Anomaly Detection Using Transformer</h3>
<ul>
<li><strong>Authors: </strong>Jie Huang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04473">https://arxiv.org/abs/2412.04473</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04473">https://arxiv.org/pdf/2412.04473</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04473]] Take Package as Language: Anomaly Detection Using Transformer(https://arxiv.org/abs/2412.04473)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Network data packet anomaly detection faces numerous challenges, including exploring new anomaly supervision signals, researching weakly supervised anomaly detection, and improving model interpretability. This paper proposes NIDS-GPT, a GPT-based causal language model for network intrusion detection. Unlike previous work, NIDS-GPT innovatively treats each number in the packet as an independent "word" rather than packet fields, enabling a more fine-grained data representation. We adopt an improved GPT-2 model and design special tokenizers and embedding layers to better capture the structure and semantics of network data. NIDS-GPT has good scalability, supports unsupervised pre-training, and enhances model interpretability through attention weight visualization. Experiments on the CICIDS2017 and car-hacking datasets show that NIDS-GPT achieves 100\% accuracy under extreme imbalance conditions, far surpassing traditional methods; it also achieves over 90\% accuracy in one-shot learning. These results demonstrate NIDS-GPT's excellent performance and potential in handling complex network anomaly detection tasks, especially in data-imbalanced and resource-constrained scenarios. The code is available at \url{this https URL</li>
</ul>

<h3>Title: Large Language Models in Politics and Democracy: A Comprehensive Survey</h3>
<ul>
<li><strong>Authors: </strong>Goshi Aoki</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04498">https://arxiv.org/abs/2412.04498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04498">https://arxiv.org/pdf/2412.04498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04498]] Large Language Models in Politics and Democracy: A Comprehensive Survey(https://arxiv.org/abs/2412.04498)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The advancement of generative AI, particularly large language models (LLMs), has a significant impact on politics and democracy, offering potential across various domains, including policymaking, political communication, analysis, and governance. This paper surveys the recent and potential applications of LLMs in politics, examining both their promises and the associated challenges. This paper examines the ways in which LLMs are being employed in legislative processes, political communication, and political analysis. Moreover, we investigate the potential of LLMs in diplomatic and national security contexts, economic and social modeling, and legal applications. While LLMs offer opportunities to enhance efficiency, inclusivity, and decision-making in political processes, they also present challenges related to bias, transparency, and accountability. The paper underscores the necessity for responsible development, ethical considerations, and governance frameworks to ensure that the integration of LLMs into politics aligns with democratic values and promotes a more just and equitable society.</li>
</ul>

<h3>Title: WinTSR: A Windowed Temporal Saliency Rescaling Method for Interpreting Time Series Deep Learning Models</h3>
<ul>
<li><strong>Authors: </strong>Md. Khairul Islam, Judy Fox</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04532">https://arxiv.org/abs/2412.04532</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04532">https://arxiv.org/pdf/2412.04532</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04532]] WinTSR: A Windowed Temporal Saliency Rescaling Method for Interpreting Time Series Deep Learning Models(https://arxiv.org/abs/2412.04532)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Interpreting complex time series forecasting models is challenging due to the temporal dependencies between time steps and the dynamic relevance of input features over time. Existing interpretation methods are limited by focusing mostly on classification tasks, evaluating using custom baseline models instead of the latest time series models, using simple synthetic datasets, and requiring training another model. We introduce a novel interpretation method called Windowed Temporal Saliency Rescaling (WinTSR) addressing these limitations. WinTSR explicitly captures temporal dependencies among the past time steps and efficiently scales the feature importance with this time importance. We benchmark WinTSR against 10 recent interpretation techniques with 5 state-of-the-art deep-learning models of different architectures, including a time series foundation model. We use 3 real-world datasets for both time-series classification and regression. Our comprehensive analysis shows that WinTSR significantly outranks the other local interpretation methods in overall performance. Finally, we provide a novel and open-source framework to interpret the latest time series transformers and foundation models.</li>
</ul>

<h3>Title: Solving High-dimensional Inverse Problems Using Amortized Likelihood-free Inference with Noisy and Incomplete Data</h3>
<ul>
<li><strong>Authors: </strong>Jice Zeng, Yuanzhe Wang, Alexandre M. Tartakovsky, David Barajas-Solano</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04565">https://arxiv.org/abs/2412.04565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04565">https://arxiv.org/pdf/2412.04565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04565]] Solving High-dimensional Inverse Problems Using Amortized Likelihood-free Inference with Noisy and Incomplete Data(https://arxiv.org/abs/2412.04565)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present a novel likelihood-free probabilistic inversion method based on normalizing flows for high-dimensional inverse problems. The proposed method is comprised of two complementary networks: a summary network for data compression, and an inference network for parameter estimation. The summary network encodes raw observations into a fixed-size vector of summary statistics, while the inference network generates samples of the approximate posterior distribution of the model parameters based on these summary statistics. The posterior samples are produced in a deep generative fashion by sampling from a latent Gaussian distribution and passing these samples through an invertible transformation. We construct this invertible transformation by sequentially alternating conditional invertible neural network (cINN) and conditional neural spline flow (cNSF) layers. The summary and inference networks are trained simultaneously. We apply the proposed method to an inversion problem in groundwater hydrology to estimate the posterior distribution of the system's log-conductivity field conditioned on spatially sparse time-series observations of the system's hydraulic head responses. The conductivity field is represented with 706 degrees of freedom in the considered problem. The comparison with the likelihood-based iterative ensemble smoother PEST-IES method demonstrates that the proposed method accurately estimates the parameter posterior distribution and the observations' predictive posterior distribution at a fraction of the inference time of PEST-IES.</li>
</ul>

<h3>Title: ARTeFACT: Benchmarking Segmentation Models on Diverse Analogue Media Damage</h3>
<ul>
<li><strong>Authors: </strong>Daniela Ivanova, Marco Aversa, Paul Henderson, John Williamson</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04580">https://arxiv.org/abs/2412.04580</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04580">https://arxiv.org/pdf/2412.04580</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04580]] ARTeFACT: Benchmarking Segmentation Models on Diverse Analogue Media Damage(https://arxiv.org/abs/2412.04580)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Accurately detecting and classifying damage in analogue media such as paintings, photographs, textiles, mosaics, and frescoes is essential for cultural heritage preservation. While machine learning models excel in correcting degradation if the damage operator is known a priori, we show that they fail to robustly predict where the damage is even after supervised training; thus, reliable damage detection remains a challenge. Motivated by this, we introduce ARTeFACT, a dataset for damage detection in diverse types analogue media, with over 11,000 annotations covering 15 kinds of damage across various subjects, media, and historical provenance. Furthermore, we contribute human-verified text prompts describing the semantic contents of the images, and derive additional textual descriptions of the annotated damage. We evaluate CNN, Transformer, diffusion-based segmentation models, and foundation vision models in zero-shot, supervised, unsupervised and text-guided settings, revealing their limitations in generalising across media types. Our dataset is available at $\href{this https URL}{this https URL}$ as the first-of-its-kind benchmark for analogue media damage detection and restoration.</li>
</ul>

<h3>Title: Using Diffusion Priors for Video Amodal Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Kaihua Chen, Deva Ramanan, Tarasha Khurana</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04623">https://arxiv.org/abs/2412.04623</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04623">https://arxiv.org/pdf/2412.04623</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04623]] Using Diffusion Priors for Video Amodal Segmentation(https://arxiv.org/abs/2412.04623)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Object permanence in humans is a fundamental cue that helps in understanding persistence of objects, even when they are fully occluded in the scene. Present day methods in object segmentation do not account for this amodal nature of the world, and only work for segmentation of visible or modal objects. Few amodal methods exist; single-image segmentation methods cannot handle high-levels of occlusions which are better inferred using temporal information, and multi-frame methods have focused solely on segmenting rigid objects. To this end, we propose to tackle video amodal segmentation by formulating it as a conditional generation task, capitalizing on the foundational knowledge in video generative models. Our method is simple; we repurpose these models to condition on a sequence of modal mask frames of an object along with contextual pseudo-depth maps, to learn which object boundary may be occluded and therefore, extended to hallucinate the complete extent of an object. This is followed by a content completion stage which is able to inpaint the occluded regions of an object. We benchmark our approach alongside a wide array of state-of-the-art methods on four datasets and show a dramatic improvement of upto 13% for amodal segmentation in an object's occluded region.</li>
</ul>

<h3>Title: Improving LLM Group Fairness on Tabular Data via In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Valeriia Cherepanova, Chia-Jung Lee, Nil-Jana Akpinar, Riccardo Fogliato, Martin Andres Bertran, Michael Kearns, James Zou</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04642">https://arxiv.org/abs/2412.04642</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04642">https://arxiv.org/pdf/2412.04642</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04642]] Improving LLM Group Fairness on Tabular Data via In-Context Learning(https://arxiv.org/abs/2412.04642)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have been shown to be effective on tabular prediction tasks in the low-data regime, leveraging their internal knowledge and ability to learn from instructions and examples. However, LLMs can fail to generate predictions that satisfy group fairness, that is, produce equitable outcomes across groups. Critically, conventional debiasing approaches for natural language tasks do not directly translate to mitigating group unfairness in tabular settings. In this work, we systematically investigate four empirical approaches to improve group fairness of LLM predictions on tabular datasets, including fair prompt optimization, soft prompt tuning, strategic selection of few-shot examples, and self-refining predictions via chain-of-thought reasoning. Through experiments on four tabular datasets using both open-source and proprietary LLMs, we show the effectiveness of these methods in enhancing demographic parity while maintaining high overall performance. Our analysis provides actionable insights for practitioners in selecting the most suitable approach based on their specific requirements and constraints.</li>
</ul>

<h3>Title: One Communication Round is All It Needs for Federated Fine-Tuning Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Ziyao Wang, Bowei Tian, Yexiao He, Zheyu Shen, Luyang Liu, Ang Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.DC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04650">https://arxiv.org/abs/2412.04650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04650">https://arxiv.org/pdf/2412.04650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04650]] One Communication Round is All It Needs for Federated Fine-Tuning Foundation Models(https://arxiv.org/abs/2412.04650)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>The recent advancement of large foundation models (FMs) has increased the demand for fine-tuning these models on large-scale and cross-domain datasets. To address this, federated fine-tuning has emerged as a solution, allowing models to be fine-tuned on distributed datasets across multiple devices while ensuring data privacy. However, the substantial parameter size of FMs and the multi-round communication required by traditional federated fine-tuning algorithms result in prohibitively high communication costs, challenging the practicality of federated fine-tuning. In this paper, we are the first to reveal, both theoretically and empirically, that the traditional multi-round aggregation algorithms may not be necessary for federated fine-tuning large FMs. Our experiments reveal that a single round of communication (i.e., one-shot federated fine-tuning) yields a global model performance comparable to that achieved through multiple rounds of communication. Through rigorous mathematical and empirical analyses, we demonstrate that large FMs, due to their extensive parameter sizes and pre-training on general tasks, achieve significantly lower training loss in one-shot federated fine-tuning compared to smaller models. Our extensive experiments show that one-shot federated fine-tuning not only reduces communication costs but also enables asynchronous aggregation, enhances privacy, and maintains performance consistency with multi-round federated fine-tuning for models larger than 1 billion parameters, on text generation and text-to-image generation tasks. Our findings have the potential to revolutionize federated fine-tuning in practice, enhancing efficiency, reducing costs, and expanding accessibility for large-scale models. This breakthrough paves the way for broader adoption and application of federated fine-tuning across various domains.</li>
</ul>

<h3>Title: Hidden in the Noise: Two-Stage Robust Watermarking for Images</h3>
<ul>
<li><strong>Authors: </strong>Kasra Arabi, Benjamin Feuer, R. Teal Witter, Chinmay Hegde, Niv Cohen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04653">https://arxiv.org/abs/2412.04653</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04653">https://arxiv.org/pdf/2412.04653</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04653]] Hidden in the Noise: Two-Stage Robust Watermarking for Images(https://arxiv.org/abs/2412.04653)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>As the quality of image generators continues to improve, deepfakes become a topic of considerable societal debate. Image watermarking allows responsible model owners to detect and label their AI-generated content, which can mitigate the harm. Yet, current state-of-the-art methods in image watermarking remain vulnerable to forgery and removal attacks. This vulnerability occurs in part because watermarks distort the distribution of generated images, unintentionally revealing information about the watermarking techniques. In this work, we first demonstrate a distortion-free watermarking method for images, based on a diffusion model's initial noise. However, detecting the watermark requires comparing the initial noise reconstructed for an image to all previously used initial noises. To mitigate these issues, we propose a two-stage watermarking framework for efficient detection. During generation, we augment the initial noise with generated Fourier patterns to embed information about the group of initial noises we used. For detection, we (i) retrieve the relevant group of noises, and (ii) search within the given group for an initial noise that might match our image. This watermarking approach achieves state-of-the-art robustness to forgery and removal against a large battery of attacks.</li>
</ul>

<h3>Title: LAA-Net: A Physical-prior-knowledge Based Network for Robust Nighttime Depth Estimation</h3>
<ul>
<li><strong>Authors: </strong>Kebin Peng, Haotang Li, Zhenyu Qi, Huashan Chen, Zi Wang, Wei Zhang, Sen He</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04666">https://arxiv.org/abs/2412.04666</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04666">https://arxiv.org/pdf/2412.04666</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04666]] LAA-Net: A Physical-prior-knowledge Based Network for Robust Nighttime Depth Estimation(https://arxiv.org/abs/2412.04666)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Existing self-supervised monocular depth estimation (MDE) models attempt to improve nighttime performance by using GANs to transfer nighttime images into their daytime versions. However, this can introduce inconsistencies due to the complexities of real-world daytime lighting variations, which may finally lead to inaccurate estimation results. To address this issue, we leverage physical-prior-knowledge about light wavelength and light attenuation during nighttime. Specifically, our model, Light-Attenuation-Aware Network (LAA-Net), incorporates physical insights from Rayleigh scattering theory for robust nighttime depth estimation: LAA-Net is trained based on red channel values because red light preserves more information under nighttime scenarios due to its longer wavelength. Additionally, based on Beer-Lambert law, we introduce Red Channel Attenuation (RCA) loss to guide LAA-Net's training. Experiments on the RobotCar-Night, nuScenes-Night, RobotCar-Day, and KITTI datasets demonstrate that our model outperforms SOTA models.</li>
</ul>

<h3>Title: Diffusion-Augmented Coreset Expansion for Scalable Dataset Distillation</h3>
<ul>
<li><strong>Authors: </strong>Ali Abbasi, Shima Imani, Chenyang An, Gayathri Mahalingam, Harsh Shrivastava, Maurice Diesendruck, Hamed Pirsiavash, Pramod Sharma, Soheil Kolouri</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04668">https://arxiv.org/abs/2412.04668</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04668">https://arxiv.org/pdf/2412.04668</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04668]] Diffusion-Augmented Coreset Expansion for Scalable Dataset Distillation(https://arxiv.org/abs/2412.04668)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model, generative</a></li>
<li><strong>Abstract: </strong>With the rapid scaling of neural networks, data storage and communication demands have intensified. Dataset distillation has emerged as a promising solution, condensing information from extensive datasets into a compact set of synthetic samples by solving a bilevel optimization problem. However, current methods face challenges in computational efficiency, particularly with high-resolution data and complex architectures. Recently, knowledge-distillation-based dataset condensation approaches have made this process more computationally feasible. Yet, with the recent developments of generative foundation models, there is now an opportunity to achieve even greater compression, enhance the quality of distilled data, and introduce valuable diversity into the data representation. In this work, we propose a two-stage solution. First, we compress the dataset by selecting only the most informative patches to form a coreset. Next, we leverage a generative foundation model to dynamically expand this compressed set in real-time, enhancing the resolution of these patches and introducing controlled variability to the coreset. Our extensive experiments demonstrate the robustness and efficiency of our approach across a range of dataset distillation benchmarks. We demonstrate a significant improvement of over 10% compared to the state-of-the-art on several large-scale dataset distillation benchmarks. The code will be released soon.</li>
</ul>

<h3>Title: Zephyr quantum-assisted hierarchical Calo4pQVAE for particle-calorimeter interactions</h3>
<ul>
<li><strong>Authors: </strong>Ian Lu, Hao Jia, Sebastian Gonzalez, Deniz Sogutlu, J. Quetzalcoatl Toledo-Marin, Sehmimul Hoque, Abhishek Abhishek, Colin Gay, Roger Melko, Eric Paquet, Geoffrey Fox, Maximilian Swiatlowski, Wojciech Fedorko</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, hep-ph, physics.comp-ph, quant-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04677">https://arxiv.org/abs/2412.04677</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04677">https://arxiv.org/pdf/2412.04677</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04677]] Zephyr quantum-assisted hierarchical Calo4pQVAE for particle-calorimeter interactions(https://arxiv.org/abs/2412.04677)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>With the approach of the High Luminosity Large Hadron Collider (HL-LHC) era set to begin particle collisions by the end of this decade, it is evident that the computational demands of traditional collision simulation methods are becoming increasingly unsustainable. Existing approaches, which rely heavily on first-principles Monte Carlo simulations for modeling event showers in calorimeters, are projected to require millions of CPU-years annually -- far exceeding current computational capacities. This bottleneck presents an exciting opportunity for advancements in computational physics by integrating deep generative models with quantum simulations. We propose a quantum-assisted hierarchical deep generative surrogate founded on a variational autoencoder (VAE) in combination with an energy conditioned restricted Boltzmann machine (RBM) embedded in the model's latent space as a prior. By mapping the topology of D-Wave's Zephyr quantum annealer (QA) into the nodes and couplings of a 4-partite RBM, we leverage quantum simulation to accelerate our shower generation times significantly. To evaluate our framework, we use Dataset 2 of the CaloChallenge 2022. Through the integration of classical computation and quantum simulation, this hybrid framework paves way for utilizing large-scale quantum simulations as priors in deep generative models.</li>
</ul>

<h3>Title: Unsupervised Segmentation by Diffusing, Walking and Cutting</h3>
<ul>
<li><strong>Authors: </strong>Daniela Ivanova, Marco Aversa, Paul Henderson, John Williamson</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04678">https://arxiv.org/abs/2412.04678</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04678">https://arxiv.org/pdf/2412.04678</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04678]] Unsupervised Segmentation by Diffusing, Walking and Cutting(https://arxiv.org/abs/2412.04678)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We propose an unsupervised image segmentation method using features from pre-trained text-to-image diffusion models. Inspired by classic spectral clustering approaches, we construct adjacency matrices from self-attention layers between image patches and recursively partition using Normalised Cuts. A key insight is that self-attention probability distributions, which capture semantic relations between patches, can be interpreted as a transition matrix for random walks across the image. We leverage this by first using Random Walk Normalized Cuts directly on these self-attention activations to partition the image, minimizing transition probabilities between clusters while maximizing coherence within clusters. Applied recursively, this yields a hierarchical segmentation that reflects the rich semantics in the pre-trained attention layers, without any additional training. Next, we explore other ways to build the NCuts adjacency matrix from features, and how we can use the random walk interpretation of self-attention to capture long-range relationships. Finally, we propose an approach to automatically determine the NCut cost criterion, avoiding the need to tune this manually. We quantitatively analyse the effect incorporating different features, a constant versus dynamic NCut threshold, and incorporating multi-node paths when constructing the NCuts adjacency matrix. We show that our approach surpasses all existing methods for zero-shot unsupervised segmentation, achieving state-of-the-art results on COCO-Stuff-27 and Cityscapes.</li>
</ul>

<h3>Title: Transformers Struggle to Learn to Search</h3>
<ul>
<li><strong>Authors: </strong>Abulhair Saparov, Srushti Pawar, Shreyas Pimpalgaonkar, Nitish Joshi, Richard Yuanzhe Pang, Vishakh Padmakumar, Seyed Mehran Kazemi, Najoung Kim, He He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04703">https://arxiv.org/abs/2412.04703</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04703">https://arxiv.org/pdf/2412.04703</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04703]] Transformers Struggle to Learn to Search(https://arxiv.org/abs/2412.04703)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Search is an ability foundational in many important tasks, and recent studies have shown that large language models (LLMs) struggle to perform search robustly. It is unknown whether this inability is due to a lack of data, insufficient model parameters, or fundamental limitations of the transformer architecture. In this work, we use the foundational graph connectivity problem as a testbed to generate effectively limitless high-coverage data to train small transformers and test whether they can learn to perform search. We find that, when given the right training distribution, the transformer is able to learn to search. We analyze the algorithm that the transformer has learned through a novel mechanistic interpretability technique that enables us to extract the computation graph from the trained model. We find that for each vertex in the input graph, transformers compute the set of vertices reachable from that vertex. Each layer then progressively expands these sets, allowing the model to search over a number of vertices exponential in the number of layers. However, we find that as the input graph size increases, the transformer has greater difficulty in learning the task. This difficulty is not resolved even as the number of parameters is increased, suggesting that increasing model scale will not lead to robust search abilities. We also find that performing search in-context (i.e., chain-of-thought) does not resolve this inability to learn to search on larger graphs.</li>
</ul>

<h3>Title: Addressing Attribute Leakages in Diffusion-based Image Editing without Training</h3>
<ul>
<li><strong>Authors: </strong>Sunung Mun, Jinhwan Nam, Sunghyun Cho, Jungseul Ok</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04715">https://arxiv.org/abs/2412.04715</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04715">https://arxiv.org/pdf/2412.04715</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04715]] Addressing Attribute Leakages in Diffusion-based Image Editing without Training(https://arxiv.org/abs/2412.04715)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have become a cornerstone in image editing, offering flexibility with language prompts and source images. However, a key challenge is attribute leakage, where unintended modifications occur in non-target regions or within target regions due to attribute interference. Existing methods often suffer from leakage due to naive text embeddings and inadequate handling of End-of-Sequence (EOS) token embeddings. We propose a novel framework to address attribute leakage with three components: (1) Object-Restricted Embeddings (ORE) to localize object-specific attributes in text embeddings, (2) Region-Guided Blending for Cross-Attention Masking (RGB-CAM) to align attention with target regions, and (3) Background Blending (BB) to preserve non-edited regions. Additionally, we introduce ALE-Bench, a benchmark for evaluating attribute leakage with new metrics for target-external and target-internal leakage. Experiments demonstrate that our framework significantly reduces attribute leakage while maintaining high editing quality, providing an efficient and tuning-free solution for multi-object image editing.</li>
</ul>

<h3>Title: Generative Humanization for Therapeutic Antibodies</h3>
<ul>
<li><strong>Authors: </strong>Cade Gordon, Aniruddh Raghu, Hunter Elliott, Peyton Greenside</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04737">https://arxiv.org/abs/2412.04737</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04737">https://arxiv.org/pdf/2412.04737</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04737]] Generative Humanization for Therapeutic Antibodies(https://arxiv.org/abs/2412.04737)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Antibody therapies have been employed to address some of today's most challenging diseases, but must meet many criteria during drug development before reaching a patient. Humanization is a sequence optimization strategy that addresses one critical risk called immunogenicity - a patient's immune response to the drug - by making an antibody more "human-like" in the absence of a predictive lab-based test for immunogenicity. However, existing humanization strategies generally yield very few humanized candidates, which may have degraded biophysical properties or decreased drug efficacy. Here, we re-frame humanization as a conditional generative modeling task, where humanizing mutations are sampled from a language model trained on human antibody data. We describe a sampling process that incorporates models of therapeutic attributes, such as antigen binding affinity, to obtain candidate sequences that have both reduced immunogenicity risk and maintained or improved therapeutic properties, allowing this algorithm to be readily embedded into an iterative antibody optimization campaign. We demonstrate in silico and in lab validation that in real therapeutic programs our generative humanization method produces diverse sets of antibodies that are both (1) highly-human and (2) have favorable therapeutic properties, such as improved binding to target antigens.</li>
</ul>

<h3>Title: Revitalizing Reconstruction Models for Multi-class Anomaly Detection via Class-Aware Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Lei Fan, Junjie Huang, Donglin Di, Anyang Su, Maurice Pagnucco, Yang Song</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04769">https://arxiv.org/abs/2412.04769</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04769">https://arxiv.org/pdf/2412.04769</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04769]] Revitalizing Reconstruction Models for Multi-class Anomaly Detection via Class-Aware Contrastive Learning(https://arxiv.org/abs/2412.04769)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>For anomaly detection (AD), early approaches often train separate models for individual classes, yielding high performance but posing challenges in scalability and resource management. Recent efforts have shifted toward training a single model capable of handling multiple classes. However, directly extending early AD methods to multi-class settings often results in degraded performance. In this paper, we analyze this degradation observed in reconstruction-based methods, identifying two key issues: catastrophic forgetting and inter-class confusion. To this end, we propose a plug-and-play modification by incorporating class-aware contrastive learning (CL). By explicitly leveraging raw object category information (e.g., carpet or wood) as supervised signals, we apply local CL to fine-tune multiscale features and global CL to learn more compact feature representations of normal patterns, thereby effectively adapting the models to multi-class settings. Experiments across four datasets (over 60 categories) verify the effectiveness of our approach, yielding significant improvements and superior performance compared to advanced methods. Notably, ablation studies show that even using pseudo-class labels can achieve comparable performance.</li>
</ul>

<h3>Title: Foundation Models for Low-Resource Language Education (Vision Paper)</h3>
<ul>
<li><strong>Authors: </strong>Zhaojun Ding, Zhengliang Liu, Hanqi Jiang, Yizhu Gao, Xiaoming Zhai, Tianming Liu, Ninghao Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04774">https://arxiv.org/abs/2412.04774</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04774">https://arxiv.org/pdf/2412.04774</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04774]] Foundation Models for Low-Resource Language Education (Vision Paper)(https://arxiv.org/abs/2412.04774)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Recent studies show that large language models (LLMs) are powerful tools for working with natural language, bringing advances in many areas of computational linguistics. However, these models face challenges when applied to low-resource languages due to limited training data and difficulty in understanding cultural nuances. Research is now focusing on multilingual models to improve LLM performance for these languages. Education in these languages also struggles with a lack of resources and qualified teachers, particularly in underdeveloped regions. Here, LLMs can be transformative, supporting innovative methods like community-driven learning and digital platforms. This paper discusses how LLMs could enhance education for low-resource languages, emphasizing practical applications and benefits.</li>
</ul>

<h3>Title: Megatron: Evasive Clean-Label Backdoor Attacks against Vision Transformer</h3>
<ul>
<li><strong>Authors: </strong>Xueluan Gong, Bowei Tian, Meng Xue, Shuike Li, Yanjiao Chen, Qian Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04776">https://arxiv.org/abs/2412.04776</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04776">https://arxiv.org/pdf/2412.04776</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04776]] Megatron: Evasive Clean-Label Backdoor Attacks against Vision Transformer(https://arxiv.org/abs/2412.04776)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Vision transformers have achieved impressive performance in various vision-related tasks, but their vulnerability to backdoor attacks is under-explored. A handful of existing works focus on dirty-label attacks with wrongly-labeled poisoned training samples, which may fail if a benign model trainer corrects the labels. In this paper, we propose Megatron, an evasive clean-label backdoor attack against vision transformers, where the attacker injects the backdoor without manipulating the data-labeling process. To generate an effective trigger, we customize two loss terms based on the attention mechanism used in transformer networks, i.e., latent loss and attention diffusion loss. The latent loss aligns the last attention layer between triggered samples and clean samples of the target label. The attention diffusion loss emphasizes the attention diffusion area that encompasses the trigger. A theoretical analysis is provided to underpin the rationale behind the attention diffusion loss. Extensive experiments on CIFAR-10, GTSRB, CIFAR-100, and Tiny ImageNet demonstrate the effectiveness of Megatron. Megatron can achieve attack success rates of over 90% even when the position of the trigger is slightly shifted during testing. Furthermore, Megatron achieves better evasiveness than baselines regarding both human visual inspection and defense strategies (i.e., DBAVT, BAVT, Beatrix, TeCo, and SAGE).</li>
</ul>

<h3>Title: Anomaly Detection and Classification in Knowledge Graphs</h3>
<ul>
<li><strong>Authors: </strong>Asara Senaratne, Peter Christen, Pouya Omran, Graham Williams</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04780">https://arxiv.org/abs/2412.04780</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04780">https://arxiv.org/pdf/2412.04780</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04780]] Anomaly Detection and Classification in Knowledge Graphs(https://arxiv.org/abs/2412.04780)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Anomalies such as redundant, inconsistent, contradictory, and deficient values in a Knowledge Graph (KG) are unavoidable, as these graphs are often curated manually, or extracted using machine learning and natural language processing techniques. Therefore, anomaly detection is a task that can enhance the quality of KGs. In this paper, we propose SEKA (SEeking Knowledge graph Anomalies), an unsupervised approach for the detection of abnormal triples and entities in KGs. SEKA can help improve the correctness of a KG whilst retaining its coverage. We propose an adaption of the Path Rank Algorithm (PRA), named the Corroborative Path Rank Algorithm (CPRA), which is an efficient adaptation of PRA that is customized to detect anomalies in KGs. Furthermore, we also present TAXO (TAXOnomy of anomaly types in KGs), a taxonomy of possible anomaly types that can occur in a KG. This taxonomy provides a classification of the anomalies discovered by SEKA with an extensive discussion of possible data quality issues in a KG. We evaluate both approaches using the four real-world KGs YAGO-1, KBpedia, Wikidata, and DSKG to demonstrate the ability of SEKA and TAXO to outperform the baselines.</li>
</ul>

<h3>Title: DPGIIL: Dirichlet Process-Deep Generative Model-Integrated Incremental Learning for Clustering in Transmissibility-based Online Structural Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Lin-Feng Mei, Wang-Ji Yan</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.data-an, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04781">https://arxiv.org/abs/2412.04781</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04781">https://arxiv.org/pdf/2412.04781</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04781]] DPGIIL: Dirichlet Process-Deep Generative Model-Integrated Incremental Learning for Clustering in Transmissibility-based Online Structural Anomaly Detection(https://arxiv.org/abs/2412.04781)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, anomaly</a></li>
<li><strong>Abstract: </strong>Clustering based on vibration responses, such as transmissibility functions (TFs), is promising in structural anomaly detection, but most existing approaches struggle with determining the optimal cluster number and handling high-dimensional streaming data, while their shallow structures also make them sensitive to manually-engineered feature quality. To bridge this gap, this work proposes the Dirichlet process-deep generative model-integrated incremental learning (DPGIIL) for clustering by combining the advantages of deep generative models (DGMs) in representation learning and the Dirichlet process mixture model (DPMM) in identifying distinct patterns in observed data. By introducing a DPMM prior into the latent space of DGMs, DPGIIL automatically captures dissimilarities in extracted latent representations, enabling both generative modeling and clustering. Within the context of variational Bayesian inference, a lower bound on the log marginal likelihood of DPGIIL, tighter than the evidence lower bound given sufficient training data, is derived analytically, which enables the joint optimization of DGM and DPMM parameters, thereby allowing the DPMM to regularize the DGM's feature extraction process. Additionally, a greedy split-merge scheme-based coordinate ascent variational inference method is devised to accelerate the optimization. The summary statistics of the DPMM, along with the network parameters, are used to retain information about previous data for incremental learning. Notably, this study uses variational autoencoder (VAE) within DPGIIL as an illustrative example, while this framework is adaptable to other DGMs. Two case studies show that the proposed method outperforms some state-of-the-art approaches in structural anomaly detection and clustering, while also dynamically generating new clusters to indicate the emergence of new structural conditions for online monitoring.</li>
</ul>

<h3>Title: NLP-ADBench: NLP Anomaly Detection Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Yuangang Li, Jiaqi Li, Zhuo Xiao, Tiankai Yang, Yi Nian, Xiyang Hu, Yue Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04784">https://arxiv.org/abs/2412.04784</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04784">https://arxiv.org/pdf/2412.04784</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04784]] NLP-ADBench: NLP Anomaly Detection Benchmark(https://arxiv.org/abs/2412.04784)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Anomaly detection (AD) is a critical machine learning task with diverse applications in web systems, including fraud detection, content moderation, and user behavior analysis. Despite its significance, AD in natural language processing (NLP) remains underexplored, limiting advancements in detecting anomalies in text data such as harmful content, phishing attempts, or spam reviews. In this paper, we introduce NLP-ADBench, the most comprehensive benchmark for NLP anomaly detection (NLP-AD), comprising eight curated datasets and evaluations of nineteen state-of-the-art algorithms. These include three end-to-end methods and sixteen two-step algorithms that apply traditional anomaly detection techniques to language embeddings generated by bert-base-uncased and OpenAI's text-embedding-3-large models. Our results reveal critical insights and future directions for NLP-AD. Notably, no single model excels across all datasets, highlighting the need for automated model selection. Moreover, two-step methods leveraging transformer-based embeddings consistently outperform specialized end-to-end approaches, with OpenAI embeddings demonstrating superior performance over BERT embeddings. By releasing NLP-ADBench at this https URL, we provide a standardized framework for evaluating NLP-AD methods, fostering the development of innovative approaches. This work fills a crucial gap in the field and establishes a foundation for advancing NLP anomaly detection, particularly in the context of improving the safety and reliability of web-based systems.</li>
</ul>

<h3>Title: LiFT: Leveraging Human Feedback for Text-to-Video Model Alignment</h3>
<ul>
<li><strong>Authors: </strong>Yibin Wang, Zhiyu Tan, Junyan Wang, Xiaomeng Yang, Cheng Jin, Hao Li</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04814">https://arxiv.org/abs/2412.04814</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04814">https://arxiv.org/pdf/2412.04814</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04814]] LiFT: Leveraging Human Feedback for Text-to-Video Model Alignment(https://arxiv.org/abs/2412.04814)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in text-to-video (T2V) generative models have shown impressive capabilities. However, these models are still inadequate in aligning synthesized videos with human preferences (e.g., accurately reflecting text descriptions), which is particularly difficult to address, as human preferences are inherently subjective and challenging to formalize as objective functions. Therefore, this paper proposes LiFT, a novel fine-tuning method leveraging human feedback for T2V model alignment. Specifically, we first construct a Human Rating Annotation dataset, LiFT-HRA, consisting of approximately 10k human annotations, each including a score and its corresponding rationale. Based on this, we train a reward model LiFT-Critic to learn reward function effectively, which serves as a proxy for human judgment, measuring the alignment between given videos and human expectations. Lastly, we leverage the learned reward function to align the T2V model by maximizing the reward-weighted likelihood. As a case study, we apply our pipeline to CogVideoX-2B, showing that the fine-tuned model outperforms the CogVideoX-5B across all 16 metrics, highlighting the potential of human feedback in improving the alignment and quality of synthesized videos.</li>
</ul>

<h3>Title: DAug: Diffusion-based Channel Augmentation for Radiology Image Retrieval and Classification</h3>
<ul>
<li><strong>Authors: </strong>Ying Jin, Zhuoran Zhou, Haoquan Fang, Jenq-Neng Hwang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04828">https://arxiv.org/abs/2412.04828</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04828">https://arxiv.org/pdf/2412.04828</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04828]] DAug: Diffusion-based Channel Augmentation for Radiology Image Retrieval and Classification(https://arxiv.org/abs/2412.04828)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Medical image understanding requires meticulous examination of fine visual details, with particular regions requiring additional attention. While radiologists build such expertise over years of experience, it is challenging for AI models to learn where to look with limited amounts of training data. This limitation results in unsatisfying robustness in medical image understanding. To address this issue, we propose Diffusion-based Feature Augmentation (DAug), a portable method that improves a perception model's performance with a generative model's output. Specifically, we extend a radiology image to multiple channels, with the additional channels being the heatmaps of regions where diseases tend to develop. A diffusion-based image-to-image translation model was used to generate such heatmaps conditioned on selected disease classes. Our method is motivated by the fact that generative models learn the distribution of normal and abnormal images, and such knowledge is complementary to image understanding tasks. In addition, we propose the Image-Text-Class Hybrid Contrastive learning to utilize both text and class labels. With two novel approaches combined, our method surpasses baseline models without changing the model architecture, and achieves state-of-the-art performance on both medical image retrieval and classification tasks.</li>
</ul>

<h3>Title: Wavelet Diffusion Neural Operator</h3>
<ul>
<li><strong>Authors: </strong>Peiyan Hu, Rui Wang, Xiang Zheng, Tao Zhang, Haodong Feng, Ruiqi Feng, Long Wei, Yue Wang, Zhi-Ming Ma, Tailin Wu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04833">https://arxiv.org/abs/2412.04833</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04833">https://arxiv.org/pdf/2412.04833</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04833]] Wavelet Diffusion Neural Operator(https://arxiv.org/abs/2412.04833)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Simulating and controlling physical systems described by partial differential equations (PDEs) are crucial tasks across science and engineering. Recently, diffusion generative models have emerged as a competitive class of methods for these tasks due to their ability to capture long-term dependencies and model high-dimensional states. However, diffusion models typically struggle with handling system states with abrupt changes and generalizing to higher resolutions. In this work, we propose Wavelet Diffusion Neural Operator (WDNO), a novel PDE simulation and control framework that enhances the handling of these complexities. WDNO comprises two key innovations. Firstly, WDNO performs diffusion-based generative modeling in the wavelet domain for the entire trajectory to handle abrupt changes and long-term dependencies effectively. Secondly, to address the issue of poor generalization across different resolutions, which is one of the fundamental tasks in modeling physical systems, we introduce multi-resolution training. We validate WDNO on five physical systems, including 1D advection equation, three challenging physical systems with abrupt changes (1D Burgers' equation, 1D compressible Navier-Stokes equation and 2D incompressible fluid), and a real-world dataset ERA5, which demonstrates superior performance on both simulation and control tasks over state-of-the-art methods, with significant improvements in long-term and detail prediction accuracy. Remarkably, in the challenging context of the 2D high-dimensional and indirect control task aimed at reducing smoke leakage, WDNO reduces the leakage by 33.2% compared to the second-best baseline.</li>
</ul>

<h3>Title: SleeperMark: Towards Robust Watermark against Fine-Tuning Text-to-image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Zilan Wang, Junfeng Guo, Jiacheng Zhu, Yiming Li, Heng Huang, Muhao Chen, Zhengzhong Tu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04852">https://arxiv.org/abs/2412.04852</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04852">https://arxiv.org/pdf/2412.04852</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04852]] SleeperMark: Towards Robust Watermark against Fine-Tuning Text-to-image Diffusion Models(https://arxiv.org/abs/2412.04852)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in large-scale text-to-image (T2I) diffusion models have enabled a variety of downstream applications, including style customization, subject-driven personalization, and conditional generation. As T2I models require extensive data and computational resources for training, they constitute highly valued intellectual property (IP) for their legitimate owners, yet making them incentive targets for unauthorized fine-tuning by adversaries seeking to leverage these models for customized, usually profitable applications. Existing IP protection methods for diffusion models generally involve embedding watermark patterns and then verifying ownership through generated outputs examination, or inspecting the model's feature space. However, these techniques are inherently ineffective in practical scenarios when the watermarked model undergoes fine-tuning, and the feature space is inaccessible during verification ((i.e., black-box setting). The model is prone to forgetting the previously learned watermark knowledge when it adapts to a new task. To address this challenge, we propose SleeperMark, a novel framework designed to embed resilient watermarks into T2I diffusion models. SleeperMark explicitly guides the model to disentangle the watermark information from the semantic concepts it learns, allowing the model to retain the embedded watermark while continuing to be fine-tuned to new downstream tasks. Our extensive experiments demonstrate the effectiveness of SleeperMark across various types of diffusion models, including latent diffusion models (e.g., Stable Diffusion) and pixel diffusion models (e.g., DeepFloyd-IF), showing robustness against downstream fine-tuning and various attacks at both the image and model levels, with minimal impact on the model's generative capability. The code is available at this https URL.</li>
</ul>

<h3>Title: MANTA: A Large-Scale Multi-View and Visual-Text Anomaly Detection Dataset for Tiny Objects</h3>
<ul>
<li><strong>Authors: </strong>Lei Fan, Dongdong Fan, Zhiguang Hu, Yiwen Ding, Donglin Di, Kai Yi, Maurice Pagnucco, Yang Song</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04867">https://arxiv.org/abs/2412.04867</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04867">https://arxiv.org/pdf/2412.04867</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04867]] MANTA: A Large-Scale Multi-View and Visual-Text Anomaly Detection Dataset for Tiny Objects(https://arxiv.org/abs/2412.04867)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>We present MANTA, a visual-text anomaly detection dataset for tiny objects. The visual component comprises over 137.3K images across 38 object categories spanning five typical domains, of which 8.6K images are labeled as anomalous with pixel-level annotations. Each image is captured from five distinct viewpoints to ensure comprehensive object coverage. The text component consists of two subsets: Declarative Knowledge, including 875 words that describe common anomalies across various domains and specific categories, with detailed explanations for < what, why, how>, including causes and visual characteristics; and Constructivist Learning, providing 2K multiple-choice questions with varying levels of difficulty, each paired with images and corresponded answer explanations. We also propose a baseline for visual-text tasks and conduct extensive benchmarking experiments to evaluate advanced methods across different settings, highlighting the challenges and efficacy of our dataset.</li>
</ul>

<h3>Title: Mitigating Instance-Dependent Label Noise: Integrating Self-Supervised Pretraining with Pseudo-Label Refinement</h3>
<ul>
<li><strong>Authors: </strong>Gouranga Bala, Anuj Gupta, Subrat Kumar Behera, Amit Sethi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04898">https://arxiv.org/abs/2412.04898</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04898">https://arxiv.org/pdf/2412.04898</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04898]] Mitigating Instance-Dependent Label Noise: Integrating Self-Supervised Pretraining with Pseudo-Label Refinement(https://arxiv.org/abs/2412.04898)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Deep learning models rely heavily on large volumes of labeled data to achieve high performance. However, real-world datasets often contain noisy labels due to human error, ambiguity, or resource constraints during the annotation process. Instance-dependent label noise (IDN), where the probability of a label being corrupted depends on the input features, poses a significant challenge because it is more prevalent and harder to address than instance-independent noise. In this paper, we propose a novel hybrid framework that combines self-supervised learning using SimCLR with iterative pseudo-label refinement to mitigate the effects of IDN. The self-supervised pre-training phase enables the model to learn robust feature representations without relying on potentially noisy labels, establishing a noise-agnostic foundation. Subsequently, we employ an iterative training process with pseudo-label refinement, where confidently predicted samples are identified through a multistage approach and their labels are updated to improve label quality progressively. We evaluate our method on the CIFAR-10 and CIFAR-100 datasets augmented with synthetic instance-dependent noise at varying noise levels. Experimental results demonstrate that our approach significantly outperforms several state-of-the-art methods, particularly under high noise conditions, achieving notable improvements in classification accuracy and robustness. Our findings suggest that integrating self-supervised learning with iterative pseudo-label refinement offers an effective strategy for training deep neural networks on noisy datasets afflicted by instance-dependent label noise.</li>
</ul>

<h3>Title: Encryption-Aware Anomaly Detection in Power Grid Communication Networks</h3>
<ul>
<li><strong>Authors: </strong>Omer Sen, Mehdi Akbari Gurabi, Milan Deruelle, Andreas Ulbig, Stefan Decker</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04901">https://arxiv.org/abs/2412.04901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04901">https://arxiv.org/pdf/2412.04901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04901]] Encryption-Aware Anomaly Detection in Power Grid Communication Networks(https://arxiv.org/abs/2412.04901)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>The shift to smart grids has made electrical power systems more vulnerable to sophisticated cyber threats. To protect these systems, holistic security measures that encompass preventive, detective, and reactive components are required, even with encrypted data. However, traditional intrusion detection methods struggle with encrypted traffic, our research focuses on the low-level communication layers of encrypted power grid systems to identify irregular patterns using statistics and machine learning. Our results indicate that a harmonic security concept based on encrypted traffic and anomaly detection is promising for smart grid security; however, further research is necessary to improve detection accuracy.</li>
</ul>

<h3>Title: Continuous Video Process: Modeling Videos as Continuous Multi-Dimensional Processes for Video Prediction</h3>
<ul>
<li><strong>Authors: </strong>Gaurav Shrivastava, Abhinav Shrivastava</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04929">https://arxiv.org/abs/2412.04929</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04929">https://arxiv.org/pdf/2412.04929</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04929]] Continuous Video Process: Modeling Videos as Continuous Multi-Dimensional Processes for Video Prediction(https://arxiv.org/abs/2412.04929)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have made significant strides in image generation, mastering tasks such as unconditional image synthesis, text-image translation, and image-to-image conversions. However, their capability falls short in the realm of video prediction, mainly because they treat videos as a collection of independent images, relying on external constraints such as temporal attention mechanisms to enforce temporal coherence. In our paper, we introduce a novel model class, that treats video as a continuous multi-dimensional process rather than a series of discrete frames. We also report a reduction of 75\% sampling steps required to sample a new frame thus making our framework more efficient during the inference time. Through extensive experimentation, we establish state-of-the-art performance in video prediction, validated on benchmark datasets including KTH, BAIR, Human3.6M, and UCF101. Navigate to the project page this https URL for video results.}</li>
</ul>

<h3>Title: HOLa: HoloLens Object Labeling</h3>
<ul>
<li><strong>Authors: </strong>Michael Schwimmbeck, Serouj Khajarian, Konstantin Holzapfel, Johannes Schmidt, Stefanie Remmele</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04945">https://arxiv.org/abs/2412.04945</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04945">https://arxiv.org/pdf/2412.04945</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04945]] HOLa: HoloLens Object Labeling(https://arxiv.org/abs/2412.04945)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>In the context of medical Augmented Reality (AR) applications, object tracking is a key challenge and requires a significant amount of annotation masks. As segmentation foundation models like the Segment Anything Model (SAM) begin to emerge, zero-shot segmentation requires only minimal human participation obtaining high-quality object masks. We introduce a HoloLens-Object-Labeling (HOLa) Unity and Python application based on the SAM-Track algorithm that offers fully automatic single object annotation for HoloLens 2 while requiring minimal human participation. HOLa does not have to be adjusted to a specific image appearance and could thus alleviate AR research in any application field. We evaluate HOLa for different degrees of image complexity in open liver surgery and in medical phantom experiments. Using HOLa for image annotation can increase the labeling speed by more than 500 times while providing Dice scores between 0.875 and 0.982, which are comparable to human annotators. Our code is publicly available at: this https URL</li>
</ul>

<h3>Title: KaLM: Knowledge-aligned Autoregressive Language Modeling via Dual-view Knowledge Graph Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Peng Yu, Cheng Deng, Beiya Dai, Xinbing Wang, Ying Wen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04948">https://arxiv.org/abs/2412.04948</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04948">https://arxiv.org/pdf/2412.04948</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04948]] KaLM: Knowledge-aligned Autoregressive Language Modeling via Dual-view Knowledge Graph Contrastive Learning(https://arxiv.org/abs/2412.04948)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Autoregressive large language models (LLMs) pre-trained by next token prediction are inherently proficient in generative tasks. However, their performance on knowledge-driven tasks such as factual knowledge querying remains unsatisfactory. Knowledge graphs (KGs), as high-quality structured knowledge bases, can provide reliable knowledge for LLMs, potentially compensating for their knowledge deficiencies. Aligning LLMs with explicit, structured knowledge from KGs has been a challenge; previous attempts either failed to effectively align knowledge representations or compromised the generative capabilities of LLMs, leading to less-than-optimal outcomes. This paper proposes \textbf{KaLM}, a \textit{Knowledge-aligned Language Modeling} approach, which fine-tunes autoregressive LLMs to align with KG knowledge via the joint objective of explicit knowledge alignment and implicit knowledge alignment. The explicit knowledge alignment objective aims to directly optimize the knowledge representation of LLMs through dual-view knowledge graph contrastive learning. The implicit knowledge alignment objective focuses on incorporating textual patterns of knowledge into LLMs through triple completion language modeling. Notably, our method achieves a significant performance boost in evaluations of knowledge-driven tasks, specifically embedding-based knowledge graph completion and generation-based knowledge graph question answering.</li>
</ul>

<h3>Title: ETLNet: An Efficient TCN-BiLSTM Network for Road Anomaly Detection Using Smartphone Sensors</h3>
<ul>
<li><strong>Authors: </strong>Mohd Faiz Ansari, Rakshit Sandilya, Mohammed Javed, David Doermann</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.04990">https://arxiv.org/abs/2412.04990</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.04990">https://arxiv.org/pdf/2412.04990</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.04990]] ETLNet: An Efficient TCN-BiLSTM Network for Road Anomaly Detection Using Smartphone Sensors(https://arxiv.org/abs/2412.04990)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Road anomalies can be defined as irregularities on the road surface or in the surface itself. Some may be intentional (such as speedbumps), accidental (such as materials falling off a truck), or the result of roads' excessive use or low or no maintenance, such as potholes. Despite their varying origins, these irregularities often harm vehicles substantially. Speed bumps are intentionally placed for safety but are dangerous due to their non-standard shape, size, and lack of proper markings. Potholes are unintentional and can also cause severe damage. To address the detection of these anomalies, we need an automated road monitoring system. Today, various systems exist that use visual information to track these anomalies. Still, due to poor lighting conditions and improper or missing markings, they may go undetected and have severe consequences for public transport, automated vehicles, etc. In this paper, the Enhanced Temporal-BiLSTM Network (ETLNet) is introduced as a novel approach that integrates two Temporal Convolutional Network (TCN) layers with a Bidirectional Long Short-Term Memory (BiLSTM) layer. This combination is tailored to detect anomalies effectively irrespective of lighting conditions, as it depends not on visuals but smartphone inertial sensor data. Our methodology employs accelerometer and gyroscope sensors, typically in smartphones, to gather data on road conditions. Empirical evaluations demonstrate that the ETLNet model maintains an F1-score for detecting speed bumps of 99.3%. The ETLNet model's robustness and efficiency significantly advance automated road surface monitoring technologies.</li>
</ul>

<h3>Title: Noise Matters: Diffusion Model-based Urban Mobility Generation with Collaborative Noise Priors</h3>
<ul>
<li><strong>Authors: </strong>Yuheng Zhang, Yuan Yuan, Jingtao Ding, Jian Yuan, Yong Li</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05000">https://arxiv.org/abs/2412.05000</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05000">https://arxiv.org/pdf/2412.05000</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05000]] Noise Matters: Diffusion Model-based Urban Mobility Generation with Collaborative Noise Priors(https://arxiv.org/abs/2412.05000)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>With global urbanization, the focus on sustainable cities has largely grown, driving research into equity, resilience, and urban planning, which often relies on mobility data. The rise of web-based apps and mobile devices has provided valuable user data for mobility-related research. However, real-world mobility data is costly and raises privacy concerns. To protect privacy while retaining key features of real-world movement, the demand for synthetic data has steadily increased. Recent advances in diffusion models have shown great potential for mobility trajectory generation due to their ability to model randomness and uncertainty. However, existing approaches often directly apply identically distributed (i.i.d.) noise sampling from image generation techniques, which fail to account for the spatiotemporal correlations and social interactions that shape urban mobility patterns. In this paper, we propose CoDiffMob, a diffusion method for urban mobility generation with collaborative noise priors, we emphasize the critical role of noise in diffusion models for generating mobility data. By leveraging both individual movement characteristics and population-wide dynamics, we construct novel collaborative noise priors that provide richer and more informative guidance throughout the generation process. Extensive experiments demonstrate the superiority of our method, with generated data accurately capturing both individual preferences and collective patterns, achieving an improvement of over 32\%. Furthermore, it can effectively replace web-derived mobility data to better support downstream applications, while safeguarding user privacy and fostering a more secure and ethical web. This highlights its tremendous potential for applications in sustainable city-related research.</li>
</ul>

<h3>Title: ReF-LDM: A Latent Diffusion Model for Reference-based Face Image Restoration</h3>
<ul>
<li><strong>Authors: </strong>Chi-Wei Hsiao, Yu-Lun Liu, Cheng-Kun Yang, Sheng-Po Kuo, Kevin Jou, Chia-Ping Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05043">https://arxiv.org/abs/2412.05043</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05043">https://arxiv.org/pdf/2412.05043</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05043]] ReF-LDM: A Latent Diffusion Model for Reference-based Face Image Restoration(https://arxiv.org/abs/2412.05043)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>While recent works on blind face image restoration have successfully produced impressive high-quality (HQ) images with abundant details from low-quality (LQ) input images, the generated content may not accurately reflect the real appearance of a person. To address this problem, incorporating well-shot personal images as additional reference inputs could be a promising strategy. Inspired by the recent success of the Latent Diffusion Model (LDM), we propose ReF-LDM, an adaptation of LDM designed to generate HQ face images conditioned on one LQ image and multiple HQ reference images. Our model integrates an effective and efficient mechanism, CacheKV, to leverage the reference images during the generation process. Additionally, we design a timestep-scaled identity loss, enabling our LDM-based model to focus on learning the discriminating features of human faces. Lastly, we construct FFHQ-Ref, a dataset consisting of 20,405 high-quality (HQ) face images with corresponding reference images, which can serve as both training and evaluation data for reference-based face restoration models.</li>
</ul>

<h3>Title: BimArt: A Unified Approach for the Synthesis of 3D Bimanual Interaction with Articulated Objects</h3>
<ul>
<li><strong>Authors: </strong>Wanyue Zhang, Rishabh Dabral, Vladislav Golyanik, Vasileios Choutas, Eduardo Alvarado, Thabo Beeler, Marc Habermann, Christian Theobalt</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05066">https://arxiv.org/abs/2412.05066</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05066">https://arxiv.org/pdf/2412.05066</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05066]] BimArt: A Unified Approach for the Synthesis of 3D Bimanual Interaction with Articulated Objects(https://arxiv.org/abs/2412.05066)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present BimArt, a novel generative approach for synthesizing 3D bimanual hand interactions with articulated objects. Unlike prior works, we do not rely on a reference grasp, a coarse hand trajectory, or separate modes for grasping and articulating. To achieve this, we first generate distance-based contact maps conditioned on the object trajectory with an articulation-aware feature representation, revealing rich bimanual patterns for manipulation. The learned contact prior is then used to guide our hand motion generator, producing diverse and realistic bimanual motions for object movement and articulation. Our work offers key insights into feature representation and contact prior for articulated objects, demonstrating their effectiveness in taming the complex, high-dimensional space of bimanual hand-object interactions. Through comprehensive quantitative experiments, we demonstrate a clear step towards simplified and high-quality hand-object animations that excel over the state-of-the-art in motion quality and diversity.</li>
</ul>

<h3>Title: The Silent Prompt: Initial Noise as Implicit Guidance for Goal-Driven Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Ruoyu Wang, Huayang Huang, Ye Zhu, Olga Russakovsky, Yu Wu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05101">https://arxiv.org/abs/2412.05101</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05101">https://arxiv.org/pdf/2412.05101</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05101]] The Silent Prompt: Initial Noise as Implicit Guidance for Goal-Driven Image Generation(https://arxiv.org/abs/2412.05101)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Text-to-image synthesis (T2I) has advanced remarkably with the emergence of large-scale diffusion models. In the conventional setup, the text prompt provides explicit, user-defined guidance, directing the generation process by denoising a randomly sampled Gaussian noise. In this work, we reveal that the often-overlooked noise itself encodes inherent generative tendencies, acting as a "silent prompt" that implicitly guides the output. This implicit guidance, embedded in the noise scheduler design of diffusion model formulations and their training stages, generalizes across a wide range of T2I models and backbones. Building on this insight, we introduce NoiseQuery, a novel strategy that selects optimal initial noise from a pre-built noise library to meet diverse user needs. Our approach not only enhances high-level semantic alignment with text prompts, but also allows for nuanced adjustments of low-level visual attributes, such as texture, sharpness, shape, and color, which are typically challenging to control through text alone. Extensive experiments across various models and target attributes demonstrate the strong performance and zero-shot transferability of our approach, requiring no additional optimization.</li>
</ul>

<h3>Title: Learning Hidden Physics and System Parameters with Deep Operator Networks</h3>
<ul>
<li><strong>Authors: </strong>Vijay Kag, Dibakar Roy Sarkar, Birupaksha Pal, Somdatta Goswami</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05133">https://arxiv.org/abs/2412.05133</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05133">https://arxiv.org/pdf/2412.05133</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05133]] Learning Hidden Physics and System Parameters with Deep Operator Networks(https://arxiv.org/abs/2412.05133)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Big data is transforming scientific progress by enabling the discovery of novel models, enhancing existing frameworks, and facilitating precise uncertainty quantification, while advancements in scientific machine learning complement this by providing powerful tools to solve inverse problems to identify the complex systems where traditional methods falter due to sparse or noisy data. We introduce two innovative neural operator frameworks tailored for discovering hidden physics and identifying unknown system parameters from sparse measurements. The first framework integrates a popular neural operator, DeepONet, and a physics-informed neural network to capture the relationship between sparse data and the underlying physics, enabling the accurate discovery of a family of governing equations. The second framework focuses on system parameter identification, leveraging a DeepONet pre-trained on sparse sensor measurements to initialize a physics-constrained inverse model. Both frameworks excel in handling limited data and preserving physical consistency. Benchmarking on the Burgers' equation and reaction-diffusion system demonstrates state-of-the-art performance, achieving average $L_2$ errors of $\mathcal{O}(10^{-2})$ for hidden physics discovery and absolute errors of $\mathcal{O}(10^{-3})$ for parameter identification. These results underscore the frameworks' robustness, efficiency, and potential for solving complex scientific problems with minimal observational data.</li>
</ul>

<h3>Title: A text-to-tabular approach to generate synthetic patient data using LLMs</h3>
<ul>
<li><strong>Authors: </strong>Margaux Tornqvist, Jean-Daniel Zucker, Tristan Fauvel, Nicolas Lambert, Mathilde Berthelot, Antoine Movschin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05153">https://arxiv.org/abs/2412.05153</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05153">https://arxiv.org/pdf/2412.05153</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05153]] A text-to-tabular approach to generate synthetic patient data using LLMs(https://arxiv.org/abs/2412.05153)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Access to large-scale high-quality healthcare databases is key to accelerate medical research and make insightful discoveries about diseases. However, access to such data is often limited by patient privacy concerns, data sharing restrictions and high costs. To overcome these limitations, synthetic patient data has emerged as an alternative. However, synthetic data generation (SDG) methods typically rely on machine learning (ML) models trained on original data, leading back to the data scarcity problem. We propose an approach to generate synthetic tabular patient data that does not require access to the original data, but only a description of the desired database. We leverage prior medical knowledge and in-context learning capabilities of large language models (LLMs) to generate realistic patient data, even in a low-resource setting. We quantitatively evaluate our approach against state-of-the-art SDG models, using fidelity, privacy, and utility metrics. Our results show that while LLMs may not match the performance of state-of-the-art models trained on the original data, they effectively generate realistic patient data with well-preserved clinical correlations. An ablation study highlights key elements of our prompt contributing to high-quality synthetic patient data generation. This approach, which is easy to use and does not require original data or advanced ML skills, is particularly valuable for quickly generating custom-designed patient data, supporting project implementation and providing educational resources.</li>
</ul>

<h3>Title: DNF: Unconditional 4D Generation with Dictionary-based Neural Fields</h3>
<ul>
<li><strong>Authors: </strong>Xinyi Zhang, Naiqi Li, Angela Dai</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05161">https://arxiv.org/abs/2412.05161</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05161">https://arxiv.org/pdf/2412.05161</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05161]] DNF: Unconditional 4D Generation with Dictionary-based Neural Fields(https://arxiv.org/abs/2412.05161)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>While remarkable success has been achieved through diffusion-based 3D generative models for shapes, 4D generative modeling remains challenging due to the complexity of object deformations over time. We propose DNF, a new 4D representation for unconditional generative modeling that efficiently models deformable shapes with disentangled shape and motion while capturing high-fidelity details in the deforming objects. To achieve this, we propose a dictionary learning approach to disentangle 4D motion from shape as neural fields. Both shape and motion are represented as learned latent spaces, where each deformable shape is represented by its shape and motion global latent codes, shape-specific coefficient vectors, and shared dictionary information. This captures both shape-specific detail and global shared information in the learned dictionary. Our dictionary-based representation well balances fidelity, contiguity and compression -- combined with a transformer-based diffusion model, our method is able to generate effective, high-fidelity 4D animations.</li>
</ul>

<h3>Title: Variational Encoder-Decoders for Learning Latent Representations of Physical Systems</h3>
<ul>
<li><strong>Authors: </strong>Subashree Venkatasubramanian, David A. Barajas-Solano</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05175">https://arxiv.org/abs/2412.05175</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05175">https://arxiv.org/pdf/2412.05175</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05175]] Variational Encoder-Decoders for Learning Latent Representations of Physical Systems(https://arxiv.org/abs/2412.05175)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present a deep-learning Variational Encoder-Decoder (VED) framework for learning data-driven low-dimensional representations of the relationship between high-dimensional parameters of a physical system and the system's high-dimensional observable response. The framework consists of two deep learning-based probabilistic transformations: An encoder mapping parameters to latent codes and a decoder mapping latent codes to the observable response. The hyperparameters of these transformations are identified by maximizing a variational lower bound on the log-conditional distribution of the observable response given parameters. To promote the disentanglement of latent codes, we equip this variational loss with a penalty on the off-diagonal entries of the aggregate distribution covariance of codes. This regularization penalty encourages the pushforward of a standard Gaussian distribution of latent codes to approximate the marginal distribution of the observable response. Using the proposed framework we successfully model the hydraulic pressure response at observation wells of a groundwater flow model as a function of its discrete log-hydraulic transmissivity field. Compared to the canonical correlation analysis encoding, the VED model achieves a lower-dimensional latent representation, with as low as $r = 50$ latent dimensions without a significant loss of reconstruction accuracy. We explore the impact of regularization on model performance, finding that KL-divergence and covariance regularization improve feature disentanglement in latent space while maintaining reconstruction accuracy. Furthermore, we evaluate the generative capabilities of the regularized model by decoding random Gaussian noise, revealing that tuning both $\beta$ and $\lambda$ parameters enhances the quality of the generated observable response data.</li>
</ul>

<h3>Title: Enhancing Foundation Models for Time Series Forecasting via Wavelet-based Tokenization</h3>
<ul>
<li><strong>Authors: </strong>Luca Masserano, Abdul Fatir Ansari, Boran Han, Xiyuan Zhang, Christos Faloutsos, Michael W. Mahoney, Andrew Gordon Wilson, Youngsuk Park, Syama Rangapuram, Danielle C. Maddix, Yuyang Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05244">https://arxiv.org/abs/2412.05244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05244">https://arxiv.org/pdf/2412.05244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05244]] Enhancing Foundation Models for Time Series Forecasting via Wavelet-based Tokenization(https://arxiv.org/abs/2412.05244)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>How to best develop foundational models for time series forecasting remains an important open question. Tokenization is a crucial consideration in this effort: what is an effective discrete vocabulary for a real-valued sequential input? To address this question, we develop WaveToken, a wavelet-based tokenizer that allows models to learn complex representations directly in the space of time-localized frequencies. Our method first scales and decomposes the input time series, then thresholds and quantizes the wavelet coefficients, and finally pre-trains an autoregressive model to forecast coefficients for the forecast horizon. By decomposing coarse and fine structures in the inputs, wavelets provide an eloquent and compact language for time series forecasting that simplifies learning. Empirical results on a comprehensive benchmark, including 42 datasets for both in-domain and zero-shot settings, show that WaveToken: i) provides better accuracy than recently proposed foundation models for forecasting while using a much smaller vocabulary (1024 tokens), and performs on par or better than modern deep learning models trained specifically on each dataset; and ii) exhibits superior generalization capabilities, achieving the best average rank across all datasets for three complementary metrics. In addition, we show that our method can easily capture complex temporal patterns of practical relevance that are challenging for other recent pre-trained models, including trends, sparse spikes, and non-stationary time series with varying frequencies evolving over time.</li>
</ul>

<h3>Title: Extrapolated Urban View Synthesis Benchmark</h3>
<ul>
<li><strong>Authors: </strong>Xiangyu Han, Zhen Jia, Boyi Li, Yan Wang, Boris Ivanovic, Yurong You, Lingjie Liu, Yue Wang, Marco Pavone, Chen Feng, Yiming Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05256">https://arxiv.org/abs/2412.05256</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05256">https://arxiv.org/pdf/2412.05256</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05256]] Extrapolated Urban View Synthesis Benchmark(https://arxiv.org/abs/2412.05256)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Photorealistic simulators are essential for the training and evaluation of vision-centric autonomous vehicles (AVs). At their core is Novel View Synthesis (NVS), a crucial capability that generates diverse unseen viewpoints to accommodate the broad and continuous pose distribution of AVs. Recent advances in radiance fields, such as 3D Gaussian Splatting, achieve photorealistic rendering at real-time speeds and have been widely used in modeling large-scale driving scenes. However, their performance is commonly evaluated using an interpolated setup with highly correlated training and test views. In contrast, extrapolation, where test views largely deviate from training views, remains underexplored, limiting progress in generalizable simulation technology. To address this gap, we leverage publicly available AV datasets with multiple traversals, multiple vehicles, and multiple cameras to build the first Extrapolated Urban View Synthesis (EUVS) benchmark. Meanwhile, we conduct quantitative and qualitative evaluations of state-of-the-art Gaussian Splatting methods across different difficulty levels. Our results show that Gaussian Splatting is prone to overfitting to training views. Besides, incorporating diffusion priors and improving geometry cannot fundamentally improve NVS under large view changes, highlighting the need for more robust approaches and large-scale training. We have released our data to help advance self-driving and urban robotics simulation technology.</li>
</ul>

<h3>Title: Mind the Time: Temporally-Controlled Multi-Event Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Ziyi Wu, Aliaksandr Siarohin, Willi Menapace, Ivan Skorokhodov, Yuwei Fang, Varnith Chordia, Igor Gilitschenski, Sergey Tulyakov</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05263">https://arxiv.org/abs/2412.05263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05263">https://arxiv.org/pdf/2412.05263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05263]] Mind the Time: Temporally-Controlled Multi-Event Video Generation(https://arxiv.org/abs/2412.05263)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Real-world videos consist of sequences of events. Generating such sequences with precise temporal control is infeasible with existing video generators that rely on a single paragraph of text as input. When tasked with generating multiple events described using a single prompt, such methods often ignore some of the events or fail to arrange them in the correct order. To address this limitation, we present MinT, a multi-event video generator with temporal control. Our key insight is to bind each event to a specific period in the generated video, which allows the model to focus on one event at a time. To enable time-aware interactions between event captions and video tokens, we design a time-based positional encoding method, dubbed ReRoPE. This encoding helps to guide the cross-attention operation. By fine-tuning a pre-trained video diffusion transformer on temporally grounded data, our approach produces coherent videos with smoothly connected events. For the first time in the literature, our model offers control over the timing of events in generated videos. Extensive experiments demonstrate that MinT outperforms existing open-source models by a large margin.</li>
</ul>

<h3>Title: Chimera: Accurate retrosynthesis prediction by ensembling models with diverse inductive biases</h3>
<ul>
<li><strong>Authors: </strong>Krzysztof Maziarz, Guoqing Liu, Hubert Misztela, Aleksei Kornev, Piotr Gaiski, Holger Hoefling, Mike Fortunato, Rishi Gupta, Marwin Segler</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05269">https://arxiv.org/abs/2412.05269</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05269">https://arxiv.org/pdf/2412.05269</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05269]] Chimera: Accurate retrosynthesis prediction by ensembling models with diverse inductive biases(https://arxiv.org/abs/2412.05269)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Planning and conducting chemical syntheses remains a major bottleneck in the discovery of functional small molecules, and prevents fully leveraging generative AI for molecular inverse design. While early work has shown that ML-based retrosynthesis models can predict reasonable routes, their low accuracy for less frequent, yet important reactions has been pointed out. As multi-step search algorithms are limited to reactions suggested by the underlying model, the applicability of those tools is inherently constrained by the accuracy of retrosynthesis prediction. Inspired by how chemists use different strategies to ideate reactions, we propose Chimera: a framework for building highly accurate reaction models that combine predictions from diverse sources with complementary inductive biases using a learning-based ensembling strategy. We instantiate the framework with two newly developed models, which already by themselves achieve state of the art in their categories. Through experiments across several orders of magnitude in data scale and time-splits, we show Chimera outperforms all major models by a large margin, owing both to the good individual performance of its constituents, but also to the scalability of our ensembling strategy. Moreover, we find that PhD-level organic chemists prefer predictions from Chimera over baselines in terms of quality. Finally, we transfer the largest-scale checkpoint to an internal dataset from a major pharmaceutical company, showing robust generalization under distribution shift. With the new dimension that our framework unlocks, we anticipate further acceleration in the development of even more accurate models.</li>
</ul>

<h3>Title: MotionFlow: Attention-Driven Motion Transfer in Video Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Tuna Han Salih Meral, Hidir Yesiltepe, Connor Dunlop, Pinar Yanardag</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05275">https://arxiv.org/abs/2412.05275</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05275">https://arxiv.org/pdf/2412.05275</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05275]] MotionFlow: Attention-Driven Motion Transfer in Video Diffusion Models(https://arxiv.org/abs/2412.05275)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Text-to-video models have demonstrated impressive capabilities in producing diverse and captivating video content, showcasing a notable advancement in generative AI. However, these models generally lack fine-grained control over motion patterns, limiting their practical applicability. We introduce MotionFlow, a novel framework designed for motion transfer in video diffusion models. Our method utilizes cross-attention maps to accurately capture and manipulate spatial and temporal dynamics, enabling seamless motion transfers across various contexts. Our approach does not require training and works on test-time by leveraging the inherent capabilities of pre-trained video diffusion models. In contrast to traditional approaches, which struggle with comprehensive scene changes while maintaining consistent motion, MotionFlow successfully handles such complex transformations through its attention-based mechanism. Our qualitative and quantitative experiments demonstrate that MotionFlow significantly outperforms existing models in both fidelity and versatility even during drastic scene alterations.</li>
</ul>

<h3>Title: Sparse autoencoders reveal selective remapping of visual concepts during adaptation</h3>
<ul>
<li><strong>Authors: </strong>Hyesu Lim, Jinho Choi, Jaegul Choo, Steffen Schneider</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05276">https://arxiv.org/abs/2412.05276</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05276">https://arxiv.org/pdf/2412.05276</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05276]] Sparse autoencoders reveal selective remapping of visual concepts during adaptation(https://arxiv.org/abs/2412.05276)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Adapting foundation models for specific purposes has become a standard approach to build machine learning systems for downstream applications. Yet, it is an open question which mechanisms take place during adaptation. Here we develop a new Sparse Autoencoder (SAE) for the CLIP vision transformer, named PatchSAE, to extract interpretable concepts at granular levels (e.g. shape, color, or semantics of an object) and their patch-wise spatial attributions. We explore how these concepts influence the model output in downstream image classification tasks and investigate how recent state-of-the-art prompt-based adaptation techniques change the association of model inputs to these concepts. While activations of concepts slightly change between adapted and non-adapted models, we find that the majority of gains on common adaptation tasks can be explained with the existing concepts already present in the non-adapted foundation model. This work provides a concrete framework to train and use SAEs for Vision Transformers and provides insights into explaining adaptation mechanisms.</li>
</ul>

<h3>Title: Birth and Death of a Rose</h3>
<ul>
<li><strong>Authors: </strong>Chen Geng, Yunzhi Zhang, Shangzhe Wu, Jiajun Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05278">https://arxiv.org/abs/2412.05278</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05278">https://arxiv.org/pdf/2412.05278</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05278]] Birth and Death of a Rose(https://arxiv.org/abs/2412.05278)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>We study the problem of generating temporal object intrinsics -- temporally evolving sequences of object geometry, reflectance, and texture, such as a blooming rose -- from pre-trained 2D foundation models. Unlike conventional 3D modeling and animation techniques that require extensive manual effort and expertise, we introduce a method that generates such assets with signals distilled from pre-trained 2D diffusion models. To ensure the temporal consistency of object intrinsics, we propose Neural Templates for temporal-state-guided distillation, derived automatically from image features from self-supervised learning. Our method can generate high-quality temporal object intrinsics for several natural phenomena and enable the sampling and controllable rendering of these dynamic objects from any viewpoint, under any environmental lighting conditions, at any time of their lifespan. Project website: this https URL</li>
</ul>

<h3>Title: Perturb-and-Revise: Flexible 3D Editing with Generative Trajectories</h3>
<ul>
<li><strong>Authors: </strong>Susung Hong, Johanna Karras, Ricardo Martin-Brualla, Ira Kemelmacher-Shlizerman</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05279">https://arxiv.org/abs/2412.05279</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05279">https://arxiv.org/pdf/2412.05279</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05279]] Perturb-and-Revise: Flexible 3D Editing with Generative Trajectories(https://arxiv.org/abs/2412.05279)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The fields of 3D reconstruction and text-based 3D editing have advanced significantly with the evolution of text-based diffusion models. While existing 3D editing methods excel at modifying color, texture, and style, they struggle with extensive geometric or appearance changes, thus limiting their applications. We propose Perturb-and-Revise, which makes possible a variety of NeRF editing. First, we perturb the NeRF parameters with random initializations to create a versatile initialization. We automatically determine the perturbation magnitude through analysis of the local loss landscape. Then, we revise the edited NeRF via generative trajectories. Combined with the generative process, we impose identity-preserving gradients to refine the edited NeRF. Extensive experiments demonstrate that Perturb-and-Revise facilitates flexible, effective, and consistent editing of color, appearance, and geometry in 3D. For 360 results, please visit our project page: this https URL.</li>
</ul>

<h3>Title: Stag-1: Towards Realistic 4D Driving Simulation with Video Generation Model</h3>
<ul>
<li><strong>Authors: </strong>Lening Wang, Wenzhao Zheng, Dalong Du, Yunpeng Zhang, Yilong Ren, Han Jiang, Zhiyong Cui, Haiyang Yu, Jie Zhou, Jiwen Lu, Shanghang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.05280">https://arxiv.org/abs/2412.05280</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.05280">https://arxiv.org/pdf/2412.05280</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.05280]] Stag-1: Towards Realistic 4D Driving Simulation with Video Generation Model(https://arxiv.org/abs/2412.05280)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>4D driving simulation is essential for developing realistic autonomous driving simulators. Despite advancements in existing methods for generating driving scenes, significant challenges remain in view transformation and spatial-temporal dynamic modeling. To address these limitations, we propose a Spatial-Temporal simulAtion for drivinG (Stag-1) model to reconstruct real-world scenes and design a controllable generative network to achieve 4D simulation. Stag-1 constructs continuous 4D point cloud scenes using surround-view data from autonomous vehicles. It decouples spatial-temporal relationships and produces coherent keyframe videos. Additionally, Stag-1 leverages video generation models to obtain photo-realistic and controllable 4D driving simulation videos from any perspective. To expand the range of view generation, we train vehicle motion videos based on decomposed camera poses, enhancing modeling capabilities for distant scenes. Furthermore, we reconstruct vehicle camera trajectories to integrate 3D points across consecutive views, enabling comprehensive scene understanding along the temporal dimension. Following extensive multi-level scene training, Stag-1 can simulate from any desired viewpoint and achieve a deep understanding of scene evolution under static spatial-temporal conditions. Compared to existing methods, our approach shows promising performance in multi-view scene consistency, background coherence, and accuracy, and contributes to the ongoing advancements in realistic autonomous driving simulation. Code: this https URL.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
