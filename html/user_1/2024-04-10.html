<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-04-10</h1>
<h3>Title: Streamlining Ocean Dynamics Modeling with Fourier Neural Operators: A  Multiobjective Hyperparameter and Architecture Optimization Approach</h3>
<ul>
<li><strong>Authors: </strong>Yixuan Sun, Ololade Sowunmi, Romain Egele, Sri Hari Krishna Narayanan, Luke Van Roekel, Prasanna Balaprakash</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.ao-ph, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.05768">https://arxiv.org/abs/2404.05768</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.05768">https://arxiv.org/pdf/2404.05768</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.05768]] Streamlining Ocean Dynamics Modeling with Fourier Neural Operators: A  Multiobjective Hyperparameter and Architecture Optimization Approach(https://arxiv.org/abs/2404.05768)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Training an effective deep learning model to learn ocean processes involves careful choices of various hyperparameters. We leverage DeepHyper's advanced search algorithms for multiobjective optimization, streamlining the development of neural networks tailored for ocean modeling. The focus is on optimizing Fourier neural operators (FNOs), a data-driven model capable of simulating complex ocean behaviors. Selecting the correct model and tuning the hyperparameters are challenging tasks, requiring much effort to ensure model accuracy. DeepHyper allows efficient exploration of hyperparameters associated with data preprocessing, FNO architecture-related hyperparameters, and various model training strategies. We aim to obtain an optimal set of hyperparameters leading to the most performant model. Moreover, on top of the commonly used mean squared error for model training, we propose adopting the negative anomaly correlation coefficient as the additional loss term to improve model performance and investigate the potential trade-off between the two terms. The experimental results show that the optimal set of hyperparameters enhanced model performance in single timestepping forecasting and greatly exceeded the baseline configuration in the autoregressive rollout for long-horizon forecasting up to 30 days. Utilizing DeepHyper, we demonstrate an approach to enhance the use of FNOs in ocean dynamics forecasting, offering a scalable solution with improved precision.</li>
</ul>

<h3>Title: WILBUR: Adaptive In-Context Learning for Robust and Accurate Web Agents</h3>
<ul>
<li><strong>Authors: </strong>Michael Lutz, Arth Bohra, Manvel Saroyan, Artem Harutyunyan, Giovanni Campagna</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.05902">https://arxiv.org/abs/2404.05902</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.05902">https://arxiv.org/pdf/2404.05902</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.05902]] WILBUR: Adaptive In-Context Learning for Robust and Accurate Web Agents(https://arxiv.org/abs/2404.05902)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, in-context</a></li>
<li><strong>Abstract: </strong>In the realm of web agent research, achieving both generalization and accuracy remains a challenging problem. Due to high variance in website structure, existing approaches often fail. Moreover, existing fine-tuning and in-context learning techniques fail to generalize across multiple websites. We introduce Wilbur, an approach that uses a differentiable ranking model and a novel instruction synthesis technique to optimally populate a black-box large language model's prompt with task demonstrations from previous runs. To maximize end-to-end success rates, we also propose an intelligent backtracking mechanism that learns and recovers from its mistakes. Finally, we show that our ranking model can be trained on data from a generative auto-curriculum which samples representative goals from an LLM, runs the agent, and automatically evaluates it, with no manual annotation. Wilbur achieves state-of-the-art results on the WebVoyager benchmark, beating text-only models by 8% overall, and up to 36% on certain websites. On the same benchmark, Wilbur is within 5% of a strong multi-modal model despite only receiving textual inputs, and further analysis reveals a substantial number of failures are due to engineering challenges of operating the web.</li>
</ul>

<h3>Title: StoryImager: A Unified and Efficient Framework for Coherent Story  Visualization and Completion</h3>
<ul>
<li><strong>Authors: </strong>Ming Tao, Bing-Kun Bao, Hao Tang, Yaowei Wang, Changsheng Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.05979">https://arxiv.org/abs/2404.05979</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.05979">https://arxiv.org/pdf/2404.05979</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.05979]] StoryImager: A Unified and Efficient Framework for Coherent Story  Visualization and Completion(https://arxiv.org/abs/2404.05979)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Story visualization aims to generate a series of realistic and coherent images based on a storyline. Current models adopt a frame-by-frame architecture by transforming the pre-trained text-to-image model into an auto-regressive manner. Although these models have shown notable progress, there are still three flaws. 1) The unidirectional generation of auto-regressive manner restricts the usability in many scenarios. 2) The additional introduced story history encoders bring an extremely high computational cost. 3) The story visualization and continuation models are trained and inferred independently, which is not user-friendly. To these ends, we propose a bidirectional, unified, and efficient framework, namely StoryImager. The StoryImager enhances the storyboard generative ability inherited from the pre-trained text-to-image model for a bidirectional generation. Specifically, we introduce a Target Frame Masking Strategy to extend and unify different story image generation tasks. Furthermore, we propose a Frame-Story Cross Attention Module that decomposes the cross attention for local fidelity and global coherence. Moreover, we design a Contextual Feature Extractor to extract contextual information from the whole storyline. The extensive experimental results demonstrate the excellent performance of our StoryImager. The code is available at https://github.com/tobran/StoryImager.</li>
</ul>

<h3>Title: Tackling Structural Hallucination in Image Translation with Local  Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Seunghoi Kim, Chen Jin, Tom Diethe, Matteo Figini, Henry F. J. Tregidgo, Asher Mullokandov, Philip Teare, Daniel C. Alexander</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.05980">https://arxiv.org/abs/2404.05980</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.05980">https://arxiv.org/pdf/2404.05980</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.05980]] Tackling Structural Hallucination in Image Translation with Local  Diffusion(https://arxiv.org/abs/2404.05980)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent developments in diffusion models have advanced conditioned image generation, yet they struggle with reconstructing out-of-distribution (OOD) images, such as unseen tumors in medical images, causing ``image hallucination'' and risking misdiagnosis. We hypothesize such hallucinations result from local OOD regions in the conditional images. We verify that partitioning the OOD region and conducting separate image generations alleviates hallucinations in several applications. From this, we propose a training-free diffusion framework that reduces hallucination with multiple Local Diffusion processes. Our approach involves OOD estimation followed by two modules: a ``branching'' module generates locally both within and outside OOD regions, and a ``fusion'' module integrates these predictions into one. Our evaluation shows our method mitigates hallucination over baseline models quantitatively and qualitatively, reducing misdiagnosis by 40% and 25% in the real-world medical and natural image datasets, respectively. It also demonstrates compatibility with various pre-trained diffusion models.</li>
</ul>

<h3>Title: Boosting Digital Safeguards: Blending Cryptography and Steganography</h3>
<ul>
<li><strong>Authors: </strong>Anamitra Maiti, Subham Laha, Rishav Upadhaya, Soumyajit Biswas, Vikas Choudhary, Biplab Kar, Nikhil Kumar, Jaydip Sen</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.05985">https://arxiv.org/abs/2404.05985</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.05985">https://arxiv.org/pdf/2404.05985</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.05985]] Boosting Digital Safeguards: Blending Cryptography and Steganography(https://arxiv.org/abs/2404.05985)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In today's digital age, the internet is essential for communication and the sharing of information, creating a critical need for sophisticated data security measures to prevent unauthorized access and exploitation. Cryptography encrypts messages into a cipher text that is incomprehensible to unauthorized readers, thus safeguarding data during its transmission. Steganography, on the other hand, originates from the Greek term for "covered writing" and involves the art of hiding data within another medium, thereby facilitating covert communication by making the message invisible. This proposed approach takes advantage of the latest advancements in Artificial Intelligence (AI) and Deep Learning (DL), especially through the application of Generative Adversarial Networks (GANs), to improve upon traditional steganographic methods. By embedding encrypted data within another medium, our method ensures that the communication remains hidden from prying eyes. The application of GANs enables a smart, secure system that utilizes the inherent sensitivity of neural networks to slight alterations in data, enhancing the protection against detection. By merging the encryption techniques of cryptography with the hiding capabilities of steganography, and augmenting these with the strengths of AI, we introduce a comprehensive security system designed to maintain both the privacy and integrity of information. This system is crafted not just to prevent unauthorized access or modification of data, but also to keep the existence of the data hidden. This fusion of technologies tackles the core challenges of data security in the current era of open digital communication, presenting an advanced solution with the potential to transform the landscape of information security.</li>
</ul>

<h3>Title: Event-enhanced Retrieval in Real-time Search</h3>
<ul>
<li><strong>Authors: </strong>Yanan Zhang, Xiaoling Bai, Tianhua Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.05989">https://arxiv.org/abs/2404.05989</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.05989">https://arxiv.org/pdf/2404.05989</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.05989]] Event-enhanced Retrieval in Real-time Search(https://arxiv.org/abs/2404.05989)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The embedding-based retrieval (EBR) approach is widely used in mainstream search engine retrieval systems and is crucial in recent retrieval-augmented methods for eliminating LLM illusions. However, existing EBR models often face the "semantic drift" problem and insufficient focus on key information, leading to a low adoption rate of retrieval results in subsequent steps. This issue is especially noticeable in real-time search scenarios, where the various expressions of popular events on the Internet make real-time retrieval heavily reliant on crucial event information. To tackle this problem, this paper proposes a novel approach called EER, which enhances real-time retrieval performance by improving the dual-encoder model of traditional EBR. We incorporate contrastive learning to accompany pairwise learning for encoder optimization. Furthermore, to strengthen the focus on critical event information in events, we include a decoder module after the document encoder, introduce a generative event triplet extraction scheme based on prompt-tuning, and correlate the events with query encoder optimization through comparative learning. This decoder module can be removed during inference. Extensive experiments demonstrate that EER can significantly improve the real-time search retrieval performance. We believe that this approach will provide new perspectives in the field of information retrieval. The codes and dataset are available at https://github.com/open-event-hub/Event-enhanced_Retrieval .</li>
</ul>

<h3>Title: AEGIS: Online Adaptive AI Content Safety Moderation with Ensemble of LLM  Experts</h3>
<ul>
<li><strong>Authors: </strong>Shaona Ghosh, Prasoon Varshney, Erick Galinkin, Christopher Parisien</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL, cs.CY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.05993">https://arxiv.org/abs/2404.05993</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.05993">https://arxiv.org/pdf/2404.05993</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.05993]] AEGIS: Online Adaptive AI Content Safety Moderation with Ensemble of LLM  Experts(https://arxiv.org/abs/2404.05993)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>As Large Language Models (LLMs) and generative AI become more widespread, the content safety risks associated with their use also increase. We find a notable deficiency in high-quality content safety datasets and benchmarks that comprehensively cover a wide range of critical safety areas. To address this, we define a broad content safety risk taxonomy, comprising 13 critical risk and 9 sparse risk categories. Additionally, we curate AEGISSAFETYDATASET, a new dataset of approximately 26, 000 human-LLM interaction instances, complete with human annotations adhering to the taxonomy. We plan to release this dataset to the community to further research and to help benchmark LLM models for safety. To demonstrate the effectiveness of the dataset, we instruction-tune multiple LLM-based safety models. We show that our models (named AEGISSAFETYEXPERTS), not only surpass or perform competitively with the state-of-the-art LLM-based safety models and general purpose LLMs, but also exhibit robustness across multiple jail-break attack categories. We also show how using AEGISSAFETYDATASET during the LLM alignment phase does not negatively impact the performance of the aligned models on MT Bench scores. Furthermore, we propose AEGIS, a novel application of a no-regret online adaptation framework with strong theoretical guarantees, to perform content moderation with an ensemble of LLM content safety experts in deployment</li>
</ul>

<h3>Title: Privacy Preserving Prompt Engineering: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Kennedy Edemacu, Xintao Wu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06001">https://arxiv.org/abs/2404.06001</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06001">https://arxiv.org/pdf/2404.06001</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06001]] Privacy Preserving Prompt Engineering: A Survey(https://arxiv.org/abs/2404.06001)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Pre-trained language models (PLMs) have demonstrated significant proficiency in solving a wide range of general natural language processing (NLP) tasks. Researchers have observed a direct correlation between the performance of these models and their sizes. As a result, the sizes of these models have notably expanded in recent years, persuading researchers to adopt the term large language models (LLMs) to characterize the larger-sized PLMs. The increased size is accompanied by a distinct capability known as in-context learning (ICL), which represents a specialized form of prompting. This enables the utilization of LLMs for specific downstream tasks by presenting them with demonstration examples while keeping the model parameters frozen. Although interesting, privacy concerns have become a major obstacle in its widespread usage. Multiple studies have examined the privacy risks linked to ICL and prompting in general, and have devised techniques to alleviate these risks. Thus, there is a necessity to organize these mitigation techniques for the benefit of the community. This survey provides a systematic overview of the privacy protection methods employed during ICL and prompting in general. We review, analyze, and compare different methods under this paradigm. Furthermore, we provide a summary of the resources accessible for the development of these frameworks. Finally, we discuss the limitations of these frameworks and offer a detailed examination of the promising areas that necessitate further exploration.</li>
</ul>

<h3>Title: Diffusion-Based Point Cloud Super-Resolution for mmWave Radar Data</h3>
<ul>
<li><strong>Authors: </strong>Kai Luan, Chenghao Shi, Neng Wang, Yuwei Cheng, Huimin Lu, Xieyuanli Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06012">https://arxiv.org/abs/2404.06012</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06012">https://arxiv.org/pdf/2404.06012</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06012]] Diffusion-Based Point Cloud Super-Resolution for mmWave Radar Data(https://arxiv.org/abs/2404.06012)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The millimeter-wave radar sensor maintains stable performance under adverse environmental conditions, making it a promising solution for all-weather perception tasks, such as outdoor mobile robotics. However, the radar point clouds are relatively sparse and contain massive ghost points, which greatly limits the development of mmWave radar technology. In this paper, we propose a novel point cloud super-resolution approach for 3D mmWave radar data, named Radar-diffusion. Our approach employs the diffusion model defined by mean-reverting stochastic differential equations(SDE). Using our proposed new objective function with supervision from corresponding LiDAR point clouds, our approach efficiently handles radar ghost points and enhances the sparse mmWave radar point clouds to dense LiDAR-like point clouds. We evaluate our approach on two different datasets, and the experimental results show that our method outperforms the state-of-the-art baseline methods in 3D radar super-resolution tasks. Furthermore, we demonstrate that our enhanced radar point cloud is capable of downstream radar point-based registration tasks.</li>
</ul>

<h3>Title: Greedy-DiM: Greedy Algorithms for Unreasonably Effective Face Morphs</h3>
<ul>
<li><strong>Authors: </strong>Zander W. Blasingame, Chen Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06025">https://arxiv.org/abs/2404.06025</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06025">https://arxiv.org/pdf/2404.06025</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06025]] Greedy-DiM: Greedy Algorithms for Unreasonably Effective Face Morphs(https://arxiv.org/abs/2404.06025)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Morphing attacks are an emerging threat to state-of-the-art Face Recognition (FR) systems, which aim to create a single image that contains the biometric information of multiple identities. Diffusion Morphs (DiM) are a recently proposed morphing attack that has achieved state-of-the-art performance for representation-based morphing attacks. However, none of the existing research on DiMs have leveraged the iterative nature of DiMs and left the DiM model as a black box, treating it no differently than one would a Generative Adversarial Network (GAN) or Varational AutoEncoder (VAE). We propose a greedy strategy on the iterative sampling process of DiM models which searches for an optimal step guided by an identity-based heuristic function. We compare our proposed algorithm against ten other state-of-the-art morphing algorithms using the open-source SYN-MAD 2022 competition dataset. We find that our proposed algorithm is unreasonably effective, fooling all of the tested FR systems with an MMPMR of 100%, outperforming all other morphing algorithms compared.</li>
</ul>

<h3>Title: All in One: An Empirical Study of GPT for Few-Shot Aspect-Based  Sentiment Anlaysis</h3>
<ul>
<li><strong>Authors: </strong>Baoxing Jiang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06063">https://arxiv.org/abs/2404.06063</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06063">https://arxiv.org/pdf/2404.06063</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06063]] All in One: An Empirical Study of GPT for Few-Shot Aspect-Based  Sentiment Anlaysis(https://arxiv.org/abs/2404.06063)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Aspect-Based Sentiment Analysis (ABSA) is an indispensable and highly challenging task in natural language processing. Current efforts have focused on specific sub-tasks, making it difficult to comprehensively cover all sub-tasks within the ABSA domain. With the development of Generative Pre-trained Transformers (GPTs), there came inspiration for a one-stop solution to sentiment analysis. In this study, we used GPTs for all sub-tasks of few-shot ABSA while defining a general learning paradigm for this application. We propose the All in One (AiO) model, a simple yet effective two-stage model for all ABSA sub-tasks. In the first stage, a specific backbone network learns the semantic information of the review and generates heuristically enhanced candidates. In the second stage, AiO leverages GPT contextual learning capabilities to generate predictions. The study conducted comprehensive comparative and ablation experiments on five benchmark datasets, and the results show that AiO can effectively handle all ABSA sub-tasks, even with few-shot data.</li>
</ul>

<h3>Title: Hash3D: Training-free Acceleration for 3D Generation</h3>
<ul>
<li><strong>Authors: </strong>Xingyi Yang, Xinchao Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06091">https://arxiv.org/abs/2404.06091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06091">https://arxiv.org/pdf/2404.06091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06091]] Hash3D: Training-free Acceleration for 3D Generation(https://arxiv.org/abs/2404.06091)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The evolution of 3D generative modeling has been notably propelled by the adoption of 2D diffusion models. Despite this progress, the cumbersome optimization process per se presents a critical hurdle to efficiency. In this paper, we introduce Hash3D, a universal acceleration for 3D generation without model training. Central to Hash3D is the insight that feature-map redundancy is prevalent in images rendered from camera positions and diffusion time-steps in close proximity. By effectively hashing and reusing these feature maps across neighboring timesteps and camera angles, Hash3D substantially prevents redundant calculations, thus accelerating the diffusion model's inference in 3D generation tasks. We achieve this through an adaptive grid-based hashing. Surprisingly, this feature-sharing mechanism not only speed up the generation but also enhances the smoothness and view consistency of the synthesized 3D objects. Our experiments covering 5 text-to-3D and 3 image-to-3D models, demonstrate Hash3D's versatility to speed up optimization, enhancing efficiency by 1.3 to 4 times. Additionally, Hash3D's integration with 3D Gaussian splatting largely speeds up 3D model creation, reducing text-to-3D processing to about 10 minutes and image-to-3D conversion to roughly 30 seconds. The project page is at https://adamdad.github.io/hash3D/.</li>
</ul>

<h3>Title: Cendol: Open Instruction-tuned Generative Large Language Models for  Indonesian Languages</h3>
<ul>
<li><strong>Authors: </strong>Samuel Cahyawijaya, Holy Lovenia, Fajri Koto, Rifki Afina Putri, Emmanuel Dave, Jhonson Lee, Nuur Shadieq, Wawan Cenggoro, Salsabil Maulana Akbar, Muhammad Ihza Mahendra, Dea Annisayanti Putri, Bryan Wilie, Genta Indra Winata, Alham Fikri Aji, Ayu Purwarianti, Pascale Fung</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06138">https://arxiv.org/abs/2404.06138</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06138">https://arxiv.org/pdf/2404.06138</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06138]] Cendol: Open Instruction-tuned Generative Large Language Models for  Indonesian Languages(https://arxiv.org/abs/2404.06138)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) show remarkable human-like capability in various domains and languages. However, a notable quality gap arises in low-resource languages, e.g., Indonesian indigenous languages, rendering them ineffective and inefficient in such linguistic contexts. To bridge this quality gap, we introduce Cendol, a collection of Indonesian LLMs encompassing both decoder-only and encoder-decoder architectures across a range of model sizes. We highlight Cendol's effectiveness across a diverse array of tasks, attaining 20% improvement, and demonstrate its capability to generalize to unseen tasks and indigenous languages of Indonesia. Furthermore, Cendol models showcase improved human favorability despite their limitations in capturing indigenous knowledge and cultural values in Indonesia. In addition, we discuss the shortcomings of parameter-efficient tunings, such as LoRA, for language adaptation. Alternatively, we propose the usage of vocabulary adaptation to enhance efficiency. Lastly, we evaluate the safety of Cendol and showcase that safety in pre-training in one language such as English is transferable to low-resource languages, such as Indonesian, even without RLHF and safety fine-tuning.</li>
</ul>

<h3>Title: DiffHarmony: Latent Diffusion Model Meets Image Harmonization</h3>
<ul>
<li><strong>Authors: </strong>Pengfei Zhou, Fangxiang Feng, Xiaojie Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06139">https://arxiv.org/abs/2404.06139</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06139">https://arxiv.org/pdf/2404.06139</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06139]] DiffHarmony: Latent Diffusion Model Meets Image Harmonization(https://arxiv.org/abs/2404.06139)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Image harmonization, which involves adjusting the foreground of a composite image to attain a unified visual consistency with the background, can be conceptualized as an image-to-image translation task. Diffusion models have recently promoted the rapid development of image-to-image translation tasks . However, training diffusion models from scratch is computationally intensive. Fine-tuning pre-trained latent diffusion models entails dealing with the reconstruction error induced by the image compression autoencoder, making it unsuitable for image generation tasks that involve pixel-level evaluation metrics. To deal with these issues, in this paper, we first adapt a pre-trained latent diffusion model to the image harmonization task to generate the harmonious but potentially blurry initial images. Then we implement two strategies: utilizing higher-resolution images during inference and incorporating an additional refinement stage, to further enhance the clarity of the initially harmonized images. Extensive experiments on iHarmony4 datasets demonstrate the superiority of our proposed method. The code and model will be made publicly available at https://github.com/nicecv/DiffHarmony .</li>
</ul>

<h3>Title: Differential Privacy for Anomaly Detection: Analyzing the Trade-off  Between Privacy and Explainability</h3>
<ul>
<li><strong>Authors: </strong>Fatima Ezzeddine, Mirna Saad, Omran Ayoub, Davide Andreoletti, Martin Gjoreski, Ihab Sbeity, Marc Langheinrich, Silvia Giordano</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06144">https://arxiv.org/abs/2404.06144</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06144">https://arxiv.org/pdf/2404.06144</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06144]] Differential Privacy for Anomaly Detection: Analyzing the Trade-off  Between Privacy and Explainability(https://arxiv.org/abs/2404.06144)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Anomaly detection (AD), also referred to as outlier detection, is a statistical process aimed at identifying observations within a dataset that significantly deviate from the expected pattern of the majority of the data. Such a process finds wide application in various fields, such as finance and healthcare. While the primary objective of AD is to yield high detection accuracy, the requirements of explainability and privacy are also paramount. The first ensures the transparency of the AD process, while the second guarantees that no sensitive information is leaked to untrusted parties. In this work, we exploit the trade-off of applying Explainable AI (XAI) through SHapley Additive exPlanations (SHAP) and differential privacy (DP). We perform AD with different models and on various datasets, and we thoroughly evaluate the cost of privacy in terms of decreased accuracy and explainability. Our results show that the enforcement of privacy through DP has a significant impact on detection accuracy and explainability, which depends on both the dataset and the considered AD model. We further show that the visual interpretation of explanations is also influenced by the choice of the AD algorithm.</li>
</ul>

<h3>Title: HFNeRF: Learning Human Biomechanic Features with Neural Radiance Fields</h3>
<ul>
<li><strong>Authors: </strong>Arnab Dey, Di Yang, Antitza Dantcheva, Jean Martinet</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06152">https://arxiv.org/abs/2404.06152</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06152">https://arxiv.org/pdf/2404.06152</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06152]] HFNeRF: Learning Human Biomechanic Features with Neural Radiance Fields(https://arxiv.org/abs/2404.06152)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>In recent advancements in novel view synthesis, generalizable Neural Radiance Fields (NeRF) based methods applied to human subjects have shown remarkable results in generating novel views from few images. However, this generalization ability cannot capture the underlying structural features of the skeleton shared across all instances. Building upon this, we introduce HFNeRF: a novel generalizable human feature NeRF aimed at generating human biomechanic features using a pre-trained image encoder. While previous human NeRF methods have shown promising results in the generation of photorealistic virtual avatars, such methods lack underlying human structure or biomechanic features such as skeleton or joint information that are crucial for downstream applications including Augmented Reality (AR)/Virtual Reality (VR). HFNeRF leverages 2D pre-trained foundation models toward learning human features in 3D using neural rendering, and then volume rendering towards generating 2D feature maps. We evaluate HFNeRF in the skeleton estimation task by predicting heatmaps as features. The proposed method is fully differentiable, allowing to successfully learn color, geometry, and human skeleton in a simultaneous manner. This paper presents preliminary results of HFNeRF, illustrating its potential in generating realistic virtual avatars with biomechanic features using NeRF.</li>
</ul>

<h3>Title: scRDiT: Generating single-cell RNA-seq data by diffusion transformers  and accelerating sampling</h3>
<ul>
<li><strong>Authors: </strong>Shengze Dong, Zhuorui Cui, Ding Liu, Jinzhi Lei</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06153">https://arxiv.org/abs/2404.06153</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06153">https://arxiv.org/pdf/2404.06153</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06153]] scRDiT: Generating single-cell RNA-seq data by diffusion transformers  and accelerating sampling(https://arxiv.org/abs/2404.06153)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Motivation: Single-cell RNA sequencing (scRNA-seq) is a groundbreaking technology extensively utilized in biological research, facilitating the examination of gene expression at the individual cell level within a given tissue sample. While numerous tools have been developed for scRNA-seq data analysis, the challenge persists in capturing the distinct features of such data and replicating virtual datasets that share analogous statistical properties. Results: Our study introduces a generative approach termed scRNA-seq Diffusion Transformer (scRDiT). This method generates virtual scRNA-seq data by leveraging a real dataset. The method is a neural network constructed based on Denoising Diffusion Probabilistic Models (DDPMs) and Diffusion Transformers (DiTs). This involves subjecting Gaussian noises to the real dataset through iterative noise-adding steps and ultimately restoring the noises to form scRNA-seq samples. This scheme allows us to learn data features from actual scRNA-seq samples during model training. Our experiments, conducted on two distinct scRNA-seq datasets, demonstrate superior performance. Additionally, the model sampling process is expedited by incorporating Denoising Diffusion Implicit Models (DDIM). scRDiT presents a unified methodology empowering users to train neural network models with their unique scRNA-seq datasets, enabling the generation of numerous high-quality scRNA-seq samples. Availability and implementation: https://github.com/DongShengze/scRDiT</li>
</ul>

<h3>Title: scCDCG: Efficient Deep Structural Clustering for single-cell RNA-seq via  Deep Cut-informed Graph Embedding</h3>
<ul>
<li><strong>Authors: </strong>Ping Xu, Zhiyuan Ning, Meng Xiao, Guihai Feng, Xin Li, Yuanchun Zhou, Pengfei Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06167">https://arxiv.org/abs/2404.06167</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06167">https://arxiv.org/pdf/2404.06167</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06167]] scCDCG: Efficient Deep Structural Clustering for single-cell RNA-seq via  Deep Cut-informed Graph Embedding(https://arxiv.org/abs/2404.06167)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Single-cell RNA sequencing (scRNA-seq) is essential for unraveling cellular heterogeneity and diversity, offering invaluable insights for bioinformatics advancements. Despite its potential, traditional clustering methods in scRNA-seq data analysis often neglect the structural information embedded in gene expression profiles, crucial for understanding cellular correlations and dependencies. Existing strategies, including graph neural networks, face challenges in handling the inefficiency due to scRNA-seq data's intrinsic high-dimension and high-sparsity. Addressing these limitations, we introduce scCDCG (single-cell RNA-seq Clustering via Deep Cut-informed Graph), a novel framework designed for efficient and accurate clustering of scRNA-seq data that simultaneously utilizes intercellular high-order structural information. scCDCG comprises three main components: (i) A graph embedding module utilizing deep cut-informed techniques, which effectively captures intercellular high-order structural information, overcoming the over-smoothing and inefficiency issues prevalent in prior graph neural network methods. (ii) A self-supervised learning module guided by optimal transport, tailored to accommodate the unique complexities of scRNA-seq data, specifically its high-dimension and high-sparsity. (iii) An autoencoder-based feature learning module that simplifies model complexity through effective dimension reduction and feature extraction. Our extensive experiments on 6 datasets demonstrate scCDCG's superior performance and efficiency compared to 7 established models, underscoring scCDCG's potential as a transformative tool in scRNA-seq data analysis. Our code is available at: https://github.com/XPgogogo/scCDCG.</li>
</ul>

<h3>Title: Improving Interpretable Embeddings for Ad-hoc Video Search with  Generative Captions and Multi-word Concept Bank</h3>
<ul>
<li><strong>Authors: </strong>Jiaxin Wu, Chong-Wah Ngo, Wing-Kwong Chan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06173">https://arxiv.org/abs/2404.06173</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06173">https://arxiv.org/pdf/2404.06173</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06173]] Improving Interpretable Embeddings for Ad-hoc Video Search with  Generative Captions and Multi-word Concept Bank(https://arxiv.org/abs/2404.06173)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Aligning a user query and video clips in cross-modal latent space and that with semantic concepts are two mainstream approaches for ad-hoc video search (AVS). However, the effectiveness of existing approaches is bottlenecked by the small sizes of available video-text datasets and the low quality of concept banks, which results in the failures of unseen queries and the out-of-vocabulary problem. This paper addresses these two problems by constructing a new dataset and developing a multi-word concept bank. Specifically, capitalizing on a generative model, we construct a new dataset consisting of 7 million generated text and video pairs for pre-training. To tackle the out-of-vocabulary problem, we develop a multi-word concept bank based on syntax analysis to enhance the capability of a state-of-the-art interpretable AVS method in modeling relationships between query words. We also study the impact of current advanced features on the method. Experimental results show that the integration of the above-proposed elements doubles the R@1 performance of the AVS method on the MSRVTT dataset and improves the xinfAP on the TRECVid AVS query sets for 2016-2023 (eight years) by a margin from 2% to 77%, with an average about 20%.</li>
</ul>

<h3>Title: Exploring the Potential of Large Foundation Models for Open-Vocabulary  HOI Detection</h3>
<ul>
<li><strong>Authors: </strong>Ting Lei, Shaofeng Yin, Yang Liu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06194">https://arxiv.org/abs/2404.06194</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06194">https://arxiv.org/pdf/2404.06194</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06194]] Exploring the Potential of Large Foundation Models for Open-Vocabulary  HOI Detection(https://arxiv.org/abs/2404.06194)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Open-vocabulary human-object interaction (HOI) detection, which is concerned with the problem of detecting novel HOIs guided by natural language, is crucial for understanding human-centric scenes. However, prior zero-shot HOI detectors often employ the same levels of feature maps to model HOIs with varying distances, leading to suboptimal performance in scenes containing human-object pairs with a wide range of distances. In addition, these detectors primarily rely on category names and overlook the rich contextual information that language can provide, which is essential for capturing open vocabulary concepts that are typically rare and not well-represented by category names alone. In this paper, we introduce a novel end-to-end open vocabulary HOI detection framework with conditional multi-level decoding and fine-grained semantic enhancement (CMD-SE), harnessing the potential of Visual-Language Models (VLMs). Specifically, we propose to model human-object pairs with different distances with different levels of feature maps by incorporating a soft constraint during the bipartite matching process. Furthermore, by leveraging large language models (LLMs) such as GPT models, we exploit their extensive world knowledge to generate descriptions of human body part states for various interactions. Then we integrate the generalizable and fine-grained semantics of human body parts to improve interaction recognition. Experimental results on two datasets, SWIG-HOI and HICO-DET, demonstrate that our proposed method achieves state-of-the-art results in open vocabulary HOI detection. The code and models are available at https://github.com/ltttpku/CMD-SE-release.</li>
</ul>

<h3>Title: Elephants Never Forget: Memorization and Learning of Tabular Data in  Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Sebastian Bordt, Harsha Nori, Vanessa Rodrigues, Besmira Nushi, Rich Caruana</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06209">https://arxiv.org/abs/2404.06209</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06209">https://arxiv.org/pdf/2404.06209</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06209]] Elephants Never Forget: Memorization and Learning of Tabular Data in  Large Language Models(https://arxiv.org/abs/2404.06209)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>While many have shown how Large Language Models (LLMs) can be applied to a diverse set of tasks, the critical issues of data contamination and memorization are often glossed over. In this work, we address this concern for tabular data. Specifically, we introduce a variety of different techniques to assess whether a language model has seen a tabular dataset during training. This investigation reveals that LLMs have memorized many popular tabular datasets verbatim. We then compare the few-shot learning performance of LLMs on datasets that were seen during training to the performance on datasets released after training. We find that LLMs perform better on datasets seen during training, indicating that memorization leads to overfitting. At the same time, LLMs show non-trivial performance on novel datasets and are surprisingly robust to data transformations. We then investigate the in-context statistical learning abilities of LLMs. Without fine-tuning, we find them to be limited. This suggests that much of the few-shot performance on novel datasets is due to the LLM's world knowledge. Overall, our results highlight the importance of testing whether an LLM has seen an evaluation dataset during pre-training. We make the exposure tests we developed available as the tabmemcheck Python package at https://github.com/interpretml/LLM-Tabular-Memorization-Checker</li>
</ul>

<h3>Title: From Barlow Twins to Triplet Training: Differentiating Dementia with  Limited Data</h3>
<ul>
<li><strong>Authors: </strong>Yitong Li, Tom Nuno Wolf, Sebastian PÃ¶lsterl, Igor Yakushev, Dennis M. Hedderich, Christian Wachinger</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06253">https://arxiv.org/abs/2404.06253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06253">https://arxiv.org/pdf/2404.06253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06253]] From Barlow Twins to Triplet Training: Differentiating Dementia with  Limited Data(https://arxiv.org/abs/2404.06253)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Differential diagnosis of dementia is challenging due to overlapping symptoms, with structural magnetic resonance imaging (MRI) being the primary method for diagnosis. Despite the clinical value of computer-aided differential diagnosis, research has been limited, mainly due to the absence of public datasets that contain diverse types of dementia. This leaves researchers with small in-house datasets that are insufficient for training deep neural networks (DNNs). Self-supervised learning shows promise for utilizing unlabeled MRI scans in training, but small batch sizes for volumetric brain scans make its application challenging. To address these issues, we propose Triplet Training for differential diagnosis with limited target data. It consists of three key stages: (i) self-supervised pre-training on unlabeled data with Barlow Twins, (ii) self-distillation on task-related data, and (iii) fine-tuning on the target dataset. Our approach significantly outperforms traditional training strategies, achieving a balanced accuracy of 75.6%. We further provide insights into the training process by visualizing changes in the latent space after each step. Finally, we validate the robustness of Triplet Training in terms of its individual components in a comprehensive ablation study. Our code is available at https://github.com/ai-med/TripletTraining.</li>
</ul>

<h3>Title: Playing to Vision Foundation Model's Strengths in Stereo Matching</h3>
<ul>
<li><strong>Authors: </strong>Chuang-Wei Liu, Qijun Chen, Rui Fan</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06261">https://arxiv.org/abs/2404.06261</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06261">https://arxiv.org/pdf/2404.06261</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06261]] Playing to Vision Foundation Model's Strengths in Stereo Matching(https://arxiv.org/abs/2404.06261)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Stereo matching has become a key technique for 3D environment perception in intelligent vehicles. For a considerable time, convolutional neural networks (CNNs) have remained the mainstream choice for feature extraction in this domain. Nonetheless, there is a growing consensus that the existing paradigm should evolve towards vision foundation models (VFM), particularly those developed based on vision Transformers (ViTs) and pre-trained through self-supervision on extensive, unlabeled datasets. While VFMs are adept at extracting informative, general-purpose visual features, specifically for dense prediction tasks, their performance often lacks in geometric vision tasks. This study serves as the first exploration of a viable approach for adapting VFMs to stereo matching. Our ViT adapter, referred to as ViTAS, is constructed upon three types of modules: spatial differentiation, patch attention fusion, and cross-attention. The first module initializes feature pyramids, while the latter two aggregate stereo and multi-scale contextual information into fine-grained features, respectively. ViTAStereo, which combines ViTAS with cost volume-based stereo matching back-end processes, achieves the top rank on the KITTI Stereo 2012 dataset and outperforms the second-best network StereoBase by approximately 7.9% in terms of the percentage of error pixels, with a tolerance of 3 pixels. Additional experiments across diverse scenarios further demonstrate its superior generalizability compared to all other state-of-the-art approaches. We believe this new paradigm will pave the way for the next generation of stereo matching networks.</li>
</ul>

<h3>Title: Learning Embeddings with Centroid Triplet Loss for Object Identification  in Robotic Grasping</h3>
<ul>
<li><strong>Authors: </strong>Anas Gouda, Max Schwarz, Christopher Reining, Sven Behnke, Alice Kirchheim</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06277">https://arxiv.org/abs/2404.06277</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06277">https://arxiv.org/pdf/2404.06277</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06277]] Learning Embeddings with Centroid Triplet Loss for Object Identification  in Robotic Grasping(https://arxiv.org/abs/2404.06277)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Foundation models are a strong trend in deep learning and computer vision. These models serve as a base for applications as they require minor or no further fine-tuning by developers to integrate into their applications. Foundation models for zero-shot object segmentation such as Segment Anything (SAM) output segmentation masks from images without any further object information. When they are followed in a pipeline by an object identification model, they can perform object detection without training. Here, we focus on training such an object identification model. A crucial practical aspect for an object identification model is to be flexible in input size. As object identification is an image retrieval problem, a suitable method should handle multi-query multi-gallery situations without constraining the number of input images (e.g. by having fixed-size aggregation layers). The key solution to train such a model is the centroid triplet loss (CTL), which aggregates image features to their centroids. CTL yields high accuracy, avoids misleading training signals and keeps the model input size flexible. In our experiments, we establish a new state of the art on the ArmBench object identification task, which shows general applicability of our model. We furthermore demonstrate an integrated unseen object detection pipeline on the challenging HOPE dataset, which requires fine-grained detection. There, our pipeline matches and surpasses related methods which have been trained on dataset-specific data.</li>
</ul>

<h3>Title: NoiseNCA: Noisy Seed Improves Spatio-Temporal Continuity of Neural  Cellular Automata</h3>
<ul>
<li><strong>Authors: </strong>Ehsan Pajouheshgar, Yitao Xu, Sabine SÃ¼sstrunk</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06279">https://arxiv.org/abs/2404.06279</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06279">https://arxiv.org/pdf/2404.06279</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06279]] NoiseNCA: Noisy Seed Improves Spatio-Temporal Continuity of Neural  Cellular Automata(https://arxiv.org/abs/2404.06279)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Neural Cellular Automata (NCA) is a class of Cellular Automata where the update rule is parameterized by a neural network that can be trained using gradient descent. In this paper, we focus on NCA models used for texture synthesis, where the update rule is inspired by partial differential equations (PDEs) describing reaction-diffusion systems. To train the NCA model, the spatio-termporal domain is discretized, and Euler integration is used to numerically simulate the PDE. However, whether a trained NCA truly learns the continuous dynamic described by the corresponding PDE or merely overfits the discretization used in training remains an open question. We study NCA models at the limit where space-time discretization approaches continuity. We find that existing NCA models tend to overfit the training discretization, especially in the proximity of the initial condition, also called "seed". To address this, we propose a solution that utilizes uniform noise as the initial condition. We demonstrate the effectiveness of our approach in preserving the consistency of NCA dynamics across a wide range of spatio-temporal granularities. Our improved NCA model enables two new test-time interactions by allowing continuous control over the speed of pattern formation and the scale of the synthesized patterns. We demonstrate this new NCA feature in our interactive online demo. Our work reveals that NCA models can learn continuous dynamics and opens new venues for NCA research from a dynamical systems' perspective.</li>
</ul>

<h3>Title: Generative Pre-Trained Transformer for Symbolic Regression Base  In-Context Reinforcement Learning</h3>
<ul>
<li><strong>Authors: </strong>Yanjie Li, Weijun Li, Lina Yu, Min Wu, Jingyi Liu, Wenqiang Li, Meilan Hao, Shu Wei, Yusong Deng</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06330">https://arxiv.org/abs/2404.06330</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06330">https://arxiv.org/pdf/2404.06330</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06330]] Generative Pre-Trained Transformer for Symbolic Regression Base  In-Context Reinforcement Learning(https://arxiv.org/abs/2404.06330)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, in-context</a></li>
<li><strong>Abstract: </strong>The mathematical formula is the human language to describe nature and is the essence of scientific research. Finding mathematical formulas from observational data is a major demand of scientific research and a major challenge of artificial intelligence. This area is called symbolic regression. Originally symbolic regression was often formulated as a combinatorial optimization problem and solved using GP or reinforcement learning algorithms. These two kinds of algorithms have strong noise robustness ability and good Versatility. However, inference time usually takes a long time, so the search efficiency is relatively low. Later, based on large-scale pre-training data proposed, such methods use a large number of synthetic data points and expression pairs to train a Generative Pre-Trained Transformer(GPT). Then this GPT can only need to perform one forward propagation to obtain the results, the advantage is that the inference speed is very fast. However, its performance is very dependent on the training data and performs poorly on data outside the training set, which leads to poor noise robustness and Versatility of such methods. So, can we combine the advantages of the above two categories of SR algorithms? In this paper, we propose \textbf{FormulaGPT}, which trains a GPT using massive sparse reward learning histories of reinforcement learning-based SR algorithms as training data. After training, the SR algorithm based on reinforcement learning is distilled into a Transformer. When new test data comes, FormulaGPT can directly generate a "reinforcement learning process" and automatically update the learning policy in context. Tested on more than ten datasets including SRBench, formulaGPT achieves the state-of-the-art performance in fitting ability compared with four baselines. In addition, it achieves satisfactory results in noise robustness, versatility, and inference efficiency.</li>
</ul>

<h3>Title: Policy-Guided Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Matthew Thomas Jackson, Michael Tryfan Matthews, Cong Lu, Benjamin Ellis, Shimon Whiteson, Jakob Foerster</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06356">https://arxiv.org/abs/2404.06356</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06356">https://arxiv.org/pdf/2404.06356</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06356]] Policy-Guided Diffusion(https://arxiv.org/abs/2404.06356)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In many real-world settings, agents must learn from an offline dataset gathered by some prior behavior policy. Such a setting naturally leads to distribution shift between the behavior policy and the target policy being trained - requiring policy conservatism to avoid instability and overestimation bias. Autoregressive world models offer a different solution to this by generating synthetic, on-policy experience. However, in practice, model rollouts must be severely truncated to avoid compounding error. As an alternative, we propose policy-guided diffusion. Our method uses diffusion models to generate entire trajectories under the behavior distribution, applying guidance from the target policy to move synthetic experience further on-policy. We show that policy-guided diffusion models a regularized form of the target distribution that balances action likelihood under both the target and behavior policies, leading to plausible trajectories with high target policy probability, while retaining a lower dynamics error than an offline world model baseline. Using synthetic experience from policy-guided diffusion as a drop-in substitute for real data, we demonstrate significant improvements in performance across a range of standard offline reinforcement learning algorithms and environments. Our approach provides an effective alternative to autoregressive offline world models, opening the door to the controllable generation of synthetic training data.</li>
</ul>

<h3>Title: Test-Time Adaptation with SaLIP: A Cascade of SAM and CLIP for Zero shot  Medical Image Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Sidra Aleem, Fangyijie Wang, Mayug Maniparambil, Eric Arazo, Julia Dietlmeier, Kathleen Curran, Noel E. O'Connor, Suzanne Little</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06362">https://arxiv.org/abs/2404.06362</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06362">https://arxiv.org/pdf/2404.06362</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06362]] Test-Time Adaptation with SaLIP: A Cascade of SAM and CLIP for Zero shot  Medical Image Segmentation(https://arxiv.org/abs/2404.06362)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>The Segment Anything Model (SAM) and CLIP are remarkable vision foundation models (VFMs). SAM, a prompt driven segmentation model, excels in segmentation tasks across diverse domains, while CLIP is renowned for its zero shot recognition capabilities. However, their unified potential has not yet been explored in medical image segmentation. To adapt SAM to medical imaging, existing methods primarily rely on tuning strategies that require extensive data or prior prompts tailored to the specific task, making it particularly challenging when only a limited number of data samples are available. This work presents an in depth exploration of integrating SAM and CLIP into a unified framework for medical image segmentation. Specifically, we propose a simple unified framework, SaLIP, for organ segmentation. Initially, SAM is used for part based segmentation within the image, followed by CLIP to retrieve the mask corresponding to the region of interest (ROI) from the pool of SAM generated masks. Finally, SAM is prompted by the retrieved ROI to segment a specific organ. Thus, SaLIP is training and fine tuning free and does not rely on domain expertise or labeled data for prompt engineering. Our method shows substantial enhancements in zero shot segmentation, showcasing notable improvements in DICE scores across diverse segmentation tasks like brain (63.46%), lung (50.11%), and fetal head (30.82%), when compared to un prompted SAM. Code and text prompts will be available online.</li>
</ul>

<h3>Title: ZeST: Zero-Shot Material Transfer from a Single Image</h3>
<ul>
<li><strong>Authors: </strong>Ta-Ying Cheng, Prafull Sharma, Andrew Markham, Niki Trigoni, Varun Jampani</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06425">https://arxiv.org/abs/2404.06425</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06425">https://arxiv.org/pdf/2404.06425</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06425]] ZeST: Zero-Shot Material Transfer from a Single Image(https://arxiv.org/abs/2404.06425)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We propose ZeST, a method for zero-shot material transfer to an object in the input image given a material exemplar image. ZeST leverages existing diffusion adapters to extract implicit material representation from the exemplar image. This representation is used to transfer the material using pre-trained inpainting diffusion model on the object in the input image using depth estimates as geometry cue and grayscale object shading as illumination cues. The method works on real images without any training resulting a zero-shot approach. Both qualitative and quantitative results on real and synthetic datasets demonstrate that ZeST outputs photorealistic images with transferred materials. We also show the application of ZeST to perform multiple edits and robust material assignment under different illuminations. Project Page: https://ttchengab.github.io/zest</li>
</ul>

<h3>Title: Magic-Boost: Boost 3D Generation with Mutli-View Conditioned Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Fan Yang, Jianfeng Zhang, Yichun Shi, Bowen Chen, Chenxu Zhang, Huichao Zhang, Xiaofeng Yang, Jiashi Feng, Guosheng Lin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06429">https://arxiv.org/abs/2404.06429</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06429">https://arxiv.org/pdf/2404.06429</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06429]] Magic-Boost: Boost 3D Generation with Mutli-View Conditioned Diffusion(https://arxiv.org/abs/2404.06429)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Benefiting from the rapid development of 2D diffusion models, 3D content creation has made significant progress recently. One promising solution involves the fine-tuning of pre-trained 2D diffusion models to harness their capacity for producing multi-view images, which are then lifted into accurate 3D models via methods like fast-NeRFs or large reconstruction models. However, as inconsistency still exists and limited generated resolution, the generation results of such methods still lack intricate textures and complex geometries. To solve this problem, we propose Magic-Boost, a multi-view conditioned diffusion model that significantly refines coarse generative results through a brief period of SDS optimization ($\sim15$min). Compared to the previous text or single image based diffusion models, Magic-Boost exhibits a robust capability to generate images with high consistency from pseudo synthesized multi-view images. It provides precise SDS guidance that well aligns with the identity of the input images, enriching the local detail in both geometry and texture of the initial generative results. Extensive experiments show Magic-Boost greatly enhances the coarse inputs and generates high-quality 3D assets with rich geometric and textural details. (Project Page: https://magic-research.github.io/magic-boost/)</li>
</ul>

<h3>Title: Automated Federated Pipeline for Parameter-Efficient Fine-Tuning of  Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Zihan Fang, Zheng Lin, Zhe Chen, Xianhao Chen, Yue Gao, Yuguang Fang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2404.06448">https://arxiv.org/abs/2404.06448</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2404.06448">https://arxiv.org/pdf/2404.06448</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2404.06448]] Automated Federated Pipeline for Parameter-Efficient Fine-Tuning of  Large Language Models(https://arxiv.org/abs/2404.06448)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recently, there has been a surge in the development of advanced intelligent generative content (AIGC), especially large language models (LLMs). However, for many downstream tasks, it is necessary to fine-tune LLMs using private data. While federated learning offers a promising privacy-preserving solution to LLM fine-tuning, the substantial size of an LLM, combined with high computational and communication demands, makes it hard to apply to downstream tasks. More importantly, private edge servers often possess varying computing and network resources in real-world scenarios, introducing additional complexities to LLM fine-tuning. To tackle these problems, we design and implement an automated federated pipeline, named FedPipe, to fine-tune LLMs with minimal training cost but without adding any inference latency. FedPipe firstly identifies the weights to be fine-tuned based on their contributions to the LLM training. It then configures a low-rank adapter for each selected weight to train local low-rank adapters on an edge server, and aggregate local adapters of all edge servers to fine-tune the whole LLM. Finally, it appropriately quantizes the parameters of LLM to reduce memory space according to the requirements of edge servers. Extensive experiments demonstrate that FedPipe expedites the model training and achieves higher accuracy than state-of-the-art benchmarks.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
