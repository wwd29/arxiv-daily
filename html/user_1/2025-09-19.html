<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-09-19</h1>
<h3>Title: LLM-JEPA: Large Language Models Meet Joint Embedding Predictive Architectures</h3>
<ul>
<li><strong>Authors: </strong>Hai Huang, Yann LeCun, Randall Balestriero</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14252">https://arxiv.org/abs/2509.14252</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14252">https://arxiv.org/pdf/2509.14252</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14252]] LLM-JEPA: Large Language Models Meet Joint Embedding Predictive Architectures(https://arxiv.org/abs/2509.14252)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large Language Model (LLM) pretraining, finetuning, and evaluation rely on input-space reconstruction and generative capabilities. Yet, it has been observed in vision that embedding-space training objectives, e.g., with Joint Embedding Predictive Architectures (JEPAs), are far superior to their input-space counterpart. That mismatch in how training is achieved between language and vision opens up a natural question: {\em can language training methods learn a few tricks from the vision ones?} The lack of JEPA-style LLM is a testimony of the challenge in designing such objectives for language. In this work, we propose a first step in that direction where we develop LLM-JEPA, a JEPA based solution for LLMs applicable both to finetuning and pretraining. Thus far, LLM-JEPA is able to outperform the standard LLM training objectives by a significant margin across models, all while being robust to overfiting. Those findings are observed across numerous datasets (NL-RX, GSM8K, Spider, RottenTomatoes) and various models from the Llama3, OpenELM, Gemma2 and Olmo families. Code: this https URL.</li>
</ul>

<h3>Title: Persuasive or Neutral? A Field Experiment on Generative AI in Online Travel Planning</h3>
<ul>
<li><strong>Authors: </strong>Lynna Jirpongopas, Bernhard Lutz, JÃ¶rg Ebner, Rustam Vahidov, Dirk Neumann</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14259">https://arxiv.org/abs/2509.14259</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14259">https://arxiv.org/pdf/2509.14259</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14259]] Persuasive or Neutral? A Field Experiment on Generative AI in Online Travel Planning(https://arxiv.org/abs/2509.14259)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative AI (GenAI) offers new opportunities for customer support in online travel agencies, yet little is known about how its design influences user engagement, purchase behavior, and user experience. We report results from a randomized field experiment in online travel itinerary planning, comparing GenAI that expressed (A) positive enthusiasm, (B) neutral expression, and (C) no tone instructions (control). Users in group A wrote significantly longer prompts than those in groups B and C. At the same time, users in groups A and B were more likely to purchase subscriptions of the webservice. We further analyze linguistic cues across experimental groups to explore differences in user experience and explain subscription purchases and affiliate link clicks based on these cues. Our findings provide implications for the design of persuasive and engaging GenAI interfaces in consumer-facing contexts and contribute to understanding how linguistic framing shapes user behavior in AI-mediated decision support.</li>
</ul>

<h3>Title: Discovering New Theorems via LLMs with In-Context Proof Learning in Lean</h3>
<ul>
<li><strong>Authors: </strong>Kazumi Kasaura, Naoto Onda, Yuta Oriike, Masaya Taniguchi, Akiyoshi Sannai, Sho Sonoda</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14274">https://arxiv.org/abs/2509.14274</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14274">https://arxiv.org/pdf/2509.14274</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14274]] Discovering New Theorems via LLMs with In-Context Proof Learning in Lean(https://arxiv.org/abs/2509.14274)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large Language Models have demonstrated significant promise in formal theorem proving. However, previous works mainly focus on solving existing problems. In this paper, we focus on the ability of LLMs to find novel theorems. We propose Conjecturing-Proving Loop pipeline for automatically generating mathematical conjectures and proving them in Lean 4 format. A feature of our approach is that we generate and prove further conjectures with context including previously generated theorems and their proofs, which enables the generation of more difficult proofs by in-context learning of proof strategies without changing parameters of LLMs. We demonstrated that our framework rediscovered theorems with verification, which were published in past mathematical papers and have not yet formalized. Moreover, at least one of these theorems could not be proved by the LLM without in-context learning, even in natural language, which means that in-context learning was effective for neural theorem proving. The source code is available at this https URL.</li>
</ul>

<h3>Title: Causal-Counterfactual RAG: The Integration of Causal-Counterfactual Reasoning into RAG</h3>
<ul>
<li><strong>Authors: </strong>Harshad Khadilkar, Abhay Gupta</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14435">https://arxiv.org/abs/2509.14435</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14435">https://arxiv.org/pdf/2509.14435</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14435]] Causal-Counterfactual RAG: The Integration of Causal-Counterfactual Reasoning into RAG(https://arxiv.org/abs/2509.14435)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have transformed natural language processing (NLP), enabling diverse applications by integrating large-scale pre-trained knowledge. However, their static knowledge limits dynamic reasoning over external information, especially in knowledge-intensive domains. Retrieval-Augmented Generation (RAG) addresses this challenge by combining retrieval mechanisms with generative modeling to improve contextual understanding. Traditional RAG systems suffer from disrupted contextual integrity due to text chunking and over-reliance on semantic similarity for retrieval, often resulting in shallow and less accurate responses. We propose Causal-Counterfactual RAG, a novel framework that integrates explicit causal graphs representing cause-effect relationships into the retrieval process and incorporates counterfactual reasoning grounded on the causal structure. Unlike conventional methods, our framework evaluates not only direct causal evidence but also the counterfactuality of associated causes, combining results from both to generate more robust, accurate, and interpretable answers. By leveraging causal pathways and associated hypothetical scenarios, Causal-Counterfactual RAG preserves contextual coherence, reduces hallucination, and enhances reasoning fidelity.</li>
</ul>

<h3>Title: Not What the Doctor Ordered: Surveying LLM-based De-identification and Quantifying Clinical Information Loss</h3>
<ul>
<li><strong>Authors: </strong>Kiana Aghakasiri, Noopur Zambare, JoAnn Thai, Carrie Ye, Mayur Mehta, J. Ross Mitchell, Mohamed Abdalla</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14464">https://arxiv.org/abs/2509.14464</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14464">https://arxiv.org/pdf/2509.14464</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14464]] Not What the Doctor Ordered: Surveying LLM-based De-identification and Quantifying Clinical Information Loss(https://arxiv.org/abs/2509.14464)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>De-identification in the healthcare setting is an application of NLP where automated algorithms are used to remove personally identifying information of patients (and, sometimes, providers). With the recent rise of generative large language models (LLMs), there has been a corresponding rise in the number of papers that apply LLMs to de-identification. Although these approaches often report near-perfect results, significant challenges concerning reproducibility and utility of the research papers persist. This paper identifies three key limitations in the current literature: inconsistent reporting metrics hindering direct comparisons, the inadequacy of traditional classification metrics in capturing errors which LLMs may be more prone to (i.e., altering clinically relevant information), and lack of manual validation of automated metrics which aim to quantify these errors. To address these issues, we first present a survey of LLM-based de-identification research, highlighting the heterogeneity in reporting standards. Second, we evaluated a diverse set of models to quantify the extent of inappropriate removal of clinical information. Next, we conduct a manual validation of an existing evaluation metric to measure the removal of clinical information, employing clinical experts to assess their efficacy. We highlight poor performance and describe the inherent limitations of such metrics in identifying clinically significant changes. Lastly, we propose a novel methodology for the detection of clinically relevant information removal.</li>
</ul>

<h3>Title: H-Alpha Anomalyzer: An Explainable Anomaly Detector for Solar H-Alpha Observations</h3>
<ul>
<li><strong>Authors: </strong>Mahsa Khazaei, Azim Ahmadzadeh, Alexei Pevtsov, Luca Bertello, Alexander Pevtsov</a></li>
<li><strong>Subjects: </strong>cs.LG, astro-ph.IM, astro-ph.SR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14472">https://arxiv.org/abs/2509.14472</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14472">https://arxiv.org/pdf/2509.14472</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14472]] H-Alpha Anomalyzer: An Explainable Anomaly Detector for Solar H-Alpha Observations(https://arxiv.org/abs/2509.14472)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>The plethora of space-borne and ground-based observatories has provided astrophysicists with an unprecedented volume of data, which can only be processed at scale using advanced computing algorithms. Consequently, ensuring the quality of data fed into machine learning (ML) models is critical. The H$\alpha$ observations from the GONG network represent one such data stream, producing several observations per minute, 24/7, since 2010. In this study, we introduce a lightweight (non-ML) anomaly-detection algorithm, called H-Alpha Anomalyzer, designed to identify anomalous observations based on user-defined criteria. Unlike many black-box algorithms, our approach highlights exactly which regions triggered the anomaly flag and quantifies the corresponding anomaly likelihood. For our comparative analysis, we also created and released a dataset of 2,000 observations, equally divided between anomalous and non-anomalous cases. Our results demonstrate that the proposed model not only outperforms existing methods but also provides explainability, enabling qualitative evaluation by domain experts.</li>
</ul>

<h3>Title: Process-Supervised Reinforcement Learning for Interactive Multimodal Tool-Use Agents</h3>
<ul>
<li><strong>Authors: </strong>Weiting Tan, Xinghua Qu, Ming Tu, Meng Ge, Andy T. Liu, Philipp Koehn, Lu Lu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14480">https://arxiv.org/abs/2509.14480</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14480">https://arxiv.org/pdf/2509.14480</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14480]] Process-Supervised Reinforcement Learning for Interactive Multimodal Tool-Use Agents(https://arxiv.org/abs/2509.14480)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Effective interactive tool use requires agents to master Tool Integrated Reasoning (TIR): a complex process involving multi-turn planning and long-context dialogue management. To train agents for this dynamic process, particularly in multi-modal contexts, we introduce a sandbox environment for reinforcement learning (RL) that supports interleaved speech-text rollouts. Our core strategy, Turn-level Adjudicated Reinforcement Learning (TARL), addresses the challenge of credit assignment in long-horizon tasks by employing a Large Language Model (LLM) as a judge to provide turn-level evaluation. To enhance exploration, we integrate a mixed-task training curriculum with mathematical reasoning problems. This unified approach boosts the task pass rate on the text-based $\tau$-bench by over 6% compared to strong RL baselines. Crucially, we demonstrate our framework's suitability for fine-tuning a multi-modal foundation model for agentic tasks. By training a base multi-modal LLM on interleaved speech-text rollouts, we equip it with tool-use abilities, paving the way for more natural, voice-driven interactive agents.</li>
</ul>

<h3>Title: Catch Me If You Can? Not Yet: LLMs Still Struggle to Imitate the Implicit Writing Styles of Everyday Authors</h3>
<ul>
<li><strong>Authors: </strong>Zhengxiang Wang, Nafis Irtiza Tripto, Solha Park, Zhenzhen Li, Jiawei Zhou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14543">https://arxiv.org/abs/2509.14543</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14543">https://arxiv.org/pdf/2509.14543</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14543]] Catch Me If You Can? Not Yet: LLMs Still Struggle to Imitate the Implicit Writing Styles of Everyday Authors(https://arxiv.org/abs/2509.14543)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>As large language models (LLMs) become increasingly integrated into personal writing tools, a critical question arises: can LLMs faithfully imitate an individual's writing style from just a few examples? Personal style is often subtle and implicit, making it difficult to specify through prompts yet essential for user-aligned generation. This work presents a comprehensive evaluation of state-of-the-art LLMs' ability to mimic personal writing styles via in-context learning from a small number of user-authored samples. We introduce an ensemble of complementary metrics-including authorship attribution, authorship verification, style matching, and AI detection-to robustly assess style imitation. Our evaluation spans over 40000 generations per model across domains such as news, email, forums, and blogs, covering writing samples from more than 400 real-world authors. Results show that while LLMs can approximate user styles in structured formats like news and email, they struggle with nuanced, informal writing in blogs and forums. Further analysis on various prompting strategies such as number of demonstrations reveal key limitations in effective personalization. Our findings highlight a fundamental gap in personalized LLM adaptation and the need for improved techniques to support implicit, style-consistent generation. To aid future research and for reproducibility, we open-source our data and code.</li>
</ul>

<h3>Title: Adaptive and Iterative Point Cloud Denoising with Score-Based Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Zhaonan Wang, Manyi Li, ShiQing Xin, Changhe Tu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14560">https://arxiv.org/abs/2509.14560</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14560">https://arxiv.org/pdf/2509.14560</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14560]] Adaptive and Iterative Point Cloud Denoising with Score-Based Diffusion Model(https://arxiv.org/abs/2509.14560)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Point cloud denoising task aims to recover the clean point cloud from the scanned data coupled with different levels or patterns of noise. The recent state-of-the-art methods often train deep neural networks to update the point locations towards the clean point cloud, and empirically repeat the denoising process several times in order to obtain the denoised results. It is not clear how to efficiently arrange the iterative denoising processes to deal with different levels or patterns of noise. In this paper, we propose an adaptive and iterative point cloud denoising method based on the score-based diffusion model. For a given noisy point cloud, we first estimate the noise variation and determine an adaptive denoising schedule with appropriate step sizes, then invoke the trained network iteratively to update point clouds following the adaptive schedule. To facilitate this adaptive and iterative denoising process, we design the network architecture and a two-stage sampling strategy for the network training to enable feature fusion and gradient fusion for iterative denoising. Compared to the state-of-the-art point cloud denoising methods, our approach obtains clean and smooth denoised point clouds, while preserving the shape boundary and details better. Our results not only outperform the other methods both qualitatively and quantitatively, but also are preferable on the synthetic dataset with different patterns of noises, as well as the real-scanned dataset.</li>
</ul>

<h3>Title: Learning to Retrieve for Environmental Knowledge Discovery: An Augmentation-Adaptive Self-Supervised Learning Framework</h3>
<ul>
<li><strong>Authors: </strong>Shiyuan Luo, Runlong Yu, Chonghao Qiu, Rahul Ghosh, Robert Ladwig, Paul C. Hanson, Yiqun Xie, Xiaowei Jia</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14563">https://arxiv.org/abs/2509.14563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14563">https://arxiv.org/pdf/2509.14563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14563]] Learning to Retrieve for Environmental Knowledge Discovery: An Augmentation-Adaptive Self-Supervised Learning Framework(https://arxiv.org/abs/2509.14563)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>The discovery of environmental knowledge depends on labeled task-specific data, but is often constrained by the high cost of data collection. Existing machine learning approaches usually struggle to generalize in data-sparse or atypical conditions. To this end, we propose an Augmentation-Adaptive Self-Supervised Learning (A$^2$SL) framework, which retrieves relevant observational samples to enhance modeling of the target ecosystem. Specifically, we introduce a multi-level pairwise learning loss to train a scenario encoder that captures varying degrees of similarity among scenarios. These learned similarities drive a retrieval mechanism that supplements a target scenario with relevant data from different locations or time periods. Furthermore, to better handle variable scenarios, particularly under atypical or extreme conditions where traditional models struggle, we design an augmentation-adaptive mechanism that selectively enhances these scenarios through targeted data augmentation. Using freshwater ecosystems as a case study, we evaluate A$^2$SL in modeling water temperature and dissolved oxygen dynamics in real-world lakes. Experimental results show that A$^2$SL significantly improves predictive accuracy and enhances robustness in data-scarce and atypical scenarios. Although this study focuses on freshwater ecosystems, the A$^2$SL framework offers a broadly applicable solution in various scientific domains.</li>
</ul>

<h3>Title: DiffVL: Diffusion-Based Visual Localization on 2D Maps via BEV-Conditioned GPS Denoising</h3>
<ul>
<li><strong>Authors: </strong>Li Gao, Hongyang Sun, Liu Liu, Yunhao Li, Yang Cai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14565">https://arxiv.org/abs/2509.14565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14565">https://arxiv.org/pdf/2509.14565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14565]] DiffVL: Diffusion-Based Visual Localization on 2D Maps via BEV-Conditioned GPS Denoising(https://arxiv.org/abs/2509.14565)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Accurate visual localization is crucial for autonomous driving, yet existing methods face a fundamental dilemma: While high-definition (HD) maps provide high-precision localization references, their costly construction and maintenance hinder scalability, which drives research toward standard-definition (SD) maps like OpenStreetMap. Current SD-map-based approaches primarily focus on Bird's-Eye View (BEV) matching between images and maps, overlooking a ubiquitous signal-noisy GPS. Although GPS is readily available, it suffers from multipath errors in urban environments. We propose DiffVL, the first framework to reformulate visual localization as a GPS denoising task using diffusion models. Our key insight is that noisy GPS trajectory, when conditioned on visual BEV features and SD maps, implicitly encode the true pose distribution, which can be recovered through iterative diffusion refinement. DiffVL, unlike prior BEV-matching methods (e.g., OrienterNet) or transformer-based registration approaches, learns to reverse GPS noise perturbations by jointly modeling GPS, SD map, and visual signals, achieving sub-meter accuracy without relying on HD maps. Experiments on multiple datasets demonstrate that our method achieves state-of-the-art accuracy compared to BEV-matching baselines. Crucially, our work proves that diffusion models can enable scalable localization by treating noisy GPS as a generative prior-making a paradigm shift from traditional matching-based methods.</li>
</ul>

<h3>Title: DICE: Diffusion Consensus Equilibrium for Sparse-view CT Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Leon Suarez-Rodriguez, Roman Jacome, Romario Gualdron-Hurtado, Ana Mantilla-Dulcey, Henry Arguello</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14566">https://arxiv.org/abs/2509.14566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14566">https://arxiv.org/pdf/2509.14566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14566]] DICE: Diffusion Consensus Equilibrium for Sparse-view CT Reconstruction(https://arxiv.org/abs/2509.14566)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Sparse-view computed tomography (CT) reconstruction is fundamentally challenging due to undersampling, leading to an ill-posed inverse problem. Traditional iterative methods incorporate handcrafted or learned priors to regularize the solution but struggle to capture the complex structures present in medical images. In contrast, diffusion models (DMs) have recently emerged as powerful generative priors that can accurately model complex image distributions. In this work, we introduce Diffusion Consensus Equilibrium (DICE), a framework that integrates a two-agent consensus equilibrium into the sampling process of a DM. DICE alternates between: (i) a data-consistency agent, implemented through a proximal operator enforcing measurement consistency, and (ii) a prior agent, realized by a DM performing a clean image estimation at each sampling step. By balancing these two complementary agents iteratively, DICE effectively combines strong generative prior capabilities with measurement consistency. Experimental results show that DICE significantly outperforms state-of-the-art baselines in reconstructing high-quality CT images under uniform and non-uniform sparse-view settings of 15, 30, and 60 views (out of a total of 180), demonstrating both its effectiveness and robustness.</li>
</ul>

<h3>Title: DeCoP: Enhancing Self-Supervised Time Series Representation with Dependency Controlled Pre-training</h3>
<ul>
<li><strong>Authors: </strong>Yuemin Wu, Zhongze Wu, Xiu Su, Feng Yang, Hongyan Xu, Xi Lin, Wenti Huang, Shan You, Chang Xu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14642">https://arxiv.org/abs/2509.14642</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14642">https://arxiv.org/pdf/2509.14642</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14642]] DeCoP: Enhancing Self-Supervised Time Series Representation with Dependency Controlled Pre-training(https://arxiv.org/abs/2509.14642)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Modeling dynamic temporal dependencies is a critical challenge in time series pre-training, which evolve due to distribution shifts and multi-scale patterns. This temporal variability severely impairs the generalization of pre-trained models to downstream tasks. Existing frameworks fail to capture the complex interactions of short- and long-term dependencies, making them susceptible to spurious correlations that degrade generalization. To address these limitations, we propose DeCoP, a Dependency Controlled Pre-training framework that explicitly models dynamic, multi-scale dependencies by simulating evolving inter-patch dependencies. At the input level, DeCoP introduces Instance-wise Patch Normalization (IPN) to mitigate distributional shifts while preserving the unique characteristics of each patch, creating a robust foundation for representation learning. At the latent level, a hierarchical Dependency Controlled Learning (DCL) strategy explicitly models inter-patch dependencies across multiple temporal scales, with an Instance-level Contrastive Module (ICM) enhances global generalization by learning instance-discriminative representations from time-invariant positive pairs. DeCoP achieves state-of-the-art results on ten datasets with lower computing resources, improving MSE by 3% on ETTh1 over PatchTST using only 37% of the FLOPs.</li>
</ul>

<h3>Title: Attention Lattice Adapter: Visual Explanation Generation for Visual Foundation Model</h3>
<ul>
<li><strong>Authors: </strong>Shinnosuke Hirano, Yuiga Wada, Tsumugi Iida, Komei Sugiura</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14664">https://arxiv.org/abs/2509.14664</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14664">https://arxiv.org/pdf/2509.14664</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14664]] Attention Lattice Adapter: Visual Explanation Generation for Visual Foundation Model(https://arxiv.org/abs/2509.14664)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>In this study, we consider the problem of generating visual explanations in visual foundation models. Numerous methods have been proposed for this purpose; however, they often cannot be applied to complex models due to their lack of adaptability. To overcome these limitations, we propose a novel explanation generation method in visual foundation models that is aimed at both generating explanations and partially updating model parameters to enhance interpretability. Our approach introduces two novel mechanisms: Attention Lattice Adapter (ALA) and Alternating Epoch Architect (AEA). ALA mechanism simplifies the process by eliminating the need for manual layer selection, thus enhancing the model's adaptability and interpretability. Moreover, the AEA mechanism, which updates ALA's parameters every other epoch, effectively addresses the common issue of overly small attention regions. We evaluated our method on two benchmark datasets, CUB-200-2011 and ImageNet-S. Our results showed that our method outperformed the baseline methods in terms of mean intersection over union (IoU), insertion score, deletion score, and insertion-deletion score on both the CUB-200-2011 and ImageNet-S datasets. Notably, our best model achieved a 53.2-point improvement in mean IoU on the CUB-200-2011 dataset compared with the baselines.</li>
</ul>

<h3>Title: DACoN: DINO for Anime Paint Bucket Colorization with Any Number of Reference Images</h3>
<ul>
<li><strong>Authors: </strong>Kazuma Nagata, Naoshi Kaneko</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14685">https://arxiv.org/abs/2509.14685</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14685">https://arxiv.org/pdf/2509.14685</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14685]] DACoN: DINO for Anime Paint Bucket Colorization with Any Number of Reference Images(https://arxiv.org/abs/2509.14685)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Automatic colorization of line drawings has been widely studied to reduce the labor cost of hand-drawn anime production. Deep learning approaches, including image/video generation and feature-based correspondence, have improved accuracy but struggle with occlusions, pose variations, and viewpoint changes. To address these challenges, we propose DACoN, a framework that leverages foundation models to capture part-level semantics, even in line drawings. Our method fuses low-resolution semantic features from foundation models with high-resolution spatial features from CNNs for fine-grained yet robust feature extraction. In contrast to previous methods that rely on the Multiplex Transformer and support only one or two reference images, DACoN removes this constraint, allowing any number of references. Quantitative and qualitative evaluations demonstrate the benefits of using multiple reference images, achieving superior colorization performance. Our code and model are available at this https URL.</li>
</ul>

<h3>Title: HARNESS: Lightweight Distilled Arabic Speech Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Vrunda N. sukhadia, Shammur Absar Chowdhury</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14689">https://arxiv.org/abs/2509.14689</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14689">https://arxiv.org/pdf/2509.14689</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14689]] HARNESS: Lightweight Distilled Arabic Speech Foundation Models(https://arxiv.org/abs/2509.14689)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>Large pre-trained speech models excel in downstream tasks but their deployment is impractical for resource-limited environments. In this paper, we introduce HArnESS, the first Arabic-centric self-supervised speech model family, designed to capture Arabic speech nuances. Using iterative self-distillation, we train large bilingual HArnESS (HL) SSL models and then distill knowledge into compressed student models (HS, HST), preserving Arabic-specific representations. We use low-rank approximation to further compact the teacher's discrete supervision into shallow, thin models. We evaluate HArnESS on Arabic ASR, Speaker Emotion Recognition (SER), and Dialect Identification (DID), demonstrating effectiveness against HuBERT and XLS-R. With minimal fine-tuning, HArnESS achieves SOTA or comparable performance, making it a lightweight yet powerful alternative for real-world use. We release our distilled models and findings to support responsible research and deployment in low-resource settings.</li>
</ul>

<h3>Title: Towards Pre-trained Graph Condensation via Optimal Transport</h3>
<ul>
<li><strong>Authors: </strong>Yeyu Yan, Shuai Zheng, Wenjun Hui, Xiangkai Zhu, Dong Chen, Zhenfeng Zhu, Yao Zhao, Kunlun He</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14722">https://arxiv.org/abs/2509.14722</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14722">https://arxiv.org/pdf/2509.14722</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14722]] Towards Pre-trained Graph Condensation via Optimal Transport(https://arxiv.org/abs/2509.14722)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Graph condensation (GC) aims to distill the original graph into a small-scale graph, mitigating redundancy and accelerating GNN training. However, conventional GC approaches heavily rely on rigid GNNs and task-specific supervision. Such a dependency severely restricts their reusability and generalization across various tasks and architectures. In this work, we revisit the goal of ideal GC from the perspective of GNN optimization consistency, and then a generalized GC optimization objective is derived, by which those traditional GC methods can be viewed nicely as special cases of this optimization paradigm. Based on this, Pre-trained Graph Condensation (PreGC) via optimal transport is proposed to transcend the limitations of task- and architecture-dependent GC methods. Specifically, a hybrid-interval graph diffusion augmentation is presented to suppress the weak generalization ability of the condensed graph on particular architectures by enhancing the uncertainty of node states. Meanwhile, the matching between optimal graph transport plan and representation transport plan is tactfully established to maintain semantic consistencies across source graph and condensed graph spaces, thereby freeing graph condensation from task dependencies. To further facilitate the adaptation of condensed graphs to various downstream tasks, a traceable semantic harmonizer from source nodes to condensed nodes is proposed to bridge semantic associations through the optimized representation transport plan in pre-training. Extensive experiments verify the superiority and versatility of PreGC, demonstrating its task-independent nature and seamless compatibility with arbitrary GNNs.</li>
</ul>

<h3>Title: Transcoder-based Circuit Analysis for Interpretable Single-Cell Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Sosuke Hosokawa, Toshiharu Kawakami, Satoshi Kodera, Masamichi Ito, Norihiko Takeda</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14723">https://arxiv.org/abs/2509.14723</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14723">https://arxiv.org/pdf/2509.14723</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14723]] Transcoder-based Circuit Analysis for Interpretable Single-Cell Foundation Models(https://arxiv.org/abs/2509.14723)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Single-cell foundation models (scFMs) have demonstrated state-of-the-art performance on various tasks, such as cell-type annotation and perturbation response prediction, by learning gene regulatory networks from large-scale transcriptome data. However, a significant challenge remains: the decision-making processes of these models are less interpretable compared to traditional methods like differential gene expression analysis. Recently, transcoders have emerged as a promising approach for extracting interpretable decision circuits from large language models (LLMs). In this work, we train a transcoder on the cell2sentence (C2S) model, a state-of-the-art scFM. By leveraging the trained transcoder, we extract internal decision-making circuits from the C2S model. We demonstrate that the discovered circuits correspond to real-world biological mechanisms, confirming the potential of transcoders to uncover biologically plausible pathways within complex single-cell models.</li>
</ul>

<h3>Title: FMGS-Avatar: Mesh-Guided 2D Gaussian Splatting with Foundation Model Priors for 3D Monocular Avatar Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Jinlong Fan, Bingyu Hu, Xingguang Li, Yuxiang Yang, Jing Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14739">https://arxiv.org/abs/2509.14739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14739">https://arxiv.org/pdf/2509.14739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14739]] FMGS-Avatar: Mesh-Guided 2D Gaussian Splatting with Foundation Model Priors for 3D Monocular Avatar Reconstruction(https://arxiv.org/abs/2509.14739)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Reconstructing high-fidelity animatable human avatars from monocular videos remains challenging due to insufficient geometric information in single-view observations. While recent 3D Gaussian Splatting methods have shown promise, they struggle with surface detail preservation due to the free-form nature of 3D Gaussian primitives. To address both the representation limitations and information scarcity, we propose a novel method, \textbf{FMGS-Avatar}, that integrates two key innovations. First, we introduce Mesh-Guided 2D Gaussian Splatting, where 2D Gaussian primitives are attached directly to template mesh faces with constrained position, rotation, and movement, enabling superior surface alignment and geometric detail preservation. Second, we leverage foundation models trained on large-scale datasets, such as Sapiens, to complement the limited visual cues from monocular videos. However, when distilling multi-modal prior knowledge from foundation models, conflicting optimization objectives can emerge as different modalities exhibit distinct parameter sensitivities. We address this through a coordinated training strategy with selective gradient isolation, enabling each loss component to optimize its relevant parameters without interference. Through this combination of enhanced representation and coordinated information distillation, our approach significantly advances 3D monocular human avatar reconstruction. Experimental evaluation demonstrates superior reconstruction quality compared to existing methods, with notable gains in geometric accuracy and appearance fidelity while providing rich semantic information. Additionally, the distilled prior knowledge within a shared canonical space naturally enables spatially and temporally consistent rendering under novel views and poses.</li>
</ul>

<h3>Title: Data Augmentation via Latent Diffusion Models for Detecting Smell-Related Objects in Historical Artworks</h3>
<ul>
<li><strong>Authors: </strong>Ahmed Sheta, Mathias Zinnen, Aline Sindel, Andreas Maier, Vincent Christlein</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14755">https://arxiv.org/abs/2509.14755</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14755">https://arxiv.org/pdf/2509.14755</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14755]] Data Augmentation via Latent Diffusion Models for Detecting Smell-Related Objects in Historical Artworks(https://arxiv.org/abs/2509.14755)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Finding smell references in historic artworks is a challenging problem. Beyond artwork-specific challenges such as stylistic variations, their recognition demands exceptionally detailed annotation classes, resulting in annotation sparsity and extreme class imbalance. In this work, we explore the potential of synthetic data generation to alleviate these issues and enable accurate detection of smell-related objects. We evaluate several diffusion-based augmentation strategies and demonstrate that incorporating synthetic data into model training can improve detection performance. Our findings suggest that leveraging the large-scale pretraining of diffusion models offers a promising approach for improving detection accuracy, particularly in niche applications where annotations are scarce and costly to obtain. Furthermore, the proposed approach proves to be effective even with relatively small amounts of data, and scaling it up provides high potential for further enhancements.</li>
</ul>

<h3>Title: Dataset Distillation for Super-Resolution without Class Labels and Pre-trained Models</h3>
<ul>
<li><strong>Authors: </strong>Sunwoo Cho, Yejin Jung, Nam Ik Cho, Jae Woong Soh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14777">https://arxiv.org/abs/2509.14777</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14777">https://arxiv.org/pdf/2509.14777</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14777]] Dataset Distillation for Super-Resolution without Class Labels and Pre-trained Models(https://arxiv.org/abs/2509.14777)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Training deep neural networks has become increasingly demanding, requiring large datasets and significant computational resources, especially as model complexity advances. Data distillation methods, which aim to improve data efficiency, have emerged as promising solutions to this challenge. In the field of single image super-resolution (SISR), the reliance on large training datasets highlights the importance of these techniques. Recently, a generative adversarial network (GAN) inversion-based data distillation framework for SR was proposed, showing potential for better data utilization. However, the current method depends heavily on pre-trained SR networks and class-specific information, limiting its generalizability and applicability. To address these issues, we introduce a new data distillation approach for image SR that does not need class labels or pre-trained SR models. In particular, we first extract high-gradient patches and categorize images based on CLIP features, then fine-tune a diffusion model on the selected patches to learn their distribution and synthesize distilled training images. Experimental results show that our method achieves state-of-the-art performance while using significantly less training data and requiring less computational time. Specifically, when we train a baseline Transformer model for SR with only 0.68\% of the original dataset, the performance drop is just 0.3 dB. In this case, diffusion model fine-tuning takes 4 hours, and SR model training completes within 1 hour, much shorter than the 11-hour training time with the full dataset.</li>
</ul>

<h3>Title: Radiology Report Conditional 3D CT Generation with Multi Encoder Latent diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Sina Amirrajab, Zohaib Salahuddin, Sheng Kuang, Henry C. Woodruff, Philippe Lambin</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14780">https://arxiv.org/abs/2509.14780</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14780">https://arxiv.org/pdf/2509.14780</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14780]] Radiology Report Conditional 3D CT Generation with Multi Encoder Latent diffusion Model(https://arxiv.org/abs/2509.14780)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text to image latent diffusion models have recently advanced medical image synthesis, but applications to 3D CT generation remain limited. Existing approaches rely on simplified prompts, neglecting the rich semantic detail in full radiology reports, which reduces text image alignment and clinical fidelity. We propose Report2CT, a radiology report conditional latent diffusion framework for synthesizing 3D chest CT volumes directly from free text radiology reports, incorporating both findings and impression sections using multiple text encoder. Report2CT integrates three pretrained medical text encoders (BiomedVLP CXR BERT, MedEmbed, and ClinicalBERT) to capture nuanced clinical context. Radiology reports and voxel spacing information condition a 3D latent diffusion model trained on 20000 CT volumes from the CT RATE dataset. Model performance was evaluated using Frechet Inception Distance (FID) for real synthetic distributional similarity and CLIP based metrics for semantic alignment, with additional qualitative and quantitative comparisons against GenerateCT model. Report2CT generated anatomically consistent CT volumes with excellent visual quality and text image alignment. Multi encoder conditioning improved CLIP scores, indicating stronger preservation of fine grained clinical details in the free text radiology reports. Classifier free guidance further enhanced alignment with only a minor trade off in FID. We ranked first in the VLM3D Challenge at MICCAI 2025 on Text Conditional CT Generation and achieved state of the art performance across all evaluation metrics. By leveraging complete radiology reports and multi encoder text conditioning, Report2CT advances 3D CT synthesis, producing clinically faithful and high quality synthetic data.</li>
</ul>

<h3>Title: Diffusion-Based Scenario Tree Generation for Multivariate Time Series Prediction and Multistage Stochastic Optimization</h3>
<ul>
<li><strong>Authors: </strong>Stelios Zarifis, Ioannis Kordonis, Petros Maragos</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14832">https://arxiv.org/abs/2509.14832</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14832">https://arxiv.org/pdf/2509.14832</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14832]] Diffusion-Based Scenario Tree Generation for Multivariate Time Series Prediction and Multistage Stochastic Optimization(https://arxiv.org/abs/2509.14832)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Stochastic forecasting is critical for efficient decision-making in uncertain systems, such as energy markets and finance, where estimating the full distribution of future scenarios is essential. We propose Diffusion Scenario Tree (DST), a general framework for constructing scenario trees for multivariate prediction tasks using diffusion-based probabilistic forecasting models. DST recursively samples future trajectories and organizes them into a tree via clustering, ensuring non-anticipativity (decisions depending only on observed history) at each stage. We evaluate the framework on the optimization task of energy arbitrage in New York State's day-ahead electricity market. Experimental results show that our approach consistently outperforms the same optimization algorithms that use scenario trees from more conventional models and Model-Free Reinforcement Learning baselines. Furthermore, using DST for stochastic optimization yields more efficient decision policies, achieving higher performance by better handling uncertainty than deterministic and stochastic MPC variants using the same diffusion-based forecaster.</li>
</ul>

<h3>Title: [Re] Improving Interpretation Faithfulness for Vision Transformers</h3>
<ul>
<li><strong>Authors: </strong>Izabela Kurek, Wojciech Trejter, Stipe Frkovic, Andro Erdelez</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14846">https://arxiv.org/abs/2509.14846</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14846">https://arxiv.org/pdf/2509.14846</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14846]] [Re] Improving Interpretation Faithfulness for Vision Transformers(https://arxiv.org/abs/2509.14846)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This work aims to reproduce the results of Faithful Vision Transformers (FViTs) proposed by arXiv:2311.17983 alongside interpretability methods for Vision Transformers from arXiv:2012.09838 and Xu (2022) et al. We investigate claims made by arXiv:2311.17983, namely that the usage of Diffusion Denoised Smoothing (DDS) improves interpretability robustness to (1) attacks in a segmentation task and (2) perturbation and attacks in a classification task. We also extend the original study by investigating the authors' claims that adding DDS to any interpretability method can improve its robustness under attack. This is tested on baseline methods and the recently proposed Attribution Rollout method. In addition, we measure the computational costs and environmental impact of obtaining an FViT through DDS. Our results broadly agree with the original study's findings, although minor discrepancies were found and discussed.</li>
</ul>

<h3>Title: Controllable Localized Face Anonymization Via Diffusion Inpainting</h3>
<ul>
<li><strong>Authors: </strong>Ali Salar, Qing Liu, Guoying Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14866">https://arxiv.org/abs/2509.14866</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14866">https://arxiv.org/pdf/2509.14866</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14866]] Controllable Localized Face Anonymization Via Diffusion Inpainting(https://arxiv.org/abs/2509.14866)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The growing use of portrait images in computer vision highlights the need to protect personal identities. At the same time, anonymized images must remain useful for downstream computer vision tasks. In this work, we propose a unified framework that leverages the inpainting ability of latent diffusion models to generate realistic anonymized images. Unlike prior approaches, we have complete control over the anonymization process by designing an adaptive attribute-guidance module that applies gradient correction during the reverse denoising process, aligning the facial attributes of the generated image with those of the synthesized target image. Our framework also supports localized anonymization, allowing users to specify which facial regions are left unchanged. Extensive experiments conducted on the public CelebA-HQ and FFHQ datasets show that our method outperforms state-of-the-art approaches while requiring no additional model training. The source code is available on our page.</li>
</ul>

<h3>Title: Trade-offs in Cross-Domain Generalization of Foundation Model Fine-Tuned for Biometric Applications</h3>
<ul>
<li><strong>Authors: </strong>Tahar Chettaoui, Naser Damer, Fadi Boutros</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14921">https://arxiv.org/abs/2509.14921</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14921">https://arxiv.org/pdf/2509.14921</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14921]] Trade-offs in Cross-Domain Generalization of Foundation Model Fine-Tuned for Biometric Applications(https://arxiv.org/abs/2509.14921)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Foundation models such as CLIP have demonstrated exceptional zero- and few-shot transfer capabilities across diverse vision tasks. However, when fine-tuned for highly specialized biometric tasks, face recognition (FR), morphing attack detection (MAD), and presentation attack detection (PAD), these models may suffer from over-specialization. Thus, they may lose one of their foundational strengths, cross-domain generalization. In this work, we systematically quantify these trade-offs by evaluating three instances of CLIP fine-tuned for FR, MAD, and PAD. We evaluate each adapted model as well as the original CLIP baseline on 14 general vision datasets under zero-shot and linear-probe protocols, alongside common FR, MAD, and PAD benchmarks. Our results indicate that fine-tuned models suffer from over-specialization, especially when fine-tuned for complex tasks of FR. Also, our results pointed out that task complexity and classification head design, multi-class (FR) vs. binary (MAD and PAD), correlate with the degree of catastrophic forgetting. The FRoundation model with the ViT-L backbone outperforms other approaches on the large-scale FR benchmark IJB-C, achieving an improvement of up to 58.52%. However, it experiences a substantial performance drop on ImageNetV2, reaching only 51.63% compared to 69.84% achieved by the baseline CLIP model. Moreover, the larger CLIP architecture consistently preserves more of the model's original generalization ability than the smaller variant, indicating that increased model capacity may help mitigate over-specialization.</li>
</ul>

<h3>Title: GenKOL: Modular Generative AI Framework For Scalable Virtual KOL Generation</h3>
<ul>
<li><strong>Authors: </strong>Tan-Hiep To, Duy-Khang Nguyen, Tam V. Nguyen, Minh-Triet Tran, Trung-Nghia Le</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14927">https://arxiv.org/abs/2509.14927</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14927">https://arxiv.org/pdf/2509.14927</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14927]] GenKOL: Modular Generative AI Framework For Scalable Virtual KOL Generation(https://arxiv.org/abs/2509.14927)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Key Opinion Leader (KOL) play a crucial role in modern marketing by shaping consumer perceptions and enhancing brand credibility. However, collaborating with human KOLs often involves high costs and logistical challenges. To address this, we present GenKOL, an interactive system that empowers marketing professionals to efficiently generate high-quality virtual KOL images using generative AI. GenKOL enables users to dynamically compose promotional visuals through an intuitive interface that integrates multiple AI capabilities, including garment generation, makeup transfer, background synthesis, and hair editing. These capabilities are implemented as modular, interchangeable services that can be deployed flexibly on local machines or in the cloud. This modular architecture ensures adaptability across diverse use cases and computational environments. Our system can significantly streamline the production of branded content, lowering costs and accelerating marketing workflows through scalable virtual KOL creation.</li>
</ul>

<h3>Title: Seeing 3D Through 2D Lenses: 3D Few-Shot Class-Incremental Learning via Cross-Modal Geometric Rectification</h3>
<ul>
<li><strong>Authors: </strong>Xiang Tuo, Xu Xuemiao, Liu Bangzhen, Li Jinyi, Li Yong, He Shengfeng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14958">https://arxiv.org/abs/2509.14958</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14958">https://arxiv.org/pdf/2509.14958</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14958]] Seeing 3D Through 2D Lenses: 3D Few-Shot Class-Incremental Learning via Cross-Modal Geometric Rectification(https://arxiv.org/abs/2509.14958)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>The rapid growth of 3D digital content necessitates expandable recognition systems for open-world scenarios. However, existing 3D class-incremental learning methods struggle under extreme data scarcity due to geometric misalignment and texture bias. While recent approaches integrate 3D data with 2D foundation models (e.g., CLIP), they suffer from semantic blurring caused by texture-biased projections and indiscriminate fusion of geometric-textural cues, leading to unstable decision prototypes and catastrophic forgetting. To address these issues, we propose Cross-Modal Geometric Rectification (CMGR), a framework that enhances 3D geometric fidelity by leveraging CLIP's hierarchical spatial semantics. Specifically, we introduce a Structure-Aware Geometric Rectification module that hierarchically aligns 3D part structures with CLIP's intermediate spatial priors through attention-driven geometric fusion. Additionally, a Texture Amplification Module synthesizes minimal yet discriminative textures to suppress noise and reinforce cross-modal consistency. To further stabilize incremental prototypes, we employ a Base-Novel Discriminator that isolates geometric variations. Extensive experiments demonstrate that our method significantly improves 3D few-shot class-incremental learning, achieving superior geometric coherence and robustness to texture bias across cross-domain and within-domain settings.</li>
</ul>

<h3>Title: SPATIALGEN: Layout-guided 3D Indoor Scene Generation</h3>
<ul>
<li><strong>Authors: </strong>Chuan Fang, Heng Li, Yixun Liang, Jia Zheng, Yongsen Mao, Yuan Liu, Rui Tang, Zihan Zhou, Ping Tan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.14981">https://arxiv.org/abs/2509.14981</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.14981">https://arxiv.org/pdf/2509.14981</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.14981]] SPATIALGEN: Layout-guided 3D Indoor Scene Generation(https://arxiv.org/abs/2509.14981)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Creating high-fidelity 3D models of indoor environments is essential for applications in design, virtual reality, and robotics. However, manual 3D modeling remains time-consuming and labor-intensive. While recent advances in generative AI have enabled automated scene synthesis, existing methods often face challenges in balancing visual quality, diversity, semantic consistency, and user control. A major bottleneck is the lack of a large-scale, high-quality dataset tailored to this task. To address this gap, we introduce a comprehensive synthetic dataset, featuring 12,328 structured annotated scenes with 57,440 rooms, and 4.7M photorealistic 2D renderings. Leveraging this dataset, we present SpatialGen, a novel multi-view multi-modal diffusion model that generates realistic and semantically consistent 3D indoor scenes. Given a 3D layout and a reference image (derived from a text prompt), our model synthesizes appearance (color image), geometry (scene coordinate map), and semantic (semantic segmentation map) from arbitrary viewpoints, while preserving spatial consistency across modalities. SpatialGen consistently generates superior results to previous methods in our experiments. We are open-sourcing our data and models to empower the community and advance the field of indoor scene understanding and generation.</li>
</ul>

<h3>Title: AutoEdit: Automatic Hyperparameter Tuning for Image Editing</h3>
<ul>
<li><strong>Authors: </strong>Chau Pham, Quan Dao, Mahesh Bhosale, Yunjie Tian, Dimitris Metaxas, David Doermann</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15031">https://arxiv.org/abs/2509.15031</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15031">https://arxiv.org/pdf/2509.15031</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15031]] AutoEdit: Automatic Hyperparameter Tuning for Image Editing(https://arxiv.org/abs/2509.15031)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in diffusion models have revolutionized text-guided image editing, yet existing editing methods face critical challenges in hyperparameter identification. To get the reasonable editing performance, these methods often require the user to brute-force tune multiple interdependent hyperparameters, such as inversion timesteps and attention modification, \textit{etc.} This process incurs high computational costs due to the huge hyperparameter search space. We consider searching optimal editing's hyperparameters as a sequential decision-making task within the diffusion denoising process. Specifically, we propose a reinforcement learning framework, which establishes a Markov Decision Process that dynamically adjusts hyperparameters across denoising steps, integrating editing objectives into a reward function. The method achieves time efficiency through proximal policy optimization while maintaining optimal hyperparameter configurations. Experiments demonstrate significant reduction in search time and computational overhead compared to existing brute-force approaches, advancing the practical deployment of a diffusion-based image editing framework in the real world.</li>
</ul>

<h3>Title: Beyond Marginals: Learning Joint Spatio-Temporal Patterns for Multivariate Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Padmaksha Roy, Almuatazbellah Boker, Lamine Mili</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15033">https://arxiv.org/abs/2509.15033</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15033">https://arxiv.org/pdf/2509.15033</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15033]] Beyond Marginals: Learning Joint Spatio-Temporal Patterns for Multivariate Anomaly Detection(https://arxiv.org/abs/2509.15033)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, anomaly</a></li>
<li><strong>Abstract: </strong>In this paper, we aim to improve multivariate anomaly detection (AD) by modeling the \textit{time-varying non-linear spatio-temporal correlations} found in multivariate time series data . In multivariate time series data, an anomaly may be indicated by the simultaneous deviation of interrelated time series from their expected collective behavior, even when no individual time series exhibits a clearly abnormal pattern on its own. In many existing approaches, time series variables are assumed to be (conditionally) independent, which oversimplifies real-world interactions. Our approach addresses this by modeling joint dependencies in the latent space and decoupling the modeling of \textit{marginal distributions, temporal dynamics, and inter-variable dependencies}. We use a transformer encoder to capture temporal patterns, and to model spatial (inter-variable) dependencies, we fit a multi-variate likelihood and a copula. The temporal and the spatial components are trained jointly in a latent space using a self-supervised contrastive learning objective to learn meaningful feature representations to separate normal and anomaly samples.</li>
</ul>

<h3>Title: Forecasting and Visualizing Air Quality from Sky Images with Vision-Language Models</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Saleh Vahdatpour, Maryam Eyvazi, Yanqing Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15076">https://arxiv.org/abs/2509.15076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15076">https://arxiv.org/pdf/2509.15076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15076]] Forecasting and Visualizing Air Quality from Sky Images with Vision-Language Models(https://arxiv.org/abs/2509.15076)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Air pollution remains a critical threat to public health and environmental sustainability, yet conventional monitoring systems are often constrained by limited spatial coverage and accessibility. This paper proposes an AI-driven agent that predicts ambient air pollution levels from sky images and synthesizes realistic visualizations of pollution scenarios using generative modeling. Our approach combines statistical texture analysis with supervised learning for pollution classification, and leverages vision-language model (VLM)-guided image generation to produce interpretable representations of air quality conditions. The generated visuals simulate varying degrees of pollution, offering a foundation for user-facing interfaces that improve transparency and support informed environmental decision-making. These outputs can be seamlessly integrated into intelligent applications aimed at enhancing situational awareness and encouraging behavioral responses based on real-time forecasts. We validate our method using a dataset of urban sky images and demonstrate its effectiveness in both pollution level estimation and semantically consistent visual synthesis. The system design further incorporates human-centered user experience principles to ensure accessibility, clarity, and public engagement in air quality forecasting. To support scalable and energy-efficient deployment, future iterations will incorporate a green CNN architecture enhanced with FPGA-based incremental learning, enabling real-time inference on edge platforms.</li>
</ul>

<h3>Title: The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Saleh Vahdatpour, Huaiyuan Chu, Yanqing Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15097">https://arxiv.org/abs/2509.15097</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15097">https://arxiv.org/pdf/2509.15097</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15097]] The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning(https://arxiv.org/abs/2509.15097)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>The rising computational and energy demands of deep learning, particularly in large-scale architectures such as foundation models and large language models (LLMs), pose significant challenges to sustainability. Traditional gradient-based training methods are inefficient, requiring numerous iterative updates and high power consumption. To address these limitations, we propose a hybrid framework that combines hierarchical decomposition with FPGA-based direct equation solving and incremental learning. Our method divides the neural network into two functional tiers: lower layers are optimized via single-step equation solving on FPGAs for efficient and parallelizable feature extraction, while higher layers employ adaptive incremental learning to support continual updates without full retraining. Building upon this foundation, we introduce the Compound LLM framework, which explicitly deploys LLM modules across both hierarchy levels. The lower-level LLM handles reusable representation learning with minimal energy overhead, while the upper-level LLM performs adaptive decision-making through energy-aware updates. This integrated design enhances scalability, reduces redundant computation, and aligns with the principles of sustainable AI. Theoretical analysis and architectural insights demonstrate that our method reduces computational costs significantly while preserving high model performance, making it well-suited for edge deployment and real-time adaptation in energy-constrained environments.</li>
</ul>

<h3>Title: Self-Improving Embodied Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Seyed Kamyar Seyed Ghasemipour, Ayzaan Wahid, Jonathan Tompson, Pannag Sanketi, Igor Mordatch</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15155">https://arxiv.org/abs/2509.15155</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15155">https://arxiv.org/pdf/2509.15155</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15155]] Self-Improving Embodied Foundation Models(https://arxiv.org/abs/2509.15155)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Foundation models trained on web-scale data have revolutionized robotics, but their application to low-level control remains largely limited to behavioral cloning. Drawing inspiration from the success of the reinforcement learning stage in fine-tuning large language models, we propose a two-stage post-training approach for robotics. The first stage, Supervised Fine-Tuning (SFT), fine-tunes pretrained foundation models using both: a) behavioral cloning, and b) steps-to-go prediction objectives. In the second stage, Self-Improvement, steps-to-go prediction enables the extraction of a well-shaped reward function and a robust success detector, enabling a fleet of robots to autonomously practice downstream tasks with minimal human supervision. Through extensive experiments on real-world and simulated robot embodiments, our novel post-training recipe unveils significant results on Embodied Foundation Models. First, we demonstrate that the combination of SFT and Self-Improvement is significantly more sample-efficient than scaling imitation data collection for supervised learning, and that it leads to policies with significantly higher success rates. Further ablations highlight that the combination of web-scale pretraining and Self-Improvement is the key to this sample-efficiency. Next, we demonstrate that our proposed combination uniquely unlocks a capability that current methods cannot achieve: autonomously practicing and acquiring novel skills that generalize far beyond the behaviors observed in the imitation learning datasets used during training. These findings highlight the transformative potential of combining pretrained foundation models with online Self-Improvement to enable autonomous skill acquisition in robotics. Our project website can be found at this https URL .</li>
</ul>

<h3>Title: Watermarking and Anomaly Detection in Machine Learning Models for LORA RF Fingerprinting</h3>
<ul>
<li><strong>Authors: </strong>Aarushi Mahajan, Wayne Burleson</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15170">https://arxiv.org/abs/2509.15170</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15170">https://arxiv.org/pdf/2509.15170</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15170]] Watermarking and Anomaly Detection in Machine Learning Models for LORA RF Fingerprinting(https://arxiv.org/abs/2509.15170)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Radio frequency fingerprint identification (RFFI) distinguishes wireless devices by the small variations in their analog circuits, avoiding heavy cryptographic authentication. While deep learning on spectrograms improves accuracy, models remain vulnerable to copying, tampering, and evasion. We present a stronger RFFI system combining watermarking for ownership proof and anomaly detection for spotting suspicious inputs. Using a ResNet-34 on log-Mel spectrograms, we embed three watermarks: a simple trigger, an adversarially trained trigger robust to noise and filtering, and a hidden gradient/weight signature. A convolutional Variational Autoencoders (VAE) with Kullback-Leibler (KL) warm-up and free-bits flags off-distribution queries. On the LoRa dataset, our system achieves 94.6% accuracy, 98% watermark success, and 0.94 AUROC, offering verifiable, tamper-resistant authentication.</li>
</ul>

<h3>Title: Understand Before You Generate: Self-Guided Training for Autoregressive Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Xiaoyu Yue, Zidong Wang, Yuqing Wang, Wenlong Zhang, Xihui Liu, Wanli Ouyang, Lei Bai, Luping Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15185">https://arxiv.org/abs/2509.15185</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15185">https://arxiv.org/pdf/2509.15185</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15185]] Understand Before You Generate: Self-Guided Training for Autoregressive Image Generation(https://arxiv.org/abs/2509.15185)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, generative</a></li>
<li><strong>Abstract: </strong>Recent studies have demonstrated the importance of high-quality visual representations in image generation and have highlighted the limitations of generative models in image understanding. As a generative paradigm originally designed for natural language, autoregressive models face similar challenges. In this work, we present the first systematic investigation into the mechanisms of applying the next-token prediction paradigm to the visual domain. We identify three key properties that hinder the learning of high-level visual semantics: local and conditional dependence, inter-step semantic inconsistency, and spatial invariance deficiency. We show that these issues can be effectively addressed by introducing self-supervised objectives during training, leading to a novel training framework, Self-guided Training for AutoRegressive models (ST-AR). Without relying on pre-trained representation models, ST-AR significantly enhances the image understanding ability of autoregressive models and leads to improved generation quality. Specifically, ST-AR brings approximately 42% FID improvement for LlamaGen-L and 49% FID improvement for LlamaGen-XL, while maintaining the same sampling strategy.</li>
</ul>

<h3>Title: Fast and Fluent Diffusion Language Models via Convolutional Decoding and Rejective Fine-tuning</h3>
<ul>
<li><strong>Authors: </strong>Yeongbin Seo, Dongha Lee, Jaehyung Kim, Jinyoung Yeo</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15188">https://arxiv.org/abs/2509.15188</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15188">https://arxiv.org/pdf/2509.15188</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15188]] Fast and Fluent Diffusion Language Models via Convolutional Decoding and Rejective Fine-tuning(https://arxiv.org/abs/2509.15188)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Autoregressive (AR) language models generate text one token at a time, which limits their inference speed. Diffusion-based language models offer a promising alternative, as they can decode multiple tokens in parallel. However, we identify a key bottleneck in current diffusion LMs: the long decoding-window problem, where tokens generated far from the input context often become irrelevant or repetitive. Previous solutions like semi-autoregressive address this issue by splitting windows into blocks, but this sacrifices speed and bidirectionality, eliminating the main advantage of diffusion models. To overcome this, we propose Convolutional decoding (Conv), a normalization-based method that narrows the decoding window without hard segmentation, leading to better fluency and flexibility. Additionally, we introduce Rejecting Rule-based Fine-Tuning (R2FT), a post-hoc training scheme that better aligns tokens at positions far from context. Our methods achieve state-of-the-art results on open-ended generation benchmarks (e.g., AlpacaEval) among diffusion LM baselines, with significantly lower step size than previous works, demonstrating both speed and quality improvements.</li>
</ul>

<h3>Title: Fair-GPTQ: Bias-Aware Quantization for Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Irina Proskurina, Guillaume Metzler, Julien Velcin</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15206">https://arxiv.org/abs/2509.15206</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15206">https://arxiv.org/pdf/2509.15206</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15206]] Fair-GPTQ: Bias-Aware Quantization for Large Language Models(https://arxiv.org/abs/2509.15206)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>High memory demands of generative language models have drawn attention to quantization, which reduces computational cost, memory usage, and latency by mapping model weights to lower-precision integers. Approaches such as GPTQ effectively minimize input-weight product errors during quantization; however, recent empirical studies show that they can increase biased outputs and degrade performance on fairness benchmarks, and it remains unclear which specific weights cause this issue. In this work, we draw new links between quantization and model fairness by adding explicit group-fairness constraints to the quantization objective and introduce Fair-GPTQ, the first quantization method explicitly designed to reduce unfairness in large language models. The added constraints guide the learning of the rounding operation toward less-biased text generation for protected groups. Specifically, we focus on stereotype generation involving occupational bias and discriminatory language spanning gender, race, and religion. Fair-GPTQ has minimal impact on performance, preserving at least 90% of baseline accuracy on zero-shot benchmarks, reduces unfairness relative to a half-precision model, and retains the memory and speed benefits of 4-bit quantization. We also compare the performance of Fair-GPTQ with existing debiasing methods and find that it achieves performance on par with the iterative null-space projection debiasing approach on racial-stereotype benchmarks. Overall, the results validate our theoretical solution to the quantization problem with a group-bias term, highlight its applicability for reducing group bias at quantization time in generative models, and demonstrate that our approach can further be used to analyze channel- and weight-level contributions to fairness during quantization.</li>
</ul>

<h3>Title: RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation</h3>
<ul>
<li><strong>Authors: </strong>Yuming Jiang, Siteng Huang, Shengke Xue, Yaxi Zhao, Jun Cen, Sicong Leng, Kehan Li, Jiayan Guo, Kexiang Wang, Mingxiu Chen, Fan Wang, Deli Zhao, Xin Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15212">https://arxiv.org/abs/2509.15212</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15212">https://arxiv.org/pdf/2509.15212</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15212]] RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation(https://arxiv.org/abs/2509.15212)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper presents RynnVLA-001, a vision-language-action(VLA) model built upon large-scale video generative pretraining from human demonstrations. We propose a novel two-stage pretraining methodology. The first stage, Ego-Centric Video Generative Pretraining, trains an Image-to-Video model on 12M ego-centric manipulation videos to predict future frames conditioned on an initial frame and a language instruction. The second stage, Human-Centric Trajectory-Aware Modeling, extends this by jointly predicting future keypoint trajectories, thereby effectively bridging visual frame prediction with action prediction. Furthermore, to enhance action representation, we propose ActionVAE, a variational autoencoder that compresses sequences of actions into compact latent embeddings, reducing the complexity of the VLA output space. When finetuned on the same downstream robotics datasets, RynnVLA-001 achieves superior performance over state-of-the-art baselines, demonstrating that the proposed pretraining strategy provides a more effective initialization for VLA models.</li>
</ul>

<h3>Title: Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Fangjinhua Wang, Qingshan Xu, Yew-Soon Ong, Marc Pollefeys</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15220">https://arxiv.org/abs/2509.15220</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15220">https://arxiv.org/pdf/2509.15220</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15220]] Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model(https://arxiv.org/abs/2509.15220)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>To reconstruct the 3D geometry from calibrated images, learning-based multi-view stereo (MVS) methods typically perform multi-view depth estimation and then fuse depth maps into a mesh or point cloud. To improve the computational efficiency, many methods initialize a coarse depth map and then gradually refine it in higher resolutions. Recently, diffusion models achieve great success in generation tasks. Starting from a random noise, diffusion models gradually recover the sample with an iterative denoising process. In this paper, we propose a novel MVS framework, which introduces diffusion models in MVS. Specifically, we formulate depth refinement as a conditional diffusion process. Considering the discriminative characteristic of depth estimation, we design a condition encoder to guide the diffusion process. To improve efficiency, we propose a novel diffusion network combining lightweight 2D U-Net and convolutional GRU. Moreover, we propose a novel confidence-based sampling strategy to adaptively sample depth hypotheses based on the confidence estimated by diffusion model. Based on our novel MVS framework, we propose two novel MVS methods, DiffMVS and CasDiffMVS. DiffMVS achieves competitive performance with state-of-the-art efficiency in run-time and GPU memory. CasDiffMVS achieves state-of-the-art performance on DTU, Tanks & Temples and ETH3D. Code is available at: this https URL.</li>
</ul>

<h3>Title: ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data</h3>
<ul>
<li><strong>Authors: </strong>Zhaoyang Liu, JingJing Xie, Zichen Ding, Zehao Li, Bowen Yang, Zhenyu Wu, Xuehui Wang, Qiushi Sun, Shi Liu, Weiyun Wang, Shenglong Ye, Qingyun Li, Zeyue Tian, Gen Luo, Xiangyu Yue, Biqing Qi, Kai Chen, Bowen Zhou, Yu Qiao, Qifeng Chen, Wenhai Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15221">https://arxiv.org/abs/2509.15221</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15221">https://arxiv.org/pdf/2509.15221</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15221]] ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data(https://arxiv.org/abs/2509.15221)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Vision-Language Models (VLMs) have enabled computer use agents (CUAs) that operate GUIs autonomously, showing great potential, yet progress is limited by the lack of large-scale, open-source computer use data and foundation models. In this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It offers a large-scale dataset spanning 6 operating systems and 3 task domains, built via a closed-loop pipeline uniting automated agents with human experts. Trained on this scaled-up data, ScaleCUA can operate seamlessly across platforms. Specifically, it delivers strong gains over baselines (+26.6 on WebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art results (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on WebArena-Lite-v2). These findings underscore the power of data-driven scaling for general-purpose computer use agents. We will release data, models, and code to advance future research: this https URL.</li>
</ul>

<h3>Title: Depth AnyEvent: A Cross-Modal Distillation Paradigm for Event-Based Monocular Depth Estimation</h3>
<ul>
<li><strong>Authors: </strong>Luca Bartolomei, Enrico Mannocci, Fabio Tosi, Matteo Poggi, Stefano Mattoccia</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2509.15224">https://arxiv.org/abs/2509.15224</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2509.15224">https://arxiv.org/pdf/2509.15224</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2509.15224]] Depth AnyEvent: A Cross-Modal Distillation Paradigm for Event-Based Monocular Depth Estimation(https://arxiv.org/abs/2509.15224)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Event cameras capture sparse, high-temporal-resolution visual information, making them particularly suitable for challenging environments with high-speed motion and strongly varying lighting conditions. However, the lack of large datasets with dense ground-truth depth annotations hinders learning-based monocular depth estimation from event data. To address this limitation, we propose a cross-modal distillation paradigm to generate dense proxy labels leveraging a Vision Foundation Model (VFM). Our strategy requires an event stream spatially aligned with RGB frames, a simple setup even available off-the-shelf, and exploits the robustness of large-scale VFMs. Additionally, we propose to adapt VFMs, either a vanilla one like Depth Anything v2 (DAv2), or deriving from it a novel recurrent architecture to infer depth from monocular event cameras. We evaluate our approach with synthetic and real-world datasets, demonstrating that i) our cross-modal paradigm achieves competitive performance compared to fully supervised methods without requiring expensive depth annotations, and ii) our VFM-based models achieve state-of-the-art performance.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
