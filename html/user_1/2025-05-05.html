<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2025-05-05</h1>
<h3>Title: Responsive DNN Adaptation for Video Analytics against Environment Shift via Hierarchical Mobile-Cloud Collaborations</h3>
<ul>
<li><strong>Authors: </strong>Maozhe Zhao, Shengzhong Liu, Fan Wu, Guihai Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00745">https://arxiv.org/abs/2505.00745</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00745">https://arxiv.org/pdf/2505.00745</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00745]] Responsive DNN Adaptation for Video Analytics against Environment Shift via Hierarchical Mobile-Cloud Collaborations(https://arxiv.org/abs/2505.00745)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Mobile video analysis systems often encounter various deploying environments, where environment shifts present greater demands for responsiveness in adaptations of deployed "expert DNN models". Existing model adaptation frameworks primarily operate in a cloud-centric way, exhibiting degraded performance during adaptation and delayed reactions to environment shifts. Instead, this paper proposes MOCHA, a novel framework optimizing the responsiveness of continuous model adaptation through hierarchical collaborations between mobile and cloud resources. Specifically, MOCHA (1) reduces adaptation response delays by performing on-device model reuse and fast fine-tuning before requesting cloud model retrieval and end-to-end retraining; (2) accelerates history expert model retrieval by organizing them into a structured taxonomy utilizing domain semantics analyzed by a cloud foundation model as indices; (3) enables efficient local model reuse by maintaining onboard expert model caches for frequent scenes, which proactively prefetch model weights from the cloud model database. Extensive evaluations with real-world videos on three DNN tasks show MOCHA improves the model accuracy during adaptation by up to 6.8% while saving the response delay and retraining time by up to 35.5x and 3.0x respectively.</li>
</ul>

<h3>Title: InstructAttribute: Fine-grained Object Attributes editing with Instruction</h3>
<ul>
<li><strong>Authors: </strong>Xingxi Yin, Jingfeng Zhang, Zhi Li, Yicheng Li, Yin Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00751">https://arxiv.org/abs/2505.00751</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00751">https://arxiv.org/pdf/2505.00751</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00751]] InstructAttribute: Fine-grained Object Attributes editing with Instruction(https://arxiv.org/abs/2505.00751)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Text-to-image (T2I) diffusion models, renowned for their advanced generative abilities, are extensively utilized in image editing applications, demonstrating remarkable effectiveness. However, achieving precise control over fine-grained attributes still presents considerable challenges. Existing image editing techniques either fail to modify the attributes of an object or struggle to preserve its structure and maintain consistency in other areas of the image. To address these challenges, we propose the Structure-Preserving and Attribute Amplification (SPAA), a training-free method which enables precise control over the color and material transformations of objects by editing the self-attention maps and cross-attention values. Furthermore, we constructed the Attribute Dataset, which encompasses nearly all colors and materials associated with various objects, by integrating multimodal large language models (MLLM) to develop an automated pipeline for data filtering and instruction labeling. Training on this dataset, we present our InstructAttribute, an instruction-based model designed to facilitate fine-grained editing of color and material attributes. Extensive experiments demonstrate that our method achieves superior performance in object-level color and material editing, outperforming existing instruction-based image editing approaches.</li>
</ul>

<h3>Title: Multi-Modal Language Models as Text-to-Image Model Evaluators</h3>
<ul>
<li><strong>Authors: </strong>Jiahui Chen, Candace Ross, Reyhane Askari-Hemmat, Koustuv Sinha, Melissa Hall, Michal Drozdzal, Adriana Romero-Soriano</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00759">https://arxiv.org/abs/2505.00759</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00759">https://arxiv.org/pdf/2505.00759</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00759]] Multi-Modal Language Models as Text-to-Image Model Evaluators(https://arxiv.org/abs/2505.00759)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The steady improvements of text-to-image (T2I) generative models lead to slow deprecation of automatic evaluation benchmarks that rely on static datasets, motivating researchers to seek alternative ways to evaluate the T2I progress. In this paper, we explore the potential of multi-modal large language models (MLLMs) as evaluator agents that interact with a T2I model, with the objective of assessing prompt-generation consistency and image aesthetics. We present Multimodal Text-to-Image Eval (MT2IE), an evaluation framework that iteratively generates prompts for evaluation, scores generated images and matches T2I evaluation of existing benchmarks with a fraction of the prompts used in existing static benchmarks. Moreover, we show that MT2IE's prompt-generation consistency scores have higher correlation with human judgment than scores previously introduced in the literature. MT2IE generates prompts that are efficient at probing T2I model performance, producing the same relative T2I model rankings as existing benchmarks while using only 1/80th the number of prompts for evaluation.</li>
</ul>

<h3>Title: Data-Driven Optical To Thermal Inference in Pool Boiling Using Generative Adversarial Networks</h3>
<ul>
<li><strong>Authors: </strong>Qianxi Fu, Youngjoon Suh, Xiaojing Zhang, Yoonjin Won</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.app-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00823">https://arxiv.org/abs/2505.00823</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00823">https://arxiv.org/pdf/2505.00823</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00823]] Data-Driven Optical To Thermal Inference in Pool Boiling Using Generative Adversarial Networks(https://arxiv.org/abs/2505.00823)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Phase change plays a critical role in thermal management systems, yet quantitative characterization of multiphase heat transfer remains limited by the challenges of measuring temperature fields in chaotic, rapidly evolving flow regimes. While computational methods offer spatiotemporal resolution in idealized cases, replicating complex experimental conditions remains prohibitively difficult. Here, we present a data-driven framework that leverages a conditional generative adversarial network (CGAN) to infer temperature fields from geometric phase contours in a canonical pool boiling configuration where advanced data collection techniques are restricted. Using high-speed imaging data and simulation-informed training, our model demonstrates the ability to reconstruct temperature fields with errors below 6%. We further show that standard data augmentation strategies are effective in enhancing both accuracy and physical plausibility of the predicted maps across both simulation and experimental datasets when precise physical constraints are not applicable. Our results highlight the potential of deep generative models to bridge the gap between observable multiphase phenomena and underlying thermal transport, offering a powerful approach to augment and interpret experimental measurements in complex two-phase systems.</li>
</ul>

<h3>Title: From Texts to Shields: Convergence of Large Language Models and Cybersecurity</h3>
<ul>
<li><strong>Authors: </strong>Tao Li, Ya-Ting Yang, Yunian Pan, Quanyan Zhu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00841">https://arxiv.org/abs/2505.00841</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00841">https://arxiv.org/pdf/2505.00841</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00841]] From Texts to Shields: Convergence of Large Language Models and Cybersecurity(https://arxiv.org/abs/2505.00841)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This report explores the convergence of large language models (LLMs) and cybersecurity, synthesizing interdisciplinary insights from network security, artificial intelligence, formal methods, and human-centered design. It examines emerging applications of LLMs in software and network security, 5G vulnerability analysis, and generative security engineering. The report highlights the role of agentic LLMs in automating complex tasks, improving operational efficiency, and enabling reasoning-driven security analytics. Socio-technical challenges associated with the deployment of LLMs -- including trust, transparency, and ethical considerations -- can be addressed through strategies such as human-in-the-loop systems, role-specific training, and proactive robustness testing. The report further outlines critical research challenges in ensuring interpretability, safety, and fairness in LLM-based systems, particularly in high-stakes domains. By integrating technical advances with organizational and societal considerations, this report presents a forward-looking research agenda for the secure and effective adoption of LLMs in cybersecurity.</li>
</ul>

<h3>Title: Robust Root Cause Diagnosis using In-Distribution Interventions</h3>
<ul>
<li><strong>Authors: </strong>Lokesh Nagalapatti, Ashutosh Srivastava, Sunita Sarawagi, Amit Sharma</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00930">https://arxiv.org/abs/2505.00930</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00930">https://arxiv.org/pdf/2505.00930</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00930]] Robust Root Cause Diagnosis using In-Distribution Interventions(https://arxiv.org/abs/2505.00930)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Diagnosing the root cause of an anomaly in a complex interconnected system is a pressing problem in today's cloud services and industrial operations. We propose In-Distribution Interventions (IDI), a novel algorithm that predicts root cause as nodes that meet two criteria: 1) **Anomaly:** root cause nodes should take on anomalous values; 2) **Fix:** had the root cause nodes assumed usual values, the target node would not have been anomalous. Prior methods of assessing the fix condition rely on counterfactuals inferred from a Structural Causal Model (SCM) trained on historical data. But since anomalies are rare and fall outside the training distribution, the fitted SCMs yield unreliable counterfactual estimates. IDI overcomes this by relying on interventional estimates obtained by solely probing the fitted SCM at in-distribution inputs. We present a theoretical analysis comparing and bounding the errors in assessing the fix condition using interventional and counterfactual estimates. We then conduct experiments by systematically varying the SCM's complexity to demonstrate the cases where IDI's interventional approach outperforms the counterfactual approach and vice versa. Experiments on both synthetic and PetShop RCD benchmark datasets demonstrate that \our\ consistently identifies true root causes more accurately and robustly than nine existing state-of-the-art RCD baselines. Code is released at this https URL.</li>
</ul>

<h3>Title: A Self-Supervised Transformer for Unusable Shared Bike Detection</h3>
<ul>
<li><strong>Authors: </strong>Yin Huang, Yongqi Dong, Youhua Tang, Alvaro García Hernandez</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00932">https://arxiv.org/abs/2505.00932</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00932">https://arxiv.org/pdf/2505.00932</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00932]] A Self-Supervised Transformer for Unusable Shared Bike Detection(https://arxiv.org/abs/2505.00932)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>The rapid expansion of bike-sharing systems (BSS) has greatly improved urban "last-mile" connectivity, yet large-scale deployments face escalating operational challenges, particularly in detecting faulty bikes. Existing detection approaches either rely on static model-based thresholds that overlook dynamic spatiotemporal (ST) usage patterns or employ supervised learning methods that struggle with label scarcity and class imbalance. To address these limitations, this paper proposes a novel Self-Supervised Transformer (SSTransformer) framework for automatically detecting unusable shared bikes, leveraging ST features extracted from GPS trajectories and trip records. The model incorporates a self-supervised pre-training strategy to enhance its feature extraction capabilities, followed by fine-tuning for efficient status recognition. In the pre-training phase, the Transformer encoder learns generalized representations of bike movement via a self-supervised objective; in the fine-tuning phase, the encoder is adapted to a downstream binary classification task. Comprehensive experiments on a real-world dataset of 10,730 bikes (1,870 unusable, 8,860 normal) from Chengdu, China, demonstrate that SSTransformer significantly outperforms traditional machine learning, ensemble learning, and deep learning baselines, achieving the best accuracy (97.81%), precision (0.8889), and F1-score (0.9358). This work highlights the effectiveness of self-supervised Transformer on ST data for capturing complex anomalies in BSS, paving the way toward more reliable and scalable maintenance solutions for shared mobility.</li>
</ul>

<h3>Title: FreCT: Frequency-augmented Convolutional Transformer for Robust Time Series Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Wenxin Zhang, Ding Xu, Guangzhen Yao, Xiaojian Lin, Renxiang Guan, Chengze Du, Renda Han, Xi Xuan, Cuicui Luo</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00941">https://arxiv.org/abs/2505.00941</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00941">https://arxiv.org/pdf/2505.00941</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00941]] FreCT: Frequency-augmented Convolutional Transformer for Robust Time Series Anomaly Detection(https://arxiv.org/abs/2505.00941)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Time series anomaly detection is critical for system monitoring and risk identification, across various domains, such as finance and healthcare. However, for most reconstruction-based approaches, detecting anomalies remains a challenge due to the complexity of sequential patterns in time series data. On the one hand, reconstruction-based techniques are susceptible to computational deviation stemming from anomalies, which can lead to impure representations of normal sequence patterns. On the other hand, they often focus on the time-domain dependencies of time series, while ignoring the alignment of frequency information beyond the time domain. To address these challenges, we propose a novel Frequency-augmented Convolutional Transformer (FreCT). FreCT utilizes patch operations to generate contrastive views and employs an improved Transformer architecture integrated with a convolution module to capture long-term dependencies while preserving local topology information. The introduced frequency analysis based on Fourier transformation could enhance the model's ability to capture crucial characteristics beyond the time domain. To protect the training quality from anomalies and improve the robustness, FreCT deploys stop-gradient Kullback-Leibler (KL) divergence and absolute error to optimize consistency information in both time and frequency domains. Extensive experiments on four public datasets demonstrate that FreCT outperforms existing methods in identifying anomalies.</li>
</ul>

<h3>Title: Tree-Sliced Wasserstein Distance with Nonlinear Projection</h3>
<ul>
<li><strong>Authors: </strong>Thanh Tran, Viet-Hoang Tran, Thanh Chu, Trang Pham, Laurent El Ghaoui, Tam Le, Tan M. Nguyen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00968">https://arxiv.org/abs/2505.00968</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00968">https://arxiv.org/pdf/2505.00968</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00968]] Tree-Sliced Wasserstein Distance with Nonlinear Projection(https://arxiv.org/abs/2505.00968)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, generative</a></li>
<li><strong>Abstract: </strong>Tree-Sliced methods have recently emerged as an alternative to the traditional Sliced Wasserstein (SW) distance, replacing one-dimensional lines with tree-based metric spaces and incorporating a splitting mechanism for projecting measures. This approach enhances the ability to capture the topological structures of integration domains in Sliced Optimal Transport while maintaining low computational costs. Building on this foundation, we propose a novel nonlinear projectional framework for the Tree-Sliced Wasserstein (TSW) distance, substituting the linear projections in earlier versions with general projections, while ensuring the injectivity of the associated Radon Transform and preserving the well-definedness of the resulting metric. By designing appropriate projections, we construct efficient metrics for measures on both Euclidean spaces and spheres. Finally, we validate our proposed metric through extensive numerical experiments for Euclidean and spherical datasets. Applications include gradient flows, self-supervised learning, and generative models, where our methods demonstrate significant improvements over recent SW and TSW variants.</li>
</ul>

<h3>Title: A Character-based Diffusion Embedding Algorithm for Enhancing the Generation Quality of Generative Linguistic Steganographic Texts</h3>
<ul>
<li><strong>Authors: </strong>Yingquan Chen, Qianmu Li, Xiaocong Wu, Huifeng Li, Qing Chang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00977">https://arxiv.org/abs/2505.00977</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00977">https://arxiv.org/pdf/2505.00977</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00977]] A Character-based Diffusion Embedding Algorithm for Enhancing the Generation Quality of Generative Linguistic Steganographic Texts(https://arxiv.org/abs/2505.00977)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generating high-quality steganographic text is a fundamental challenge in the field of generative linguistic steganography. This challenge arises primarily from two aspects: firstly, the capabilities of existing models in text generation are limited; secondly, embedding algorithms fail to effectively mitigate the negative impacts of sensitive information's properties, such as semantic content or randomness. Specifically, to ensure that the recipient can accurately extract hidden information, embedding algorithms often have to consider selecting candidate words with relatively low probabilities. This phenomenon leads to a decrease in the number of high-probability candidate words and an increase in low-probability candidate words, thereby compromising the semantic coherence and logical fluency of the steganographic text and diminishing the overall quality of the generated steganographic material. To address this issue, this paper proposes a novel embedding algorithm, character-based diffusion embedding algorithm (CDEA). Unlike existing embedding algorithms that strive to eliminate the impact of sensitive information's properties on the generation process, CDEA leverages sensitive information's properties. It enhances the selection frequency of high-probability candidate words in the candidate pool based on general statistical properties at the character level and grouping methods based on power-law distributions, while reducing the selection frequency of low-probability candidate words in the candidate pool. Furthermore, to ensure the effective transformation of sensitive information in long sequences, we also introduce the XLNet model. Experimental results demonstrate that the combination of CDEA and XLNet significantly improves the quality of generated steganographic text, particularly in terms of perceptual-imperceptibility.</li>
</ul>

<h3>Title: Deterministic-to-Stochastic Diverse Latent Feature Mapping for Human Motion Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Yu Hua, Weiming Liu, Gui Xu, Yaqing Hou, Yew-Soon Ong, Qiang Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.00998">https://arxiv.org/abs/2505.00998</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.00998">https://arxiv.org/pdf/2505.00998</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.00998]] Deterministic-to-Stochastic Diverse Latent Feature Mapping for Human Motion Synthesis(https://arxiv.org/abs/2505.00998)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Human motion synthesis aims to generate plausible human motion sequences, which has raised widespread attention in computer animation. Recent score-based generative models (SGMs) have demonstrated impressive results on this task. However, their training process involves complex curvature trajectories, leading to unstable training process. In this paper, we propose a Deterministic-to-Stochastic Diverse Latent Feature Mapping (DSDFM) method for human motion synthesis. DSDFM consists of two stages. The first human motion reconstruction stage aims to learn the latent space distribution of human motions. The second diverse motion generation stage aims to build connections between the Gaussian distribution and the latent space distribution of human motions, thereby enhancing the diversity and accuracy of the generated human motions. This stage is achieved by the designed deterministic feature mapping procedure with DerODE and stochastic diverse output generation procedure with this http URL is easy to train compared to previous SGMs-based methods and can enhance diversity without introducing additional training this http URL qualitative and quantitative experiments, DSDFM achieves state-of-the-art results surpassing the latest methods, validating its superiority in human motion synthesis.</li>
</ul>

<h3>Title: Where's the liability in the Generative Era? Recovery-based Black-Box Detection of AI-Generated Content</h3>
<ul>
<li><strong>Authors: </strong>Haoyue Bai, Yiyou Sun, Wei Cheng, Haifeng Chen</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01008">https://arxiv.org/abs/2505.01008</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01008">https://arxiv.org/pdf/2505.01008</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01008]] Where's the liability in the Generative Era? Recovery-based Black-Box Detection of AI-Generated Content(https://arxiv.org/abs/2505.01008)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The recent proliferation of photorealistic images created by generative models has sparked both excitement and concern, as these images are increasingly indistinguishable from real ones to the human eye. While offering new creative and commercial possibilities, the potential for misuse, such as in misinformation and fraud, highlights the need for effective detection methods. Current detection approaches often rely on access to model weights or require extensive collections of real image datasets, limiting their scalability and practical application in real world scenarios. In this work, we introduce a novel black box detection framework that requires only API access, sidestepping the need for model weights or large auxiliary datasets. Our approach leverages a corrupt and recover strategy: by masking part of an image and assessing the model ability to reconstruct it, we measure the likelihood that the image was generated by the model itself. For black-box models that do not support masked image inputs, we incorporate a cost efficient surrogate model trained to align with the target model distribution, enhancing detection capability. Our framework demonstrates strong performance, outperforming baseline methods by 4.31% in mean average precision across eight diffusion model variant datasets.</li>
</ul>

<h3>Title: Multi-Step Consistency Models: Fast Generation with Theoretical Guarantees</h3>
<ul>
<li><strong>Authors: </strong>Nishant Jain, Xunpeng Huang, Yian Ma, Tong Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, math.AP, math.ST, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01049">https://arxiv.org/abs/2505.01049</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01049">https://arxiv.org/pdf/2505.01049</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01049]] Multi-Step Consistency Models: Fast Generation with Theoretical Guarantees(https://arxiv.org/abs/2505.01049)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Consistency models have recently emerged as a compelling alternative to traditional SDE based diffusion models, offering a significant acceleration in generation by producing high quality samples in very few steps. Despite their empirical success, a proper theoretic justification for their speed up is still lacking. In this work, we provide the analysis which bridges this gap, showing that given a consistency model which can map the input at a given time to arbitrary timestamps along the reverse trajectory, one can achieve KL divergence of order $ O(\varepsilon^2) $ using only $ O\left(\log\left(\frac{d}{\varepsilon}\right)\right) $ iterations with constant step size, where d is the data dimension. Additionally, under minimal assumptions on the data distribution an increasingly common setting in recent diffusion model analyses we show that a similar KL convergence guarantee can be obtained, with the number of steps scaling as $ O\left(d \log\left(\frac{d}{\varepsilon}\right)\right) $. Going further, we also provide a theoretical analysis for estimation of such consistency models, concluding that accurate learning is feasible using small discretization steps, both in smooth and non smooth settings. Notably, our results for the non smooth case yield best in class convergence rates compared to existing SDE or ODE based analyses under minimal assumptions.</li>
</ul>

<h3>Title: Federated Adapter on Foundation Models: An Out-Of-Distribution Approach</h3>
<ul>
<li><strong>Authors: </strong>Yiyuan Yang, Guodong Long, Tianyi Zhou, Qinghua Lu, Shanshan Ye, Jing Jiang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01075">https://arxiv.org/abs/2505.01075</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01075">https://arxiv.org/pdf/2505.01075</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01075]] Federated Adapter on Foundation Models: An Out-Of-Distribution Approach(https://arxiv.org/abs/2505.01075)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>As foundation models gain prominence, Federated Foundation Models (FedFM) have emerged as a privacy-preserving approach to collaboratively fine-tune models in federated learning (FL) frameworks using distributed datasets across clients. A key challenge for FedFM, given the versatile nature of foundation models, is addressing out-of-distribution (OOD) generalization, where unseen tasks or clients may exhibit distribution shifts leading to suboptimal performance. Although numerous studies have explored OOD generalization in conventional FL, these methods are inadequate for FedFM due to the challenges posed by large parameter scales and increased data heterogeneity. To address these, we propose FedOA, which employs adapter-based parameter-efficient fine-tuning methods for efficacy and introduces personalized adapters with feature distance-based regularization to align distributions and guarantee OOD generalization for each client. Theoretically, we demonstrate that the conventional aggregated global model in FedFM inherently retains OOD generalization capabilities, and our proposed method enhances the personalized model's OOD generalization through regularization informed by the global model, with proven convergence under general non-convex settings. Empirically, the effectiveness of the proposed method is validated on benchmark datasets across various NLP tasks.</li>
</ul>

<h3>Title: Any-to-Any Vision-Language Model for Multimodal X-ray Imaging and Radiological Report Generation</h3>
<ul>
<li><strong>Authors: </strong>Daniele Molino, Francesco di Feola, Linlin Shen, Paolo Soda, Valerio Guarrasi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01091">https://arxiv.org/abs/2505.01091</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01091">https://arxiv.org/pdf/2505.01091</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01091]] Any-to-Any Vision-Language Model for Multimodal X-ray Imaging and Radiological Report Generation(https://arxiv.org/abs/2505.01091)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative models have revolutionized Artificial Intelligence (AI), particularly in multimodal applications. However, adapting these models to the medical domain poses unique challenges due to the complexity of medical data and the stringent need for clinical accuracy. In this work, we introduce a framework specifically designed for multimodal medical data generation. By enabling the generation of multi-view chest X-rays and their associated clinical report, it bridges the gap between general-purpose vision-language models and the specialized requirements of healthcare. Leveraging the MIMIC-CXR dataset, the proposed framework shows superior performance in generating high-fidelity images and semantically coherent reports. Our quantitative evaluation reveals significant results in terms of FID and BLEU scores, showcasing the quality of the generated data. Notably, our framework achieves comparable or even superior performance compared to real data on downstream disease classification tasks, underlining its potential as a tool for medical research and diagnostics. This study highlights the importance of domain-specific adaptations in enhancing the relevance and utility of generative models for clinical applications, paving the way for future advancements in synthetic multimodal medical data generation.</li>
</ul>

<h3>Title: VSC: Visual Search Compositional Text-to-Image Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Do Huu Dat, Nam Hyeonu, Po-Yuan Mao, Tae-Hyun Oh</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01104">https://arxiv.org/abs/2505.01104</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01104">https://arxiv.org/pdf/2505.01104</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01104]] VSC: Visual Search Compositional Text-to-Image Diffusion Model(https://arxiv.org/abs/2505.01104)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models have shown impressive capabilities in generating realistic visuals from natural-language prompts, yet they often struggle with accurately binding attributes to corresponding objects, especially in prompts containing multiple attribute-object pairs. This challenge primarily arises from the limitations of commonly used text encoders, such as CLIP, which can fail to encode complex linguistic relationships and modifiers effectively. Existing approaches have attempted to mitigate these issues through attention map control during inference and the use of layout information or fine-tuning during training, yet they face performance drops with increased prompt complexity. In this work, we introduce a novel compositional generation method that leverages pairwise image embeddings to improve attribute-object binding. Our approach decomposes complex prompts into sub-prompts, generates corresponding images, and computes visual prototypes that fuse with text embeddings to enhance representation. By applying segmentation-based localization training, we address cross-attention misalignment, achieving improved accuracy in binding multiple attributes to objects. Our approaches outperform existing compositional text-to-image diffusion models on the benchmark T2I CompBench, achieving better image quality, evaluated by humans, and emerging robustness under scaling number of binding pairs in the prompt.</li>
</ul>

<h3>Title: Self-Supervision Enhances Instance-based Multiple Instance Learning Methods in Digital Pathology: A Benchmark Study</h3>
<ul>
<li><strong>Authors: </strong>Ali Mammadov, Loic Le Folgoc, Julien Adam, Anne Buronfosse, Gilles Hayem, Guillaume Hocquet, Pietro Gori</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01109">https://arxiv.org/abs/2505.01109</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01109">https://arxiv.org/pdf/2505.01109</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01109]] Self-Supervision Enhances Instance-based Multiple Instance Learning Methods in Digital Pathology: A Benchmark Study(https://arxiv.org/abs/2505.01109)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>Multiple Instance Learning (MIL) has emerged as the best solution for Whole Slide Image (WSI) classification. It consists of dividing each slide into patches, which are treated as a bag of instances labeled with a global label. MIL includes two main approaches: instance-based and embedding-based. In the former, each patch is classified independently, and then the patch scores are aggregated to predict the bag label. In the latter, bag classification is performed after aggregating patch embeddings. Even if instance-based methods are naturally more interpretable, embedding-based MILs have usually been preferred in the past due to their robustness to poor feature extractors. However, recently, the quality of feature embeddings has drastically increased using self-supervised learning (SSL). Nevertheless, many authors continue to endorse the superiority of embedding-based MIL. To investigate this further, we conduct 710 experiments across 4 datasets, comparing 10 MIL strategies, 6 self-supervised methods with 4 backbones, 4 foundation models, and various pathology-adapted techniques. Furthermore, we introduce 4 instance-based MIL methods never used before in the pathology domain. Through these extensive experiments, we show that with a good SSL feature extractor, simple instance-based MILs, with very few parameters, obtain similar or better performance than complex, state-of-the-art (SOTA) embedding-based MIL methods, setting new SOTA results on the BRACS and Camelyon16 datasets. Since simple instance-based MIL methods are naturally more interpretable and explainable to clinicians, our results suggest that more effort should be put into well-adapted SSL methods for WSI rather than into complex embedding-based MIL methods.</li>
</ul>

<h3>Title: MateICL: Mitigating Attention Dispersion in Large-Scale In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Murtadha Ahmed, Wenbo, Liu yunfeng</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01110">https://arxiv.org/abs/2505.01110</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01110">https://arxiv.org/pdf/2505.01110</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01110]] MateICL: Mitigating Attention Dispersion in Large-Scale In-Context Learning(https://arxiv.org/abs/2505.01110)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated remarkable capabilities in In-Context Learning (ICL). However, the fixed position length constraints in pre-trained models limit the number of demonstration examples. Recent efforts to extend context suffer from attention dispersion as the number of demonstrations increases. In this paper, we introduce Mitigating Attention Dispersion in large-scale ICL (MateICL) that enables LLMs to maintain effective self-attention as the context size grows. We first split the context into multiple windows, each filled to the model's context capacity, which are processed separately. Then, we introduce an additional layer to recalibrate the attention weights, prioritizing the query tokens as the number of demonstrations increases. Our empirical results show that MateICL can effectively leverage larger contexts to improve ICL performance. Compared to retrieval-based baselines, MateICL consistently achieves better performance without requiring an externally trained retrieval model. Despite recent advances in inference strategies (e.g., 32k token contexts), our results demonstrate that MateICL remains beneficial in computationally resource-constrained settings. The code is publicly available at this https URL.</li>
</ul>

<h3>Title: Incorporating Inductive Biases to Energy-based Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Yukun Li, Li-Ping Liu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01111">https://arxiv.org/abs/2505.01111</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01111">https://arxiv.org/pdf/2505.01111</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01111]] Incorporating Inductive Biases to Energy-based Generative Models(https://arxiv.org/abs/2505.01111)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>With the advent of score-matching techniques for model training and Langevin dynamics for sample generation, energy-based models (EBMs) have gained renewed interest as generative models. Recent EBMs usually use neural networks to define their energy functions. In this work, we introduce a novel hybrid approach that combines an EBM with an exponential family model to incorporate inductive bias into data modeling. Specifically, we augment the energy term with a parameter-free statistic function to help the model capture key data statistics. Like an exponential family model, the hybrid model aims to align the distribution statistics with data statistics during model training, even when it only approximately maximizes the data likelihood. This property enables us to impose constraints on the hybrid model. Our empirical study validates the hybrid model's ability to match statistics. Furthermore, experimental results show that data fitting and generation improve when suitable informative statistics are incorporated into the hybrid model.</li>
</ul>

<h3>Title: Aggregation of Dependent Expert Distributions in Multimodal Variational Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Rogelio A Mancisidor, Robert Jenssen, Shujian Yu, Michael Kampffmeyer</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01134">https://arxiv.org/abs/2505.01134</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01134">https://arxiv.org/pdf/2505.01134</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01134]] Aggregation of Dependent Expert Distributions in Multimodal Variational Autoencoders(https://arxiv.org/abs/2505.01134)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Multimodal learning with variational autoencoders (VAEs) requires estimating joint distributions to evaluate the evidence lower bound (ELBO). Current methods, the product and mixture of experts, aggregate single-modality distributions assuming independence for simplicity, which is an overoptimistic assumption. This research introduces a novel methodology for aggregating single-modality distributions by exploiting the principle of consensus of dependent experts (CoDE), which circumvents the aforementioned assumption. Utilizing the CoDE method, we propose a novel ELBO that approximates the joint likelihood of the multimodal data by learning the contribution of each subset of modalities. The resulting CoDE-VAE model demonstrates better performance in terms of balancing the trade-off between generative coherence and generative quality, as well as generating more precise log-likelihood estimations. CoDE-VAE further minimizes the generative quality gap as the number of modalities increases. In certain cases, it reaches a generative quality similar to that of unimodal VAEs, which is a desirable property that is lacking in most current methods. Finally, the classification accuracy achieved by CoDE-VAE is comparable to that of state-of-the-art multimodal VAE models.</li>
</ul>

<h3>Title: FreePCA: Integrating Consistency Information across Long-short Frames in Training-free Long Video Generation via Principal Component Analysis</h3>
<ul>
<li><strong>Authors: </strong>Jiangtong Tan, Hu Yu, Jie Huang, Jie Xiao, Feng Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01172">https://arxiv.org/abs/2505.01172</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01172">https://arxiv.org/pdf/2505.01172</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01172]] FreePCA: Integrating Consistency Information across Long-short Frames in Training-free Long Video Generation via Principal Component Analysis(https://arxiv.org/abs/2505.01172)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Long video generation involves generating extended videos using models trained on short videos, suffering from distribution shifts due to varying frame counts. It necessitates the use of local information from the original short frames to enhance visual and motion quality, and global information from the entire long frames to ensure appearance consistency. Existing training-free methods struggle to effectively integrate the benefits of both, as appearance and motion in videos are closely coupled, leading to motion inconsistency and visual quality. In this paper, we reveal that global and local information can be precisely decoupled into consistent appearance and motion intensity information by applying Principal Component Analysis (PCA), allowing for refined complementary integration of global consistency and local quality. With this insight, we propose FreePCA, a training-free long video generation paradigm based on PCA that simultaneously achieves high consistency and quality. Concretely, we decouple consistent appearance and motion intensity features by measuring cosine similarity in the principal component space. Critically, we progressively integrate these features to preserve original quality and ensure smooth transitions, while further enhancing consistency by reusing the mean statistics of the initial noise. Experiments demonstrate that FreePCA can be applied to various video diffusion models without requiring training, leading to substantial improvements. Code is available at this https URL.</li>
</ul>

<h3>Title: TSTMotion: Training-free Scene-awarenText-to-motion Generation</h3>
<ul>
<li><strong>Authors: </strong>Ziyan Guo, Haoxuan Qu, Hossein Rahmani, Dewen Soh, Ping Hu, Qiuhong Ke, Jun Liu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01182">https://arxiv.org/abs/2505.01182</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01182">https://arxiv.org/pdf/2505.01182</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01182]] TSTMotion: Training-free Scene-awarenText-to-motion Generation(https://arxiv.org/abs/2505.01182)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Text-to-motion generation has recently garnered significant research interest, primarily focusing on generating human motion sequences in blank backgrounds. However, human motions commonly occur within diverse 3D scenes, which has prompted exploration into scene-aware text-to-motion generation methods. Yet, existing scene-aware methods often rely on large-scale ground-truth motion sequences in diverse 3D scenes, which poses practical challenges due to the expensive cost. To mitigate this challenge, we are the first to propose a \textbf{T}raining-free \textbf{S}cene-aware \textbf{T}ext-to-\textbf{Motion} framework, dubbed as \textbf{TSTMotion}, that efficiently empowers pre-trained blank-background motion generators with the scene-aware capability. Specifically, conditioned on the given 3D scene and text description, we adopt foundation models together to reason, predict and validate a scene-aware motion guidance. Then, the motion guidance is incorporated into the blank-background motion generators with two modifications, resulting in scene-aware text-driven motion sequences. Extensive experiments demonstrate the efficacy and generalizability of our proposed framework. We release our code in \href{this https URL}{Project Page}.</li>
</ul>

<h3>Title: Secure Cluster-Based Hierarchical Federated Learning in Vehicular Networks</h3>
<ul>
<li><strong>Authors: </strong>M. Saeid HaghighiFard, Sinem Coleri</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.DC, cs.LG, eess.SY</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01186">https://arxiv.org/abs/2505.01186</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01186">https://arxiv.org/pdf/2505.01186</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01186]] Secure Cluster-Based Hierarchical Federated Learning in Vehicular Networks(https://arxiv.org/abs/2505.01186)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Hierarchical Federated Learning (HFL) has recently emerged as a promising solution for intelligent decision-making in vehicular networks, helping to address challenges such as limited communication resources, high vehicle mobility, and data heterogeneity. However, HFL remains vulnerable to adversarial and unreliable vehicles, whose misleading updates can significantly compromise the integrity and convergence of the global model. To address these challenges, we propose a novel defense framework that integrates dynamic vehicle selection with robust anomaly detection within a cluster-based HFL architecture, specifically designed to counter Gaussian noise and gradient ascent attacks. The framework performs a comprehensive reliability assessment for each vehicle by evaluating historical accuracy, contribution frequency, and anomaly records. Anomaly detection combines Z-score and cosine similarity analyses on model updates to identify both statistical outliers and directional deviations in model updates. To further refine detection, an adaptive thresholding mechanism is incorporated into the cosine similarity metric, dynamically adjusting the threshold based on the historical accuracy of each vehicle to enforce stricter standards for consistently high-performing vehicles. In addition, a weighted gradient averaging mechanism is implemented, which assigns higher weights to gradient updates from more trustworthy vehicles. To defend against coordinated attacks, a cross-cluster consistency check is applied to identify collaborative attacks in which multiple compromised clusters coordinate misleading updates. Together, these mechanisms form a multi-level defense strategy to filter out malicious contributions effectively. Simulation results show that the proposed algorithm significantly reduces convergence time compared to benchmark methods across both 1-hop and 3-hop topologies.</li>
</ul>

<h3>Title: Enhancing Obsolescence Forecasting with Deep Generative Data Augmentation: A Semi-Supervised Framework for Low-Data Industrial Applications</h3>
<ul>
<li><strong>Authors: </strong>Elie Saad, Mariem Besbes, Marc Zolghadri, Victor Czmil, Claude Baron, Vincent Bourgeois</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01261">https://arxiv.org/abs/2505.01261</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01261">https://arxiv.org/pdf/2505.01261</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01261]] Enhancing Obsolescence Forecasting with Deep Generative Data Augmentation: A Semi-Supervised Framework for Low-Data Industrial Applications(https://arxiv.org/abs/2505.01261)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The challenge of electronic component obsolescence is particularly critical in systems with long life cycles. Various obsolescence management methods are employed to mitigate its impact, with obsolescence forecasting being a highly sought-after and prominent approach. As a result, numerous machine learning-based forecasting methods have been proposed. However, machine learning models require a substantial amount of relevant data to achieve high precision, which is lacking in the current obsolescence landscape in some situations. This work introduces a novel framework for obsolescence forecasting based on deep learning. The proposed framework solves the lack of available data through deep generative modeling, where new obsolescence cases are generated and used to augment the training dataset. The augmented dataset is then used to train a classical machine learning-based obsolescence forecasting model. To train classical forecasting models using augmented datasets, existing classical supervised-learning classifiers are adapted for semi-supervised learning within this framework. The proposed framework demonstrates state-of-the-art results on benchmarking datasets.</li>
</ul>

<h3>Title: Diffusion-based Adversarial Purification from the Perspective of the Frequency Domain</h3>
<ul>
<li><strong>Authors: </strong>Gaozheng Pei, Ke Ma, Yingfei Sun, Qianqian Xu, Qingming Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01267">https://arxiv.org/abs/2505.01267</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01267">https://arxiv.org/pdf/2505.01267</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01267]] Diffusion-based Adversarial Purification from the Perspective of the Frequency Domain(https://arxiv.org/abs/2505.01267)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The diffusion-based adversarial purification methods attempt to drown adversarial perturbations into a part of isotropic noise through the forward process, and then recover the clean images through the reverse process. Due to the lack of distribution information about adversarial perturbations in the pixel domain, it is often unavoidable to damage normal semantics. We turn to the frequency domain perspective, decomposing the image into amplitude spectrum and phase spectrum. We find that for both spectra, the damage caused by adversarial perturbations tends to increase monotonically with frequency. This means that we can extract the content and structural information of the original clean sample from the frequency components that are less damaged. Meanwhile, theoretical analysis indicates that existing purification methods indiscriminately damage all frequency components, leading to excessive damage to the image. Therefore, we propose a purification method that can eliminate adversarial perturbations while maximizing the preservation of the content and structure of the original image. Specifically, at each time step during the reverse process, for the amplitude spectrum, we replace the low-frequency components of the estimated image's amplitude spectrum with the corresponding parts of the adversarial image. For the phase spectrum, we project the phase of the estimated image into a designated range of the adversarial image's phase spectrum, focusing on the low frequencies. Empirical evidence from extensive experiments demonstrates that our method significantly outperforms most current defense methods.</li>
</ul>

<h3>Title: FreeInsert: Disentangled Text-Guided Object Insertion in 3D Gaussian Scene without Spatial Priors</h3>
<ul>
<li><strong>Authors: </strong>Chenxi Li, Weijie Wang, Qiang Li, Bruno Lepri, Nicu Sebe, Weizhi Nie</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01322">https://arxiv.org/abs/2505.01322</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01322">https://arxiv.org/pdf/2505.01322</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01322]] FreeInsert: Disentangled Text-Guided Object Insertion in 3D Gaussian Scene without Spatial Priors(https://arxiv.org/abs/2505.01322)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model</a></li>
<li><strong>Abstract: </strong>Text-driven object insertion in 3D scenes is an emerging task that enables intuitive scene editing through natural language. However, existing 2D editing-based methods often rely on spatial priors such as 2D masks or 3D bounding boxes, and they struggle to ensure consistency of the inserted object. These limitations hinder flexibility and scalability in real-world applications. In this paper, we propose FreeInsert, a novel framework that leverages foundation models including MLLMs, LGMs, and diffusion models to disentangle object generation from spatial placement. This enables unsupervised and flexible object insertion in 3D scenes without spatial priors. FreeInsert starts with an MLLM-based parser that extracts structured semantics, including object types, spatial relationships, and attachment regions, from user instructions. These semantics guide both the reconstruction of the inserted object for 3D consistency and the learning of its degrees of freedom. We leverage the spatial reasoning capabilities of MLLMs to initialize object pose and scale. A hierarchical, spatially aware refinement stage further integrates spatial semantics and MLLM-inferred priors to enhance placement. Finally, the appearance of the object is improved using the inserted-object image to enhance visual fidelity. Experimental results demonstrate that FreeInsert achieves semantically coherent, spatially precise, and visually realistic 3D insertions without relying on spatial priors, offering a user-friendly and flexible editing experience.</li>
</ul>

<h3>Title: VIDSTAMP: A Temporally-Aware Watermark for Ownership and Integrity in Video Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Mohammadreza Teymoorianfard, Shiqing Ma, Amir Houmansadr</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01406">https://arxiv.org/abs/2505.01406</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01406">https://arxiv.org/pdf/2505.01406</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01406]] VIDSTAMP: A Temporally-Aware Watermark for Ownership and Integrity in Video Diffusion Models(https://arxiv.org/abs/2505.01406)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The rapid rise of video diffusion models has enabled the generation of highly realistic and temporally coherent videos, raising critical concerns about content authenticity, provenance, and misuse. Existing watermarking approaches, whether passive, post-hoc, or adapted from image-based techniques, often struggle to withstand video-specific manipulations such as frame insertion, dropping, or reordering, and typically degrade visual quality. In this work, we introduce VIDSTAMP, a watermarking framework that embeds per-frame or per-segment messages directly into the latent space of temporally-aware video diffusion models. By fine-tuning the model's decoder through a two-stage pipeline, first on static image datasets to promote spatial message separation, and then on synthesized video sequences to restore temporal consistency, VIDSTAMP learns to embed high-capacity, flexible watermarks with minimal perceptual impact. Leveraging architectural components such as 3D convolutions and temporal attention, our method imposes no additional inference cost and offers better perceptual quality than prior methods, while maintaining comparable robustness against common distortions and tampering. VIDSTAMP embeds 768 bits per video (48 bits per frame) with a bit accuracy of 95.0%, achieves a log P-value of -166.65 (lower is better), and maintains a video quality score of 0.836, comparable to unwatermarked outputs (0.838) and surpassing prior methods in capacity-quality tradeoffs. Code: Code: \url{this https URL}</li>
</ul>

<h3>Title: How Effective are Large Time Series Models in Hydrology? A Study on Water Level Forecasting in Everglades</h3>
<ul>
<li><strong>Authors: </strong>Rahuul Rangaraj, Jimeng Shi, Azam Shirali, Rajendra Paudel, Yanzhao Wu, Giri Narasimhan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2505.01415">https://arxiv.org/abs/2505.01415</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2505.01415">https://arxiv.org/pdf/2505.01415</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2505.01415]] How Effective are Large Time Series Models in Hydrology? A Study on Water Level Forecasting in Everglades(https://arxiv.org/abs/2505.01415)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>The Everglades play a crucial role in flood and drought regulation, water resource planning, and ecosystem management in the surrounding regions. However, traditional physics-based and statistical methods for predicting water levels often face significant challenges, including high computational costs and limited adaptability to diverse or unforeseen conditions. Recent advancements in large time series models have demonstrated the potential to address these limitations, with state-of-the-art deep learning and foundation models achieving remarkable success in time series forecasting across various domains. Despite this progress, their application to critical environmental systems, such as the Everglades, remains underexplored. In this study, we fill the gap by investigating twelve task-specific models and five time series foundation models across six categories for a real-world application focused on water level prediction in the Everglades. Our primary results show that the foundation model, Chronos, significantly outperforms all other models while the remaining foundation models exhibit relatively poor performance. Moreover, the performance of task-specific models varies with the model architectures. Lastly, we discuss the possible reasons for the varying performance of models.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
