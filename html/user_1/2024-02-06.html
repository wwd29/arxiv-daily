<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-02-06</h1>
<h3>Title: Detection of Machine-Generated Text: Literature Survey</h3>
<ul>
<li><strong>Authors: </strong>Dmytro Valiaiev</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01642">https://arxiv.org/abs/2402.01642</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01642">https://arxiv.org/pdf/2402.01642</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01642]] Detection of Machine-Generated Text: Literature Survey(https://arxiv.org/abs/2402.01642)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Since language models produce fake text quickly and easily, there is an oversupply of such content in the public domain. The degree of sophistication and writing style has reached a point where differentiating between human authored and machine-generated content is nearly impossible. As a result, works generated by language models rather than human authors have gained significant media attention and stirred controversy.Concerns regarding the possible influence of advanced language models on society have also arisen, needing a fuller knowledge of these processes. Natural language generation (NLG) and generative pre-trained transformer (GPT) models have revolutionized a variety of sectors: the scope not only permeated throughout journalism and customer service but also reached academia. To mitigate the hazardous implications that may arise from the use of these models, preventative measures must be implemented, such as providing human agents with the capacity to distinguish between artificially made and human composed texts utilizing automated systems and possibly reverse-engineered language models. Furthermore, to ensure a balanced and responsible approach, it is critical to have a full grasp of the socio-technological ramifications of these breakthroughs. This literature survey aims to compile and synthesize accomplishments and developments in the aforementioned work, while also identifying future prospects. It also gives an overview of machine-generated text trends and explores the larger societal implications. Ultimately, this survey intends to contribute to the development of robust and effective approaches for resolving the issues connected with the usage and detection of machine-generated text by exploring the interplay between the capabilities of language models and their possible implications.</li>
</ul>

<h3>Title: SMUTF: Schema Matching Using Generative Tags and Hybrid Features</h3>
<ul>
<li><strong>Authors: </strong>Yu Zhang, Mei Di, Haozheng Luo, Chenwei Xu, Richard Tzong-Han Tsai</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.DB</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01685">https://arxiv.org/abs/2402.01685</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01685">https://arxiv.org/pdf/2402.01685</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01685]] SMUTF: Schema Matching Using Generative Tags and Hybrid Features(https://arxiv.org/abs/2402.01685)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce SMUTF, a unique approach for large-scale tabular data schema matching (SM), which assumes that supervised learning does not affect performance in open-domain tasks, thereby enabling effective cross-domain matching. This system uniquely combines rule-based feature engineering, pre-trained language models, and generative large language models. In an innovative adaptation inspired by the Humanitarian Exchange Language, we deploy 'generative tags' for each data column, enhancing the effectiveness of SM. SMUTF exhibits extensive versatility, working seamlessly with any pre-existing pre-trained embeddings, classification methods, and generative models. Recognizing the lack of extensive, publicly available datasets for SM, we have created and open-sourced the HDXSM dataset from the public humanitarian data. We believe this to be the most exhaustive SM dataset currently available. In evaluations across various public datasets and the novel HDXSM dataset, SMUTF demonstrated exceptional performance, surpassing existing state-of-the-art models in terms of accuracy and efficiency, and} improving the F1 score by 11.84% and the AUC of ROC by 5.08%.</li>
</ul>

<h3>Title: Maximizing Data Efficiency for Cross-Lingual TTS Adaptation by  Self-Supervised Representation Mixing and Embedding Initialization</h3>
<ul>
<li><strong>Authors: </strong>Wei-Ping Huang, Sung-Feng Huang, Hung-yi Lee</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01692">https://arxiv.org/abs/2402.01692</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01692">https://arxiv.org/pdf/2402.01692</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01692]] Maximizing Data Efficiency for Cross-Lingual TTS Adaptation by  Self-Supervised Representation Mixing and Embedding Initialization(https://arxiv.org/abs/2402.01692)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>This paper presents an effective transfer learning framework for language adaptation in text-to-speech systems, with a focus on achieving language adaptation using minimal labeled and unlabeled data. While many works focus on reducing the usage of labeled data, very few consider minimizing the usage of unlabeled data. By utilizing self-supervised features in the pretraining stage, replacing the noisy portion of pseudo labels with these features during fine-tuning, and incorporating an embedding initialization trick, our method leverages more information from unlabeled data compared to conventional approaches. Experimental results show that our framework is able to synthesize intelligible speech in unseen languages with only 4 utterances of labeled data and 15 minutes of unlabeled data. Our methodology continues to surpass conventional techniques, even when a greater volume of data is accessible. These findings highlight the potential of our data-efficient language adaptation framework.</li>
</ul>

<h3>Title: Quality of Answers of Generative Large Language Models vs Peer Patients  for Interpreting Lab Test Results for Lay Patients: Evaluation Study</h3>
<ul>
<li><strong>Authors: </strong>Zhe He, Balu Bhasuran, Qiao Jin, Shubo Tian, Karim Hanna, Cindy Shavor, Lisbeth Garcia Arguello, Patrick Murray, Zhiyong Lu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01693">https://arxiv.org/abs/2402.01693</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01693">https://arxiv.org/pdf/2402.01693</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01693]] Quality of Answers of Generative Large Language Models vs Peer Patients  for Interpreting Lab Test Results for Lay Patients: Evaluation Study(https://arxiv.org/abs/2402.01693)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Lab results are often confusing and hard to understand. Large language models (LLMs) such as ChatGPT have opened a promising avenue for patients to get their questions answered. We aim to assess the feasibility of using LLMs to generate relevant, accurate, helpful, and unharmful responses to lab test-related questions asked by patients and to identify potential issues that can be mitigated with augmentation approaches. We first collected lab test results related question and answer data from Yahoo! Answers and selected 53 QA pairs for this study. Using the LangChain framework and ChatGPT web portal, we generated responses to the 53 questions from four LLMs including GPT-4, Meta LLaMA 2, MedAlpaca, and ORCA_mini. We first assessed the similarity of their answers using standard QA similarity-based evaluation metrics including ROUGE, BLEU, METEOR, BERTScore. We also utilized an LLM-based evaluator to judge whether a target model has higher quality in terms of relevance, correctness, helpfulness, and safety than the baseline model. Finally, we performed a manual evaluation with medical experts for all the responses to seven selected questions on the same four aspects. The results of Win Rate and medical expert evaluation both showed that GPT-4's responses achieved better scores than all the other LLM responses and human responses on all four aspects (relevance, correctness, helpfulness, and safety). However, LLM responses occasionally also suffer from a lack of interpretation in one's medical context, incorrect statements, and lack of references. We find that compared to other three LLMs and human answer from the Q&A website, GPT-4's responses are more accurate, helpful, relevant, and safer. However, there are cases which GPT-4 responses are inaccurate and not individualized. We identified a number of ways to improve the quality of LLM responses.</li>
</ul>

<h3>Title: Large language model empowered participatory urban planning</h3>
<ul>
<li><strong>Authors: </strong>Zhilun Zhou, Yuming Lin, Yong Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01698">https://arxiv.org/abs/2402.01698</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01698">https://arxiv.org/pdf/2402.01698</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01698]] Large language model empowered participatory urban planning(https://arxiv.org/abs/2402.01698)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Participatory urban planning is the mainstream of modern urban planning and involves the active engagement of different stakeholders. However, the traditional participatory paradigm encounters challenges in time and manpower, while the generative planning tools fail to provide adjustable and inclusive solutions. This research introduces an innovative urban planning approach integrating Large Language Models (LLMs) within the participatory process. The framework, based on the crafted LLM agent, consists of role-play, collaborative generation, and feedback iteration, solving a community-level land-use task catering to 1000 distinct interests. Empirical experiments in diverse urban communities exhibit LLM's adaptability and effectiveness across varied planning scenarios. The results were evaluated on four metrics, surpassing human experts in satisfaction and inclusion, and rivaling state-of-the-art reinforcement learning methods in service and ecology. Further analysis shows the advantage of LLM agents in providing adjustable and inclusive solutions with natural language reasoning and strong scalability. While implementing the recent advancements in emulating human behavior for planning, this work envisions both planners and citizens benefiting from low-cost, efficient LLM agents, which is crucial for enhancing participation and realizing participatory urban planning.</li>
</ul>

<h3>Title: States as Strings as Strategies: Steering Language Models with  Game-Theoretic Solvers</h3>
<ul>
<li><strong>Authors: </strong>Ian Gemp, Yoram Bachrach, Marc Lanctot, Roma Patel, Vibhavari Dasagi, Luke Marris, Georgios Piliouras, Karl Tuyls</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.GT</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01704">https://arxiv.org/abs/2402.01704</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01704">https://arxiv.org/pdf/2402.01704</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01704]] States as Strings as Strategies: Steering Language Models with  Game-Theoretic Solvers(https://arxiv.org/abs/2402.01704)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Game theory is the study of mathematical models of strategic interactions among rational agents. Language is a key medium of interaction for humans, though it has historically proven difficult to model dialogue and its strategic motivations mathematically. A suitable model of the players, strategies, and payoffs associated with linguistic interactions (i.e., a binding to the conventional symbolic logic of game theory) would enable existing game-theoretic algorithms to provide strategic solutions in the space of language. In other words, a binding could provide a route to computing stable, rational conversational strategies in dialogue. Large language models (LLMs) have arguably reached a point where their generative capabilities can enable realistic, human-like simulations of natural dialogue. By prompting them in various ways, we can steer their responses towards different output utterances. Leveraging the expressivity of natural language, LLMs can also help us quickly generate new dialogue scenarios, which are grounded in real world applications. In this work, we present one possible binding from dialogue to game theory as well as generalizations of existing equilibrium finding algorithms to this setting. In addition, by exploiting LLMs generation capabilities along with our proposed binding, we can synthesize a large repository of formally-defined games in which one can study and test game-theoretic solution concepts. We also demonstrate how one can combine LLM-driven game generation, game-theoretic solvers, and imitation learning to construct a process for improving the strategic capabilities of LLMs.</li>
</ul>

<h3>Title: Not My Voice! A Taxonomy of Ethical and Safety Harms of Speech  Generators</h3>
<ul>
<li><strong>Authors: </strong>Wiebke Hutiri, Oresiti Papakyriakopoulos, Alice Xiang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CY, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01708">https://arxiv.org/abs/2402.01708</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01708">https://arxiv.org/pdf/2402.01708</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01708]] Not My Voice! A Taxonomy of Ethical and Safety Harms of Speech  Generators(https://arxiv.org/abs/2402.01708)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid and wide-scale adoption of AI to generate human speech poses a range of significant ethical and safety risks to society that need to be addressed. For example, a growing number of speech generation incidents are associated with swatting attacks in the United States, where anonymous perpetrators create synthetic voices that call police officers to close down schools and hospitals, or to violently gain access to innocent citizens' homes. Incidents like this demonstrate that multimodal generative AI risks and harms do not exist in isolation, but arise from the interactions of multiple stakeholders and technical AI systems. In this paper we analyse speech generation incidents to study how patterns of specific harms arise. We find that specific harms can be categorised according to the exposure of affected individuals, that is to say whether they are a subject of, interact with, suffer due to, or are excluded from speech generation systems. Similarly, specific harms are also a consequence of the motives of the creators and deployers of the systems. Based on these insights we propose a conceptual framework for modelling pathways to ethical and safety harms of AI, which we use to develop a taxonomy of harms of speech generators. Our relational approach captures the complexity of risks and harms in sociotechnical AI systems, and yields an extensible taxonomy that can support appropriate policy interventions and decision making for responsible multimodal model development and release of speech generators.</li>
</ul>

<h3>Title: Socially Aware Synthetic Data Generation for Suicidal Ideation Detection  Using Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Hamideh Ghanadian, Isar Nejadgholi, Hussein Al Osman</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01712">https://arxiv.org/abs/2402.01712</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01712">https://arxiv.org/pdf/2402.01712</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01712]] Socially Aware Synthetic Data Generation for Suicidal Ideation Detection  Using Large Language Models(https://arxiv.org/abs/2402.01712)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Suicidal ideation detection is a vital research area that holds great potential for improving mental health support systems. However, the sensitivity surrounding suicide-related data poses challenges in accessing large-scale, annotated datasets necessary for training effective machine learning models. To address this limitation, we introduce an innovative strategy that leverages the capabilities of generative AI models, such as ChatGPT, Flan-T5, and Llama, to create synthetic data for suicidal ideation detection. Our data generation approach is grounded in social factors extracted from psychology literature and aims to ensure coverage of essential information related to suicidal ideation. In our study, we benchmarked against state-of-the-art NLP classification models, specifically, those centered around the BERT family structures. When trained on the real-world dataset, UMD, these conventional models tend to yield F1-scores ranging from 0.75 to 0.87. Our synthetic data-driven method, informed by social factors, offers consistent F1-scores of 0.82 for both models, suggesting that the richness of topics in synthetic data can bridge the performance gap across different model complexities. Most impressively, when we combined a mere 30% of the UMD dataset with our synthetic data, we witnessed a substantial increase in performance, achieving an F1-score of 0.88 on the UMD test set. Such results underscore the cost-effectiveness and potential of our approach in confronting major challenges in the field, such as data scarcity and the quest for diversity in data representation.</li>
</ul>

<h3>Title: Prompting Large Language Models for Zero-Shot Clinical Prediction with  Structured Longitudinal Electronic Health Record Data</h3>
<ul>
<li><strong>Authors: </strong>Yinghao Zhu, Zixiang Wang, Junyi Gao, Yuning Tong, Jingkun An, Weibin Liao, Ewen M. Harrison, Liantao Ma, Chengwei Pan</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01713">https://arxiv.org/abs/2402.01713</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01713">https://arxiv.org/pdf/2402.01713</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01713]] Prompting Large Language Models for Zero-Shot Clinical Prediction with  Structured Longitudinal Electronic Health Record Data(https://arxiv.org/abs/2402.01713)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>The inherent complexity of structured longitudinal Electronic Health Records (EHR) data poses a significant challenge when integrated with Large Language Models (LLMs), which are traditionally tailored for natural language processing. Motivated by the urgent need for swift decision-making during new disease outbreaks, where traditional predictive models often fail due to a lack of historical data, this research investigates the adaptability of LLMs, like GPT-4, to EHR data. We particularly focus on their zero-shot capabilities, which enable them to make predictions in scenarios in which they haven't been explicitly trained. In response to the longitudinal, sparse, and knowledge-infused nature of EHR data, our prompting approach involves taking into account specific EHR characteristics such as units and reference ranges, and employing an in-context learning strategy that aligns with clinical contexts. Our comprehensive experiments on the MIMIC-IV and TJH datasets demonstrate that with our elaborately designed prompting framework, LLMs can improve prediction performance in key tasks such as mortality, length-of-stay, and 30-day readmission by about 35\%, surpassing ML models in few-shot settings. Our research underscores the potential of LLMs in enhancing clinical decision-making, especially in urgent healthcare situations like the outbreak of emerging diseases with no labeled data. The code is publicly available at https://github.com/yhzhu99/llm4healthcare for reproducibility.</li>
</ul>

<h3>Title: From RAG to QA-RAG: Integrating Generative AI for Pharmaceutical  Regulatory Compliance Process</h3>
<ul>
<li><strong>Authors: </strong>Jaewoong Kim (Sungkyunkwan University), Moohong Min (Sungkyunkwan University)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01717">https://arxiv.org/abs/2402.01717</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01717">https://arxiv.org/pdf/2402.01717</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01717]] From RAG to QA-RAG: Integrating Generative AI for Pharmaceutical  Regulatory Compliance Process(https://arxiv.org/abs/2402.01717)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Regulatory compliance in the pharmaceutical industry entails navigating through complex and voluminous guidelines, often requiring significant human resources. To address these challenges, our study introduces a chatbot model that utilizes generative AI and the Retrieval Augmented Generation (RAG) method. This chatbot is designed to search for guideline documents relevant to the user inquiries and provide answers based on the retrieved guidelines. Recognizing the inherent need for high reliability in this domain, we propose the Question and Answer Retrieval Augmented Generation (QA-RAG) model. In comparative experiments, the QA-RAG model demonstrated a significant improvement in accuracy, outperforming all other baselines including conventional RAG methods. This paper details QA-RAG's structure and performance evaluation, emphasizing its potential for the regulatory compliance domain in the pharmaceutical industry and beyond. We have made our work publicly available for further research and development.</li>
</ul>

<h3>Title: Contextualization Distillation from Large Language Model for Knowledge  Graph Completion</h3>
<ul>
<li><strong>Authors: </strong>Dawei Li, Zhen Tan, Tianlong Chen, Huan Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01729">https://arxiv.org/abs/2402.01729</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01729">https://arxiv.org/pdf/2402.01729</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01729]] Contextualization Distillation from Large Language Model for Knowledge  Graph Completion(https://arxiv.org/abs/2402.01729)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>While textual information significantly enhances the performance of pre-trained language models (PLMs) in knowledge graph completion (KGC), the static and noisy nature of existing corpora collected from Wikipedia articles or synsets definitions often limits the potential of PLM-based KGC models. To surmount these challenges, we introduce the Contextualization Distillation strategy, a versatile plug-in-and-play approach compatible with both discriminative and generative KGC frameworks. Our method begins by instructing large language models (LLMs) to transform compact, structural triplets into context-rich segments. Subsequently, we introduce two tailored auxiliary tasks, reconstruction and contextualization, allowing smaller KGC models to assimilate insights from these enriched triplets. Comprehensive evaluations across diverse datasets and KGC techniques highlight the efficacy and adaptability of our approach, revealing consistent performance enhancements irrespective of underlying pipelines or architectures. Moreover, our analysis makes our method more explainable and provides insight into generating path selection, as well as the choosing of suitable distillation tasks. All the code and data in this work will be released at https://github.com/David-Li0406/Contextulization-Distillation</li>
</ul>

<h3>Title: Assistive Large Language Model Agents for Socially-Aware Negotiation  Dialogues</h3>
<ul>
<li><strong>Authors: </strong>Yuncheng Hua, Lizhen Qu, Gholamreza Haffari</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01737">https://arxiv.org/abs/2402.01737</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01737">https://arxiv.org/pdf/2402.01737</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01737]] Assistive Large Language Model Agents for Socially-Aware Negotiation  Dialogues(https://arxiv.org/abs/2402.01737)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>In this work, we aim to develop LLM agents to mitigate social norm violations in negotiations in a multi-agent setting. We simulate real-world negotiations by letting two large Language Models (LLMs) play the roles of two negotiators in each conversation. A third LLM acts as a remediation agent to rewrite utterances violating norms for improving negotiation outcomes. As it is a novel task, no manually constructed data is available. To address this limitation, we introduce a value impact based In-Context Learning (ICL) method to identify high-quality ICL examples for the LLM-based remediation agents, where the value impact function measures the quality of negotiation outcomes. We show the connection of this method to policy learning and provide rich empirical evidence to demonstrate its effectiveness in negotiations across three different topics: product sale, housing price, and salary negotiation. The source code and the generated dataset will be publicly available upon acceptance.</li>
</ul>

<h3>Title: Towards Optimizing the Costs of LLM Usage</h3>
<ul>
<li><strong>Authors: </strong>Shivanshu Shekhar, Tanishq Dubey, Koyel Mukherjee, Apoorv Saxena, Atharv Tyagi, Nishanth Kotla</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01742">https://arxiv.org/abs/2402.01742</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01742">https://arxiv.org/pdf/2402.01742</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01742]] Towards Optimizing the Costs of LLM Usage(https://arxiv.org/abs/2402.01742)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative AI and LLMs in particular are heavily used nowadays for various document processing tasks such as question answering and summarization. However, different LLMs come with different capabilities for different tasks as well as with different costs, tokenization, and latency. In fact, enterprises are already incurring huge costs of operating or using LLMs for their respective use cases. In this work, we propose optimizing the usage costs of LLMs by estimating their output quality (without actually invoking the LLMs), and then solving an optimization routine for the LLM selection to either keep costs under a budget, or minimize the costs, in a quality and latency aware manner. We propose a model to predict the output quality of LLMs on document processing tasks like summarization, followed by an LP rounding algorithm to optimize the selection of LLMs. We study optimization problems trading off the quality and costs, both theoretically and empirically. We further propose a sentence simplification model for reducing the number of tokens in a controlled manner. Additionally, we propose several deterministic heuristics for reducing tokens in a quality aware manner, and study the related optimization problem of applying the heuristics optimizing the quality and cost trade-off. We perform extensive empirical validation of our methods on not only enterprise datasets but also on open-source datasets, annotated by us, and show that we perform much better compared to closest baselines. Our methods reduce costs by 40%- 90% while improving quality by 4%-7%. We will release the annotated open source datasets to the community for further research and exploration.</li>
</ul>

<h3>Title: AOC-IDS: Autonomous Online Framework with Contrastive Learning for  Intrusion Detection</h3>
<ul>
<li><strong>Authors: </strong>Xinchen Zhang, Running Zhao, Zhihan Jiang, Zhicong Sun, Yulong Ding, Edith C.H. Ngai, Shuang-Hua Yang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01807">https://arxiv.org/abs/2402.01807</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01807">https://arxiv.org/pdf/2402.01807</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01807]] AOC-IDS: Autonomous Online Framework with Contrastive Learning for  Intrusion Detection(https://arxiv.org/abs/2402.01807)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>The rapid expansion of the Internet of Things (IoT) has raised increasing concern about targeted cyber attacks. Previous research primarily focused on static Intrusion Detection Systems (IDSs), which employ offline training to safeguard IoT systems. However, such static IDSs struggle with real-world scenarios where IoT system behaviors and attack strategies can undergo rapid evolution, necessitating dynamic and adaptable IDSs. In response to this challenge, we propose AOC-IDS, a novel online IDS that features an autonomous anomaly detection module (ADM) and a labor-free online framework for continual adaptation. In order to enhance data comprehension, the ADM employs an Autoencoder (AE) with a tailored Cluster Repelling Contrastive (CRC) loss function to generate distinctive representation from limited or incrementally incoming data in the online setting. Moreover, to reduce the burden of manual labeling, our online framework leverages pseudo-labels automatically generated from the decision-making process in the ADM to facilitate periodic updates of the ADM. The elimination of human intervention for labeling and decision-making boosts the system's compatibility and adaptability in the online setting to remain synchronized with dynamic environments. Experimental validation using the NSL-KDD and UNSW-NB15 datasets demonstrates the superior performance and adaptability of AOC-IDS, surpassing the state-of-the-art solutions. The code is released at https://github.com/xinchen930/AOC-IDS.</li>
</ul>

<h3>Title: Retrieval Augmented End-to-End Spoken Dialog Models</h3>
<ul>
<li><strong>Authors: </strong>Mingqiu Wang, Izhak Shafran, Hagen Soltau, Wei Han, Yuan Cao, Dian Yu, Laurent El Shafey</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01828">https://arxiv.org/abs/2402.01828</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01828">https://arxiv.org/pdf/2402.01828</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01828]] Retrieval Augmented End-to-End Spoken Dialog Models(https://arxiv.org/abs/2402.01828)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>We recently developed SLM, a joint speech and language model, which fuses a pretrained foundational speech model and a large language model (LLM), while preserving the in-context learning capability intrinsic to the pretrained LLM. In this paper, we apply SLM to speech dialog applications where the dialog states are inferred directly from the audio signal. Task-oriented dialogs often contain domain-specific entities, i.e., restaurants, hotels, train stations, and city names, which are difficult to recognize, however, critical for the downstream applications. Inspired by the RAG (retrieval-augmented generation) paradigm, we propose a retrieval augmented SLM (ReSLM) that overcomes this weakness. We first train a speech retriever to retrieve text entities mentioned in the audio. The retrieved entities are then added as text inputs to the underlying SLM to bias model predictions. We evaluated ReSLM on speech MultiWoz task (DSTC-11 challenge), and found that this retrieval augmentation boosts model performance, achieving joint goal accuracy (38.6% vs 32.7%), slot error rate (20.6% vs 24.8%) and ASR word error rate (5.5% vs 6.7%). While demonstrated on dialog state tracking, our approach is broadly applicable to other speech tasks requiring contextual information or domain-specific entities, such as contextual ASR with biasing capability.</li>
</ul>

<h3>Title: SynthCLIP: Are We Ready for a Fully Synthetic CLIP Training?</h3>
<ul>
<li><strong>Authors: </strong>Hasan Abed Al Kader Hammoud, Hani Itani, Fabio Pizzati, Philip Torr, Adel Bibi, Bernard Ghanem</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01832">https://arxiv.org/abs/2402.01832</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01832">https://arxiv.org/pdf/2402.01832</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01832]] SynthCLIP: Are We Ready for a Fully Synthetic CLIP Training?(https://arxiv.org/abs/2402.01832)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present SynthCLIP, a novel framework for training CLIP models with entirely synthetic text-image pairs, significantly departing from previous methods relying on real data. Leveraging recent text-to-image (TTI) generative networks and large language models (LLM), we are able to generate synthetic datasets of images and corresponding captions at any scale, with no human intervention. With training at scale, SynthCLIP achieves performance comparable to CLIP models trained on real datasets. We also introduce SynthCI-30M, a purely synthetic dataset comprising 30 million captioned images. Our code, trained models, and generated data are released at https://github.com/hammoudhasan/SynthCLIP</li>
</ul>

<h3>Title: Position Paper: Assessing Robustness, Privacy, and Fairness in Federated  Learning Integrated with Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Xi Li, Jiaqi Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01857">https://arxiv.org/abs/2402.01857</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01857">https://arxiv.org/pdf/2402.01857</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01857]] Position Paper: Assessing Robustness, Privacy, and Fairness in Federated  Learning Integrated with Foundation Models(https://arxiv.org/abs/2402.01857)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Federated Learning (FL), while a breakthrough in decentralized machine learning, contends with significant challenges such as limited data availability and the variability of computational resources, which can stifle the performance and scalability of the models. The integration of Foundation Models (FMs) into FL presents a compelling solution to these issues, with the potential to enhance data richness and reduce computational demands through pre-training and data augmentation. However, this incorporation introduces novel issues in terms of robustness, privacy, and fairness, which have not been sufficiently addressed in the existing research. We make a preliminary investigation into this field by systematically evaluating the implications of FM-FL integration across these dimensions. We analyze the trade-offs involved, uncover the threats and issues introduced by this integration, and propose a set of criteria and strategies for navigating these challenges. Furthermore, we identify potential research directions for advancing this field, laying a foundation for future development in creating reliable, secure, and equitable FL systems.</li>
</ul>

<h3>Title: Explaining latent representations of generative models with large  multimodal models</h3>
<ul>
<li><strong>Authors: </strong>Mengdan Zhu, Zhenke Liu, Bo Pan, Abhinav Angirekula, Liang Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01858">https://arxiv.org/abs/2402.01858</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01858">https://arxiv.org/pdf/2402.01858</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01858]] Explaining latent representations of generative models with large  multimodal models(https://arxiv.org/abs/2402.01858)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Learning interpretable representations of data generative latent factors is an important topic for the development of artificial intelligence. With the rise of the large multimodal model, it can align images with text to generate answers. In this work, we propose a framework to comprehensively explain each latent factor in the generative models using a large multimodal model. We further measure the uncertainty of our generated explanations, quantitatively evaluate the performance of explanation generation among multiple large multimodal models, and qualitatively visualize the variations of each latent factor to learn the disentanglement effects of different generative models on explanations. Finally, we discuss the explanatory capabilities and limitations of state-of-the-art large multimodal models.</li>
</ul>

<h3>Title: Parametric Feature Transfer: One-shot Federated Learning with Foundation  Models</h3>
<ul>
<li><strong>Authors: </strong>Mahdi Beitollahi, Alex Bie, Sobhan Hemati, Leo Maxime Brunswic, Xu Li, Xi Chen, Guojun Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01862">https://arxiv.org/abs/2402.01862</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01862">https://arxiv.org/pdf/2402.01862</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01862]] Parametric Feature Transfer: One-shot Federated Learning with Foundation  Models(https://arxiv.org/abs/2402.01862)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>In one-shot federated learning (FL), clients collaboratively train a global model in a single round of communication. Existing approaches for one-shot FL enhance communication efficiency at the expense of diminished accuracy. This paper introduces FedPFT (Federated Learning with Parametric Feature Transfer), a methodology that harnesses the transferability of foundation models to enhance both accuracy and communication efficiency in one-shot FL. The approach involves transferring per-client parametric models (specifically, Gaussian mixtures) of features extracted from foundation models. Subsequently, each parametric model is employed to generate synthetic features for training a classifier head. Experimental results on eight datasets demonstrate that FedPFT enhances the communication-accuracy frontier in both centralized and decentralized FL scenarios, as well as across diverse data-heterogeneity settings such as covariate shift and task shift, with improvements of up to 20.6%. Additionally, FedPFT adheres to the data minimization principle of FL, as clients do not send real features. We demonstrate that sending real features is vulnerable to potent reconstruction attacks. Moreover, we show that FedPFT is amenable to formal privacy guarantees via differential privacy, demonstrating favourable privacy-accuracy tradeoffs.</li>
</ul>

<h3>Title: EBV: Electronic Bee-Veterinarian for Principled Mining and Forecasting  of Honeybee Time Series</h3>
<ul>
<li><strong>Authors: </strong>Mst. Shamima Hossain, Christos Faloutsos, Boris Baer, Hyoseung Kim, Vassilis J. Tsotras</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01902">https://arxiv.org/abs/2402.01902</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01902">https://arxiv.org/pdf/2402.01902</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01902]] EBV: Electronic Bee-Veterinarian for Principled Mining and Forecasting  of Honeybee Time Series(https://arxiv.org/abs/2402.01902)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Honeybees are vital for pollination and food production. Among many factors, extreme temperature (e.g., due to climate change) is particularly dangerous for bee health. Anticipating such extremities would allow beekeepers to take early preventive action. Thus, given sensor (temperature) time series data from beehives, how can we find patterns and do forecasting? Forecasting is crucial as it helps spot unexpected behavior and thus issue warnings to the beekeepers. In that case, what are the right models for forecasting? ARIMA, RNNs, or something else? We propose the EBV (Electronic Bee-Veterinarian) method, which has the following desirable properties: (i) principled: it is based on a) diffusion equations from physics and b) control theory for feedback-loop controllers; (ii) effective: it works well on multiple, real-world time sequences, (iii) explainable: it needs only a handful of parameters (e.g., bee strength) that beekeepers can easily understand and trust, and (iv) scalable: it performs linearly in time. We applied our method to multiple real-world time sequences, and found that it yields accurate forecasting (up to 49% improvement in RMSE compared to baselines), and segmentation. Specifically, discontinuities detected by EBV mostly coincide with domain expert's opinions, showcasing our approach's potential and practical feasibility. Moreover, EBV is scalable and fast, taking about 20 minutes on a stock laptop for reconstructing two months of sensor data.</li>
</ul>

<h3>Title: On Catastrophic Inheritance of Large Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Hao Chen, Bhiksha Raj, Xing Xie, Jindong Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01909">https://arxiv.org/abs/2402.01909</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01909">https://arxiv.org/pdf/2402.01909</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01909]] On Catastrophic Inheritance of Large Foundation Models(https://arxiv.org/abs/2402.01909)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Large foundation models (LFMs) are claiming incredible performances. Yet great concerns have been raised about their mythic and uninterpreted potentials not only in machine learning, but also in various other disciplines. In this position paper, we propose to identify a neglected issue deeply rooted in LFMs: Catastrophic Inheritance, describing the weaknesses and limitations inherited from biased large-scale pre-training data to behaviors of LFMs on the downstream tasks, including samples that are corrupted, long-tailed, noisy, out-of-distributed, to name a few. Such inheritance can potentially cause catastrophes to downstream applications, such as bias, lack of generalization, deteriorated performance, security vulnerability, privacy leakage, and value misalignment. We discuss the challenges behind this issue and propose UIM, a framework to Understand the catastrophic inheritance of LFMs from both pre-training and downstream adaptation, Interpret the implications of catastrophic inheritance on downstream tasks, and how to Mitigate it. UIM aims to unite both the machine learning and social sciences communities for more responsible and promising AI development and deployment.</li>
</ul>

<h3>Title: Robust Inverse Graphics via Probabilistic Inference</h3>
<ul>
<li><strong>Authors: </strong>Tuan Anh Le, Pavel Sountsov, Matthew D. Hoffman, Ben Lee, Brian Patton, Rif A. Saurous</a></li>
<li><strong>Subjects: </strong>cs.CV, stat.CO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01915">https://arxiv.org/abs/2402.01915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01915">https://arxiv.org/pdf/2402.01915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01915]] Robust Inverse Graphics via Probabilistic Inference(https://arxiv.org/abs/2402.01915)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>How do we infer a 3D scene from a single image in the presence of corruptions like rain, snow or fog? Straightforward domain randomization relies on knowing the family of corruptions ahead of time. Here, we propose a Bayesian approach-dubbed robust inverse graphics (RIG)-that relies on a strong scene prior and an uninformative uniform corruption prior, making it applicable to a wide range of corruptions. Given a single image, RIG performs posterior inference jointly over the scene and the corruption. We demonstrate this idea by training a neural radiance field (NeRF) scene prior and using a secondary NeRF to represent the corruptions over which we place an uninformative prior. RIG, trained only on clean data, outperforms depth estimators and alternative NeRF approaches that perform point estimation instead of full inference. The results hold for a number of scene prior architectures based on normalizing flows and diffusion models. For the latter, we develop reconstruction-guidance with auxiliary latents (ReGAL)-a diffusion conditioning algorithm that is applicable in the presence of auxiliary latent variables such as the corruption. RIG demonstrates how scene priors can be used beyond generation tasks.</li>
</ul>

<h3>Title: Sample, estimate, aggregate: A recipe for causal discovery foundation  models</h3>
<ul>
<li><strong>Authors: </strong>Menghua Wu, Yujia Bao, Regina Barzilay, Tommi Jaakkola</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01929">https://arxiv.org/abs/2402.01929</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01929">https://arxiv.org/pdf/2402.01929</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01929]] Sample, estimate, aggregate: A recipe for causal discovery foundation  models(https://arxiv.org/abs/2402.01929)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Causal discovery, the task of inferring causal structure from data, promises to accelerate scientific research, inform policy making, and more. However, the per-dataset nature of existing causal discovery algorithms renders them slow, data hungry, and brittle. Inspired by foundation models, we propose a causal discovery framework where a deep learning model is pretrained to resolve predictions from classical discovery algorithms run over smaller subsets of variables. This method is enabled by the observations that the outputs from classical algorithms are fast to compute for small problems, informative of (marginal) data structure, and their structure outputs as objects remain comparable across datasets. Our method achieves state-of-the-art performance on synthetic and realistic datasets, generalizes to data generating mechanisms not seen during training, and offers inference speeds that are orders of magnitude faster than existing models.</li>
</ul>

<h3>Title: Analyzing Neural Network-Based Generative Diffusion Models through  Convex Optimization</h3>
<ul>
<li><strong>Authors: </strong>Fangzhao Zhang, Mert Pilanci</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.01965">https://arxiv.org/abs/2402.01965</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.01965">https://arxiv.org/pdf/2402.01965</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.01965]] Analyzing Neural Network-Based Generative Diffusion Models through  Convex Optimization(https://arxiv.org/abs/2402.01965)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models are becoming widely used in state-of-the-art image, video and audio generation. Score-based diffusion models stand out among these methods, necessitating the estimation of score function of the input data distribution. In this study, we present a theoretical framework to analyze two-layer neural network-based diffusion models by reframing score matching and denoising score matching as convex optimization. Though existing diffusion theory is mainly asymptotic, we characterize the exact predicted score function and establish the convergence result for neural network-based diffusion models with finite data. This work contributes to understanding what neural network-based diffusion model learns in non-asymptotic settings.</li>
</ul>

<h3>Title: GenFace: A Large-Scale Fine-Grained Face Forgery Benchmark and Cross  Appearance-Edge Learning</h3>
<ul>
<li><strong>Authors: </strong>Yaning Zhang, Zitong Yu, Xiaobin Huang, Linlin Shen, Jianfeng Ren</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02003">https://arxiv.org/abs/2402.02003</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02003">https://arxiv.org/pdf/2402.02003</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02003]] GenFace: A Large-Scale Fine-Grained Face Forgery Benchmark and Cross  Appearance-Edge Learning(https://arxiv.org/abs/2402.02003)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The rapid advancement of photorealistic generators has reached a critical juncture where the discrepancy between authentic and manipulated images is increasingly indistinguishable. Thus, benchmarking and advancing techniques detecting digital manipulation become an urgent issue. Although there have been a number of publicly available face forgery datasets, the forgery faces are mostly generated using GAN-based synthesis technology, which does not involve the most recent technologies like diffusion. The diversity and quality of images generated by diffusion models have been significantly improved and thus a much more challenging face forgery dataset shall be used to evaluate SOTA forgery detection literature. In this paper, we propose a large-scale, diverse, and fine-grained high-fidelity dataset, namely GenFace, to facilitate the advancement of deepfake detection, which contains a large number of forgery faces generated by advanced generators such as the diffusion-based model and more detailed labels about the manipulation approaches and adopted generators. In addition to evaluating SOTA approaches on our benchmark, we design an innovative cross appearance-edge learning (CAEL) detector to capture multi-grained appearance and edge global representations, and detect discriminative and general forgery traces. Moreover, we devise an appearance-edge cross-attention (AECA) module to explore the various integrations across two domains. Extensive experiment results and visualizations show that our detection model outperforms the state of the arts on different settings like cross-generator, cross-forgery, and cross-dataset evaluations. Code and datasets will be available at \url{https://github.com/Jenine-321/GenFace</li>
</ul>

<h3>Title: Understanding Time Series Anomaly State Detection through One-Class  Classification</h3>
<ul>
<li><strong>Authors: </strong>Hanxu Zhou, Yuan Zhang, Guangjie Leng, Ruofan Wang, Zhi-Qin John Xu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02007">https://arxiv.org/abs/2402.02007</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02007">https://arxiv.org/pdf/2402.02007</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02007]] Understanding Time Series Anomaly State Detection through One-Class  Classification(https://arxiv.org/abs/2402.02007)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>For a long time, research on time series anomaly detection has mainly focused on finding outliers within a given time series. Admittedly, this is consistent with some practical problems, but in other practical application scenarios, people are concerned about: assuming a standard time series is given, how to judge whether another test time series deviates from the standard time series, which is more similar to the problem discussed in one-class classification (OCC). Therefore, in this article, we try to re-understand and define the time series anomaly detection problem through OCC, which we call 'time series anomaly state detection problem'. We first use stochastic processes and hypothesis testing to strictly define the 'time series anomaly state detection problem', and its corresponding anomalies. Then, we use the time series classification dataset to construct an artificial dataset corresponding to the problem. We compile 38 anomaly detection algorithms and correct some of the algorithms to adapt to handle this problem. Finally, through a large number of experiments, we fairly compare the actual performance of various time series anomaly detection algorithms, providing insights and directions for future research by researchers.</li>
</ul>

<h3>Title: Self-Supervised Contrastive Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Junwoo Park, Daehoon Gwak, Jaegul Choo, Edward Choi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02023">https://arxiv.org/abs/2402.02023</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02023">https://arxiv.org/pdf/2402.02023</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02023]] Self-Supervised Contrastive Forecasting(https://arxiv.org/abs/2402.02023)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Long-term forecasting presents unique challenges due to the time and memory complexity of handling long sequences. Existing methods, which rely on sliding windows to process long sequences, struggle to effectively capture long-term variations that are partially caught within the short window (i.e., outer-window variations). In this paper, we introduce a novel approach that overcomes this limitation by employing contrastive learning and enhanced decomposition architecture, specifically designed to focus on long-term variations. To this end, our contrastive loss incorporates global autocorrelation held in the whole time series, which facilitates the construction of positive and negative pairs in a self-supervised manner. When combined with our decomposition networks, our contrastive learning significantly improves long-term forecasting performance. Extensive experiments demonstrate that our approach outperforms 14 baseline models in multiple experiments over nine long-term benchmarks, especially in challenging scenarios that require a significantly long output for forecasting. Source code is available at https://github.com/junwoopark92/Self-Supervised-Contrastive-Forecsating.</li>
</ul>

<h3>Title: DiffVein: A Unified Diffusion Network for Finger Vein Segmentation and  Authentication</h3>
<ul>
<li><strong>Authors: </strong>Yanjun Liu, Wenming Yang, Qingmin Liao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02060">https://arxiv.org/abs/2402.02060</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02060">https://arxiv.org/pdf/2402.02060</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02060]] DiffVein: A Unified Diffusion Network for Finger Vein Segmentation and  Authentication(https://arxiv.org/abs/2402.02060)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Finger vein authentication, recognized for its high security and specificity, has become a focal point in biometric research. Traditional methods predominantly concentrate on vein feature extraction for discriminative modeling, with a limited exploration of generative approaches. Suffering from verification failure, existing methods often fail to obtain authentic vein patterns by segmentation. To fill this gap, we introduce DiffVein, a unified diffusion model-based framework which simultaneously addresses vein segmentation and authentication tasks. DiffVein is composed of two dedicated branches: one for segmentation and the other for denoising. For better feature interaction between these two branches, we introduce two specialized modules to improve their collective performance. The first, a mask condition module, incorporates the semantic information of vein patterns from the segmentation branch into the denoising process. Additionally, we also propose a Semantic Difference Transformer (SD-Former), which employs Fourier-space self-attention and cross-attention modules to extract category embedding before feeding it to the segmentation task. In this way, our framework allows for a dynamic interplay between diffusion and segmentation embeddings, thus vein segmentation and authentication tasks can inform and enhance each other in the joint training. To further optimize our model, we introduce a Fourier-space Structural Similarity (FourierSIM) loss function, which is tailored to improve the denoising network's learning efficacy. Extensive experiments on the USM and THU-MVFV3V datasets substantiates DiffVein's superior performance, setting new benchmarks in both vein segmentation and authentication tasks.</li>
</ul>

<h3>Title: Risk-Sensitive Diffusion: Learning the Underlying Distribution from  Noisy Samples</h3>
<ul>
<li><strong>Authors: </strong>Yangming Li, Max Ruiz Luyten, Mihaela van der Schaar</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02081">https://arxiv.org/abs/2402.02081</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02081">https://arxiv.org/pdf/2402.02081</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02081]] Risk-Sensitive Diffusion: Learning the Underlying Distribution from  Noisy Samples(https://arxiv.org/abs/2402.02081)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>While achieving remarkable performances, we show that diffusion models are fragile to the presence of noisy samples, limiting their potential in the vast amount of settings where, unlike image synthesis, we are not blessed with clean data. Motivated by our finding that such fragility originates from the distribution gaps between noisy and clean samples along the diffusion process, we introduce risk-sensitive SDE, a stochastic differential equation that is parameterized by the risk (i.e., data "dirtiness") to adjust the distributions of noisy samples, reducing misguidance while benefiting from their contained information. The optimal expression for risk-sensitive SDE depends on the specific noise distribution, and we derive its parameterizations that minimize the misguidance of noisy samples for both Gaussian and general non-Gaussian perturbations. We conduct extensive experiments on both synthetic and real-world datasets (e.g., medical time series), showing that our model effectively recovers the clean data distribution from noisy samples, significantly outperforming conditional generation baselines.</li>
</ul>

<h3>Title: DCS-Net: Pioneering Leakage-Free Point Cloud Pretraining Framework with  Global Insights</h3>
<ul>
<li><strong>Authors: </strong>Zhe Li, Zhangyang Gao, Cheng Tan, Stan Z. Li, Laurence T. Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02088">https://arxiv.org/abs/2402.02088</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02088">https://arxiv.org/pdf/2402.02088</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02088]] DCS-Net: Pioneering Leakage-Free Point Cloud Pretraining Framework with  Global Insights(https://arxiv.org/abs/2402.02088)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Masked autoencoding and generative pretraining have achieved remarkable success in computer vision and natural language processing, and more recently, they have been extended to the point cloud domain. Nevertheless, existing point cloud models suffer from the issue of information leakage due to the pre-sampling of center points, which leads to trivial proxy tasks for the models. These approaches primarily focus on local feature reconstruction, limiting their ability to capture global patterns within point clouds. In this paper, we argue that the reduced difficulty of pretext tasks hampers the model's capacity to learn expressive representations. To address these limitations, we introduce a novel solution called the Differentiable Center Sampling Network (DCS-Net). It tackles the information leakage problem by incorporating both global feature reconstruction and local feature reconstruction as non-trivial proxy tasks, enabling simultaneous learning of both the global and local patterns within point cloud. Experimental results demonstrate that our method enhances the expressive capacity of existing point cloud models and effectively addresses the issue of information leakage.</li>
</ul>

<h3>Title: Recent Advances in Digital Image and Video Forensics, Anti-forensics and  Counter Anti-forensics</h3>
<ul>
<li><strong>Authors: </strong>Maryam Al-Fehani, Saif Al-Kuwari</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02089">https://arxiv.org/abs/2402.02089</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02089">https://arxiv.org/pdf/2402.02089</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02089]] Recent Advances in Digital Image and Video Forensics, Anti-forensics and  Counter Anti-forensics(https://arxiv.org/abs/2402.02089)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Image and video forensics have recently gained increasing attention due to the proliferation of manipulated images and videos, especially on social media platforms, such as Twitter and Instagram, which spread disinformation and fake news. This survey explores image and video identification and forgery detection covering both manipulated digital media and generative media. However, media forgery detection techniques are susceptible to anti-forensics; on the other hand, such anti-forensics techniques can themselves be detected. We therefore further cover both anti-forensics and counter anti-forensics techniques in image and video. Finally, we conclude this survey by highlighting some open problems in this domain.</li>
</ul>

<h3>Title: From Synthetic to Real: Unveiling the Power of Synthetic Data for Video  Person Re-ID</h3>
<ul>
<li><strong>Authors: </strong>Xiangqun Zhang, Ruize Han, Wei Feng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02108">https://arxiv.org/abs/2402.02108</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02108">https://arxiv.org/pdf/2402.02108</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02108]] From Synthetic to Real: Unveiling the Power of Synthetic Data for Video  Person Re-ID(https://arxiv.org/abs/2402.02108)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>In this paper, we study a new problem of cross-domain video based person re-identification (Re-ID). Specifically, we take the synthetic video dataset as the source domain for training and use the real-world videos for testing, which significantly reduces the dependence on real training data collection and annotation. To unveil the power of synthetic data for video person Re-ID, we first propose a self-supervised domain invariant feature learning strategy for both static and temporal features. Then, to further improve the person identification ability in the target domain, we develop a mean-teacher scheme with the self-supervised ID consistency loss. Experimental results on four real datasets verify the rationality of cross-synthetic-real domain adaption and the effectiveness of our method. We are also surprised to find that the synthetic data performs even better than the real data in the cross-domain setting.</li>
</ul>

<h3>Title: Enhancing crop classification accuracy by synthetic SAR-Optical data  generation using deep learning</h3>
<ul>
<li><strong>Authors: </strong>Ali Mirzaei, Hossein Bagheri, Iman Khosravi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02121">https://arxiv.org/abs/2402.02121</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02121">https://arxiv.org/pdf/2402.02121</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02121]] Enhancing crop classification accuracy by synthetic SAR-Optical data  generation using deep learning(https://arxiv.org/abs/2402.02121)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Crop classification using remote sensing data has emerged as a prominent research area in recent decades. Studies have demonstrated that fusing SAR and optical images can significantly enhance the accuracy of classification. However, a major challenge in this field is the limited availability of training data, which adversely affects the performance of classifiers. In agricultural regions, the dominant crops typically consist of one or two specific types, while other crops are scarce. Consequently, when collecting training samples to create a map of agricultural products, there is an abundance of samples from the dominant crops, forming the majority classes. Conversely, samples from other crops are scarce, representing the minority classes. Addressing this issue requires overcoming several challenges and weaknesses associated with traditional data generation methods. These methods have been employed to tackle the imbalanced nature of the training data. Nevertheless, they still face limitations in effectively handling the minority classes. Overall, the issue of inadequate training data, particularly for minority classes, remains a hurdle that traditional methods struggle to overcome. In this research, We explore the effectiveness of conditional tabular generative adversarial network (CTGAN) as a synthetic data generation method based on a deep learning network, in addressing the challenge of limited training data for minority classes in crop classification using the fusion of SAR-optical data. Our findings demonstrate that the proposed method generates synthetic data with higher quality that can significantly increase the number of samples for minority classes leading to better performance of crop classifiers.</li>
</ul>

<h3>Title: Generative Visual Compression: A Review</h3>
<ul>
<li><strong>Authors: </strong>Bolin Chen, Shanzhi Yin, Peilin Chen, Shiqi Wang, Yan Ye</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02140">https://arxiv.org/abs/2402.02140</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02140">https://arxiv.org/pdf/2402.02140</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02140]] Generative Visual Compression: A Review(https://arxiv.org/abs/2402.02140)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Artificial Intelligence Generated Content (AIGC) is leading a new technical revolution for the acquisition of digital content and impelling the progress of visual compression towards competitive performance gains and diverse functionalities over traditional codecs. This paper provides a thorough review on the recent advances of generative visual compression, illustrating great potentials and promising applications in ultra-low bitrate communication, user-specified reconstruction/filtering, and intelligent machine analysis. In particular, we review the visual data compression methodologies with deep generative models, and summarize how compact representation and high-fidelity reconstruction could be actualized via generative techniques. In addition, we generalize related generative compression technologies for machine vision and intelligent analytics. Finally, we discuss the fundamental challenges on generative visual compression techniques and envision their future research directions.</li>
</ul>

<h3>Title: Improving Diffusion Models for Inverse Problems Using Optimal Posterior  Covariance</h3>
<ul>
<li><strong>Authors: </strong>Xinyu Peng, Ziyang Zheng, Wenrui Dai, Nuoqian Xiao, Chenglin Li, Junni Zou, Hongkai Xiong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02149">https://arxiv.org/abs/2402.02149</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02149">https://arxiv.org/pdf/2402.02149</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02149]] Improving Diffusion Models for Inverse Problems Using Optimal Posterior  Covariance(https://arxiv.org/abs/2402.02149)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent diffusion models provide a promising zero-shot solution to noisy linear inverse problems without retraining for specific inverse problems. In this paper, we propose the first unified interpretation for existing zero-shot methods from the perspective of approximating the conditional posterior mean for the reverse diffusion process of conditional sampling. We reveal that recent methods are equivalent to making isotropic Gaussian approximations to intractable posterior distributions over clean images given diffused noisy images, with the only difference in the handcrafted design of isotropic posterior covariances. Inspired by this finding, we propose a general plug-and-play posterior covariance optimization based on maximum likelihood estimation to improve recent methods. To achieve optimal posterior covariance without retraining, we provide general solutions based on two approaches specifically designed to leverage pre-trained models with and without reverse covariances. Experimental results demonstrate that the proposed methods significantly enhance the overall performance or robustness to hyperparameters of recent methods. Code is available at https://github.com/xypeng9903/k-diffusion-inverse-problems</li>
</ul>

<h3>Title: Data Poisoning for In-context Learning</h3>
<ul>
<li><strong>Authors: </strong>Pengfei He, Han Xu, Yue Xing, Hui Liu, Makoto Yamada, Jiliang Tang</a></li>
<li><strong>Subjects: </strong>cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02160">https://arxiv.org/abs/2402.02160</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02160">https://arxiv.org/pdf/2402.02160</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02160]] Data Poisoning for In-context Learning(https://arxiv.org/abs/2402.02160)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>In the domain of large language models (LLMs), in-context learning (ICL) has been recognized for its innovative ability to adapt to new tasks, relying on examples rather than retraining or fine-tuning. This paper delves into the critical issue of ICL's susceptibility to data poisoning attacks, an area not yet fully explored. We wonder whether ICL is vulnerable, with adversaries capable of manipulating example data to degrade model performance. To address this, we introduce ICLPoison, a specialized attacking framework conceived to exploit the learning mechanisms of ICL. Our approach uniquely employs discrete text perturbations to strategically influence the hidden states of LLMs during the ICL process. We outline three representative strategies to implement attacks under our framework, each rigorously evaluated across a variety of models and tasks. Our comprehensive tests, including trials on the sophisticated GPT-4 model, demonstrate that ICL's performance is significantly compromised under our framework. These revelations indicate an urgent need for enhanced defense mechanisms to safeguard the integrity and reliability of LLMs in applications relying on in-context learning.</li>
</ul>

<h3>Title: Evolution Guided Generative Flow Networks</h3>
<ul>
<li><strong>Authors: </strong>Zarif Ikram, Ling Pan, Dianbo Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02186">https://arxiv.org/abs/2402.02186</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02186">https://arxiv.org/pdf/2402.02186</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02186]] Evolution Guided Generative Flow Networks(https://arxiv.org/abs/2402.02186)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative Flow Networks (GFlowNets) are a family of probabilistic generative models that learn to sample compositional objects proportional to their rewards. One big challenge of GFlowNets is training them effectively when dealing with long time horizons and sparse rewards. To address this, we propose Evolution guided generative flow networks (EGFN), a simple but powerful augmentation to the GFlowNets training using Evolutionary algorithms (EA). Our method can work on top of any GFlowNets training objective, by training a set of agent parameters using EA, storing the resulting trajectories in the prioritized replay buffer, and training the GFlowNets agent using the stored trajectories. We present a thorough investigation over a wide range of toy and real-world benchmark tasks showing the effectiveness of our method in handling long trajectories and sparse rewards.</li>
</ul>

<h3>Title: On the Exploitation of DCT-Traces in the Generative-AI Domain</h3>
<ul>
<li><strong>Authors: </strong>Orazio Pontorno (1), Luca Guarnera (1), Sebastiano Battiato (1) ((1) University of Catania)</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02209">https://arxiv.org/abs/2402.02209</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02209">https://arxiv.org/pdf/2402.02209</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02209]] On the Exploitation of DCT-Traces in the Generative-AI Domain(https://arxiv.org/abs/2402.02209)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Since their appearance, Deepfakes represent one of the toughest challenges in the world of Cybersecurity and Digital Forensics. In recent years, researchers have discovered that generative models leave unique traces in synthetic data that, if analyzed and identified in detail, can be exploited to improve the generalization limitations of existing deepfake detectors. To capture this evidence, in this paper we analyzed deepfake images in the frequency domain, examining in detail the beta-AC coefficients of the Discrete Cosine Transform (DCT). Recognizing that not all coefficients contribute equally to image recognition, we hypothesize the existence of a unique "discriminative fingerprint" for each type of image, embedded in specific combinations of coefficients. To identify them, Machine Learning classifiers were trained on various combinations of coefficients. The integration of the Explainable AI (XAI) LIME algorithm combined with a neural classifier to explore alternative combinations of coefficients provides a deeper insight into the discriminative features of synthetic images. Experimental results reveal the significant potential of using a specific combination of beta-AC coefficients in order to improve the analysis of traces left by generative models.</li>
</ul>

<h3>Title: A Data Generation Perspective to the Mechanism of In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Haitao Mao, Guangliang Liu, Yao Ma, Rongrong Wang, Jiliang Tang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02212">https://arxiv.org/abs/2402.02212</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02212">https://arxiv.org/pdf/2402.02212</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02212]] A Data Generation Perspective to the Mechanism of In-Context Learning(https://arxiv.org/abs/2402.02212)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>In-Context Learning (ICL) empowers Large Language Models (LLMs) with the capacity to learn in context, achieving downstream generalization without gradient updates but with a few in-context examples. Despite the encouraging empirical success, the underlying mechanism of ICL remains unclear, and existing research offers various viewpoints of understanding. These studies propose intuition-driven and ad-hoc technical solutions for interpreting ICL, illustrating an ambiguous road map. In this paper, we leverage a data generation perspective to reinterpret recent efforts and demonstrate the potential broader usage of popular technical solutions, approaching a systematic angle. For a conceptual definition, we rigorously adopt the terms of skill learning and skill recognition. The difference between them is skill learning can learn new data generation functions from in-context data. We also provide a comprehensive study on the merits and weaknesses of different solutions, and highlight the uniformity among them given the perspective of data generation, establishing a technical foundation for future research to incorporate the strengths of different lines of research.</li>
</ul>

<h3>Title: Graph Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Haitao Mao, Zhikai Chen, Wenzhuo Tang, Jianan Zhao, Yao Ma, Tong Zhao, Neil Shah, Michael Galkin, Jiliang Tang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02216">https://arxiv.org/abs/2402.02216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02216">https://arxiv.org/pdf/2402.02216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02216]] Graph Foundation Models(https://arxiv.org/abs/2402.02216)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Graph Foundation Model (GFM) is a new trending research topic in the graph domain, aiming to develop a graph model capable of generalizing across different graphs and tasks. However, a versatile GFM has not yet been achieved. The key challenge in building GFM is how to enable positive transfer across graphs with diverse structural patterns. Inspired by the existing foundation models in the CV and NLP domains, we propose a novel perspective for the GFM development by advocating for a ``graph vocabulary'', in which the basic transferable units underlying graphs encode the invariance on graphs. We ground the graph vocabulary construction from essential aspects including network analysis, theoretical foundations, and stability. Such a vocabulary perspective can potentially advance the future GFM design following the neural scaling laws.</li>
</ul>

<h3>Title: Revisiting Generative Adversarial Networks for Binary Semantic  Segmentation on Imbalanced Datasets</h3>
<ul>
<li><strong>Authors: </strong>Lei Xu, Moncef Gabbouj</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02245">https://arxiv.org/abs/2402.02245</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02245">https://arxiv.org/pdf/2402.02245</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02245]] Revisiting Generative Adversarial Networks for Binary Semantic  Segmentation on Imbalanced Datasets(https://arxiv.org/abs/2402.02245)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Anomalous pavement surface conditions detection aims to detect pixels representing anomalous states, such as cracks, on pavement surface images automatically by algorithms. Recently, deep learning models have been intensively applied to related topics with outstanding performance. However, most existing deep learning-related solutions rarely achieve a stable performance on diverse datasets. To address this issue, in this work, we propose a deep learning framework based on conditional Generative Adversarial Networks for anomalous region detection on pavement images at the pixel level. In particular, the proposed framework is developed to enhance the generator's ability to estimate the probability feature map from heterogeneous inputs with two training stages and multiscale feature representation. Moreover, several attention mechanisms are incorporated into the proposed framework to mitigate the performance deterioration of model training on severely imbalanced datasets. We implement experiments on six accessible pavement datasets. Extensive qualitative and quantitative experiments demonstrate that the proposed framework can achieve SOTA results on these datasets efficiently and robustly.</li>
</ul>

<h3>Title: SudokuSens: Enhancing Deep Learning Robustness for IoT Sensing  Applications using a Generative Approach</h3>
<ul>
<li><strong>Authors: </strong>Tianshi Wang, Jinyang Li, Ruijie Wang, Denizhan Kara, Shengzhong Liu, Davis Wertheimer, Antoni Viros-i-Martin, Raghu Ganti, Mudhakar Srivatsa, Tarek Abdelzaher</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02275">https://arxiv.org/abs/2402.02275</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02275">https://arxiv.org/pdf/2402.02275</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02275]] SudokuSens: Enhancing Deep Learning Robustness for IoT Sensing  Applications using a Generative Approach(https://arxiv.org/abs/2402.02275)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This paper introduces SudokuSens, a generative framework for automated generation of training data in machine-learning-based Internet-of-Things (IoT) applications, such that the generated synthetic data mimic experimental configurations not encountered during actual sensor data collection. The framework improves the robustness of resulting deep learning models, and is intended for IoT applications where data collection is expensive. The work is motivated by the fact that IoT time-series data entangle the signatures of observed objects with the confounding intrinsic properties of the surrounding environment and the dynamic environmental disturbances experienced. To incorporate sufficient diversity into the IoT training data, one therefore needs to consider a combinatorial explosion of training cases that are multiplicative in the number of objects considered and the possible environmental conditions in which such objects may be encountered. Our framework substantially reduces these multiplicative training needs. To decouple object signatures from environmental conditions, we employ a Conditional Variational Autoencoder (CVAE) that allows us to reduce data collection needs from multiplicative to (nearly) linear, while synthetically generating (data for) the missing conditions. To obtain robustness with respect to dynamic disturbances, a session-aware temporal contrastive learning approach is taken. Integrating the aforementioned two approaches, SudokuSens significantly improves the robustness of deep learning for IoT applications. We explore the degree to which SudokuSens benefits downstream inference tasks in different data sets and discuss conditions under which the approach is particularly effective.</li>
</ul>

<h3>Title: SynthDST: Synthetic Data is All You Need for Few-Shot Dialog State  Tracking</h3>
<ul>
<li><strong>Authors: </strong>Atharva Kulkarni, Bo-Hsiang Tseng, Joel Ruben Antony Moniz, Dhivya Piraviperumal, Hong Yu, Shruti Bhargava</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02285">https://arxiv.org/abs/2402.02285</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02285">https://arxiv.org/pdf/2402.02285</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02285]] SynthDST: Synthetic Data is All You Need for Few-Shot Dialog State  Tracking(https://arxiv.org/abs/2402.02285)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>In-context learning with Large Language Models (LLMs) has emerged as a promising avenue of research in Dialog State Tracking (DST). However, the best-performing in-context learning methods involve retrieving and adding similar examples to the prompt, requiring access to labeled training data. Procuring such training data for a wide range of domains and applications is time-consuming, expensive, and, at times, infeasible. While zero-shot learning requires no training data, it significantly lags behind the few-shot setup. Thus, `\textit{Can we efficiently generate synthetic data for any dialogue schema to enable few-shot prompting?}' Addressing this question, we propose \method, a data generation framework tailored for DST, utilizing LLMs. Our approach only requires the dialogue schema and a few hand-crafted dialogue templates to synthesize natural, coherent, and free-flowing dialogues with DST annotations. Few-shot learning using data from {\method} results in $4-5%$ improvement in Joint Goal Accuracy over the zero-shot baseline on MultiWOZ 2.1 and 2.4. Remarkably, our few-shot learning approach recovers nearly $98%$ of the performance compared to the few-shot setup using human-annotated training data. Our synthetic data and code can be accessed at https://github.com/apple/ml-synthdst</li>
</ul>

<h3>Title: Your Diffusion Model is Secretly a Certifiably Robust Classifier</h3>
<ul>
<li><strong>Authors: </strong>Huanran Chen, Yinpeng Dong, Shitong Shao, Zhongkai Hao, Xiao Yang, Hang Su, Jun Zhu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02316">https://arxiv.org/abs/2402.02316</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02316">https://arxiv.org/pdf/2402.02316</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02316]] Your Diffusion Model is Secretly a Certifiably Robust Classifier(https://arxiv.org/abs/2402.02316)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models are recently employed as generative classifiers for robust classification. However, a comprehensive theoretical understanding of the robustness of diffusion classifiers is still lacking, leading us to question whether they will be vulnerable to future stronger attacks. In this study, we propose a new family of diffusion classifiers, named Noised Diffusion Classifiers~(NDCs), that possess state-of-the-art certified robustness. Specifically, we generalize the diffusion classifiers to classify Gaussian-corrupted data by deriving the evidence lower bounds (ELBOs) for these distributions, approximating the likelihood using the ELBO, and calculating classification probabilities via Bayes' theorem. We integrate these generalized diffusion classifiers with randomized smoothing to construct smoothed classifiers possessing non-constant Lipschitzness. Experimental results demonstrate the superior certified robustness of our proposed NDCs. Notably, we are the first to achieve 80\%+ and 70\%+ certified robustness on CIFAR-10 under adversarial perturbations with $\ell_2$ norm less than 0.25 and 0.5, respectively, using a single off-the-shelf diffusion model without any additional data.</li>
</ul>

<h3>Title: Copyright Protection in Generative AI: A Technical Perspective</h3>
<ul>
<li><strong>Authors: </strong>Jie Ren, Han Xu, Pengfei He, Yingqian Cui, Shenglai Zeng, Jiankun Zhang, Hongzhi Wen, Jiayuan Ding, Hui Liu, Yi Chang, Jiliang Tang</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02333">https://arxiv.org/abs/2402.02333</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02333">https://arxiv.org/pdf/2402.02333</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02333]] Copyright Protection in Generative AI: A Technical Perspective(https://arxiv.org/abs/2402.02333)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative AI has witnessed rapid advancement in recent years, expanding their capabilities to create synthesized content such as text, images, audio, and code. The high fidelity and authenticity of contents generated by these Deep Generative Models (DGMs) have sparked significant copyright concerns. There have been various legal debates on how to effectively safeguard copyrights in DGMs. This work delves into this issue by providing a comprehensive overview of copyright protection from a technical perspective. We examine from two distinct viewpoints: the copyrights pertaining to the source data held by the data owners and those of the generative models maintained by the model builders. For data copyright, we delve into methods data owners can protect their content and DGMs can be utilized without infringing upon these rights. For model copyright, our discussion extends to strategies for preventing model theft and identifying outputs generated by specific models. Finally, we highlight the limitations of existing techniques and identify areas that remain unexplored. Furthermore, we discuss prospective directions for the future of copyright protection, underscoring its importance for the sustainable and ethical development of Generative AI.</li>
</ul>

<h3>Title: Stereographic Spherical Sliced Wasserstein Distances</h3>
<ul>
<li><strong>Authors: </strong>Huy Tran, Yikun Bai, Abihith Kothapalli, Ashkan Shahbazi, Xinran Liu, Rocio Diaz Martin, Soheil Kolouri</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02345">https://arxiv.org/abs/2402.02345</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02345">https://arxiv.org/pdf/2402.02345</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02345]] Stereographic Spherical Sliced Wasserstein Distances(https://arxiv.org/abs/2402.02345)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Comparing spherical probability distributions is of great interest in various fields, including geology, medical domains, computer vision, and deep representation learning. The utility of optimal transport-based distances, such as the Wasserstein distance, for comparing probability measures has spurred active research in developing computationally efficient variations of these distances for spherical probability measures. This paper introduces a high-speed and highly parallelizable distance for comparing spherical measures using the stereographic projection and the generalized Radon transform, which we refer to as the Stereographic Spherical Sliced Wasserstein (S3W) distance. We carefully address the distance distortion caused by the stereographic projection and provide an extensive theoretical analysis of our proposed metric and its rotationally invariant variation. Finally, we evaluate the performance of the proposed metrics and compare them with recent baselines in terms of both speed and accuracy through a wide range of numerical studies, including gradient flows and self-supervised learning.</li>
</ul>

<h3>Title: Closed-Loop Unsupervised Representation Disentanglement with $β$-VAE  Distillation and Diffusion Probabilistic Feedback</h3>
<ul>
<li><strong>Authors: </strong>Xin Jin, Bohan Li, BAAO Xie, Wenyao Zhang, Jinming Liu, Ziqiang Li, Tao Yang, Wenjun Zeng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02346">https://arxiv.org/abs/2402.02346</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02346">https://arxiv.org/pdf/2402.02346</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02346]] Closed-Loop Unsupervised Representation Disentanglement with $β$-VAE  Distillation and Diffusion Probabilistic Feedback(https://arxiv.org/abs/2402.02346)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, self-supervised</a></li>
<li><strong>Abstract: </strong>Representation disentanglement may help AI fundamentally understand the real world and thus benefit both discrimination and generation tasks. It currently has at least three unresolved core issues: (i) heavy reliance on label annotation and synthetic data -- causing poor generalization on natural scenarios; (ii) heuristic/hand-craft disentangling constraints make it hard to adaptively achieve an optimal training trade-off; (iii) lacking reasonable evaluation metric, especially for the real label-free data. To address these challenges, we propose a \textbf{C}losed-\textbf{L}oop unsupervised representation \textbf{Dis}entanglement approach dubbed \textbf{CL-Dis}. Specifically, we use diffusion-based autoencoder (Diff-AE) as a backbone while resorting to $\beta$-VAE as a co-pilot to extract semantically disentangled representations. The strong generation ability of diffusion model and the good disentanglement ability of VAE model are complementary. To strengthen disentangling, VAE-latent distillation and diffusion-wise feedback are interconnected in a closed-loop system for a further mutual promotion. Then, a self-supervised \textbf{Navigation} strategy is introduced to identify interpretable semantic directions in the disentangled latent space. Finally, a new metric based on content tracking is designed to evaluate the disentanglement effect. Experiments demonstrate the superiority of CL-Dis on applications like real image manipulation and visual analysis.</li>
</ul>

<h3>Title: Riemannian Preconditioned LoRA for Fine-Tuning Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Fangzhao Zhang, Mert Pilanci</a></li>
<li><strong>Subjects: </strong>cs.LG, math.OC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02347">https://arxiv.org/abs/2402.02347</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02347">https://arxiv.org/pdf/2402.02347</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02347]] Riemannian Preconditioned LoRA for Fine-Tuning Foundation Models(https://arxiv.org/abs/2402.02347)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, foundation model</a></li>
<li><strong>Abstract: </strong>In this work we study the enhancement of Low Rank Adaptation (LoRA) fine-tuning procedure by introducing a Riemannian preconditioner in its optimization step. Specifically, we introduce an $r\times r$ preconditioner in each gradient step where $r$ is the LoRA rank. This preconditioner requires a small change to existing optimizer code and creates virtually minuscule storage and runtime overhead. Our experimental results with both large language models and text-to-image diffusion models show that with our preconditioner, the convergence and reliability of SGD and AdamW can be significantly enhanced. Moreover, the training process becomes much more robust to hyperparameter choices such as learning rate. Theoretically, we show that fine-tuning a two-layer ReLU network in the convex paramaterization with our preconditioner has convergence rate independent of condition number of the data matrix. This new Riemannian preconditioner, previously explored in classic low-rank matrix recovery, is introduced to deep learning tasks for the first time in our work. We release our code at https://github.com/pilancilab/Riemannian_Preconditioned_LoRA.</li>
</ul>

<h3>Title: The Developmental Landscape of In-Context Learning</h3>
<ul>
<li><strong>Authors: </strong>Jesse Hoogland, George Wang, Matthew Farrugia-Roberts, Liam Carroll, Susan Wei, Daniel Murfet</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02364">https://arxiv.org/abs/2402.02364</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02364">https://arxiv.org/pdf/2402.02364</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02364]] The Developmental Landscape of In-Context Learning(https://arxiv.org/abs/2402.02364)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>We show that in-context learning emerges in transformers in discrete developmental stages, when they are trained on either language modeling or linear regression tasks. We introduce two methods for detecting the milestones that separate these stages, by probing the geometry of the population loss in both parameter space and function space. We study the stages revealed by these new methods using a range of behavioral and structural metrics to establish their validity.</li>
</ul>

<h3>Title: Exploring Intrinsic Properties of Medical Images for Self-Supervised  Binary Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Pranav Singh, Jacopo Cirrone</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02367">https://arxiv.org/abs/2402.02367</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02367">https://arxiv.org/pdf/2402.02367</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02367]] Exploring Intrinsic Properties of Medical Images for Self-Supervised  Binary Semantic Segmentation(https://arxiv.org/abs/2402.02367)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Recent advancements in self-supervised learning have unlocked the potential to harness unlabeled data for auxiliary tasks, facilitating the learning of beneficial priors. This has been particularly advantageous in fields like medical image analysis, where labeled data are scarce. Although effective for classification tasks, this methodology has shown limitations in more complex applications, such as medical image segmentation. In this paper, we introduce Medical imaging Enhanced with Dynamic Self-Adaptive Semantic Segmentation (MedSASS), a dedicated self-supervised framework tailored for medical image segmentation. We evaluate MedSASS against existing state- of-the-art methods across four diverse medical datasets, showcasing its superiority. MedSASS outperforms existing CNN-based self-supervised methods by 3.83% and matches the performance of ViT-based methods. Furthermore, when MedSASS is trained end-to-end, covering both encoder and decoder, it demonstrates significant improvements of 14.4% for CNNs and 6% for ViT-based architectures compared to existing state-of-the-art self-supervised strategies.</li>
</ul>

<h3>Title: Timer: Transformers for Time Series Analysis at Scale</h3>
<ul>
<li><strong>Authors: </strong>Yong Liu, Haoran Zhang, Chenyu Li, Xiangdong Huang, Jianmin Wang, Mingsheng Long</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02368">https://arxiv.org/abs/2402.02368</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02368">https://arxiv.org/pdf/2402.02368</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02368]] Timer: Transformers for Time Series Analysis at Scale(https://arxiv.org/abs/2402.02368)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, anomaly</a></li>
<li><strong>Abstract: </strong>Deep learning has contributed remarkably to the advancement of time series analysis. Still, deep models can encounter performance bottlenecks in real-world small-sample scenarios, which can be concealed due to the performance saturation with small models on current benchmarks. Meanwhile, large models have demonstrated great powers in these scenarios through large-scale pre-training. Continuous progresses have been achieved as the emergence of large language models, exhibiting unprecedented ability in few-shot generalization, scalability, and task generality, which is however absent in time series models. To change the current practices of training small models on specific datasets from scratch, this paper aims at an early development of large time series models (LTSM). During pre-training, we curate large-scale datasets with up to 1 billion time points, unify heterogeneous time series into single-series sequence (S3) format, and develop the GPT-style architecture toward LTSMs. To meet diverse application needs, we convert forecasting, imputation, and anomaly detection of time series into a unified generative task. The outcome of this study is a Time Series Transformer (Timer), that is pre-trained by autoregressive next token prediction on large multi-domain datasets, and is fine-tuned to downstream scenarios with promising abilities as an LTSM.</li>
</ul>

<h3>Title: AutoTimes: Autoregressive Time Series Forecasters via Large Language  Models</h3>
<ul>
<li><strong>Authors: </strong>Yong Liu, Guo Qin, Xiangdong Huang, Jianmin Wang, Mingsheng Long</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02370">https://arxiv.org/abs/2402.02370</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02370">https://arxiv.org/pdf/2402.02370</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02370]] AutoTimes: Autoregressive Time Series Forecasters via Large Language  Models(https://arxiv.org/abs/2402.02370)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, in-context</a></li>
<li><strong>Abstract: </strong>Foundation models of time series have not been fully developed due to the limited availability of large-scale time series and the underexploration of scalable pre-training. Based on the similar sequential structure of time series and natural language, increasing research demonstrates the feasibility of leveraging large language models (LLM) for time series. Nevertheless, prior methods may overlook the consistency in aligning time series and natural language, resulting in insufficient utilization of the LLM potentials. To fully exploit the general-purpose token transitions learned from language modeling, we propose AutoTimes to repurpose LLMs as Autoregressive Time series forecasters, which is consistent with the acquisition and utilization of LLMs without updating the parameters. The consequent forecasters can handle flexible series lengths and achieve competitive performance as prevalent models. Further, we present token-wise prompting that utilizes corresponding timestamps to make our method applicable to multimodal scenarios. Analysis demonstrates our forecasters inherit zero-shot and in-context learning capabilities of LLMs. Empirically, AutoTimes exhibits notable method generality and achieves enhanced performance by basing on larger LLMs, additional texts, or time series as instructions.</li>
</ul>

<h3>Title: PromptRR: Diffusion Models as Prompt Generators for Single Image  Reflection Removal</h3>
<ul>
<li><strong>Authors: </strong>Tao Wang, Wanglong Lu, Kaihao Zhang, Wenhan Luo, Tae-Kyun Kim, Tong Lu, Hongdong Li, Ming-Hsuan Yang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02374">https://arxiv.org/abs/2402.02374</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02374">https://arxiv.org/pdf/2402.02374</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02374]] PromptRR: Diffusion Models as Prompt Generators for Single Image  Reflection Removal(https://arxiv.org/abs/2402.02374)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Existing single image reflection removal (SIRR) methods using deep learning tend to miss key low-frequency (LF) and high-frequency (HF) differences in images, affecting their effectiveness in removing reflections. To address this problem, this paper proposes a novel prompt-guided reflection removal (PromptRR) framework that uses frequency information as new visual prompts for better reflection performance. Specifically, the proposed framework decouples the reflection removal process into the prompt generation and subsequent prompt-guided restoration. For the prompt generation, we first propose a prompt pre-training strategy to train a frequency prompt encoder that encodes the ground-truth image into LF and HF prompts. Then, we adopt diffusion models (DMs) as prompt generators to generate the LF and HF prompts estimated by the pre-trained frequency prompt encoder. For the prompt-guided restoration, we integrate specially generated prompts into the PromptFormer network, employing a novel Transformer-based prompt block to effectively steer the model toward enhanced reflection removal. The results on commonly used benchmarks show that our method outperforms state-of-the-art approaches. The codes and models are available at https://github.com/TaoWangzj/PromptRR.</li>
</ul>

<h3>Title: Revisiting the Power of Prompt for Visual Tuning</h3>
<ul>
<li><strong>Authors: </strong>Yuzhu Wang, Lechao Cheng, Chaowei Fang, Dingwen Zhang, Manni Duan, Meng Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02382">https://arxiv.org/abs/2402.02382</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02382">https://arxiv.org/pdf/2402.02382</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02382]] Revisiting the Power of Prompt for Visual Tuning(https://arxiv.org/abs/2402.02382)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Visual prompt tuning (VPT) is a promising solution incorporating learnable prompt tokens to customize pre-trained models for downstream tasks. However, VPT and its variants often encounter challenges like prompt initialization, prompt length, and subpar performance in self-supervised pretraining, hindering successful contextual adaptation. This study commences by exploring the correlation evolvement between prompts and patch tokens during proficient training. Inspired by the observation that the prompt tokens tend to share high mutual information with patch tokens, we propose initializing prompts with downstream token prototypes. The strategic initialization, a stand-in for the previous initialization, substantially improves performance in fine-tuning. To refine further, we optimize token construction with a streamlined pipeline that maintains excellent performance with almost no increase in computational expenses compared to VPT. Exhaustive experiments show our proposed approach outperforms existing methods by a remarkable margin. For instance, it surpasses full fine-tuning in 19 out of 24 tasks, using less than 0.4% of learnable parameters on the FGVC and VTAB-1K benchmarks. Notably, our method significantly advances the adaptation for self-supervised pretraining, achieving impressive task performance gains of at least 10% to 30%. Besides, the experimental results demonstrate the proposed SPT is robust to prompt lengths and scales well with model capacity and training data size. We finally provide an insightful exploration into the amount of target data facilitating the adaptation of pre-trained models to downstream tasks.</li>
</ul>

<h3>Title: Solution-oriented Agent-based Models Generation with Verifier-assisted  Iterative In-context Learning</h3>
<ul>
<li><strong>Authors: </strong>Tong Niu, Weihao Zhang, Rong Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02388">https://arxiv.org/abs/2402.02388</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02388">https://arxiv.org/pdf/2402.02388</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02388]] Solution-oriented Agent-based Models Generation with Verifier-assisted  Iterative In-context Learning(https://arxiv.org/abs/2402.02388)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Agent-based models (ABMs) stand as an essential paradigm for proposing and validating hypothetical solutions or policies aimed at addressing challenges posed by complex systems and achieving various objectives. This process demands labor-intensive endeavors and multidisciplinary expertise. Large language models (LLMs) encapsulating cross-domain knowledge and programming proficiency could potentially alleviate the difficulty of this process. However, LLMs excel in handling sequential information, making it challenging for analyzing the intricate interactions and nonlinear dynamics inherent in ABMs. Additionally, due to the lack of self-evaluation capability of LLMs, relying solely on LLMs is insufficient to effectively accomplish this process. In this paper, we present SAGE, a general solution-oriented ABM generation framework designed for automatic modeling and generating solutions for targeted problems. Unlike approaches reliant on expert handcrafting or resource-intensive neural network training, SAGE establishes a verifier-assisted iterative in-context learning process employing large language models (LLMs) to leverages their inherent cross-domain knowledge for tackling intricate demands from diverse domain scenarios. In SAGE, we introduce an semi-structured conceptual representation expliciting the intricate structures of ABMs and an objective representation to guide LLMs in modeling scenarios and proposing hypothetical solutions through in-context learning. To ensure the model executability and solution feasibility, SAGE devises a two-level verifier with chain-of-thought prompting tailored to the complex interactions and non-linear dynamics of ABMs, driving the iterative generation optimization. Moreover, we construct an evaluation dataset of solution-oriented ABMs from open sources.It contains practical models across various domains.</li>
</ul>

<h3>Title: KICGPT: Large Language Model with Knowledge in Context for Knowledge  Graph Completion</h3>
<ul>
<li><strong>Authors: </strong>Yanbin Wei, Qiushi Huang, James T. Kwok, Yu Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02389">https://arxiv.org/abs/2402.02389</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02389">https://arxiv.org/pdf/2402.02389</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02389]] KICGPT: Large Language Model with Knowledge in Context for Knowledge  Graph Completion(https://arxiv.org/abs/2402.02389)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Knowledge Graph Completion (KGC) is crucial for addressing knowledge graph incompleteness and supporting downstream applications. Many models have been proposed for KGC. They can be categorized into two main classes: triple-based and text-based approaches. Triple-based methods struggle with long-tail entities due to limited structural information and imbalanced entity distributions. Text-based methods alleviate this issue but require costly training for language models and specific finetuning for knowledge graphs, which limits their efficiency. To alleviate these limitations, in this paper, we propose KICGPT, a framework that integrates a large language model (LLM) and a triple-based KGC retriever. It alleviates the long-tail problem without incurring additional training overhead. KICGPT uses an in-context learning strategy called Knowledge Prompt, which encodes structural knowledge into demonstrations to guide the LLM. Empirical results on benchmark datasets demonstrate the effectiveness of KICGPT with smaller training overhead and no finetuning.</li>
</ul>

<h3>Title: Physics-Inspired Degradation Models for Hyperspectral Image Fusion</h3>
<ul>
<li><strong>Authors: </strong>Jie Lian, Lizhi Wang, Lin Zhu, Renwei Dian, Zhiwei Xiong, Hua Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02411">https://arxiv.org/abs/2402.02411</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02411">https://arxiv.org/pdf/2402.02411</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02411]] Physics-Inspired Degradation Models for Hyperspectral Image Fusion(https://arxiv.org/abs/2402.02411)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>The fusion of a low-spatial-resolution hyperspectral image (LR-HSI) with a high-spatial-resolution multispectral image (HR-MSI) has garnered increasing research interest. However, most fusion methods solely focus on the fusion algorithm itself and overlook the degradation models, which results in unsatisfactory performance in practical scenarios. To fill this gap, we propose physics-inspired degradation models (PIDM) to model the degradation of LR-HSI and HR-MSI, which comprises a spatial degradation network (SpaDN) and a spectral degradation network (SpeDN). SpaDN and SpeDN are designed based on two insights. First, we employ spatial warping and spectral modulation operations to simulate lens aberrations, thereby introducing non-uniformity into the spatial and spectral degradation processes. Second, we utilize asymmetric downsampling and parallel downsampling operations to separately reduce the spatial and spectral resolutions of the images, thus ensuring the matching of spatial and spectral degradation processes with specific physical characteristics. Once SpaDN and SpeDN are established, we adopt a self-supervised training strategy to optimize the network parameters and provide a plug-and-play solution for fusion methods. Comprehensive experiments demonstrate that our proposed PIDM can boost the fusion performance of existing fusion methods in practical scenarios.</li>
</ul>

<h3>Title: DiffStitch: Boosting Offline Reinforcement Learning with Diffusion-based  Trajectory Stitching</h3>
<ul>
<li><strong>Authors: </strong>Guanghe Li, Yixiang Shan, Zhengbang Zhu, Ting Long, Weinan Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02439">https://arxiv.org/abs/2402.02439</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02439">https://arxiv.org/pdf/2402.02439</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02439]] DiffStitch: Boosting Offline Reinforcement Learning with Diffusion-based  Trajectory Stitching(https://arxiv.org/abs/2402.02439)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In offline reinforcement learning (RL), the performance of the learned policy highly depends on the quality of offline datasets. However, in many cases, the offline dataset contains very limited optimal trajectories, which poses a challenge for offline RL algorithms as agents must acquire the ability to transit to high-reward regions. To address this issue, we introduce Diffusion-based Trajectory Stitching (DiffStitch), a novel diffusion-based data augmentation pipeline that systematically generates stitching transitions between trajectories. DiffStitch effectively connects low-reward trajectories with high-reward trajectories, forming globally optimal trajectories to address the challenges faced by offline RL algorithms. Empirical experiments conducted on D4RL datasets demonstrate the effectiveness of DiffStitch across RL methodologies. Notably, DiffStitch demonstrates substantial enhancements in the performance of one-step methods (IQL), imitation learning methods (TD3+BC), and trajectory optimization methods (DT).</li>
</ul>

<h3>Title: AI Art Neural Constellation: Revealing the Collective and Contrastive  State of AI-Generated and Human Art</h3>
<ul>
<li><strong>Authors: </strong>Faizan Farooq Khan, Diana Kim, Divyansh Jha, Youssef Mohamed, Hanna H Chang, Ahmed Elgammal, Luba Elliott, Mohamed Elhoseiny</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02453">https://arxiv.org/abs/2402.02453</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02453">https://arxiv.org/pdf/2402.02453</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02453]] AI Art Neural Constellation: Revealing the Collective and Contrastive  State of AI-Generated and Human Art(https://arxiv.org/abs/2402.02453)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Discovering the creative potentials of a random signal to various artistic expressions in aesthetic and conceptual richness is a ground for the recent success of generative machine learning as a way of art creation. To understand the new artistic medium better, we conduct a comprehensive analysis to position AI-generated art within the context of human art heritage. Our comparative analysis is based on an extensive dataset, dubbed ``ArtConstellation,'' consisting of annotations about art principles, likability, and emotions for 6,000 WikiArt and 3,200 AI-generated artworks. After training various state-of-the-art generative models, art samples are produced and compared with WikiArt data on the last hidden layer of a deep-CNN trained for style classification. We actively examined the various art principles to interpret the neural representations and used them to drive the comparative knowledge about human and AI-generated art. A key finding in the semantic analysis is that AI-generated artworks are visually related to the principle concepts for modern period art made in 1800-2000. In addition, through Out-Of-Distribution (OOD) and In-Distribution (ID) detection in CLIP space, we find that AI-generated artworks are ID to human art when they depict landscapes and geometric abstract figures, while detected as OOD when the machine art consists of deformed and twisted figures. We observe that machine-generated art is uniquely characterized by incomplete and reduced figuration. Lastly, we conducted a human survey about emotional experience. Color composition and familiar subjects are the key factors of likability and emotions in art appreciation. We propose our whole methodologies and collected dataset as our analytical framework to contrast human and AI-generated art, which we refer to as ``ArtNeuralConstellation''. Code is available at: https://github.com/faixan-khan/ArtNeuralConstellation</li>
</ul>

<h3>Title: Deep Spectral Improvement for Unsupervised Image Instance Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Farnoosh Arefi, Amir M. Mansourian, Shohreh Kasaei</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02474">https://arxiv.org/abs/2402.02474</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02474">https://arxiv.org/pdf/2402.02474</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02474]] Deep Spectral Improvement for Unsupervised Image Instance Segmentation(https://arxiv.org/abs/2402.02474)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Deep spectral methods reframe the image decomposition process as a graph partitioning task by extracting features using self-supervised learning and utilizing the Laplacian of the affinity matrix to obtain eigensegments. However, instance segmentation has received less attention compared to other tasks within the context of deep spectral methods. This paper addresses the fact that not all channels of the feature map extracted from a self-supervised backbone contain sufficient information for instance segmentation purposes. In fact, Some channels are noisy and hinder the accuracy of the task. To overcome this issue, this paper proposes two channel reduction modules: Noise Channel Reduction (NCR) and Deviation-based Channel Reduction (DCR). The NCR retains channels with lower entropy, as they are less likely to be noisy, while DCR prunes channels with low standard deviation, as they lack sufficient information for effective instance segmentation. Furthermore, the paper demonstrates that the dot product, commonly used in deep spectral methods, is not suitable for instance segmentation due to its sensitivity to feature map values, potentially leading to incorrect instance segments. A new similarity metric called Bray-Curtis over Chebyshev (BoC) is proposed to address this issue. It takes into account the distribution of features in addition to their values, providing a more robust similarity measure for instance segmentation. Quantitative and qualitative results on the Youtube-VIS2019 dataset highlight the improvements achieved by the proposed channel reduction methods and the use of BoC instead of the conventional dot product for creating the affinity matrix. These improvements are observed in terms of mean Intersection over Union and extracted instance segments, demonstrating enhanced instance segmentation performance. The code is available on: https://github.com/farnooshar/SpecUnIIS</li>
</ul>

<h3>Title: TimeSiam: A Pre-Training Framework for Siamese Time-Series Modeling</h3>
<ul>
<li><strong>Authors: </strong>Jiaxiang Dong, Haixu Wu, Yuxuan Wang, Yunzhong Qiu, Li Zhang, Jianmin Wang, Mingsheng Long</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02475">https://arxiv.org/abs/2402.02475</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02475">https://arxiv.org/pdf/2402.02475</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02475]] TimeSiam: A Pre-Training Framework for Siamese Time-Series Modeling(https://arxiv.org/abs/2402.02475)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Time series pre-training has recently garnered wide attention for its potential to reduce labeling expenses and benefit various downstream tasks. Prior methods are mainly based on pre-training techniques well-acknowledged in vision or language, such as masked modeling and contrastive learning. However, randomly masking time series or calculating series-wise similarity will distort or neglect inherent temporal correlations crucial in time series data. To emphasize temporal correlation modeling, this paper proposes TimeSiam as a simple but effective self-supervised pre-training framework for Time series based on Siamese networks. Concretely, TimeSiam pre-trains Siamese encoders to capture intrinsic temporal correlations between randomly sampled past and current subseries. With a simple data augmentation method (e.g.~masking), TimeSiam can benefit from diverse augmented subseries and learn internal time-dependent representations through a past-to-current reconstruction. Moreover, learnable lineage embeddings are also introduced to distinguish temporal distance between sampled series and further foster the learning of diverse temporal correlations. TimeSiam consistently outperforms extensive advanced pre-training baselines, demonstrating superior forecasting and classification capabilities across 13 standard benchmarks in both intra- and cross-domain scenarios.</li>
</ul>

<h3>Title: Latent Graph Diffusion: A Unified Framework for Generation and  Prediction on Graphs</h3>
<ul>
<li><strong>Authors: </strong>Zhou Cai, Xiyuan Wang, Muhan Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02518">https://arxiv.org/abs/2402.02518</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02518">https://arxiv.org/pdf/2402.02518</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02518]] Latent Graph Diffusion: A Unified Framework for Generation and  Prediction on Graphs(https://arxiv.org/abs/2402.02518)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In this paper, we propose the first framework that enables solving graph learning tasks of all levels (node, edge and graph) and all types (generation, regression and classification) with one model. We first propose Latent Graph Diffusion (LGD), a generative model that can generate node, edge, and graph-level features of all categories simultaneously. We achieve this goal by embedding the graph structures and features into a latent space leveraging a powerful encoder which can also be decoded, then training a diffusion model in the latent space. LGD is also capable of conditional generation through a specifically designed cross-attention mechanism. Then we formulate prediction tasks including regression and classification as (conditional) generation, which enables our LGD to solve tasks of all levels and all types with provable guarantees. We verify the effectiveness of our framework with extensive experiments, where our models achieve state-of-the-art or highly competitive results across generation and regression tasks.</li>
</ul>

<h3>Title: Are Large Language Models Table-based Fact-Checkers?</h3>
<ul>
<li><strong>Authors: </strong>Hangwen Zhang, Qingyi Si, Peng Fu, Zheng Lin, Weiping Wang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02549">https://arxiv.org/abs/2402.02549</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02549">https://arxiv.org/pdf/2402.02549</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02549]] Are Large Language Models Table-based Fact-Checkers?(https://arxiv.org/abs/2402.02549)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Table-based Fact Verification (TFV) aims to extract the entailment relation between statements and structured tables. Existing TFV methods based on small-scaled models suffer from insufficient labeled data and weak zero-shot ability. Recently, the appearance of Large Language Models (LLMs) has gained lots of attraction in research fields. They have shown powerful zero-shot and in-context learning abilities on several NLP tasks, but their potential on TFV is still unknown. In this work, we implement a preliminary study about whether LLMs are table-based fact-checkers. In detail, we design diverse prompts to explore how the in-context learning can help LLMs in TFV, i.e., zero-shot and few-shot TFV capability. Besides, we carefully design and construct TFV instructions to study the performance gain brought by the instruction tuning of LLMs. Experimental results demonstrate that LLMs can achieve acceptable results on zero-shot and few-shot TFV with prompt engineering, while instruction-tuning can stimulate the TFV capability significantly. We also make some valuable findings about the format of zero-shot prompts and the number of in-context examples. Finally, we analyze some possible directions to promote the accuracy of TFV via LLMs, which is beneficial to further research of table reasoning.</li>
</ul>

<h3>Title: Foundation Model Makes Clustering a Better Initialization for Active  Learning</h3>
<ul>
<li><strong>Authors: </strong>Han Yuan, Chuan Hong</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02561">https://arxiv.org/abs/2402.02561</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02561">https://arxiv.org/pdf/2402.02561</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02561]] Foundation Model Makes Clustering a Better Initialization for Active  Learning(https://arxiv.org/abs/2402.02561)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>Active learning selects the most informative samples from the unlabeled dataset to annotate in the context of a limited annotation budget. While numerous methods have been proposed for subsequent sample selection based on an initialized model, scant attention has been paid to the indispensable phase of active learning: selecting samples for model initialization. Most of the previous studies resort to random sampling or naive clustering. However, random sampling is prone to fluctuation, and naive clustering suffers from convergence speed, particularly when dealing with high-dimensional data such as imaging data. In this work, we propose to integrate foundation models with clustering methods to select samples for active learning initialization. Foundation models refer to those trained on massive datasets by the self-supervised paradigm and capable of generating informative and compacted embeddings for various downstream tasks. Leveraging these embeddings to replace raw features such as pixel values, clustering quickly converges and identifies better initial samples. For a comprehensive comparison, we included a classic ImageNet-supervised model to acquire embeddings. Experiments on two clinical tasks of image classification and segmentation demonstrated that foundation model-based clustering efficiently pinpointed informative initial samples, leading to models showcasing enhanced performance than the baseline methods. We envisage that this study provides an effective paradigm for future active learning.</li>
</ul>

<h3>Title: DiffEditor: Boosting Accuracy and Flexibility on Diffusion-based Image  Editing</h3>
<ul>
<li><strong>Authors: </strong>Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, Jian Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02583">https://arxiv.org/abs/2402.02583</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02583">https://arxiv.org/pdf/2402.02583</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02583]] DiffEditor: Boosting Accuracy and Flexibility on Diffusion-based Image  Editing(https://arxiv.org/abs/2402.02583)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Large-scale Text-to-Image (T2I) diffusion models have revolutionized image generation over the last few years. Although owning diverse and high-quality generation capabilities, translating these abilities to fine-grained image editing remains challenging. In this paper, we propose DiffEditor to rectify two weaknesses in existing diffusion-based image editing: (1) in complex scenarios, editing results often lack editing accuracy and exhibit unexpected artifacts; (2) lack of flexibility to harmonize editing operations, e.g., imagine new content. In our solution, we introduce image prompts in fine-grained image editing, cooperating with the text prompt to better describe the editing content. To increase the flexibility while maintaining content consistency, we locally combine stochastic differential equation (SDE) into the ordinary differential equation (ODE) sampling. In addition, we incorporate regional score-based gradient guidance and a time travel strategy into the diffusion sampling, further improving the editing quality. Extensive experiments demonstrate that our method can efficiently achieve state-of-the-art performance on various fine-grained image editing tasks, including editing within a single image (e.g., object moving, resizing, and content dragging) and across images (e.g., appearance replacing and object pasting). Our source code is released at https://github.com/MC-E/DragonDiffusion.</li>
</ul>

<h3>Title: Layer-Wise Analysis of Self-Supervised Acoustic Word Embeddings: A Study  on Speech Emotion Recognition</h3>
<ul>
<li><strong>Authors: </strong>Alexandra Saliba, Yuanchao Li, Ramon Sanabria, Catherine Lai</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02617">https://arxiv.org/abs/2402.02617</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02617">https://arxiv.org/pdf/2402.02617</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02617]] Layer-Wise Analysis of Self-Supervised Acoustic Word Embeddings: A Study  on Speech Emotion Recognition(https://arxiv.org/abs/2402.02617)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>The efficacy of self-supervised speech models has been validated, yet the optimal utilization of their representations remains challenging across diverse tasks. In this study, we delve into Acoustic Word Embeddings (AWEs), a fixed-length feature derived from continuous representations, to explore their advantages in specific tasks. AWEs have previously shown utility in capturing acoustic discriminability. In light of this, we propose measuring layer-wise similarity between AWEs and word embeddings, aiming to further investigate the inherent context within AWEs. Moreover, we evaluate the contribution of AWEs, in comparison to other types of speech features, in the context of Speech Emotion Recognition (SER). Through a comparative experiment and a layer-wise accuracy analysis on two distinct corpora, IEMOCAP and ESD, we explore differences between AWEs and raw self-supervised representations, as well as the proper utilization of AWEs alone and in combination with word embeddings. Our findings underscore the acoustic context conveyed by AWEs and showcase the highly competitive SER accuracies by appropriately employing AWEs.</li>
</ul>

<h3>Title: Image-Caption Encoding for Improving Zero-Shot Generalization</h3>
<ul>
<li><strong>Authors: </strong>Eric Yang Yu, Christopher Liao, Sathvik Ravi, Theodoros Tsiligkaridis, Brian Kulis</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02662">https://arxiv.org/abs/2402.02662</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02662">https://arxiv.org/pdf/2402.02662</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02662]] Image-Caption Encoding for Improving Zero-Shot Generalization(https://arxiv.org/abs/2402.02662)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advances in vision-language models have combined contrastive approaches with generative methods to achieve state-of-the-art (SOTA) on downstream inference tasks like zero-shot image classification. However, a persistent issue of these models for image classification is their out-of-distribution (OOD) generalization capabilities. We first show that when an OOD data point is misclassified, the correct class can be typically found in the Top-K predicted classes. In order to steer the model prediction toward the correct class within the top predicted classes, we propose the Image-Caption Encoding (ICE) method, a straightforward approach that directly enforces consistency between the image-conditioned and caption-conditioned predictions at evaluation time only. Intuitively, we take advantage of unique properties of the generated captions to guide our local search for the correct class label within the Top-K predicted classes. We show that our method can be easily combined with other SOTA methods to enhance Top-1 OOD accuracies by 0.5% on average and up to 3% on challenging datasets. Our code: https://github.com/Chris210634/ice</li>
</ul>

<h3>Title: Large Language Models are Geographically Biased</h3>
<ul>
<li><strong>Authors: </strong>Rohin Manvi, Samar Khanna, Marshall Burke, David Lobell, Stefano Ermon</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02680">https://arxiv.org/abs/2402.02680</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02680">https://arxiv.org/pdf/2402.02680</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02680]] Large Language Models are Geographically Biased(https://arxiv.org/abs/2402.02680)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) inherently carry the biases contained in their training corpora, which can lead to the perpetuation of societal harm. As the impact of these foundation models grows, understanding and evaluating their biases becomes crucial to achieving fairness and accuracy. We propose to study what LLMs know about the world we live in through the lens of geography. This approach is particularly powerful as there is ground truth for the numerous aspects of human life that are meaningfully projected onto geographic space such as culture, race, language, politics, and religion. We show various problematic geographic biases, which we define as systemic errors in geospatial predictions. Initially, we demonstrate that LLMs are capable of making accurate zero-shot geospatial predictions in the form of ratings that show strong monotonic correlation with ground truth (Spearman's $\rho$ of up to 0.89). We then show that LLMs exhibit common biases across a range of objective and subjective topics. In particular, LLMs are clearly biased against locations with lower socioeconomic conditions (e.g. most of Africa) on a variety of sensitive subjective topics such as attractiveness, morality, and intelligence (Spearman's $\rho$ of up to 0.70). Finally, we introduce a bias score to quantify this and find that there is significant variation in the magnitude of bias across existing LLMs.</li>
</ul>

<h3>Title: FDNet: Frequency Domain Denoising Network For Cell Segmentation in  Astrocytes Derived From Induced Pluripotent Stem Cells</h3>
<ul>
<li><strong>Authors: </strong>Haoran Li, Jiahua Shi, Huaming Chen, Bo Du, Simon Maksour, Gabrielle Phillips, Mirella Dottori, Jun Shen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02724">https://arxiv.org/abs/2402.02724</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02724">https://arxiv.org/pdf/2402.02724</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02724]] FDNet: Frequency Domain Denoising Network For Cell Segmentation in  Astrocytes Derived From Induced Pluripotent Stem Cells(https://arxiv.org/abs/2402.02724)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Artificially generated induced pluripotent stem cells (iPSCs) from somatic cells play an important role for disease modeling and drug screening of neurodegenerative diseases. Astrocytes differentiated from iPSCs are important targets to investigate neuronal metabolism. The astrocyte differentiation progress can be monitored through the variations of morphology observed from microscopy images at different differentiation stages, then determined by molecular biology techniques upon maturation. However, the astrocytes usually ``perfectly'' blend into the background and some of them are covered by interference information (i.e., dead cells, media sediments, and cell debris), which makes astrocytes difficult to observe. Due to the lack of annotated datasets, the existing state-of-the-art deep learning approaches cannot be used to address this issue. In this paper, we introduce a new task named astrocyte segmentation with a novel dataset, called IAI704, which contains 704 images and their corresponding pixel-level annotation masks. Moreover, a novel frequency domain denoising network, named FDNet, is proposed for astrocyte segmentation. In detail, our FDNet consists of a contextual information fusion module (CIF), an attention block (AB), and a Fourier transform block (FTB). CIF and AB fuse multi-scale feature embeddings to localize the astrocytes. FTB transforms feature embeddings into the frequency domain and conducts a high-pass filter to eliminate interference information. Experimental results demonstrate the superiority of our proposed FDNet over the state-of-the-art substitutes in astrocyte segmentation, shedding insights for iPSC differentiation progress prediction.</li>
</ul>

<h3>Title: A Generative Approach to Surrogate-based Black-box Attacks</h3>
<ul>
<li><strong>Authors: </strong>Raha Moraffah, Huan Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02732">https://arxiv.org/abs/2402.02732</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02732">https://arxiv.org/pdf/2402.02732</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02732]] A Generative Approach to Surrogate-based Black-box Attacks(https://arxiv.org/abs/2402.02732)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Surrogate-based black-box attacks have exposed the heightened vulnerability of DNNs. These attacks are designed to craft adversarial examples for any samples with black-box target feedback for only a given set of samples. State-of-the-art surrogate-based attacks involve training a discriminative surrogate that mimics the target's outputs. The goal is to learn the decision boundaries of the target. The surrogate is then attacked by white-box attacks to craft adversarial examples similar to the original samples but belong to other classes. With limited samples, the discriminative surrogate fails to accurately learn the target's decision boundaries, and these surrogate-based attacks suffer from low success rates. Different from the discriminative approach, we propose a generative surrogate that learns the distribution of samples residing on or close to the target's decision boundaries. The distribution learned by the generative surrogate can be used to craft adversarial examples that have imperceptible differences from the original samples but belong to other classes. The proposed generative approach results in attacks with remarkably high attack success rates on various targets and datasets.</li>
</ul>

<h3>Title: ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer</h3>
<ul>
<li><strong>Authors: </strong>Bumsoo Kim, Abdul Muqeet, Kyuchul Lee, Sanghyun Seo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02733">https://arxiv.org/abs/2402.02733</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02733">https://arxiv.org/pdf/2402.02733</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02733]] ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer(https://arxiv.org/abs/2402.02733)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Face re-aging is a prominent field in computer vision and graphics, with significant applications in photorealistic domains such as movies, advertising, and live streaming. Recently, the need to apply face re-aging to non-photorealistic images, like comics, illustrations, and animations, has emerged as an extension in various entertainment sectors. However, the absence of a network capable of seamlessly editing the apparent age on NPR images means that these tasks have been confined to a naive approach, applying each task sequentially. This often results in unpleasant artifacts and a loss of facial attributes due to domain discrepancies. In this paper, we introduce a novel one-stage method for face re-aging combined with portrait style transfer, executed in a single generative step. We leverage existing face re-aging and style transfer networks, both trained within the same PR domain. Our method uniquely fuses distinct latent vectors, each responsible for managing aging-related attributes and NPR appearance. Adopting an exemplar-based approach, our method offers greater flexibility than domain-level fine-tuning approaches, which typically require separate training or fine-tuning for each domain. This effectively addresses the limitation of requiring paired datasets for re-aging and domain-level, data-driven approaches for stylization. Our experiments show that our model can effortlessly generate re-aged images while simultaneously transferring the style of examples, maintaining both natural appearance and controllability.</li>
</ul>

<h3>Title: DisDet: Exploring Detectability of Backdoor Attack on Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Yang Sui, Huy Phan, Jinqi Xiao, Tianfang Zhang, Zijie Tang, Cong Shi, Yan Wang, Yingying Chen, Bo Yuan</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02739">https://arxiv.org/abs/2402.02739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02739">https://arxiv.org/pdf/2402.02739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02739]] DisDet: Exploring Detectability of Backdoor Attack on Diffusion Models(https://arxiv.org/abs/2402.02739)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>In the exciting generative AI era, the diffusion model has emerged as a very powerful and widely adopted content generation and editing tool for various data modalities, making the study of their potential security risks very necessary and critical. Very recently, some pioneering works have shown the vulnerability of the diffusion model against backdoor attacks, calling for in-depth analysis and investigation of the security challenges of this popular and fundamental AI technique. In this paper, for the first time, we systematically explore the detectability of the poisoned noise input for the backdoored diffusion models, an important performance metric yet little explored in the existing works. Starting from the perspective of a defender, we first analyze the properties of the trigger pattern in the existing diffusion backdoor attacks, discovering the important role of distribution discrepancy in Trojan detection. Based on this finding, we propose a low-cost trigger detection mechanism that can effectively identify the poisoned input noise. We then take a further step to study the same problem from the attack side, proposing a backdoor attack strategy that can learn the unnoticeable trigger to evade our proposed detection scheme. Empirical evaluations across various diffusion models and datasets demonstrate the effectiveness of the proposed trigger detection and detection-evading attack strategy. For trigger detection, our distribution discrepancy-based solution can achieve a 100\% detection rate for the Trojan triggers used in the existing works. For evading trigger detection, our proposed stealthy trigger design approach performs end-to-end learning to make the distribution of poisoned noise input approach that of benign noise, enabling nearly 100\% detection pass rate with very high attack and benign performance for the backdoored diffusion models.</li>
</ul>

<h3>Title: Contrastive Diffuser: Planning Towards High Return States via  Contrastive Learning</h3>
<ul>
<li><strong>Authors: </strong>Yixiang Shan, Zhengbang Zhu, Ting Long, Qifan Liang, Yi Chang, Weinan Zhang, Liang Yin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02772">https://arxiv.org/abs/2402.02772</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02772">https://arxiv.org/pdf/2402.02772</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02772]] Contrastive Diffuser: Planning Towards High Return States via  Contrastive Learning(https://arxiv.org/abs/2402.02772)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Applying diffusion models in reinforcement learning for long-term planning has gained much attention recently. Several diffusion-based methods have successfully leveraged the modeling capabilities of diffusion for arbitrary distributions. These methods generate subsequent trajectories for planning and have demonstrated significant improvement. However, these methods are limited by their plain base distributions and their overlooking of the diversity of samples, in which different states have different returns. They simply leverage diffusion to learn the distribution of offline dataset, generate the trajectories whose states share the same distribution with the offline dataset. As a result, the probability of these models reaching the high-return states is largely dependent on the dataset distribution. Even equipped with the guidance model, the performance is still suppressed. To address these limitations, in this paper, we propose a novel method called CDiffuser, which devises a return contrast mechanism to pull the states in generated trajectories towards high-return states while pushing them away from low-return states to improve the base distribution. Experiments on 14 commonly used D4RL benchmarks demonstrate the effectiveness of our proposed method. Our code is publicly available at https://anonymous.4open.science/r/ContrastiveDiffuser.</li>
</ul>

<h3>Title: From Partial to Strictly Incremental Constituent Parsing</h3>
<ul>
<li><strong>Authors: </strong>Ana Ezquerro, Carlos Gómez-Rodríguez, David Vilares</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02782">https://arxiv.org/abs/2402.02782</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02782">https://arxiv.org/pdf/2402.02782</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02782]] From Partial to Strictly Incremental Constituent Parsing(https://arxiv.org/abs/2402.02782)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We study incremental constituent parsers to assess their capacity to output trees based on prefix representations alone. Guided by strictly left-to-right generative language models and tree-decoding modules, we build parsers that adhere to a strong definition of incrementality across languages. This builds upon work that asserted incrementality, but that mostly only enforced it on either the encoder or the decoder. Finally, we conduct an analysis against non-incremental and partially incremental models.</li>
</ul>

<h3>Title: Extreme Two-View Geometry From Object Poses with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Yujing Sun, Caiyi Sun, Yuan Liu, Yuexin Ma, Siu Ming Yiu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02800">https://arxiv.org/abs/2402.02800</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02800">https://arxiv.org/pdf/2402.02800</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02800]] Extreme Two-View Geometry From Object Poses with Diffusion Models(https://arxiv.org/abs/2402.02800)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Human has an incredible ability to effortlessly perceive the viewpoint difference between two images containing the same object, even when the viewpoint change is astonishingly vast with no co-visible regions in the images. This remarkable skill, however, has proven to be a challenge for existing camera pose estimation methods, which often fail when faced with large viewpoint differences due to the lack of overlapping local features for matching. In this paper, we aim to effectively harness the power of object priors to accurately determine two-view geometry in the face of extreme viewpoint changes. In our method, we first mathematically transform the relative camera pose estimation problem to an object pose estimation problem. Then, to estimate the object pose, we utilize the object priors learned from a diffusion model Zero123 to synthesize novel-view images of the object. The novel-view images are matched to determine the object pose and thus the two-view camera pose. In experiments, our method has demonstrated extraordinary robustness and resilience to large viewpoint changes, consistently estimating two-view poses with exceptional generalization ability across both synthetic and real-world datasets. Code will be available at https://github.com/scy639/Extreme-Two-View-Geometry-From-Object-Poses-with-Diffusion-Models.</li>
</ul>

<h3>Title: Revisiting VAE for Unsupervised Time Series Anomaly Detection: A  Frequency Perspective</h3>
<ul>
<li><strong>Authors: </strong>Zexin Wang, Changhua Pei, Minghua Ma, Xin Wang, Zhihan Li, Dan Pei, Saravan Rajmohan, Dongmei Zhang, Qingwei Lin, Haiming Zhang, Jianhui Li, Gaogang Xie</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02820">https://arxiv.org/abs/2402.02820</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02820">https://arxiv.org/pdf/2402.02820</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02820]] Revisiting VAE for Unsupervised Time Series Anomaly Detection: A  Frequency Perspective(https://arxiv.org/abs/2402.02820)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Time series Anomaly Detection (AD) plays a crucial role for web systems. Various web systems rely on time series data to monitor and identify anomalies in real time, as well as to initiate diagnosis and remediation procedures. Variational Autoencoders (VAEs) have gained popularity in recent decades due to their superior de-noising capabilities, which are useful for anomaly detection. However, our study reveals that VAE-based methods face challenges in capturing long-periodic heterogeneous patterns and detailed short-periodic trends simultaneously. To address these challenges, we propose Frequency-enhanced Conditional Variational Autoencoder (FCVAE), a novel unsupervised AD method for univariate time series. To ensure an accurate AD, FCVAE exploits an innovative approach to concurrently integrate both the global and local frequency features into the condition of Conditional Variational Autoencoder (CVAE) to significantly increase the accuracy of reconstructing the normal data. Together with a carefully designed "target attention" mechanism, our approach allows the model to pick the most useful information from the frequency domain for better short-periodic trend construction. Our FCVAE has been evaluated on public datasets and a large-scale cloud system, and the results demonstrate that it outperforms state-of-the-art methods. This confirms the practical applicability of our approach in addressing the limitations of current VAE-based anomaly detection models.</li>
</ul>

<h3>Title: SynthVision - Harnessing Minimal Input for Maximal Output in Computer  Vision Models using Synthetic Image data</h3>
<ul>
<li><strong>Authors: </strong>Yudara Kularathne, Prathapa Janitha, Sithira Ambepitiya, Thanveer Ahamed, Dinuka Wijesundara, Prarththanan Sothyrajah</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02826">https://arxiv.org/abs/2402.02826</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02826">https://arxiv.org/pdf/2402.02826</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02826]] SynthVision - Harnessing Minimal Input for Maximal Output in Computer  Vision Models using Synthetic Image data(https://arxiv.org/abs/2402.02826)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Rapid development of disease detection computer vision models is vital in response to urgent medical crises like epidemics or events of bioterrorism. However, traditional data gathering methods are too slow for these scenarios necessitating innovative approaches to generate reliable models quickly from minimal data. We demonstrate our new approach by building a comprehensive computer vision model for detecting Human Papilloma Virus Genital warts using only synthetic data. In our study, we employed a two phase experimental design using diffusion models. In the first phase diffusion models were utilized to generate a large number of diverse synthetic images from 10 HPV guide images explicitly focusing on accurately depicting genital warts. The second phase involved the training and testing vision model using this synthetic dataset. This method aimed to assess the effectiveness of diffusion models in rapidly generating high quality training data and the subsequent impact on the vision model performance in medical image recognition. The study findings revealed significant insights into the performance of the vision model trained on synthetic images generated through diffusion models. The vision model showed exceptional performance in accurately identifying cases of genital warts. It achieved an accuracy rate of 96% underscoring its effectiveness in medical image classification. For HPV cases the model demonstrated a high precision of 99% and a recall of 94%. In normal cases the precision was 95% with an impressive recall of 99%. These metrics indicate the model capability to correctly identify true positive cases and minimize false positives. The model achieved an F1 Score of 96% for HPV cases and 97% for normal cases. The high F1 Score across both categories highlights the balanced nature of the model precision and recall ensuring reliability and robustness in its predictions.</li>
</ul>

<h3>Title: Enhancing Compositional Generalization via Compositional Feature  Alignment</h3>
<ul>
<li><strong>Authors: </strong>Haoxiang Wang, Haozhe Si, Huajie Shao, Han Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02851">https://arxiv.org/abs/2402.02851</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02851">https://arxiv.org/pdf/2402.02851</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02851]] Enhancing Compositional Generalization via Compositional Feature  Alignment(https://arxiv.org/abs/2402.02851)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Real-world applications of machine learning models often confront data distribution shifts, wherein discrepancies exist between the training and test data distributions. In the common multi-domain multi-class setup, as the number of classes and domains scales up, it becomes infeasible to gather training data for every domain-class combination. This challenge naturally leads the quest for models with Compositional Generalization (CG) ability, where models can generalize to unseen domain-class combinations. To delve into the CG challenge, we develop CG-Bench, a suite of CG benchmarks derived from existing real-world image datasets, and observe that the prevalent pretraining-finetuning paradigm on foundational models, such as CLIP and DINOv2, struggles with the challenge. To address this challenge, we propose Compositional Feature Alignment (CFA), a simple two-stage finetuning technique that i) learns two orthogonal linear heads on a pretrained encoder with respect to class and domain labels, and ii) fine-tunes the encoder with the newly learned head frozen. We theoretically and empirically justify that CFA encourages compositional feature learning of pretrained models. We further conduct extensive experiments on CG-Bench for CLIP and DINOv2, two powerful pretrained vision foundation models. Experiment results show that CFA outperforms common finetuning techniques in compositional generalization, corroborating CFA's efficacy in compositional feature learning.</li>
</ul>

<h3>Title: Fine-tuning Reinforcement Learning Models is Secretly a Forgetting  Mitigation Problem</h3>
<ul>
<li><strong>Authors: </strong>Maciej Wołczyk, Bartłomiej Cupiał, Mateusz Ostaszewski, Michał Bortkiewicz, Michał Zając, Razvan Pascanu, Łukasz Kuciński, Piotr Miłoś</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02868">https://arxiv.org/abs/2402.02868</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02868">https://arxiv.org/pdf/2402.02868</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02868]] Fine-tuning Reinforcement Learning Models is Secretly a Forgetting  Mitigation Problem(https://arxiv.org/abs/2402.02868)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Fine-tuning is a widespread technique that allows practitioners to transfer pre-trained capabilities, as recently showcased by the successful applications of foundation models. However, fine-tuning reinforcement learning (RL) models remains a challenge. This work conceptualizes one specific cause of poor transfer, accentuated in the RL setting by the interplay between actions and observations: forgetting of pre-trained capabilities. Namely, a model deteriorates on the state subspace of the downstream task not visited in the initial phase of fine-tuning, on which the model behaved well due to pre-training. This way, we lose the anticipated transfer benefits. We identify conditions when this problem occurs, showing that it is common and, in many cases, catastrophic. Through a detailed empirical analysis of the challenging NetHack and Montezuma's Revenge environments, we show that standard knowledge retention techniques mitigate the problem and thus allow us to take full advantage of the pre-trained capabilities. In particular, in NetHack, we achieve a new state-of-the-art for neural models, improving the previous best score from $5$K to over $10$K points in the Human Monk scenario.</li>
</ul>

<h3>Title: How do Large Language Models Learn In-Context? Query and Key Matrices of  In-Context Heads are Two Towers for Metric Learning</h3>
<ul>
<li><strong>Authors: </strong>Zeping Yu, Sophia Ananiadou</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02872">https://arxiv.org/abs/2402.02872</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02872">https://arxiv.org/pdf/2402.02872</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02872]] How do Large Language Models Learn In-Context? Query and Key Matrices of  In-Context Heads are Two Towers for Metric Learning(https://arxiv.org/abs/2402.02872)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>We explore the mechanism of in-context learning and propose a hypothesis using locate-and-project method. In shallow layers, the features of demonstrations are merged into their corresponding labels, and the features of the input text are aggregated into the last token. In deep layers, in-context heads make great contributions. In each in-context head, the value-output matrix extracts the labels' features. Query and key matrices compute the attention weights between the input text and each demonstration. The larger the attention weight is, the more label information is transferred into the last token for predicting the next word. Query and key matrices can be regarded as two towers for learning the similarity metric between the input text and each demonstration. Based on this hypothesis, we explain why imbalanced labels and demonstration order affect predictions. We conduct experiments on GPT2 large, Llama 7B, 13B and 30B. The results can support our analysis. Overall, our study provides a new method and a reasonable hypothesis for understanding the mechanism of in-context learning. Our code will be released on github.</li>
</ul>

<h3>Title: Time-, Memory- and Parameter-Efficient Visual Adaptation</h3>
<ul>
<li><strong>Authors: </strong>Otniel-Bogdan Mercea, Alexey Gritsenko, Cordelia Schmid, Anurag Arnab</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02887">https://arxiv.org/abs/2402.02887</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02887">https://arxiv.org/pdf/2402.02887</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02887]] Time-, Memory- and Parameter-Efficient Visual Adaptation(https://arxiv.org/abs/2402.02887)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>As foundation models become more popular, there is a growing need to efficiently finetune them for downstream tasks. Although numerous adaptation methods have been proposed, they are designed to be efficient only in terms of how many parameters are trained. They, however, typically still require backpropagating gradients throughout the model, meaning that their training-time and -memory cost does not reduce as significantly. We propose an adaptation method which does not backpropagate gradients through the backbone. We achieve this by designing a lightweight network in parallel that operates on features from the frozen, pretrained backbone. As a result, our method is efficient not only in terms of parameters, but also in training-time and memory usage. Our approach achieves state-of-the-art accuracy-parameter trade-offs on the popular VTAB benchmark, and we further show how we outperform prior works with respect to training-time and -memory usage too. We further demonstrate the training efficiency and scalability of our method by adapting a vision transformer backbone of 4 billion parameters for the computationally demanding task of video classification, without any intricate model parallelism. Here, we outperform a prior adaptor-based method which could only scale to a 1 billion parameter backbone, or fully-finetuning a smaller backbone, with the same GPU and less training time.</li>
</ul>

<h3>Title: ViewFusion: Learning Composable Diffusion Models for Novel View  Synthesis</h3>
<ul>
<li><strong>Authors: </strong>Bernard Spiegl, Andrea Perin, Stéphane Deny, Alexander Ilin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02906">https://arxiv.org/abs/2402.02906</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02906">https://arxiv.org/pdf/2402.02906</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02906]] ViewFusion: Learning Composable Diffusion Models for Novel View  Synthesis(https://arxiv.org/abs/2402.02906)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Deep learning is providing a wealth of new approaches to the old problem of novel view synthesis, from Neural Radiance Field (NeRF) based approaches to end-to-end style architectures. Each approach offers specific strengths but also comes with specific limitations in their applicability. This work introduces ViewFusion, a state-of-the-art end-to-end generative approach to novel view synthesis with unparalleled flexibility. ViewFusion consists in simultaneously applying a diffusion denoising step to any number of input views of a scene, then combining the noise gradients obtained for each view with an (inferred) pixel-weighting mask, ensuring that for each region of the target scene only the most informative input views are taken into account. Our approach resolves several limitations of previous approaches by (1) being trainable and generalizing across multiple scenes and object classes, (2) adaptively taking in a variable number of pose-free views at both train and test time, (3) generating plausible views even in severely undetermined conditions (thanks to its generative nature) -- all while generating views of quality on par or even better than state-of-the-art methods. Limitations include not generating a 3D embedding of the scene, resulting in a relatively slow inference speed, and our method only being tested on the relatively small dataset NMR. Code is available.</li>
</ul>

<h3>Title: Delving into Multi-modal Multi-task Foundation Models for Road Scene  Understanding: From Learning Paradigm Perspectives</h3>
<ul>
<li><strong>Authors: </strong>Sheng Luo, Wei Chen, Wanxin Tian, Rui Liu, Luanxuan Hou, Xiubao Zhang, Haifeng Shen, Ruiqi Wu, Shuyi Geng, Yi Zhou, Ling Shao, Yi Yang, Bojun Gao, Qun Li, Guobin Wu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02968">https://arxiv.org/abs/2402.02968</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02968">https://arxiv.org/pdf/2402.02968</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02968]] Delving into Multi-modal Multi-task Foundation Models for Road Scene  Understanding: From Learning Paradigm Perspectives(https://arxiv.org/abs/2402.02968)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, generative</a></li>
<li><strong>Abstract: </strong>Foundation models have indeed made a profound impact on various fields, emerging as pivotal components that significantly shape the capabilities of intelligent systems. In the context of intelligent vehicles, leveraging the power of foundation models has proven to be transformative, offering notable advancements in visual understanding. Equipped with multi-modal and multi-task learning capabilities, multi-modal multi-task visual understanding foundation models (MM-VUFMs) effectively process and fuse data from diverse modalities and simultaneously handle various driving-related tasks with powerful adaptability, contributing to a more holistic understanding of the surrounding scene. In this survey, we present a systematic analysis of MM-VUFMs specifically designed for road scenes. Our objective is not only to provide a comprehensive overview of common practices, referring to task-specific models, unified multi-modal models, unified multi-task models, and foundation model prompting techniques, but also to highlight their advanced capabilities in diverse learning paradigms. These paradigms include open-world understanding, efficient transfer for road scenes, continual learning, interactive and generative capability. Moreover, we provide insights into key challenges and future trends, such as closed-loop driving systems, interpretability, embodied driving agents, and world models. To facilitate researchers in staying abreast of the latest developments in MM-VUFMs for road scenes, we have established a continuously updated repository at https://github.com/rolsheng/MM-VUFM4DS</li>
</ul>

<h3>Title: Retrieval-Augmented Score Distillation for Text-to-3D Generation</h3>
<ul>
<li><strong>Authors: </strong>Junyoung Seo, Susung Hong, Wooseok Jang, Inès Hyeonsu Kim, Minseop Kwak, Doyup Lee, Seungryong Kim</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02972">https://arxiv.org/abs/2402.02972</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02972">https://arxiv.org/pdf/2402.02972</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02972]] Retrieval-Augmented Score Distillation for Text-to-3D Generation(https://arxiv.org/abs/2402.02972)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-3D generation has achieved significant success by incorporating powerful 2D diffusion models, but insufficient 3D prior knowledge also leads to the inconsistency of 3D geometry. Recently, since large-scale multi-view datasets have been released, fine-tuning the diffusion model on the multi-view datasets becomes a mainstream to solve the 3D inconsistency problem. However, it has confronted with fundamental difficulties regarding the limited quality and diversity of 3D data, compared with 2D data. To sidestep these trade-offs, we explore a retrieval-augmented approach tailored for score distillation, dubbed RetDream. We postulate that both expressiveness of 2D diffusion models and geometric consistency of 3D assets can be fully leveraged by employing the semantically relevant assets directly within the optimization process. To this end, we introduce novel framework for retrieval-based quality enhancement in text-to-3D generation. We leverage the retrieved asset to incorporate its geometric prior in the variational objective and adapt the diffusion model's 2D prior toward view consistency, achieving drastic improvements in both geometry and fidelity of generated scenes. We conduct extensive experiments to demonstrate that RetDream exhibits superior quality with increased geometric consistency. Project page is available at https://ku-cvlab.github.io/RetDream/.</li>
</ul>

<h3>Title: Variational Flow Models: Flowing in Your Style</h3>
<ul>
<li><strong>Authors: </strong>Kien Do, Duc Kieu, Toan Nguyen, Dang Nguyen, Hung Le, Dung Nguyen, Thin Nguyen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02977">https://arxiv.org/abs/2402.02977</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02977">https://arxiv.org/pdf/2402.02977</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02977]] Variational Flow Models: Flowing in Your Style(https://arxiv.org/abs/2402.02977)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We introduce a variational inference interpretation for models of "posterior flows" - generalizations of "probability flows" to a broader class of stochastic processes not necessarily diffusion processes. We coin the resulting models as "Variational Flow Models". Additionally, we propose a systematic training-free method to transform the posterior flow of a "linear" stochastic process characterized by the equation Xt = at * X0 + st * X1 into a straight constant-speed (SC) flow, reminiscent of Rectified Flow. This transformation facilitates fast sampling along the original posterior flow without training a new model of the SC flow. The flexibility of our approach allows us to extend our transformation to inter-convert two posterior flows from distinct "linear" stochastic processes. Moreover, we can easily integrate high-order numerical solvers into the transformed SC flow, further enhancing sampling accuracy and efficiency. Rigorous theoretical analysis and extensive experimental results substantiate the advantages of our framework.</li>
</ul>

<h3>Title: Unsupervised semantic segmentation of high-resolution UAV imagery for  road scene parsing</h3>
<ul>
<li><strong>Authors: </strong>Zihan Ma, Yongshang Li, Ronggui Ma, Chen Liang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.02985">https://arxiv.org/abs/2402.02985</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.02985">https://arxiv.org/pdf/2402.02985</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.02985]] Unsupervised semantic segmentation of high-resolution UAV imagery for  road scene parsing(https://arxiv.org/abs/2402.02985)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>Two challenges are presented when parsing road scenes in UAV images. First, the high resolution of UAV images makes processing difficult. Second, supervised deep learning methods require a large amount of manual annotations to train robust and accurate models. In this paper, an unsupervised road parsing framework that leverages recent advances in vision language models and fundamental computer vision model is introduced.Initially, a vision language model is employed to efficiently process ultra-large resolution UAV images to quickly detect road regions of interest in the images. Subsequently, the vision foundation model SAM is utilized to generate masks for the road regions without category information. Following that, a self-supervised representation learning network extracts feature representations from all masked regions. Finally, an unsupervised clustering algorithm is applied to cluster these feature representations and assign IDs to each cluster. The masked regions are combined with the corresponding IDs to generate initial pseudo-labels, which initiate an iterative self-training process for regular semantic segmentation. The proposed method achieves an impressive 89.96% mIoU on the development dataset without relying on any manual annotation. Particularly noteworthy is the extraordinary flexibility of the proposed method, which even goes beyond the limitations of human-defined categories and is able to acquire knowledge of new categories from the dataset itself.</li>
</ul>

<h3>Title: Automatic Combination of Sample Selection Strategies for Few-Shot  Learning</h3>
<ul>
<li><strong>Authors: </strong>Branislav Pecher, Ivan Srba, Maria Bielikova, Joaquin Vanschoren</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03038">https://arxiv.org/abs/2402.03038</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03038">https://arxiv.org/pdf/2402.03038</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03038]] Automatic Combination of Sample Selection Strategies for Few-Shot  Learning(https://arxiv.org/abs/2402.03038)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>In few-shot learning, such as meta-learning, few-shot fine-tuning or in-context learning, the limited number of samples used to train a model have a significant impact on the overall success. Although a large number of sample selection strategies exist, their impact on the performance of few-shot learning is not extensively known, as most of them have been so far evaluated in typical supervised settings only. In this paper, we thoroughly investigate the impact of 20 sample selection strategies on the performance of 5 few-shot learning approaches over 8 image and 6 text datasets. In addition, we propose a new method for automatic combination of sample selection strategies (ACSESS) that leverages the strengths and complementary information of the individual strategies. The experimental results show that our method consistently outperforms the individual selection strategies, as well as the recently proposed method for selecting support examples for in-context learning. We also show a strong modality, dataset and approach dependence for the majority of strategies as well as their dependence on the number of shots - demonstrating that the sample selection strategies play a significant role for lower number of shots, but regresses to random selection at higher number of shots.</li>
</ul>

<h3>Title: InteractiveVideo: User-Centric Controllable Video Generation with  Synergistic Multimodal Instructions</h3>
<ul>
<li><strong>Authors: </strong>Yiyuan Zhang, Yuhao Kang, Zhixin Zhang, Xiaohan Ding, Sanyuan Zhao, Xiangyu Yue</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, cs.MM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03040">https://arxiv.org/abs/2402.03040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03040">https://arxiv.org/pdf/2402.03040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03040]] InteractiveVideo: User-Centric Controllable Video Generation with  Synergistic Multimodal Instructions(https://arxiv.org/abs/2402.03040)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce $\textit{InteractiveVideo}$, a user-centric framework for video generation. Different from traditional generative approaches that operate based on user-provided images or text, our framework is designed for dynamic interaction, allowing users to instruct the generative model through various intuitive mechanisms during the whole generation process, e.g. text and image prompts, painting, drag-and-drop, etc. We propose a Synergistic Multimodal Instruction mechanism, designed to seamlessly integrate users' multimodal instructions into generative models, thus facilitating a cooperative and responsive interaction between user inputs and the generative process. This approach enables iterative and fine-grained refinement of the generation result through precise and effective user instructions. With $\textit{InteractiveVideo}$, users are given the flexibility to meticulously tailor key aspects of a video. They can paint the reference image, edit semantics, and adjust video motions until their requirements are fully met. Code, models, and demo are available at https://github.com/invictus717/InteractiveVideo</li>
</ul>

<h3>Title: PFDM: Parser-Free Virtual Try-on via Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Yunfang Niu, Dong Yi, Lingxiang Wu, Zhiwei Liu, Pengxiang Cai, Jinqiao Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03047">https://arxiv.org/abs/2402.03047</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03047">https://arxiv.org/pdf/2402.03047</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03047]] PFDM: Parser-Free Virtual Try-on via Diffusion Model(https://arxiv.org/abs/2402.03047)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Virtual try-on can significantly improve the garment shopping experiences in both online and in-store scenarios, attracting broad interest in computer vision. However, to achieve high-fidelity try-on performance, most state-of-the-art methods still rely on accurate segmentation masks, which are often produced by near-perfect parsers or manual labeling. To overcome the bottleneck, we propose a parser-free virtual try-on method based on the diffusion model (PFDM). Given two images, PFDM can "wear" garments on the target person seamlessly by implicitly warping without any other information. To learn the model effectively, we synthesize many pseudo-images and construct sample pairs by wearing various garments on persons. Supervised by the large-scale expanded dataset, we fuse the person and garment features using a proposed Garment Fusion Attention (GFA) mechanism. Experiments demonstrate that our proposed PFDM can successfully handle complex cases, synthesize high-fidelity images, and outperform both state-of-the-art parser-free and parser-based models.</li>
</ul>

<h3>Title: Visual Text Meets Low-level Vision: A Comprehensive Survey on Visual  Text Processing</h3>
<ul>
<li><strong>Authors: </strong>Yan Shu, Weichao Zeng, Zhenhang Li, Fangmin Zhao, Yu Zhou</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03082">https://arxiv.org/abs/2402.03082</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03082">https://arxiv.org/pdf/2402.03082</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03082]] Visual Text Meets Low-level Vision: A Comprehensive Survey on Visual  Text Processing(https://arxiv.org/abs/2402.03082)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Visual text, a pivotal element in both document and scene images, speaks volumes and attracts significant attention in the computer vision domain. Beyond visual text detection and recognition, the field of visual text processing has experienced a surge in research, driven by the advent of fundamental generative models. However, challenges persist due to the unique properties and features that distinguish text from general objects. Effectively leveraging these unique textual characteristics is crucial in visual text processing, as observed in our study. In this survey, we present a comprehensive, multi-perspective analysis of recent advancements in this field. Initially, we introduce a hierarchical taxonomy encompassing areas ranging from text image enhancement and restoration to text image manipulation, followed by different learning paradigms. Subsequently, we conduct an in-depth discussion of how specific textual features such as structure, stroke, semantics, style, and spatial context are seamlessly integrated into various tasks. Furthermore, we explore available public datasets and benchmark the reviewed methods on several widely-used datasets. Finally, we identify principal challenges and potential avenues for future research. Our aim is to establish this survey as a fundamental resource, fostering continued exploration and innovation in the dynamic area of visual text processing.</li>
</ul>

<h3>Title: Transcending Adversarial Perturbations: Manifold-Aided Adversarial  Examples with Legitimate Semantics</h3>
<ul>
<li><strong>Authors: </strong>Shuai Li, Xiaoyu Jiang, Xiaoguang Ma</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03095">https://arxiv.org/abs/2402.03095</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03095">https://arxiv.org/pdf/2402.03095</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03095]] Transcending Adversarial Perturbations: Manifold-Aided Adversarial  Examples with Legitimate Semantics(https://arxiv.org/abs/2402.03095)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Deep neural networks were significantly vulnerable to adversarial examples manipulated by malicious tiny perturbations. Although most conventional adversarial attacks ensured the visual imperceptibility between adversarial examples and corresponding raw images by minimizing their geometric distance, these constraints on geometric distance led to limited attack transferability, inferior visual quality, and human-imperceptible interpretability. In this paper, we proposed a supervised semantic-transformation generative model to generate adversarial examples with real and legitimate semantics, wherein an unrestricted adversarial manifold containing continuous semantic variations was constructed for the first time to realize a legitimate transition from non-adversarial examples to adversarial ones. Comprehensive experiments on MNIST and industrial defect datasets showed that our adversarial examples not only exhibited better visual quality but also achieved superior attack transferability and more effective explanations for model vulnerabilities, indicating their great potential as generic adversarial examples. The code and pre-trained models were available at https://github.com/shuaili1027/MAELS.git.</li>
</ul>

<h3>Title: Video-LaVIT: Unified Video-Language Pre-training with Decoupled  Visual-Motional Tokenization</h3>
<ul>
<li><strong>Authors: </strong>Yang Jin, Zhicheng Sun, Kun Xu, Kun Xu, Liwei Chen, Hao Jiang, Quzhe Huang, Chengru Song, Yuliang Liu, Di Zhang, Yang Song, Kun Gai, Yadong Mu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03161">https://arxiv.org/abs/2402.03161</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03161">https://arxiv.org/pdf/2402.03161</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03161]] Video-LaVIT: Unified Video-Language Pre-training with Decoupled  Visual-Motional Tokenization(https://arxiv.org/abs/2402.03161)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In light of recent advances in multimodal Large Language Models (LLMs), there is increasing attention to scaling them from image-text data to more informative real-world videos. Compared to static images, video poses unique challenges for effective large-scale pre-training due to the modeling of its spatiotemporal dynamics. In this paper, we address such limitations in video-language pre-training with an efficient video decomposition that represents each video as keyframes and temporal motions. These are then adapted to an LLM using well-designed tokenizers that discretize visual and temporal information as a few tokens, thus enabling unified generative pre-training of videos, images, and text. At inference, the generated tokens from the LLM are carefully recovered to the original continuous pixel space to create various video content. Our proposed framework is both capable of comprehending and generating image and video content, as demonstrated by its competitive performance across 13 multimodal benchmarks in image and video understanding and generation. Our code and models will be available at https://video-lavit.github.io.</li>
</ul>

<h3>Title: Direct-a-Video: Customized Video Generation with User-Directed Camera  Movement and Object Motion</h3>
<ul>
<li><strong>Authors: </strong>Shiyuan Yang, Liang Hou, Haibin Huang, Chongyang Ma, Pengfei Wan, Di Zhang, Xiaodong Chen, Jing Liao</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03162">https://arxiv.org/abs/2402.03162</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03162">https://arxiv.org/pdf/2402.03162</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03162]] Direct-a-Video: Customized Video Generation with User-Directed Camera  Movement and Object Motion(https://arxiv.org/abs/2402.03162)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, self-supervised</a></li>
<li><strong>Abstract: </strong>Recent text-to-video diffusion models have achieved impressive progress. In practice, users often desire the ability to control object motion and camera movement independently for customized video creation. However, current methods lack the focus on separately controlling object motion and camera movement in a decoupled manner, which limits the controllability and flexibility of text-to-video models. In this paper, we introduce Direct-a-Video, a system that allows users to independently specify motions for one or multiple objects and/or camera movements, as if directing a video. We propose a simple yet effective strategy for the decoupled control of object motion and camera movement. Object motion is controlled through spatial cross-attention modulation using the model's inherent priors, requiring no additional optimization. For camera movement, we introduce new temporal cross-attention layers to interpret quantitative camera movement parameters. We further employ an augmentation-based approach to train these layers in a self-supervised manner on a small-scale dataset, eliminating the need for explicit motion annotation. Both components operate independently, allowing individual or combined control, and can generalize to open-domain scenarios. Extensive experiments demonstrate the superiority and effectiveness of our method. Project page: https://direct-a-video.github.io/.</li>
</ul>

<h3>Title: Is Mamba Capable of In-Context Learning?</h3>
<ul>
<li><strong>Authors: </strong>Riccardo Grazzi, Julien Siems, Simon Schrodi, Thomas Brox, Frank Hutter</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03170">https://arxiv.org/abs/2402.03170</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03170">https://arxiv.org/pdf/2402.03170</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03170]] Is Mamba Capable of In-Context Learning?(https://arxiv.org/abs/2402.03170)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>This work provides empirical evidence that Mamba, a newly proposed selective structured state space model, has similar in-context learning (ICL) capabilities as transformers. We evaluated Mamba on tasks involving simple function approximation as well as more complex natural language processing problems. Our results demonstrate that across both categories of tasks, Mamba matches the performance of transformer models for ICL. Further analysis reveals that like transformers, Mamba appears to solve ICL problems by incrementally optimizing its internal representations. Overall, our work suggests that Mamba can be an efficient alternative to transformers for ICL tasks involving longer input sequences.</li>
</ul>

<h3>Title: Multi: Multimodal Understanding Leaderboard with Text and Images</h3>
<ul>
<li><strong>Authors: </strong>Zichen Zhu, Yang Xu, Lu Chen, Jingkai Yang, Yichuan Ma, Yiming Sun, Hailin Wen, Jiaqi Liu, Jinyu Cai, Yingzi Ma, Situo Zhang, Zihan Zhao, Liangtai Sun, Kai Yu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03173">https://arxiv.org/abs/2402.03173</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03173">https://arxiv.org/pdf/2402.03173</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03173]] Multi: Multimodal Understanding Leaderboard with Text and Images(https://arxiv.org/abs/2402.03173)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Rapid progress in multimodal large language models (MLLMs) highlights the need to introduce challenging yet realistic benchmarks to the academic community. Existing benchmarks primarily focus on simple natural image understanding, but Multi emerges as a cutting-edge benchmark for MLLMs, offering a comprehensive dataset for evaluating MLLMs against understanding complex figures and tables, and scientific questions. This benchmark, reflecting current realistic examination styles, provides multimodal inputs and requires responses that are either precise or open-ended, similar to real-life school tests. It challenges MLLMs with a variety of tasks, ranging from formula derivation to image detail analysis, and cross-modality reasoning. Multi includes over 18,000 questions, with a focus on science-based QA in diverse formats. We also introduce Multi-Elite, a 500-question subset for testing the extremities of MLLMs, and Multi-Extend, which enhances In-Context Learning research with more than 4,500 knowledge pieces. Our evaluation indicates significant potential for MLLM advancement, with GPT-4V achieving a 63.7% accuracy rate on Multi, in contrast to other MLLMs scoring between 31.3% and 53.7%. Multi serves not only as a robust evaluation platform but also paves the way for the development of expert-level AI.</li>
</ul>

<h3>Title: The Matrix: A Bayesian learning model for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Siddhartha Dalal, Vishal Misra</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03175">https://arxiv.org/abs/2402.03175</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03175">https://arxiv.org/pdf/2402.03175</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03175]] The Matrix: A Bayesian learning model for LLMs(https://arxiv.org/abs/2402.03175)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, in-context</a></li>
<li><strong>Abstract: </strong>In this paper, we introduce a Bayesian learning model to understand the behavior of Large Language Models (LLMs). We explore the optimization metric of LLMs, which is based on predicting the next token, and develop a novel model grounded in this principle. Our approach involves constructing an ideal generative text model represented by a multinomial transition probability matrix with a prior, and we examine how LLMs approximate this matrix. We discuss the continuity of the mapping between embeddings and multinomial distributions, and present the Dirichlet approximation theorem to approximate any prior. Additionally, we demonstrate how text generation by LLMs aligns with Bayesian learning principles and delve into the implications for in-context learning, specifically explaining why in-context learning emerges in larger models where prompts are considered as samples to be updated. Our findings indicate that the behavior of LLMs is consistent with Bayesian Learning, offering new insights into their functioning and potential applications.</li>
</ul>

<h3>Title: Guidance with Spherical Gaussian Constraint for Conditional Diffusion</h3>
<ul>
<li><strong>Authors: </strong>Lingxiao Yang, Shutong Ding, Yifan Cai, Jingyi Yu, Jingya Wang, Ye Shi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03201">https://arxiv.org/abs/2402.03201</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03201">https://arxiv.org/pdf/2402.03201</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03201]] Guidance with Spherical Gaussian Constraint for Conditional Diffusion(https://arxiv.org/abs/2402.03201)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent advances in diffusion models attempt to handle conditional generative tasks by utilizing a differentiable loss function for guidance without the need for additional training. While these methods achieved certain success, they often compromise on sample quality and require small guidance step sizes, leading to longer sampling processes. This paper reveals that the fundamental issue lies in the manifold deviation during the sampling process when loss guidance is employed. We theoretically show the existence of manifold deviation by establishing a certain lower bound for the estimation error of the loss guidance. To mitigate this problem, we propose Diffusion with Spherical Gaussian constraint (DSG), drawing inspiration from the concentration phenomenon in high-dimensional Gaussian distributions. DSG effectively constrains the guidance step within the intermediate data manifold through optimization and enables the use of larger guidance steps. Furthermore, we present a closed-form solution for DSG denoising with the Spherical Gaussian constraint. Notably, DSG can seamlessly integrate as a plugin module within existing training-free conditional diffusion methods. Implementing DSG merely involves a few lines of additional code with almost no extra computational overhead, yet it leads to significant performance improvements. Comprehensive experimental results in various conditional generation tasks validate the superiority and adaptability of DSG in terms of both sample quality and time efficiency.</li>
</ul>

<h3>Title: Light and Optimal Schrödinger Bridge Matching</h3>
<ul>
<li><strong>Authors: </strong>Nikita Gushchin, Sergei Kholkin, Evgeny Burnaev, Alexander Korotin</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03207">https://arxiv.org/abs/2402.03207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03207">https://arxiv.org/pdf/2402.03207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03207]] Light and Optimal Schrödinger Bridge Matching(https://arxiv.org/abs/2402.03207)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Schr\"odinger Bridges (SB) have recently gained the attention of the ML community as a promising extension of classic diffusion models which is also interconnected to the Entropic Optimal Transport (EOT). Recent solvers for SB exploit the pervasive bridge matching procedures. Such procedures aim to recover a stochastic process transporting the mass between distributions given only a transport plan between them. In particular, given the EOT plan, these procedures can be adapted to solve SB. This fact is heavily exploited by recent works giving rives to matching-based SB solvers. The cornerstone here is recovering the EOT plan: recent works either use heuristical approximations (e.g., the minibatch OT) or establish iterative matching procedures which by the design accumulate the error during the training. We address these limitations and propose a novel procedure to learn SB which we call the \textbf{optimal Schr\"odinger bridge matching}. It exploits the optimal parameterization of the diffusion process and provably recovers the SB process \textbf{(a)} with a single bridge matching step and \textbf{(b)} with arbitrary transport plan as the input. Furthermore, we show that the optimal bridge matching objective coincides with the recently discovered energy-based modeling (EBM) objectives to learn EOT/SB. Inspired by this observation, we develop a light solver (which we call LightSB-M) to implement optimal matching in practice using the Gaussian mixture parameterization of the Schr\"odinger potential. We experimentally showcase the performance of our solver in a range of practical tasks. The code for the LightSB-M solver can be found at \url{https://github.com/SKholkin/LightSB-Matching}.</li>
</ul>

<h3>Title: Organic or Diffused: Can We Distinguish Human Art from AI-generated  Images?</h3>
<ul>
<li><strong>Authors: </strong>Anna Yoo Jeong Ha, Josephine Passananti, Ronik Bhaskar, Shawn Shan, Reid Southen, Haitao Zheng, Ben Y. Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03214">https://arxiv.org/abs/2402.03214</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03214">https://arxiv.org/pdf/2402.03214</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03214]] Organic or Diffused: Can We Distinguish Human Art from AI-generated  Images?(https://arxiv.org/abs/2402.03214)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The advent of generative AI images has completely disrupted the art world. Identifying AI generated images from human art is a challenging problem whose impact is growing over time. The failure to address this problem allows bad actors to defraud individuals paying a premium for human art, and companies whose stated policies forbid AI imagery. This is also critical for AI model trainers, who need to filter training data to avoid potential model collapse. There are several different approaches to distinguishing human art from AI images, including classifiers trained by supervised learning, research tools targeting diffusion models, and identification by professional artists using their knowledge of artistic techniques. In this paper, we seek to understand how well these approaches can perform against today's modern generative models in both benign and adversarial settings. We curate real human art across 7 styles, generate matching images from 5 generative models, and apply 8 detectors (5 automated detectors and 3 different human groups including 180 crowdworkers, 4000+ professional artists, and 13 expert artists experienced at detecting AI). Both Hive and expert artists do very well, but make mistakes in different ways (Hive is weaker against adversarial perturbations while Expert artists produce higher false positives). We believe these weaknesses will remain as models continue to evolve, and use our data to demonstrate why a combined team of human and automated detectors provides the best combination of accuracy and robustness.</li>
</ul>

<h3>Title: Skill Set Optimization: Reinforcing Language Model Behavior via  Transferable Skills</h3>
<ul>
<li><strong>Authors: </strong>Kolby Nottingham, Bodhisattwa Prasad Majumder, Bhavana Dalvi Mishra, Sameer Singh, Peter Clark, Roy Fox</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03244">https://arxiv.org/abs/2402.03244</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03244">https://arxiv.org/pdf/2402.03244</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03244]] Skill Set Optimization: Reinforcing Language Model Behavior via  Transferable Skills(https://arxiv.org/abs/2402.03244)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have recently been used for sequential decision making in interactive environments. However, leveraging environment reward signals for continual LLM actor improvement is not straightforward. We propose Skill Set Optimization (SSO) for improving LLM actor performance through constructing and refining sets of transferable skills. SSO constructs skills by extracting common subtrajectories with high rewards and generating subgoals and instructions to represent each skill. These skills are provided to the LLM actor in-context to reinforce behaviors with high rewards. Then, SSO further refines the skill set by pruning skills that do not continue to result in high rewards. We evaluate our method in the classic videogame NetHack and the text environment ScienceWorld to demonstrate SSO's ability to optimize a set of skills and perform in-context policy improvement. SSO outperforms baselines by 40% in our custom NetHack task and outperforms the previous state-of-the-art in ScienceWorld by 35%.</li>
</ul>

<h3>Title: CLIP Can Understand Depth</h3>
<ul>
<li><strong>Authors: </strong>Dunam Kim, Seokju Lee</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03251">https://arxiv.org/abs/2402.03251</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03251">https://arxiv.org/pdf/2402.03251</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03251]] CLIP Can Understand Depth(https://arxiv.org/abs/2402.03251)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Recent studies on generalizing CLIP for monocular depth estimation reveal that CLIP pre-trained on web-crawled data is inefficient for deriving proper similarities between image patches and depth-related prompts. In this paper, we adapt CLIP for meaningful quality of monocular depth estimation with dense prediction, without fine-tuning its original vision-language alignment. By jointly training a compact deconvolutional decoder with a tiny learnable embedding matrix named mirror, as a static prompt for its text encoder, CLIP is enabled to understand depth. With this approach, our model exhibits impressive performance matching several previous state-of-the-art vision-only models on the NYU Depth v2 and KITTI datasets, outperforming every CLIP-based depth estimation model with a large margin. Experiments on temporal depth consistency and spatial continuity demonstrate that the prior knowledge of CLIP can be effectively refined by our proposed framework. Furthermore, an ablation study on mirror proves that the resulting model estimates depth utilizing knowledge not only from the image encoder but also text encoder despite not being given any prompt written in a human way. This research demonstrates that through minimal adjustments, the prior knowledge of vision-language foundation models, such as CLIP, can be generalized even to domains where learning during pretraining is challenging. We facilitate future works focused on methods to adjust suboptimal prior knowledge of vision-language models using non-human language prompts, achieving performance on par with task-specific state-of-the-art methodologies.</li>
</ul>

<h3>Title: MobilityGPT: Enhanced Human Mobility Modeling with a GPT model</h3>
<ul>
<li><strong>Authors: </strong>Ammar Haydari, Dongjie Chen, Zhengfeng Lai, Chen-Nee Chuah</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03264">https://arxiv.org/abs/2402.03264</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03264">https://arxiv.org/pdf/2402.03264</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03264]] MobilityGPT: Enhanced Human Mobility Modeling with a GPT model(https://arxiv.org/abs/2402.03264)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative models have shown promising results in capturing human mobility characteristics and generating synthetic trajectories. However, it remains challenging to ensure that the generated geospatial mobility data is semantically realistic, including consistent location sequences, and reflects real-world characteristics, such as constraining on geospatial limits. To address these issues, we reformat human mobility modeling as an autoregressive generation task, leveraging Generative Pre-trained Transformer (GPT). To ensure its controllable generation to alleviate the above challenges, we propose a geospatially-aware generative model, MobilityGPT. We propose a gravity-based sampling method to train a transformer for semantic sequence similarity. Then, we constrained the training process via a road connectivity matrix that provides the connectivity of sequences in trajectory generation, thereby keeping generated trajectories in geospatial limits. Lastly, we constructed a Reinforcement Learning from Trajectory Feedback (RLTF) to minimize the travel distance between training and the synthetically generated trajectories. Our experiments on real-world datasets demonstrate that MobilityGPT outperforms state-of-the-art methods in generating high-quality mobility trajectories that are closest to real data in terms of origin-destination similarity, trip length, travel radius, link, and gravity distributions.</li>
</ul>

<h3>Title: InstanceDiffusion: Instance-level Control for Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Xudong Wang, Trevor Darrell, Sai Saketh Rambhatla, Rohit Girdhar, Ishan Misra</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03290">https://arxiv.org/abs/2402.03290</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03290">https://arxiv.org/pdf/2402.03290</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03290]] InstanceDiffusion: Instance-level Control for Image Generation(https://arxiv.org/abs/2402.03290)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-image diffusion models produce high quality images but do not offer control over individual instances in the image. We introduce InstanceDiffusion that adds precise instance-level control to text-to-image diffusion models. InstanceDiffusion supports free-form language conditions per instance and allows flexible ways to specify instance locations such as simple single points, scribbles, bounding boxes or intricate instance segmentation masks, and combinations thereof. We propose three major changes to text-to-image models that enable precise instance-level control. Our UniFusion block enables instance-level conditions for text-to-image models, the ScaleU block improves image fidelity, and our Multi-instance Sampler improves generations for multiple instances. InstanceDiffusion significantly surpasses specialized state-of-the-art models for each location condition. Notably, on the COCO dataset, we outperform previous state-of-the-art by 20.4% AP$_{50}^\text{box}$ for box inputs, and 25.4% IoU for mask inputs.</li>
</ul>

<h3>Title: Zero-shot Object-Level OOD Detection with Context-Aware Inpainting</h3>
<ul>
<li><strong>Authors: </strong>Quang-Huy Nguyen, Jin Peng Zhou, Zhenzhen Liu, Khanh-Huyen Bui, Kilian Q. Weinberger, Dung D. Le</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03292">https://arxiv.org/abs/2402.03292</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03292">https://arxiv.org/pdf/2402.03292</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03292]] Zero-shot Object-Level OOD Detection with Context-Aware Inpainting(https://arxiv.org/abs/2402.03292)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Machine learning algorithms are increasingly provided as black-box cloud services or pre-trained models, without access to their training data. This motivates the problem of zero-shot out-of-distribution (OOD) detection. Concretely, we aim to detect OOD objects that do not belong to the classifier's label set but are erroneously classified as in-distribution (ID) objects. Our approach, RONIN, uses an off-the-shelf diffusion model to replace detected objects with inpainting. RONIN conditions the inpainting process with the predicted ID label, drawing the input object closer to the in-distribution domain. As a result, the reconstructed object is very close to the original in the ID cases and far in the OOD cases, allowing RONIN to effectively distinguish ID and OOD samples. Throughout extensive experiments, we demonstrate that RONIN achieves competitive results compared to previous approaches across several datasets, both in zero-shot and non-zero-shot settings.</li>
</ul>

<h3>Title: Do Diffusion Models Learn Semantically Meaningful and Efficient  Representations?</h3>
<ul>
<li><strong>Authors: </strong>Qiyao Liang, Ziming Liu, Ila Fiete</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03305">https://arxiv.org/abs/2402.03305</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03305">https://arxiv.org/pdf/2402.03305</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03305]] Do Diffusion Models Learn Semantically Meaningful and Efficient  Representations?(https://arxiv.org/abs/2402.03305)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models are capable of impressive feats of image generation with uncommon juxtapositions such as astronauts riding horses on the moon with properly placed shadows. These outputs indicate the ability to perform compositional generalization, but how do the models do so? We perform controlled experiments on conditional DDPMs learning to generate 2D spherical Gaussian bumps centered at specified $x$- and $y$-positions. Our results show that the emergence of semantically meaningful latent representations is key to achieving high performance. En route to successful performance over learning, the model traverses three distinct phases of latent representations: (phase A) no latent structure, (phase B) a 2D manifold of disordered states, and (phase C) a 2D ordered manifold. Corresponding to each of these phases, we identify qualitatively different generation behaviors: 1) multiple bumps are generated, 2) one bump is generated but at inaccurate $x$ and $y$ locations, 3) a bump is generated at the correct $x$ and y location. Furthermore, we show that even under imbalanced datasets where features ($x$- versus $y$-positions) are represented with skewed frequencies, the learning process for $x$ and $y$ is coupled rather than factorized, demonstrating that simple vanilla-flavored diffusion models cannot learn efficient representations in which localization in $x$ and $y$ are factorized into separate 1D tasks. These findings suggest the need for future work to find inductive biases that will push generative models to discover and exploit factorizable independent structures in their inputs, which will be required to vault these models into more data-efficient regimes.</li>
</ul>

<h3>Title: HASSOD: Hierarchical Adaptive Self-Supervised Object Detection</h3>
<ul>
<li><strong>Authors: </strong>Shengcao Cao, Dhiraj Joshi, Liang-Yan Gui, Yu-Xiong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.03311">https://arxiv.org/abs/2402.03311</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.03311">https://arxiv.org/pdf/2402.03311</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.03311]] HASSOD: Hierarchical Adaptive Self-Supervised Object Detection(https://arxiv.org/abs/2402.03311)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>The human visual perception system demonstrates exceptional capabilities in learning without explicit supervision and understanding the part-to-whole composition of objects. Drawing inspiration from these two abilities, we propose Hierarchical Adaptive Self-Supervised Object Detection (HASSOD), a novel approach that learns to detect objects and understand their compositions without human supervision. HASSOD employs a hierarchical adaptive clustering strategy to group regions into object masks based on self-supervised visual representations, adaptively determining the number of objects per image. Furthermore, HASSOD identifies the hierarchical levels of objects in terms of composition, by analyzing coverage relations between masks and constructing tree structures. This additional self-supervised learning task leads to improved detection performance and enhanced interpretability. Lastly, we abandon the inefficient multi-round self-training process utilized in prior methods and instead adapt the Mean Teacher framework from semi-supervised learning, which leads to a smoother and more efficient training process. Through extensive experiments on prevalent image datasets, we demonstrate the superiority of HASSOD over existing methods, thereby advancing the state of the art in self-supervised object detection. Notably, we improve Mask AR from 20.2 to 22.5 on LVIS, and from 17.0 to 26.0 on SA-1B. Project page: https://HASSOD-NeurIPS23.github.io.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
