<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>diffusion</h2>
<h3>Title: Diffusion Shape Prior for Wrinkle-Accurate Cloth Registration. (arXiv:2311.05828v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.05828">http://arxiv.org/abs/2311.05828</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.05828]] Diffusion Shape Prior for Wrinkle-Accurate Cloth Registration(http://arxiv.org/abs/2311.05828)</code></li>
<li>Summary: <p>Registering clothes from 4D scans with vertex-accurate correspondence is
challenging, yet important for dynamic appearance modeling and physics
parameter estimation from real-world data. However, previous methods either
rely on texture information, which is not always reliable, or achieve only
coarse-level alignment. In this work, we present a novel approach to enabling
accurate surface registration of texture-less clothes with large deformation.
Our key idea is to effectively leverage a shape prior learned from pre-captured
clothing using diffusion models. We also propose a multi-stage guidance scheme
based on learned functional maps, which stabilizes registration for large-scale
deformation even when they vary significantly from training data. Using
high-fidelity real captured clothes, our experiments show that the proposed
approach based on diffusion models generalizes better than surface registration
with VAE or PCA-based priors, outperforming both optimization-based and
learning-based non-rigid registration methods for both interpolation and
extrapolation tests.
</p></li>
</ul>

<h3>Title: Instant3D: Fast Text-to-3D with Sparse-View Generation and Large Reconstruction Model. (arXiv:2311.06214v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06214">http://arxiv.org/abs/2311.06214</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06214]] Instant3D: Fast Text-to-3D with Sparse-View Generation and Large Reconstruction Model(http://arxiv.org/abs/2311.06214)</code></li>
<li>Summary: <p>Text-to-3D with diffusion models have achieved remarkable progress in recent
years. However, existing methods either rely on score distillation-based
optimization which suffer from slow inference, low diversity and Janus
problems, or are feed-forward methods that generate low quality results due to
the scarcity of 3D training data. In this paper, we propose Instant3D, a novel
method that generates high-quality and diverse 3D assets from text prompts in a
feed-forward manner. We adopt a two-stage paradigm, which first generates a
sparse set of four structured and consistent views from text in one shot with a
fine-tuned 2D text-to-image diffusion model, and then directly regresses the
NeRF from the generated images with a novel transformer-based sparse-view
reconstructor. Through extensive experiments, we demonstrate that our method
can generate high-quality, diverse and Janus-free 3D assets within 20 seconds,
which is two order of magnitude faster than previous optimization-based methods
that can take 1 to 10 hours. Our project webpage: https://jiahao.ai/instant3d/.
</p></li>
</ul>

<h3>Title: Diffusion Models for Earth Observation Use-cases: from cloud removal to urban change detection. (arXiv:2311.06222v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06222">http://arxiv.org/abs/2311.06222</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06222]] Diffusion Models for Earth Observation Use-cases: from cloud removal to urban change detection(http://arxiv.org/abs/2311.06222)</code></li>
<li>Summary: <p>The advancements in the state of the art of generative Artificial
Intelligence (AI) brought by diffusion models can be highly beneficial in novel
contexts involving Earth observation data. After introducing this new family of
generative models, this work proposes and analyses three use cases which
demonstrate the potential of diffusion-based approaches for satellite image
data. Namely, we tackle cloud removal and inpainting, dataset generation for
change-detection tasks, and urban replanning.
</p></li>
</ul>

<h3>Title: Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization. (arXiv:2311.06243v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06243">http://arxiv.org/abs/2311.06243</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06243]] Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization(http://arxiv.org/abs/2311.06243)</code></li>
<li>Summary: <p>Large foundation models are becoming ubiquitous, but training them from
scratch is prohibitively expensive. Thus, efficiently adapting these powerful
models to downstream tasks is increasingly important. In this paper, we study a
principled finetuning paradigm -- Orthogonal Finetuning (OFT) -- for downstream
task adaptation. Despite demonstrating good generalizability, OFT still uses a
fairly large number of trainable parameters due to the high dimensionality of
orthogonal matrices. To address this, we start by examining OFT from an
information transmission perspective, and then identify a few key desiderata
that enable better parameter-efficiency. Inspired by how the Cooley-Tukey fast
Fourier transform algorithm enables efficient information transmission, we
propose an efficient orthogonal parameterization using butterfly structures. We
apply this parameterization to OFT, creating a novel parameter-efficient
finetuning method, called Orthogonal Butterfly (BOFT). By subsuming OFT as a
special case, BOFT introduces a generalized orthogonal finetuning framework.
Finally, we conduct an extensive empirical study of adapting large vision
transformers, large language models, and text-to-image diffusion models to
various downstream tasks in vision and language.
</p></li>
</ul>

<h2>self-supervised</h2>
<h3>Title: OmniVec: Learning robust representations with cross modal sharing. (arXiv:2311.05709v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.05709">http://arxiv.org/abs/2311.05709</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.05709]] OmniVec: Learning robust representations with cross modal sharing(http://arxiv.org/abs/2311.05709)</code></li>
<li>Summary: <p>Majority of research in learning based methods has been towards designing and
training networks for specific tasks. However, many of the learning based
tasks, across modalities, share commonalities and could be potentially tackled
in a joint framework. We present an approach in such direction, to learn
multiple tasks, in multiple modalities, with a unified architecture. The
proposed network is composed of task specific encoders, a common trunk in the
middle, followed by task specific prediction heads. We first pre-train it by
self-supervised masked training, followed by sequential training for the
different tasks. We train the network on all major modalities, e.g.\ visual,
audio, text and 3D, and report results on $22$ diverse and challenging public
benchmarks. We demonstrate empirically that, using a joint network to train
across modalities leads to meaningful information sharing and this allows us to
achieve state-of-the-art results on most of the benchmarks. We also show
generalization of the trained network on cross-modal tasks as well as unseen
datasets and tasks.
</p></li>
</ul>

<h3>Title: MonoProb: Self-Supervised Monocular Depth Estimation with Interpretable Uncertainty. (arXiv:2311.06137v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06137">http://arxiv.org/abs/2311.06137</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06137]] MonoProb: Self-Supervised Monocular Depth Estimation with Interpretable Uncertainty(http://arxiv.org/abs/2311.06137)</code></li>
<li>Summary: <p>Self-supervised monocular depth estimation methods aim to be used in critical
applications such as autonomous vehicles for environment analysis. To
circumvent the potential imperfections of these approaches, a quantification of
the prediction confidence is crucial to guide decision-making systems that rely
on depth estimation. In this paper, we propose MonoProb, a new unsupervised
monocular depth estimation method that returns an interpretable uncertainty,
which means that the uncertainty reflects the expected error of the network in
its depth predictions. We rethink the stereo or the structure-from-motion
paradigms used to train unsupervised monocular depth models as a probabilistic
problem. Within a single forward pass inference, this model provides a depth
prediction and a measure of its confidence, without increasing the inference
time. We then improve the performance on depth and uncertainty with a novel
self-distillation loss for which a student is supervised by a pseudo ground
truth that is a probability distribution on depth output by a teacher. To
quantify the performance of our models we design new metrics that, unlike
traditional ones, measure the absolute performance of uncertainty predictions.
Our experiments highlight enhancements achieved by our method on standard depth
and uncertainty metrics as well as on our tailored metrics.
https://github.com/CEA-LIST/MonoProb
</p></li>
</ul>

<h2>foundation model</h2>
<h3>Title: Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks. (arXiv:2311.06242v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06242">http://arxiv.org/abs/2311.06242</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06242]] Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks(http://arxiv.org/abs/2311.06242)</code></li>
<li>Summary: <p>We introduce Florence-2, a novel vision foundation model with a unified,
prompt-based representation for a variety of computer vision and
vision-language tasks. While existing large vision models excel in transfer
learning, they struggle to perform a diversity of tasks with simple
instructions, a capability that implies handling the complexity of various
spatial hierarchy and semantic granularity. Florence-2 was designed to take
text-prompt as task instructions and generate desirable results in text forms,
whether it be captioning, object detection, grounding or segmentation. This
multi-task learning setup demands large-scale, high-quality annotated data. To
this end, we co-developed FLD-5B that consists of 5.4 billion comprehensive
visual annotations on 126 million images, using an iterative strategy of
automated image annotation and model refinement. We adopted a
sequence-to-sequence structure to train Florence-2 to perform versatile and
comprehensive vision tasks. Extensive evaluations on numerous tasks
demonstrated Florence-2 to be a strong vision foundation model contender with
unprecedented zero-shot and fine-tuning capabilities.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Enhancing Rock Image Segmentation in Digital Rock Physics: A Fusion of Generative AI and State-of-the-Art Neural Networks. (arXiv:2311.06079v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06079">http://arxiv.org/abs/2311.06079</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06079]] Enhancing Rock Image Segmentation in Digital Rock Physics: A Fusion of Generative AI and State-of-the-Art Neural Networks(http://arxiv.org/abs/2311.06079)</code></li>
<li>Summary: <p>In digital rock physics, analysing microstructures from CT and SEM scans is
crucial for estimating properties like porosity and pore connectivity.
Traditional segmentation methods like thresholding and CNNs often fall short in
accurately detailing rock microstructures and are prone to noise. U-Net
improved segmentation accuracy but required many expert-annotated samples, a
laborious and error-prone process due to complex pore shapes. Our study
employed an advanced generative AI model, the diffusion model, to overcome
these limitations. This model generated a vast dataset of CT/SEM and binary
segmentation pairs from a small initial dataset. We assessed the efficacy of
three neural networks: U-Net, Attention-U-net, and TransUNet, for segmenting
these enhanced images. The diffusion model proved to be an effective data
augmentation technique, improving the generalization and robustness of deep
learning models. TransU-Net, incorporating Transformer structures, demonstrated
superior segmentation accuracy and IoU metrics, outperforming both U-Net and
Attention-U-net. Our research advances rock image segmentation by combining the
diffusion model with cutting-edge neural networks, reducing dependency on
extensive expert data and boosting segmentation accuracy and robustness.
TransU-Net sets a new standard in digital rock physics, paving the way for
future geoscience and engineering breakthroughs.
</p></li>
</ul>

<h3>Title: FinGPT: Large Generative Models for a Small Language. (arXiv:2311.05640v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.05640">http://arxiv.org/abs/2311.05640</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.05640]] FinGPT: Large Generative Models for a Small Language(http://arxiv.org/abs/2311.05640)</code></li>
<li>Summary: <p>Large language models (LLMs) excel in many tasks in NLP and beyond, but most
open models have very limited coverage of smaller languages and LLM work tends
to focus on languages where nearly unlimited data is available for pretraining.
In this work, we study the challenges of creating LLMs for Finnish, a language
spoken by less than 0.1% of the world population. We compile an extensive
dataset of Finnish combining web crawls, news, social media and eBooks. We
pursue two approaches to pretrain models: 1) we train seven monolingual models
from scratch (186M to 13B parameters) dubbed FinGPT, 2) we continue the
pretraining of the multilingual BLOOM model on a mix of its original training
data and Finnish, resulting in a 176 billion parameter model we call BLUUMI.
For model evaluation, we introduce FIN-bench, a version of BIG-bench with
Finnish tasks. We also assess other model qualities such as toxicity and bias.
Our models and tools are openly available at https://turkunlp.org/gpt3-finnish.
</p></li>
</ul>

<h3>Title: Syntax-semantics interface: an algebraic model. (arXiv:2311.06189v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06189">http://arxiv.org/abs/2311.06189</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06189]] Syntax-semantics interface: an algebraic model(http://arxiv.org/abs/2311.06189)</code></li>
<li>Summary: <p>We extend our formulation of Merge and Minimalism in terms of Hopf algebras
to an algebraic model of a syntactic-semantic interface. We show that methods
adopted in the formulation of renormalization (extraction of meaningful
physical values) in theoretical physics are relevant to describe the extraction
of meaning from syntactic expressions. We show how this formulation relates to
computational models of semantics and we answer some recent controversies about
implications for generative linguistics of the current functioning of large
language models.
</p></li>
</ul>

<h3>Title: BanglaBait: Semi-Supervised Adversarial Approach for Clickbait Detection on Bangla Clickbait Dataset. (arXiv:2311.06204v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06204">http://arxiv.org/abs/2311.06204</a></li>
<li>Code URL: https://github.com/mdmotaharmahtab/banglabait</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06204]] BanglaBait: Semi-Supervised Adversarial Approach for Clickbait Detection on Bangla Clickbait Dataset(http://arxiv.org/abs/2311.06204)</code></li>
<li>Summary: <p>Intentionally luring readers to click on a particular content by exploiting
their curiosity defines a title as clickbait. Although several studies focused
on detecting clickbait titles in English articles, low resource language like
Bangla has not been given adequate attention. To tackle clickbait titles in
Bangla, we have constructed the first Bangla clickbait detection dataset
containing 15,056 labeled news articles and 65,406 unlabelled news articles
extracted from clickbait dense news sites. Each article has been labeled by
three expert linguists and includes an article's title, body, and other
metadata. By incorporating labeled and unlabelled data, we finetune a
pretrained Bangla transformer model in an adversarial fashion using Semi
Supervised Generative Adversarial Networks (SS GANs). The proposed model acts
as a good baseline for this dataset, outperforming traditional neural network
models (LSTM, GRU, CNN) and linguistic feature based models. We expect that
this dataset and the detailed analysis and comparison of these clickbait
detection models will provide a fundamental basis for future research into
detecting clickbait titles in Bengali articles. We have released the
corresponding code and dataset.
</p></li>
</ul>

<h3>Title: Generative Explanations for Graph Neural Network: Methods and Evaluations. (arXiv:2311.05764v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.05764">http://arxiv.org/abs/2311.05764</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.05764]] Generative Explanations for Graph Neural Network: Methods and Evaluations(http://arxiv.org/abs/2311.05764)</code></li>
<li>Summary: <p>Graph Neural Networks (GNNs) achieve state-of-the-art performance in various
graph-related tasks. However, the black-box nature often limits their
interpretability and trustworthiness. Numerous explainability methods have been
proposed to uncover the decision-making logic of GNNs, by generating underlying
explanatory substructures. In this paper, we conduct a comprehensive review of
the existing explanation methods for GNNs from the perspective of graph
generation. Specifically, we propose a unified optimization objective for
generative explanation methods, comprising two sub-objectives: Attribution and
Information constraints. We further demonstrate their specific manifestations
in various generative model architectures and different explanation scenarios.
With the unified objective of the explanation problem, we reveal the shared
characteristics and distinctions among current methods, laying the foundation
for future methodological advancements. Empirical results demonstrate the
advantages and limitations of different explainability approaches in terms of
explanation performance, efficiency, and generalizability.
</p></li>
</ul>

<h3>Title: Scale-MIA: A Scalable Model Inversion Attack against Secure Federated Learning via Latent Space Reconstruction. (arXiv:2311.05808v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.05808">http://arxiv.org/abs/2311.05808</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.05808]] Scale-MIA: A Scalable Model Inversion Attack against Secure Federated Learning via Latent Space Reconstruction(http://arxiv.org/abs/2311.05808)</code></li>
<li>Summary: <p>Federated learning is known for its capability to safeguard participants'
data privacy. However, recently emerged model inversion attacks (MIAs) have
shown that a malicious parameter server can reconstruct individual users' local
data samples through model updates. The state-of-the-art attacks either rely on
computation-intensive search-based optimization processes to recover each input
batch, making scaling difficult, or they involve the malicious parameter server
adding extra modules before the global model architecture, rendering the
attacks too conspicuous and easily detectable.
</p>
<p>To overcome these limitations, we propose Scale-MIA, a novel MIA capable of
efficiently and accurately recovering training samples of clients from the
aggregated updates, even when the system is under the protection of a robust
secure aggregation protocol. Unlike existing approaches treating models as
black boxes, Scale-MIA recognizes the importance of the intricate architecture
and inner workings of machine learning models. It identifies the latent space
as the critical layer for breaching privacy and decomposes the complex recovery
task into an innovative two-step process to reduce computation complexity. The
first step involves reconstructing the latent space representations (LSRs) from
the aggregated model updates using a closed-form inversion mechanism,
leveraging specially crafted adversarial linear layers. In the second step, the
whole input batches are recovered from the LSRs by feeding them into a
fine-tuned generative decoder.
</p>
<p>We implemented Scale-MIA on multiple commonly used machine learning models
and conducted comprehensive experiments across various settings. The results
demonstrate that Scale-MIA achieves excellent recovery performance on different
datasets, exhibiting high reconstruction rates, accuracy, and attack efficiency
on a larger scale compared to state-of-the-art MIAs.
</p></li>
</ul>

<h3>Title: Testing Dependency of Unlabeled Databases. (arXiv:2311.05874v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.05874">http://arxiv.org/abs/2311.05874</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.05874]] Testing Dependency of Unlabeled Databases(http://arxiv.org/abs/2311.05874)</code></li>
<li>Summary: <p>In this paper, we investigate the problem of deciding whether two random
databases $\mathsf{X}\in\mathcal{X}^{n\times d}$ and
$\mathsf{Y}\in\mathcal{Y}^{n\times d}$ are statistically dependent or not. This
is formulated as a hypothesis testing problem, where under the null hypothesis,
these two databases are statistically independent, while under the alternative,
there exists an unknown row permutation $\sigma$, such that $\mathsf{X}$ and
$\mathsf{Y}^\sigma$, a permuted version of $\mathsf{Y}$, are statistically
dependent with some known joint distribution, but have the same marginal
distributions as the null. We characterize the thresholds at which optimal
testing is information-theoretically impossible and possible, as a function of
$n$, $d$, and some spectral properties of the generative distributions of the
datasets. For example, we prove that if a certain function of the eigenvalues
of the likelihood function and $d$, is below a certain threshold, as
$d\to\infty$, then weak detection (performing slightly better than random
guessing) is statistically impossible, no matter what the value of $n$ is. This
mimics the performance of an efficient test that thresholds a centered version
of the log-likelihood function of the observed matrices. We also analyze the
case where $d$ is fixed, for which we derive strong (vanishing error) and weak
detection lower and upper bounds.
</p></li>
</ul>

<h2>anomaly</h2>
<h3>Title: Interpretable Graph Anomaly Detection using Gradient Attention Maps. (arXiv:2311.06153v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.06153">http://arxiv.org/abs/2311.06153</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.06153]] Interpretable Graph Anomaly Detection using Gradient Attention Maps(http://arxiv.org/abs/2311.06153)</code></li>
<li>Summary: <p>Detecting unusual patterns in graph data is a crucial task in data mining.
However, existing methods often face challenges in consistently achieving
satisfactory performance and lack interpretability, which hinders our
understanding of anomaly detection decisions. In this paper, we propose a novel
approach to graph anomaly detection that leverages the power of
interpretability to enhance performance. Specifically, our method extracts an
attention map derived from gradients of graph neural networks, which serves as
a basis for scoring anomalies. In addition, we conduct theoretical analysis
using synthetic data to validate our method and gain insights into its
decision-making process. To demonstrate the effectiveness of our method, we
extensively evaluate our approach against state-of-the-art graph anomaly
detection techniques. The results consistently demonstrate the superior
performance of our method compared to the baselines.
</p></li>
</ul>

<h2>in-context</h2>
<h3>Title: Deep Natural Language Feature Learning for Interpretable Prediction. (arXiv:2311.05754v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.05754">http://arxiv.org/abs/2311.05754</a></li>
<li>Code URL: https://github.com/furrutiav/nllf-emnlp-2023</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.05754]] Deep Natural Language Feature Learning for Interpretable Prediction(http://arxiv.org/abs/2311.05754)</code></li>
<li>Summary: <p>We propose a general method to break down a main complex task into a set of
intermediary easier sub-tasks, which are formulated in natural language as
binary questions related to the final target task. Our method allows for
representing each example by a vector consisting of the answers to these
questions. We call this representation Natural Language Learned Features
(NLLF). NLLF is generated by a small transformer language model (e.g., BERT)
that has been trained in a Natural Language Inference (NLI) fashion, using weak
labels automatically obtained from a Large Language Model (LLM). We show that
the LLM normally struggles for the main task using in-context learning, but can
handle these easiest subtasks and produce useful weak labels to train a BERT.
The NLI-like training of the BERT allows for tackling zero-shot inference with
any binary question, and not necessarily the ones seen during the training. We
show that this NLLF vector not only helps to reach better performances by
enhancing any classifier, but that it can be used as input of an
easy-to-interpret machine learning model like a decision tree. This decision
tree is interpretable but also reaches high performances, surpassing those of a
pre-trained transformer in some cases.We have successfully applied this method
to two completely different tasks: detecting incoherence in students' answers
to open-ended mathematics exam questions, and screening abstracts for a
systematic literature review of scientific papers on climate change and
agroecology.
</p></li>
</ul>

<h3>Title: Chain of Thought with Explicit Evidence Reasoning for Few-shot Relation Extraction. (arXiv:2311.05922v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.05922">http://arxiv.org/abs/2311.05922</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.05922]] Chain of Thought with Explicit Evidence Reasoning for Few-shot Relation Extraction(http://arxiv.org/abs/2311.05922)</code></li>
<li>Summary: <p>Few-shot relation extraction involves identifying the type of relationship
between two specific entities within a text, using a limited number of
annotated samples. A variety of solutions to this problem have emerged by
applying meta-learning and neural graph techniques which typically necessitate
a training process for adaptation. Recently, the strategy of in-context
learning has been demonstrating notable results without the need of training.
Few studies have already utilized in-context learning for zero-shot information
extraction. Unfortunately, the evidence for inference is either not considered
or implicitly modeled during the construction of chain-of-thought prompts. In
this paper, we propose a novel approach for few-shot relation extraction using
large language models, named CoT-ER, chain-of-thought with explicit evidence
reasoning. In particular, CoT-ER first induces large language models to
generate evidences using task-specific and concept-level knowledge. Then these
evidences are explicitly incorporated into chain-of-thought prompting for
relation extraction. Experimental results demonstrate that our CoT-ER approach
(with 0% training data) achieves competitive performance compared to the
fully-supervised (with 100% training data) state-of-the-art approach on the
FewRel1.0 and FewRel2.0 datasets.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
