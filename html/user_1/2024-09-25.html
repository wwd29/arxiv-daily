<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-09-25</h1>
<h3>Title: Global Context Enhanced Anomaly Detection of Cyber Attacks via Decoupled Graph Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Ahmad Hafez</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15304">https://arxiv.org/abs/2409.15304</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15304">https://arxiv.org/pdf/2409.15304</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15304]] Global Context Enhanced Anomaly Detection of Cyber Attacks via Decoupled Graph Neural Networks(https://arxiv.org/abs/2409.15304)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Recently, there has been a substantial amount of interest in GNN-based anomaly detection. Existing efforts have focused on simultaneously mastering the node representations and the classifier necessary for identifying abnormalities with relatively shallow models to create an embedding. Therefore, the existing state-of-the-art models are incapable of capturing nonlinear network information and producing suboptimal outcomes. In this thesis, we deploy decoupled GNNs to overcome this issue. Specifically, we decouple the essential node representations and classifier for detecting anomalies. In addition, for node representation learning, we develop a GNN architecture with two modules for aggregating node feature information to produce the final node embedding. Finally, we conduct empirical experiments to verify the effectiveness of our proposed approach. The findings demonstrate that decoupled training along with the global context enhanced representation of the nodes is superior to the state-of-the-art models in terms of AUC and introduces a novel way of capturing the node information.</li>
</ul>

<h3>Title: Visual Prompting in Multimodal Large Language Models: A Survey</h3>
<ul>
<li><strong>Authors: </strong>Junda Wu, Zhehao Zhang, Yu Xia, Xintong Li, Zhaoyang Xia, Aaron Chang, Tong Yu, Sungchul Kim, Ryan A. Rossi, Ruiyi Zhang, Subrata Mitra, Dimitris N. Metaxas, Lina Yao, Jingbo Shang, Julian McAuley</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15310">https://arxiv.org/abs/2409.15310</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15310">https://arxiv.org/pdf/2409.15310</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15310]] Visual Prompting in Multimodal Large Language Models: A Survey(https://arxiv.org/abs/2409.15310)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative, in-context</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) equip pre-trained large-language models (LLMs) with visual capabilities. While textual prompting in LLMs has been widely studied, visual prompting has emerged for more fine-grained and free-form visual instructions. This paper presents the first comprehensive survey on visual prompting methods in MLLMs, focusing on visual prompting, prompt generation, compositional reasoning, and prompt learning. We categorize existing visual prompts and discuss generative methods for automatic prompt annotations on the images. We also examine visual prompting methods that enable better alignment between visual encoders and backbone LLMs, concerning MLLM's visual grounding, object referring, and compositional reasoning abilities. In addition, we provide a summary of model training and in-context learning methods to improve MLLM's perception and understanding of visual prompts. This paper examines visual prompting methods developed in MLLMs and provides a vision of the future of these methods.</li>
</ul>

<h3>Title: Electrooptical Image Synthesis from SAR Imagery Using Generative Adversarial Networks</h3>
<ul>
<li><strong>Authors: </strong>Grant Rosario, David Noever</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15331">https://arxiv.org/abs/2409.15331</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15331">https://arxiv.org/pdf/2409.15331</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15331]] Electrooptical Image Synthesis from SAR Imagery Using Generative Adversarial Networks(https://arxiv.org/abs/2409.15331)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The utility of Synthetic Aperture Radar (SAR) imagery in remote sensing and satellite image analysis is well established, offering robustness under various weather and lighting conditions. However, SAR images, characterized by their unique structural and texture characteristics, often pose interpretability challenges for analysts accustomed to electrooptical (EO) imagery. This application compares state-of-the-art Generative Adversarial Networks (GANs) including Pix2Pix, CycleGan, S-CycleGan, and a novel dual?generator GAN utilizing partial convolutions and a novel dual-generator architecture utilizing transformers. These models are designed to progressively refine the realism in the translated optical images, thereby enhancing the visual interpretability of SAR data. We demonstrate the efficacy of our approach through qualitative and quantitative evaluations, comparing the synthesized EO images with actual EO images in terms of visual fidelity and feature preservation. The results show significant improvements in interpretability, making SAR data more accessible for analysts familiar with EO imagery. Furthermore, we explore the potential of this technology in various applications, including environmental monitoring, urban planning, and military reconnaissance, where rapid, accurate interpretation of SAR data is crucial. Our research contributes to the field of remote sensing by bridging the gap between SAR and EO imagery, offering a novel tool for enhanced data interpretation and broader application of SAR technology in various domains.</li>
</ul>

<h3>Title: Watch Your Steps: Observable and Modular Chains of Thought</h3>
<ul>
<li><strong>Authors: </strong>Cassandra A. Cohen, William W. Cohen</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15359">https://arxiv.org/abs/2409.15359</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15359">https://arxiv.org/pdf/2409.15359</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15359]] Watch Your Steps: Observable and Modular Chains of Thought(https://arxiv.org/abs/2409.15359)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>We propose a variant of chain of thought (CoT) prompting called Program Trace Prompting that makes explanations more observable while preserving the power, generality and flexibility of CoT. In our approach, few-shot CoT demonstrations are wrapped in a formal syntax based on Python, and each prompt: identifies and names steps; defines the input/output behavior of steps; and replaces CoT explanations of in-context examples with chains of these formalized steps on the same examples. Program Trace Prompting is applicable to many tasks, achieving strong results on the 23 diverse tasks in the BIG-Bench Hard benchmark. More importantly, by instrumenting explanations in this way, we enable new types of analysis. In particular, we identify "non-local errors" (which correspond to incorrectly learning the reasoning method illustrated in the demonstrations) as an unaddressed issue in CoT learning, and we present methods for verifying the modularity of steps in a CoT explanation.</li>
</ul>

<h3>Title: Trajectory Anomaly Detection with Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jonathan Mbuya, Dieter Pfoser, Antonios Anastasopoulos</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15366">https://arxiv.org/abs/2409.15366</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15366">https://arxiv.org/pdf/2409.15366</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15366]] Trajectory Anomaly Detection with Language Models(https://arxiv.org/abs/2409.15366)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>This paper presents a novel approach for trajectory anomaly detection using an autoregressive causal-attention model, termed LM-TAD. This method leverages the similarities between language statements and trajectories, both of which consist of ordered elements requiring coherence through external rules and contextual variations. By treating trajectories as sequences of tokens, our model learns the probability distributions over trajectories, enabling the identification of anomalous locations with high precision. We incorporate user-specific tokens to account for individual behavior patterns, enhancing anomaly detection tailored to user context. Our experiments demonstrate the effectiveness of LM-TAD on both synthetic and real-world datasets. In particular, the model outperforms existing methods on the Pattern of Life (PoL) dataset by detecting user-contextual anomalies and achieves competitive results on the Porto taxi dataset, highlighting its adaptability and robustness. Additionally, we introduce the use of perplexity and surprisal rate metrics for detecting outliers and pinpointing specific anomalous locations within trajectories. The LM-TAD framework supports various trajectory representations, including GPS coordinates, staypoints, and activity types, proving its versatility in handling diverse trajectory data. Moreover, our approach is well-suited for online trajectory anomaly detection, significantly reducing computational latency by caching key-value states of the attention mechanism, thereby avoiding repeated computations.</li>
</ul>

<h3>Title: Fine-Tuning a Time Series Foundation Model with Wasserstein Loss</h3>
<ul>
<li><strong>Authors: </strong>Andrei Chernov</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15367">https://arxiv.org/abs/2409.15367</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15367">https://arxiv.org/pdf/2409.15367</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15367]] Fine-Tuning a Time Series Foundation Model with Wasserstein Loss(https://arxiv.org/abs/2409.15367)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Inspired by recent advancements in large language models (LLMs) for Natural Language Processing (NLP), there has been a surge in research focused on developing foundational models for time series forecasting. One approach involves training LLM architectures on tokenized time series data using cross-entropy loss. Although this method has demonstrated promising results, cross-entropy loss is primarily designed for classification tasks and does not account for the distance between classes. To address this limitation, we propose using the Wasserstein loss for such architectures. To validate our approach, we fine-tuned a foundational time series model on $22$ zero-shot datasets, comparing the performance of cross-entropy loss with that of Wasserstein loss. Our results demonstrate that replacing cross-entropy loss with Wasserstein loss significantly improves point estimation.</li>
</ul>

<h3>Title: MedCodER: A Generative AI Assistant for Medical Coding</h3>
<ul>
<li><strong>Authors: </strong>Krishanu Das Baksi, Elijah Soba, John J. Higgins, Ravi Saini, Jaden Wood, Jane Cook, Jack Scott, Nirmala Pudota, Tim Weninger, Edward Bowen, Sanmitra Bhattacharya</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.ET, cs.IR, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15368">https://arxiv.org/abs/2409.15368</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15368">https://arxiv.org/pdf/2409.15368</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15368]] MedCodER: A Generative AI Assistant for Medical Coding(https://arxiv.org/abs/2409.15368)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Medical coding is essential for standardizing clinical data and communication but is often time-consuming and prone to errors. Traditional Natural Language Processing (NLP) methods struggle with automating coding due to the large label space, lengthy text inputs, and the absence of supporting evidence annotations that justify code selection. Recent advancements in Generative Artificial Intelligence (AI) offer promising solutions to these challenges. In this work, we introduce MedCodER, a Generative AI framework for automatic medical coding that leverages extraction, retrieval, and re-ranking techniques as core components. MedCodER achieves a micro-F1 score of 0.60 on International Classification of Diseases (ICD) code prediction, significantly outperforming state-of-the-art methods. Additionally, we present a new dataset containing medical records annotated with disease diagnoses, ICD codes, and supporting evidence texts (this https URL). Ablation tests confirm that MedCodER's performance depends on the integration of each of its aforementioned components, as performance declines when these components are evaluated in isolation.</li>
</ul>

<h3>Title: Smirk: An Atomically Complete Tokenizer for Molecular Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Alexius Wadell, Anoushka Bhutani, Venkatasubramanian Viswanathan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.chem-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15370">https://arxiv.org/abs/2409.15370</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15370">https://arxiv.org/pdf/2409.15370</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15370]] Smirk: An Atomically Complete Tokenizer for Molecular Foundation Models(https://arxiv.org/abs/2409.15370)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Molecular Foundation Models are emerging as powerful tools for accelerating molecular design, material science, and cheminformatics, leveraging transformer architectures to speed up the discovery of new materials and drugs while reducing the computational cost of traditional ab initio methods. However, current models are constrained by closed-vocabulary tokenizers that fail to capture the full diversity of molecular structures. In this work, we systematically evaluate thirteen chemistry-specific tokenizers for their coverage of the SMILES language, uncovering substantial gaps. Using N-gram language models, we accessed the impact of tokenizer choice on model performance and quantified the information loss of unknown tokens. We introduce two new tokenizers, <i>smirk</i> and <i>smirk-gpe</i>, which can represent the entirety of the OpenSMILES specification while avoiding the pitfalls of existing tokenizers. Our work highlights the importance of open-vocabulary modeling for molecular foundation models and the need for chemically diverse benchmarks for cheminformatics.</li>
</ul>

<h3>Title: Prompting Large Language Models for Supporting the Differential Diagnosis of Anemia</h3>
<ul>
<li><strong>Authors: </strong>Elisa Castagnari (HeKA), Lillian Muyama (HeKA), Adrien Coulet (HeKA)</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15377">https://arxiv.org/abs/2409.15377</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15377">https://arxiv.org/pdf/2409.15377</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15377]] Prompting Large Language Models for Supporting the Differential Diagnosis of Anemia(https://arxiv.org/abs/2409.15377)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In practice, clinicians achieve a diagnosis by following a sequence of steps, such as laboratory exams, observations, or imaging. The pathways to reach diagnosis decisions are documented by guidelines authored by expert organizations, which guide clinicians to reach a correct diagnosis through these sequences of steps. While these guidelines are beneficial for following medical reasoning and consolidating medical knowledge, they have some drawbacks. They often fail to address patients with uncommon conditions due to their focus on the majority population, and are slow and costly to update, making them unsuitable for rapidly emerging diseases or new practices. Inspired by clinical guidelines, our study aimed to develop pathways similar to those that can be obtained in clinical guidelines. We tested three Large Language Models (LLMs) -Generative Pretrained Transformer 4 (GPT-4), Large Language Model Meta AI (LLaMA), and Mistral -on a synthetic yet realistic dataset to differentially diagnose anemia and its subtypes. By using advanced prompting techniques to enhance the decision-making process, we generated diagnostic pathways using these models. Experimental results indicate that LLMs hold huge potential in clinical pathway discovery from patient data, with GPT-4 exhibiting the best performance in all conducted experiments.</li>
</ul>

<h3>Title: Parse Trees Guided LLM Prompt Compression</h3>
<ul>
<li><strong>Authors: </strong>Wenhao Mao, Chengbin Hou, Tianyu Zhang, Xinyu Lin, Ke Tang, Hairong Lv</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15395">https://arxiv.org/abs/2409.15395</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15395">https://arxiv.org/pdf/2409.15395</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15395]] Parse Trees Guided LLM Prompt Compression(https://arxiv.org/abs/2409.15395)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Offering rich contexts to Large Language Models (LLMs) has shown to boost the performance in various tasks, but the resulting longer prompt would increase the computational cost and might exceed the input limit of LLMs. Recently, some prompt compression methods have been suggested to shorten the length of prompts by using language models to generate shorter prompts or by developing computational models to select important parts of original prompt. The generative compression methods would suffer from issues like hallucination, while the selective compression methods have not involved linguistic rules and overlook the global structure of prompt. To this end, we propose a novel selective compression method called PartPrompt. It first obtains a parse tree for each sentence based on linguistic rules, and calculates local information entropy for each node in a parse tree. These local parse trees are then organized into a global tree according to the hierarchical structure such as the dependency of sentences, paragraphs, and sections. After that, the root-ward propagation and leaf-ward propagation are proposed to adjust node values over the global tree. Finally, a recursive algorithm is developed to prune the global tree based on the adjusted node values. The experiments show that PartPrompt receives the state-of-the-art performance across various datasets, metrics, compression ratios, and target LLMs for inference. The in-depth ablation studies confirm the effectiveness of designs in PartPrompt, and other additional experiments also demonstrate its superiority in terms of the coherence of compressed prompts and in the extreme long prompt scenario.</li>
</ul>

<h3>Title: Attack Atlas: A Practitioner's Perspective on Challenges and Pitfalls in Red Teaming GenAI</h3>
<ul>
<li><strong>Authors: </strong>Ambrish Rawat, Stefan Schoepf, Giulio Zizzo, Giandomenico Cornacchia, Muhammad Zaid Hameed, Kieran Fraser, Erik Miehling, Beat Buesser, Elizabeth M. Daly, Mark Purcell, Prasanna Sattigeri, Pin-Yu Chen, Kush R. Varshney</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15398">https://arxiv.org/abs/2409.15398</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15398">https://arxiv.org/pdf/2409.15398</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15398]] Attack Atlas: A Practitioner's Perspective on Challenges and Pitfalls in Red Teaming GenAI(https://arxiv.org/abs/2409.15398)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>As generative AI, particularly large language models (LLMs), become increasingly integrated into production applications, new attack surfaces and vulnerabilities emerge and put a focus on adversarial threats in natural language and multi-modal systems. Red-teaming has gained importance in proactively identifying weaknesses in these systems, while blue-teaming works to protect against such adversarial attacks. Despite growing academic interest in adversarial risks for generative AI, there is limited guidance tailored for practitioners to assess and mitigate these challenges in real-world environments. To address this, our contributions include: (1) a practical examination of red- and blue-teaming strategies for securing generative AI, (2) identification of key challenges and open questions in defense development and evaluation, and (3) the Attack Atlas, an intuitive framework that brings a practical approach to analyzing single-turn input attacks, placing it at the forefront for practitioners. This work aims to bridge the gap between academic insights and practical security measures for the protection of generative AI systems.</li>
</ul>

<h3>Title: Revealing an Unattractivity Bias in Mental Reconstruction of Occluded Faces using Generative Image Models</h3>
<ul>
<li><strong>Authors: </strong>Frederik Riedmann, Bernhard Egger, Tim Rohe</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15443">https://arxiv.org/abs/2409.15443</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15443">https://arxiv.org/pdf/2409.15443</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15443]] Revealing an Unattractivity Bias in Mental Reconstruction of Occluded Faces using Generative Image Models(https://arxiv.org/abs/2409.15443)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Previous studies have shown that faces are rated as more attractive when they are partially occluded. The cause of this observation remains unclear. One explanation is a mental reconstruction of the occluded face parts which is biased towards a more attractive percept as shown in face-attractiveness rating tasks. We aimed to test for this hypothesis by using a delayed matching-to-sample task, which directly requires mental reconstruction. In two online experiments, we presented observers with unattractive, neutral or attractive synthetic reconstructions of the occluded face parts using a state-of-the-art diffusion-based image generator. Our experiments do not support the initial hypothesis and reveal an unattractiveness bias for occluded faces instead. This suggests that facial attractiveness rating tasks do not prompt reconstructions. Rather, the attractivity bias may arise from global image features, and faces may actually be reconstructed with unattractive properties when mental reconstruction is applied.</li>
</ul>

<h3>Title: In-Context Learning May Not Elicit Trustworthy Reasoning: A-Not-B Errors in Pretrained Language Models</h3>
<ul>
<li><strong>Authors: </strong>Pengrui Han, Peiyang Song, Haofei Yu, Jiaxuan You</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15454">https://arxiv.org/abs/2409.15454</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15454">https://arxiv.org/pdf/2409.15454</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15454]] In-Context Learning May Not Elicit Trustworthy Reasoning: A-Not-B Errors in Pretrained Language Models(https://arxiv.org/abs/2409.15454)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Recent advancements in artificial intelligence have led to the creation of highly capable large language models (LLMs) that can perform tasks in a human-like manner. However, LLMs exhibit only infant-level cognitive abilities in certain areas. One such area is the A-Not-B error, a phenomenon seen in infants where they repeat a previously rewarded behavior despite well-observed changed conditions. This highlights their lack of inhibitory control -- the ability to stop a habitual or impulsive response. In our work, we design a text-based multi-choice QA scenario similar to the A-Not-B experimental settings to systematically test the inhibitory control abilities of LLMs. We found that state-of-the-art LLMs (like Llama3-8b) perform consistently well with in-context learning (ICL) but make errors and show a significant drop of as many as 83.3% in reasoning tasks when the context changes trivially. This suggests that LLMs only have inhibitory control abilities on par with human infants in this regard, often failing to suppress the previously established response pattern during ICL.</li>
</ul>

<h3>Title: MediConfusion: Can you trust your AI radiologist? Probing the reliability of multimodal medical foundation models</h3>
<ul>
<li><strong>Authors: </strong>Mohammad Shahab Sepehri, Zalan Fabian, Maryam Soltanolkotabi, Mahdi Soltanolkotabi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15477">https://arxiv.org/abs/2409.15477</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15477">https://arxiv.org/pdf/2409.15477</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15477]] MediConfusion: Can you trust your AI radiologist? Probing the reliability of multimodal medical foundation models(https://arxiv.org/abs/2409.15477)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Multimodal Large Language Models (MLLMs) have tremendous potential to improve the accuracy, availability, and cost-effectiveness of healthcare by providing automated solutions or serving as aids to medical professionals. Despite promising first steps in developing medical MLLMs in the past few years, their capabilities and limitations are not well-understood. Recently, many benchmark datasets have been proposed that test the general medical knowledge of such models across a variety of medical areas. However, the systematic failure modes and vulnerabilities of such models are severely underexplored with most medical benchmarks failing to expose the shortcomings of existing models in this safety-critical domain. In this paper, we introduce MediConfusion, a challenging medical Visual Question Answering (VQA) benchmark dataset, that probes the failure modes of medical MLLMs from a vision perspective. We reveal that state-of-the-art models are easily confused by image pairs that are otherwise visually dissimilar and clearly distinct for medical experts. Strikingly, all available models (open-source or proprietary) achieve performance below random guessing on MediConfusion, raising serious concerns about the reliability of existing medical MLLMs for healthcare deployment. We also extract common patterns of model failure that may help the design of a new generation of more trustworthy and reliable MLLMs in healthcare.</li>
</ul>

<h3>Title: Analysis of Human Perception in Distinguishing Real and AI-Generated Faces: An Eye-Tracking Based Study</h3>
<ul>
<li><strong>Authors: </strong>Jin Huang, Subhadra Gopalakrishnan, Trisha Mittal, Jake Zuena, Jaclyn Pytlarz</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15498">https://arxiv.org/abs/2409.15498</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15498">https://arxiv.org/pdf/2409.15498</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15498]] Analysis of Human Perception in Distinguishing Real and AI-Generated Faces: An Eye-Tracking Based Study(https://arxiv.org/abs/2409.15498)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in Artificial Intelligence have led to remarkable improvements in generating realistic human faces. While these advancements demonstrate significant progress in generative models, they also raise concerns about the potential misuse of these generated images. In this study, we investigate how humans perceive and distinguish between real and fake images. We designed a perceptual experiment using eye-tracking technology to analyze how individuals differentiate real faces from those generated by AI. Our analysis of StyleGAN-3 generated images reveals that participants can distinguish real from fake faces with an average accuracy of 76.80%. Additionally, we found that participants scrutinize images more closely when they suspect an image to be fake. We believe this study offers valuable insights into human perception of AI-generated media.</li>
</ul>

<h3>Title: Mixture of Efficient Diffusion Experts Through Automatic Interval and Sub-Network Selection</h3>
<ul>
<li><strong>Authors: </strong>Alireza Ganjdanesh, Yan Kang, Yuchen Liu, Richard Zhang, Zhe Lin, Heng Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15557">https://arxiv.org/abs/2409.15557</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15557">https://arxiv.org/pdf/2409.15557</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15557]] Mixture of Efficient Diffusion Experts Through Automatic Interval and Sub-Network Selection(https://arxiv.org/abs/2409.15557)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion probabilistic models can generate high-quality samples. Yet, their sampling process requires numerous denoising steps, making it slow and computationally intensive. We propose to reduce the sampling cost by pruning a pretrained diffusion model into a mixture of efficient experts. First, we study the similarities between pairs of denoising timesteps, observing a natural clustering, even across different datasets. This suggests that rather than having a single model for all time steps, separate models can serve as ``experts'' for their respective time intervals. As such, we separately fine-tune the pretrained model on each interval, with elastic dimensions in depth and width, to obtain experts specialized in their corresponding denoising interval. To optimize the resource usage between experts, we introduce our Expert Routing Agent, which learns to select a set of proper network configurations. By doing so, our method can allocate the computing budget between the experts in an end-to-end manner without requiring manual heuristics. Finally, with a selected configuration, we fine-tune our pruned experts to obtain our mixture of efficient experts. We demonstrate the effectiveness of our method, DiffPruning, across several datasets, LSUN-Church, LSUN-Beds, FFHQ, and ImageNet, on the Latent Diffusion Model architecture.</li>
</ul>

<h3>Title: GEM-RAG: Graphical Eigen Memories For Retrieval Augmented Generation</h3>
<ul>
<li><strong>Authors: </strong>Brendan Hogan Rappazzo, Yingheng Wang, Aaron Ferber, Carla Gomes</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15566">https://arxiv.org/abs/2409.15566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15566">https://arxiv.org/pdf/2409.15566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15566]] GEM-RAG: Graphical Eigen Memories For Retrieval Augmented Generation(https://arxiv.org/abs/2409.15566)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>The ability to form, retrieve, and reason about memories in response to stimuli serves as the cornerstone for general intelligence - shaping entities capable of learning, adaptation, and intuitive insight. Large Language Models (LLMs) have proven their ability, given the proper memories or context, to reason and respond meaningfully to stimuli. However, they are still unable to optimally encode, store, and retrieve memories - the ability to do this would unlock their full ability to operate as AI agents, and to specialize to niche domains. To remedy this, one promising area of research is Retrieval Augmented Generation (RAG), which aims to augment LLMs by providing them with rich in-context examples and information. In question-answering (QA) applications, RAG methods embed the text of interest in chunks, and retrieve the most relevant chunks for a prompt using text embeddings. Motivated by human memory encoding and retrieval, we aim to improve over standard RAG methods by generating and encoding higher-level information and tagging the chunks by their utility to answer questions. We introduce Graphical Eigen Memories For Retrieval Augmented Generation (GEM-RAG). GEM-RAG works by tagging each chunk of text in a given text corpus with LLM generated ``utility'' questions, connecting chunks in a graph based on the similarity of both their text and utility questions, and then using the eigendecomposition of the memory graph to build higher level summary nodes that capture the main themes of the text. We evaluate GEM-RAG, using both UnifiedQA and GPT-3.5 Turbo as the LLMs, with SBERT, and OpenAI's text encoders on two standard QA tasks, showing that GEM-RAG outperforms other state-of-the-art RAG methods on these tasks. We also discuss the implications of having a robust RAG system and future directions.</li>
</ul>

<h3>Title: Clinical-grade Multi-Organ Pathology Report Generation for Multi-scale Whole Slide Images via a Semantically Guided Medical Text Foundation Model</h3>
<ul>
<li><strong>Authors: </strong>Jing Wei Tan, SeungKyu Kim, Eunsu Kim, Sung Hak Lee, Sangjeong Ahn, Won-Ki Jeong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15574">https://arxiv.org/abs/2409.15574</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15574">https://arxiv.org/pdf/2409.15574</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15574]] Clinical-grade Multi-Organ Pathology Report Generation for Multi-scale Whole Slide Images via a Semantically Guided Medical Text Foundation Model(https://arxiv.org/abs/2409.15574)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Vision language models (VLM) have achieved success in both natural language comprehension and image recognition tasks. However, their use in pathology report generation for whole slide images (WSIs) is still limited due to the huge size of multi-scale WSIs and the high cost of WSI annotation. Moreover, in most of the existing research on pathology report generation, sufficient validation regarding clinical efficacy has not been conducted. Herein, we propose a novel Patient-level Multi-organ Pathology Report Generation (PMPRG) model, which utilizes the multi-scale WSI features from our proposed multi-scale regional vision transformer (MR-ViT) model and their real pathology reports to guide VLM training for accurate pathology report generation. The model then automatically generates a report based on the provided key features attended regional features. We assessed our model using a WSI dataset consisting of multiple organs, including the colon and kidney. Our model achieved a METEOR score of 0.68, demonstrating the effectiveness of our approach. This model allows pathologists to efficiently generate pathology reports for patients, regardless of the number of WSIs involved.</li>
</ul>

<h3>Title: Revolutionizing Biomarker Discovery: Leveraging Generative AI for Bio-Knowledge-Embedded Continuous Space Exploration</h3>
<ul>
<li><strong>Authors: </strong>Wangyang Ying, Dongjie Wang, Xuanming Hu, Ji Qiu, Jin Park, Yanjie Fu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15612">https://arxiv.org/abs/2409.15612</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15612">https://arxiv.org/pdf/2409.15612</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15612]] Revolutionizing Biomarker Discovery: Leveraging Generative AI for Bio-Knowledge-Embedded Continuous Space Exploration(https://arxiv.org/abs/2409.15612)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Biomarker discovery is vital in advancing personalized medicine, offering insights into disease diagnosis, prognosis, and therapeutic efficacy. Traditionally, the identification and validation of biomarkers heavily depend on extensive experiments and statistical analyses. These approaches are time-consuming, demand extensive domain expertise, and are constrained by the complexity of biological systems. These limitations motivate us to ask: Can we automatically identify the effective biomarker subset without substantial human efforts? Inspired by the success of generative AI, we think that the intricate knowledge of biomarker identification can be compressed into a continuous embedding space, thus enhancing the search for better biomarkers. Thus, we propose a new biomarker identification framework with two important modules:1) training data preparation and 2) embedding-optimization-generation. The first module uses a multi-agent system to automatically collect pairs of biomarker subsets and their corresponding prediction accuracy as training data. These data establish a strong knowledge base for biomarker identification. The second module employs an encoder-evaluator-decoder learning paradigm to compress the knowledge of the collected data into a continuous space. Then, it utilizes gradient-based search techniques and autoregressive-based reconstruction to efficiently identify the optimal subset of biomarkers. Finally, we conduct extensive experiments on three real-world datasets to show the efficiency, robustness, and effectiveness of our method.</li>
</ul>

<h3>Title: Data Augmentation for Sparse Multidimensional Learning Performance Data Using Generative AI</h3>
<ul>
<li><strong>Authors: </strong>Liang Zhang, Jionghao Lin, John Sabatini, Conrad Borchers, Daniel Weitekamp, Meng Cao, John Hollander, Xiangen Hu, Arthur C. Graesser</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15631">https://arxiv.org/abs/2409.15631</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15631">https://arxiv.org/pdf/2409.15631</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15631]] Data Augmentation for Sparse Multidimensional Learning Performance Data Using Generative AI(https://arxiv.org/abs/2409.15631)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Learning performance data describe correct and incorrect answers or problem-solving attempts in adaptive learning, such as in intelligent tutoring systems (ITSs). Learning performance data tend to be highly sparse (80\%\(\sim\)90\% missing observations) in most real-world applications due to adaptive item selection. This data sparsity presents challenges to using learner models to effectively predict future performance explore new hypotheses about learning. This article proposes a systematic framework for augmenting learner data to address data sparsity in learning performance data. First, learning performance is represented as a three-dimensional tensor of learners' questions, answers, and attempts, capturing longitudinal knowledge states during learning. Second, a tensor factorization method is used to impute missing values in sparse tensors of collected learner data, thereby grounding the imputation on knowledge tracing tasks that predict missing performance values based on real observations. Third, a module for generating patterns of learning is used. This study contrasts two forms of generative Artificial Intelligence (AI), including Generative Adversarial Networks (GANs) and Generate Pre-Trained Transformers (GPT) to generate data associated with different clusters of learner data. We tested this approach on an adult literacy dataset from AutoTutor lessons developed for Adult Reading Comprehension (ARC). We found that: (1) tensor factorization improved the performance in tracing and predicting knowledge mastery compared with other knowledge tracing techniques without data augmentation, showing higher relative fidelity for this imputation method, and (2) the GAN-based simulation showed greater overall stability and less statistical bias based on a divergence evaluation with varying simulation sample sizes compared to GPT.</li>
</ul>

<h3>Title: ImPoster: Text and Frequency Guidance for Subject Driven Action Personalization using Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Divya Kothandaraman, Kuldeep Kulkarni, Sumit Shekhar, Balaji Vasan Srinivasan, Dinesh Manocha</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15650">https://arxiv.org/abs/2409.15650</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15650">https://arxiv.org/pdf/2409.15650</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15650]] ImPoster: Text and Frequency Guidance for Subject Driven Action Personalization using Diffusion Models(https://arxiv.org/abs/2409.15650)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present ImPoster, a novel algorithm for generating a target image of a 'source' subject performing a 'driving' action. The inputs to our algorithm are a single pair of a source image with the subject that we wish to edit and a driving image with a subject of an arbitrary class performing the driving action, along with the text descriptions of the two images. Our approach is completely unsupervised and does not require any access to additional annotations like keypoints or pose. Our approach builds on a pretrained text-to-image latent diffusion model and learns the characteristics of the source and the driving image by finetuning the diffusion model for a small number of iterations. At inference time, ImPoster performs step-wise text prompting i.e. it denoises by first moving in the direction of the image manifold corresponding to the driving image followed by the direction of the image manifold corresponding to the text description of the desired target image. We propose a novel diffusion guidance formulation, image frequency guidance, to steer the generation towards the manifold of the source subject and the driving action at every step of the inference denoising. Our frequency guidance formulations are derived from the frequency domain properties of images. We extensively evaluate ImPoster on a diverse set of source-driving image pairs to demonstrate improvements over baselines. To the best of our knowledge, ImPoster is the first approach towards achieving both subject-driven as well as action-driven image personalization. Code and data is available at this https URL.</li>
</ul>

<h3>Title: Teaching Tailored to Talent: Adverse Weather Restoration via Prompt Pool and Depth-Anything Constraint</h3>
<ul>
<li><strong>Authors: </strong>Sixiang Chen, Tian Ye, Kai Zhang, Zhaohu Xing, Yunlong Lin, Lei Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15739">https://arxiv.org/abs/2409.15739</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15739">https://arxiv.org/pdf/2409.15739</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15739]] Teaching Tailored to Talent: Adverse Weather Restoration via Prompt Pool and Depth-Anything Constraint(https://arxiv.org/abs/2409.15739)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advancements in adverse weather restoration have shown potential, yet the unpredictable and varied combinations of weather degradations in the real world pose significant challenges. Previous methods typically struggle with dynamically handling intricate degradation combinations and carrying on background reconstruction precisely, leading to performance and generalization limitations. Drawing inspiration from prompt learning and the "Teaching Tailored to Talent" concept, we introduce a novel pipeline, T3-DiffWeather. Specifically, we employ a prompt pool that allows the network to autonomously combine sub-prompts to construct weather-prompts, harnessing the necessary attributes to adaptively tackle unforeseen weather input. Moreover, from a scene modeling perspective, we incorporate general prompts constrained by Depth-Anything feature to provide the scene-specific condition for the diffusion process. Furthermore, by incorporating contrastive prompt loss, we ensures distinctive representations for both types of prompts by a mutual pushing strategy. Experimental results demonstrate that our method achieves state-of-the-art performance across various synthetic and real-world datasets, markedly outperforming existing diffusion techniques in terms of computational efficiency.</li>
</ul>

<h3>Title: The Roles of Generative Artificial Intelligence in Internet of Electric Vehicles</h3>
<ul>
<li><strong>Authors: </strong>Hanwen Zhang, Dusit Niyato, Wei Zhang, Changyuan Zhao, Hongyang Du, Abbas Jamalipour, Sumei Sun, Yiyang Pei</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.ET</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15750">https://arxiv.org/abs/2409.15750</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15750">https://arxiv.org/pdf/2409.15750</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15750]] The Roles of Generative Artificial Intelligence in Internet of Electric Vehicles(https://arxiv.org/abs/2409.15750)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>With the advancement of generative artificial intelligence (GenAI) models, their capability to generate content is seeing significant enhancement, leading to widespread applications in the field of data generation and forecasting. Furthermore, GenAI has strong capabilities in data modeling and analysis, which enhances Internet of electric vehicles (IoEV) applications in various aspects. In this paper, we investigate and survey applications of GenAI in the IoEV. Specifically, we categorize GenAI for IoEV into four different layers namely, EV's battery layer, individual electric vehicle (EV) layer, smart grid with EV layer, and security layer. We first introduce various GenAI techniques used in each layer of IoEV applications. Subsequently, public datasets available for training the GenAI models are summarized. Finally, we provide recommendations for future directions. This survey not only categorizes the applications of GenAI in IoEV across different layers but also serves as a valuable resource for researchers and practitioners by highlighting the design and implementation challenges within each layer. Furthermore, it provides a roadmap for future research directions, enabling the development of more robust and efficient IoEV systems through the integration of advanced GenAI techniques.</li>
</ul>

<h3>Title: TFG: Unified Training-Free Guidance for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Haotian Ye, Haowei Lin, Jiaqi Han, Minkai Xu, Sheng Liu, Yitao Liang, Jianzhu Ma, James Zou, Stefano Ermon</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15761">https://arxiv.org/abs/2409.15761</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15761">https://arxiv.org/pdf/2409.15761</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15761]] TFG: Unified Training-Free Guidance for Diffusion Models(https://arxiv.org/abs/2409.15761)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Given an unconditional diffusion model and a predictor for a target property of interest (e.g., a classifier), the goal of training-free guidance is to generate samples with desirable target properties without additional training. Existing methods, though effective in various individual applications, often lack theoretical grounding and rigorous testing on extensive benchmarks. As a result, they could even fail on simple tasks, and applying them to a new problem becomes unavoidably difficult. This paper introduces a novel algorithmic framework encompassing existing methods as special cases, unifying the study of training-free guidance into the analysis of an algorithm-agnostic design space. Via theoretical and empirical investigation, we propose an efficient and effective hyper-parameter searching strategy that can be readily applied to any downstream task. We systematically benchmark across 7 diffusion models on 16 tasks with 40 targets, and improve performance by 8.5% on average. Our framework and benchmark offer a solid foundation for conditional generation in a training-free manner.</li>
</ul>

<h3>Title: Zero-shot forecasting of chaotic systems</h3>
<ul>
<li><strong>Authors: </strong>Yuanzhao Zhang, William Gilpin</a></li>
<li><strong>Subjects: </strong>cs.LG, nlin.CD, physics.comp-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15771">https://arxiv.org/abs/2409.15771</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15771">https://arxiv.org/pdf/2409.15771</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15771]] Zero-shot forecasting of chaotic systems(https://arxiv.org/abs/2409.15771)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Time-series forecasting is a challenging task that traditionally requires specialized models custom-trained for the specific task at hand. Recently, inspired by the success of large language models, foundation models pre-trained on vast amounts of time-series data from diverse domains have emerged as a promising candidate for general-purpose time-series forecasting. The defining characteristic of these foundation models is their ability to perform zero-shot learning, that is, forecasting a new system from limited context data without explicit re-training or fine-tuning. Here, we evaluate whether the zero-shot learning paradigm extends to the challenging task of forecasting chaotic systems. Across 135 distinct chaotic dynamical systems and $10^8$ timepoints, we find that foundation models produce competitive forecasts compared to custom-trained models (including NBEATS, TiDE, etc.), particularly when training data is limited. Interestingly, even after point forecasts fail, foundation models preserve the geometric and statistical properties of the chaotic attractors, demonstrating a surprisingly strong ability to capture the long-term behavior of chaotic dynamical systems. Our results highlight the promises and pitfalls of foundation models in making zero-shot forecasts of chaotic systems.</li>
</ul>

<h3>Title: Small Language Models: Survey, Measurements, and Insights</h3>
<ul>
<li><strong>Authors: </strong>Zhenyan Lu, Xiang Li, Dongqi Cai, Rongjie Yi, Fangming Liu, Xiwen Zhang, Nicholas D. Lane, Mengwei Xu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15790">https://arxiv.org/abs/2409.15790</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15790">https://arxiv.org/pdf/2409.15790</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15790]] Small Language Models: Survey, Measurements, and Insights(https://arxiv.org/abs/2409.15790)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Small language models (SLMs), despite their widespread adoption in modern smart devices, have received significantly less academic attention compared to their large language model (LLM) counterparts, which are predominantly deployed in data centers and cloud environments. While researchers continue to improve the capabilities of LLMs in the pursuit of artificial general intelligence, SLM research aims to make machine intelligence more accessible, affordable, and efficient for everyday tasks. Focusing on transformer-based, decoder-only language models with 100M-5B parameters, we survey 59 state-of-the-art open-source SLMs, analyzing their technical innovations across three axes: architectures, training datasets, and training algorithms. In addition, we evaluate their capabilities in various domains, including commonsense reasoning, in-context learning, mathematics, and coding. To gain further insight into their on-device runtime costs, we benchmark their inference latency and memory footprints. Through in-depth analysis of our benchmarking data, we offer valuable insights to advance research in this field.</li>
</ul>

<h3>Title: Towards Universal Large-Scale Foundational Model for Natural Gas Demand Forecasting</h3>
<ul>
<li><strong>Authors: </strong>Xinxing Zhou, Jiaqi Ye, Shubao Zhao, Ming Jin, Zhaoxiang Hou, Chengyi Yang, Zengxiang Li, Yanlong Wen, Xiaojie Yuan</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15794">https://arxiv.org/abs/2409.15794</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15794">https://arxiv.org/pdf/2409.15794</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15794]] Towards Universal Large-Scale Foundational Model for Natural Gas Demand Forecasting(https://arxiv.org/abs/2409.15794)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>In the context of global energy strategy, accurate natural gas demand forecasting is crucial for ensuring efficient resource allocation and operational planning. Traditional forecasting methods struggle to cope with the growing complexity and variability of gas consumption patterns across diverse industries and commercial sectors. To address these challenges, we propose the first foundation model specifically tailored for natural gas demand forecasting. Foundation models, known for their ability to generalize across tasks and datasets, offer a robust solution to the limitations of traditional methods, such as the need for separate models for different customer segments and their limited generalization capabilities. Our approach leverages contrastive learning to improve prediction accuracy in real-world scenarios, particularly by tackling issues such as noise in historical consumption data and the potential misclassification of similar data samples, which can lead to degradation in the quaility of the representation and thus the accuracy of downstream forecasting tasks. By integrating advanced noise filtering techniques within the contrastive learning framework, our model enhances the quality of learned representations, leading to more accurate predictions. Furthermore, the model undergoes industry-specific fine-tuning during pretraining, enabling it to better capture the unique characteristics of gas consumption across various sectors. We conducted extensive experiments using a large-scale dataset from ENN Group, which includes data from over 10,000 industrial, commercial, and welfare-related customers across multiple regions. Our model outperformed existing state-of-the-art methods, demonstrating a relative improvement in MSE by 3.68\% and in MASE by 6.15\% compared to the best available model.</li>
</ul>

<h3>Title: 3D-JEPA: A Joint Embedding Predictive Architecture for 3D Self-Supervised Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Naiwen Hu, Haozhe Cheng, Yifan Xie, Shiqi Li, Jihua Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15803">https://arxiv.org/abs/2409.15803</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15803">https://arxiv.org/pdf/2409.15803</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15803]] 3D-JEPA: A Joint Embedding Predictive Architecture for 3D Self-Supervised Representation Learning(https://arxiv.org/abs/2409.15803)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, generative</a></li>
<li><strong>Abstract: </strong>Invariance-based and generative methods have shown a conspicuous performance for 3D self-supervised representation learning (SSRL). However, the former relies on hand-crafted data augmentations that introduce bias not universally applicable to all downstream tasks, and the latter indiscriminately reconstructs masked regions, resulting in irrelevant details being saved in the representation space. To solve the problem above, we introduce 3D-JEPA, a novel non-generative 3D SSRL framework. Specifically, we propose a multi-block sampling strategy that produces a sufficiently informative context block and several representative target blocks. We present the context-aware decoder to enhance the reconstruction of the target blocks. Concretely, the context information is fed to the decoder continuously, facilitating the encoder in learning semantic modeling rather than memorizing the context information related to target blocks. Overall, 3D-JEPA predicts the representation of target blocks from a context block using the encoder and context-aware decoder architecture. Various downstream tasks on different datasets demonstrate 3D-JEPA's effectiveness and efficiency, achieving higher accuracy with fewer pretraining epochs, e.g., 88.65% accuracy on PB_T50_RS with 150 pretraining epochs.</li>
</ul>

<h3>Title: Aided design of bridge aesthetics based on Stable Diffusion fine-tuning</h3>
<ul>
<li><strong>Authors: </strong>Leye Zhang, Xiangxiang Tian, Chengli Zhang, Hongjun Zhang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15812">https://arxiv.org/abs/2409.15812</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15812">https://arxiv.org/pdf/2409.15812</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15812]] Aided design of bridge aesthetics based on Stable Diffusion fine-tuning(https://arxiv.org/abs/2409.15812)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Stable Diffusion fine-tuning technique is tried to assist bridge-type innovation. The bridge real photo dataset is built, and Stable Diffusion is fine tuned by using four methods that are Textual Inversion, Dreambooth, Hypernetwork and Lora. All of them can capture the main characteristics of dataset images and realize the personalized customization of Stable Diffusion. Through fine-tuning, Stable Diffusion is not only a drawing tool, but also has the designer's innovative thinking ability. The fine tuned model can generate a large number of innovative new bridge types, which can provide rich inspiration for human designers. The result shows that this technology can be used as an engine of creativity and a power multiplier for human designers.</li>
</ul>

<h3>Title: PseudoNeg-MAE: Self-Supervised Point Cloud Learning using Conditional Pseudo-Negative Embeddings</h3>
<ul>
<li><strong>Authors: </strong>Sutharsan Mahendren, Saimunur Rahman, Piotr Koniusz, Tharindu Fernando, Sridha Sridharan, Clinton Fookes, Peyman Moghadam</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15832">https://arxiv.org/abs/2409.15832</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15832">https://arxiv.org/pdf/2409.15832</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15832]] PseudoNeg-MAE: Self-Supervised Point Cloud Learning using Conditional Pseudo-Negative Embeddings(https://arxiv.org/abs/2409.15832)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>We propose PseudoNeg-MAE, a novel self-supervised learning framework that enhances global feature representation of point cloud mask autoencoder by making them both discriminative and sensitive to transformations. Traditional contrastive learning methods focus on achieving invariance, which can lead to the loss of valuable transformation-related information. In contrast, PseudoNeg-MAE explicitly models the relationship between original and transformed data points using a parametric network COPE, which learns the localized displacements caused by transformations within the latent space. However, jointly training COPE with the MAE leads to undesirable trivial solutions where COPE outputs collapse to an identity. To address this, we introduce a novel loss function incorporating pseudo-negatives, which effectively penalizes these trivial invariant solutions and promotes transformation sensitivity in the embeddings. We validate PseudoNeg-MAE on shape classification and relative pose estimation tasks, where PseudoNeg-MAE achieves state-of-the-art performance on the ModelNet40 and ScanObjectNN datasets under challenging evaluation protocols and demonstrates superior accuracy in estimating relative poses. These results show the effectiveness of PseudoNeg-MAE in learning discriminative and transformation-sensitive representations.</li>
</ul>

<h3>Title: iGAiVA: Integrated Generative AI and Visual Analytics in a Machine Learning Workflow for Text Classification</h3>
<ul>
<li><strong>Authors: </strong>Yuanzhe Jin, Adrian Carrasco-Revilla, Min Chen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15848">https://arxiv.org/abs/2409.15848</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15848">https://arxiv.org/pdf/2409.15848</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15848]] iGAiVA: Integrated Generative AI and Visual Analytics in a Machine Learning Workflow for Text Classification(https://arxiv.org/abs/2409.15848)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In developing machine learning (ML) models for text classification, one common challenge is that the collected data is often not ideally distributed, especially when new classes are introduced in response to changes of data and tasks. In this paper, we present a solution for using visual analytics (VA) to guide the generation of synthetic data using large language models. As VA enables model developers to identify data-related deficiency, data synthesis can be targeted to address such deficiency. We discuss different types of data deficiency, describe different VA techniques for supporting their identification, and demonstrate the effectiveness of targeted data synthesis in improving model accuracy. In addition, we present a software tool, iGAiVA, which maps four groups of ML tasks into four VA views, integrating generative AI and VA into an ML workflow for developing and improving text classification models.</li>
</ul>

<h3>Title: Zero-Shot Detection of AI-Generated Images</h3>
<ul>
<li><strong>Authors: </strong>Davide Cozzolino, Giovanni Poggi, Matthias Nießner, Luisa Verdoliva</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15875">https://arxiv.org/abs/2409.15875</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15875">https://arxiv.org/pdf/2409.15875</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15875]] Zero-Shot Detection of AI-Generated Images(https://arxiv.org/abs/2409.15875)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Detecting AI-generated images has become an extraordinarily difficult challenge as new generative architectures emerge on a daily basis with more and more capabilities and unprecedented realism. New versions of many commercial tools, such as DALLE, Midjourney, and Stable Diffusion, have been released recently, and it is impractical to continually update and retrain supervised forensic detectors to handle such a large variety of models. To address this challenge, we propose a zero-shot entropy-based detector (ZED) that neither needs AI-generated training data nor relies on knowledge of generative architectures to artificially synthesize their artifacts. Inspired by recent works on machine-generated text detection, our idea is to measure how surprising the image under analysis is compared to a model of real images. To this end, we rely on a lossless image encoder that estimates the probability distribution of each pixel given its context. To ensure computational efficiency, the encoder has a multi-resolution architecture and contexts comprise mostly pixels of the lower-resolution version of the image.Since only real images are needed to learn the model, the detector is independent of generator architectures and synthetic training data. Using a single discriminative feature, the proposed detector achieves state-of-the-art performance. On a wide variety of generative models it achieves an average improvement of more than 3% over the SoTA in terms of accuracy. Code is available at this https URL.</li>
</ul>

<h3>Title: Self-Supervised Graph Embedding Clustering</h3>
<ul>
<li><strong>Authors: </strong>Fangfang Li, Quanxue Gao, Ming Yang, Cheng Deng, Wei Xia</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15887">https://arxiv.org/abs/2409.15887</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15887">https://arxiv.org/pdf/2409.15887</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15887]] Self-Supervised Graph Embedding Clustering(https://arxiv.org/abs/2409.15887)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>The K-means one-step dimensionality reduction clustering method has made some progress in addressing the curse of dimensionality in clustering tasks. However, it combines the K-means clustering and dimensionality reduction processes for optimization, leading to limitations in the clustering effect due to the introduced hyperparameters and the initialization of clustering centers. Moreover, maintaining class balance during clustering remains challenging. To overcome these issues, we propose a unified framework that integrates manifold learning with K-means, resulting in the self-supervised graph embedding framework. Specifically, we establish a connection between K-means and the manifold structure, allowing us to perform K-means without explicitly defining centroids. Additionally, we use this centroid-free K-means to generate labels in low-dimensional space and subsequently utilize the label information to determine the similarity between samples. This approach ensures consistency between the manifold structure and the labels. Our model effectively achieves one-step clustering without the need for redundant balancing hyperparameters. Notably, we have discovered that maximizing the $\ell_{2,1}$-norm naturally maintains class balance during clustering, a result that we have theoretically proven. Finally, experiments on multiple datasets demonstrate that the clustering results of Our-LPP and Our-MFA exhibit excellent and reliable performance.</li>
</ul>

<h3>Title: CAD: Memory Efficient Convolutional Adapter for Segment Anything</h3>
<ul>
<li><strong>Authors: </strong>Joohyeok Kim, Joonhyeon Song, Seohwan Yun, Seongho Yoon, Sangmin Lee</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15889">https://arxiv.org/abs/2409.15889</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15889">https://arxiv.org/pdf/2409.15889</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15889]] CAD: Memory Efficient Convolutional Adapter for Segment Anything(https://arxiv.org/abs/2409.15889)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>The Foundation model for image segmentation, Segment Anything (SAM), has been actively researched in various fields since its proposal. Various researches have been proposed to adapt SAM to specific domains, with one notable approach involving the addition and training of lightweight adapter modules. While adapter-based fine-tuning approaches have reported parameter efficiency and significant performance improvements, they face a often overlooked issue: the excessive consumption of GPU memory relative to the number of trainable parameters. Addressing this issue, this paper proposes a memory-efficient parallel convolutional adapter architecture. This architecture connects in parallel with SAM's image encoder, eliminating the need to store activations and gradients of the image encoder during model training. Our proposed architecture demonstrated competitive experimental results while using less than half the GPU memory compared to SAM Adapter, indicating its value as an alternative to simple decoder fine-tuning when hardware limitations preclude adapter-based learning. Our code implementation is available at our github.</li>
</ul>

<h3>Title: Self-supervised Shape Completion via Involution and Implicit Correspondences</h3>
<ul>
<li><strong>Authors: </strong>Mengya Liu, Ajad Chhatkuli, Janis Postels, Luc Van Gool, Federico Tombari</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15939">https://arxiv.org/abs/2409.15939</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15939">https://arxiv.org/pdf/2409.15939</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15939]] Self-supervised Shape Completion via Involution and Implicit Correspondences(https://arxiv.org/abs/2409.15939)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>3D shape completion is traditionally solved using supervised training or by distribution learning on complete shape examples. Recently self-supervised learning approaches that do not require any complete 3D shape examples have gained more interests. In this paper, we propose a non-adversarial self-supervised approach for the shape completion task. Our first finding is that completion problems can be formulated as an involutory function trivially, which implies a special constraint on the completion function G, such that G(G(X)) = X. Our second constraint on self-supervised shape completion relies on the fact that shape completion becomes easier to solve with correspondences and similarly, completion can simplify the correspondences problem. We formulate a consistency measure in the canonical space in order to supervise the completion function. We efficiently optimize the completion and correspondence modules using "freeze and alternate" strategy. The overall approach performs well for rigid shapes in a category as well as dynamic non-rigid shapes. We ablate our design choices and compare our solution against state-of-the-art methods, showing remarkable accuracy approaching supervised accuracy in some cases.</li>
</ul>

<h3>Title: Mind the Prompt: A Novel Benchmark for Prompt-based Class-Agnostic Counting</h3>
<ul>
<li><strong>Authors: </strong>Luca Ciampi, Nicola Messina, Matteo Pierucci, Giuseppe Amato, Marco Avvenuti, Fabrizio Falchi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15953">https://arxiv.org/abs/2409.15953</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15953">https://arxiv.org/pdf/2409.15953</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15953]] Mind the Prompt: A Novel Benchmark for Prompt-based Class-Agnostic Counting(https://arxiv.org/abs/2409.15953)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Class-agnostic counting (CAC) is a recent task in computer vision that aims to estimate the number of instances of arbitrary object classes never seen during model training. With the recent advancement of robust vision-and-language foundation models, there is a growing interest in prompt-based CAC, where object categories to be counted can be specified using natural language. However, we identify significant limitations in current benchmarks for evaluating this task, which hinder both accurate assessment and the development of more effective solutions. Specifically, we argue that the current evaluation protocols do not measure the ability of the model to understand which object has to be counted. This is due to two main factors: (i) the shortcomings of CAC datasets, which primarily consist of images containing objects from a single class, and (ii) the limitations of current counting performance evaluators, which are based on traditional class-specific counting and focus solely on counting errors. To fill this gap, we introduce the Prompt-Aware Counting (PrACo) benchmark, which comprises two targeted tests, each accompanied by appropriate evaluation metrics. We evaluate state-of-the-art methods and demonstrate that, although some achieve impressive results on standard class-specific counting metrics, they exhibit a significant deficiency in understanding the input prompt, indicating the need for more careful training procedures or revised designs. The code for reproducing our results is available at this https URL.</li>
</ul>

<h3>Title: Leveraging Unsupervised Learning for Cost-Effective Visual Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Yunbo Long, Zhengyang Ling, Sam Brook, Duncan McFarlane, Alexandra Brintrup</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15980">https://arxiv.org/abs/2409.15980</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15980">https://arxiv.org/pdf/2409.15980</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15980]] Leveraging Unsupervised Learning for Cost-Effective Visual Anomaly Detection(https://arxiv.org/abs/2409.15980)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Traditional machine learning-based visual inspection systems require extensive data collection and repetitive model training to improve accuracy. These systems typically require expensive camera, computing equipment and significant machine learning ex- pertise, which can substantially burden small and medium-sized enterprises. This study explores leveraging unsupervised learning methods with pre-trained models and low-cost hardware to create a cost-effective visual anomaly detection system. The research aims to develop a low-cost visual anomaly detection solution that uses minimal data for model training while maintaining general- izability and scalability. The system utilises unsupervised learning models from Anomalib and is deployed on affordable Raspberry Pi hardware through openVINO. The results show that this cost-effective system can complete anomaly defection training and inference on a Raspberry Pi in just 90 seconds using only 10 normal product images, achieving an F1 macro score exceeding 0.95. While the system is slightly sensitive to environmental changes like lighting, product positioning, or background, it remains a swift and economical method for factory automation inspection for small and medium-sized manufacturers</li>
</ul>

<h3>Title: Exploring the Impact of Outlier Variability on Anomaly Detection Evaluation Metrics</h3>
<ul>
<li><strong>Authors: </strong>Minjae Ok, Simon Klüttermann, Emmanuel Müller</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15986">https://arxiv.org/abs/2409.15986</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15986">https://arxiv.org/pdf/2409.15986</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15986]] Exploring the Impact of Outlier Variability on Anomaly Detection Evaluation Metrics(https://arxiv.org/abs/2409.15986)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Anomaly detection is a dynamic field, in which the evaluation of models plays a critical role in understanding their effectiveness. The selection and interpretation of the evaluation metrics are pivotal, particularly in scenarios with varying amounts of anomalies. This study focuses on examining the behaviors of three widely used anomaly detection metrics under different conditions: F1 score, Receiver Operating Characteristic Area Under Curve (ROC AUC), and Precision-Recall Curve Area Under Curve (AUCPR). Our study critically analyzes the extent to which these metrics provide reliable and distinct insights into model performance, especially considering varying levels of outlier fractions and contamination thresholds in datasets. Through a comprehensive experimental setup involving widely recognized algorithms for anomaly detection, we present findings that challenge the conventional understanding of these metrics and reveal nuanced behaviors under varying conditions. We demonstrated that while the F1 score and AUCPR are sensitive to outlier fractions, the ROC AUC maintains consistency and is unaffected by such variability. Additionally, under conditions of a fixed outlier fraction in the test set, we observe an alignment between ROC AUC and AUCPR, indicating that the choice between these two metrics may be less critical in such scenarios. The results of our study contribute to a more refined understanding of metric selection and interpretation in anomaly detection, offering valuable insights for both researchers and practitioners in the field.</li>
</ul>

<h3>Title: Improvements to SDXL in NovelAI Diffusion V3</h3>
<ul>
<li><strong>Authors: </strong>Juan Ossa, Eren Doğan, Alex Birch, F. Johnson</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.15997">https://arxiv.org/abs/2409.15997</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.15997">https://arxiv.org/pdf/2409.15997</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.15997]] Improvements to SDXL in NovelAI Diffusion V3(https://arxiv.org/abs/2409.15997)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>In this technical report, we document the changes we made to SDXL in the process of training NovelAI Diffusion V3, our state of the art anime image generation model.</li>
</ul>

<h3>Title: Unleashing the Potential of Synthetic Images: A Study on Histopathology Image Classification</h3>
<ul>
<li><strong>Authors: </strong>Leire Benito-Del-Valle, Aitor Alvarez-Gila, Itziar Eguskiza, Cristina L. Saratxaga</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.16002">https://arxiv.org/abs/2409.16002</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.16002">https://arxiv.org/pdf/2409.16002</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.16002]] Unleashing the Potential of Synthetic Images: A Study on Histopathology Image Classification(https://arxiv.org/abs/2409.16002)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Histopathology image classification is crucial for the accurate identification and diagnosis of various diseases but requires large and diverse datasets. Obtaining such datasets, however, is often costly and time-consuming due to the need for expert annotations and ethical constraints. To address this, we examine the suitability of different generative models and image selection approaches to create realistic synthetic histopathology image patches conditioned on class labels. Our findings highlight the importance of selecting an appropriate generative model type and architecture to enhance performance. Our experiments over the PCam dataset show that diffusion models are effective for transfer learning, while GAN-generated samples are better suited for augmentation. Additionally, transformer-based generative models do not require image filtering, in contrast to those derived from Convolutional Neural Networks (CNNs), which benefit from realism score-based selection. Therefore, we show that synthetic images can effectively augment existing datasets, ultimately improving the performance of the downstream histopathology image classification task.</li>
</ul>

<h3>Title: Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts</h3>
<ul>
<li><strong>Authors: </strong>Xiaoming Shi, Shiyu Wang, Yuqi Nie, Dianqi Li, Zhou Ye, Qingsong Wen, Ming Jin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.16040">https://arxiv.org/abs/2409.16040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.16040">https://arxiv.org/pdf/2409.16040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.16040]] Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts(https://arxiv.org/abs/2409.16040)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Deep learning for time series forecasting has seen significant advancements over the past decades. However, despite the success of large-scale pre-training in language and vision domains, pre-trained time series models remain limited in scale and operate at a high cost, hindering the development of larger capable forecasting models in real-world applications. In response, we introduce Time-MoE, a scalable and unified architecture designed to pre-train larger, more capable forecasting foundation models while reducing inference costs. By leveraging a sparse mixture-of-experts (MoE) design, Time-MoE enhances computational efficiency by activating only a subset of networks for each prediction, reducing computational load while maintaining high model capacity. This allows Time-MoE to scale effectively without a corresponding increase in inference costs. Time-MoE comprises a family of decoder-only transformer models that operate in an auto-regressive manner and support flexible forecasting horizons with varying input context lengths. We pre-trained these models on our newly introduced large-scale data Time-300B, which spans over 9 domains and encompassing over 300 billion time points. For the first time, we scaled a time series foundation model up to 2.4 billion parameters, achieving significantly improved forecasting precision. Our results validate the applicability of scaling laws for training tokens and model size in the context of time series forecasting. Compared to dense models with the same number of activated parameters or equivalent computation budgets, our models consistently outperform them by large margin. These advancements position Time-MoE as a state-of-the-art solution for tackling real-world time series forecasting challenges with superior capability, efficiency, and flexibility.</li>
</ul>

<h3>Title: Generative 3D Cardiac Shape Modelling for In-Silico Trials</h3>
<ul>
<li><strong>Authors: </strong>Andrei Gasparovici, Alex Serban</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.16058">https://arxiv.org/abs/2409.16058</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.16058">https://arxiv.org/pdf/2409.16058</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.16058]] Generative 3D Cardiac Shape Modelling for In-Silico Trials(https://arxiv.org/abs/2409.16058)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We propose a deep learning method to model and generate synthetic aortic shapes based on representing shapes as the zero-level set of a neural signed distance field, conditioned by a family of trainable embedding vectors with encode the geometric features of each shape. The network is trained on a dataset of aortic root meshes reconstructed from CT images by making the neural field vanish on sampled surface points and enforcing its spatial gradient to have unit norm. Empirical results show that our model can represent aortic shapes with high fidelity. Moreover, by sampling from the learned embedding vectors, we can generate novel shapes that resemble real patient anatomies, which can be used for in-silico trials.</li>
</ul>

<h3>Title: Machine learning approaches for automatic defect detection in photovoltaic systems</h3>
<ul>
<li><strong>Authors: </strong>Swayam Rajat Mohanty, Moin Uddin Maruf, Vaibhav Singh, Zeeshan Ahmad</a></li>
<li><strong>Subjects: </strong>cs.CV, physics.app-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.16069">https://arxiv.org/abs/2409.16069</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.16069">https://arxiv.org/pdf/2409.16069</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.16069]] Machine learning approaches for automatic defect detection in photovoltaic systems(https://arxiv.org/abs/2409.16069)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Solar photovoltaic (PV) modules are prone to damage during manufacturing, installation and operation which reduces their power conversion efficiency. This diminishes their positive environmental impact over the lifecycle. Continuous monitoring of PV modules during operation via unmanned aerial vehicles is essential to ensure that defective panels are promptly replaced or repaired to maintain high power conversion efficiencies. Computer vision provides an automatic, non-destructive and cost-effective tool for monitoring defects in large-scale PV plants. We review the current landscape of deep learning-based computer vision techniques used for detecting defects in solar modules. We compare and evaluate the existing approaches at different levels, namely the type of images used, data collection and processing method, deep learning architectures employed, and model interpretability. Most approaches use convolutional neural networks together with data augmentation or generative adversarial network-based techniques. We evaluate the deep learning approaches by performing interpretability analysis on classification tasks. This analysis reveals that the model focuses on the darker regions of the image to perform the classification. We find clear gaps in the existing approaches while also laying out the groundwork for mitigating these challenges when building new models. We conclude with the relevant research gaps that need to be addressed and approaches for progress in this field: integrating geometric deep learning with existing approaches for building more robust and reliable models, leveraging physics-based neural networks that combine domain expertise of physical laws to build more domain-aware deep learning models, and incorporating interpretability as a factor for building models that can be trusted. The review points towards a clear roadmap for making this technology commercially relevant.</li>
</ul>

<h3>Title: Open-World Object Detection with Instance Representation Learning</h3>
<ul>
<li><strong>Authors: </strong>Sunoh Lee, Minsik Jeon, Jihong Min, Junwon Seo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.16073">https://arxiv.org/abs/2409.16073</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.16073">https://arxiv.org/pdf/2409.16073</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.16073]] Open-World Object Detection with Instance Representation Learning(https://arxiv.org/abs/2409.16073)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>While humans naturally identify novel objects and understand their relationships, deep learning-based object detectors struggle to detect and relate objects that are not observed during training. To overcome this issue, Open World Object Detection(OWOD) has been introduced to enable models to detect unknown objects in open-world scenarios. However, OWOD methods fail to capture the fine-grained relationships between detected objects, which are crucial for comprehensive scene understanding and applications such as class discovery and tracking. In this paper, we propose a method to train an object detector that can both detect novel objects and extract semantically rich features in open-world conditions by leveraging the knowledge of Vision Foundation Models(VFM). We first utilize the semantic masks from the Segment Anything Model to supervise the box regression of unknown objects, ensuring accurate localization. By transferring the instance-wise similarities obtained from the VFM features to the detector's instance embeddings, our method then learns a semantically rich feature space of these embeddings. Extensive experiments show that our method learns a robust and generalizable feature space, outperforming other OWOD-based feature extraction methods. Additionally, we demonstrate that the enhanced feature from our model increases the detector's applicability to tasks such as open-world tracking.</li>
</ul>

<h3>Title: Exploring Hint Generation Approaches in Open-Domain Question Answering</h3>
<ul>
<li><strong>Authors: </strong>Jamshid Mozafari, Abdelrahman Abdallah, Bhawna Piryani, Adam Jatowt</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.16096">https://arxiv.org/abs/2409.16096</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.16096">https://arxiv.org/pdf/2409.16096</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.16096]] Exploring Hint Generation Approaches in Open-Domain Question Answering(https://arxiv.org/abs/2409.16096)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Automatic Question Answering (QA) systems rely on contextual information to provide accurate answers. Commonly, contexts are prepared through either retrieval-based or generation-based methods. The former involves retrieving relevant documents from a corpus like Wikipedia, whereas the latter uses generative models such as Large Language Models (LLMs) to generate the context. In this paper, we introduce a novel context preparation approach called HINTQA, which employs Automatic Hint Generation (HG) techniques. Unlike traditional methods, HINTQA prompts LLMs to produce hints about potential answers for the question rather than generating relevant context. We evaluate our approach across three QA datasets including TriviaQA, NaturalQuestions, and Web Questions, examining how the number and order of hints impact performance. Our findings show that the HINTQA surpasses both retrieval-based and generation-based approaches. We demonstrate that hints enhance the accuracy of answers more than retrieved and generated contexts.</li>
</ul>

<h3>Title: TabEBM: A Tabular Data Augmentation Method with Distinct Class-Specific Energy-Based Models</h3>
<ul>
<li><strong>Authors: </strong>Andrei Margeloiu, Xiangjian Jiang, Nikola Simidjievski, Mateja Jamnik</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.16118">https://arxiv.org/abs/2409.16118</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.16118">https://arxiv.org/pdf/2409.16118</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.16118]] TabEBM: A Tabular Data Augmentation Method with Distinct Class-Specific Energy-Based Models(https://arxiv.org/abs/2409.16118)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Data collection is often difficult in critical fields such as medicine, physics, and chemistry. As a result, classification methods usually perform poorly with these small datasets, leading to weak predictive performance. Increasing the training set with additional synthetic data, similar to data augmentation in images, is commonly believed to improve downstream classification performance. However, current tabular generative methods that learn either the joint distribution $ p(\mathbf{x}, y) $ or the class-conditional distribution $ p(\mathbf{x} \mid y) $ often overfit on small datasets, resulting in poor-quality synthetic data, usually worsening classification performance compared to using real data alone. To solve these challenges, we introduce TabEBM, a novel class-conditional generative method using Energy-Based Models (EBMs). Unlike existing methods that use a shared model to approximate all class-conditional densities, our key innovation is to create distinct EBM generative models for each class, each modelling its class-specific data distribution individually. This approach creates robust energy landscapes, even in ambiguous class distributions. Our experiments show that TabEBM generates synthetic data with higher quality and better statistical fidelity than existing methods. When used for data augmentation, our synthetic data consistently improves the classification performance across diverse datasets of various sizes, especially small ones.</li>
</ul>

<h3>Title: MIMO: Controllable Character Video Synthesis with Spatial Decomposed Modeling</h3>
<ul>
<li><strong>Authors: </strong>Yifang Men, Yuan Yao, Miaomiao Cui, Liefeng Bo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.16160">https://arxiv.org/abs/2409.16160</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.16160">https://arxiv.org/pdf/2409.16160</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.16160]] MIMO: Controllable Character Video Synthesis with Spatial Decomposed Modeling(https://arxiv.org/abs/2409.16160)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Character video synthesis aims to produce realistic videos of animatable characters within lifelike scenes. As a fundamental problem in the computer vision and graphics community, 3D works typically require multi-view captures for per-case training, which severely limits their applicability of modeling arbitrary characters in a short time. Recent 2D methods break this limitation via pre-trained diffusion models, but they struggle for pose generality and scene interaction. To this end, we propose MIMO, a novel framework which can not only synthesize character videos with controllable attributes (i.e., character, motion and scene) provided by simple user inputs, but also simultaneously achieve advanced scalability to arbitrary characters, generality to novel 3D motions, and applicability to interactive real-world scenes in a unified framework. The core idea is to encode the 2D video to compact spatial codes, considering the inherent 3D nature of video occurrence. Concretely, we lift the 2D frame pixels into 3D using monocular depth estimators, and decompose the video clip to three spatial components (i.e., main human, underlying scene, and floating occlusion) in hierarchical layers based on the 3D depth. These components are further encoded to canonical identity code, structured motion code and full scene code, which are utilized as control signals of synthesis process. The design of spatial decomposed modeling enables flexible user control, complex motion expression, as well as 3D-aware synthesis for scene interactions. Experimental results demonstrate effectiveness and robustness of the proposed method.</li>
</ul>

<h3>Title: Fine Tuning Text-to-Image Diffusion Models for Correcting Anomalous Images</h3>
<ul>
<li><strong>Authors: </strong>Hyunwoo Yoo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.16174">https://arxiv.org/abs/2409.16174</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.16174">https://arxiv.org/pdf/2409.16174</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.16174]] Fine Tuning Text-to-Image Diffusion Models for Correcting Anomalous Images(https://arxiv.org/abs/2409.16174)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Since the advent of GANs and VAEs, image generation models have continuously evolved, opening up various real-world applications with the introduction of Stable Diffusion and DALL-E models. These text-to-image models can generate high-quality images for fields such as art, design, and advertising. However, they often produce aberrant images for certain prompts. This study proposes a method to mitigate such issues by fine-tuning the Stable Diffusion 3 model using the DreamBooth technique. Experimental results targeting the prompt "lying on the grass/street" demonstrate that the fine-tuned model shows improved performance in visual evaluation and metrics such as Structural Similarity Index (SSIM), Peak Signal-to-Noise Ratio (PSNR), and Frechet Inception Distance (FID). User surveys also indicated a higher preference for the fine-tuned model. This research is expected to make contributions to enhancing the practicality and reliability of text-to-image models.</li>
</ul>

<h3>Title: Expert-level vision-language foundation model for real-world radiology and comprehensive evaluation</h3>
<ul>
<li><strong>Authors: </strong>Xiaohong Liu, Guoxing Yang, Yulin Luo, Jiaji Mao, Xiang Zhang, Ming Gao, Shanghang Zhang, Jun Shen, Guangyu Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.16183">https://arxiv.org/abs/2409.16183</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.16183">https://arxiv.org/pdf/2409.16183</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.16183]] Expert-level vision-language foundation model for real-world radiology and comprehensive evaluation(https://arxiv.org/abs/2409.16183)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Radiology is a vital and complex component of modern clinical workflow and covers many tasks. Recently, vision-language (VL) foundation models in medicine have shown potential in processing multimodal information, offering a unified solution for various radiology tasks. However, existing studies either pre-trained VL models on natural data or did not fully integrate vision-language architecture and pretraining, often neglecting the unique multimodal complexity in radiology images and their textual contexts. Additionally, their practical applicability in real-world scenarios remains underexplored. Here, we present RadFound, a large and open-source vision-language foundation model tailored for radiology, that is trained on the most extensive dataset of over 8.1 million images and 250,000 image-text pairs, covering 19 major organ systems and 10 imaging modalities. To establish expert-level multimodal perception and generation capabilities, RadFound introduces an enhanced vision encoder to capture intra-image local features and inter-image contextual information, and a unified cross-modal learning design tailored to radiology. To fully assess the models' capability, we construct a benchmark, RadVLBench, including radiology interpretation tasks like medical vision-language question-answering, as well as text generation tasks ranging from captioning to report generation. We also propose a human evaluation framework. When evaluated on the real-world benchmark involving three representative modalities, 2D images (chest X-rays), multi-view images (mammograms), and 3D images (thyroid CT scans), RadFound significantly outperforms other VL foundation models on both quantitative metrics and human evaluation. In summary, the development of RadFound represents an advancement in radiology generalists, demonstrating broad applicability potential for integration into clinical workflows.</li>
</ul>

<h3>Title: MaskBit: Embedding-free Image Generation via Bit Tokens</h3>
<ul>
<li><strong>Authors: </strong>Mark Weber, Lijun Yu, Qihang Yu, Xueqing Deng, Xiaohui Shen, Daniel Cremers, Liang-Chieh Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.16211">https://arxiv.org/abs/2409.16211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.16211">https://arxiv.org/pdf/2409.16211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.16211]] MaskBit: Embedding-free Image Generation via Bit Tokens(https://arxiv.org/abs/2409.16211)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Masked transformer models for class-conditional image generation have become a compelling alternative to diffusion models. Typically comprising two stages - an initial VQGAN model for transitioning between latent space and image space, and a subsequent Transformer model for image generation within latent space - these frameworks offer promising avenues for image synthesis. In this study, we present two primary contributions: Firstly, an empirical and systematic examination of VQGANs, leading to a modernized VQGAN. Secondly, a novel embedding-free generation network operating directly on bit tokens - a binary quantized representation of tokens with rich semantics. The first contribution furnishes a transparent, reproducible, and high-performing VQGAN model, enhancing accessibility and matching the performance of current state-of-the-art methods while revealing previously undisclosed details. The second contribution demonstrates that embedding-free image generation using bit tokens achieves a new state-of-the-art FID of 1.52 on the ImageNet 256x256 benchmark, with a compact generator model of mere 305M parameters.</li>
</ul>

<h3>Title: Fine-Tuning is Fine, if Calibrated</h3>
<ul>
<li><strong>Authors: </strong>Zheda Mai, Arpita Chowdhury, Ping Zhang, Cheng-Hao Tu, Hong-You Chen, Vardaan Pahuja, Tanya Berger-Wolf, Song Gao, Charles Stewart, Yu Su, Wei-Lun Chao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.16223">https://arxiv.org/abs/2409.16223</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.16223">https://arxiv.org/pdf/2409.16223</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.16223]] Fine-Tuning is Fine, if Calibrated(https://arxiv.org/abs/2409.16223)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Fine-tuning is arguably the most straightforward way to tailor a pre-trained model (e.g., a foundation model) to downstream applications, but it also comes with the risk of losing valuable knowledge the model had learned in pre-training. For example, fine-tuning a pre-trained classifier capable of recognizing a large number of classes to master a subset of classes at hand is shown to drastically degrade the model's accuracy in the other classes it had previously learned. As such, it is hard to further use the fine-tuned model when it encounters classes beyond the fine-tuning data. In this paper, we systematically dissect the issue, aiming to answer the fundamental question, ''What has been damaged in the fine-tuned model?'' To our surprise, we find that the fine-tuned model neither forgets the relationship among the other classes nor degrades the features to recognize these classes. Instead, the fine-tuned model often produces more discriminative features for these other classes, even if they were missing during fine-tuning! {What really hurts the accuracy is the discrepant logit scales between the fine-tuning classes and the other classes}, implying that a simple post-processing calibration would bring back the pre-trained model's capability and at the same time unveil the feature improvement over all classes. We conduct an extensive empirical study to demonstrate the robustness of our findings and provide preliminary explanations underlying them, suggesting new directions for future theoretical analysis. Our code is available at this https URL.</li>
</ul>

<h3>Title: VideoPatchCore: An Effective Method to Memorize Normality for Video Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Sunghyun Ahn, Youngwan Jo, Kijung Lee, Sanghyun Park</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.16225">https://arxiv.org/abs/2409.16225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.16225">https://arxiv.org/pdf/2409.16225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.16225]] VideoPatchCore: An Effective Method to Memorize Normality for Video Anomaly Detection(https://arxiv.org/abs/2409.16225)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Video anomaly detection (VAD) is a crucial task in video analysis and surveillance within computer vision. Currently, VAD is gaining attention with memory techniques that store the features of normal frames. The stored features are utilized for frame reconstruction, identifying an abnormality when a significant difference exists between the reconstructed and input frames. However, this approach faces several challenges due to the simultaneous optimization required for both the memory and encoder-decoder model. These challenges include increased optimization difficulty, complexity of implementation, and performance variability depending on the memory size. To address these challenges,we propose an effective memory method for VAD, called VideoPatchCore. Inspired by PatchCore, our approach introduces a structure that prioritizes memory optimization and configures three types of memory tailored to the characteristics of video data. This method effectively addresses the limitations of existing memory-based methods, achieving good performance comparable to state-of-the-art methods. Furthermore, our method requires no training and is straightforward to implement, making VAD tasks more accessible. Our code is available online at this http URL.</li>
</ul>

<h3>Title: MonoFormer: One Transformer for Both Diffusion and Autoregression</h3>
<ul>
<li><strong>Authors: </strong>Chuyang Zhao, Yuxing Song, Wenhao Wang, Haocheng Feng, Errui Ding, Yifan Sun, Xinyan Xiao, Jingdong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.16280">https://arxiv.org/abs/2409.16280</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.16280">https://arxiv.org/pdf/2409.16280</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.16280]] MonoFormer: One Transformer for Both Diffusion and Autoregression(https://arxiv.org/abs/2409.16280)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Most existing multimodality methods use separate backbones for autoregression-based discrete text generation and diffusion-based continuous visual generation, or the same backbone by discretizing the visual data to use autoregression for both text and visual generation. In this paper, we propose to study a simple idea: share one transformer for both autoregression and diffusion. The feasibility comes from two main aspects: (i) Transformer is successfully applied to diffusion for visual generation, and (ii) transformer training for autoregression and diffusion is very similar, and the difference merely lies in that diffusion uses bidirectional attention mask and autoregression uses causal attention mask. Experimental results show that our approach achieves comparable image generation performance to current state-of-the-art methods as well as maintains the text generation capability. The project is publicly available at this https URL.</li>
</ul>

<h3>Title: Self-Supervised Any-Point Tracking by Contrastive Random Walks</h3>
<ul>
<li><strong>Authors: </strong>Ayush Shrivastava, Andrew Owens</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2409.16288">https://arxiv.org/abs/2409.16288</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2409.16288">https://arxiv.org/pdf/2409.16288</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2409.16288]] Self-Supervised Any-Point Tracking by Contrastive Random Walks(https://arxiv.org/abs/2409.16288)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>We present a simple, self-supervised approach to the Tracking Any Point (TAP) problem. We train a global matching transformer to find cycle consistent tracks through video via contrastive random walks, using the transformer's attention-based global matching to define the transition matrices for a random walk on a space-time graph. The ability to perform "all pairs" comparisons between points allows the model to obtain high spatial precision and to obtain a strong contrastive learning signal, while avoiding many of the complexities of recent approaches (such as coarse-to-fine matching). To do this, we propose a number of design decisions that allow global matching architectures to be trained through self-supervision using cycle consistency. For example, we identify that transformer-based methods are sensitive to shortcut solutions, and propose a data augmentation scheme to address them. Our method achieves strong performance on the TapVid benchmarks, outperforming previous self-supervised tracking methods, such as DIFT, and is competitive with several supervised methods.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
