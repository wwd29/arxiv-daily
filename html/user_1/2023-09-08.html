<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>diffusion</h2>
<h3>Title: SADIR: Shape-Aware Diffusion Models for 3D Image Reconstruction. (arXiv:2309.03335v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03335">http://arxiv.org/abs/2309.03335</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03335]] SADIR: Shape-Aware Diffusion Models for 3D Image Reconstruction(http://arxiv.org/abs/2309.03335)</code></li>
<li>Summary: <p>3D image reconstruction from a limited number of 2D images has been a
long-standing challenge in computer vision and image analysis. While deep
learning-based approaches have achieved impressive performance in this area,
existing deep networks often fail to effectively utilize the shape structures
of objects presented in images. As a result, the topology of reconstructed
objects may not be well preserved, leading to the presence of artifacts such as
discontinuities, holes, or mismatched connections between different parts. In
this paper, we propose a shape-aware network based on diffusion models for 3D
image reconstruction, named SADIR, to address these issues. In contrast to
previous methods that primarily rely on spatial correlations of image
intensities for 3D reconstruction, our model leverages shape priors learned
from the training data to guide the reconstruction process. To achieve this, we
develop a joint learning network that simultaneously learns a mean shape under
deformation models. Each reconstructed image is then considered as a deformed
variant of the mean shape. We validate our model, SADIR, on both brain and
cardiac magnetic resonance images (MRIs). Experimental results show that our
method outperforms the baselines with lower reconstruction error and better
preservation of the shape structure of objects within the images.
</p></li>
</ul>

<h3>Title: Relay Diffusion: Unifying diffusion process across resolutions for image synthesis. (arXiv:2309.03350v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03350">http://arxiv.org/abs/2309.03350</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03350]] Relay Diffusion: Unifying diffusion process across resolutions for image synthesis(http://arxiv.org/abs/2309.03350)</code></li>
<li>Summary: <p>Diffusion models achieved great success in image synthesis, but still face
challenges in high-resolution generation. Through the lens of discrete cosine
transformation, we find the main reason is that \emph{the same noise level on a
higher resolution results in a higher Signal-to-Noise Ratio in the frequency
domain}. In this work, we present Relay Diffusion Model (RDM), which transfers
a low-resolution image or noise into an equivalent high-resolution one for
diffusion model via blurring diffusion and block noise. Therefore, the
diffusion process can continue seamlessly in any new resolution or model
without restarting from pure noise or low-resolution conditioning. RDM achieves
state-of-the-art FID on CelebA-HQ and sFID on ImageNet 256$\times$256,
surpassing previous works such as ADM, LDM and DiT by a large margin. All the
codes and checkpoints are open-sourced at
\url{https://github.com/THUDM/RelayDiffusion}.
</p></li>
</ul>

<h3>Title: Underwater Image Enhancement by Transformer-based Diffusion Model with Non-uniform Sampling for Skip Strategy. (arXiv:2309.03445v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03445">http://arxiv.org/abs/2309.03445</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03445]] Underwater Image Enhancement by Transformer-based Diffusion Model with Non-uniform Sampling for Skip Strategy(http://arxiv.org/abs/2309.03445)</code></li>
<li>Summary: <p>In this paper, we present an approach to image enhancement with diffusion
model in underwater scenes. Our method adapts conditional denoising diffusion
probabilistic models to generate the corresponding enhanced images by using the
underwater images and the Gaussian noise as the inputs. Additionally, in order
to improve the efficiency of the reverse process in the diffusion model, we
adopt two different ways. We firstly propose a lightweight transformer-based
denoising network, which can effectively promote the time of network forward
per iteration. On the other hand, we introduce a skip sampling strategy to
reduce the number of iterations. Besides, based on the skip sampling strategy,
we propose two different non-uniform sampling methods for the sequence of the
time step, namely piecewise sampling and searching with the evolutionary
algorithm. Both of them are effective and can further improve performance by
using the same steps against the previous uniform sampling. In the end, we
conduct a relative evaluation of the widely used underwater enhancement
datasets between the recent state-of-the-art methods and the proposed approach.
The experimental results prove that our approach can achieve both competitive
performance and high efficiency. Our code is available at
\href{mailto:https://github.com/piggy2009/DM_underwater}{\color{blue}{https://github.com/piggy2009/DM\_underwater}}.
</p></li>
</ul>

<h3>Title: SyncDreamer: Generating Multiview-consistent Images from a Single-view Image. (arXiv:2309.03453v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03453">http://arxiv.org/abs/2309.03453</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03453]] SyncDreamer: Generating Multiview-consistent Images from a Single-view Image(http://arxiv.org/abs/2309.03453)</code></li>
<li>Summary: <p>In this paper, we present a novel diffusion model called that generates
multiview-consistent images from a single-view image. Using pretrained
large-scale 2D diffusion models, recent work Zero123 demonstrates the ability
to generate plausible novel views from a single-view image of an object.
However, maintaining consistency in geometry and colors for the generated
images remains a challenge. To address this issue, we propose a synchronized
multiview diffusion model that models the joint probability distribution of
multiview images, enabling the generation of multiview-consistent images in a
single reverse process. SyncDreamer synchronizes the intermediate states of all
the generated images at every step of the reverse process through a 3D-aware
feature attention mechanism that correlates the corresponding features across
different views. Experiments show that SyncDreamer generates images with high
consistency across different views, thus making it well-suited for various 3D
generation tasks such as novel-view-synthesis, text-to-3D, and image-to-3D.
</p></li>
</ul>

<h3>Title: Reuse and Diffuse: Iterative Denoising for Text-to-Video Generation. (arXiv:2309.03549v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03549">http://arxiv.org/abs/2309.03549</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03549]] Reuse and Diffuse: Iterative Denoising for Text-to-Video Generation(http://arxiv.org/abs/2309.03549)</code></li>
<li>Summary: <p>Inspired by the remarkable success of Latent Diffusion Models (LDMs) for
image synthesis, we study LDM for text-to-video generation, which is a
formidable challenge due to the computational and memory constraints during
both model training and inference. A single LDM is usually only capable of
generating a very limited number of video frames. Some existing works focus on
separate prediction models for generating more video frames, which suffer from
additional training cost and frame-level jittering, however. In this paper, we
propose a framework called "Reuse and Diffuse" dubbed $\textit{VidRD}$ to
produce more frames following the frames already generated by an LDM.
Conditioned on an initial video clip with a small number of frames, additional
frames are iteratively generated by reusing the original latent features and
following the previous diffusion process. Besides, for the autoencoder used for
translation between pixel space and latent space, we inject temporal layers
into its decoder and fine-tune these layers for higher temporal consistency. We
also propose a set of strategies for composing video-text data that involve
diverse content from multiple existing datasets including video datasets for
action recognition and image-text datasets. Extensive experiments show that our
method achieves good results in both quantitative and qualitative evaluations.
Our project page is available
$\href{https://anonymous0x233.github.io/ReuseAndDiffuse/}{here}$.
</p></li>
</ul>

<h3>Title: Text2Control3D: Controllable 3D Avatar Generation in Neural Radiance Fields using Geometry-Guided Text-to-Image Diffusion Model. (arXiv:2309.03550v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03550">http://arxiv.org/abs/2309.03550</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03550]] Text2Control3D: Controllable 3D Avatar Generation in Neural Radiance Fields using Geometry-Guided Text-to-Image Diffusion Model(http://arxiv.org/abs/2309.03550)</code></li>
<li>Summary: <p>Recent advances in diffusion models such as ControlNet have enabled
geometrically controllable, high-fidelity text-to-image generation. However,
none of them addresses the question of adding such controllability to
text-to-3D generation. In response, we propose Text2Control3D, a controllable
text-to-3D avatar generation method whose facial expression is controllable
given a monocular video casually captured with hand-held camera. Our main
strategy is to construct the 3D avatar in Neural Radiance Fields (NeRF)
optimized with a set of controlled viewpoint-aware images that we generate from
ControlNet, whose condition input is the depth map extracted from the input
video. When generating the viewpoint-aware images, we utilize cross-reference
attention to inject well-controlled, referential facial expression and
appearance via cross attention. We also conduct low-pass filtering of Gaussian
latent of the diffusion model in order to ameliorate the viewpoint-agnostic
texture problem we observed from our empirical analysis, where the
viewpoint-aware images contain identical textures on identical pixel positions
that are incomprehensible in 3D. Finally, to train NeRF with the images that
are viewpoint-aware yet are not strictly consistent in geometry, our approach
considers per-image geometric variation as a view of deformation from a shared
3D canonical space. Consequently, we construct the 3D avatar in a canonical
space of deformable NeRF by learning a set of per-image deformation via
deformation field table. We demonstrate the empirical results and discuss the
effectiveness of our method.
</p></li>
</ul>

<h3>Title: DiffDefense: Defending against Adversarial Attacks via Diffusion Models. (arXiv:2309.03702v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03702">http://arxiv.org/abs/2309.03702</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03702]] DiffDefense: Defending against Adversarial Attacks via Diffusion Models(http://arxiv.org/abs/2309.03702)</code></li>
<li>Summary: <p>This paper presents a novel reconstruction method that leverages Diffusion
Models to protect machine learning classifiers against adversarial attacks, all
without requiring any modifications to the classifiers themselves. The
susceptibility of machine learning models to minor input perturbations renders
them vulnerable to adversarial attacks. While diffusion-based methods are
typically disregarded for adversarial defense due to their slow reverse
process, this paper demonstrates that our proposed method offers robustness
against adversarial threats while preserving clean accuracy, speed, and
plug-and-play compatibility. Code at:
https://github.com/HondamunigePrasannaSilva/DiffDefence.
</p></li>
</ul>

<h3>Title: Phasic Content Fusing Diffusion Model with Directional Distribution Consistency for Few-Shot Model Adaption. (arXiv:2309.03729v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03729">http://arxiv.org/abs/2309.03729</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03729]] Phasic Content Fusing Diffusion Model with Directional Distribution Consistency for Few-Shot Model Adaption(http://arxiv.org/abs/2309.03729)</code></li>
<li>Summary: <p>Training a generative model with limited number of samples is a challenging
task. Current methods primarily rely on few-shot model adaption to train the
network. However, in scenarios where data is extremely limited (less than 10),
the generative network tends to overfit and suffers from content degradation.
To address these problems, we propose a novel phasic content fusing few-shot
diffusion model with directional distribution consistency loss, which targets
different learning objectives at distinct training stages of the diffusion
model. Specifically, we design a phasic training strategy with phasic content
fusion to help our model learn content and style information when t is large,
and learn local details of target domain when t is small, leading to an
improvement in the capture of content, style and local details. Furthermore, we
introduce a novel directional distribution consistency loss that ensures the
consistency between the generated and source distributions more efficiently and
stably than the prior methods, preventing our model from overfitting. Finally,
we propose a cross-domain structure guidance strategy that enhances structure
consistency during domain adaptation. Theoretical analysis, qualitative and
quantitative experiments demonstrate the superiority of our approach in
few-shot generative model adaption tasks compared to state-of-the-art methods.
The source code is available at:
https://github.com/sjtuplayer/few-shot-diffusion.
</p></li>
</ul>

<h3>Title: Text-to-feature diffusion for audio-visual few-shot learning. (arXiv:2309.03869v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03869">http://arxiv.org/abs/2309.03869</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03869]] Text-to-feature diffusion for audio-visual few-shot learning(http://arxiv.org/abs/2309.03869)</code></li>
<li>Summary: <p>Training deep learning models for video classification from audio-visual data
commonly requires immense amounts of labeled training data collected via a
costly process. A challenging and underexplored, yet much cheaper, setup is
few-shot learning from video data. In particular, the inherently multi-modal
nature of video data with sound and visual information has not been leveraged
extensively for the few-shot video classification task. Therefore, we introduce
a unified audio-visual few-shot video classification benchmark on three
datasets, i.e. the VGGSound-FSL, UCF-FSL, ActivityNet-FSL datasets, where we
adapt and compare ten methods. In addition, we propose AV-DIFF, a
text-to-feature diffusion framework, which first fuses the temporal and
audio-visual features via cross-modal attention and then generates multi-modal
features for the novel classes. We show that AV-DIFF obtains state-of-the-art
performance on our proposed benchmark for audio-visual (generalised) few-shot
learning. Our benchmark paves the way for effective audio-visual classification
when only limited labeled data is available. Code and data are available at
https://github.com/ExplainableML/AVDIFF-GFSL.
</p></li>
</ul>

<h3>Title: DiffusionEngine: Diffusion Model is Scalable Data Engine for Object Detection. (arXiv:2309.03893v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03893">http://arxiv.org/abs/2309.03893</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03893]] DiffusionEngine: Diffusion Model is Scalable Data Engine for Object Detection(http://arxiv.org/abs/2309.03893)</code></li>
<li>Summary: <p>Data is the cornerstone of deep learning. This paper reveals that the
recently developed Diffusion Model is a scalable data engine for object
detection. Existing methods for scaling up detection-oriented data often
require manual collection or generative models to obtain target images,
followed by data augmentation and labeling to produce training pairs, which are
costly, complex, or lacking diversity. To address these issues, we
presentDiffusionEngine (DE), a data scaling-up engine that provides
high-quality detection-oriented training pairs in a single stage. DE consists
of a pre-trained diffusion model and an effective Detection-Adapter,
contributing to generating scalable, diverse and generalizable detection data
in a plug-and-play manner. Detection-Adapter is learned to align the implicit
semantic and location knowledge in off-the-shelf diffusion models with
detection-aware signals to make better bounding-box predictions. Additionally,
we contribute two datasets, i.e., COCO-DE and VOC-DE, to scale up existing
detection benchmarks for facilitating follow-up research. Extensive experiments
demonstrate that data scaling-up via DE can achieve significant improvements in
diverse scenarios, such as various detection algorithms, self-supervised
pre-training, data-sparse, label-scarce, cross-domain, and semi-supervised
learning. For example, when using DE with a DINO-based adapter to scale up
data, mAP is improved by 3.1% on COCO, 7.6% on VOC, and 11.5% on Clipart.
</p></li>
</ul>

<h3>Title: InstructDiffusion: A Generalist Modeling Interface for Vision Tasks. (arXiv:2309.03895v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03895">http://arxiv.org/abs/2309.03895</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03895]] InstructDiffusion: A Generalist Modeling Interface for Vision Tasks(http://arxiv.org/abs/2309.03895)</code></li>
<li>Summary: <p>We present InstructDiffusion, a unifying and generic framework for aligning
computer vision tasks with human instructions. Unlike existing approaches that
integrate prior knowledge and pre-define the output space (e.g., categories and
coordinates) for each vision task, we cast diverse vision tasks into a
human-intuitive image-manipulating process whose output space is a flexible and
interactive pixel space. Concretely, the model is built upon the diffusion
process and is trained to predict pixels according to user instructions, such
as encircling the man's left shoulder in red or applying a blue mask to the
left car. InstructDiffusion could handle a variety of vision tasks, including
understanding tasks (such as segmentation and keypoint detection) and
generative tasks (such as editing and enhancement). It even exhibits the
ability to handle unseen tasks and outperforms prior methods on novel datasets.
This represents a significant step towards a generalist modeling interface for
vision tasks, advancing artificial general intelligence in the field of
computer vision.
</p></li>
</ul>

<h2>self-supervised</h2>
<h3>Title: ViewMix: Augmentation for Robust Representation in Self-Supervised Learning. (arXiv:2309.03360v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03360">http://arxiv.org/abs/2309.03360</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03360]] ViewMix: Augmentation for Robust Representation in Self-Supervised Learning(http://arxiv.org/abs/2309.03360)</code></li>
<li>Summary: <p>Joint Embedding Architecture-based self-supervised learning methods have
attributed the composition of data augmentations as a crucial factor for their
strong representation learning capabilities. While regional dropout strategies
have proven to guide models to focus on lesser indicative parts of the objects
in supervised methods, it hasn't been adopted by self-supervised methods for
generating positive pairs. This is because the regional dropout methods are not
suitable for the input sampling process of the self-supervised methodology.
Whereas dropping informative pixels from the positive pairs can result in
inefficient training, replacing patches of a specific object with a different
one can steer the model from maximizing the agreement between different
positive pairs. Moreover, joint embedding representation learning methods have
not made robustness their primary training outcome. To this end, we propose the
ViewMix augmentation policy, specially designed for self-supervised learning,
upon generating different views of the same image, patches are cut and pasted
from one view to another. By leveraging the different views created by this
augmentation strategy, multiple joint embedding-based self-supervised
methodologies obtained better localization capability and consistently
outperformed their corresponding baseline methods. It is also demonstrated that
incorporating ViewMix augmentation policy promotes robustness of the
representations in the state-of-the-art methods. Furthermore, our
experimentation and analysis of compute times suggest that ViewMix augmentation
doesn't introduce any additional overhead compared to other counterparts.
</p></li>
</ul>

<h3>Title: Self-Supervised Masked Digital Elevation Models Encoding for Low-Resource Downstream Tasks. (arXiv:2309.03367v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03367">http://arxiv.org/abs/2309.03367</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03367]] Self-Supervised Masked Digital Elevation Models Encoding for Low-Resource Downstream Tasks(http://arxiv.org/abs/2309.03367)</code></li>
<li>Summary: <p>The lack of quality labeled data is one of the main bottlenecks for training
Deep Learning models. As the task increases in complexity, there is a higher
penalty for overfitting and unstable learning. The typical paradigm employed
today is Self-Supervised learning, where the model attempts to learn from a
large corpus of unstructured and unlabeled data and then transfer that
knowledge to the required task. Some notable examples of self-supervision in
other modalities are BERT for Large Language Models, Wav2Vec for Speech
Recognition, and the Masked AutoEncoder for Vision, which all utilize
Transformers to solve a masked prediction task. GeoAI is uniquely poised to
take advantage of the self-supervised methodology due to the decades of data
collected, little of which is precisely and dependably annotated. Our goal is
to extract building and road segmentations from Digital Elevation Models (DEM)
that provide a detailed topography of the earths surface. The proposed
architecture is the Masked Autoencoder pre-trained on ImageNet (with the
limitation that there is a large domain discrepancy between ImageNet and DEM)
with an UperNet Head for decoding segmentations. We tested this model with 450
and 50 training images only, utilizing roughly 5% and 0.5% of the original data
respectively. On the building segmentation task, this model obtains an 82.1%
Intersection over Union (IoU) with 450 Images and 69.1% IoU with only 50
images. On the more challenging road detection task the model obtains an 82.7%
IoU with 450 images and 73.2% IoU with only 50 images. Any hand-labeled dataset
made today about the earths surface will be immediately obsolete due to the
constantly changing nature of the landscape. This motivates the clear necessity
for data-efficient learners that can be used for a wide variety of downstream
tasks.
</p></li>
</ul>

<h3>Title: Toward High Quality Facial Representation Learning. (arXiv:2309.03575v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03575">http://arxiv.org/abs/2309.03575</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03575]] Toward High Quality Facial Representation Learning(http://arxiv.org/abs/2309.03575)</code></li>
<li>Summary: <p>Face analysis tasks have a wide range of applications, but the universal
facial representation has only been explored in a few works. In this paper, we
explore high-performance pre-training methods to boost the face analysis tasks
such as face alignment and face parsing. We propose a self-supervised
pre-training framework, called \textbf{\it Mask Contrastive Face (MCF)}, with
mask image modeling and a contrastive strategy specially adjusted for face
domain tasks. To improve the facial representation quality, we use feature map
of a pre-trained visual backbone as a supervision item and use a partially
pre-trained decoder for mask image modeling. To handle the face identity during
the pre-training stage, we further use random masks to build contrastive
learning pairs. We conduct the pre-training on the LAION-FACE-cropped dataset,
a variants of LAION-FACE 20M, which contains more than 20 million face images
from Internet websites. For efficiency pre-training, we explore our framework
pre-training performance on a small part of LAION-FACE-cropped and verify the
superiority with different pre-training settings. Our model pre-trained with
the full pre-training dataset outperforms the state-of-the-art methods on
multiple downstream tasks. Our model achieves 0.932 NME$_{diag}$ for AFLW-19
face alignment and 93.96 F1 score for LaPa face parsing. Code is available at
https://github.com/nomewang/MCF.
</p></li>
</ul>

<h3>Title: DropPos: Pre-Training Vision Transformers by Reconstructing Dropped Positions. (arXiv:2309.03576v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03576">http://arxiv.org/abs/2309.03576</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03576]] DropPos: Pre-Training Vision Transformers by Reconstructing Dropped Positions(http://arxiv.org/abs/2309.03576)</code></li>
<li>Summary: <p>As it is empirically observed that Vision Transformers (ViTs) are quite
insensitive to the order of input tokens, the need for an appropriate
self-supervised pretext task that enhances the location awareness of ViTs is
becoming evident. To address this, we present DropPos, a novel pretext task
designed to reconstruct Dropped Positions. The formulation of DropPos is
simple: we first drop a large random subset of positional embeddings and then
the model classifies the actual position for each non-overlapping patch among
all possible positions solely based on their visual appearance. To avoid
trivial solutions, we increase the difficulty of this task by keeping only a
subset of patches visible. Additionally, considering there may be different
patches with similar visual appearances, we propose position smoothing and
attentive reconstruction strategies to relax this classification problem, since
it is not necessary to reconstruct their exact positions in these cases.
Empirical evaluations of DropPos show strong capabilities. DropPos outperforms
supervised pre-training and achieves competitive results compared with
state-of-the-art self-supervised alternatives on a wide range of downstream
benchmarks. This suggests that explicitly encouraging spatial reasoning
abilities, as DropPos does, indeed contributes to the improved location
awareness of ViTs. The code is publicly available at
https://github.com/Haochen-Wang409/DropPos.
</p></li>
</ul>

<h3>Title: Spatio-Temporal Contrastive Self-Supervised Learning for POI-level Crowd Flow Inference. (arXiv:2309.03239v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03239">http://arxiv.org/abs/2309.03239</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03239]] Spatio-Temporal Contrastive Self-Supervised Learning for POI-level Crowd Flow Inference(http://arxiv.org/abs/2309.03239)</code></li>
<li>Summary: <p>Accurate acquisition of crowd flow at Points of Interest (POIs) is pivotal
for effective traffic management, public service, and urban planning. Despite
this importance, due to the limitations of urban sensing techniques, the data
quality from most sources is inadequate for monitoring crowd flow at each POI.
This renders the inference of accurate crowd flow from low-quality data a
critical and challenging task. The complexity is heightened by three key
factors: 1) \emph{The scarcity and rarity of labeled data}, 2) \emph{The
intricate spatio-temporal dependencies among POIs}, and 3) \emph{The myriad
correlations between precise crowd flow and GPS reports}.
</p>
<p>To address these challenges, we recast the crowd flow inference problem as a
self-supervised attributed graph representation learning task and introduce a
novel \underline{C}ontrastive \underline{S}elf-learning framework for
\underline{S}patio-\underline{T}emporal data (\model). Our approach initiates
with the construction of a spatial adjacency graph founded on the POIs and
their respective distances. We then employ a contrastive learning technique to
exploit large volumes of unlabeled spatio-temporal data. We adopt a swapped
prediction approach to anticipate the representation of the target subgraph
from similar instances. Following the pre-training phase, the model is
fine-tuned with accurate crowd flow data. Our experiments, conducted on two
real-world datasets, demonstrate that the \model pre-trained on extensive noisy
data consistently outperforms models trained from scratch.
</p></li>
</ul>

<h2>foundation model</h2>
<h2>generative</h2>
<h3>Title: Autoregressive Omni-Aware Outpainting for Open-Vocabulary 360-Degree Image Generation. (arXiv:2309.03467v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03467">http://arxiv.org/abs/2309.03467</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03467]] Autoregressive Omni-Aware Outpainting for Open-Vocabulary 360-Degree Image Generation(http://arxiv.org/abs/2309.03467)</code></li>
<li>Summary: <p>A 360-degree (omni-directional) image provides an all-encompassing spherical
view of a scene. Recently, there has been an increasing interest in
synthesising 360-degree images from conventional narrow field of view (NFoV)
images captured by digital cameras and smartphones, for providing immersive
experiences in various scenarios such as virtual reality. Yet, existing methods
typically fall short in synthesizing intricate visual details or ensure the
generated images align consistently with user-provided prompts. In this study,
autoregressive omni-aware generative network (AOG-Net) is proposed for
360-degree image generation by out-painting an incomplete 360-degree image
progressively with NFoV and text guidances joinly or individually. This
autoregressive scheme not only allows for deriving finer-grained and
text-consistent patterns by dynamically generating and adjusting the process
but also offers users greater flexibility to edit their conditions throughout
the generation process. A global-local conditioning mechanism is devised to
comprehensively formulate the outpainting guidance in each autoregressive step.
Text guidances, omni-visual cues, NFoV inputs and omni-geometry are encoded and
further formulated with cross-attention based transformers into a global stream
and a local stream into a conditioned generative backbone model. As AOG-Net is
compatible to leverage large-scale models for the conditional encoder and the
generative prior, it enables the generation to use extensive open-vocabulary
text guidances. Comprehensive experiments on two commonly used 360-degree image
datasets for both indoor and outdoor settings demonstrate the state-of-the-art
performance of our proposed method. Our code will be made publicly available.
</p></li>
</ul>

<h3>Title: Perceptual Quality Assessment of 360$^\circ$ Images Based on Generative Scanpath Representation. (arXiv:2309.03472v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03472">http://arxiv.org/abs/2309.03472</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03472]] Perceptual Quality Assessment of 360$^\circ$ Images Based on Generative Scanpath Representation(http://arxiv.org/abs/2309.03472)</code></li>
<li>Summary: <p>Despite substantial efforts dedicated to the design of heuristic models for
omnidirectional (i.e., 360$^\circ$) image quality assessment (OIQA), a
conspicuous gap remains due to the lack of consideration for the diversity of
viewing behaviors that leads to the varying perceptual quality of 360$^\circ$
images. Two critical aspects underline this oversight: the neglect of viewing
conditions that significantly sway user gaze patterns and the overreliance on a
single viewport sequence from the 360$^\circ$ image for quality inference. To
address these issues, we introduce a unique generative scanpath representation
(GSR) for effective quality inference of 360$^\circ$ images, which aggregates
varied perceptual experiences of multi-hypothesis users under a predefined
viewing condition. More specifically, given a viewing condition characterized
by the starting point of viewing and exploration time, a set of scanpaths
consisting of dynamic visual fixations can be produced using an apt scanpath
generator. Following this vein, we use the scanpaths to convert the 360$^\circ$
image into the unique GSR, which provides a global overview of gazed-focused
contents derived from scanpaths. As such, the quality inference of the
360$^\circ$ image is swiftly transformed to that of GSR. We then propose an
efficient OIQA computational framework by learning the quality maps of GSR.
Comprehensive experimental results validate that the predictions of the
proposed framework are highly consistent with human perception in the
spatiotemporal domain, especially in the challenging context of locally
distorted 360$^\circ$ images under varied viewing conditions. The code will be
released at https://github.com/xiangjieSui/GSR
</p></li>
</ul>

<h3>Title: AnthroNet: Conditional Generation of Humans via Anthropometrics. (arXiv:2309.03812v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03812">http://arxiv.org/abs/2309.03812</a></li>
<li>Code URL: https://github.com/Unity-Technologies/AnthroNet</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03812]] AnthroNet: Conditional Generation of Humans via Anthropometrics(http://arxiv.org/abs/2309.03812)</code></li>
<li>Summary: <p>We present a novel human body model formulated by an extensive set of
anthropocentric measurements, which is capable of generating a wide range of
human body shapes and poses. The proposed model enables direct modeling of
specific human identities through a deep generative architecture, which can
produce humans in any arbitrary pose. It is the first of its kind to have been
trained end-to-end using only synthetically generated data, which not only
provides highly accurate human mesh representations but also allows for precise
anthropometry of the body. Moreover, using a highly diverse animation library,
we articulated our synthetic humans' body and hands to maximize the diversity
of the learnable priors for model training. Our model was trained on a dataset
of $100k$ procedurally-generated posed human meshes and their corresponding
anthropometric measurements. Our synthetic data generator can be used to
generate millions of unique human identities and poses for non-commercial
academic research purposes.
</p></li>
</ul>

<h3>Title: T2IW: Joint Text to Image & Watermark Generation. (arXiv:2309.03815v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03815">http://arxiv.org/abs/2309.03815</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03815]] T2IW: Joint Text to Image & Watermark Generation(http://arxiv.org/abs/2309.03815)</code></li>
<li>Summary: <p>Recent developments in text-conditioned image generative models have
revolutionized the production of realistic results. Unfortunately, this has
also led to an increase in privacy violations and the spread of false
information, which requires the need for traceability, privacy protection, and
other security measures. However, existing text-to-image paradigms lack the
technical capabilities to link traceable messages with image generation. In
this study, we introduce a novel task for the joint generation of text to image
and watermark (T2IW). This T2IW scheme ensures minimal damage to image quality
when generating a compound image by forcing the semantic feature and the
watermark signal to be compatible in pixels. Additionally, by utilizing
principles from Shannon information theory and non-cooperative game theory, we
are able to separate the revealed image and the revealed watermark from the
compound image. Furthermore, we strengthen the watermark robustness of our
approach by subjecting the compound image to various post-processing attacks,
with minimal pixel distortion observed in the revealed watermark. Extensive
experiments have demonstrated remarkable achievements in image quality,
watermark invisibility, and watermark robustness, supported by our proposed set
of evaluation metrics.
</p></li>
</ul>

<h3>Title: The Making and Breaking of Camouflage. (arXiv:2309.03899v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03899">http://arxiv.org/abs/2309.03899</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03899]] The Making and Breaking of Camouflage(http://arxiv.org/abs/2309.03899)</code></li>
<li>Summary: <p>Not all camouflages are equally effective, as even a partially visible
contour or a slight color difference can make the animal stand out and break
its camouflage. In this paper, we address the question of what makes a
camouflage successful, by proposing three scores for automatically assessing
its effectiveness. In particular, we show that camouflage can be measured by
the similarity between background and foreground features and boundary
visibility. We use these camouflage scores to assess and compare all available
camouflage datasets. We also incorporate the proposed camouflage score into a
generative model as an auxiliary loss and show that effective camouflage images
or videos can be synthesised in a scalable manner. The generated synthetic
dataset is used to train a transformer-based model for segmenting camouflaged
animals in videos. Experimentally, we demonstrate state-of-the-art camouflage
breaking performance on the public MoCA-Mask benchmark.
</p></li>
</ul>

<h3>Title: Exploring Sparse MoE in GANs for Text-conditioned Image Synthesis. (arXiv:2309.03904v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03904">http://arxiv.org/abs/2309.03904</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03904]] Exploring Sparse MoE in GANs for Text-conditioned Image Synthesis(http://arxiv.org/abs/2309.03904)</code></li>
<li>Summary: <p>Due to the difficulty in scaling up, generative adversarial networks (GANs)
seem to be falling from grace on the task of text-conditioned image synthesis.
Sparsely-activated mixture-of-experts (MoE) has recently been demonstrated as a
valid solution to training large-scale models with limited computational
resources. Inspired by such a philosophy, we present Aurora, a GAN-based
text-to-image generator that employs a collection of experts to learn feature
processing, together with a sparse router to help select the most suitable
expert for each feature point. To faithfully decode the sampling stochasticity
and the text condition to the final synthesis, our router adaptively makes its
decision by taking into account the text-integrated global latent code. At
64x64 image resolution, our model trained on LAION2B-en and COYO-700M achieves
6.2 zero-shot FID on MS COCO. We release the code and checkpoints to facilitate
the community for further development.
</p></li>
</ul>

<h3>Title: Exploring an LM to generate Prolog Predicates from Mathematics Questions. (arXiv:2309.03667v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03667">http://arxiv.org/abs/2309.03667</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03667]] Exploring an LM to generate Prolog Predicates from Mathematics Questions(http://arxiv.org/abs/2309.03667)</code></li>
<li>Summary: <p>Recently, there has been a surge in interest in NLP driven by ChatGPT.
ChatGPT, a transformer-based generative language model of substantial scale,
exhibits versatility in performing various tasks based on natural language.
Nevertheless, large language models often exhibit poor performance in solving
mathematics questions that require reasoning. Prior research has demonstrated
the effectiveness of chain-of-thought prompting in enhancing reasoning
capabilities. Now, we aim to investigate whether fine-tuning a model for the
generation of Prolog codes, a logic language, and subsequently passing these
codes to a compiler can further improve accuracy. Consequently, we employ
chain-of-thought to fine-tune LLaMA7B as a baseline model and develop other
fine-tuned LLaMA7B models for the generation of Prolog code, Prolog code +
chain-of-thought, and chain-of-thought + Prolog code, respectively. The results
reveal that the Prolog generation model surpasses the baseline in performance,
while the combination generation models do not yield significant improvements.
The Prolog corpus based on GSM8K and the correspondingly finetuned Prolog
generation model based on LLaMA7B are released to the research community.
</p></li>
</ul>

<h2>anomaly</h2>
<h3>Title: Reasonable Anomaly Detection in Long Sequences. (arXiv:2309.03401v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03401">http://arxiv.org/abs/2309.03401</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03401]] Reasonable Anomaly Detection in Long Sequences(http://arxiv.org/abs/2309.03401)</code></li>
<li>Summary: <p>Video anomaly detection is a challenging task due to the lack in approaches
for representing samples. The visual representations of most existing
approaches are limited by short-term sequences of observations which cannot
provide enough clues for achieving reasonable detections. In this paper, we
propose to completely represent the motion patterns of objects by learning from
long-term sequences. Firstly, a Stacked State Machine (SSM) model is proposed
to represent the temporal dependencies which are consistent across long-range
observations. Then SSM model functions in predicting future states based on
past ones, the divergence between the predictions with inherent normal patterns
and observed ones determines anomalies which violate normal motion patterns.
Extensive experiments are carried out to evaluate the proposed approach on the
dataset and existing ones. Improvements over state-of-the-art methods can be
observed. Our code is available at
https://github.com/AllenYLJiang/Anomaly-Detection-in-Sequences.
</p></li>
</ul>

<h3>Title: Personalized Tucker Decomposition: Modeling Commonality and Peculiarity on Tensor Data. (arXiv:2309.03439v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03439">http://arxiv.org/abs/2309.03439</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03439]] Personalized Tucker Decomposition: Modeling Commonality and Peculiarity on Tensor Data(http://arxiv.org/abs/2309.03439)</code></li>
<li>Summary: <p>We propose personalized Tucker decomposition (perTucker) to address the
limitations of traditional tensor decomposition methods in capturing
heterogeneity across different datasets. perTucker decomposes tensor data into
shared global components and personalized local components. We introduce a mode
orthogonality assumption and develop a proximal gradient regularized block
coordinate descent algorithm that is guaranteed to converge to a stationary
point. By learning unique and common representations across datasets, we
demonstrate perTucker's effectiveness in anomaly detection, client
classification, and clustering through a simulation study and two case studies
on solar flare detection and tonnage signal classification.
</p></li>
</ul>

<h3>Title: TSGBench: Time Series Generation Benchmark. (arXiv:2309.03755v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03755">http://arxiv.org/abs/2309.03755</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03755]] TSGBench: Time Series Generation Benchmark(http://arxiv.org/abs/2309.03755)</code></li>
<li>Summary: <p>Synthetic Time Series Generation (TSG) is crucial in a range of applications,
including data augmentation, anomaly detection, and privacy preservation.
Although significant strides have been made in this field, existing methods
exhibit three key limitations: (1) They often benchmark against similar model
types, constraining a holistic view of performance capabilities. (2) The use of
specialized synthetic and private datasets introduces biases and hampers
generalizability. (3) Ambiguous evaluation measures, often tied to custom
networks or downstream tasks, hinder consistent and fair comparison.
</p>
<p>To overcome these limitations, we introduce \textsf{TSGBench}, the inaugural
TSG Benchmark, designed for a unified and comprehensive assessment of TSG
methods. It comprises three modules: (1) a curated collection of publicly
available, real-world datasets tailored for TSG, together with a standardized
preprocessing pipeline; (2) a comprehensive evaluation measures suite including
vanilla measures, new distance-based assessments, and visualization tools; (3)
a pioneering generalization test rooted in Domain Adaptation (DA), compatible
with all methods. We have conducted extensive experiments across ten real-world
datasets from diverse domains, utilizing ten advanced TSG methods and twelve
evaluation measures, all gauged through \textsf{TSGBench}. The results
highlight its remarkable efficacy and consistency. More importantly,
\textsf{TSGBench} delivers a statistical breakdown of method rankings,
illuminating performance variations across different datasets and measures, and
offering nuanced insights into the effectiveness of each method.
</p></li>
</ul>

<h2>in-context</h2>
<h3>Title: Improving Open Information Extraction with Large Language Models: A Study on Demonstration Uncertainty. (arXiv:2309.03433v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.03433">http://arxiv.org/abs/2309.03433</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.03433]] Improving Open Information Extraction with Large Language Models: A Study on Demonstration Uncertainty(http://arxiv.org/abs/2309.03433)</code></li>
<li>Summary: <p>Open Information Extraction (OIE) task aims at extracting structured facts
from unstructured text, typically in the form of (subject, relation, object)
triples. Despite the potential of large language models (LLMs) like ChatGPT as
a general task solver, they lag behind state-of-the-art (supervised) methods in
OIE tasks due to two key issues. First, LLMs struggle to distinguish irrelevant
context from relevant relations and generate structured output due to the
restrictions on fine-tuning the model. Second, LLMs generates responses
autoregressively based on probability, which makes the predicted relations lack
confidence. In this paper, we assess the capabilities of LLMs in improving the
OIE task. Particularly, we propose various in-context learning strategies to
enhance LLM's instruction-following ability and a demonstration uncertainty
quantification module to enhance the confidence of the generated relations. Our
experiments on three OIE benchmark datasets show that our approach holds its
own against established supervised methods, both quantitatively and
qualitatively.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
