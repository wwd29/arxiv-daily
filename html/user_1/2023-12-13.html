<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>diffusion</h2>
<h3>Title: Perceptual Similarity guidance and text guidance optimization for Editing Real Images using Guided Diffusion Models. (arXiv:2312.06680v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06680">http://arxiv.org/abs/2312.06680</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06680]] Perceptual Similarity guidance and text guidance optimization for Editing Real Images using Guided Diffusion Models(http://arxiv.org/abs/2312.06680)</code></li>
<li>Summary: <p>When using a diffusion model for image editing, there are times when the
modified image can differ greatly from the source. To address this, we apply a
dual-guidance approach to maintain high fidelity to the original in areas that
are not altered. First, we employ text-guided optimization, using text
embeddings to direct latent space and classifier-free guidance. Second, we use
perceptual similarity guidance, optimizing latent vectors with posterior
sampling via Tweedie formula during the reverse process. This method ensures
the realistic rendering of both the edited elements and the preservation of the
unedited parts of the original image.
</p></li>
</ul>

<h3>Title: SIFU: Side-view Conditioned Implicit Function for Real-world Usable Clothed Human Reconstruction. (arXiv:2312.06704v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06704">http://arxiv.org/abs/2312.06704</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06704]] SIFU: Side-view Conditioned Implicit Function for Real-world Usable Clothed Human Reconstruction(http://arxiv.org/abs/2312.06704)</code></li>
<li>Summary: <p>Creating high-quality 3D models of clothed humans from single images for
real-world applications is crucial. Despite recent advancements, accurately
reconstructing humans in complex poses or with loose clothing from in-the-wild
images, along with predicting textures for unseen areas, remains a significant
challenge. A key limitation of previous methods is their insufficient prior
guidance in transitioning from 2D to 3D and in texture prediction. In response,
we introduce SIFU (Side-view Conditioned Implicit Function for Real-world
Usable Clothed Human Reconstruction), a novel approach combining a Side-view
Decoupling Transformer with a 3D Consistent Texture Refinement pipeline.SIFU
employs a cross-attention mechanism within the transformer, using SMPL-X
normals as queries to effectively decouple side-view features in the process of
mapping 2D features to 3D. This method not only improves the precision of the
3D models but also their robustness, especially when SMPL-X estimates are not
perfect. Our texture refinement process leverages text-to-image diffusion-based
prior to generate realistic and consistent textures for invisible views.
Through extensive experiments, SIFU surpasses SOTA methods in both geometry and
texture reconstruction, showcasing enhanced robustness in complex scenarios and
achieving an unprecedented Chamfer and P2S measurement. Our approach extends to
practical applications such as 3D printing and scene building, demonstrating
its broad utility in real-world scenarios. Project page
https://river-zhang.github.io/SIFU-projectpage/ .
</p></li>
</ul>

<h3>Title: Neutral Editing Framework for Diffusion-based Video Editing. (arXiv:2312.06708v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06708">http://arxiv.org/abs/2312.06708</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06708]] Neutral Editing Framework for Diffusion-based Video Editing(http://arxiv.org/abs/2312.06708)</code></li>
<li>Summary: <p>Text-conditioned image editing has succeeded in various types of editing
based on a diffusion framework. Unfortunately, this success did not carry over
to a video, which continues to be challenging. Existing video editing systems
are still limited to rigid-type editing such as style transfer and object
overlay. To this end, this paper proposes Neutral Editing (NeuEdit) framework
to enable complex non-rigid editing by changing the motion of a person/object
in a video, which has never been attempted before. NeuEdit introduces a concept
of `neutralization' that enhances a tuning-editing process of diffusion-based
editing systems in a model-agnostic manner by leveraging input video and text
without any other auxiliary aids (e.g., visual masks, video captions).
Extensive experiments on numerous videos demonstrate adaptability and
effectiveness of the NeuEdit framework. The website of our work is available
here: https://neuedit.github.io
</p></li>
</ul>

<h3>Title: Separate-and-Enhance: Compositional Finetuning for Text2Image Diffusion Models. (arXiv:2312.06712v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06712">http://arxiv.org/abs/2312.06712</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06712]] Separate-and-Enhance: Compositional Finetuning for Text2Image Diffusion Models(http://arxiv.org/abs/2312.06712)</code></li>
<li>Summary: <p>Despite recent significant strides achieved by diffusion-based Text-to-Image
(T2I) models, current systems are still less capable of ensuring decent
compositional generation aligned with text prompts, particularly for the
multi-object generation. This work illuminates the fundamental reasons for such
misalignment, pinpointing issues related to low attention activation scores and
mask overlaps. While previous research efforts have individually tackled these
issues, we assert that a holistic approach is paramount. Thus, we propose two
novel objectives, the Separate loss and the Enhance loss, that reduce object
mask overlaps and maximize attention scores, respectively. Our method diverges
from conventional test-time-adaptation techniques, focusing on finetuning
critical parameters, which enhances scalability and generalizability.
Comprehensive evaluations demonstrate the superior performance of our model in
terms of image realism, text-image alignment, and adaptability, notably
outperforming prominent baselines. Ultimately, this research paves the way for
T2I diffusion models with enhanced compositional capacities and broader
applicability. The project webpage is available at
https://zpbao.github.io/projects/SepEn/.
</p></li>
</ul>

<h3>Title: EpiDiff: Enhancing Multi-View Synthesis via Localized Epipolar-Constrained Diffusion. (arXiv:2312.06725v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06725">http://arxiv.org/abs/2312.06725</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06725]] EpiDiff: Enhancing Multi-View Synthesis via Localized Epipolar-Constrained Diffusion(http://arxiv.org/abs/2312.06725)</code></li>
<li>Summary: <p>Generating multiview images from a single view facilitates the rapid
generation of a 3D mesh conditioned on a single image. Recent methods that
introduce 3D global representation into diffusion models have shown the
potential to generate consistent multiviews, but they have reduced generation
speed and face challenges in maintaining generalizability and quality. To
address this issue, we propose EpiDiff, a localized interactive multiview
diffusion model. At the core of the proposed approach is to insert a
lightweight epipolar attention block into the frozen diffusion model,
leveraging epipolar constraints to enable cross-view interaction among feature
maps of neighboring views. The newly initialized 3D modeling module preserves
the original feature distribution of the diffusion model, exhibiting
compatibility with a variety of base diffusion models. Experiments show that
EpiDiff generates 16 multiview images in just 12 seconds, and it surpasses
previous methods in quality evaluation metrics, including PSNR, SSIM and LPIPS.
Additionally, EpiDiff can generate a more diverse distribution of views,
improving the reconstruction quality from generated multiviews. Please see our
project page at https://huanngzh.github.io/EpiDiff/.
</p></li>
</ul>

<h3>Title: DiffCast: A Unified Framework via Residual Diffusion for Precipitation Nowcasting. (arXiv:2312.06734v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06734">http://arxiv.org/abs/2312.06734</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06734]] DiffCast: A Unified Framework via Residual Diffusion for Precipitation Nowcasting(http://arxiv.org/abs/2312.06734)</code></li>
<li>Summary: <p>Precipitation nowcasting is an important spatio-temporal prediction task to
predict the radar echoes sequences based on current observations, which can
serve both meteorological science and smart city applications. Due to the
chaotic evolution nature of the precipitation systems, it is a very challenging
problem. Previous studies address the problem either from the perspectives of
deterministic modeling or probabilistic modeling. However, their predictions
suffer from the blurry, high-value echoes fading away and position inaccurate
issues. The root reason of these issues is that the chaotic evolutionary
precipitation systems are not appropriately modeled. Inspired by the nature of
the systems, we propose to decompose and model them from the perspective of
global deterministic motion and local stochastic variations with residual
mechanism. A unified and flexible framework that can equip any type of
spatio-temporal models is proposed based on residual diffusion, which
effectively tackles the shortcomings of previous methods. Extensive
experimental results on four publicly available radar datasets demonstrate the
effectiveness and superiority of the proposed framework, compared to
state-of-the-art techniques. Our code will be made publicly available soon.
</p></li>
</ul>

<h3>Title: InstructAny2Pix: Flexible Visual Editing via Multimodal Instruction Following. (arXiv:2312.06738v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06738">http://arxiv.org/abs/2312.06738</a></li>
<li>Code URL: https://github.com/jacklishufan/instructany2pix</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06738]] InstructAny2Pix: Flexible Visual Editing via Multimodal Instruction Following(http://arxiv.org/abs/2312.06738)</code></li>
<li>Summary: <p>The ability to provide fine-grained control for generating and editing visual
imagery has profound implications for computer vision and its applications.
Previous works have explored extending controllability in two directions:
instruction tuning with text-based prompts and multi-modal conditioning.
However, these works make one or more unnatural assumptions on the number
and/or type of modality inputs used to express controllability. We propose
InstructAny2Pix, a flexible multi-modal instruction-following system that
enables users to edit an input image using instructions involving audio,
images, and text. InstructAny2Pix consists of three building blocks that
facilitate this capability: a multi-modal encoder that encodes different
modalities such as images and audio into a unified latent space, a diffusion
model that learns to decode representations in this latent space into images,
and a multi-modal LLM that can understand instructions involving multiple
images and audio pieces and generate a conditional embedding of the desired
output, which can be used by the diffusion decoder. Additionally, to facilitate
training efficiency and improve generation quality, we include an additional
refinement prior module that enhances the visual quality of LLM outputs. These
designs are critical to the performance of our system. We demonstrate that our
system can perform a series of novel instruction-guided editing tasks. The code
is available at https://github.com/jacklishufan/InstructAny2Pix.git
</p></li>
</ul>

<h3>Title: SmartEdit: Exploring Complex Instruction-based Image Editing with Multimodal Large Language Models. (arXiv:2312.06739v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06739">http://arxiv.org/abs/2312.06739</a></li>
<li>Code URL: https://github.com/TencentARC/SmartEdit</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06739]] SmartEdit: Exploring Complex Instruction-based Image Editing with Multimodal Large Language Models(http://arxiv.org/abs/2312.06739)</code></li>
<li>Summary: <p>Current instruction-based editing methods, such as InstructPix2Pix, often
fail to produce satisfactory results in complex scenarios due to their
dependence on the simple CLIP text encoder in diffusion models. To rectify
this, this paper introduces SmartEdit, a novel approach to instruction-based
image editing that leverages Multimodal Large Language Models (MLLMs) to
enhance their understanding and reasoning capabilities. However, direct
integration of these elements still faces challenges in situations requiring
complex reasoning. To mitigate this, we propose a Bidirectional Interaction
Module that enables comprehensive bidirectional information interactions
between the input image and the MLLM output. During training, we initially
incorporate perception data to boost the perception and understanding
capabilities of diffusion models. Subsequently, we demonstrate that a small
amount of complex instruction editing data can effectively stimulate
SmartEdit's editing capabilities for more complex instructions. We further
construct a new evaluation dataset, Reason-Edit, specifically tailored for
complex instruction-based image editing. Both quantitative and qualitative
results on this evaluation dataset indicate that our SmartEdit surpasses
previous methods, paving the way for the practical application of complex
instruction-based image editing.
</p></li>
</ul>

<h3>Title: Relightful Harmonization: Lighting-aware Portrait Background Replacement. (arXiv:2312.06886v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06886">http://arxiv.org/abs/2312.06886</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06886]] Relightful Harmonization: Lighting-aware Portrait Background Replacement(http://arxiv.org/abs/2312.06886)</code></li>
<li>Summary: <p>Portrait harmonization aims to composite a subject into a new background,
adjusting its lighting and color to ensure harmony with the background scene.
Existing harmonization techniques often only focus on adjusting the global
color and brightness of the foreground and ignore crucial illumination cues
from the background such as apparent lighting direction, leading to unrealistic
compositions. We introduce Relightful Harmonization, a lighting-aware diffusion
model designed to seamlessly harmonize sophisticated lighting effect for the
foreground portrait using any background image. Our approach unfolds in three
stages. First, we introduce a lighting representation module that allows our
diffusion model to encode lighting information from target image background.
Second, we introduce an alignment network that aligns lighting features learned
from image background with lighting features learned from panorama environment
maps, which is a complete representation for scene illumination. Last, to
further boost the photorealism of the proposed method, we introduce a novel
data simulation pipeline that generates synthetic training pairs from a diverse
range of natural images, which are used to refine the model. Our method
outperforms existing benchmarks in visual fidelity and lighting coherence,
showing superior generalization in real-world testing scenarios, highlighting
its versatility and practicality.
</p></li>
</ul>

<h3>Title: LoRA-Enhanced Distillation on Guided Diffusion Models. (arXiv:2312.06899v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06899">http://arxiv.org/abs/2312.06899</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06899]] LoRA-Enhanced Distillation on Guided Diffusion Models(http://arxiv.org/abs/2312.06899)</code></li>
<li>Summary: <p>Diffusion models, such as Stable Diffusion (SD), offer the ability to
generate high-resolution images with diverse features, but they come at a
significant computational and memory cost. In classifier-free guided diffusion
models, prolonged inference times are attributed to the necessity of computing
two separate diffusion models at each denoising step. Recent work has shown
promise in improving inference time through distillation techniques, teaching
the model to perform similar denoising steps with reduced computations.
However, the application of distillation introduces additional memory overhead
to these already resource-intensive diffusion models, making it less practical.
</p>
<p>To address these challenges, our research explores a novel approach that
combines Low-Rank Adaptation (LoRA) with model distillation to efficiently
compress diffusion models. This approach not only reduces inference time but
also mitigates memory overhead, and notably decreases memory consumption even
before applying distillation. The results are remarkable, featuring a
significant reduction in inference time due to the distillation process and a
substantial 50% reduction in memory consumption. Our examination of the
generated images underscores that the incorporation of LoRA-enhanced
distillation maintains image quality and alignment with the provided prompts.
In summary, while conventional distillation tends to increase memory
consumption, LoRA-enhanced distillation offers optimization without any
trade-offs or compromises in quality.
</p></li>
</ul>

<h3>Title: CCM: Adding Conditional Controls to Text-to-Image Consistency Models. (arXiv:2312.06971v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06971">http://arxiv.org/abs/2312.06971</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06971]] CCM: Adding Conditional Controls to Text-to-Image Consistency Models(http://arxiv.org/abs/2312.06971)</code></li>
<li>Summary: <p>Consistency Models (CMs) have showed a promise in creating visual content
efficiently and with high quality. However, the way to add new conditional
controls to the pretrained CMs has not been explored. In this technical report,
we consider alternative strategies for adding ControlNet-like conditional
control to CMs and present three significant findings. 1) ControlNet trained
for diffusion models (DMs) can be directly applied to CMs for high-level
semantic controls but struggles with low-level detail and realism control. 2)
CMs serve as an independent class of generative models, based on which
ControlNet can be trained from scratch using Consistency Training proposed by
Song et al. 3) A lightweight adapter can be jointly optimized under multiple
conditions through Consistency Training, allowing for the swift transfer of
DMs-based ControlNet to CMs. We study these three solutions across various
conditional controls, including edge, depth, human pose, low-resolution image
and masked image with text-to-image latent consistency models.
</p></li>
</ul>

<h3>Title: Diff-OP3D: Bridging 2D Diffusion for Open Pose 3D Zero-Shot Classification. (arXiv:2312.07039v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07039">http://arxiv.org/abs/2312.07039</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07039]] Diff-OP3D: Bridging 2D Diffusion for Open Pose 3D Zero-Shot Classification(http://arxiv.org/abs/2312.07039)</code></li>
<li>Summary: <p>With the explosive 3D data growth, the urgency of utilizing zero-shot
learning to facilitate data labeling becomes evident. Recently, the methods via
transferring Contrastive Language-Image Pre-training (CLIP) to 3D vision have
made great progress in the 3D zero-shot classification task. However, these
methods primarily focus on aligned pose 3D objects (ap-3os), overlooking the
recognition of 3D objects with open poses (op-3os) typically encountered in
real-world scenarios, such as an overturned chair or a lying teddy bear. To
this end, we propose a more challenging benchmark for 3D open-pose zero-shot
classification. Echoing our benchmark, we design a concise angle-refinement
mechanism that automatically optimizes one ideal pose as well as classifies
these op-3os. Furthermore, we make a first attempt to bridge 2D pre-trained
diffusion model as a classifer to 3D zero-shot classification without any
additional training. Such 2D diffusion to 3D objects proves vital in improving
zero-shot classification for both ap-3os and op-3os. Our model notably improves
by 3.5% and 15.8% on ModelNet10$^{\ddag}$ and McGill$^{\ddag}$ open pose
benchmarks, respectively, and surpasses the current state-of-the-art by 6.8% on
the aligned pose ModelNet10, affirming diffusion's efficacy in 3D zero-shot
tasks.
</p></li>
</ul>

<h3>Title: Template Free Reconstruction of Human-object Interaction with Procedural Interaction Generation. (arXiv:2312.07063v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07063">http://arxiv.org/abs/2312.07063</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07063]] Template Free Reconstruction of Human-object Interaction with Procedural Interaction Generation(http://arxiv.org/abs/2312.07063)</code></li>
<li>Summary: <p>Reconstructing human-object interaction in 3D from a single RGB image is a
challenging task and existing data driven methods do not generalize beyond the
objects present in the carefully curated 3D interaction datasets. Capturing
large-scale real data to learn strong interaction and 3D shape priors is very
expensive due to the combinatorial nature of human-object interactions. In this
paper, we propose ProciGen (Procedural interaction Generation), a method to
procedurally generate datasets with both, plausible interaction and diverse
object variation. We generate 1M+ human-object interaction pairs in 3D and
leverage this large-scale data to train our HDM (Hierarchical Diffusion Model),
a novel method to reconstruct interacting human and unseen objects, without any
templates. Our HDM is an image-conditioned diffusion model that learns both
realistic interaction and highly accurate human and object shapes. Experiments
show that our HDM trained with ProciGen significantly outperforms prior methods
that requires template meshes and that our dataset allows training methods with
strong generalization ability to unseen object instances. Our code and data
will be publicly released at:
https://virtualhumans.mpi-inf.mpg.de/procigen-hdm.
</p></li>
</ul>

<h3>Title: DiffuVST: Narrating Fictional Scenes with Global-History-Guided Denoising Models. (arXiv:2312.07066v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07066">http://arxiv.org/abs/2312.07066</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07066]] DiffuVST: Narrating Fictional Scenes with Global-History-Guided Denoising Models(http://arxiv.org/abs/2312.07066)</code></li>
<li>Summary: <p>Recent advances in image and video creation, especially AI-based image
synthesis, have led to the production of numerous visual scenes that exhibit a
high level of abstractness and diversity. Consequently, Visual Storytelling
(VST), a task that involves generating meaningful and coherent narratives from
a collection of images, has become even more challenging and is increasingly
desired beyond real-world imagery. While existing VST techniques, which
typically use autoregressive decoders, have made significant progress, they
suffer from low inference speed and are not well-suited for synthetic scenes.
To this end, we propose a novel diffusion-based system DiffuVST, which models
the generation of a series of visual descriptions as a single conditional
denoising process. The stochastic and non-autoregressive nature of DiffuVST at
inference time allows it to generate highly diverse narratives more
efficiently. In addition, DiffuVST features a unique design with bi-directional
text history guidance and multimodal adapter modules, which effectively improve
inter-sentence coherence and image-to-text fidelity. Extensive experiments on
the story generation task covering four fictional visual-story datasets
demonstrate the superiority of DiffuVST over traditional autoregressive models
in terms of both text quality and inference speed.
</p></li>
</ul>

<h3>Title: Text2AC-Zero: Consistent Synthesis of Animated Characters using 2D Diffusion. (arXiv:2312.07133v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07133">http://arxiv.org/abs/2312.07133</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07133]] Text2AC-Zero: Consistent Synthesis of Animated Characters using 2D Diffusion(http://arxiv.org/abs/2312.07133)</code></li>
<li>Summary: <p>We propose a zero-shot approach for consistent Text-to-Animated-Characters
synthesis based on pre-trained Text-to-Image (T2I) diffusion models. Existing
Text-to-Video (T2V) methods are expensive to train and require large-scale
video datasets to produce diverse characters and motions. At the same time,
their zero-shot alternatives fail to produce temporally consistent videos. We
strive to bridge this gap, and we introduce a zero-shot approach that produces
temporally consistent videos of animated characters and requires no training or
fine-tuning. We leverage existing text-based motion diffusion models to
generate diverse motions that we utilize to guide a T2I model. To achieve
temporal consistency, we introduce the Spatial Latent Alignment module that
exploits cross-frame dense correspondences that we compute to align the latents
of the video frames. Furthermore, we propose Pixel-Wise Guidance to steer the
diffusion process in a direction that minimizes visual discrepancies. Our
proposed approach generates temporally consistent videos with diverse motions
and styles, outperforming existing zero-shot T2V approaches in terms of
pixel-wise consistency and user preference.
</p></li>
</ul>

<h3>Title: Fast Training of Diffusion Transformer with Extreme Masking for 3D Point Clouds Generation. (arXiv:2312.07231v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07231">http://arxiv.org/abs/2312.07231</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07231]] Fast Training of Diffusion Transformer with Extreme Masking for 3D Point Clouds Generation(http://arxiv.org/abs/2312.07231)</code></li>
<li>Summary: <p>Diffusion Transformers have recently shown remarkable effectiveness in
generating high-quality 3D point clouds. However, training voxel-based
diffusion models for high-resolution 3D voxels remains prohibitively expensive
due to the cubic complexity of attention operators, which arises from the
additional dimension of voxels. Motivated by the inherent redundancy of 3D
compared to 2D, we propose FastDiT-3D, a novel masked diffusion transformer
tailored for efficient 3D point cloud generation, which greatly reduces
training costs. Specifically, we draw inspiration from masked autoencoders to
dynamically operate the denoising process on masked voxelized point clouds. We
also propose a novel voxel-aware masking strategy to adaptively aggregate
background/foreground information from voxelized point clouds. Our method
achieves state-of-the-art performance with an extreme masking ratio of nearly
99%. Moreover, to improve multi-category 3D generation, we introduce
Mixture-of-Expert (MoE) in 3D diffusion model. Each category can learn a
distinct diffusion path with different experts, relieving gradient conflict.
Experimental results on the ShapeNet dataset demonstrate that our method
achieves state-of-the-art high-fidelity and diverse 3D point cloud generation
performance. Our FastDiT-3D improves 1-Nearest Neighbor Accuracy and Coverage
metrics when generating 128-resolution voxel point clouds, using only 6.5% of
the original training cost.
</p></li>
</ul>

<h3>Title: Scalable Motion Style Transfer with Constrained Diffusion Generation. (arXiv:2312.07311v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07311">http://arxiv.org/abs/2312.07311</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07311]] Scalable Motion Style Transfer with Constrained Diffusion Generation(http://arxiv.org/abs/2312.07311)</code></li>
<li>Summary: <p>Current training of motion style transfer systems relies on consistency
losses across style domains to preserve contents, hindering its scalable
application to a large number of domains and private data. Recent image
transfer works show the potential of independent training on each domain by
leveraging implicit bridging between diffusion models, with the content
preservation, however, limited to simple data patterns. We address this by
imposing biased sampling in backward diffusion while maintaining the domain
independence in the training stage. We construct the bias from the source
domain keyframes and apply them as the gradient of content constraints,
yielding a framework with keyframe manifold constraint gradients (KMCGs). Our
validation demonstrates the success of training separate models to transfer
between as many as ten dance motion styles. Comprehensive experiments find a
significant improvement in preserving motion contents in comparison to baseline
and ablative diffusion-based style transfer models. In addition, we perform a
human study for a subjective assessment of the quality of generated dance
motions. The results validate the competitiveness of KMCGs.
</p></li>
</ul>

<h3>Title: GenHowTo: Learning to Generate Actions and State Transformations from Instructional Videos. (arXiv:2312.07322v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07322">http://arxiv.org/abs/2312.07322</a></li>
<li>Code URL: https://github.com/soCzech/GenHowto</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07322]] GenHowTo: Learning to Generate Actions and State Transformations from Instructional Videos(http://arxiv.org/abs/2312.07322)</code></li>
<li>Summary: <p>We address the task of generating temporally consistent and physically
plausible images of actions and object state transformations. Given an input
image and a text prompt describing the targeted transformation, our generated
images preserve the environment and transform objects in the initial image. Our
contributions are threefold. First, we leverage a large body of instructional
videos and automatically mine a dataset of triplets of consecutive frames
corresponding to initial object states, actions, and resulting object
transformations. Second, equipped with this data, we develop and train a
conditioned diffusion model dubbed GenHowTo. Third, we evaluate GenHowTo on a
variety of objects and actions and show superior performance compared to
existing methods. In particular, we introduce a quantitative evaluation where
GenHowTo achieves 88% and 74% on seen and unseen interaction categories,
respectively, outperforming prior work by a large margin.
</p></li>
</ul>

<h3>Title: Learned representation-guided diffusion models for large-image generation. (arXiv:2312.07330v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07330">http://arxiv.org/abs/2312.07330</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07330]] Learned representation-guided diffusion models for large-image generation(http://arxiv.org/abs/2312.07330)</code></li>
<li>Summary: <p>To synthesize high-fidelity samples, diffusion models typically require
auxiliary data to guide the generation process. However, it is impractical to
procure the painstaking patch-level annotation effort required in specialized
domains like histopathology and satellite imagery; it is often performed by
domain experts and involves hundreds of millions of patches. Modern-day
self-supervised learning (SSL) representations encode rich semantic and visual
information. In this paper, we posit that such representations are expressive
enough to act as proxies to fine-grained human labels. We introduce a novel
approach that trains diffusion models conditioned on embeddings from SSL. Our
diffusion models successfully project these features back to high-quality
histopathology and remote sensing images. In addition, we construct larger
images by assembling spatially consistent patches inferred from SSL embeddings,
preserving long-range dependencies. Augmenting real data by generating
variations of real images improves downstream classifier accuracy for
patch-level and larger, image-scale classification tasks. Our models are
effective even on datasets not encountered during training, demonstrating their
robustness and generalizability. Generating images from learned embeddings is
agnostic to the source of the embeddings. The SSL embeddings used to generate a
large image can either be extracted from a reference image, or sampled from an
auxiliary model conditioned on any related modality (e.g. class labels, text,
genomic data). As proof of concept, we introduce the text-to-large image
synthesis paradigm where we successfully synthesize large pathology and
satellite images out of text descriptions.
</p></li>
</ul>

<h3>Title: Boosting Latent Diffusion with Flow Matching. (arXiv:2312.07360v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07360">http://arxiv.org/abs/2312.07360</a></li>
<li>Code URL: https://github.com/compvis/fm-boosting</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07360]] Boosting Latent Diffusion with Flow Matching(http://arxiv.org/abs/2312.07360)</code></li>
<li>Summary: <p>Recently, there has been tremendous progress in visual synthesis and the
underlying generative models. Here, diffusion models (DMs) stand out
particularly, but lately, flow matching (FM) has also garnered considerable
interest. While DMs excel in providing diverse images, they suffer from long
training and slow generation. With latent diffusion, these issues are only
partially alleviated. Conversely, FM offers faster training and inference but
exhibits less diversity in synthesis. We demonstrate that introducing FM
between the Diffusion model and the convolutional decoder offers
high-resolution image synthesis with reduced computational cost and model size.
Diffusion can then efficiently provide the necessary generation diversity. FM
compensates for the lower resolution, mapping the small latent space to a
high-dimensional one. Subsequently, the convolutional decoder of the LDM maps
these latents to high-resolution images. By combining the diversity of DMs, the
efficiency of FMs, and the effectiveness of convolutional decoders, we achieve
state-of-the-art high-resolution image synthesis at $1024^2$ with minimal
computational cost. Importantly, our approach is orthogonal to recent
approximation and speed-up strategies for the underlying DMs, making it easily
integrable into various DM frameworks.
</p></li>
</ul>

<h3>Title: DiffMorpher: Unleashing the Capability of Diffusion Models for Image Morphing. (arXiv:2312.07409v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07409">http://arxiv.org/abs/2312.07409</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07409]] DiffMorpher: Unleashing the Capability of Diffusion Models for Image Morphing(http://arxiv.org/abs/2312.07409)</code></li>
<li>Summary: <p>Diffusion models have achieved remarkable image generation quality surpassing
previous generative models. However, a notable limitation of diffusion models,
in comparison to GANs, is their difficulty in smoothly interpolating between
two image samples, due to their highly unstructured latent space. Such a smooth
interpolation is intriguing as it naturally serves as a solution for the image
morphing task with many applications. In this work, we present DiffMorpher, the
first approach enabling smooth and natural image interpolation using diffusion
models. Our key idea is to capture the semantics of the two images by fitting
two LoRAs to them respectively, and interpolate between both the LoRA
parameters and the latent noises to ensure a smooth semantic transition, where
correspondence automatically emerges without the need for annotation. In
addition, we propose an attention interpolation and injection technique and a
new sampling schedule to further enhance the smoothness between consecutive
images. Extensive experiments demonstrate that DiffMorpher achieves starkly
better image morphing effects than previous methods across a variety of object
categories, bridging a critical functional gap that distinguished diffusion
models from GANs.
</p></li>
</ul>

<h3>Title: MinD-3D: Reconstruct High-quality 3D objects in Human Brain. (arXiv:2312.07485v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07485">http://arxiv.org/abs/2312.07485</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07485]] MinD-3D: Reconstruct High-quality 3D objects in Human Brain(http://arxiv.org/abs/2312.07485)</code></li>
<li>Summary: <p>In this paper, we introduce Recon3DMind, a groundbreaking task focused on
reconstructing 3D visuals from Functional Magnetic Resonance Imaging (fMRI)
signals. This represents a major step forward in cognitive neuroscience and
computer vision. To support this task, we present the fMRI-Shape dataset,
utilizing 360-degree view videos of 3D objects for comprehensive fMRI signal
capture. Containing 55 categories of common objects from daily life, this
dataset will bolster future research endeavors. We also propose MinD-3D, a
novel and effective three-stage framework that decodes and reconstructs the
brain's 3D visual information from fMRI signals. This method starts by
extracting and aggregating features from fMRI frames using a neuro-fusion
encoder, then employs a feature bridge diffusion model to generate
corresponding visual features, and ultimately recovers the 3D object through a
generative transformer decoder. Our experiments demonstrate that this method
effectively extracts features that are valid and highly correlated with visual
regions of interest (ROIs) in fMRI signals. Notably, it not only reconstructs
3D objects with high semantic relevance and spatial similarity but also
significantly deepens our understanding of the human brain's 3D visual
processing capabilities. Project page at: https://jianxgao.github.io/MinD-3D.
</p></li>
</ul>

<h3>Title: Class-Prototype Conditional Diffusion Model for Continual Learning with Generative Replay. (arXiv:2312.06710v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06710">http://arxiv.org/abs/2312.06710</a></li>
<li>Code URL: https://github.com/dnkhanh45/cpdm</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06710]] Class-Prototype Conditional Diffusion Model for Continual Learning with Generative Replay(http://arxiv.org/abs/2312.06710)</code></li>
<li>Summary: <p>Mitigating catastrophic forgetting is a key hurdle in continual learning.
Deep Generative Replay (GR) provides techniques focused on generating samples
from prior tasks to enhance the model's memory capabilities. With the
progression in generative AI, generative models have advanced from Generative
Adversarial Networks (GANs) to the more recent Diffusion Models (DMs). A major
issue is the deterioration in the quality of generated data compared to the
original, as the generator continuously self-learns from its outputs. This
degradation can lead to the potential risk of catastrophic forgetting occurring
in the classifier. To address this, we propose the Class-Prototype Conditional
Diffusion Model (CPDM), a GR-based approach for continual learning that
enhances image quality in generators and thus reduces catastrophic forgetting
in classifiers. The cornerstone of CPDM is a learnable class-prototype that
captures the core characteristics of images in a given class. This prototype,
integrated into the diffusion model's denoising process, ensures the generation
of high-quality images. It maintains its effectiveness for old tasks even when
new tasks are introduced, preserving image generation quality and reducing the
risk of catastrophic forgetting in classifiers. Our empirical studies on
diverse datasets demonstrate that our proposed method significantly outperforms
existing state-of-the-art models, highlighting its exceptional ability to
preserve image quality and enhance the model's memory retention.
</p></li>
</ul>

<h3>Title: Generating High-Resolution Regional Precipitation Using Conditional Diffusion Model. (arXiv:2312.07112v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07112">http://arxiv.org/abs/2312.07112</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07112]] Generating High-Resolution Regional Precipitation Using Conditional Diffusion Model(http://arxiv.org/abs/2312.07112)</code></li>
<li>Summary: <p>Climate downscaling is a crucial technique within climate research, serving
to project low-resolution (LR) climate data to higher resolutions (HR).
Previous research has demonstrated the effectiveness of deep learning for
downscaling tasks. However, most deep learning models for climate downscaling
may not perform optimally for high scaling factors (i.e., 4x, 8x) due to their
limited ability to capture the intricate details required for generating HR
climate data. Furthermore, climate data behaves differently from image data,
necessitating a nuanced approach when employing deep generative models. In
response to these challenges, this paper presents a deep generative model for
downscaling climate data, specifically precipitation on a regional scale. We
employ a denoising diffusion probabilistic model (DDPM) conditioned on multiple
LR climate variables. The proposed model is evaluated using precipitation data
from the Community Earth System Model (CESM) v1.2.2 simulation. Our results
demonstrate significant improvements over existing baselines, underscoring the
effectiveness of the conditional diffusion model in downscaling climate data.
</p></li>
</ul>

<h3>Title: Equivariant Flow Matching with Hybrid Probability Transport. (arXiv:2312.07168v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07168">http://arxiv.org/abs/2312.07168</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07168]] Equivariant Flow Matching with Hybrid Probability Transport(http://arxiv.org/abs/2312.07168)</code></li>
<li>Summary: <p>The generation of 3D molecules requires simultaneously deciding the
categorical features~(atom types) and continuous features~(atom coordinates).
Deep generative models, especially Diffusion Models (DMs), have demonstrated
effectiveness in generating feature-rich geometries. However, existing DMs
typically suffer from unstable probability dynamics with inefficient sampling
speed. In this paper, we introduce geometric flow matching, which enjoys the
advantages of both equivariant modeling and stabilized probability dynamics.
More specifically, we propose a hybrid probability path where the coordinates
probability path is regularized by an equivariant optimal transport, and the
information between different modalities is aligned. Experimentally, the
proposed method could consistently achieve better performance on multiple
molecule generation benchmarks with 4.75$\times$ speed up of sampling on
average.
</p></li>
</ul>

<h3>Title: Momentum Particle Maximum Likelihood. (arXiv:2312.07335v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07335">http://arxiv.org/abs/2312.07335</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07335]] Momentum Particle Maximum Likelihood(http://arxiv.org/abs/2312.07335)</code></li>
<li>Summary: <p>Maximum likelihood estimation (MLE) of latent variable models is often recast
as an optimization problem over the extended space of parameters and
probability distributions. For example, the Expectation Maximization (EM)
algorithm can be interpreted as coordinate descent applied to a suitable free
energy functional over this space. Recently, this perspective has been combined
with insights from optimal transport and Wasserstein gradient flows to develop
particle-based algorithms applicable to wider classes of models than standard
EM.
</p>
<p>Drawing inspiration from prior works which interpret `momentum-enriched'
optimisation algorithms as discretizations of ordinary differential equations,
we propose an analogous dynamical systems-inspired approach to minimizing the
free energy functional over the extended space of parameters and probability
distributions. The result is a dynamic system that blends elements of
Nesterov's Accelerated Gradient method, the underdamped Langevin diffusion, and
particle methods.
</p>
<p>Under suitable assumptions, we establish quantitative convergence of the
proposed system to the unique minimiser of the functional in continuous time.
We then propose a numerical discretization of this system which enables its
application to parameter estimation in latent variable models. Through
numerical experiments, we demonstrate that the resulting algorithm converges
faster than existing methods and compares favourably with other (approximate)
MLE algorithms.
</p></li>
</ul>

<h2>self-supervised</h2>
<h3>Title: Benchmarking Pretrained Vision Embeddings for Near- and Duplicate Detection in Medical Images. (arXiv:2312.07273v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07273">http://arxiv.org/abs/2312.07273</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07273]] Benchmarking Pretrained Vision Embeddings for Near- and Duplicate Detection in Medical Images(http://arxiv.org/abs/2312.07273)</code></li>
<li>Summary: <p>Near- and duplicate image detection is a critical concern in the field of
medical imaging. Medical datasets often contain similar or duplicate images
from various sources, which can lead to significant performance issues and
evaluation biases, especially in machine learning tasks due to data leakage
between training and testing subsets. In this paper, we present an approach for
identifying near- and duplicate 3D medical images leveraging publicly available
2D computer vision embeddings. We assessed our approach by comparing embeddings
extracted from two state-of-the-art self-supervised pretrained models and two
different vector index structures for similarity retrieval. We generate an
experimental benchmark based on the publicly available Medical Segmentation
Decathlon dataset. The proposed method yields promising results for near- and
duplicate image detection achieving a mean sensitivity and specificity of
0.9645 and 0.8559, respectively.
</p></li>
</ul>

<h3>Title: NearbyPatchCL: Leveraging Nearby Patches for Self-Supervised Patch-Level Multi-Class Classification in Whole-Slide Images. (arXiv:2312.07489v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07489">http://arxiv.org/abs/2312.07489</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07489]] NearbyPatchCL: Leveraging Nearby Patches for Self-Supervised Patch-Level Multi-Class Classification in Whole-Slide Images(http://arxiv.org/abs/2312.07489)</code></li>
<li>Summary: <p>Whole-slide image (WSI) analysis plays a crucial role in cancer diagnosis and
treatment. In addressing the demands of this critical task, self-supervised
learning (SSL) methods have emerged as a valuable resource, leveraging their
efficiency in circumventing the need for a large number of annotations, which
can be both costly and time-consuming to deploy supervised methods.
Nevertheless, patch-wise representation may exhibit instability in performance,
primarily due to class imbalances stemming from patch selection within WSIs. In
this paper, we introduce Nearby Patch Contrastive Learning (NearbyPatchCL), a
novel self-supervised learning method that leverages nearby patches as positive
samples and a decoupled contrastive loss for robust representation learning.
Our method demonstrates a tangible enhancement in performance for downstream
tasks involving patch-level multi-class classification. Additionally, we curate
a new dataset derived from WSIs sourced from the Canine Cutaneous Cancer
Histology, thus establishing a benchmark for the rigorous evaluation of
patch-level multi-class classification methodologies. Intensive experiments
show that our method significantly outperforms the supervised baseline and
state-of-the-art SSL methods with top-1 classification accuracy of 87.56%. Our
method also achieves comparable results while utilizing a mere 1% of labeled
data, a stark contrast to the 100% labeled data requirement of other
approaches. Source code: https://github.com/nvtien457/NearbyPatchCL
</p></li>
</ul>

<h3>Title: Evaluating Self-supervised Speech Models on a Taiwanese Hokkien Corpus. (arXiv:2312.06668v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06668">http://arxiv.org/abs/2312.06668</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06668]] Evaluating Self-supervised Speech Models on a Taiwanese Hokkien Corpus(http://arxiv.org/abs/2312.06668)</code></li>
<li>Summary: <p>Taiwanese Hokkien is declining in use and status due to a language shift
towards Mandarin in Taiwan. This is partly why it is a low resource language in
NLP and speech research today. To ensure that the state of the art in speech
processing does not leave Taiwanese Hokkien behind, we contribute a 1.5-hour
dataset of Taiwanese Hokkien to ML-SUPERB's hidden set. Evaluating ML-SUPERB's
suite of self-supervised learning (SSL) speech representations on our dataset,
we find that model size does not consistently determine performance. In fact,
certain smaller models outperform larger ones. Furthermore, linguistic
alignment between pretraining data and the target language plays a crucial
role.
</p></li>
</ul>

<h3>Title: Multimodal Pretraining of Medical Time Series and Notes. (arXiv:2312.06855v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06855">http://arxiv.org/abs/2312.06855</a></li>
<li>Code URL: https://github.com/kingrc15/multimodal-clinical-pretraining</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06855]] Multimodal Pretraining of Medical Time Series and Notes(http://arxiv.org/abs/2312.06855)</code></li>
<li>Summary: <p>Within the intensive care unit (ICU), a wealth of patient data, including
clinical measurements and clinical notes, is readily available. This data is a
valuable resource for comprehending patient health and informing medical
decisions, but it also contains many challenges in analysis. Deep learning
models show promise in extracting meaningful patterns, but they require
extensive labeled data, a challenge in critical care. To address this, we
propose a novel approach employing self-supervised pretraining, focusing on the
alignment of clinical measurements and notes. Our approach combines contrastive
and masked token prediction tasks during pretraining. Semi-supervised
experiments on the MIMIC-III dataset demonstrate the effectiveness of our
self-supervised pretraining. In downstream tasks, including in-hospital
mortality prediction and phenotyping, our pretrained model outperforms
baselines in settings where only a fraction of the data is labeled, emphasizing
its ability to enhance ICU data analysis. Notably, our method excels in
situations where very few labels are available, as evidenced by an increase in
the AUC-ROC for in-hospital mortality by 0.17 and in AUC-PR for phenotyping by
0.1 when only 1% of labels are accessible. This work advances self-supervised
learning in the healthcare domain, optimizing clinical insights from abundant
yet challenging ICU data.
</p></li>
</ul>

<h3>Title: Self-supervised Adaptive Pre-training of Multilingual Speech Models for Language and Dialect Identification. (arXiv:2312.07338v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07338">http://arxiv.org/abs/2312.07338</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07338]] Self-supervised Adaptive Pre-training of Multilingual Speech Models for Language and Dialect Identification(http://arxiv.org/abs/2312.07338)</code></li>
<li>Summary: <p>Pre-trained Transformer-based speech models have shown striking performance
when fine-tuned on various downstream tasks such as automatic speech
recognition and spoken language identification (SLID). However, the problem of
domain mismatch remains a challenge in this area, where the domain of the
pre-training data might differ from that of the downstream labeled data used
for fine-tuning. In multilingual tasks such as SLID, the pre-trained speech
model may not support all the languages in the downstream task. To address this
challenge, we propose self-supervised adaptive pre-training (SAPT) to adapt the
pre-trained model to the target domain and languages of the downstream task. We
apply SAPT to the XLSR-128 model and investigate the effectiveness of this
approach for the SLID task. First, we demonstrate that SAPT improves XLSR
performance on the FLEURS benchmark with substantial gains up to 40.1% for
under-represented languages. Second, we apply SAPT on four different datasets
in a few-shot learning setting, showing that our approach improves the sample
efficiency of XLSR during fine-tuning. Our experiments provide strong empirical
evidence that continual adaptation via self-supervision improves downstream
performance for multilingual speech models.
</p></li>
</ul>

<h2>foundation model</h2>
<h3>Title: AM-RADIO: Agglomerative Model -- Reduce All Domains Into One. (arXiv:2312.06709v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06709">http://arxiv.org/abs/2312.06709</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06709]] AM-RADIO: Agglomerative Model -- Reduce All Domains Into One(http://arxiv.org/abs/2312.06709)</code></li>
<li>Summary: <p>A handful of visual foundation models (VFMs) have recently emerged as the
backbones for numerous downstream tasks. VFMs like CLIP, DINOv2, SAM are
trained with distinct objectives, exhibiting unique characteristics for various
downstream tasks. We find that despite their conceptual differences, these
models can be effectively merged into a unified model through multi-teacher
distillation. We name this approach AM-RADIO (Agglomerative Model -- Reduce All
Domains Into One). This integrative approach not only surpasses the performance
of individual teacher models but also amalgamates their distinctive features,
such as zero-shot vision-language comprehension, detailed pixel-level
understanding, and open vocabulary segmentation capabilities. In pursuit of the
most hardware-efficient backbone, we evaluated numerous architectures in our
multi-teacher distillation pipeline using the same training recipe. This led to
the development of a novel architecture (E-RADIO) that exceeds the performance
of its predecessors and is at least 7x faster than the teacher models. Our
comprehensive benchmarking process covers downstream tasks including ImageNet
classification, ADE20k semantic segmentation, COCO object detection and
LLaVa-1.5 framework.
</p>
<p>Code: https://github.com/NVlabs/RADIO
</p></li>
</ul>

<h3>Title: SqueezeSAM: User friendly mobile interactive segmentation. (arXiv:2312.06736v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06736">http://arxiv.org/abs/2312.06736</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06736]] SqueezeSAM: User friendly mobile interactive segmentation(http://arxiv.org/abs/2312.06736)</code></li>
<li>Summary: <p>Segment Anything Model (SAM) is a foundation model for interactive
segmentation, and it has catalyzed major advances in generative AI,
computational photography, and medical imaging. This model takes in an
arbitrary user input and provides segmentation masks of the corresponding
objects. It is our goal to develop a version of SAM that is appropriate for use
in a photography app. The original SAM model has a few challenges in this
setting. First, original SAM a 600 million parameter based on ViT-H, and its
high computational cost and large model size that are not suitable for todays
mobile hardware. We address this by proposing the SqueezeSAM model
architecture, which is 50x faster and 100x smaller than SAM. Next, when a user
takes a photo on their phone, it might not occur to them to click on the image
and get a mask. Our solution is to use salient object detection to generate the
first few clicks. This produces an initial segmentation mask that the user can
interactively edit. Finally, when a user clicks on an object, they typically
expect all related pieces of the object to be segmented. For instance, if a
user clicks on a person t-shirt in a photo, they expect the whole person to be
segmented, but SAM typically segments just the t-shirt. We address this with a
new data augmentation scheme, and the end result is that if the user clicks on
a person holding a basketball, the person and the basketball are all segmented
together.
</p></li>
</ul>

<h3>Title: Remote Sensing Vision-Language Foundation Models without Annotations via Ground Remote Alignment. (arXiv:2312.06960v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06960">http://arxiv.org/abs/2312.06960</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06960]] Remote Sensing Vision-Language Foundation Models without Annotations via Ground Remote Alignment(http://arxiv.org/abs/2312.06960)</code></li>
<li>Summary: <p>We introduce a method to train vision-language models for remote-sensing
images without using any textual annotations. Our key insight is to use
co-located internet imagery taken on the ground as an intermediary for
connecting remote-sensing images and language. Specifically, we train an image
encoder for remote sensing images to align with the image encoder of CLIP using
a large amount of paired internet and satellite images. Our unsupervised
approach enables the training of a first-of-its-kind large-scale vision
language model (VLM) for remote sensing images at two different resolutions. We
show that these VLMs enable zero-shot, open-vocabulary image classification,
retrieval, segmentation and visual question answering for satellite images. On
each of these tasks, our VLM trained without textual annotations outperforms
existing VLMs trained with supervision, with gains of up to 20% for
classification and 80% for segmentation.
</p></li>
</ul>

<h3>Title: Efficient Few-Shot Clinical Task Adaptation with Large Language Models. (arXiv:2312.07125v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07125">http://arxiv.org/abs/2312.07125</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07125]] Efficient Few-Shot Clinical Task Adaptation with Large Language Models(http://arxiv.org/abs/2312.07125)</code></li>
<li>Summary: <p>Few-shot learning has been studied to adapt models to tasks with very few
samples. It holds profound significance, particularly in clinical tasks, due to
the high annotation cost of medical images. Several works have explored
few-shot learning on medical images, yet they still require a large number of
medical images for pre-training models to gain domain-specific priors. Vision
foundation models recently have achieved remarkable success in natural images.
Hence, adapting rapidly advancing vision foundation models from natural images
to few-shot clinical tasks holds great promise. MedFMC has recently organized a
challenge to shed more light on this topic at NeurIPS 2023. In this work, we
present our challenge solution. We observe that a simple variant of fine-tuning
with partial freezing shows remarkable performance. Empirical evidence
demonstrates that this approach could outperform various common fine-tuning
methods under limited sample sizes. Additionally, we explore enhanced
utilization of semantic supervision to boost performance. We propose a novel
approach that contextualizes labels via large language models (LLMs). Our
findings reveal that the context generated by LLMs significantly enhances the
discrimination of semantic embeddings for similar categories, resulting in a
notable performance improvement of 3%-5% in 1-shot settings compared to
commonly employed one-hot labels and other semantic supervision methods. Our
solution secures the 1st place in the MedFMC challenge.
</p></li>
</ul>

<h3>Title: How Well Does GPT-4V(ision) Adapt to Distribution Shifts? A Preliminary Investigation. (arXiv:2312.07424v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07424">http://arxiv.org/abs/2312.07424</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07424]] How Well Does GPT-4V(ision) Adapt to Distribution Shifts? A Preliminary Investigation(http://arxiv.org/abs/2312.07424)</code></li>
<li>Summary: <p>In machine learning, generalization against distribution shifts -- where
deployment conditions diverge from the training scenarios -- is crucial,
particularly in fields like climate modeling, biomedicine, and autonomous
driving. The emergence of foundation models, distinguished by their extensive
pretraining and task versatility, has led to an increased interest in their
adaptability to distribution shifts. GPT-4V(ision) acts as the most advanced
publicly accessible multimodal foundation model, with extensive applications
across various domains, including anomaly detection, video understanding, image
generation, and medical diagnosis. However, its robustness against data
distributions remains largely underexplored. Addressing this gap, this study
rigorously evaluates GPT-4V's adaptability and generalization capabilities in
dynamic environments, benchmarking against prominent models like CLIP and
LLaVA. We delve into GPT-4V's zero-shot generalization across 13 diverse
datasets spanning natural, medical, and molecular domains. We further
investigate its adaptability to controlled data perturbations and examine the
efficacy of in-context learning as a tool to enhance its adaptation. Our
findings delineate GPT-4V's capability boundaries in distribution shifts,
shedding light on its strengths and limitations across various scenarios.
Importantly, this investigation contributes to our understanding of how AI
foundation models generalize to distribution shifts, offering pivotal insights
into their adaptability and robustness. Code is publicly available at
https://github.com/jameszhou-gl/gpt-4v-distribution-shift.
</p></li>
</ul>

<h3>Title: Model Breadcrumbs: Scaling Multi-Task Model Merging with Sparse Masks. (arXiv:2312.06795v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06795">http://arxiv.org/abs/2312.06795</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06795]] Model Breadcrumbs: Scaling Multi-Task Model Merging with Sparse Masks(http://arxiv.org/abs/2312.06795)</code></li>
<li>Summary: <p>The rapid development of AI systems has been greatly influenced by the
emergence of foundation models. A common approach for targeted problems
involves fine-tuning these pre-trained foundation models for specific target
tasks, resulting in a rapid spread of models fine-tuned across a diverse array
of tasks. This work focuses on the problem of merging multiple fine-tunings of
the same foundation model derived from a spectrum of auxiliary tasks. We
introduce a new simple method, Model Breadcrumbs, which consists of a sparsely
defined set of weights that carve out a trajectory within the weight space of a
pre-trained model, enhancing task performance when traversed. These breadcrumbs
are constructed by subtracting the weights from a pre-trained model before and
after fine-tuning, followed by a sparsification process that eliminates weight
outliers and negligible perturbations. Our experiments demonstrate the
effectiveness of Model Breadcrumbs to simultaneously improve performance across
multiple tasks. This contribution aligns with the evolving paradigm of
updatable machine learning, reminiscent of the collaborative principles
underlying open-source software development, fostering a community-driven
effort to reliably update machine learning models. Our method is shown to be
more efficient and unlike previous proposals does not require hyperparameter
tuning for each new task added. Through extensive experimentation involving
various models, tasks, and modalities we establish that integrating Model
Breadcrumbs offers a simple, efficient, and highly effective approach for
constructing multi-task models and facilitating updates to foundation models.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Leveraging Generative Language Models for Weakly Supervised Sentence Component Analysis in Video-Language Joint Learning. (arXiv:2312.06699v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06699">http://arxiv.org/abs/2312.06699</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06699]] Leveraging Generative Language Models for Weakly Supervised Sentence Component Analysis in Video-Language Joint Learning(http://arxiv.org/abs/2312.06699)</code></li>
<li>Summary: <p>A thorough comprehension of textual data is a fundamental element in
multi-modal video analysis tasks. However, recent works have shown that the
current models do not achieve a comprehensive understanding of the textual data
during the training for the target downstream tasks. Orthogonal to the previous
approaches to this limitation, we postulate that understanding the significance
of the sentence components according to the target task can potentially enhance
the performance of the models. Hence, we utilize the knowledge of a pre-trained
large language model (LLM) to generate text samples from the original ones,
targeting specific sentence components. We propose a weakly supervised
importance estimation module to compute the relative importance of the
components and utilize them to improve different video-language tasks. Through
rigorous quantitative analysis, our proposed method exhibits significant
improvement across several video-language tasks. In particular, our approach
notably enhances video-text retrieval by a relative improvement of 8.3\% in
video-to-text and 1.4\% in text-to-video retrieval over the baselines, in terms
of R@1. Additionally, in video moment retrieval, average mAP shows a relative
improvement ranging from 2.0\% to 13.7 \% across different baselines.
</p></li>
</ul>

<h3>Title: Deciphering 'What' and 'Where' Visual Pathways from Spectral Clustering of Layer-Distributed Neural Representations. (arXiv:2312.06716v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06716">http://arxiv.org/abs/2312.06716</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06716]] Deciphering 'What' and 'Where' Visual Pathways from Spectral Clustering of Layer-Distributed Neural Representations(http://arxiv.org/abs/2312.06716)</code></li>
<li>Summary: <p>We present an approach for analyzing grouping information contained within a
neural network's activations, permitting extraction of spatial layout and
semantic segmentation from the behavior of large pre-trained vision models.
Unlike prior work, our method conducts a wholistic analysis of a network's
activation state, leveraging features from all layers and obviating the need to
guess which part of the model contains relevant information. Motivated by
classic spectral clustering, we formulate this analysis in terms of an
optimization objective involving a set of affinity matrices, each formed by
comparing features within a different layer. Solving this optimization problem
using gradient descent allows our technique to scale from single images to
dataset-level analysis, including, in the latter, both intra- and inter-image
relationships. Analyzing a pre-trained generative transformer provides insight
into the computational strategy learned by such models. Equating affinity with
key-query similarity across attention layers yields eigenvectors encoding scene
spatial layout, whereas defining affinity by value vector similarity yields
eigenvectors encoding object identity. This result suggests that key and query
vectors coordinate attentional information flow according to spatial proximity
(a `where' pathway), while value vectors refine a semantic category
representation (a `what' pathway).
</p></li>
</ul>

<h3>Title: Image Content Generation with Causal Reasoning. (arXiv:2312.07132v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07132">http://arxiv.org/abs/2312.07132</a></li>
<li>Code URL: https://github.com/ieit-agi/mix-shannon</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07132]] Image Content Generation with Causal Reasoning(http://arxiv.org/abs/2312.07132)</code></li>
<li>Summary: <p>The emergence of ChatGPT has once again sparked research in generative
artificial intelligence (GAI). While people have been amazed by the generated
results, they have also noticed the reasoning potential reflected in the
generated textual content. However, this current ability for causal reasoning
is primarily limited to the domain of language generation, such as in models
like GPT-3. In visual modality, there is currently no equivalent research.
Considering causal reasoning in visual content generation is significant. This
is because visual information contains infinite granularity. Particularly,
images can provide more intuitive and specific demonstrations for certain
reasoning tasks, especially when compared to coarse-grained text. Hence, we
propose a new image generation task called visual question answering with image
(VQAI) and establish a dataset of the same name based on the classic
\textit{Tom and Jerry} animated series. Additionally, we develop a new paradigm
for image generation to tackle the challenges of this task. Finally, we perform
extensive experiments and analyses, including visualizations of the generated
content and discussions on the potentials and limitations. The code and data
are publicly available under the license of CC BY-NC-SA 4.0 for academic and
non-commercial usage. The code and dataset are publicly available at:
https://github.com/IEIT-AGI/MIX-Shannon/blob/main/projects/VQAI/lgd_vqai.md.
</p></li>
</ul>

<h3>Title: SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative Language Models. (arXiv:2312.07492v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07492">http://arxiv.org/abs/2312.07492</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07492]] SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative Language Models(http://arxiv.org/abs/2312.07492)</code></li>
<li>Summary: <p>Current datasets for unwanted social bias auditing are limited to studying
protected demographic features such as race and gender. In this work, we
introduce a comprehensive benchmark that is meant to capture the amplification
of social bias, via stigmas, in generative language models. We start with a
comprehensive list of 93 stigmas documented in social science literature and
curate a question-answering (QA) dataset which involves simple social
situations. Our benchmark, SocialStigmaQA, contains roughly 10K prompts, with a
variety of prompt styles, carefully constructed to systematically test for both
social bias and model robustness. We present results for SocialStigmaQA with
two widely used open source generative language models and we demonstrate that
the output generated by these models considerably amplifies existing social
bias against stigmatized groups. Specifically, we find that the proportion of
socially biased output ranges from 45% to 59% across a variety of decoding
strategies and prompting styles. We discover that the deliberate design of the
templates in our benchmark (e.g., by adding biasing text to the prompt or
varying the answer that indicates bias) impact the model tendencies to generate
socially biased output. Additionally, we report on patterns in the generated
chain-of-thought output, finding a variety of problems from subtle bias to
evidence of a lack of reasoning.
</p>
<p>Warning: This paper contains examples of text which is toxic, biased, and
harmful.
</p></li>
</ul>

<h2>anomaly</h2>
<h2>in-context</h2>
<h3>Title: Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an In-Context Attack. (arXiv:2312.06924v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.06924">http://arxiv.org/abs/2312.06924</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.06924]] Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an In-Context Attack(http://arxiv.org/abs/2312.06924)</code></li>
<li>Summary: <p>Recent developments in balancing the usefulness and safety of Large Language
Models (LLMs) have raised a critical question: Are mainstream NLP tasks
adequately aligned with safety consideration? Our study, focusing on
safety-sensitive documents obtained through adversarial attacks, reveals
significant disparities in the safety alignment of various NLP tasks. For
instance, LLMs can effectively summarize malicious long documents but often
refuse to translate them. This discrepancy highlights a previously unidentified
vulnerability: attacks exploiting tasks with weaker safety alignment, like
summarization, can potentially compromise the integraty of tasks traditionally
deemed more robust, such as translation and question-answering (QA). Moreover,
the concurrent use of multiple NLP tasks with lesser safety alignment increases
the risk of LLMs inadvertently processing harmful content. We demonstrate these
vulnerabilities in various safety-aligned LLMs, particularly Llama2 models and
GPT-4, indicating an urgent need for strengthening safety alignments across a
broad spectrum of NLP tasks.
</p></li>
</ul>

<h3>Title: Improving Factual Error Correction by Learning to Inject Factual Errors. (arXiv:2312.07049v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07049">http://arxiv.org/abs/2312.07049</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07049]] Improving Factual Error Correction by Learning to Inject Factual Errors(http://arxiv.org/abs/2312.07049)</code></li>
<li>Summary: <p>Factual error correction (FEC) aims to revise factual errors in false claims
with minimal editing, making them faithful to the provided evidence. This task
is crucial for alleviating the hallucination problem encountered by large
language models. Given the lack of paired data (i.e., false claims and their
corresponding correct claims), existing methods typically adopt the
mask-then-correct paradigm. This paradigm relies solely on unpaired false
claims and correct claims, thus being referred to as distantly supervised
methods. These methods require a masker to explicitly identify factual errors
within false claims before revising with a corrector. However, the absence of
paired data to train the masker makes accurately pinpointing factual errors
within claims challenging. To mitigate this, we propose to improve FEC by
Learning to Inject Factual Errors (LIFE), a three-step distantly supervised
method: mask-corrupt-correct. Specifically, we first train a corruptor using
the mask-then-corrupt procedure, allowing it to deliberately introduce factual
errors into correct text. The corruptor is then applied to correct claims,
generating a substantial amount of paired data. After that, we filter out
low-quality data, and use the remaining data to train a corrector. Notably, our
corrector does not require a masker, thus circumventing the bottleneck
associated with explicit factual error identification. Our experiments on a
public dataset verify the effectiveness of LIFE in two key aspects: Firstly, it
outperforms the previous best-performing distantly supervised method by a
notable margin of 10.59 points in SARI Final (19.3% improvement). Secondly,
even compared to ChatGPT prompted with in-context examples, LIFE achieves a
superiority of 7.16 points in SARI Final.
</p></li>
</ul>

<h3>Title: ICL Markup: Structuring In-Context Learning using Soft-Token Tags. (arXiv:2312.07405v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07405">http://arxiv.org/abs/2312.07405</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07405]] ICL Markup: Structuring In-Context Learning using Soft-Token Tags(http://arxiv.org/abs/2312.07405)</code></li>
<li>Summary: <p>Large pretrained language models (LLMs) can be rapidly adapted to a wide
variety of tasks via a text-to-text approach, where the instruction and input
are fed to the model in natural language. Combined with in-context learning
(ICL), this paradigm is impressively flexible and powerful. However, it also
burdens users with an overwhelming number of choices, many of them arbitrary.
Inspired by markup languages like HTML, we contribute a method of using
soft-token tags to compose prompt templates. This approach reduces arbitrary
decisions and streamlines the application of ICL. Our method is a form of
meta-learning for ICL; it learns these tags in advance during a
parameter-efficient fine-tuning ``warm-up'' process. The tags can subsequently
be used in templates for ICL on new, unseen tasks without any additional
fine-tuning. Our experiments with this approach yield promising initial
results, improving LLM performance on important enterprise applications such as
few-shot and open-world intent detection, as well as text classification in
news and legal domains.
</p></li>
</ul>

<h3>Title: Comparable Demonstrations are Important in In-Context Learning: A Novel Perspective on Demonstration Selection. (arXiv:2312.07476v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.07476">http://arxiv.org/abs/2312.07476</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.07476]] Comparable Demonstrations are Important in In-Context Learning: A Novel Perspective on Demonstration Selection(http://arxiv.org/abs/2312.07476)</code></li>
<li>Summary: <p>In-Context Learning (ICL) is an important paradigm for adapting Large
Language Models (LLMs) to downstream tasks through a few demonstrations.
Despite the great success of ICL, the limitation of the demonstration number
may lead to demonstration bias, i.e. the input-label mapping induced by LLMs
misunderstands the task's essence. Inspired by human experience, we attempt to
mitigate such bias through the perspective of the inter-demonstration
relationship. Specifically, we construct Comparable Demonstrations (CDs) by
minimally editing the texts to flip the corresponding labels, in order to
highlight the task's essence and eliminate potential spurious correlations
through the inter-demonstration comparison. Through a series of experiments on
CDs, we find that (1) demonstration bias does exist in LLMs, and CDs can
significantly reduce such bias; (2) CDs exhibit good performance in ICL,
especially in out-of-distribution scenarios. In summary, this study explores
the ICL mechanisms from a novel perspective, providing a deeper insight into
the demonstration selection strategy for ICL.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
