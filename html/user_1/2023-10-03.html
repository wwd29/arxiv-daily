<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>diffusion</h2>
<h3>Title: Text-image Alignment for Diffusion-based Perception. (arXiv:2310.00031v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.00031">http://arxiv.org/abs/2310.00031</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.00031]] Text-image Alignment for Diffusion-based Perception(http://arxiv.org/abs/2310.00031)</code></li>
<li>Summary: <p>Diffusion models are generative models with impressive text-to-image
synthesis capabilities and have spurred a new wave of creative methods for
classical machine learning tasks. However, the best way to harness the
perceptual knowledge of these generative models for visual tasks is still an
open question. Specifically, it is unclear how to use the prompting interface
when applying diffusion backbones to vision tasks. We find that automatically
generated captions can improve text-image alignment and significantly enhance a
model's cross-attention maps, leading to better perceptual performance. Our
approach improves upon the current SOTA in diffusion-based semantic
segmentation on ADE20K and the current overall SOTA in depth estimation on
NYUv2. Furthermore, our method generalizes to the cross-domain setting; we use
model personalization and caption modifications to align our model to the
target domain and find improvements over unaligned baselines. Our object
detection model, trained on Pascal VOC, achieves SOTA results on Watercolor2K.
Our segmentation method, trained on Cityscapes, achieves SOTA results on Dark
Zurich-val and Nighttime Driving.
</p></li>
</ul>

<h3>Title: Robustness of AI-Image Detectors: Fundamental Limits and Practical Attacks. (arXiv:2310.00076v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.00076">http://arxiv.org/abs/2310.00076</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.00076]] Robustness of AI-Image Detectors: Fundamental Limits and Practical Attacks(http://arxiv.org/abs/2310.00076)</code></li>
<li>Summary: <p>In light of recent advancements in generative AI models, it has become
essential to distinguish genuine content from AI-generated one to prevent the
malicious usage of fake materials as authentic ones and vice versa. Various
techniques have been introduced for identifying AI-generated images, with
watermarking emerging as a promising approach. In this paper, we analyze the
robustness of various AI-image detectors including watermarking and
classifier-based deepfake detectors. For watermarking methods that introduce
subtle image perturbations (i.e., low perturbation budget methods), we reveal a
fundamental trade-off between the evasion error rate (i.e., the fraction of
watermarked images detected as non-watermarked ones) and the spoofing error
rate (i.e., the fraction of non-watermarked images detected as watermarked
ones) upon an application of a diffusion purification attack. In this regime,
we also empirically show that diffusion purification effectively removes
watermarks with minimal changes to images. For high perturbation watermarking
methods where notable changes are applied to images, the diffusion purification
attack is not effective. In this case, we develop a model substitution
adversarial attack that can successfully remove watermarks. Moreover, we show
that watermarking methods are vulnerable to spoofing attacks where the attacker
aims to have real images (potentially obscene) identified as watermarked ones,
damaging the reputation of the developers. In particular, by just having
black-box access to the watermarking method, we show that one can generate a
watermarked noise image which can be added to the real images to have them
falsely flagged as watermarked ones. Finally, we extend our theory to
characterize a fundamental trade-off between the robustness and reliability of
classifier-based deep fake detectors and demonstrate it through experiments.
</p></li>
</ul>

<h3>Title: Towards Few-Call Model Stealing via Active Self-Paced Knowledge Distillation and Diffusion-Based Image Generation. (arXiv:2310.00096v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.00096">http://arxiv.org/abs/2310.00096</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.00096]] Towards Few-Call Model Stealing via Active Self-Paced Knowledge Distillation and Diffusion-Based Image Generation(http://arxiv.org/abs/2310.00096)</code></li>
<li>Summary: <p>Diffusion models showcased strong capabilities in image synthesis, being used
in many computer vision tasks with great success. To this end, we propose to
explore a new use case, namely to copy black-box classification models without
having access to the original training data, the architecture, and the weights
of the model, \ie~the model is only exposed through an inference API. More
specifically, we can only observe the (soft or hard) labels for some image
samples passed as input to the model. Furthermore, we consider an additional
constraint limiting the number of model calls, mostly focusing our research on
few-call model stealing. In order to solve the model extraction task given the
applied restrictions, we propose the following framework. As training data, we
create a synthetic data set (called proxy data set) by leveraging the ability
of diffusion models to generate realistic and diverse images. Given a maximum
number of allowed API calls, we pass the respective number of samples through
the black-box model to collect labels. Finally, we distill the knowledge of the
black-box teacher (attacked model) into a student model (copy of the attacked
model), harnessing both labeled and unlabeled data generated by the diffusion
model. We employ a novel active self-paced learning framework to make the most
of the proxy data during distillation. Our empirical results on two data sets
confirm the superiority of our framework over two state-of-the-art methods in
the few-call model extraction scenario.
</p></li>
</ul>

<h3>Title: FashionFlow: Leveraging Diffusion Models for Dynamic Fashion Video Synthesis from Static Imagery. (arXiv:2310.00106v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.00106">http://arxiv.org/abs/2310.00106</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.00106]] FashionFlow: Leveraging Diffusion Models for Dynamic Fashion Video Synthesis from Static Imagery(http://arxiv.org/abs/2310.00106)</code></li>
<li>Summary: <p>Our study introduces a new image-to-video generator called FashionFlow. By
utilising a diffusion model, we are able to create short videos from still
images. Our approach involves developing and connecting relevant components
with the diffusion model, which sets our work apart. The components include the
use of pseudo-3D convolutional layers to generate videos efficiently. VAE and
CLIP encoders capture vital characteristics from still images to influence the
diffusion model. Our research demonstrates a successful synthesis of fashion
videos featuring models posing from various angles, showcasing the fit and
appearance of the garment. Our findings hold great promise for improving and
enhancing the shopping experience for the online fashion industry.
</p></li>
</ul>

<h3>Title: Steered Diffusion: A Generalized Framework for Plug-and-Play Conditional Image Synthesis. (arXiv:2310.00224v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.00224">http://arxiv.org/abs/2310.00224</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.00224]] Steered Diffusion: A Generalized Framework for Plug-and-Play Conditional Image Synthesis(http://arxiv.org/abs/2310.00224)</code></li>
<li>Summary: <p>Conditional generative models typically demand large annotated training sets
to achieve high-quality synthesis. As a result, there has been significant
interest in designing models that perform plug-and-play generation, i.e., to
use a predefined or pretrained model, which is not explicitly trained on the
generative task, to guide the generative process (e.g., using language).
However, such guidance is typically useful only towards synthesizing high-level
semantics rather than editing fine-grained details as in image-to-image
translation tasks. To this end, and capitalizing on the powerful fine-grained
generative control offered by the recent diffusion-based generative models, we
introduce Steered Diffusion, a generalized framework for photorealistic
zero-shot conditional image generation using a diffusion model trained for
unconditional generation. The key idea is to steer the image generation of the
diffusion model at inference time via designing a loss using a pre-trained
inverse model that characterizes the conditional task. This loss modulates the
sampling trajectory of the diffusion process. Our framework allows for easy
incorporation of multiple conditions during inference. We present experiments
using steered diffusion on several tasks including inpainting, colorization,
text-guided semantic editing, and image super-resolution. Our results
demonstrate clear qualitative and quantitative improvements over
state-of-the-art diffusion-based plug-and-play models while adding negligible
additional computational cost.
</p></li>
</ul>

<h3>Title: On the Counting of Involutory MDS Matrices. (arXiv:2310.00090v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.00090">http://arxiv.org/abs/2310.00090</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.00090]] On the Counting of Involutory MDS Matrices(http://arxiv.org/abs/2310.00090)</code></li>
<li>Summary: <p>The optimal branch number of MDS matrices has established their prominence in
the design of diffusion layers for various block ciphers and hash functions.
Consequently, several matrix structures have been proposed for designing MDS
matrices, including Hadamard and circulant matrices. In this paper, we first
provide the count of Hadamard MDS matrices of order $4$ over the field
$\mathbb{F}_{2^r}$. Subsequently, we present the counts of order $2$ MDS
matrices and order $2$ involutory MDS matrices over the field
$\mathbb{F}_{2^r}$. Finally, leveraging these counts of order $2$ matrices, we
derive an upper bound for the number of all involutory MDS matrices of order
$4$ over $\mathbb{F}_{2^r}$.
</p></li>
</ul>

<h2>self-supervised</h2>
<h3>Title: Joint Self-supervised Depth and Optical Flow Estimation towards Dynamic Objects. (arXiv:2310.00011v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.00011">http://arxiv.org/abs/2310.00011</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.00011]] Joint Self-supervised Depth and Optical Flow Estimation towards Dynamic Objects(http://arxiv.org/abs/2310.00011)</code></li>
<li>Summary: <p>Significant attention has been attracted to deep learning-based depth
estimates. Dynamic objects become the most hard problems in
inter-frame-supervised depth estimates due to the uncertainty in adjacent
frames. Thus, integrating optical flow information with depth estimation is a
feasible solution, as the optical flow is an essential motion representation.
In this work, we construct a joint inter-frame-supervised depth and optical
flow estimation framework, which predicts depths in various motions by
minimizing pixel wrap errors in bilateral photometric re-projections and
optical vectors. For motion segmentation, we adaptively segment the preliminary
estimated optical flow map with large areas of connectivity. In self-supervised
depth estimation, different motion regions are predicted independently and then
composite into a complete depth. Further, the pose and depth estimations
re-synthesize the optical flow maps, serving to compute reconstruction errors
with the preliminary predictions. Our proposed joint depth and optical flow
estimation outperforms existing depth estimators on the KITTI Depth dataset,
both with and without Cityscapes pretraining. Additionally, our optical flow
results demonstrate competitive performance on the KITTI Flow 2015 dataset.
</p></li>
</ul>

<h3>Title: Prompt-Enhanced Self-supervised Representation Learning for Remote Sensing Image Understanding. (arXiv:2310.00022v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.00022">http://arxiv.org/abs/2310.00022</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.00022]] Prompt-Enhanced Self-supervised Representation Learning for Remote Sensing Image Understanding(http://arxiv.org/abs/2310.00022)</code></li>
<li>Summary: <p>Learning representations through self-supervision on a large-scale, unlabeled
dataset has proven to be highly effective for understanding diverse images,
such as those used in remote sensing image analysis. However, remote sensing
images often have complex and densely populated scenes, with multiple land
objects and no clear foreground objects. This intrinsic property can lead to
false positive pairs in contrastive learning, or missing contextual information
in reconstructive learning, which can limit the effectiveness of existing
self-supervised learning methods. To address these problems, we propose a
prompt-enhanced self-supervised representation learning method that uses a
simple yet efficient pre-training pipeline. Our approach involves utilizing
original image patches as a reconstructive prompt template, and designing a
prompt-enhanced generative branch that provides contextual information through
semantic consistency constraints. We collected a dataset of over 1.28 million
remote sensing images that is comparable to the popular ImageNet dataset, but
without specific temporal or geographical constraints. Our experiments show
that our method outperforms fully supervised learning models and
state-of-the-art self-supervised learning methods on various downstream tasks,
including land cover classification, semantic segmentation, object detection,
and instance segmentation. These results demonstrate that our approach learns
impressive remote sensing representations with high generalization and
transferability.
</p></li>
</ul>

<h3>Title: LSOR: Longitudinally-Consistent Self-Organized Representation Learning. (arXiv:2310.00213v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.00213">http://arxiv.org/abs/2310.00213</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.00213]] LSOR: Longitudinally-Consistent Self-Organized Representation Learning(http://arxiv.org/abs/2310.00213)</code></li>
<li>Summary: <p>Interpretability is a key issue when applying deep learning models to
longitudinal brain MRIs. One way to address this issue is by visualizing the
high-dimensional latent spaces generated by deep learning via self-organizing
maps (SOM). SOM separates the latent space into clusters and then maps the
cluster centers to a discrete (typically 2D) grid preserving the
high-dimensional relationship between clusters. However, learning SOM in a
high-dimensional latent space tends to be unstable, especially in a
self-supervision setting. Furthermore, the learned SOM grid does not
necessarily capture clinically interesting information, such as brain age. To
resolve these issues, we propose the first self-supervised SOM approach that
derives a high-dimensional, interpretable representation stratified by brain
age solely based on longitudinal brain MRIs (i.e., without demographic or
cognitive information). Called Longitudinally-consistent Self-Organized
Representation learning (LSOR), the method is stable during training as it
relies on soft clustering (vs. the hard cluster assignments used by existing
SOM). Furthermore, our approach generates a latent space stratified according
to brain age by aligning trajectories inferred from longitudinal MRIs to the
reference vector associated with the corresponding SOM cluster. When applied to
longitudinal MRIs of the Alzheimer's Disease Neuroimaging Initiative (ADNI,
N=632), LSOR generates an interpretable latent space and achieves comparable or
higher accuracy than the state-of-the-art representations with respect to the
downstream tasks of classification (static vs. progressive mild cognitive
impairment) and regression (determining ADAS-Cog score of all subjects). The
code is available at
https://github.com/ouyangjiahong/longitudinal-som-single-modality.
</p></li>
</ul>

<h2>foundation model</h2>
<h2>generative</h2>
<h3>Title: Feedback-guided Data Synthesis for Imbalanced Classification. (arXiv:2310.00158v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.00158">http://arxiv.org/abs/2310.00158</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.00158]] Feedback-guided Data Synthesis for Imbalanced Classification(http://arxiv.org/abs/2310.00158)</code></li>
<li>Summary: <p>Current status quo in machine learning is to use static datasets of real
images for training, which often come from long-tailed distributions. With the
recent advances in generative models, researchers have started augmenting these
static datasets with synthetic data, reporting moderate performance
improvements on classification tasks. We hypothesize that these performance
gains are limited by the lack of feedback from the classifier to the generative
model, which would promote the usefulness of the generated samples to improve
the classifier's performance. In this work, we introduce a framework for
augmenting static datasets with useful synthetic samples, which leverages
one-shot feedback from the classifier to drive the sampling of the generative
model. In order for the framework to be effective, we find that the samples
must be close to the support of the real data of the task at hand, and be
sufficiently diverse. We validate three feedback criteria on a long-tailed
dataset (ImageNet-LT) as well as a group-imbalanced dataset (NICO++). On
ImageNet-LT, we achieve state-of-the-art results, with over 4 percent
improvement on underrepresented classes while being twice efficient in terms of
the number of generated synthetic samples. NICO++ also enjoys marked boosts of
over 5 percent in worst group accuracy. With these results, our framework paves
the path towards effectively leveraging state-of-the-art text-to-image models
as data sources that can be queried to improve downstream applications.
</p></li>
</ul>

<h3>Title: Latent Space Symmetry Discovery. (arXiv:2310.00105v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.00105">http://arxiv.org/abs/2310.00105</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.00105]] Latent Space Symmetry Discovery(http://arxiv.org/abs/2310.00105)</code></li>
<li>Summary: <p>Equivariant neural networks require explicit knowledge of the symmetry group.
Automatic symmetry discovery methods aim to relax this constraint and learn
invariance and equivariance from data. However, existing symmetry discovery
methods are limited to linear symmetries in their search space and cannot
handle the complexity of symmetries in real-world, often high-dimensional data.
We propose a novel generative model, Latent LieGAN (LaLiGAN), which can
discover nonlinear symmetries from data. It learns a mapping from data to a
latent space where the symmetries become linear and simultaneously discovers
symmetries in the latent space. Theoretically, we show that our method can
express any nonlinear symmetry under certain conditions. Experimentally, our
method can capture the intrinsic symmetry in high-dimensional observations,
which results in a well-structured latent space that is useful for other
downstream tasks. We demonstrate the use cases for LaLiGAN in improving
equation discovery and long-term forecasting for various dynamical systems.
</p></li>
</ul>

<h2>anomaly</h2>
<h3>Title: Probabilistic Sampling-Enhanced Temporal-Spatial GCN: A Scalable Framework for Transaction Anomaly Detection in Ethereum Networks. (arXiv:2310.00144v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.00144">http://arxiv.org/abs/2310.00144</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.00144]] Probabilistic Sampling-Enhanced Temporal-Spatial GCN: A Scalable Framework for Transaction Anomaly Detection in Ethereum Networks(http://arxiv.org/abs/2310.00144)</code></li>
<li>Summary: <p>The rapid evolution of the Ethereum network necessitates sophisticated
techniques to ensure its robustness against potential threats and to maintain
transparency. While Graph Neural Networks (GNNs) have pioneered anomaly
detection in such platforms, capturing the intricacies of both spatial and
temporal transactional patterns has remained a challenge. This study presents a
fusion of Graph Convolutional Networks (GCNs) with Temporal Random Walks (TRW)
enhanced by probabilistic sampling to bridge this gap. Our approach, unlike
traditional GCNs, leverages the strengths of TRW to discern complex temporal
sequences in Ethereum transactions, thereby providing a more nuanced
transaction anomaly detection mechanism. Preliminary evaluations demonstrate
that our TRW-GCN framework substantially advances the performance metrics over
conventional GCNs in detecting anomalies and transaction bursts. This research
not only underscores the potential of temporal cues in Ethereum transactional
data but also offers a scalable and effective methodology for ensuring the
security and transparency of decentralized platforms. By harnessing both
spatial relationships and time-based transactional sequences as node features,
our model introduces an additional layer of granularity, making the detection
process more robust and less prone to false positives. This work lays the
foundation for future research aimed at optimizing and enhancing the
transparency of blockchain technologies, and serves as a testament to the
significance of considering both time and space dimensions in the ever-evolving
landscape of the decentralized platforms.
</p></li>
</ul>

<h2>in-context</h2>
<h3>Title: One for All: Towards Training One Graph Model for All Classification Tasks. (arXiv:2310.00149v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2310.00149">http://arxiv.org/abs/2310.00149</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2310.00149]] One for All: Towards Training One Graph Model for All Classification Tasks(http://arxiv.org/abs/2310.00149)</code></li>
<li>Summary: <p>Designing a single model that addresses multiple tasks has been a
long-standing objective in artificial intelligence. Recently, large language
models have demonstrated exceptional capability in integrating and solving
different tasks within the language domain. However, a unified model for
various tasks on graphs remains underexplored, primarily due to the challenges
unique to the graph learning domain. First, graph data from different areas
carry distinct attributes and follow different distributions. Such discrepancy
makes it hard to represent graphs in a single representation space. Second,
tasks on graphs diversify into node, link, and graph tasks, requiring distinct
embedding strategies. Finally, an appropriate graph prompting paradigm for
in-context learning is unclear. Striving to handle all the aforementioned
challenges, we propose One for All (OFA), the first general framework that can
use a single graph model to address the above challenges. Specifically, OFA
proposes text-attributed graphs to unify different graph data by describing
nodes and edges with natural language and uses language models to encode the
diverse and possibly cross-domain text attributes to feature vectors in the
same embedding space. Furthermore, OFA introduces the concept of
nodes-of-interest to standardize different tasks with a single task
representation. For in-context learning on graphs, OFA introduces a novel graph
prompting paradigm that appends prompting substructures to the input graph,
which enables it to address varied tasks without fine-tuning. We train the OFA
model using graph data from multiple domains (including citation networks,
molecular graphs, knowledge graphs, etc.) simultaneously and evaluate its
ability in supervised, few-shot, and zero-shot learning scenarios. OFA performs
well across different tasks, making it the first general-purpose graph
classification model across domains.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
