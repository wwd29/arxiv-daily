<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>diffusion</h2>
<h3>Title: NeuSD: Surface Completion with Multi-View Text-to-Image Diffusion. (arXiv:2312.04654v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04654">http://arxiv.org/abs/2312.04654</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04654]] NeuSD: Surface Completion with Multi-View Text-to-Image Diffusion(http://arxiv.org/abs/2312.04654)</code></li>
<li>Summary: <p>We present a novel method for 3D surface reconstruction from multiple images
where only a part of the object of interest is captured. Our approach builds on
two recent developments: surface reconstruction using neural radiance fields
for the reconstruction of the visible parts of the surface, and guidance of
pre-trained 2D diffusion models in the form of Score Distillation Sampling
(SDS) to complete the shape in unobserved regions in a plausible manner. We
introduce three components. First, we suggest employing normal maps as a pure
geometric representation for SDS instead of color renderings which are
entangled with the appearance information. Second, we introduce the freezing of
the SDS noise during training which results in more coherent gradients and
better convergence. Third, we propose Multi-View SDS as a way to condition the
generation of the non-observable part of the surface without fine-tuning or
making changes to the underlying 2D Stable Diffusion model. We evaluate our
approach on the BlendedMVS dataset demonstrating significant qualitative and
quantitative improvements over competing methods.
</p></li>
</ul>

<h3>Title: ECLIPSE: A Resource-Efficient Text-to-Image Prior for Image Generations. (arXiv:2312.04655v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04655">http://arxiv.org/abs/2312.04655</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04655]] ECLIPSE: A Resource-Efficient Text-to-Image Prior for Image Generations(http://arxiv.org/abs/2312.04655)</code></li>
<li>Summary: <p>Text-to-image (T2I) diffusion models, notably the unCLIP models (e.g.,
DALL-E-2), achieve state-of-the-art (SOTA) performance on various compositional
T2I benchmarks, at the cost of significant computational resources. The unCLIP
stack comprises T2I prior and diffusion image decoder. The T2I prior model
alone adds a billion parameters compared to the Latent Diffusion Models, which
increases the computational and high-quality data requirements. We introduce
ECLIPSE, a novel contrastive learning method that is both parameter and
data-efficient. ECLIPSE leverages pre-trained vision-language models (e.g.,
CLIP) to distill the knowledge into the prior model. We demonstrate that the
ECLIPSE trained prior, with only 3.3% of the parameters and trained on a mere
2.8% of the data, surpasses the baseline T2I priors with an average of 71.6%
preference score under resource-limited setting. It also attains performance on
par with SOTA big models, achieving an average of 63.36% preference score in
terms of the ability to follow the text compositions. Extensive experiments on
two unCLIP diffusion image decoders, Karlo and Kandinsky, affirm that ECLIPSE
priors consistently deliver high performance while significantly reducing
resource dependency.
</p></li>
</ul>

<h3>Title: Diffence: Fencing Membership Privacy With Diffusion Models. (arXiv:2312.04692v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04692">http://arxiv.org/abs/2312.04692</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04692]] Diffence: Fencing Membership Privacy With Diffusion Models(http://arxiv.org/abs/2312.04692)</code></li>
<li>Summary: <p>Deep learning models, while achieving remarkable performance across various
tasks, are vulnerable to member inference attacks, wherein adversaries identify
if a specific data point was part of a model's training set. This
susceptibility raises substantial privacy concerns, especially when models are
trained on sensitive datasets. Current defense methods often struggle to
provide robust protection without hurting model utility, and they often require
retraining the model or using extra data. In this work, we introduce a novel
defense framework against membership attacks by leveraging generative models.
The key intuition of our defense is to remove the differences between member
and non-member inputs which can be used to perform membership attacks, by
re-generating input samples before feeding them to the target model. Therefore,
our defense works \emph{pre-inference}, which is unlike prior defenses that are
either training-time (modify the model) or post-inference time (modify the
model's output).
</p>
<p>A unique feature of our defense is that it works on input samples only,
without modifying the training or inference phase of the target model.
Therefore, it can be cascaded with other defense mechanisms as we demonstrate
through experiments. Through extensive experimentation, we show that our
approach can serve as a robust plug-n-play defense mechanism, enhancing
membership privacy without compromising model utility in both baseline and
defended settings. For example, our method enhanced the effectiveness of recent
state-of-the-art defenses, reducing attack accuracy by an average of 5.7\% to
12.4\% across three datasets, without any impact on the model's accuracy. By
integrating our method with prior defenses, we achieve new state-of-the-art
performance in the privacy-utility trade-off.
</p></li>
</ul>

<h3>Title: Fine-Tuning InstructPix2Pix for Advanced Image Colorization. (arXiv:2312.04780v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04780">http://arxiv.org/abs/2312.04780</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04780]] Fine-Tuning InstructPix2Pix for Advanced Image Colorization(http://arxiv.org/abs/2312.04780)</code></li>
<li>Summary: <p>This paper presents a novel approach to human image colorization by
fine-tuning the InstructPix2Pix model, which integrates a language model
(GPT-3) with a text-to-image model (Stable Diffusion). Despite the original
InstructPix2Pix model's proficiency in editing images based on textual
instructions, it exhibits limitations in the focused domain of colorization. To
address this, we fine-tuned the model using the IMDB-WIKI dataset, pairing
black-and-white images with a diverse set of colorization prompts generated by
ChatGPT. This paper contributes by (1) applying fine-tuning techniques to
stable diffusion models specifically for colorization tasks, and (2) employing
generative models to create varied conditioning prompts. After finetuning, our
model outperforms the original InstructPix2Pix model on multiple metrics
quantitatively, and we produce more realistically colored images qualitatively.
The code for this project is provided on the GitHub Repository
https://github.com/AllenAnZifeng/DeepLearning282.
</p></li>
</ul>

<h3>Title: Reality's Canvas, Language's Brush: Crafting 3D Avatars from Monocular Video. (arXiv:2312.04784v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04784">http://arxiv.org/abs/2312.04784</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04784]] Reality's Canvas, Language's Brush: Crafting 3D Avatars from Monocular Video(http://arxiv.org/abs/2312.04784)</code></li>
<li>Summary: <p>Recent advancements in 3D avatar generation excel with multi-view supervision
for photorealistic models. However, monocular counterparts lag in quality
despite broader applicability. We propose ReCaLab to close this gap. ReCaLab is
a fully-differentiable pipeline that learns high-fidelity 3D human avatars from
just a single RGB video. A pose-conditioned deformable NeRF is optimized to
volumetrically represent a human subject in canonical T-pose. The canonical
representation is then leveraged to efficiently associate viewpoint-agnostic
textures using 2D-3D correspondences. This enables to separately generate
albedo and shading which jointly compose an RGB prediction. The design allows
to control intermediate results for human pose, body shape, texture, and
lighting with text prompts. An image-conditioned diffusion model thereby helps
to animate appearance and pose of the 3D avatar to create video sequences with
previously unseen human motion. Extensive experiments show that ReCaLab
outperforms previous monocular approaches in terms of image quality for image
synthesis tasks. ReCaLab even outperforms multi-view methods that leverage up
to 19x more synchronized videos for the task of novel pose rendering. Moreover,
natural language offers an intuitive user interface for creative manipulation
of 3D human avatars.
</p></li>
</ul>

<h3>Title: MimicDiffusion: Purifying Adversarial Perturbation via Mimicking Clean Diffusion Model. (arXiv:2312.04802v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04802">http://arxiv.org/abs/2312.04802</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04802]] MimicDiffusion: Purifying Adversarial Perturbation via Mimicking Clean Diffusion Model(http://arxiv.org/abs/2312.04802)</code></li>
<li>Summary: <p>Deep neural networks (DNNs) are vulnerable to adversarial perturbation, where
an imperceptible perturbation is added to the image that can fool the DNNs.
Diffusion-based adversarial purification focuses on using the diffusion model
to generate a clean image against such adversarial attacks. Unfortunately, the
generative process of the diffusion model is also inevitably affected by
adversarial perturbation since the diffusion model is also a deep network where
its input has adversarial perturbation. In this work, we propose
MimicDiffusion, a new diffusion-based adversarial purification technique, that
directly approximates the generative process of the diffusion model with the
clean image as input. Concretely, we analyze the differences between the guided
terms using the clean image and the adversarial sample. After that, we first
implement MimicDiffusion based on Manhattan distance. Then, we propose two
guidance to purify the adversarial perturbation and approximate the clean
diffusion model. Extensive experiments on three image datasets including
CIFAR-10, CIFAR-100, and ImageNet with three classifier backbones including
WideResNet-70-16, WideResNet-28-10, and ResNet50 demonstrate that
MimicDiffusion significantly performs better than the state-of-the-art
baselines. On CIFAR-10, CIFAR-100, and ImageNet, it achieves 92.67\%, 61.35\%,
and 61.53\% average robust accuracy, which are 18.49\%, 13.23\%, and 17.64\%
higher, respectively. The code is available in the supplementary material.
</p></li>
</ul>

<h3>Title: RL Dreams: Policy Gradient Optimization for Score Distillation based 3D Generation. (arXiv:2312.04806v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04806">http://arxiv.org/abs/2312.04806</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04806]] RL Dreams: Policy Gradient Optimization for Score Distillation based 3D Generation(http://arxiv.org/abs/2312.04806)</code></li>
<li>Summary: <p>3D generation has rapidly accelerated in the past decade owing to the
progress in the field of generative modeling. Score Distillation Sampling (SDS)
based rendering has improved 3D asset generation to a great extent. Further,
the recent work of Denoising Diffusion Policy Optimization (DDPO) demonstrates
that the diffusion process is compatible with policy gradient methods and has
been demonstrated to improve the 2D diffusion models using an aesthetic scoring
function. We first show that this aesthetic scorer acts as a strong guide for a
variety of SDS-based methods and demonstrates its effectiveness in text-to-3D
synthesis. Further, we leverage the DDPO approach to improve the quality of the
3D rendering obtained from 2D diffusion models. Our approach, DDPO3D, employs
the policy gradient method in tandem with aesthetic scoring. To the best of our
knowledge, this is the first method that extends policy gradient methods to 3D
score-based rendering and shows improvement across SDS-based methods such as
DreamGaussian, which are currently driving research in text-to-3D synthesis.
Our approach is compatible with score distillation-based methods, which would
facilitate the integration of diverse reward functions into the generative
process. Our project page can be accessed via https://ddpo3d.github.io.
</p></li>
</ul>

<h3>Title: RS-Corrector: Correcting the Racial Stereotypes in Latent Diffusion Models. (arXiv:2312.04810v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04810">http://arxiv.org/abs/2312.04810</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04810]] RS-Corrector: Correcting the Racial Stereotypes in Latent Diffusion Models(http://arxiv.org/abs/2312.04810)</code></li>
<li>Summary: <p>Recent text-conditioned image generation models have demonstrated an
exceptional capacity to produce diverse and creative imagery with high visual
quality. However, when pre-trained on billion-sized datasets randomly collected
from the Internet, where potential biased human preferences exist, these models
tend to produce images with common and recurring stereotypes, particularly for
certain racial groups. In this paper, we conduct an initial analysis of the
publicly available Stable Diffusion model and its derivatives, highlighting the
presence of racial stereotypes. These models often generate distorted or biased
images for certain racial groups, emphasizing stereotypical characteristics. To
address these issues, we propose a framework called "RS-Corrector", designed to
establish an anti-stereotypical preference in the latent space and update the
latent code for refined generated results. The correction process occurs during
the inference stage without requiring fine-tuning of the original model.
Extensive empirical evaluations demonstrate that the introduced \themodel
effectively corrects the racial stereotypes of the well-trained Stable
Diffusion model while leaving the original model unchanged.
</p></li>
</ul>

<h3>Title: Learn to Optimize Denoising Scores for 3D Generation: A Unified and Improved Diffusion Prior on NeRF and 3D Gaussian Splatting. (arXiv:2312.04820v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04820">http://arxiv.org/abs/2312.04820</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04820]] Learn to Optimize Denoising Scores for 3D Generation: A Unified and Improved Diffusion Prior on NeRF and 3D Gaussian Splatting(http://arxiv.org/abs/2312.04820)</code></li>
<li>Summary: <p>We propose a unified framework aimed at enhancing the diffusion priors for 3D
generation tasks. Despite the critical importance of these tasks, existing
methodologies often struggle to generate high-caliber results. We begin by
examining the inherent limitations in previous diffusion priors. We identify a
divergence between the diffusion priors and the training procedures of
diffusion models that substantially impairs the quality of 3D generation. To
address this issue, we propose a novel, unified framework that iteratively
optimizes both the 3D model and the diffusion prior. Leveraging the different
learnable parameters of the diffusion prior, our approach offers multiple
configurations, affording various trade-offs between performance and
implementation complexity. Notably, our experimental results demonstrate that
our method markedly surpasses existing techniques, establishing new
state-of-the-art in the realm of text-to-3D generation. Furthermore, our
approach exhibits impressive performance on both NeRF and the newly introduced
3D Gaussian Splatting backbones. Additionally, our framework yields insightful
contributions to the understanding of recent score distillation methods, such
as the VSD and DDS loss.
</p></li>
</ul>

<h3>Title: Assessing Neural Network Representations During Training Using Noise-Resilient Diffusion Spectral Entropy. (arXiv:2312.04823v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04823">http://arxiv.org/abs/2312.04823</a></li>
<li>Code URL: https://github.com/ChenLiu-1996/DiffusionSpectralEntropy</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04823]] Assessing Neural Network Representations During Training Using Noise-Resilient Diffusion Spectral Entropy(http://arxiv.org/abs/2312.04823)</code></li>
<li>Summary: <p>Entropy and mutual information in neural networks provide rich information on
the learning process, but they have proven difficult to compute reliably in
high dimensions. Indeed, in noisy and high-dimensional data, traditional
estimates in ambient dimensions approach a fixed entropy and are prohibitively
hard to compute. To address these issues, we leverage data geometry to access
the underlying manifold and reliably compute these information-theoretic
measures. Specifically, we define diffusion spectral entropy (DSE) in neural
representations of a dataset as well as diffusion spectral mutual information
(DSMI) between different variables representing data. First, we show that they
form noise-resistant measures of intrinsic dimensionality and relationship
strength in high-dimensional simulated data that outperform classic Shannon
entropy, nonparametric estimation, and mutual information neural estimation
(MINE). We then study the evolution of representations in classification
networks with supervised learning, self-supervision, or overfitting. We observe
that (1) DSE of neural representations increases during training; (2) DSMI with
the class label increases during generalizable learning but stays stagnant
during overfitting; (3) DSMI with the input signal shows differing trends: on
MNIST it increases, while on CIFAR-10 and STL-10 it decreases. Finally, we show
that DSE can be used to guide better network initialization and that DSMI can
be used to predict downstream classification accuracy across 962 models on
ImageNet. The official implementation is available at
https://github.com/ChenLiu-1996/DiffusionSpectralEntropy.
</p></li>
</ul>

<h3>Title: Towards Stable and Faithful Inpainting. (arXiv:2312.04831v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04831">http://arxiv.org/abs/2312.04831</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04831]] Towards Stable and Faithful Inpainting(http://arxiv.org/abs/2312.04831)</code></li>
<li>Summary: <p>Recent progress in inpainting increasingly relies on generative models,
leveraging their strong generation capabilities for addressing ill-conditioned
problems. However, this enhanced generation often introduces instability,
leading to arbitrary object generation within masked regions. This paper
proposes a balanced solution, emphasizing the importance of unmasked regions in
guiding inpainting while preserving generative capacity. Our approach, Aligned
Stable Inpainting with UnKnown Areas Prior (ASUKA), employs a
reconstruction-based masked auto-encoder (MAE) as a stable prior. Aligned with
the robust Stable Diffusion inpainting model (SD), ASUKA significantly improves
inpainting stability. ASUKA further aligns masked and unmasked regions through
an inpainting-specialized decoder, ensuring more faithful inpainting. To
validate effectiveness across domains and masking scenarios, we evaluate on
MISATO, a collection of several existing dataset. Results confirm ASUKA's
efficacy in both stability and fidelity compared to SD and other inpainting
algorithms.
</p></li>
</ul>

<h3>Title: HandDiffuse: Generative Controllers for Two-Hand Interactions via Diffusion Models. (arXiv:2312.04867v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04867">http://arxiv.org/abs/2312.04867</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04867]] HandDiffuse: Generative Controllers for Two-Hand Interactions via Diffusion Models(http://arxiv.org/abs/2312.04867)</code></li>
<li>Summary: <p>Existing hands datasets are largely short-range and the interaction is weak
due to the self-occlusion and self-similarity of hands, which can not yet fit
the need for interacting hands motion generation. To rescue the data scarcity,
we propose HandDiffuse12.5M, a novel dataset that consists of temporal
sequences with strong two-hand interactions. HandDiffuse12.5M has the largest
scale and richest interactions among the existing two-hand datasets. We further
present a strong baseline method HandDiffuse for the controllable motion
generation of interacting hands using various controllers. Specifically, we
apply the diffusion model as the backbone and design two motion representations
for different controllers. To reduce artifacts, we also propose Interaction
Loss which explicitly quantifies the dynamic interaction process. Our
HandDiffuse enables various applications with vivid two-hand interactions,
i.e., motion in-betweening and trajectory control. Experiments show that our
method outperforms the state-of-the-art techniques in motion generation and can
also contribute to data augmentation for other datasets. Our dataset,
corresponding codes, and pre-trained models will be disseminated to the
community for future research towards two-hand interaction modeling.
</p></li>
</ul>

<h3>Title: MVDD: Multi-View Depth Diffusion Models. (arXiv:2312.04875v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04875">http://arxiv.org/abs/2312.04875</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04875]] MVDD: Multi-View Depth Diffusion Models(http://arxiv.org/abs/2312.04875)</code></li>
<li>Summary: <p>Denoising diffusion models have demonstrated outstanding results in 2D image
generation, yet it remains a challenge to replicate its success in 3D shape
generation. In this paper, we propose leveraging multi-view depth, which
represents complex 3D shapes in a 2D data format that is easy to denoise. We
pair this representation with a diffusion model, MVDD, that is capable of
generating high-quality dense point clouds with 20K+ points with fine-grained
details. To enforce 3D consistency in multi-view depth, we introduce an
epipolar line segment attention that conditions the denoising step for a view
on its neighboring views. Additionally, a depth fusion module is incorporated
into diffusion steps to further ensure the alignment of depth maps. When
augmented with surface reconstruction, MVDD can also produce high-quality 3D
meshes. Furthermore, MVDD stands out in other tasks such as depth completion,
and can serve as a 3D prior, significantly boosting many downstream tasks, such
as GAN inversion. State-of-the-art results from extensive experiments
demonstrate MVDD's excellent ability in 3D shape generation, depth completion,
and its potential as a 3D prior for downstream tasks.
</p></li>
</ul>

<h3>Title: UDiffText: A Unified Framework for High-quality Text Synthesis in Arbitrary Images via Character-aware Diffusion Models. (arXiv:2312.04884v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04884">http://arxiv.org/abs/2312.04884</a></li>
<li>Code URL: https://github.com/zym-pku/udifftext</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04884]] UDiffText: A Unified Framework for High-quality Text Synthesis in Arbitrary Images via Character-aware Diffusion Models(http://arxiv.org/abs/2312.04884)</code></li>
<li>Summary: <p>Text-to-Image (T2I) generation methods based on diffusion model have garnered
significant attention in the last few years. Although these image synthesis
methods produce visually appealing results, they frequently exhibit spelling
errors when rendering text within the generated images. Such errors manifest as
missing, incorrect or extraneous characters, thereby severely constraining the
performance of text image generation based on diffusion models. To address the
aforementioned issue, this paper proposes a novel approach for text image
generation, utilizing a pre-trained diffusion model (i.e., Stable Diffusion
[27]). Our approach involves the design and training of a light-weight
character-level text encoder, which replaces the original CLIP encoder and
provides more robust text embeddings as conditional guidance. Then, we
fine-tune the diffusion model using a large-scale dataset, incorporating local
attention control under the supervision of character-level segmentation maps.
Finally, by employing an inference stage refinement process, we achieve a
notably high sequence accuracy when synthesizing text in arbitrarily given
images. Both qualitative and quantitative results demonstrate the superiority
of our method to the state of the art. Furthermore, we showcase several
potential applications of the proposed UDiffText, including text-centric image
synthesis, scene text editing, etc. Code and model will be available at
https://github.com/ZYM-PKU/UDiffText .
</p></li>
</ul>

<h3>Title: Text-to-3D Generation with Bidirectional Diffusion using both 2D and 3D priors. (arXiv:2312.04963v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04963">http://arxiv.org/abs/2312.04963</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04963]] Text-to-3D Generation with Bidirectional Diffusion using both 2D and 3D priors(http://arxiv.org/abs/2312.04963)</code></li>
<li>Summary: <p>Most 3D generation research focuses on up-projecting 2D foundation models
into the 3D space, either by minimizing 2D Score Distillation Sampling (SDS)
loss or fine-tuning on multi-view datasets. Without explicit 3D priors, these
methods often lead to geometric anomalies and multi-view inconsistency.
Recently, researchers have attempted to improve the genuineness of 3D objects
by directly training on 3D datasets, albeit at the cost of low-quality texture
generation due to the limited texture diversity in 3D datasets. To harness the
advantages of both approaches, we propose Bidirectional Diffusion(BiDiff), a
unified framework that incorporates both a 3D and a 2D diffusion process, to
preserve both 3D fidelity and 2D texture richness, respectively. Moreover, as a
simple combination may yield inconsistent generation results, we further bridge
them with novel bidirectional guidance. In addition, our method can be used as
an initialization of optimization-based models to further improve the quality
of 3D model and efficiency of optimization, reducing the generation process
from 3.4 hours to 20 minutes. Experimental results have shown that our model
achieves high-quality, diverse, and scalable 3D generation. Project website:
https://bidiff.github.io/.
</p></li>
</ul>

<h3>Title: Inversion-Free Image Editing with Natural Language. (arXiv:2312.04965v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04965">http://arxiv.org/abs/2312.04965</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04965]] Inversion-Free Image Editing with Natural Language(http://arxiv.org/abs/2312.04965)</code></li>
<li>Summary: <p>Despite recent advances in inversion-based editing, text-guided image
manipulation remains challenging for diffusion models. The primary bottlenecks
include 1) the time-consuming nature of the inversion process; 2) the struggle
to balance consistency with accuracy; 3) the lack of compatibility with
efficient consistency sampling methods used in consistency models. To address
the above issues, we start by asking ourselves if the inversion process can be
eliminated for editing. We show that when the initial sample is known, a
special variance schedule reduces the denoising step to the same form as the
multi-step consistency sampling. We name this Denoising Diffusion Consistent
Model (DDCM), and note that it implies a virtual inversion strategy without
explicit inversion in sampling. We further unify the attention control
mechanisms in a tuning-free framework for text-guided editing. Combining them,
we present inversion-free editing (InfEdit), which allows for consistent and
faithful editing for both rigid and non-rigid semantic changes, catering to
intricate modifications without compromising on the image's integrity and
explicit inversion. Through extensive experiments, InfEdit shows strong
performance in various editing tasks and also maintains a seamless workflow
(less than 3 seconds on one single A40), demonstrating the potential for
real-time applications. Project Page: https://sled-group.github.io/InfEdit/
</p></li>
</ul>

<h3>Title: Customizing Motion in Text-to-Video Diffusion Models. (arXiv:2312.04966v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04966">http://arxiv.org/abs/2312.04966</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04966]] Customizing Motion in Text-to-Video Diffusion Models(http://arxiv.org/abs/2312.04966)</code></li>
<li>Summary: <p>We introduce an approach for augmenting text-to-video generation models with
customized motions, extending their capabilities beyond the motions depicted in
the original training data. By leveraging a few video samples demonstrating
specific movements as input, our method learns and generalizes the input motion
patterns for diverse, text-specified scenarios. Our contributions are
threefold. First, to achieve our results, we finetune an existing text-to-video
model to learn a novel mapping between the depicted motion in the input
examples to a new unique token. To avoid overfitting to the new custom motion,
we introduce an approach for regularization over videos. Second, by leveraging
the motion priors in a pretrained model, our method can produce novel videos
featuring multiple people doing the custom motion, and can invoke the motion in
combination with other motions. Furthermore, our approach extends to the
multimodal customization of motion and appearance of individualized subjects,
enabling the generation of videos featuring unique characters and distinct
motions. Third, to validate our method, we introduce an approach for
quantitatively evaluating the learned custom motion and perform a systematic
ablation study. We show that our method significantly outperforms prior
appearance-based customization approaches when extended to the motion
customization task.
</p></li>
</ul>

<h3>Title: SmartMask: Context Aware High-Fidelity Mask Generation for Fine-grained Object Insertion and Layout Control. (arXiv:2312.05039v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05039">http://arxiv.org/abs/2312.05039</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05039]] SmartMask: Context Aware High-Fidelity Mask Generation for Fine-grained Object Insertion and Layout Control(http://arxiv.org/abs/2312.05039)</code></li>
<li>Summary: <p>The field of generative image inpainting and object insertion has made
significant progress with the recent advent of latent diffusion models.
Utilizing a precise object mask can greatly enhance these applications.
However, due to the challenges users encounter in creating high-fidelity masks,
there is a tendency for these methods to rely on more coarse masks (e.g.,
bounding box) for these applications. This results in limited control and
compromised background content preservation. To overcome these limitations, we
introduce SmartMask, which allows any novice user to create detailed masks for
precise object insertion. Combined with a ControlNet-Inpaint model, our
experiments demonstrate that SmartMask achieves superior object insertion
quality, preserving the background content more effectively than previous
methods. Notably, unlike prior works the proposed approach can also be used
even without user-mask guidance, which allows it to perform mask-free object
insertion at diverse positions and scales. Furthermore, we find that when used
iteratively with a novel instruction-tuning based planning model, SmartMask can
be used to design detailed layouts from scratch. As compared with user-scribble
based layout design, we observe that SmartMask allows for better quality
outputs with layout-to-image generation methods. Project page is available at
https://smartmask-gen.github.io
</p></li>
</ul>

<h3>Title: DreaMoving: A Human Dance Video Generation Framework based on Diffusion Models. (arXiv:2312.05107v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05107">http://arxiv.org/abs/2312.05107</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05107]] DreaMoving: A Human Dance Video Generation Framework based on Diffusion Models(http://arxiv.org/abs/2312.05107)</code></li>
<li>Summary: <p>In this paper, we present DreaMoving, a diffusion-based controllable video
generation framework to produce high-quality customized human dance videos.
Specifically, given target identity and posture sequences, DreaMoving can
generate a video of the target identity dancing anywhere driven by the posture
sequences. To this end, we propose a Video ControlNet for motion-controlling
and a Content Guider for identity preserving. The proposed model is easy to use
and can be adapted to most stylized diffusion models to generate diverse
results. The project page is available at
https://dreamoving.github.io/dreamoving.
</p></li>
</ul>

<h3>Title: SwiftBrush: One-Step Text-to-Image Diffusion Model with Variational Score Distillation. (arXiv:2312.05239v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05239">http://arxiv.org/abs/2312.05239</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05239]] SwiftBrush: One-Step Text-to-Image Diffusion Model with Variational Score Distillation(http://arxiv.org/abs/2312.05239)</code></li>
<li>Summary: <p>Despite their ability to generate high-resolution and diverse images from
text prompts, text-to-image diffusion models often suffer from slow iterative
sampling processes. Model distillation is one of the most effective directions
to accelerate these models. However, previous distillation methods fail to
retain the generation quality while requiring a significant amount of images
for training, either from real data or synthetically generated by the teacher
model. In response to this limitation, we present a novel image-free
distillation scheme named $\textbf{SwiftBrush}$. Drawing inspiration from
text-to-3D synthesis, in which a 3D neural radiance field that aligns with the
input prompt can be obtained from a 2D text-to-image diffusion prior via a
specialized loss without the use of any 3D data ground-truth, our approach
re-purposes that same loss for distilling a pretrained multi-step text-to-image
model to a student network that can generate high-fidelity images with just a
single inference step. In spite of its simplicity, our model stands as one of
the first one-step text-to-image generators that can produce images of
comparable quality to Stable Diffusion without reliance on any training image
data. Remarkably, SwiftBrush achieves an FID score of $\textbf{16.67}$ and a
CLIP score of $\textbf{0.29}$ on the COCO-30K benchmark, achieving competitive
results or even substantially surpassing existing state-of-the-art distillation
techniques.
</p></li>
</ul>

<h3>Title: TrustFed: A Reliable Federated Learning Framework with Malicious-Attack Resistance. (arXiv:2312.04597v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04597">http://arxiv.org/abs/2312.04597</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04597]] TrustFed: A Reliable Federated Learning Framework with Malicious-Attack Resistance(http://arxiv.org/abs/2312.04597)</code></li>
<li>Summary: <p>As a key technology in 6G research, federated learning (FL) enables
collaborative learning among multiple clients while ensuring individual data
privacy. However, malicious attackers among the participating clients can
intentionally tamper with the training data or the trained model, compromising
the accuracy and trustworthiness of the system. To address this issue, in this
paper, we propose a hierarchical audit-based FL (HiAudit-FL) framework, with
the aim to enhance the reliability and security of the learning process. The
hierarchical audit process includes two stages, namely model-audit and
parameter-audit. In the model-audit stage, a low-overhead audit method is
employed to identify suspicious clients. Subsequently, in the parameter-audit
stage, a resource-consuming method is used to detect all malicious clients with
higher accuracy among the suspicious ones. Specifically, we execute the model
audit method among partial clients for multiple rounds, which is modeled as a
partial observation Markov decision process (POMDP) with the aim to enhance the
robustness and accountability of the decision-making in complex and uncertain
environments. Meanwhile, we formulate the problem of identifying malicious
attackers through a multi-round audit as an active sequential hypothesis
testing problem and leverage a diffusion model-based AI-Enabled audit selection
strategy (ASS) to decide which clients should be audited in each round. To
accomplish efficient and effective audit selection, we design a DRL-ASS
algorithm by incorporating the ASS in a deep reinforcement learning (DRL)
framework. Our simulation results demonstrate that HiAudit-FL can effectively
identify and handle potential malicious users accurately, with small system
overhead.
</p></li>
</ul>

<h3>Title: Membership Inference Attacks on Diffusion Models via Quantile Regression. (arXiv:2312.05140v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05140">http://arxiv.org/abs/2312.05140</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05140]] Membership Inference Attacks on Diffusion Models via Quantile Regression(http://arxiv.org/abs/2312.05140)</code></li>
<li>Summary: <p>Recently, diffusion models have become popular tools for image synthesis
because of their high-quality outputs. However, like other large-scale models,
they may leak private information about their training data. Here, we
demonstrate a privacy vulnerability of diffusion models through a
\emph{membership inference (MI) attack}, which aims to identify whether a
target example belongs to the training set when given the trained diffusion
model. Our proposed MI attack learns quantile regression models that predict (a
quantile of) the distribution of reconstruction loss on examples not used in
training. This allows us to define a granular hypothesis test for determining
the membership of a point in the training set, based on thresholding the
reconstruction loss of that point using a custom threshold tailored to the
example. We also provide a simple bootstrap technique that takes a majority
membership prediction over ``a bag of weak attackers'' which improves the
accuracy over individual quantile regression models. We show that our attack
outperforms the prior state-of-the-art attack while being substantially less
computationally expensive -- prior attacks required training multiple ``shadow
models'' with the same architecture as the model under attack, whereas our
attack requires training only much smaller models.
</p></li>
</ul>

<h3>Title: StructComp: Substituting propagation with Structural Compression in Training Graph Contrastive Learning. (arXiv:2312.04865v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04865">http://arxiv.org/abs/2312.04865</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04865]] StructComp: Substituting propagation with Structural Compression in Training Graph Contrastive Learning(http://arxiv.org/abs/2312.04865)</code></li>
<li>Summary: <p>Graph contrastive learning (GCL) has become a powerful tool for learning
graph data, but its scalability remains a significant challenge. In this work,
we propose a simple yet effective training framework called Structural
Compression (StructComp) to address this issue. Inspired by a sparse low-rank
approximation on the diffusion matrix, StructComp trains the encoder with the
compressed nodes. This allows the encoder not to perform any message passing
during the training stage, and significantly reduces the number of sample pairs
in the contrastive loss. We theoretically prove that the original GCL loss can
be approximated with the contrastive loss computed by StructComp. Moreover,
StructComp can be regarded as an additional regularization term for GCL models,
resulting in a more robust encoder. Empirical studies on seven benchmark
datasets show that StructComp greatly reduces the time and memory consumption
while improving model performance compared to the vanilla GCL models and
scalable training methods.
</p></li>
</ul>

<h3>Title: KBFormer: A Diffusion Model for Structured Entity Completion. (arXiv:2312.05253v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05253">http://arxiv.org/abs/2312.05253</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05253]] KBFormer: A Diffusion Model for Structured Entity Completion(http://arxiv.org/abs/2312.05253)</code></li>
<li>Summary: <p>We develop a generative attention-based approach to modeling structured
entities comprising different property types, such as numerical, categorical,
string, and composite. This approach handles such heterogeneous data through a
mixed continuous-discrete diffusion process over the properties. Our flexible
framework can model entities with arbitrary hierarchical properties, enabling
applications to structured Knowledge Base (KB) entities and tabular data. Our
approach obtains state-of-the-art performance on a majority of cases across 15
datasets. In addition, experiments with a device KB and a nuclear physics
dataset demonstrate the model's ability to learn representations useful for
entity completion in diverse settings. This has many downstream use cases,
including modeling numerical properties with high accuracy - critical for
science applications, which also benefit from the model's inherent
probabilistic nature.
</p></li>
</ul>

<h2>self-supervised</h2>
<h3>Title: VOODOO 3D: Volumetric Portrait Disentanglement for One-Shot 3D Head Reenactment. (arXiv:2312.04651v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04651">http://arxiv.org/abs/2312.04651</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04651]] VOODOO 3D: Volumetric Portrait Disentanglement for One-Shot 3D Head Reenactment(http://arxiv.org/abs/2312.04651)</code></li>
<li>Summary: <p>We present a 3D-aware one-shot head reenactment method based on a fully
volumetric neural disentanglement framework for source appearance and driver
expressions. Our method is real-time and produces high-fidelity and
view-consistent output, suitable for 3D teleconferencing systems based on
holographic displays. Existing cutting-edge 3D-aware reenactment methods often
use neural radiance fields or 3D meshes to produce view-consistent appearance
encoding, but, at the same time, they rely on linear face models, such as 3DMM,
to achieve its disentanglement with facial expressions. As a result, their
reenactment results often exhibit identity leakage from the driver or have
unnatural expressions. To address these problems, we propose a neural
self-supervised disentanglement approach that lifts both the source image and
driver video frame into a shared 3D volumetric representation based on
tri-planes. This representation can then be freely manipulated with expression
tri-planes extracted from the driving images and rendered from an arbitrary
view using neural radiance fields. We achieve this disentanglement via
self-supervised learning on a large in-the-wild video dataset. We further
introduce a highly effective fine-tuning approach to improve the
generalizability of the 3D lifting using the same real-world data. We
demonstrate state-of-the-art performance on a wide range of datasets, and also
showcase high-quality 3D-aware head reenactment on highly challenging and
diverse subjects, including non-frontal head poses and complex expressions for
both source and driver.
</p></li>
</ul>

<h3>Title: Cross-BERT for Point Cloud Pretraining. (arXiv:2312.04891v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04891">http://arxiv.org/abs/2312.04891</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04891]] Cross-BERT for Point Cloud Pretraining(http://arxiv.org/abs/2312.04891)</code></li>
<li>Summary: <p>Introducing BERT into cross-modal settings raises difficulties in its
optimization for handling multiple modalities. Both the BERT architecture and
training objective need to be adapted to incorporate and model information from
different modalities. In this paper, we address these challenges by exploring
the implicit semantic and geometric correlations between 2D and 3D data of the
same objects/scenes. We propose a new cross-modal BERT-style self-supervised
learning paradigm, called Cross-BERT. To facilitate pretraining for irregular
and sparse point clouds, we design two self-supervised tasks to boost
cross-modal interaction. The first task, referred to as Point-Image Alignment,
aims to align features between unimodal and cross-modal representations to
capture the correspondences between the 2D and 3D modalities. The second task,
termed Masked Cross-modal Modeling, further improves mask modeling of BERT by
incorporating high-dimensional semantic information obtained by cross-modal
interaction. By performing cross-modal interaction, Cross-BERT can smoothly
reconstruct the masked tokens during pretraining, leading to notable
performance enhancements for downstream tasks. Through empirical evaluation, we
demonstrate that Cross-BERT outperforms existing state-of-the-art methods in 3D
downstream applications. Our work highlights the effectiveness of leveraging
cross-modal 2D knowledge to strengthen 3D point cloud representation and the
transferable capability of BERT across modalities.
</p></li>
</ul>

<h3>Title: Self-Supervised Behavior Cloned Transformers are Path Crawlers for Text Games. (arXiv:2312.04657v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04657">http://arxiv.org/abs/2312.04657</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04657]] Self-Supervised Behavior Cloned Transformers are Path Crawlers for Text Games(http://arxiv.org/abs/2312.04657)</code></li>
<li>Summary: <p>In this work, we introduce a self-supervised behavior cloning transformer for
text games, which are challenging benchmarks for multi-step reasoning in
virtual environments. Traditionally, Behavior Cloning Transformers excel in
such tasks but rely on supervised training data. Our approach auto-generates
training data by exploring trajectories (defined by common macro-action
sequences) that lead to reward within the games, while determining the
generality and utility of these trajectories by rapidly training small models
then evaluating their performance on unseen development games. Through
empirical analysis, we show our method consistently uncovers generalizable
training data, achieving about 90\% performance of supervised systems across
three benchmark text games.
</p></li>
</ul>

<h3>Title: From Lengthy to Lucid: A Systematic Literature Review on NLP Techniques for Taming Long Sentences. (arXiv:2312.05172v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05172">http://arxiv.org/abs/2312.05172</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05172]] From Lengthy to Lucid: A Systematic Literature Review on NLP Techniques for Taming Long Sentences(http://arxiv.org/abs/2312.05172)</code></li>
<li>Summary: <p>Long sentences have been a persistent issue in written communication for many
years since they make it challenging for readers to grasp the main points or
follow the initial intention of the writer. This survey, conducted using the
PRISMA guidelines, systematically reviews two main strategies for addressing
the issue of long sentences: a) sentence compression and b) sentence splitting.
An increased trend of interest in this area has been observed since 2005, with
significant growth after 2017. Current research is dominated by supervised
approaches for both sentence compression and splitting. Yet, there is a
considerable gap in weakly and self-supervised techniques, suggesting an
opportunity for further research, especially in domains with limited data. In
this survey, we categorize and group the most representative methods into a
comprehensive taxonomy. We also conduct a comparative evaluation analysis of
these methods on common sentence compression and splitting datasets. Finally,
we discuss the challenges and limitations of current methods, providing
valuable insights for future research directions. This survey is meant to serve
as a comprehensive resource for addressing the complexities of long sentences.
We aim to enable researchers to make further advancements in the field until
long sentences are no longer a barrier to effective communication.
</p></li>
</ul>

<h3>Title: Neural Spectral Methods: Self-supervised learning in the spectral domain. (arXiv:2312.05225v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05225">http://arxiv.org/abs/2312.05225</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05225]] Neural Spectral Methods: Self-supervised learning in the spectral domain(http://arxiv.org/abs/2312.05225)</code></li>
<li>Summary: <p>We present Neural Spectral Methods, a technique to solve parametric Partial
Differential Equations (PDEs), grounded in classical spectral methods. Our
method uses orthogonal bases to learn PDE solutions as mappings between
spectral coefficients. In contrast to current machine learning approaches which
enforce PDE constraints by minimizing the numerical quadrature of the residuals
in the spatiotemporal domain, we leverage Parseval's identity and introduce a
new training strategy through a \textit{spectral loss}. Our spectral loss
enables more efficient differentiation through the neural network, and
substantially reduces training complexity. At inference time, the computational
cost of our method remains constant, regardless of the spatiotemporal
resolution of the domain. Our experimental results demonstrate that our method
significantly outperforms previous machine learning approaches in terms of
speed and accuracy by one to two orders of magnitude on multiple different
problems. When compared to numerical solvers of the same accuracy, our method
demonstrates a $10\times$ increase in performance speed.
</p></li>
</ul>

<h2>foundation model</h2>
<h3>Title: Train 'n Trade: Foundations of Parameter Markets. (arXiv:2312.04740v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04740">http://arxiv.org/abs/2312.04740</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04740]] Train 'n Trade: Foundations of Parameter Markets(http://arxiv.org/abs/2312.04740)</code></li>
<li>Summary: <p>Organizations typically train large models individually. This is costly and
time-consuming, particularly for large-scale foundation models. Such vertical
production is known to be suboptimal. Inspired by this economic insight, we ask
whether it is possible to leverage others' expertise by trading the constituent
parts in models, i.e., sets of weights, as if they were market commodities.
While recent advances in aligning and interpolating models suggest that doing
so may be possible, a number of fundamental questions must be answered to
create viable parameter markets. In this work, we address these basic
questions, propose a framework containing the infrastructure necessary for
market operations to take place, study strategies for exchanging parameters,
and offer means for agents to monetize parameters. Excitingly, compared to
agents who train siloed models from scratch, we show that it is possible to
mutually gain by using the market, even in competitive settings. This suggests
that the notion of parameter markets may be a useful paradigm for improving
large-scale model training in the future.
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Damage GAN: A Generative Model for Imbalanced Data. (arXiv:2312.04862v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04862">http://arxiv.org/abs/2312.04862</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04862]] Damage GAN: A Generative Model for Imbalanced Data(http://arxiv.org/abs/2312.04862)</code></li>
<li>Summary: <p>This study delves into the application of Generative Adversarial Networks
(GANs) within the context of imbalanced datasets. Our primary aim is to enhance
the performance and stability of GANs in such datasets. In pursuit of this
objective, we introduce a novel network architecture known as Damage GAN,
building upon the ContraD GAN framework which seamlessly integrates GANs and
contrastive learning. Through the utilization of contrastive learning, the
discriminator is trained to develop an unsupervised representation capable of
distinguishing all provided samples. Our approach draws inspiration from the
straightforward framework for contrastive learning of visual representations
(SimCLR), leading to the formulation of a distinctive loss function. We also
explore the implementation of self-damaging contrastive learning (SDCLR) to
further enhance the optimization of the ContraD GAN model. Comparative
evaluations against baseline models including the deep convolutional GAN
(DCGAN) and ContraD GAN demonstrate the evident superiority of our proposed
model, Damage GAN, in terms of generated image distribution, model stability,
and image quality when applied to imbalanced datasets.
</p></li>
</ul>

<h3>Title: Synthesizing Traffic Datasets using Graph Neural Networks. (arXiv:2312.05031v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05031">http://arxiv.org/abs/2312.05031</a></li>
<li>Code URL: https://github.com/gvogiatzis/trafficgen</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05031]] Synthesizing Traffic Datasets using Graph Neural Networks(http://arxiv.org/abs/2312.05031)</code></li>
<li>Summary: <p>Traffic congestion in urban areas presents significant challenges, and
Intelligent Transportation Systems (ITS) have sought to address these via
automated and adaptive controls. However, these systems often struggle to
transfer simulated experiences to real-world scenarios. This paper introduces a
novel methodology for bridging this `sim-real' gap by creating photorealistic
images from 2D traffic simulations and recorded junction footage. We propose a
novel image generation approach, integrating a Conditional Generative
Adversarial Network with a Graph Neural Network (GNN) to facilitate the
creation of realistic urban traffic images. We harness GNNs' ability to process
information at different levels of abstraction alongside segmented images for
preserving locality data. The presented architecture leverages the power of
SPADE and Graph ATtention (GAT) network models to create images based on
simulated traffic scenarios. These images are conditioned by factors such as
entity positions, colors, and time of day. The uniqueness of our approach lies
in its ability to effectively translate structured and human-readable
conditions, encoded as graphs, into realistic images. This advancement
contributes to applications requiring rich traffic image datasets, from data
augmentation to urban traffic solutions. We further provide an application to
test the model's capabilities, including generating images with manually
defined positions for various entities.
</p></li>
</ul>

<h3>Title: On Sarcasm Detection with OpenAI GPT-based Models. (arXiv:2312.04642v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04642">http://arxiv.org/abs/2312.04642</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04642]] On Sarcasm Detection with OpenAI GPT-based Models(http://arxiv.org/abs/2312.04642)</code></li>
<li>Summary: <p>Sarcasm is a form of irony that requires readers or listeners to interpret
its intended meaning by considering context and social cues. Machine learning
classification models have long had difficulty detecting sarcasm due to its
social complexity and contradictory nature.
</p>
<p>This paper explores the applications of the Generative Pretrained Transformer
(GPT) models, including GPT-3, InstructGPT, GPT-3.5, and GPT-4, in detecting
sarcasm in natural language. It tests fine-tuned and zero-shot models of
different sizes and releases.
</p>
<p>The GPT models were tested on the political and balanced (pol-bal) portion of
the popular Self-Annotated Reddit Corpus (SARC 2.0) sarcasm dataset. In the
fine-tuning case, the largest fine-tuned GPT-3 model achieves accuracy and
$F_1$-score of 0.81, outperforming prior models. In the zero-shot case, one of
GPT-4 models yields an accuracy of 0.70 and $F_1$-score of 0.75. Other models
score lower. Additionally, a model's performance may improve or deteriorate
with each release, highlighting the need to reassess performance after each
release.
</p></li>
</ul>

<h3>Title: From Big to Small Without Losing It All: Text Augmentation with ChatGPT for Efficient Sentiment Analysis. (arXiv:2312.04720v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04720">http://arxiv.org/abs/2312.04720</a></li>
<li>Code URL: https://github.com/clarin-pl/text-augumentation-with-chatgpt</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04720]] From Big to Small Without Losing It All: Text Augmentation with ChatGPT for Efficient Sentiment Analysis(http://arxiv.org/abs/2312.04720)</code></li>
<li>Summary: <p>In the era of artificial intelligence, data is gold but costly to annotate.
The paper demonstrates a groundbreaking solution to this dilemma using ChatGPT
for text augmentation in sentiment analysis. We leverage ChatGPT's generative
capabilities to create synthetic training data that significantly improves the
performance of smaller models, making them competitive with, or even
outperforming, their larger counterparts. This innovation enables models to be
both efficient and effective, thereby reducing computational cost, inference
time, and memory usage without compromising on quality. Our work marks a key
advancement in the cost-effective development and deployment of robust
sentiment analysis models.
</p></li>
</ul>

<h3>Title: Forcing Generative Models to Degenerate Ones: The Power of Data Poisoning Attacks. (arXiv:2312.04748v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04748">http://arxiv.org/abs/2312.04748</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04748]] Forcing Generative Models to Degenerate Ones: The Power of Data Poisoning Attacks(http://arxiv.org/abs/2312.04748)</code></li>
<li>Summary: <p>Growing applications of large language models (LLMs) trained by a third party
raise serious concerns on the security vulnerability of LLMs.It has been
demonstrated that malicious actors can covertly exploit these vulnerabilities
in LLMs through poisoning attacks aimed at generating undesirable outputs.
While poisoning attacks have received significant attention in the image domain
(e.g., object detection), and classification tasks, their implications for
generative models, particularly in the realm of natural language generation
(NLG) tasks, remain poorly understood. To bridge this gap, we perform a
comprehensive exploration of various poisoning techniques to assess their
effectiveness across a range of generative tasks. Furthermore, we introduce a
range of metrics designed to quantify the success and stealthiness of poisoning
attacks specifically tailored to NLG tasks. Through extensive experiments on
multiple NLG tasks, LLMs and datasets, we show that it is possible to
successfully poison an LLM during the fine-tuning stage using as little as 1\%
of the total tuning data samples. Our paper presents the first systematic
approach to comprehend poisoning attacks targeting NLG tasks considering a wide
range of triggers and attack settings. We hope our findings will assist the AI
security community in devising appropriate defenses against such threats.
</p></li>
</ul>

<h3>Title: Seeing ChatGPT Through Universities' Policies, Resources and Guidelines. (arXiv:2312.05235v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05235">http://arxiv.org/abs/2312.05235</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05235]] Seeing ChatGPT Through Universities' Policies, Resources and Guidelines(http://arxiv.org/abs/2312.05235)</code></li>
<li>Summary: <p>The advancements in Artificial Intelligence (AI) technologies such as ChatGPT
have gained popularity in recent days. The integration of ChatGPT in
educational contexts has already created attractions due to a wide range of
applications. However, the automatic generation of human-like texts also poses
potential risks to academic integrity, especially when faced with
writing-intensive language courses. Considering the ongoing debates, this study
aims to investigate the academic policies and guidelines established by US
universities regarding the use of ChatGPT in teaching and learning. The data
sources include academic policies, statements, guidelines as well as relevant
resources that were provided by the top 50 universities in the United States,
according to U.S. News. Thematic analysis and qualitative analysis were
employed in the analysis and showed that most top 50 universities were open but
cautious towards the integration of generative AI in teaching and learning and
also expressed their concerns on ethical usage, accuracy, and data privacy.
Most universities also provided a variety of resources and guidelines,
including syllabus templates/samples, workshops and discussions, shared
articles, and one-on-one consultations, with focuses on general technical
introduction, ethical concerns, pedagogical applications, preventive
strategies, data privacy, limitations, and detective tools. The findings will
inform future policy-making regarding the integration of ChatGPT in
college-level education and influence the provision of supportive resources by
universities for the appropriate application of ChatGPT in education.
</p></li>
</ul>

<h3>Title: On the Inadequacy of Similarity-based Privacy Metrics: Reconstruction Attacks against "Truly Anonymous Synthetic Data''. (arXiv:2312.05114v1 [cs.CR])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.05114">http://arxiv.org/abs/2312.05114</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.05114]] On the Inadequacy of Similarity-based Privacy Metrics: Reconstruction Attacks against "Truly Anonymous Synthetic Data''(http://arxiv.org/abs/2312.05114)</code></li>
<li>Summary: <p>Training generative models to produce synthetic data is meant to provide a
privacy-friendly approach to data release. However, we get robust guarantees
only when models are trained to satisfy Differential Privacy (DP). Alas, this
is not the standard in industry as many companies use ad-hoc strategies to
empirically evaluate privacy based on the statistical similarity between
synthetic and real data. In this paper, we review the privacy metrics offered
by leading companies in this space and shed light on a few critical flaws in
reasoning about privacy entirely via empirical evaluations. We analyze the
undesirable properties of the most popular metrics and filters and demonstrate
their unreliability and inconsistency through counter-examples. We then present
a reconstruction attack, ReconSyn, which successfully recovers (i.e., leaks all
attributes of) at least 78% of the low-density train records (or outliers) with
only black-box access to a single fitted generative model and the privacy
metrics. Finally, we show that applying DP only to the model or using
low-utility generators does not mitigate ReconSyn as the privacy leakage
predominantly comes from the metrics. Overall, our work serves as a warning to
practitioners not to deviate from established privacy-preserving mechanisms.
</p></li>
</ul>

<h3>Title: SparQ Attention: Bandwidth-Efficient LLM Inference. (arXiv:2312.04985v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04985">http://arxiv.org/abs/2312.04985</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04985]] SparQ Attention: Bandwidth-Efficient LLM Inference(http://arxiv.org/abs/2312.04985)</code></li>
<li>Summary: <p>Generative large language models (LLMs) have opened up numerous novel
possibilities, but due to their significant computational requirements their
ubiquitous use remains challenging. Some of the most useful applications
require processing large numbers of samples at a time and using long contexts,
both significantly increasing the memory communication load of the models. We
introduce SparQ Attention, a technique for increasing the inference throughput
of LLMs by reducing the memory bandwidth requirements within the attention
blocks through selective fetching of the cached history. Our proposed technique
can be applied directly to off-the-shelf LLMs during inference, without
requiring any modification to the pre-training setup or additional fine-tuning.
We show how SparQ Attention can decrease the attention memory bandwidth
requirements up to eight times without any loss in accuracy by evaluating Llama
2 and Pythia models on a wide range of downstream tasks.
</p></li>
</ul>

<h2>anomaly</h2>
<h3>Title: Visual Grounding of Whole Radiology Reports for 3D CT Images. (arXiv:2312.04794v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04794">http://arxiv.org/abs/2312.04794</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04794]] Visual Grounding of Whole Radiology Reports for 3D CT Images(http://arxiv.org/abs/2312.04794)</code></li>
<li>Summary: <p>Building a large-scale training dataset is an essential problem in the
development of medical image recognition systems. Visual grounding techniques,
which automatically associate objects in images with corresponding
descriptions, can facilitate labeling of large number of images. However,
visual grounding of radiology reports for CT images remains challenging,
because so many kinds of anomalies are detectable via CT imaging, and resulting
report descriptions are long and complex. In this paper, we present the first
visual grounding framework designed for CT image and report pairs covering
various body parts and diverse anomaly types. Our framework combines two
components of 1) anatomical segmentation of images, and 2) report structuring.
The anatomical segmentation provides multiple organ masks of given CT images,
and helps the grounding model recognize detailed anatomies. The report
structuring helps to accurately extract information regarding the presence,
location, and type of each anomaly described in corresponding reports. Given
the two additional image/report features, the grounding model can achieve
better localization. In the verification process, we constructed a large-scale
dataset with region-description correspondence annotations for 10,410 studies
of 7,321 unique patients. We evaluated our framework using grounding accuracy,
the percentage of correctly localized anomalies, as a metric and demonstrated
that the combination of the anatomical segmentation and the report structuring
improves the performance with a large margin over the baseline model (66.0% vs
77.8%). Comparison with the prior techniques also showed higher performance of
our method.
</p></li>
</ul>

<h3>Title: Data-driven Semi-supervised Machine Learning with Surrogate Safety Measures for Abnormal Driving Behavior Detection. (arXiv:2312.04610v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04610">http://arxiv.org/abs/2312.04610</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04610]] Data-driven Semi-supervised Machine Learning with Surrogate Safety Measures for Abnormal Driving Behavior Detection(http://arxiv.org/abs/2312.04610)</code></li>
<li>Summary: <p>Detecting abnormal driving behavior is critical for road traffic safety and
the evaluation of drivers' behavior. With the advancement of machine learning
(ML) algorithms and the accumulation of naturalistic driving data, many ML
models have been adopted for abnormal driving behavior detection. Most existing
ML-based detectors rely on (fully) supervised ML methods, which require
substantial labeled data. However, ground truth labels are not always available
in the real world, and labeling large amounts of data is tedious. Thus, there
is a need to explore unsupervised or semi-supervised methods to make the
anomaly detection process more feasible and efficient. To fill this research
gap, this study analyzes large-scale real-world data revealing several abnormal
driving behaviors (e.g., sudden acceleration, rapid lane-changing) and develops
a Hierarchical Extreme Learning Machines (HELM) based semi-supervised ML method
using partly labeled data to accurately detect the identified abnormal driving
behaviors. Moreover, previous ML-based approaches predominantly utilize basic
vehicle motion features (such as velocity and acceleration) to label and detect
abnormal driving behaviors, while this study seeks to introduce Surrogate
Safety Measures (SSMs) as the input features for ML models to improve the
detection performance. Results from extensive experiments demonstrate the
effectiveness of the proposed semi-supervised ML model with the introduced SSMs
serving as important features. The proposed semi-supervised ML method
outperforms other baseline semi-supervised or unsupervised methods regarding
various metrics, e.g., delivering the best accuracy at 99.58% and the best F-1
measure at 0.9913. The ablation study further highlights the significance of
SSMs for advancing detection performance.
</p></li>
</ul>

<h2>in-context</h2>
<h3>Title: Zoology: Measuring and Improving Recall in Efficient Language Models. (arXiv:2312.04927v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04927">http://arxiv.org/abs/2312.04927</a></li>
<li>Code URL: https://github.com/hazyresearch/zoology</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04927]] Zoology: Measuring and Improving Recall in Efficient Language Models(http://arxiv.org/abs/2312.04927)</code></li>
<li>Summary: <p>Attention-free language models that combine gating and convolutions are
growing in popularity due to their efficiency and increasingly competitive
performance. To better understand these architectures, we pretrain a suite of
17 attention and "gated-convolution" language models, finding that SoTA
gated-convolution architectures still underperform attention by up to 2.1
perplexity points on the Pile. In fine-grained analysis, we find 82% of the gap
is explained by each model's ability to recall information that is previously
mentioned in-context, e.g. "Hakuna Matata means no worries Hakuna Matata it
means no" $\rightarrow$ "??". On this task, termed "associative recall", we
find that attention outperforms gated-convolutions by a large margin: a 70M
parameter attention model outperforms a 1.4 billion parameter gated-convolution
model on associative recall. This is surprising because prior work shows gated
convolutions can perfectly solve synthetic tests for AR capability. To close
the gap between synthetics and real language, we develop a new formalization of
the task called multi-query associative recall (MQAR) that better reflects
actual language. We perform an empirical and theoretical study of MQAR that
elucidates differences in the parameter-efficiency of attention and
gated-convolution recall. Informed by our analysis, we evaluate simple
convolution-attention hybrids and show that hybrids with input-dependent sparse
attention patterns can close 97.4% of the gap to attention, while maintaining
sub-quadratic scaling. Our code is accessible at:
https://github.com/HazyResearch/zoology.
</p></li>
</ul>

<h3>Title: The ICL Consistency Test. (arXiv:2312.04945v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2312.04945">http://arxiv.org/abs/2312.04945</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2312.04945]] The ICL Consistency Test(http://arxiv.org/abs/2312.04945)</code></li>
<li>Summary: <p>Just like the previous generation of task-tuned models, large language models
(LLMs) that are adapted to tasks via prompt-based methods like
in-context-learning (ICL) perform well in some setups but not in others. This
lack of consistency in prompt-based learning hints at a lack of robust
generalisation. We here introduce the ICL consistency test -- a contribution to
the GenBench collaborative benchmark task (CBT) -- which evaluates how
consistent a model makes predictions across many different setups while using
the same data. The test is based on different established natural language
inference tasks. We provide preprocessed data constituting 96 different
'setups' and a metric that estimates model consistency across these setups. The
metric is provided on a fine-grained level to understand what properties of a
setup render predictions unstable and on an aggregated level to compare overall
model consistency. We conduct an empirical analysis of eight state-of-the-art
models, and our consistency metric reveals how all tested LLMs lack robust
generalisation.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
