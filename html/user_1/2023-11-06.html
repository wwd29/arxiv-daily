<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>diffusion</h2>
<h3>Title: Exploring the Hyperparameter Space of Image Diffusion Models for Echocardiogram Generation. (arXiv:2311.01567v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01567">http://arxiv.org/abs/2311.01567</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01567]] Exploring the Hyperparameter Space of Image Diffusion Models for Echocardiogram Generation(http://arxiv.org/abs/2311.01567)</code></li>
<li>Summary: <p>This work presents an extensive hyperparameter search on Image Diffusion
Models for Echocardiogram generation. The objective is to establish
foundational benchmarks and provide guidelines within the realm of ultrasound
image and video generation. This study builds over the latest advancements,
including cutting-edge model architectures and training methodologies. We also
examine the distribution shift between real and generated samples and consider
potential solutions, crucial to train efficient models on generated data. We
determine an Optimal FID score of $0.88$ for our research problem and achieve
an FID of $2.60$. This work is aimed at contributing valuable insights and
serving as a reference for further developments in the specialized field of
ultrasound image and video generation.
</p></li>
</ul>

<h3>Title: Improving Fairness using Vision-Language Driven Image Augmentation. (arXiv:2311.01573v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01573">http://arxiv.org/abs/2311.01573</a></li>
<li>Code URL: https://github.com/moreno98/vision-language-bias-control</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01573]] Improving Fairness using Vision-Language Driven Image Augmentation(http://arxiv.org/abs/2311.01573)</code></li>
<li>Summary: <p>Fairness is crucial when training a deep-learning discriminative model,
especially in the facial domain. Models tend to correlate specific
characteristics (such as age and skin color) with unrelated attributes
(downstream tasks), resulting in biases which do not correspond to reality. It
is common knowledge that these correlations are present in the data and are
then transferred to the models during training. This paper proposes a method to
mitigate these correlations to improve fairness. To do so, we learn
interpretable and meaningful paths lying in the semantic space of a pre-trained
diffusion model (DiffAE) -- such paths being supervised by contrastive text
dipoles. That is, we learn to edit protected characteristics (age and skin
color). These paths are then applied to augment images to improve the fairness
of a given dataset. We test the proposed method on CelebA-HQ and UTKFace on
several downstream tasks with age and skin color as protected characteristics.
As a proxy for fairness, we compute the difference in accuracy with respect to
the protected characteristics. Quantitative results show how the augmented
images help the model improve the overall accuracy, the aforementioned metric,
and the disparity of equal opportunity. Code is available at:
https://github.com/Moreno98/Vision-Language-Bias-Control.
</p></li>
</ul>

<h3>Title: PDF: Point Diffusion Implicit Function for Large-scale Scene Neural Representation. (arXiv:2311.01773v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01773">http://arxiv.org/abs/2311.01773</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01773]] PDF: Point Diffusion Implicit Function for Large-scale Scene Neural Representation(http://arxiv.org/abs/2311.01773)</code></li>
<li>Summary: <p>Recent advances in implicit neural representations have achieved impressive
results by sampling and fusing individual points along sampling rays in the
sampling space. However, due to the explosively growing sampling space, finely
representing and synthesizing detailed textures remains a challenge for
unbounded large-scale outdoor scenes. To alleviate the dilemma of using
individual points to perceive the entire colossal space, we explore learning
the surface distribution of the scene to provide structural priors and reduce
the samplable space and propose a Point Diffusion implicit Function, PDF, for
large-scale scene neural representation. The core of our method is a
large-scale point cloud super-resolution diffusion module that enhances the
sparse point cloud reconstructed from several training images into a dense
point cloud as an explicit prior. Then in the rendering stage, only sampling
points with prior points within the sampling radius are retained. That is, the
sampling space is reduced from the unbounded space to the scene surface.
Meanwhile, to fill in the background of the scene that cannot be provided by
point clouds, the region sampling based on Mip-NeRF 360 is employed to model
the background representation. Expensive experiments have demonstrated the
effectiveness of our method for large-scale scene novel view synthesis, which
outperforms relevant state-of-the-art baselines.
</p></li>
</ul>

<h3>Title: DiffDub: Person-generic Visual Dubbing Using Inpainting Renderer with Diffusion Auto-encoder. (arXiv:2311.01811v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01811">http://arxiv.org/abs/2311.01811</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01811]] DiffDub: Person-generic Visual Dubbing Using Inpainting Renderer with Diffusion Auto-encoder(http://arxiv.org/abs/2311.01811)</code></li>
<li>Summary: <p>Generating high-quality and person-generic visual dubbing remains a
challenge. Recent innovation has seen the advent of a two-stage paradigm,
decoupling the rendering and lip synchronization process facilitated by
intermediate representation as a conduit. Still, previous methodologies rely on
rough landmarks or are confined to a single speaker, thus limiting their
performance. In this paper, we propose DiffDub: Diffusion-based dubbing. We
first craft the Diffusion auto-encoder by an inpainting renderer incorporating
a mask to delineate editable zones and unaltered regions. This allows for
seamless filling of the lower-face region while preserving the remaining parts.
Throughout our experiments, we encountered several challenges. Primarily, the
semantic encoder lacks robustness, constricting its ability to capture
high-level features. Besides, the modeling ignored facial positioning, causing
mouth or nose jitters across frames. To tackle these issues, we employ
versatile strategies, including data augmentation and supplementary eye
guidance. Moreover, we encapsulated a conformer-based reference encoder and
motion generator fortified by a cross-attention mechanism. This enables our
model to learn person-specific textures with varying references and reduces
reliance on paired audio-visual data. Our rigorous experiments comprehensively
highlight that our ground-breaking approach outpaces existing methods with
considerable margins and delivers seamless, intelligible videos in
person-generic and multilingual scenarios.
</p></li>
</ul>

<h3>Title: On the Generalization Properties of Diffusion Models. (arXiv:2311.01797v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01797">http://arxiv.org/abs/2311.01797</a></li>
<li>Code URL: https://github.com/lphleo/diffusion_generalization</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01797]] On the Generalization Properties of Diffusion Models(http://arxiv.org/abs/2311.01797)</code></li>
<li>Summary: <p>Diffusion models are a class of generative models that serve to establish a
stochastic transport map between an empirically observed, yet unknown, target
distribution and a known prior. Despite their remarkable success in real-world
applications, a theoretical understanding of their generalization capabilities
remains underdeveloped. This work embarks on a comprehensive theoretical
exploration of the generalization attributes of diffusion models. We establish
theoretical estimates of the generalization gap that evolves in tandem with the
training dynamics of score-based diffusion models, suggesting a polynomially
small generalization error ($O(n^{-2/5}+m^{-4/5})$) on both the sample size $n$
and the model capacity $m$, evading the curse of dimensionality (i.e., not
exponentially large in the data dimension) when early-stopped. Furthermore, we
extend our quantitative analysis to a data-dependent scenario, wherein target
distributions are portrayed as a succession of densities with progressively
increasing distances between modes. This precisely elucidates the adverse
effect of "modes shift" in ground truths on the model generalization. Moreover,
these estimates are not solely theoretical constructs but have also been
confirmed through numerical simulations. Our findings contribute to the
rigorous understanding of diffusion models' generalization properties and
provide insights that may guide practical applications.
</p></li>
</ul>

<h2>self-supervised</h2>
<h3>Title: Flow-Based Feature Fusion for Vehicle-Infrastructure Cooperative 3D Object Detection. (arXiv:2311.01682v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01682">http://arxiv.org/abs/2311.01682</a></li>
<li>Code URL: https://github.com/haibao-yu/ffnet-vic3d</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01682]] Flow-Based Feature Fusion for Vehicle-Infrastructure Cooperative 3D Object Detection(http://arxiv.org/abs/2311.01682)</code></li>
<li>Summary: <p>Cooperatively utilizing both ego-vehicle and infrastructure sensor data can
significantly enhance autonomous driving perception abilities. However, the
uncertain temporal asynchrony and limited communication conditions can lead to
fusion misalignment and constrain the exploitation of infrastructure data. To
address these issues in vehicle-infrastructure cooperative 3D (VIC3D) object
detection, we propose the Feature Flow Net (FFNet), a novel cooperative
detection framework. FFNet is a flow-based feature fusion framework that uses a
feature flow prediction module to predict future features and compensate for
asynchrony. Instead of transmitting feature maps extracted from still-images,
FFNet transmits feature flow, leveraging the temporal coherence of sequential
infrastructure frames. Furthermore, we introduce a self-supervised training
approach that enables FFNet to generate feature flow with feature prediction
ability from raw infrastructure sequences. Experimental results demonstrate
that our proposed method outperforms existing cooperative detection methods
while only requiring about 1/100 of the transmission cost of raw data and
covers all latency in one model on the DAIR-V2X dataset. The code is available
at
\href{https://github.com/haibao-yu/FFNet-VIC3D}{https://github.com/haibao-yu/FFNet-VIC3D}.
</p></li>
</ul>

<h2>foundation model</h2>
<h3>Title: Leveraging Large-Scale Pretrained Vision Foundation Models for Label-Efficient 3D Point Cloud Segmentation. (arXiv:2311.01989v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01989">http://arxiv.org/abs/2311.01989</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01989]] Leveraging Large-Scale Pretrained Vision Foundation Models for Label-Efficient 3D Point Cloud Segmentation(http://arxiv.org/abs/2311.01989)</code></li>
<li>Summary: <p>Recently, large-scale pre-trained models such as Segment-Anything Model (SAM)
and Contrastive Language-Image Pre-training (CLIP) have demonstrated remarkable
success and revolutionized the field of computer vision. These foundation
vision models effectively capture knowledge from a large-scale broad data with
their vast model parameters, enabling them to perform zero-shot segmentation on
previously unseen data without additional training. While they showcase
competence in 2D tasks, their potential for enhancing 3D scene understanding
remains relatively unexplored. To this end, we present a novel framework that
adapts various foundational models for the 3D point cloud segmentation task.
Our approach involves making initial predictions of 2D semantic masks using
different large vision models. We then project these mask predictions from
various frames of RGB-D video sequences into 3D space. To generate robust 3D
semantic pseudo labels, we introduce a semantic label fusion strategy that
effectively combines all the results via voting. We examine diverse scenarios,
like zero-shot learning and limited guidance from sparse 2D point labels, to
assess the pros and cons of different vision foundation models. Our approach is
experimented on ScanNet dataset for 3D indoor scenes, and the results
demonstrate the effectiveness of adopting general 2D foundation models on
solving 3D point cloud segmentation tasks.
</p></li>
</ul>

<h3>Title: EmerNeRF: Emergent Spatial-Temporal Scene Decomposition via Self-Supervision. (arXiv:2311.02077v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.02077">http://arxiv.org/abs/2311.02077</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.02077]] EmerNeRF: Emergent Spatial-Temporal Scene Decomposition via Self-Supervision(http://arxiv.org/abs/2311.02077)</code></li>
<li>Summary: <p>We present EmerNeRF, a simple yet powerful approach for learning
spatial-temporal representations of dynamic driving scenes. Grounded in neural
fields, EmerNeRF simultaneously captures scene geometry, appearance, motion,
and semantics via self-bootstrapping. EmerNeRF hinges upon two core components:
First, it stratifies scenes into static and dynamic fields. This decomposition
emerges purely from self-supervision, enabling our model to learn from general,
in-the-wild data sources. Second, EmerNeRF parameterizes an induced flow field
from the dynamic field and uses this flow field to further aggregate
multi-frame features, amplifying the rendering precision of dynamic objects.
Coupling these three fields (static, dynamic, and flow) enables EmerNeRF to
represent highly-dynamic scenes self-sufficiently, without relying on ground
truth object annotations or pre-trained models for dynamic object segmentation
or optical flow estimation. Our method achieves state-of-the-art performance in
sensor simulation, significantly outperforming previous methods when
reconstructing static (+2.93 PSNR) and dynamic (+3.70 PSNR) scenes. In
addition, to bolster EmerNeRF's semantic generalization, we lift 2D visual
foundation model features into 4D space-time and address a general positional
bias in modern Transformers, significantly boosting 3D perception performance
(e.g., 37.50% relative improvement in occupancy prediction accuracy on
average). Finally, we construct a diverse and challenging 120-sequence dataset
to benchmark neural fields under extreme and highly-dynamic settings.
</p></li>
</ul>

<h3>Title: $R^3$-NL2GQL: A Hybrid Models Approach for for Accuracy Enhancing and Hallucinations Mitigation. (arXiv:2311.01862v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01862">http://arxiv.org/abs/2311.01862</a></li>
<li>Code URL: https://github.com/zhiqix/nl2gql</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01862]] $R^3$-NL2GQL: A Hybrid Models Approach for for Accuracy Enhancing and Hallucinations Mitigation(http://arxiv.org/abs/2311.01862)</code></li>
<li>Summary: <p>While current NL2SQL tasks constructed using Foundation Models have achieved
commendable results, their direct application to Natural Language to Graph
Query Language (NL2GQL) tasks poses challenges due to the significant
differences between GQL and SQL expressions, as well as the numerous types of
GQL. Our extensive experiments reveal that in NL2GQL tasks, larger Foundation
Models demonstrate superior cross-schema generalization abilities, while
smaller Foundation Models struggle to improve their GQL generation capabilities
through fine-tuning. However, after fine-tuning, smaller models exhibit better
intent comprehension and higher grammatical accuracy. Diverging from rule-based
and slot-filling techniques, we introduce R3-NL2GQL, which employs both smaller
and larger Foundation Models as reranker, rewriter and refiner. The approach
harnesses the comprehension ability of smaller models for information reranker
and rewriter, and the exceptional generalization and generation capabilities of
larger models to transform input natural language queries and code structure
schema into any form of GQLs. Recognizing the lack of established datasets in
this nascent domain, we have created a bilingual dataset derived from graph
database documentation and some open-source Knowledge Graphs (KGs). We tested
our approach on this dataset and the experimental results showed that delivers
promising performance and robustness.Our code and dataset is available at
https://github.com/zhiqix/NL2GQL
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Detecting Spurious Correlations via Robust Visual Concepts in Real and AI-Generated Image Classification. (arXiv:2311.01655v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01655">http://arxiv.org/abs/2311.01655</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01655]] Detecting Spurious Correlations via Robust Visual Concepts in Real and AI-Generated Image Classification(http://arxiv.org/abs/2311.01655)</code></li>
<li>Summary: <p>Often machine learning models tend to automatically learn associations
present in the training data without questioning their validity or
appropriateness. This undesirable property is the root cause of the
manifestation of spurious correlations, which render models unreliable and
prone to failure in the presence of distribution shifts. Research shows that
most methods attempting to remedy spurious correlations are only effective for
a model's known spurious associations. Current spurious correlation detection
algorithms either rely on extensive human annotations or are too restrictive in
their formulation. Moreover, they rely on strict definitions of visual
artifacts that may not apply to data produced by generative models, as they are
known to hallucinate contents that do not conform to standard specifications.
In this work, we introduce a general-purpose method that efficiently detects
potential spurious correlations, and requires significantly less human
interference in comparison to the prior art. Additionally, the proposed method
provides intuitive explanations while eliminating the need for pixel-level
annotations. We demonstrate the proposed method's tolerance to the peculiarity
of AI-generated images, which is a considerably challenging task, one where
most of the existing methods fall short. Consequently, our method is also
suitable for detecting spurious correlations that may propagate to downstream
applications originating from generative models.
</p></li>
</ul>

<h3>Title: Efficient Cloud Pipelines for Neural Radiance Fields. (arXiv:2311.01659v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01659">http://arxiv.org/abs/2311.01659</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01659]] Efficient Cloud Pipelines for Neural Radiance Fields(http://arxiv.org/abs/2311.01659)</code></li>
<li>Summary: <p>Since their introduction in 2020, Neural Radiance Fields (NeRFs) have taken
the computer vision community by storm. They provide a multi-view
representation of a scene or object that is ideal for eXtended Reality (XR)
applications and for creative endeavors such as virtual production, as well as
change detection operations in geospatial analytics. The computational cost of
these generative AI models is quite high, however, and the construction of
cloud pipelines to generate NeRFs is neccesary to realize their potential in
client applications. In this paper, we present pipelines on a high performance
academic computing cluster and compare it with a pipeline implemented on
Microsoft Azure. Along the way, we describe some uses of NeRFs in enabling
novel user interaction scenarios.
</p></li>
</ul>

<h3>Title: Data-Free Distillation of Language Model by Text-to-Text Transfer. (arXiv:2311.01689v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01689">http://arxiv.org/abs/2311.01689</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01689]] Data-Free Distillation of Language Model by Text-to-Text Transfer(http://arxiv.org/abs/2311.01689)</code></li>
<li>Summary: <p>Data-Free Knowledge Distillation (DFKD) plays a vital role in compressing the
model when original training data is unavailable. Previous works for DFKD in
NLP mainly focus on distilling encoder-only structures like BERT on
classification tasks, which overlook the notable progress of generative
language modeling. In this work, we propose a novel DFKD framework, namely
DFKD-T$^{3}$, where the pretrained generative language model can also serve as
a controllable data generator for model compression. This novel framework
DFKD-T$^{3}$ leads to an end-to-end learnable text-to-text framework to
transform the general domain corpus to compression-friendly task data,
targeting to improve both the \textit{specificity} and \textit{diversity}.
Extensive experiments show that our method can boost the distillation
performance in various downstream tasks such as sentiment analysis, linguistic
acceptability, and information extraction. Furthermore, we show that the
generated texts can be directly used for distilling other language models and
outperform the SOTA methods, making our method more appealing in a general DFKD
setting. Our code is available at
https://gitee.com/mindspore/models/tree/master/research/nlp/DFKD\_T3.
</p></li>
</ul>

<h3>Title: An Empirical Study of Benchmarking Chinese Aspect Sentiment Quad Prediction. (arXiv:2311.01713v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01713">http://arxiv.org/abs/2311.01713</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01713]] An Empirical Study of Benchmarking Chinese Aspect Sentiment Quad Prediction(http://arxiv.org/abs/2311.01713)</code></li>
<li>Summary: <p>Aspect sentiment quad prediction (ASQP) is a critical subtask of aspect-level
sentiment analysis. Current ASQP datasets are characterized by their small size
and low quadruple density, which hinders technical development. To expand
capacity, we construct two large Chinese ASQP datasets crawled from multiple
online platforms. The datasets hold several significant characteristics: larger
size (each with 10,000+ samples) and rich aspect categories, more words per
sentence, and higher density than existing ASQP datasets. Moreover, we are the
first to evaluate the performance of Generative Pre-trained Transformer (GPT)
series models on ASQP and exhibit potential issues. The experiments with
state-of-the-art ASQP baselines underscore the need to explore additional
techniques to address ASQP, as well as the importance of further investigation
into methods to improve the performance of GPTs.
</p></li>
</ul>

<h3>Title: Indo LEGO-ABSA: A Multitask Generative Aspect Based Sentiment Analysis for Indonesian Language. (arXiv:2311.01757v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01757">http://arxiv.org/abs/2311.01757</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01757]] Indo LEGO-ABSA: A Multitask Generative Aspect Based Sentiment Analysis for Indonesian Language(http://arxiv.org/abs/2311.01757)</code></li>
<li>Summary: <p>Aspect-based sentiment analysis is a method in natural language processing
aimed at identifying and understanding sentiments related to specific aspects
of an entity. Aspects are words or phrases that represent an aspect or
attribute of a particular entity. Previous research has utilized generative
pre-trained language models to perform aspect-based sentiment analysis.
LEGO-ABSA is one framework that has successfully employed generative
pre-trained language models in aspect-based sentiment analysis, particularly in
English. LEGO-ABSA uses a multitask learning and prompting approach to enhance
model performance. However, the application of this approach has not been done
in the context of Bahasa Indonesia. Therefore, this research aims to implement
the multitask learning and prompting approach in aspect-based sentiment
analysis for Bahasa Indonesia using generative pre-trained language models. In
this study, the Indo LEGO-ABSA model is developed, which is an aspect-based
sentiment analysis model utilizing generative pre-trained language models and
trained with multitask learning and prompting. Indo LEGO-ABSA is trained with a
hotel domain dataset in the Indonesian language. The obtained results include
an f1-score of 79.55% for the Aspect Sentiment Triplet Extraction task, 86.09%
for Unified Aspect-based Sentiment Analysis, 79.85% for Aspect Opinion Pair
Extraction, 87.45% for Aspect Term Extraction, and 88.09% for Opinion Term
Extraction. Indo LEGO-ABSA adopts the LEGO-ABSA framework that employs the T5
model, specifically mT5, by applying multitask learning to train all tasks
within aspect-based sentiment analysis.
</p></li>
</ul>

<h3>Title: Indicative Summarization of Long Discussions. (arXiv:2311.01882v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01882">http://arxiv.org/abs/2311.01882</a></li>
<li>Code URL: https://github.com/webis-de/emnlp-23</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01882]] Indicative Summarization of Long Discussions(http://arxiv.org/abs/2311.01882)</code></li>
<li>Summary: <p>Online forums encourage the exchange and discussion of different stances on
many topics. Not only do they provide an opportunity to present one's own
arguments, but may also gather a broad cross-section of others' arguments.
However, the resulting long discussions are difficult to overview. This paper
presents a novel unsupervised approach using large language models (LLMs) to
generating indicative summaries for long discussions that basically serve as
tables of contents. Our approach first clusters argument sentences, generates
cluster labels as abstractive summaries, and classifies the generated cluster
labels into argumentation frames resulting in a two-level summary. Based on an
extensively optimized prompt engineering approach, we evaluate 19~LLMs for
generative cluster labeling and frame classification. To evaluate the
usefulness of our indicative summaries, we conduct a purpose-driven user study
via a new visual interface called Discussion Explorer: It shows that our
proposed indicative summaries serve as a convenient navigation tool to explore
long discussions.
</p></li>
</ul>

<h2>anomaly</h2>
<h3>Title: InsPLAD: A Dataset and Benchmark for Power Line Asset Inspection in UAV Images. (arXiv:2311.01619v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01619">http://arxiv.org/abs/2311.01619</a></li>
<li>Code URL: https://github.com/andreluizbvs/insplad</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01619]] InsPLAD: A Dataset and Benchmark for Power Line Asset Inspection in UAV Images(http://arxiv.org/abs/2311.01619)</code></li>
<li>Summary: <p>Power line maintenance and inspection are essential to avoid power supply
interruptions, reducing its high social and financial impacts yearly.
Automating power line visual inspections remains a relevant open problem for
the industry due to the lack of public real-world datasets of power line
components and their various defects to foster new research. This paper
introduces InsPLAD, a Power Line Asset Inspection Dataset and Benchmark
containing 10,607 high-resolution Unmanned Aerial Vehicles colour images. The
dataset contains seventeen unique power line assets captured from real-world
operating power lines. Additionally, five of those assets present six defects:
four of which are corrosion, one is a broken component, and one is a bird's
nest presence. All assets were labelled according to their condition, whether
normal or the defect name found on an image level. We thoroughly evaluate
state-of-the-art and popular methods for three image-level computer vision
tasks covered by InsPLAD: object detection, through the AP metric; defect
classification, through Balanced Accuracy; and anomaly detection, through the
AUROC metric. InsPLAD offers various vision challenges from uncontrolled
environments, such as multi-scale objects, multi-size class instances, multiple
objects per image, intra-class variation, cluttered background, distinct
point-of-views, perspective distortion, occlusion, and varied lighting
conditions. To the best of our knowledge, InsPLAD is the first large real-world
dataset and benchmark for power line asset inspection with multiple components
and defects for various computer vision tasks, with a potential impact to
improve state-of-the-art methods in the field. It will be publicly available in
its integrity on a repository with a thorough description. It can be found at
https://github.com/andreluizbvs/InsPLAD.
</p></li>
</ul>

<h3>Title: Holistic Representation Learning for Multitask Trajectory Anomaly Detection. (arXiv:2311.01851v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01851">http://arxiv.org/abs/2311.01851</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01851]] Holistic Representation Learning for Multitask Trajectory Anomaly Detection(http://arxiv.org/abs/2311.01851)</code></li>
<li>Summary: <p>Video anomaly detection deals with the recognition of abnormal events in
videos. Apart from the visual signal, video anomaly detection has also been
addressed with the use of skeleton sequences. We propose a holistic
representation of skeleton trajectories to learn expected motions across
segments at different times. Our approach uses multitask learning to
reconstruct any continuous unobserved temporal segment of the trajectory
allowing the extrapolation of past or future segments and the interpolation of
in-between segments. We use an end-to-end attention-based encoder-decoder. We
encode temporally occluded trajectories, jointly learn latent representations
of the occluded segments, and reconstruct trajectories based on expected
motions across different temporal segments. Extensive experiments on three
trajectory-based video anomaly detection datasets show the advantages and
effectiveness of our approach with state-of-the-art results on anomaly
detection in skeleton trajectories.
</p></li>
</ul>

<h2>in-context</h2>
<h3>Title: Sentiment Analysis through LLM Negotiations. (arXiv:2311.01876v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01876">http://arxiv.org/abs/2311.01876</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01876]] Sentiment Analysis through LLM Negotiations(http://arxiv.org/abs/2311.01876)</code></li>
<li>Summary: <p>A standard paradigm for sentiment analysis is to rely on a singular LLM and
makes the decision in a single round under the framework of in-context
learning. This framework suffers the key disadvantage that the single-turn
output generated by a single LLM might not deliver the perfect decision, just
as humans sometimes need multiple attempts to get things right. This is
especially true for the task of sentiment analysis where deep reasoning is
required to address the complex linguistic phenomenon (e.g., clause
composition, irony, etc) in the input.
</p>
<p>To address this issue, this paper introduces a multi-LLM negotiation
framework for sentiment analysis. The framework consists of a reasoning-infused
generator to provide decision along with rationale, a explanation-deriving
discriminator to evaluate the credibility of the generator. The generator and
the discriminator iterate until a consensus is reached. The proposed framework
naturally addressed the aforementioned challenge, as we are able to take the
complementary abilities of two LLMs, have them use rationale to persuade each
other for correction.
</p>
<p>Experiments on a wide range of sentiment analysis benchmarks (SST-2, Movie
Review, Twitter, yelp, amazon, IMDB) demonstrate the effectiveness of proposed
approach: it consistently yields better performances than the ICL baseline
across all benchmarks, and even superior performances to supervised baselines
on the Twitter and movie review datasets.
</p></li>
</ul>

<h3>Title: Hint-enhanced In-Context Learning wakes Large Language Models up for knowledge-intensive tasks. (arXiv:2311.01949v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2311.01949">http://arxiv.org/abs/2311.01949</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2311.01949]] Hint-enhanced In-Context Learning wakes Large Language Models up for knowledge-intensive tasks(http://arxiv.org/abs/2311.01949)</code></li>
<li>Summary: <p>In-context learning (ICL) ability has emerged with the increasing scale of
large language models (LLMs), enabling them to learn input-label mappings from
demonstrations and perform well on downstream tasks. However, under the
standard ICL setting, LLMs may sometimes neglect query-related information in
demonstrations, leading to incorrect predictions. To address this limitation,
we propose a new paradigm called Hint-enhanced In-Context Learning (HICL) to
explore the power of ICL in open-domain question answering, an important form
in knowledge-intensive tasks. HICL leverages LLMs' reasoning ability to extract
query-related knowledge from demonstrations, then concatenates the knowledge to
prompt LLMs in a more explicit way. Furthermore, we track the source of this
knowledge to identify specific examples, and introduce a Hint-related Example
Retriever (HER) to select informative examples for enhanced demonstrations. We
evaluate HICL with HER on 3 open-domain QA benchmarks, and observe average
performance gains of 2.89 EM score and 2.52 F1 score on gpt-3.5-turbo, 7.62 EM
score and 7.27 F1 score on LLaMA-2-Chat-7B compared with standard setting.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
