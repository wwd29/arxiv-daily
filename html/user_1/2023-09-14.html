<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h2>diffusion</h2>
<h3>Title: MagiCapture: High-Resolution Multi-Concept Portrait Customization. (arXiv:2309.06895v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.06895">http://arxiv.org/abs/2309.06895</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.06895]] MagiCapture: High-Resolution Multi-Concept Portrait Customization(http://arxiv.org/abs/2309.06895)</code></li>
<li>Summary: <p>Large-scale text-to-image models including Stable Diffusion are capable of
generating high-fidelity photorealistic portrait images. There is an active
research area dedicated to personalizing these models, aiming to synthesize
specific subjects or styles using provided sets of reference images. However,
despite the plausible results from these personalization methods, they tend to
produce images that often fall short of realism and are not yet on a
commercially viable level. This is particularly noticeable in portrait image
generation, where any unnatural artifact in human faces is easily discernible
due to our inherent human bias. To address this, we introduce MagiCapture, a
personalization method for integrating subject and style concepts to generate
high-resolution portrait images using just a few subject and style references.
For instance, given a handful of random selfies, our fine-tuned model can
generate high-quality portrait images in specific styles, such as passport or
profile photos. The main challenge with this task is the absence of ground
truth for the composed concepts, leading to a reduction in the quality of the
final output and an identity shift of the source subject. To address these
issues, we present a novel Attention Refocusing loss coupled with auxiliary
priors, both of which facilitate robust learning within this weakly supervised
learning setting. Our pipeline also includes additional post-processing steps
to ensure the creation of highly realistic outputs. MagiCapture outperforms
other baselines in both quantitative and qualitative evaluations and can also
be generalized to other non-human objects.
</p></li>
</ul>

<h3>Title: DreamStyler: Paint by Style Inversion with Text-to-Image Diffusion Models. (arXiv:2309.06933v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.06933">http://arxiv.org/abs/2309.06933</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.06933]] DreamStyler: Paint by Style Inversion with Text-to-Image Diffusion Models(http://arxiv.org/abs/2309.06933)</code></li>
<li>Summary: <p>Recent progresses in large-scale text-to-image models have yielded remarkable
accomplishments, finding various applications in art domain. However,
expressing unique characteristics of an artwork (e.g. brushwork, colortone, or
composition) with text prompts alone may encounter limitations due to the
inherent constraints of verbal description. To this end, we introduce
DreamStyler, a novel framework designed for artistic image synthesis,
proficient in both text-to-image synthesis and style transfer. DreamStyler
optimizes a multi-stage textual embedding with a context-aware text prompt,
resulting in prominent image quality. In addition, with content and style
guidance, DreamStyler exhibits flexibility to accommodate a range of style
references. Experimental results demonstrate its superior performance across
multiple scenarios, suggesting its promising potential in artistic product
creation.
</p></li>
</ul>

<h3>Title: Reasoning with Latent Diffusion in Offline Reinforcement Learning. (arXiv:2309.06599v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.06599">http://arxiv.org/abs/2309.06599</a></li>
<li>Code URL: https://github.com/ldcq/ldcq</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.06599]] Reasoning with Latent Diffusion in Offline Reinforcement Learning(http://arxiv.org/abs/2309.06599)</code></li>
<li>Summary: <p>Offline reinforcement learning (RL) holds promise as a means to learn
high-reward policies from a static dataset, without the need for further
environment interactions. However, a key challenge in offline RL lies in
effectively stitching portions of suboptimal trajectories from the static
dataset while avoiding extrapolation errors arising due to a lack of support in
the dataset. Existing approaches use conservative methods that are tricky to
tune and struggle with multi-modal data (as we show) or rely on noisy Monte
Carlo return-to-go samples for reward conditioning. In this work, we propose a
novel approach that leverages the expressiveness of latent diffusion to model
in-support trajectory sequences as compressed latent skills. This facilitates
learning a Q-function while avoiding extrapolation error via
batch-constraining. The latent space is also expressive and gracefully copes
with multi-modal data. We show that the learned temporally-abstract latent
space encodes richer task-specific information for offline RL tasks as compared
to raw state-actions. This improves credit assignment and facilitates faster
reward propagation during Q-learning. Our method demonstrates state-of-the-art
performance on the D4RL benchmarks, particularly excelling in long-horizon,
sparse-reward tasks.
</p></li>
</ul>

<h2>self-supervised</h2>
<h3>Title: GelFlow: Self-supervised Learning of Optical Flow for Vision-Based Tactile Sensor Displacement Measurement. (arXiv:2309.06735v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.06735">http://arxiv.org/abs/2309.06735</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.06735]] GelFlow: Self-supervised Learning of Optical Flow for Vision-Based Tactile Sensor Displacement Measurement(http://arxiv.org/abs/2309.06735)</code></li>
<li>Summary: <p>High-resolution multi-modality information acquired by vision-based tactile
sensors can support more dexterous manipulations for robot fingers. Optical
flow is low-level information directly obtained by vision-based tactile
sensors, which can be transformed into other modalities like force, geometry
and depth. Current vision-tactile sensors employ optical flow methods from
OpenCV to estimate the deformation of markers in gels. However, these methods
need to be more precise for accurately measuring the displacement of markers
during large elastic deformation of the gel, as this can significantly impact
the accuracy of downstream tasks. This study proposes a self-supervised optical
flow method based on deep learning to achieve high accuracy in displacement
measurement for vision-based tactile sensors. The proposed method employs a
coarse-to-fine strategy to handle large deformations by constructing a
multi-scale feature pyramid from the input image. To better deal with the
elastic deformation caused by the gel, the Helmholtz velocity decomposition
constraint combined with the elastic deformation constraint are adopted to
address the distortion rate and area change rate, respectively. A local flow
fusion module is designed to smooth the optical flow, taking into account the
prior knowledge of the blurred effect of gel deformation. We trained the
proposed self-supervised network using an open-source dataset and compared it
with traditional and deep learning-based optical flow methods. The results show
that the proposed method achieved the highest displacement measurement
accuracy, thereby demonstrating its potential for enabling more precise
measurement of downstream tasks using vision-based tactile sensors.
</p></li>
</ul>

<h3>Title: Keep It SimPool: Who Said Supervised Transformers Suffer from Attention Deficit?. (arXiv:2309.06891v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.06891">http://arxiv.org/abs/2309.06891</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.06891]] Keep It SimPool: Who Said Supervised Transformers Suffer from Attention Deficit?(http://arxiv.org/abs/2309.06891)</code></li>
<li>Summary: <p>Convolutional networks and vision transformers have different forms of
pairwise interactions, pooling across layers and pooling at the end of the
network. Does the latter really need to be different? As a by-product of
pooling, vision transformers provide spatial attention for free, but this is
most often of low quality unless self-supervised, which is not well studied. Is
supervision really the problem?
</p>
<p>In this work, we develop a generic pooling framework and then we formulate a
number of existing methods as instantiations. By discussing the properties of
each group of methods, we derive SimPool, a simple attention-based pooling
mechanism as a replacement of the default one for both convolutional and
transformer encoders. We find that, whether supervised or self-supervised, this
improves performance on pre-training and downstream tasks and provides
attention maps delineating object boundaries in all cases. One could thus call
SimPool universal. To our knowledge, we are the first to obtain attention maps
in supervised transformers of at least as good quality as self-supervised,
without explicit losses or modifying the architecture. Code at:
https://github.com/billpsomas/simpool.
</p></li>
</ul>

<h3>Title: Exploiting Multiple Priors for Neural 3D Indoor Reconstruction. (arXiv:2309.07021v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07021">http://arxiv.org/abs/2309.07021</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07021]] Exploiting Multiple Priors for Neural 3D Indoor Reconstruction(http://arxiv.org/abs/2309.07021)</code></li>
<li>Summary: <p>Neural implicit modeling permits to achieve impressive 3D reconstruction
results on small objects, while it exhibits significant limitations in large
indoor scenes. In this work, we propose a novel neural implicit modeling method
that leverages multiple regularization strategies to achieve better
reconstructions of large indoor environments, while relying only on images. A
sparse but accurate depth prior is used to anchor the scene to the initial
model. A dense but less accurate depth prior is also introduced, flexible
enough to still let the model diverge from it to improve the estimated
geometry. Then, a novel self-supervised strategy to regularize the estimated
surface normals is presented. Finally, a learnable exposure compensation scheme
permits to cope with challenging lighting conditions. Experimental results show
that our approach produces state-of-the-art 3D reconstructions in challenging
indoor scenarios.
</p></li>
</ul>

<h3>Title: Domain-Aware Augmentations for Unsupervised Online General Continual Learning. (arXiv:2309.06896v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.06896">http://arxiv.org/abs/2309.06896</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.06896]] Domain-Aware Augmentations for Unsupervised Online General Continual Learning(http://arxiv.org/abs/2309.06896)</code></li>
<li>Summary: <p>Continual Learning has been challenging, especially when dealing with
unsupervised scenarios such as Unsupervised Online General Continual Learning
(UOGCL), where the learning agent has no prior knowledge of class boundaries or
task change information. While previous research has focused on reducing
forgetting in supervised setups, recent studies have shown that self-supervised
learners are more resilient to forgetting. This paper proposes a novel approach
that enhances memory usage for contrastive learning in UOGCL by defining and
using stream-dependent data augmentations together with some implementation
tricks. Our proposed method is simple yet effective, achieves state-of-the-art
results compared to other unsupervised approaches in all considered setups, and
reduces the gap between supervised and unsupervised continual learning. Our
domain-aware augmentation procedure can be adapted to other replay-based
methods, making it a promising strategy for continual learning.
</p></li>
</ul>

<h2>foundation model</h2>
<h3>Title: Leveraging Foundation models for Unsupervised Audio-Visual Segmentation. (arXiv:2309.06728v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.06728">http://arxiv.org/abs/2309.06728</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.06728]] Leveraging Foundation models for Unsupervised Audio-Visual Segmentation(http://arxiv.org/abs/2309.06728)</code></li>
<li>Summary: <p>Audio-Visual Segmentation (AVS) aims to precisely outline audible objects in
a visual scene at the pixel level. Existing AVS methods require fine-grained
annotations of audio-mask pairs in supervised learning fashion. This limits
their scalability since it is time consuming and tedious to acquire such
cross-modality pixel level labels. To overcome this obstacle, in this work we
introduce unsupervised audio-visual segmentation with no need for task-specific
data annotations and model training. For tackling this newly proposed problem,
we formulate a novel Cross-Modality Semantic Filtering (CMSF) approach to
accurately associate the underlying audio-mask pairs by leveraging the
off-the-shelf multi-modal foundation models (e.g., detection [1], open-world
segmentation [2] and multi-modal alignment [3]). Guiding the proposal
generation by either audio or visual cues, we design two training-free
variants: AT-GDINO-SAM and OWOD-BIND. Extensive experiments on the AVS-Bench
dataset show that our unsupervised approach can perform well in comparison to
prior art supervised counterparts across complex scenarios with multiple
auditory objects. Particularly, in situations where existing supervised AVS
methods struggle with overlapping foreground objects, our models still excel in
accurately segmenting overlapped auditory objects. Our code will be publicly
released.
</p></li>
</ul>

<h3>Title: SAMUS: Adapting Segment Anything Model for Clinically-Friendly and Generalizable Ultrasound Image Segmentation. (arXiv:2309.06824v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.06824">http://arxiv.org/abs/2309.06824</a></li>
<li>Code URL: https://github.com/xianlin7/samus</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.06824]] SAMUS: Adapting Segment Anything Model for Clinically-Friendly and Generalizable Ultrasound Image Segmentation(http://arxiv.org/abs/2309.06824)</code></li>
<li>Summary: <p>Segment anything model (SAM), an eminent universal image segmentation model,
has recently gathered considerable attention within the domain of medical image
segmentation. Despite the remarkable performance of SAM on natural images, it
grapples with significant performance degradation and limited generalization
when confronted with medical images, particularly with those involving objects
of low contrast, faint boundaries, intricate shapes, and diminutive sizes. In
this paper, we propose SAMUS, a universal model tailored for ultrasound image
segmentation. In contrast to previous SAM-based universal models, SAMUS pursues
not only better generalization but also lower deployment cost, rendering it
more suitable for clinical applications. Specifically, based on SAM, a parallel
CNN branch is introduced to inject local features into the ViT encoder through
cross-branch attention for better medical image segmentation. Then, a position
adapter and a feature adapter are developed to adapt SAM from natural to
medical domains and from requiring large-size inputs (1024x1024) to small-size
inputs (256x256) for more clinical-friendly deployment. A comprehensive
ultrasound dataset, comprising about 30k images and 69k masks and covering six
object categories, is collected for verification. Extensive comparison
experiments demonstrate SAMUS's superiority against the state-of-the-art
task-specific models and universal foundation models under both task-specific
evaluation and generalization evaluation. Moreover, SAMUS is deployable on
entry-level GPUs, as it has been liberated from the constraints of long
sequence encoding. The code, data, and models will be released at
https://github.com/xianlin7/SAMUS.
</p></li>
</ul>

<h3>Title: Hydra: Multi-head Low-rank Adaptation for Parameter Efficient Fine-tuning. (arXiv:2309.06922v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.06922">http://arxiv.org/abs/2309.06922</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.06922]] Hydra: Multi-head Low-rank Adaptation for Parameter Efficient Fine-tuning(http://arxiv.org/abs/2309.06922)</code></li>
<li>Summary: <p>The recent surge in large-scale foundation models has spurred the development
of efficient methods for adapting these models to various downstream tasks.
Low-rank adaptation methods, such as LoRA, have gained significant attention
due to their outstanding parameter efficiency and no additional inference
latency. This paper investigates a more general form of adapter module based on
the analysis that parallel and sequential adaptation branches learn novel and
general features during fine-tuning, respectively. The proposed method, named
Hydra, due to its multi-head computational branches, combines parallel and
sequential branch to integrate capabilities, which is more expressive than
existing single branch methods and enables the exploration of a broader range
of optimal points in the fine-tuning process. In addition, the proposed
adaptation method explicitly leverages the pre-trained weights by performing a
linear combination of the pre-trained features. It allows the learned features
to have better generalization performance across diverse downstream tasks.
Furthermore, we perform a comprehensive analysis of the characteristics of each
adaptation branch with empirical evidence. Through an extensive range of
experiments, encompassing comparisons and ablation studies, we substantiate the
efficiency and demonstrate the superior performance of Hydra. This
comprehensive evaluation underscores the potential impact and effectiveness of
Hydra in a variety of applications. Our code is available on
\url{https://github.com/extremebird/Hydra}
</p></li>
</ul>

<h2>generative</h2>
<h3>Title: Integrating GAN and Texture Synthesis for Enhanced Road Damage Detection. (arXiv:2309.06747v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.06747">http://arxiv.org/abs/2309.06747</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.06747]] Integrating GAN and Texture Synthesis for Enhanced Road Damage Detection(http://arxiv.org/abs/2309.06747)</code></li>
<li>Summary: <p>In the domain of traffic safety and road maintenance, precise detection of
road damage is crucial for ensuring safe driving and prolonging road
durability. However, current methods often fall short due to limited data.
Prior attempts have used Generative Adversarial Networks to generate damage
with diverse shapes and manually integrate it into appropriate positions.
However, the problem has not been well explored and is faced with two
challenges. First, they only enrich the location and shape of damage while
neglect the diversity of severity levels, and the realism still needs further
improvement. Second, they require a significant amount of manual effort. To
address these challenges, we propose an innovative approach. In addition to
using GAN to generate damage with various shapes, we further employ texture
synthesis techniques to extract road textures. These two elements are then
mixed with different weights, allowing us to control the severity of the
synthesized damage, which are then embedded back into the original images via
Poisson blending. Our method ensures both richness of damage severity and a
better alignment with the background. To save labor costs, we leverage
structural similarity for automated sample selection during embedding. Each
augmented data of an original image contains versions with varying severity
levels. We implement a straightforward screening strategy to mitigate
distribution drift. Experiments are conducted on a public road damage dataset.
The proposed method not only eliminates the need for manual labor but also
achieves remarkable enhancements, improving the mAP by 4.1% and the F1-score by
4.5%.
</p></li>
</ul>

<h3>Title: Instance Adaptive Prototypical Contrastive Embedding for Generalized Zero Shot Learning. (arXiv:2309.06987v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.06987">http://arxiv.org/abs/2309.06987</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.06987]] Instance Adaptive Prototypical Contrastive Embedding for Generalized Zero Shot Learning(http://arxiv.org/abs/2309.06987)</code></li>
<li>Summary: <p>Generalized zero-shot learning(GZSL) aims to classify samples from seen and
unseen labels, assuming unseen labels are not accessible during training.
Recent advancements in GZSL have been expedited by incorporating
contrastive-learning-based (instance-based) embedding in generative networks
and leveraging the semantic relationship between data points. However, existing
embedding architectures suffer from two limitations: (1) limited
discriminability of synthetic features' embedding without considering
fine-grained cluster structures; (2) inflexible optimization due to restricted
scaling mechanisms on existing contrastive embedding networks, leading to
overlapped representations in the embedding space. To enhance the quality of
representations in the embedding space, as mentioned in (1), we propose a
margin-based prototypical contrastive learning embedding network that reaps the
benefits of prototype-data (cluster quality enhancement) and implicit data-data
(fine-grained representations) interaction while providing substantial cluster
supervision to the embedding network and the generator. To tackle (2), we
propose an instance adaptive contrastive loss that leads to generalized
representations for unseen labels with increased inter-class margin. Through
comprehensive experimental evaluation, we show that our method can outperform
the current state-of-the-art on three benchmark datasets. Our approach also
consistently achieves the best unseen performance in the GZSL setting.
</p></li>
</ul>

<h3>Title: Text Encoders Lack Knowledge: Leveraging Generative LLMs for Domain-Specific Semantic Textual Similarity. (arXiv:2309.06541v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.06541">http://arxiv.org/abs/2309.06541</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.06541]] Text Encoders Lack Knowledge: Leveraging Generative LLMs for Domain-Specific Semantic Textual Similarity(http://arxiv.org/abs/2309.06541)</code></li>
<li>Summary: <p>Amidst the sharp rise in the evaluation of large language models (LLMs) on
various tasks, we find that semantic textual similarity (STS) has been
under-explored. In this study, we show that STS can be cast as a text
generation problem while maintaining strong performance on multiple STS
benchmarks. Additionally, we show generative LLMs significantly outperform
existing encoder-based STS models when characterizing the semantic similarity
between two texts with complex semantic relationships dependent on world
knowledge. We validate this claim by evaluating both generative LLMs and
existing encoder-based STS models on three newly collected STS challenge sets
which require world knowledge in the domains of Health, Politics, and Sports.
All newly collected data is sourced from social media content posted after May
2023 to ensure the performance of closed-source models like ChatGPT cannot be
credited to memorization. Our results show that, on average, generative LLMs
outperform the best encoder-only baselines by an average of 22.3% on STS tasks
requiring world knowledge. Our results suggest generative language models with
STS-specific prompting strategies achieve state-of-the-art performance in
complex, domain-specific STS tasks.
</p></li>
</ul>

<h3>Title: Do Generative Large Language Models need billions of parameters?. (arXiv:2309.06589v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.06589">http://arxiv.org/abs/2309.06589</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.06589]] Do Generative Large Language Models need billions of parameters?(http://arxiv.org/abs/2309.06589)</code></li>
<li>Summary: <p>This paper presents novel systems and methodologies for the development of
efficient large language models (LLMs). It explores the trade-offs between
model size, performance, and computational resources, with the aim of
maximizing the efficiency of these AI systems. The research explores novel
methods that allow different parts of the model to share parameters, reducing
the total number of unique parameters required. This approach ensures that the
model remains compact without sacrificing its ability to learn and represent
complex language structures. This study provides valuable insights and tools
for creating more efficient and effective LLMs, contributing to a more
sustainable and accessible future for AI language modeling.
</p></li>
</ul>

<h3>Title: Continual Learning with Dirichlet Generative-based Rehearsal. (arXiv:2309.06917v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.06917">http://arxiv.org/abs/2309.06917</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.06917]] Continual Learning with Dirichlet Generative-based Rehearsal(http://arxiv.org/abs/2309.06917)</code></li>
<li>Summary: <p>Recent advancements in data-driven task-oriented dialogue systems (ToDs)
struggle with incremental learning due to computational constraints and
time-consuming issues. Continual Learning (CL) attempts to solve this by
avoiding intensive pre-training, but it faces the problem of catastrophic
forgetting (CF). While generative-based rehearsal CL methods have made
significant strides, generating pseudo samples that accurately reflect the
underlying task-specific distribution is still a challenge. In this paper, we
present Dirichlet Continual Learning (DCL), a novel generative-based rehearsal
strategy for CL. Unlike the traditionally used Gaussian latent variable in the
Conditional Variational Autoencoder (CVAE), DCL leverages the flexibility and
versatility of the Dirichlet distribution to model the latent prior variable.
This enables it to efficiently capture sentence-level features of previous
tasks and effectively guide the generation of pseudo samples. In addition, we
introduce Jensen-Shannon Knowledge Distillation (JSKD), a robust logit-based
knowledge distillation method that enhances knowledge transfer during pseudo
sample generation. Our experiments confirm the efficacy of our approach in both
intent detection and slot-filling tasks, outperforming state-of-the-art
methods.
</p></li>
</ul>

<h2>anomaly</h2>
<h3>Title: Manufacturing Quality Control with Autoencoder-Based Defect Localization and Unsupervised Class Selection. (arXiv:2309.06884v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.06884">http://arxiv.org/abs/2309.06884</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.06884]] Manufacturing Quality Control with Autoencoder-Based Defect Localization and Unsupervised Class Selection(http://arxiv.org/abs/2309.06884)</code></li>
<li>Summary: <p>Manufacturing industries require efficient and voluminous production of
high-quality finished goods. In the context of Industry 4.0, visual anomaly
detection poses an optimistic solution for automatically controlling product
quality with high precision. Automation based on computer vision poses a
promising solution to prevent bottlenecks at the product quality checkpoint. We
considered recent advancements in machine learning to improve visual defect
localization, but challenges persist in obtaining a balanced feature set and
database of the wide variety of defects occurring in the production line. This
paper proposes a defect localizing autoencoder with unsupervised class
selection by clustering with k-means the features extracted from a pre-trained
VGG-16 network. The selected classes of defects are augmented with natural wild
textures to simulate artificial defects. The study demonstrates the
effectiveness of the defect localizing autoencoder with unsupervised class
selection for improving defect detection in manufacturing industries. The
proposed methodology shows promising results with precise and accurate
localization of quality defects on melamine-faced boards for the furniture
industry. Incorporating artificial defects into the training data shows
significant potential for practical implementation in real-world quality
control scenarios.
</p></li>
</ul>

<h3>Title: FAIR: Frequency-aware Image Restoration for Industrial Visual Anomaly Detection. (arXiv:2309.07068v1 [cs.CV])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.07068">http://arxiv.org/abs/2309.07068</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.07068]] FAIR: Frequency-aware Image Restoration for Industrial Visual Anomaly Detection(http://arxiv.org/abs/2309.07068)</code></li>
<li>Summary: <p>Image reconstruction-based anomaly detection models are widely explored in
industrial visual inspection. However, existing models usually suffer from the
trade-off between normal reconstruction fidelity and abnormal reconstruction
distinguishability, which damages the performance. In this paper, we find that
the above trade-off can be better mitigated by leveraging the distinct
frequency biases between normal and abnormal reconstruction errors. To this
end, we propose Frequency-aware Image Restoration (FAIR), a novel
self-supervised image restoration task that restores images from their
high-frequency components. It enables precise reconstruction of normal patterns
while mitigating unfavorable generalization to anomalies. Using only a simple
vanilla UNet, FAIR achieves state-of-the-art performance with higher efficiency
on various defect detection datasets. Code: https://github.com/liutongkun/FAIR.
</p></li>
</ul>

<h2>in-context</h2>
<h3>Title: Narrowing the Gap between Supervised and Unsupervised Sentence Representation Learning with Large Language Model. (arXiv:2309.06453v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.06453">http://arxiv.org/abs/2309.06453</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.06453]] Narrowing the Gap between Supervised and Unsupervised Sentence Representation Learning with Large Language Model(http://arxiv.org/abs/2309.06453)</code></li>
<li>Summary: <p>Sentence Representation Learning (SRL) is a fundamental task in Natural
Language Processing (NLP), with Contrastive learning of Sentence Embeddings
(CSE) as the mainstream technique due to its superior performance. An
intriguing phenomenon in CSE is the significant performance gap between
supervised and unsupervised methods, even when their sentence encoder and loss
function are the same. Previous works attribute this performance gap to
differences in two representation properties (alignment and uniformity).
However, alignment and uniformity only measure the results, which means they
cannot answer "What happens during the training process that leads to the
performance gap?" and "How can the performance gap be narrowed?". In this
paper, we conduct empirical experiments to answer these "What" and "How"
questions. We first answer the "What" question by thoroughly comparing the
behavior of supervised and unsupervised CSE during their respective training
processes. From the comparison, We observe a significant difference in fitting
difficulty. Thus, we introduce a metric, called Fitting Difficulty Increment
(FDI), to measure the fitting difficulty gap between the evaluation dataset and
the held-out training dataset, and use the metric to answer the "What"
question. Then, based on the insights gained from the "What" question, we
tackle the "How" question by increasing the fitting difficulty of the training
dataset. We achieve this by leveraging the In-Context Learning (ICL) capability
of the Large Language Model (LLM) to generate data that simulates complex
patterns. By utilizing the hierarchical patterns in the LLM-generated data, we
effectively narrow the gap between supervised and unsupervised CSE.
</p></li>
</ul>

<h3>Title: CONVERSER: Few-Shot Conversational Dense Retrieval with Synthetic Data Generation. (arXiv:2309.06748v1 [cs.CL])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.06748">http://arxiv.org/abs/2309.06748</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.06748]] CONVERSER: Few-Shot Conversational Dense Retrieval with Synthetic Data Generation(http://arxiv.org/abs/2309.06748)</code></li>
<li>Summary: <p>Conversational search provides a natural interface for information retrieval
(IR). Recent approaches have demonstrated promising results in applying dense
retrieval to conversational IR. However, training dense retrievers requires
large amounts of in-domain paired data. This hinders the development of
conversational dense retrievers, as abundant in-domain conversations are
expensive to collect. In this paper, we propose CONVERSER, a framework for
training conversational dense retrievers with at most 6 examples of in-domain
dialogues. Specifically, we utilize the in-context learning capability of large
language models to generate conversational queries given a passage in the
retrieval corpus. Experimental results on conversational retrieval benchmarks
OR-QuAC and TREC CAsT 19 show that the proposed CONVERSER achieves comparable
performance to fully-supervised models, demonstrating the effectiveness of our
proposed framework in few-shot conversational dense retrieval. All source code
and generated datasets are available at https://github.com/MiuLab/CONVERSER
</p></li>
</ul>

<h3>Title: Unsupervised Contrast-Consistent Ranking with Language Models. (arXiv:2309.06991v1 [cs.LG])</h3>
<ul>
<li>Paper URL: <a href="http://arxiv.org/abs/2309.06991">http://arxiv.org/abs/2309.06991</a></li>
<li>Code URL: null</li>
<li>Copy Paste: <code><input type="checkbox">[[2309.06991]] Unsupervised Contrast-Consistent Ranking with Language Models(http://arxiv.org/abs/2309.06991)</code></li>
<li>Summary: <p>Language models contain ranking-based knowledge and are powerful solvers of
in-context ranking tasks. For instance, they may have parametric knowledge
about the ordering of countries by size or may be able to rank reviews by
sentiment. Recent work focuses on pairwise, pointwise, and listwise prompting
techniques to elicit a language model's ranking knowledge. However, we find
that even with careful calibration and constrained decoding, prompting-based
techniques may not always be self-consistent in the rankings they produce. This
motivates us to explore an alternative approach that is inspired by an
unsupervised probing method called Contrast-Consistent Search (CCS). The idea
is to train a probing model guided by a logical constraint: a model's
representation of a statement and its negation must be mapped to contrastive
true-false poles consistently across multiple statements. We hypothesize that
similar constraints apply to ranking tasks where all items are related via
consistent pairwise or listwise comparisons. To this end, we extend the binary
CCS method to Contrast-Consistent Ranking (CCR) by adapting existing ranking
methods such as the Max-Margin Loss, Triplet Loss, and Ordinal Regression
objective. Our results confirm that, for the same language model, CCR probing
outperforms prompting and even performs on a par with prompting much larger
language models.
</p></li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="https://cdn.staticfile.org/clipboard.js/2.0.4/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
