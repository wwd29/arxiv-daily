<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2026-02-18</h1>
<h3>Title: Near-Optimal Sample Complexity for Online Constrained MDPs</h3>
<ul>
<li><strong>Authors: </strong>Chang Liu, Yunfan Li, Lin F. Yang</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15076">https://arxiv.org/abs/2602.15076</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15076">https://arxiv.org/pdf/2602.15076</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15076]] Near-Optimal Sample Complexity for Online Constrained MDPs(https://arxiv.org/abs/2602.15076)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Safety is a fundamental challenge in reinforcement learning (RL), particularly in real-world applications such as autonomous driving, robotics, and healthcare. To address this, Constrained Markov Decision Processes (CMDPs) are commonly used to enforce safety constraints while optimizing performance. However, existing methods often suffer from significant safety violations or require a high sample complexity to generate near-optimal policies. We address two settings: relaxed feasibility, where small violations are allowed, and strict feasibility, where no violation is allowed. We propose a model-based primal-dual algorithm that balances regret and bounded constraint violations, drawing on techniques from online RL and constrained optimization. For relaxed feasibility, we prove that our algorithm returns an $\varepsilon$-optimal policy with $\varepsilon$-bounded violation with arbitrarily high probability, requiring $\tilde{O}\left(\frac{SAH^3}{\varepsilon^2}\right)$ learning episodes, matching the lower bound for unconstrained MDPs. For strict feasibility, we prove that our algorithm returns an $\varepsilon$-optimal policy with zero violation with arbitrarily high probability, requiring $\tilde{O}\left(\frac{SAH^5}{\varepsilon^2\zeta^2}\right)$ learning episodes, where $\zeta$ is the problem-dependent Slater constant characterizing the size of the feasible region. This result matches the lower bound for learning CMDPs with access to a generative model. Our results demonstrate that learning CMDPs in an online setting is as easy as learning with a generative model and is no more challenging than learning unconstrained MDPs when small violations are allowed.</li>
</ul>

<h3>Title: Hybrid Feature Learning with Time Series Embeddings for Equipment Anomaly Prediction</h3>
<ul>
<li><strong>Authors: </strong>Takato Yasuno</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15089">https://arxiv.org/abs/2602.15089</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15089">https://arxiv.org/pdf/2602.15089</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15089]] Hybrid Feature Learning with Time Series Embeddings for Equipment Anomaly Prediction(https://arxiv.org/abs/2602.15089)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>In predictive maintenance of equipment, deep learning-based time series anomaly detection has garnered significant attention; however, pure deep learning approaches often fail to achieve sufficient accuracy on real-world data. This study proposes a hybrid approach that integrates 64-dimensional time series embeddings from Granite TinyTimeMixer with 28-dimensional statistical features based on domain knowledge for HVAC equipment anomaly prediction tasks. Specifically, we combine time series embeddings extracted from a Granite TinyTimeMixer encoder fine-tuned with LoRA (Low-Rank Adaptation) and 28 types of statistical features including trend, volatility, and drawdown indicators, which are then learned using a LightGBM gradient boosting classifier. In experiments using 64 equipment units and 51,564 samples, we achieved Precision of 91--95\% and ROC-AUC of 0.995 for anomaly prediction at 30-day, 60-day, and 90-day horizons. Furthermore, we achieved production-ready performance with a false positive rate of 1.1\% or less and a detection rate of 88--94\%, demonstrating the effectiveness of the system for predictive maintenance applications. This work demonstrates that practical anomaly detection systems can be realized by leveraging the complementary strengths between deep learning's representation learning capabilities and statistical feature engineering.</li>
</ul>

<h3>Title: Learning Representations from Incomplete EHR Data with Dual-Masked Autoencoding</h3>
<ul>
<li><strong>Authors: </strong>Xiao Xiang, David Restrepo, Hyewon Jeong, Yugang Jia, Leo Anthony Celi</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15159">https://arxiv.org/abs/2602.15159</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15159">https://arxiv.org/pdf/2602.15159</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15159]] Learning Representations from Incomplete EHR Data with Dual-Masked Autoencoding(https://arxiv.org/abs/2602.15159)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Learning from electronic health records (EHRs) time series is challenging due to irregular sam- pling, heterogeneous missingness, and the resulting sparsity of observations. Prior self-supervised meth- ods either impute before learning, represent missingness through a dedicated input signal, or optimize solely for imputation, reducing their capacity to efficiently learn representations that support clinical downstream tasks. We propose the Augmented-Intrinsic Dual-Masked Autoencoder (AID-MAE), which learns directly from incomplete time series by applying an intrinsic missing mask to represent naturally missing values and an augmented mask that hides a subset of observed values for reconstruction during training. AID-MAE processes only the unmasked subset of tokens and consistently outperforms strong baselines, including XGBoost and DuETT, across multiple clinical tasks on two datasets. In addition, the learned embeddings naturally stratify patient cohorts in the representation space.</li>
</ul>

<h3>Title: ÜberWeb: Insights from Multilingual Curation for a 20-Trillion-Token Dataset</h3>
<ul>
<li><strong>Authors: </strong>DatologyAI: Aldo Gael Carranza, Kaleigh Mentzer, Ricardo Pio Monti, Alex Fang, Alvin Deng, Amro Abbas, Anshuman Suri, Brett Larsen, Cody Blakeney, Darren Teh, David Schwab, Diego Kiner, Fan Pan, Haakon Mongstad, Jack Urbanek, Jason Lee, Jason Telanoff, Josh Wills, Luke Merrick, Parth Doshi, Paul Burstein, Pratyush Maini, Spandan Das, Tony Jiang, Vineeth Dorna, Zhengping Wang, Bogdan Gaza, Ari Morcos, Matthew Leavitt</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15210">https://arxiv.org/abs/2602.15210</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15210">https://arxiv.org/pdf/2602.15210</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15210]] ÜberWeb: Insights from Multilingual Curation for a 20-Trillion-Token Dataset(https://arxiv.org/abs/2602.15210)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Multilinguality is a core capability for modern foundation models, yet training high-quality multilingual models remains challenging due to uneven data availability across languages. A further challenge is the performance interference that can arise from joint multilingual training, commonly referred to as the "curse of multilinguality". We study multilingual data curation across thirteen languages and find that many reported regressions are not inherent to multilingual scaling but instead stem from correctable deficiencies in data quality and composition rather than fundamental capacity limits. In controlled bilingual experiments, improving data quality for any single language benefits others: curating English improves non-English performance in 12 of 13 languages, while curating non-English yields reciprocal improvements in English. Bespoke per-language curation produces substantially larger within-language improvements. Extending these findings to large-scale general-purpose training mixtures, we show that curated multilingual allocations comprising under 8% of total tokens remain remarkably effective. We operationalize this approach within an effort that produced a 20T-token pretraining corpus derived entirely from public sources. Models with 3B and 8B parameters trained on a 1T-token random subset achieve competitive multilingual accuracy with 4-10x fewer training FLOPs than strong public baselines, establishing a new Pareto frontier in multilingual performance versus compute. Moreover, these benefits extend to frontier model scale: the 20T-token corpus served as part of the pretraining dataset for Trinity Large (400B/A13B), which exhibits strong multilingual performance relative to its training FLOPs. These results show that targeted, per-language data curation mitigates multilingual interference and enables compute-efficient multilingual scaling.</li>
</ul>

<h3>Title: BindCLIP: A Unified Contrastive-Generative Representation Learning Framework for Virtual Screening</h3>
<ul>
<li><strong>Authors: </strong>Anjie Qiao, Zhen Wang, Yaliang Li, Jiahua Rao, Yuedong Yang</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15236">https://arxiv.org/abs/2602.15236</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15236">https://arxiv.org/pdf/2602.15236</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15236]] BindCLIP: A Unified Contrastive-Generative Representation Learning Framework for Virtual Screening(https://arxiv.org/abs/2602.15236)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Virtual screening aims to efficiently identify active ligands from massive chemical libraries for a given target pocket. Recent CLIP-style models such as DrugCLIP enable scalable virtual screening by embedding pockets and ligands into a shared space. However, our analyses indicate that such representations can be insensitive to fine-grained binding interactions and may rely on shortcut correlations in training data, limiting their ability to rank ligands by true binding compatibility. To address these issues, we propose BindCLIP, a unified contrastive-generative representation learning framework for virtual screening. BindCLIP jointly trains pocket and ligand encoders using CLIP-style contrastive learning together with a pocket-conditioned diffusion objective for binding pose generation, so that pose-level supervision directly shapes the retrieval embedding space toward interaction-relevant features. To further mitigate shortcut reliance, we introduce hard-negative augmentation and a ligand-ligand anchoring regularizer that prevents representation collapse. Experiments on two public benchmarks demonstrate consistent improvements over strong baselines. BindCLIP achieves substantial gains on challenging out-of-distribution virtual screening and improves ligand-analogue ranking on the FEP+ benchmark. Together, these results indicate that integrating generative, pose-level supervision with contrastive learning yields more interaction-aware embeddings and improves generalization in realistic screening settings, bringing virtual screening closer to real-world applicability.</li>
</ul>

<h3>Title: Closing the Distribution Gap in Adversarial Training for LLMs</h3>
<ul>
<li><strong>Authors: </strong>Chengzhi Hu, Jonas Dornbusch, David Lüdke, Stephan Günnemann, Leo Schwinn</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15238">https://arxiv.org/abs/2602.15238</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15238">https://arxiv.org/pdf/2602.15238</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15238]] Closing the Distribution Gap in Adversarial Training for LLMs(https://arxiv.org/abs/2602.15238)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Adversarial training for LLMs is one of the most promising methods to reliably improve robustness against adversaries. However, despite significant progress, models remain vulnerable to simple in-distribution exploits, such as rewriting prompts in the past tense or translating them into other languages. We argue that this persistent fragility stems from a fundamental limitation in current adversarial training algorithms: they minimize adversarial loss on their training set but inadequately cover the data distribution, resulting in vulnerability to seemingly simple attacks. To bridge this gap, we propose Distributional Adversarial Training, DAT. We leverage Diffusion LLMs to approximate the true joint distribution of prompts and responses, enabling generation of diverse, high-likelihood samples that address generalization failures. By combining optimization over the data distribution provided by the diffusion model with continuous adversarial training, DAT achieves substantially higher adversarial robustness than previous methods.</li>
</ul>

<h3>Title: Scaling Laws for Masked-Reconstruction Transformers on Single-Cell Transcriptomics</h3>
<ul>
<li><strong>Authors: </strong>Ihor Kendiukhov</a></li>
<li><strong>Subjects: </strong>cs.LG, q-bio.GN</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15253">https://arxiv.org/abs/2602.15253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15253">https://arxiv.org/pdf/2602.15253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15253]] Scaling Laws for Masked-Reconstruction Transformers on Single-Cell Transcriptomics(https://arxiv.org/abs/2602.15253)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Neural scaling laws -- power-law relationships between loss, model size, and data -- have been extensively documented for language and vision transformers, yet their existence in single-cell genomics remains largely unexplored. We present the first systematic study of scaling behaviour for masked-reconstruction transformers trained on single-cell RNA sequencing (scRNA-seq) data. Using expression profiles from the CELLxGENE Census, we construct two experimental regimes: a data-rich regime (512 highly variable genes, 200,000 cells) and a data-limited regime (1,024 genes, 10,000 cells). Across seven model sizes spanning three orders of magnitude in parameter count (533 to 3.4 x 10^8 parameters), we fit the parametric scaling law to validation mean squared error (MSE). The data-rich regime exhibits clear power-law scaling with an irreducible loss floor of c ~ 1.44, while the data-limited regime shows negligible scaling, indicating that model capacity is not the binding constraint when data are scarce. These results establish that scaling laws analogous to those observed in natural language processing do emerge in single-cell transcriptomics when sufficient data are available, and they identify the data-to-parameter ratio as a critical determinant of scaling behaviour. A preliminary conversion of the data-rich asymptotic floor to information-theoretic units yields an estimate of approximately 2.30 bits of entropy per masked gene position. We discuss implications for the design of single-cell foundation models and outline the additional measurements needed to refine this entropy estimate.</li>
</ul>

<h3>Title: Training-Free Zero-Shot Anomaly Detection in 3D Brain MRI with 2D Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Tai Le-Gia, Jaehyun Ahn</a></li>
<li><strong>Subjects: </strong>cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15315">https://arxiv.org/abs/2602.15315</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15315">https://arxiv.org/pdf/2602.15315</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15315]] Training-Free Zero-Shot Anomaly Detection in 3D Brain MRI with 2D Foundation Models(https://arxiv.org/abs/2602.15315)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, anomaly</a></li>
<li><strong>Abstract: </strong>Zero-shot anomaly detection (ZSAD) has gained increasing attention in medical imaging as a way to identify abnormalities without task-specific supervision, but most advances remain limited to 2D datasets. Extending ZSAD to 3D medical images has proven challenging, with existing methods relying on slice-wise features and vision-language models, which fail to capture volumetric structure. In this paper, we introduce a fully training-free framework for ZSAD in 3D brain MRI that constructs localized volumetric tokens by aggregating multi-axis slices processed by 2D foundation models. These 3D patch tokens restore cubic spatial context and integrate directly with distance-based, batch-level anomaly detection pipelines. The framework provides compact 3D representations that are practical to compute on standard GPUs and require no fine-tuning, prompts, or supervision. Our results show that training-free, batch-based ZSAD can be effectively extended from 2D encoders to full 3D MRI volumes, offering a simple and robust approach for volumetric anomaly detection.</li>
</ul>

<h3>Title: Prescriptive Scaling Reveals the Evolution of Language Model Capabilities</h3>
<ul>
<li><strong>Authors: </strong>Hanlin Zhang, Jikai Jin, Vasilis Syrgkanis, Sham Kakade</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15327">https://arxiv.org/abs/2602.15327</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15327">https://arxiv.org/pdf/2602.15327</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15327]] Prescriptive Scaling Reveals the Evolution of Language Model Capabilities(https://arxiv.org/abs/2602.15327)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>For deploying foundation models, practitioners increasingly need prescriptive scaling laws: given a pre training compute budget, what downstream accuracy is attainable with contemporary post training practice, and how stable is that mapping as the field evolves? Using large scale observational evaluations with 5k observational and 2k newly sampled data on model performance, we estimate capability boundaries, high conditional quantiles of benchmark scores as a function of log pre training FLOPs, via smoothed quantile regression with a monotone, saturating sigmoid parameterization. We validate the temporal reliability by fitting on earlier model generations and evaluating on later releases. Across various tasks, the estimated boundaries are mostly stable, with the exception of math reasoning that exhibits a consistently advancing boundary over time. We then extend our approach to analyze task dependent saturation and to probe contamination related shifts on math reasoning tasks. Finally, we introduce an efficient algorithm that recovers near full data frontiers using roughly 20% of evaluation budget. Together, our work releases the Proteus 2k, the latest model performance evaluation dataset, and introduces a practical methodology for translating compute budgets into reliable performance expectations and for monitoring when capability boundaries shift across time.</li>
</ul>

<h3>Title: DAV-GSWT: Diffusion-Active-View Sampling for Data-Efficient Gaussian Splatting Wang Tiles</h3>
<ul>
<li><strong>Authors: </strong>Rong Fu, Jiekai Wu, Haiyun Wei, Yee Tan Jia, Wenxin Zhang, Yang Li, Xiaowen Ma, Wangyu Wu, Simon Fong</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15355">https://arxiv.org/abs/2602.15355</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15355">https://arxiv.org/pdf/2602.15355</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15355]] DAV-GSWT: Diffusion-Active-View Sampling for Data-Efficient Gaussian Splatting Wang Tiles(https://arxiv.org/abs/2602.15355)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The emergence of 3D Gaussian Splatting has fundamentally redefined the capabilities of photorealistic neural rendering by enabling high-throughput synthesis of complex environments. While procedural methods like Wang Tiles have recently been integrated to facilitate the generation of expansive landscapes, these systems typically remain constrained by a reliance on densely sampled exemplar reconstructions. We present DAV-GSWT, a data-efficient framework that leverages diffusion priors and active view sampling to synthesize high-fidelity Gaussian Splatting Wang Tiles from minimal input observations. By integrating a hierarchical uncertainty quantification mechanism with generative diffusion models, our approach autonomously identifies the most informative viewpoints while hallucinating missing structural details to ensure seamless tile transitions. Experimental results indicate that our system significantly reduces the required data volume while maintaining the visual integrity and interactive performance necessary for large-scale virtual environments.</li>
</ul>

<h3>Title: GMAIL: Generative Modality Alignment for generated Image Learning</h3>
<ul>
<li><strong>Authors: </strong>Shentong Mo, Sukmin Yun</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15368">https://arxiv.org/abs/2602.15368</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15368">https://arxiv.org/pdf/2602.15368</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15368]] GMAIL: Generative Modality Alignment for generated Image Learning(https://arxiv.org/abs/2602.15368)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative models have made it possible to synthesize highly realistic images, potentially providing an abundant data source for training machine learning models. Despite the advantages of these synthesizable data sources, the indiscriminate use of generated images as real images for training can even cause mode collapse due to modality discrepancies between real and synthetic domains. In this paper, we propose a novel framework for discriminative use of generated images, coined GMAIL, that explicitly treats generated images as a separate modality from real images. Instead of indiscriminately replacing real images with generated ones in the pixel space, our approach bridges the two distinct modalities in the same latent space through a multi-modal learning approach. To be specific, we first fine-tune a model exclusively on generated images using a cross-modality alignment loss and then employ this aligned model to further train various vision-language models with generated images. By aligning the two modalities, our approach effectively leverages the benefits of recent advances in generative models, thereby boosting the effectiveness of generated image learning across a range of vision-language tasks. Our framework can be easily incorporated with various vision-language models, and we demonstrate its efficacy throughout extensive experiments. For example, our framework significantly improves performance on image captioning, zero-shot image retrieval, zero-shot image classification, and long caption retrieval tasks. It also shows positive generated data scaling trends and notable enhancements in the captioning performance of the large multimodal model, LLaVA.</li>
</ul>

<h3>Title: Far Out: Evaluating Language Models on Slang in Australian and Indian English</h3>
<ul>
<li><strong>Authors: </strong>Deniz Kaya Dilsiz, Dipankar Srirag, Aditya Joshi</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15373">https://arxiv.org/abs/2602.15373</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15373">https://arxiv.org/pdf/2602.15373</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15373]] Far Out: Evaluating Language Models on Slang in Australian and Indian English(https://arxiv.org/abs/2602.15373)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Language models exhibit systematic performance gaps when processing text in non-standard language varieties, yet their ability to comprehend variety-specific slang remains underexplored for several languages. We present a comprehensive evaluation of slang awareness in Indian English (en-IN) and Australian English (en-AU) across seven state-of-the-art language models. We construct two complementary datasets: \textsc{web}, containing 377 web-sourced usage examples from Urban Dictionary, and \textsc{gen}, featuring 1,492 synthetically generated usages of these slang terms, across diverse scenarios. We assess language models on three tasks: target word prediction (TWP), guided target word prediction (TWP$^*$) and target word selection (TWS). Our results reveal four key findings: (1) Higher average model performance TWS versus TWP and TWP$^*$, with average accuracy score increasing from 0.03 to 0.49 respectively (2) Stronger average model performance on \textsc{web} versus \textsc{gen} datasets, with average similarity score increasing by 0.03 and 0.05 across TWP and TWP$^*$ tasks respectively (3) en-IN tasks outperform en-AU when averaged across all models and datasets, with TWS demonstrating the largest disparity, increasing average accuracy from 0.44 to 0.54. These findings underscore fundamental asymmetries between generative and discriminative competencies for variety-specific language, particularly in the context of slang expressions despite being in a technologically rich language such as English.</li>
</ul>

<h3>Title: Efficient Generative Modeling beyond Memoryless Diffusion via Adjoint Schrödinger Bridge Matching</h3>
<ul>
<li><strong>Authors: </strong>Jeongwoo Shin, Jinhwan Sul, Joonseok Lee, Jaewong Choi, Jaemoo Choi</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15396">https://arxiv.org/abs/2602.15396</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15396">https://arxiv.org/pdf/2602.15396</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15396]] Efficient Generative Modeling beyond Memoryless Diffusion via Adjoint Schrödinger Bridge Matching(https://arxiv.org/abs/2602.15396)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models often yield highly curved trajectories and noisy score targets due to an uninformative, memoryless forward process that induces independent data-noise coupling. We propose Adjoint Schrödinger Bridge Matching (ASBM), a generative modeling framework that recovers optimal trajectories in high dimensions via two stages. First, we view the Schrödinger Bridge (SB) forward dynamic as a coupling construction problem and learn it through a data-to-energy sampling perspective that transports data to an energy-defined prior. Then, we learn the backward generative dynamic with a simple matching loss supervised by the induced optimal coupling. By operating in a non-memoryless regime, ASBM produces significantly straighter and more efficient sampling paths. Compared to prior works, ASBM scales to high-dimensional data with notably improved stability and efficiency. Extensive experiments on image generation show that ASBM improves fidelity with fewer sampling steps. We further showcase the effectiveness of our optimal trajectory via distillation to a one-step generator.</li>
</ul>

<h3>Title: Joint Enhancement and Classification using Coupled Diffusion Models of Signals and Logits</h3>
<ul>
<li><strong>Authors: </strong>Gilad Nurko, Roi Benita, Yehoshua Dissen, Tomohiro Nakatani, Marc Delcroix, Shoko Araki, Joseph Keshet</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15405">https://arxiv.org/abs/2602.15405</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15405">https://arxiv.org/pdf/2602.15405</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15405]] Joint Enhancement and Classification using Coupled Diffusion Models of Signals and Logits(https://arxiv.org/abs/2602.15405)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Robust classification in noisy environments remains a fundamental challenge in machine learning. Standard approaches typically treat signal enhancement and classification as separate, sequential stages: first enhancing the signal and then applying a classifier. This approach fails to leverage the semantic information in the classifier's output during denoising. In this work, we propose a general, domain-agnostic framework that integrates two interacting diffusion models: one operating on the input signal and the other on the classifier's output logits, without requiring any retraining or fine-tuning of the classifier. This coupled formulation enables mutual guidance, where the enhancing signal refines the class estimation and, conversely, the evolving class logits guide the signal reconstruction towards discriminative regions of the manifold. We introduce three strategies to effectively model the joint distribution of the input and the logit. We evaluated our joint enhancement method for image classification and automatic speech recognition. The proposed framework surpasses traditional sequential enhancement baselines, delivering robust and flexible improvements in classification accuracy under diverse noise conditions.</li>
</ul>

<h3>Title: Benchmarking IoT Time-Series AD with Event-Level Augmentations</h3>
<ul>
<li><strong>Authors: </strong>Dmitry Zhevnenko, Ilya Makarov, Aleksandr Kovalenko, Fedor Meshchaninov, Anton Kozhukhov, Vladislav Travnikov, Makar Ippolitov, Kirill Yashunin, Iurii Katser</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15457">https://arxiv.org/abs/2602.15457</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15457">https://arxiv.org/pdf/2602.15457</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15457]] Benchmarking IoT Time-Series AD with Event-Level Augmentations(https://arxiv.org/abs/2602.15457)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Anomaly detection (AD) for safety-critical IoT time series should be judged at the event level: reliability and earliness under realistic perturbations. Yet many studies still emphasize point-level results on curated base datasets, limiting value for model selection in practice. We introduce an evaluation protocol with unified event-level augmentations that simulate real-world issues: calibrated sensor dropout, linear and log drift, additive noise, and window shifts. We also perform sensor-level probing via mask-as-missing zeroing with per-channel influence estimation to support root-cause analysis. We evaluate 14 representative models on five public anomaly datasets (SWaT, WADI, SMD, SKAB, TEP) and two industrial datasets (steam turbine, nuclear turbogenerator) using unified splits and event aggregation. There is no universal winner: graph-structured models transfer best under dropout and long events (e.g., on SWaT under additive noise F1 drops 0.804->0.677 for a graph autoencoder, 0.759->0.680 for a graph-attention variant, and 0.762->0.756 for a hybrid graph attention model); density/flow models work well on clean stationary plants but can be fragile to monotone drift; spectral CNNs lead when periodicity is strong; reconstruction autoencoders become competitive after basic sensor vetting; predictive/hybrid dynamics help when faults break temporal dependencies but remain window-sensitive. The protocol also informs design choices: on SWaT under log drift, replacing normalizing flows with Gaussian density reduces high-stress F1 from ~0.75 to ~0.57, and fixing a learned DAG gives a small clean-set gain (~0.5-1.0 points) but increases drift sensitivity by ~8x.</li>
</ul>

<h3>Title: Approximation Theory for Lipschitz Continuous Transformers</h3>
<ul>
<li><strong>Authors: </strong>Takashi Furuya, Davide Murari, Carola-Bibiane Schönlieb</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15503">https://arxiv.org/abs/2602.15503</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15503">https://arxiv.org/pdf/2602.15503</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15503]] Approximation Theory for Lipschitz Continuous Transformers(https://arxiv.org/abs/2602.15503)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Stability and robustness are critical for deploying Transformers in safety-sensitive settings. A principled way to enforce such behavior is to constrain the model's Lipschitz constant. However, approximation-theoretic guarantees for architectures that explicitly preserve Lipschitz continuity have yet to be established. In this work, we bridge this gap by introducing a class of gradient-descent-type in-context Transformers that are Lipschitz-continuous by construction. We realize both MLP and attention blocks as explicit Euler steps of negative gradient flows, ensuring inherent stability without sacrificing expressivity. We prove a universal approximation theorem for this class within a Lipschitz-constrained function space. Crucially, our analysis adopts a measure-theoretic formalism, interpreting Transformers as operators on probability measures, to yield approximation guarantees independent of token count. These results provide a rigorous theoretical foundation for the design of robust, Lipschitz continuous Transformer architectures.</li>
</ul>

<h3>Title: ZeroSyl: Simple Zero-Resource Syllable Tokenization for Spoken Language Modeling</h3>
<ul>
<li><strong>Authors: </strong>Nicol Visser, Simon Malan, Danel Slabbert, Herman Kamper</a></li>
<li><strong>Subjects: </strong>cs.CL, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15537">https://arxiv.org/abs/2602.15537</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15537">https://arxiv.org/pdf/2602.15537</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15537]] ZeroSyl: Simple Zero-Resource Syllable Tokenization for Spoken Language Modeling(https://arxiv.org/abs/2602.15537)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Pure speech language models aim to learn language directly from raw audio without textual resources. A key challenge is that discrete tokens from self-supervised speech encoders result in excessively long sequences, motivating recent work on syllable-like units. However, methods like Sylber and SyllableLM rely on intricate multi-stage training pipelines. We propose ZeroSyl, a simple training-free method to extract syllable boundaries and embeddings directly from a frozen WavLM model. Using L2 norms of features in WavLM's intermediate layers, ZeroSyl achieves competitive syllable segmentation performance. The resulting segments are mean-pooled, discretized using K-means, and used to train a language model. ZeroSyl outperforms prior syllabic tokenizers across lexical, syntactic, and narrative benchmarks. Scaling experiments show that while finer-grained units are beneficial for lexical tasks, our discovered syllabic units exhibit better scaling behavior for syntactic modeling.</li>
</ul>

<h3>Title: Dynamic Training-Free Fusion of Subject and Style LoRAs</h3>
<ul>
<li><strong>Authors: </strong>Qinglong Cao, Yuntian Chen, Chao Ma, Xiaokang Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.SC</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15539">https://arxiv.org/abs/2602.15539</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15539">https://arxiv.org/pdf/2602.15539</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15539]] Dynamic Training-Free Fusion of Subject and Style LoRAs(https://arxiv.org/abs/2602.15539)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent studies have explored the combination of multiple LoRAs to simultaneously generate user-specified subjects and styles. However, most existing approaches fuse LoRA weights using static statistical heuristics that deviate from LoRA's original purpose of learning adaptive feature adjustments and ignore the randomness of sampled inputs. To address this, we propose a dynamic training-free fusion framework that operates throughout the generation process. During the forward pass, at each LoRA-applied layer, we dynamically compute the KL divergence between the base model's original features and those produced by subject and style LoRAs, respectively, and adaptively select the most appropriate weights for fusion. In the reverse denoising stage, we further refine the generation trajectory by dynamically applying gradient-based corrections derived from objective metrics such as CLIP and DINO scores, providing continuous semantic and stylistic guidance. By integrating these two complementary mechanisms-feature-level selection and metric-guided latent adjustment-across the entire diffusion timeline, our method dynamically achieves coherent subject-style synthesis without any retraining. Extensive experiments across diverse subject-style combinations demonstrate that our approach consistently outperforms state-of-the-art LoRA fusion methods both qualitatively and quantitatively.</li>
</ul>

<h3>Title: 1-Bit Wonder: Improving QAT Performance in the Low-Bit Regime through K-Means Quantization</h3>
<ul>
<li><strong>Authors: </strong>Sohir Maskey, Constantin Eichenberg, Johannes Messner, Douglas Orr</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15563">https://arxiv.org/abs/2602.15563</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15563">https://arxiv.org/pdf/2602.15563</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15563]] 1-Bit Wonder: Improving QAT Performance in the Low-Bit Regime through K-Means Quantization(https://arxiv.org/abs/2602.15563)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Quantization-aware training (QAT) is an effective method to drastically reduce the memory footprint of LLMs while keeping performance degradation at an acceptable level. However, the optimal choice of quantization format and bit-width presents a challenge in practice. The full design space of quantization is not fully explored in the context of QAT, and the precise trade-off between quantization and downstream performance is poorly understood, as comparisons often rely solely on perplexity-based evaluations. In this work, we address these shortcomings with an empirical study of QAT in the low-bit regime. We show that k-means based weight quantization outperforms integer formats and can be implemented efficiently on standard hardware. Furthermore, we find that, under a fixed inference memory budget, the best performance on generative downstream tasks is achieved with $1$-bit quantized weights.</li>
</ul>

<h3>Title: Guided Diffusion by Optimized Loss Functions on Relaxed Parameters for Inverse Material Design</h3>
<ul>
<li><strong>Authors: </strong>Jens U. Kreber, Christian Weißenfels, Joerg Stueckler</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15648">https://arxiv.org/abs/2602.15648</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15648">https://arxiv.org/pdf/2602.15648</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15648]] Guided Diffusion by Optimized Loss Functions on Relaxed Parameters for Inverse Material Design(https://arxiv.org/abs/2602.15648)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Inverse design problems are common in engineering and materials science. The forward direction, i.e., computing output quantities from design parameters, typically requires running a numerical simulation, such as a FEM, as an intermediate step, which is an optimization problem by itself. In many scenarios, several design parameters can lead to the same or similar output values. For such cases, multi-modal probabilistic approaches are advantageous to obtain diverse solutions. A major difficulty in inverse design stems from the structure of the design space, since discrete parameters or further constraints disallow the direct use of gradient-based optimization. To tackle this problem, we propose a novel inverse design method based on diffusion models. Our approach relaxes the original design space into a continuous grid representation, where gradients can be computed by implicit differentiation in the forward simulation. A diffusion model is trained on this relaxed parameter space in order to serve as a prior for plausible relaxed designs. Parameters are sampled by guided diffusion using gradients that are propagated from an objective function specified at inference time through the differentiable simulation. A design sample is obtained by backprojection into the original parameter space. We develop our approach for a composite material design problem where the forward process is modeled as a linear FEM problem. We evaluate the performance of our approach in finding designs that match a specified bulk modulus. We demonstrate that our method can propose diverse designs within 1% relative error margin from medium to high target bulk moduli in 2D and 3D settings. We also demonstrate that the material density of generated samples can be minimized simultaneously by using a multi-objective loss function.</li>
</ul>

<h3>Title: Continuous-Time Piecewise-Linear Recurrent Neural Networks</h3>
<ul>
<li><strong>Authors: </strong>Alena Brändle, Lukas Eisenmann, Florian Götz, Daniel Durstewitz</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15649">https://arxiv.org/abs/2602.15649</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15649">https://arxiv.org/pdf/2602.15649</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15649]] Continuous-Time Piecewise-Linear Recurrent Neural Networks(https://arxiv.org/abs/2602.15649)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In dynamical systems reconstruction (DSR) we aim to recover the dynamical system (DS) underlying observed time series. Specifically, we aim to learn a generative surrogate model which approximates the underlying, data-generating DS, and recreates its long-term properties (`climate statistics'). In scientific and medical areas, in particular, these models need to be mechanistically tractable -- through their mathematical analysis we would like to obtain insight into the recovered system's workings. Piecewise-linear (PL), ReLU-based RNNs (PLRNNs) have a strong track-record in this regard, representing SOTA DSR models while allowing mathematical insight by virtue of their PL design. However, all current PLRNN variants are discrete-time maps. This is in disaccord with the assumed continuous-time nature of most physical and biological processes, and makes it hard to accommodate data arriving at irregular temporal intervals. Neural ODEs are one solution, but they do not reach the DSR performance of PLRNNs and often lack their tractability. Here we develop theory for continuous-time PLRNNs (cPLRNNs): We present a novel algorithm for training and simulating such models, bypassing numerical integration by efficiently exploiting their PL structure. We further demonstrate how important topological objects like equilibria or limit cycles can be determined semi-analytically in trained models. We compare cPLRNNs to both their discrete-time cousins as well as Neural ODEs on DSR benchmarks, including systems with discontinuities which come with hard thresholds.</li>
</ul>

<h3>Title: Learning to Retrieve Navigable Candidates for Efficient Vision-and-Language Navigation</h3>
<ul>
<li><strong>Authors: </strong>Shutian Gu, Chengkai Huang, Ruoyu Wang, Lina Yao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15724">https://arxiv.org/abs/2602.15724</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15724">https://arxiv.org/pdf/2602.15724</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15724]] Learning to Retrieve Navigable Candidates for Efficient Vision-and-Language Navigation(https://arxiv.org/abs/2602.15724)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Vision-and-Language Navigation (VLN) requires an agent to follow natural-language instructions and navigate through previously unseen environments. Recent approaches increasingly employ large language models (LLMs) as high-level navigators due to their flexibility and reasoning capability. However, prompt-based LLM navigation often suffers from inefficient decision-making, as the model must repeatedly interpret instructions from scratch and reason over noisy and verbose navigable candidates at each step. In this paper, we propose a retrieval-augmented framework to improve the efficiency and stability of LLM-based VLN without modifying or fine-tuning the underlying language model. Our approach introduces retrieval at two complementary levels. At the episode level, an instruction-level embedding retriever selects semantically similar successful navigation trajectories as in-context exemplars, providing task-specific priors for instruction grounding. At the step level, an imitation-learned candidate retriever prunes irrelevant navigable directions before LLM inference, reducing action ambiguity and prompt complexity. Both retrieval modules are lightweight, modular, and trained independently of the LLM. We evaluate our method on the Room-to-Room (R2R) benchmark. Experimental results demonstrate consistent improvements in Success Rate, Oracle Success Rate, and SPL on both seen and unseen environments. Ablation studies further show that instruction-level exemplar retrieval and candidate pruning contribute complementary benefits to global guidance and step-wise decision efficiency. These results indicate that retrieval-augmented decision support is an effective and scalable strategy for enhancing LLM-based vision-and-language navigation.</li>
</ul>

<h3>Title: Language and Geometry Grounded Sparse Voxel Representations for Holistic Scene Understanding</h3>
<ul>
<li><strong>Authors: </strong>Guile Wu, David Huang, Bingbing Liu, Dongfeng Bai</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15734">https://arxiv.org/abs/2602.15734</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15734">https://arxiv.org/pdf/2602.15734</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15734]] Language and Geometry Grounded Sparse Voxel Representations for Holistic Scene Understanding(https://arxiv.org/abs/2602.15734)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Existing 3D open-vocabulary scene understanding methods mostly emphasize distilling language features from 2D foundation models into 3D feature fields, but largely overlook the synergy among scene appearance, semantics, and geometry. As a result, scene understanding often deviates from the underlying geometric structure of scenes and becomes decoupled from the reconstruction process. In this work, we propose a novel approach that leverages language and geometry grounded sparse voxel representations to comprehensively model appearance, semantics, and geometry within a unified framework. Specifically, we use 3D sparse voxels as primitives and employ an appearance field, a density field, a feature field, and a confidence field to holistically represent a 3D scene. To promote synergy among the appearance, density, and feature fields, we construct a feature modulation module and distill language features from a 2D foundation model into our 3D scene model. In addition, we integrate geometric distillation into feature field distillation to transfer geometric knowledge from a geometry foundation model to our 3D scene representations via depth correlation regularization and pattern consistency regularization. These components work together to synergistically model the appearance, semantics, and geometry of the 3D scene within a unified framework. Extensive experiments demonstrate that our approach achieves superior overall performance compared with state-of-the-art methods in holistic scene understanding and reconstruction.</li>
</ul>

<h3>Title: MRC-GAT: A Meta-Relational Copula-Based Graph Attention Network for Interpretable Multimodal Alzheimer's Disease Diagnosis</h3>
<ul>
<li><strong>Authors: </strong>Fatemeh Khalvandi, Saadat Izadi, Abdolah Chalechale</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, q-bio.QM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15740">https://arxiv.org/abs/2602.15740</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15740">https://arxiv.org/pdf/2602.15740</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15740]] MRC-GAT: A Meta-Relational Copula-Based Graph Attention Network for Interpretable Multimodal Alzheimer's Disease Diagnosis(https://arxiv.org/abs/2602.15740)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Alzheimer's disease (AD) is a progressive neurodegenerative condition necessitating early and precise diagnosis to provide prompt clinical management. Given the paramount importance of early diagnosis, recent studies have increasingly focused on computer-aided diagnostic models to enhance precision and reliability. However, most graph-based approaches still rely on fixed structural designs, which restrict their flexibility and limit generalization across heterogeneous patient data. To overcome these limitations, the Meta-Relational Copula-Based Graph Attention Network (MRC-GAT) is proposed as an efficient multimodal model for AD classification tasks. The proposed architecture, copula-based similarity alignment, relational attention, and node fusion are integrated as the core components of episodic meta-learning, such that the multimodal features, including risk factors (RF), Cognitive test scores, and MRI attributes, are first aligned via a copula-based transformation in a common statistical space and then combined by a multi-relational attention mechanism. According to evaluations performed on the TADPOLE and NACC datasets, the MRC-GAT model achieved accuracies of 96.87% and 92.31%, respectively, demonstrating state-of-the-art performance compared to existing diagnostic models. Finally, the proposed model confirms the robustness and applicability of the proposed method by providing interpretability at various stages of disease diagnosis.</li>
</ul>

<h3>Title: UrbanVerse: Learning Urban Region Representation Across Cities and Tasks</h3>
<ul>
<li><strong>Authors: </strong>Fengze Sun, Egemen Tanin, Shanika Karunasekera, Zuqing Li, Flora D. Salim, Jianzhong Qi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15750">https://arxiv.org/abs/2602.15750</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15750">https://arxiv.org/pdf/2602.15750</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15750]] UrbanVerse: Learning Urban Region Representation Across Cities and Tasks(https://arxiv.org/abs/2602.15750)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent advances in urban region representation learning have enabled a wide range of applications in urban analytics, yet existing methods remain limited in their capabilities to generalize across cities and analytic tasks. We aim to generalize urban representation learning beyond city- and task-specific settings, towards a foundation-style model for urban analytics. To this end, we propose UrbanVerse, a model for cross-city urban representation learning and cross-task urban analytics. For cross-city generalization, UrbanVerse focuses on features local to the target regions and structural features of the nearby regions rather than the entire city. We model regions as nodes on a graph, which enables a random walk-based procedure to form "sequences of regions" that reflect both local and neighborhood structural features for urban region representation learning. For cross-task generalization, we propose a cross-task learning module named HCondDiffCT. This module integrates region-conditioned prior knowledge and task-conditioned semantics into the diffusion process to jointly model multiple downstream urban prediction tasks. HCondDiffCT is generic. It can also be integrated with existing urban representation learning models to enhance their downstream task effectiveness. Experiments on real-world datasets show that UrbanVerse consistently outperforms state-of-the-art methods across six tasks under cross-city settings, achieving up to 35.89% improvements in prediction accuracy.</li>
</ul>

<h3>Title: Under-resourced studies of under-resourced languages: lemmatization and POS-tagging with LLM annotators for historical Armenian, Georgian, Greek and Syriac</h3>
<ul>
<li><strong>Authors: </strong>Chahan Vidal-Gorène (CJM, LIPN), Bastien Kindt (UCL), Florian Cafiero (PSL, CJM)</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15753">https://arxiv.org/abs/2602.15753</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15753">https://arxiv.org/pdf/2602.15753</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15753]] Under-resourced studies of under-resourced languages: lemmatization and POS-tagging with LLM annotators for historical Armenian, Georgian, Greek and Syriac(https://arxiv.org/abs/2602.15753)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Low-resource languages pose persistent challenges for Natural Language Processing tasks such as lemmatization and part-of-speech (POS) tagging. This paper investigates the capacity of recent large language models (LLMs), including GPT-4 variants and open-weight Mistral models, to address these tasks in few-shot and zero-shot settings for four historically and linguistically diverse under-resourced languages: Ancient Greek, Classical Armenian, Old Georgian, and Syriac. Using a novel benchmark comprising aligned training and out-of-domain test corpora, we evaluate the performance of foundation models across lemmatization and POS-tagging, and compare them with PIE, a task-specific RNN baseline. Our results demonstrate that LLMs, even without fine-tuning, achieve competitive or superior performance in POS-tagging and lemmatization across most languages in few-shot settings. Significant challenges persist for languages characterized by complex morphology and non-Latin scripts, but we demonstrate that LLMs are a credible and relevant option for initiating linguistic annotation tasks in the absence of data, serving as an effective aid for annotation.</li>
</ul>

<h3>Title: GLM-5: from Vibe Coding to Agentic Engineering</h3>
<ul>
<li><strong>Authors: </strong>GLM-5 Team: Aohan Zeng, Xin Lv, Zhenyu Hou, Zhengxiao Du, Qinkai Zheng, Bin Chen, Da Yin, Chendi Ge, Chengxing Xie, Cunxiang Wang, Gengzheng Pan, Hao Zeng, Haoke Zhang, Haoran Wang, Huilong Chen, Jiajie Zhang, Jian Jiao, Jiaqi Guo, Jingsen Wang, Jingzhao Du, Jinzhu Wu, Kedong Wang, Lei Li, Lin Fan, Lucen Zhong, Mingdao Liu, Mingming Zhao, Pengfan Du, Qian Dong, Rui Lu, Shuang-Li, Shulin Cao, Song Liu, Ting Jiang, Xiaodong Chen, Xiaohan Zhang, Xuancheng Huang, Xuezhen Dong, Yabo Xu, Yao Wei, Yifan An, Yilin Niu, Yitong Zhu, Yuanhao Wen, Yukuo Cen, Yushi Bai, Zhongpei Qiao, Zihan Wang, Zikang Wang, Zilin Zhu, Ziqiang Liu, Zixuan Li, Bojie Wang, Bosi Wen, Can Huang, Changpeng Cai, Chao Yu, Chen Li, Chen Li, Chenghua Huang, Chengwei Hu, Chenhui Zhang, Chenzheng Zhu, Congfeng Yin, Daoyan Lin, Dayong Yang, Di Wang, Ding Ai, Erle Zhu, Fangzhou Yi, Feiyu Chen, Guohong Wen, Hailong Sun, Haisha Zhao, Haiyi Hu, Hanchen Zhang, Hanrui Liu, Hanyu Zhang, Hao Peng, Hao Tai, Haobo Zhang, He Liu, Hongwei Wang, Hongxi Yan, Hongyu Ge, Huan Liu, Huan Liu, Huanpeng Chu, Jia'ni Zhao, Jiachen Wang, Jiajing Zhao, Jiamin Ren, Jiapeng Wang, Jiaxin Zhang, Jiayi Gui, Jiayue Zhao, Jijie Li, Jing An, Jing Li</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15763">https://arxiv.org/abs/2602.15763</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15763">https://arxiv.org/pdf/2602.15763</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15763]] GLM-5: from Vibe Coding to Agentic Engineering(https://arxiv.org/abs/2602.15763)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>We present GLM-5, a next-generation foundation model designed to transition the paradigm of vibe coding to agentic engineering. Building upon the agentic, reasoning, and coding (ARC) capabilities of its predecessor, GLM-5 adopts DSA to significantly reduce training and inference costs while maintaining long-context fidelity. To advance model alignment and autonomy, we implement a new asynchronous reinforcement learning infrastructure that drastically improves post-training efficiency by decoupling generation from training. Furthermore, we propose novel asynchronous agent RL algorithms that further improve RL quality, enabling the model to learn from complex, long-horizon interactions more effectively. Through these innovations, GLM-5 achieves state-of-the-art performance on major open benchmarks. Most critically, GLM-5 demonstrates unprecedented capability in real-world coding tasks, surpassing previous baselines in handling end-to-end software engineering challenges. Code, models, and more information are available at this https URL.</li>
</ul>

<h3>Title: Understanding vs. Generation: Navigating Optimization Dilemma in Multimodal Models</h3>
<ul>
<li><strong>Authors: </strong>Sen Ye, Mengde Xu, Shuyang Gu, Di He, Liwei Wang, Han Hu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15772">https://arxiv.org/abs/2602.15772</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15772">https://arxiv.org/pdf/2602.15772</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15772]] Understanding vs. Generation: Navigating Optimization Dilemma in Multimodal Models(https://arxiv.org/abs/2602.15772)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Current research in multimodal models faces a key challenge where enhancing generative capabilities often comes at the expense of understanding, and vice versa. We analyzed this trade-off and identify the primary cause might be the potential conflict between generation and understanding, which creates a competitive dynamic within the model. To address this, we propose the Reason-Reflect-Refine (R3) framework. This innovative algorithm re-frames the single-step generation task into a multi-step process of "generate-understand-regenerate". By explicitly leveraging the model's understanding capability during generation, we successfully mitigate the optimization dilemma, achieved stronger generation results and improved understanding ability which are related to the generation process. This offers valuable insights for designing next-generation unified multimodal models. Code is available at this https URL.</li>
</ul>

<h3>Title: NeRFscopy: Neural Radiance Fields for in-vivo Time-Varying Tissues from Endoscopy</h3>
<ul>
<li><strong>Authors: </strong>Laura Salort-Benejam, Antonio Agudo</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15775">https://arxiv.org/abs/2602.15775</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15775">https://arxiv.org/pdf/2602.15775</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15775]] NeRFscopy: Neural Radiance Fields for in-vivo Time-Varying Tissues from Endoscopy(https://arxiv.org/abs/2602.15775)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Endoscopy is essential in medical imaging, used for diagnosis, prognosis and treatment. Developing a robust dynamic 3D reconstruction pipeline for endoscopic videos could enhance visualization, improve diagnostic accuracy, aid in treatment planning, and guide surgery procedures. However, challenges arise due to the deformable nature of the tissues, the use of monocular cameras, illumination changes, occlusions and unknown camera trajectories. Inspired by neural rendering, we introduce NeRFscopy, a self-supervised pipeline for novel view synthesis and 3D reconstruction of deformable endoscopic tissues from a monocular video. NeRFscopy includes a deformable model with a canonical radiance field and a time-dependent deformation field parameterized by SE(3) transformations. In addition, the color images are efficiently exploited by introducing sophisticated terms to learn a 3D implicit model without assuming any template or pre-trained model, solely from data. NeRFscopy achieves accurate results in terms of novel view synthesis, outperforming competing methods across various challenging endoscopy scenes.</li>
</ul>

<h3>Title: VideoSketcher: Video Models Prior Enable Versatile Sequential Sketch Generation</h3>
<ul>
<li><strong>Authors: </strong>Hui Ren, Yuval Alaluf, Omer Bar Tal, Alexander Schwing, Antonio Torralba, Yael Vinker</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15819">https://arxiv.org/abs/2602.15819</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15819">https://arxiv.org/pdf/2602.15819</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15819]] VideoSketcher: Video Models Prior Enable Versatile Sequential Sketch Generation(https://arxiv.org/abs/2602.15819)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Sketching is inherently a sequential process, in which strokes are drawn in a meaningful order to explore and refine ideas. However, most generative models treat sketches as static images, overlooking the temporal structure that underlies creative drawing. We present a data-efficient approach for sequential sketch generation that adapts pretrained text-to-video diffusion models to generate sketching processes. Our key insight is that large language models and video diffusion models offer complementary strengths for this task: LLMs provide semantic planning and stroke ordering, while video diffusion models serve as strong renderers that produce high-quality, temporally coherent visuals. We leverage this by representing sketches as short videos in which strokes are progressively drawn on a blank canvas, guided by text-specified ordering instructions. We introduce a two-stage fine-tuning strategy that decouples the learning of stroke ordering from the learning of sketch appearance. Stroke ordering is learned using synthetic shape compositions with controlled temporal structure, while visual appearance is distilled from as few as seven manually authored sketching processes that capture both global drawing order and the continuous formation of individual strokes. Despite the extremely limited amount of human-drawn sketch data, our method generates high-quality sequential sketches that closely follow text-specified orderings while exhibiting rich visual detail. We further demonstrate the flexibility of our approach through extensions such as brush style conditioning and autoregressive sketch generation, enabling additional controllability and interactive, collaborative drawing.</li>
</ul>

<h3>Title: Stabilizing Test-Time Adaptation of High-Dimensional Simulation Surrogates via D-Optimal Statistics</h3>
<ul>
<li><strong>Authors: </strong>Anna Zimmel, Paul Setinek, Gianluca Galletti, Johannes Brandstetter, Werner Zellinger</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2602.15820">https://arxiv.org/abs/2602.15820</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2602.15820">https://arxiv.org/pdf/2602.15820</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2602.15820]] Stabilizing Test-Time Adaptation of High-Dimensional Simulation Surrogates via D-Optimal Statistics(https://arxiv.org/abs/2602.15820)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Machine learning surrogates are increasingly used in engineering to accelerate costly simulations, yet distribution shifts between training and deployment often cause severe performance degradation (e.g., unseen geometries or configurations). Test-Time Adaptation (TTA) can mitigate such shifts, but existing methods are largely developed for lower-dimensional classification with structured outputs and visually aligned input-output relationships, making them unstable for the high-dimensional, unstructured and regression problems common in simulation. We address this challenge by proposing a TTA framework based on storing maximally informative (D-optimal) statistics, which jointly enables stable adaptation and principled parameter selection at test time. When applied to pretrained simulation surrogates, our method yields up to 7% out-of-distribution improvements at negligible computational cost. To the best of our knowledge, this is the first systematic demonstration of effective TTA for high-dimensional simulation regression and generative design optimization, validated on the SIMSHIFT and EngiBench benchmarks.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
