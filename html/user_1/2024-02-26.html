<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-02-26</h1>
<h3>Title: Text Diffusion with Reinforced Conditioning</h3>
<ul>
<li><strong>Authors: </strong>Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14843">https://arxiv.org/abs/2402.14843</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14843">https://arxiv.org/pdf/2402.14843</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14843]] Text Diffusion with Reinforced Conditioning(https://arxiv.org/abs/2402.14843)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have demonstrated exceptional capability in generating high-quality images, videos, and audio. Due to their adaptiveness in iterative refinement, they provide a strong potential for achieving better non-autoregressive sequence generation. However, existing text diffusion models still fall short in their performance due to a challenge in handling the discreteness of language. This paper thoroughly analyzes text diffusion models and uncovers two significant limitations: degradation of self-conditioning during training and misalignment between training and sampling. Motivated by our findings, we propose a novel Text Diffusion model called TREC, which mitigates the degradation with Reinforced Conditioning and the misalignment by Time-Aware Variance Scaling. Our extensive experiments demonstrate the competitiveness of TREC against autoregressive, non-autoregressive, and diffusion baselines. Moreover, qualitative analysis shows its advanced ability to fully utilize the diffusion process in refining samples.</li>
</ul>

<h3>Title: CHATATC: Large Language Model-Driven Conversational Agents for  Supporting Strategic Air Traffic Flow Management</h3>
<ul>
<li><strong>Authors: </strong>Sinan Abdulhak, Wayne Hubbard, Karthik Gopalakrishnan, Max Z. Li</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14850">https://arxiv.org/abs/2402.14850</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14850">https://arxiv.org/pdf/2402.14850</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14850]] CHATATC: Large Language Model-Driven Conversational Agents for  Supporting Strategic Air Traffic Flow Management(https://arxiv.org/abs/2402.14850)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative artificial intelligence (AI) and large language models (LLMs) have gained rapid popularity through publicly available tools such as ChatGPT. The adoption of LLMs for personal and professional use is fueled by the natural interactions between human users and computer applications such as ChatGPT, along with powerful summarization and text generation capabilities. Given the widespread use of such generative AI tools, in this work we investigate how these tools can be deployed in a non-safety critical, strategic traffic flow management setting. Specifically, we train an LLM, CHATATC, based on a large historical data set of Ground Delay Program (GDP) issuances, spanning 2000-2023 and consisting of over 80,000 GDP implementations, revisions, and cancellations. We test the query and response capabilities of CHATATC, documenting successes (e.g., providing correct GDP rates, durations, and reason) and shortcomings (e.g,. superlative questions). We also detail the design of a graphical user interface for future users to interact and collaborate with the CHATATC conversational agent.</li>
</ul>

<h3>Title: The Wolf Within: Covert Injection of Malice into MLLM Societies via an  MLLM Operative</h3>
<ul>
<li><strong>Authors: </strong>Zhen Tan, Chengshuai Zhao, Raha Moraffah, Yifan Li, Yu Kong, Tianlong Chen, Huan Liu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.AI, cs.CY, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14859">https://arxiv.org/abs/2402.14859</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14859">https://arxiv.org/pdf/2402.14859</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14859]] The Wolf Within: Covert Injection of Malice into MLLM Societies via an  MLLM Operative(https://arxiv.org/abs/2402.14859)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Due to their unprecedented ability to process and respond to various types of data, Multimodal Large Language Models (MLLMs) are constantly defining the new boundary of Artificial General Intelligence (AGI). As these advanced generative models increasingly form collaborative networks for complex tasks, the integrity and security of these systems are crucial. Our paper, ``The Wolf Within'', explores a novel vulnerability in MLLM societies - the indirect propagation of malicious content. Unlike direct harmful output generation for MLLMs, our research demonstrates how a single MLLM agent can be subtly influenced to generate prompts that, in turn, induce other MLLM agents in the society to output malicious content. This subtle, yet potent method of indirect influence marks a significant escalation in the security risks associated with MLLMs. Our findings reveal that, with minimal or even no access to MLLMs' parameters, an MLLM agent, when manipulated to produce specific prompts or instructions, can effectively ``infect'' other agents within a society of MLLMs. This infection leads to the generation and circulation of harmful outputs, such as dangerous instructions or misinformation, across the society. We also show the transferability of these indirectly generated prompts, highlighting their possibility in propagating malice through inter-agent communication. This research provides a critical insight into a new dimension of threat posed by MLLMs, where a single agent can act as a catalyst for widespread malevolent influence. Our work underscores the urgent need for developing robust mechanisms to detect and mitigate such covert manipulations within MLLM societies, ensuring their safe and ethical utilization in societal applications. Our implementation is released at \url{https://github.com/ChengshuaiZhao0/The-Wolf-Within.git}.</li>
</ul>

<h3>Title: Ranking Large Language Models without Ground Truth</h3>
<ul>
<li><strong>Authors: </strong>Amit Dhurandhar, Rahul Nair, Moninder Singh, Elizabeth Daly, Karthikeyan Natesan Ramamurthy</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14860">https://arxiv.org/abs/2402.14860</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14860">https://arxiv.org/pdf/2402.14860</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14860]] Ranking Large Language Models without Ground Truth(https://arxiv.org/abs/2402.14860)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Evaluation and ranking of large language models (LLMs) has become an important problem with the proliferation of these models and their impact. Evaluation methods either require human responses which are expensive to acquire or use pairs of LLMs to evaluate each other which can be unreliable. In this paper, we provide a novel perspective where, given a dataset of prompts (viz. questions, instructions, etc.) and a set of LLMs, we rank them without access to any ground truth or reference responses. Inspired by real life where both an expert and a knowledgeable person can identify a novice our main idea is to consider triplets of models, where each one of them evaluates the other two, correctly identifying the worst model in the triplet with high probability. We also analyze our idea and provide sufficient conditions for it to succeed. Applying this idea repeatedly, we propose two methods to rank LLMs. In experiments on different generative tasks (summarization, multiple-choice, and dialog), our methods reliably recover close to true rankings without reference data. This points to a viable low-resource mechanism for practical use.</li>
</ul>

<h3>Title: Driving Generative Agents With Their Personality</h3>
<ul>
<li><strong>Authors: </strong>Lawrence J. Klinkert, Stephanie Buongiorno, Corey Clark</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14879">https://arxiv.org/abs/2402.14879</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14879">https://arxiv.org/pdf/2402.14879</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14879]] Driving Generative Agents With Their Personality(https://arxiv.org/abs/2402.14879)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>This research explores the potential of Large Language Models (LLMs) to utilize psychometric values, specifically personality information, within the context of video game character development. Affective Computing (AC) systems quantify a Non-Player character's (NPC) psyche, and an LLM can take advantage of the system's information by using the values for prompt generation. The research shows an LLM can consistently represent a given personality profile, thereby enhancing the human-like characteristics of game characters. Repurposing a human examination, the International Personality Item Pool (IPIP) questionnaire, to evaluate an LLM shows that the model can accurately generate content concerning the personality provided. Results show that the improvement of LLM, such as the latest GPT-4 model, can consistently utilize and interpret a personality to represent behavior.</li>
</ul>

<h3>Title: Deep Generative Model-based Synthesis of Four-bar Linkage Mechanisms  with Target Conditions</h3>
<ul>
<li><strong>Authors: </strong>Sumin Lee, Jihoon Kim, Namwoo Kang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14882">https://arxiv.org/abs/2402.14882</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14882">https://arxiv.org/pdf/2402.14882</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14882]] Deep Generative Model-based Synthesis of Four-bar Linkage Mechanisms  with Target Conditions(https://arxiv.org/abs/2402.14882)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Mechanisms are essential components designed to perform specific tasks in various mechanical systems. However, designing a mechanism that satisfies certain kinematic or quasi-static requirements is a challenging task. The kinematic requirements may include the workspace of a mechanism, while the quasi-static requirements of a mechanism may include its torque transmission, which refers to the ability of the mechanism to transfer power and torque effectively. In this paper, we propose a deep learning-based generative model for generating multiple crank-rocker four-bar linkage mechanisms that satisfy both the kinematic and quasi-static requirements aforementioned. The proposed model is based on a conditional generative adversarial network (cGAN) with modifications for mechanism synthesis, which is trained to learn the relationship between the requirements of a mechanism with respect to linkage lengths. The results demonstrate that the proposed model successfully generates multiple distinct mechanisms that satisfy specific kinematic and quasi-static requirements. To evaluate the novelty of our approach, we provide a comparison of the samples synthesized by the proposed cGAN, traditional cVAE and NSGA-II. Our approach has several advantages over traditional design methods. It enables designers to efficiently generate multiple diverse and feasible design candidates while exploring a large design space. Also, the proposed model considers both the kinematic and quasi-static requirements, which can lead to more efficient and effective mechanisms for real-world use, making it a promising tool for linkage mechanism design.</li>
</ul>

<h3>Title: The Common Stability Mechanism behind most Self-Supervised Learning  Approaches</h3>
<ul>
<li><strong>Authors: </strong>Abhishek Jha, Matthew B. Blaschko, Yuki M. Asano, Tinne Tuytelaars</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14957">https://arxiv.org/abs/2402.14957</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14957">https://arxiv.org/pdf/2402.14957</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14957]] The Common Stability Mechanism behind most Self-Supervised Learning  Approaches(https://arxiv.org/abs/2402.14957)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Last couple of years have witnessed a tremendous progress in self-supervised learning (SSL), the success of which can be attributed to the introduction of useful inductive biases in the learning process to learn meaningful visual representations while avoiding collapse. These inductive biases and constraints manifest themselves in the form of different optimization formulations in the SSL techniques, e.g. by utilizing negative examples in a contrastive formulation, or exponential moving average and predictor in BYOL and SimSiam. In this paper, we provide a framework to explain the stability mechanism of these different SSL techniques: i) we discuss the working mechanism of contrastive techniques like SimCLR, non-contrastive techniques like BYOL, SWAV, SimSiam, Barlow Twins, and DINO; ii) we provide an argument that despite different formulations these methods implicitly optimize a similar objective function, i.e. minimizing the magnitude of the expected representation over all data samples, or the mean of the data distribution, while maximizing the magnitude of the expected representation of individual samples over different data augmentations; iii) we provide mathematical and empirical evidence to support our framework. We formulate different hypotheses and test them using the Imagenet100 dataset.</li>
</ul>

<h3>Title: Unsupervised Domain Adaptation within Deep Foundation Latent Spaces</h3>
<ul>
<li><strong>Authors: </strong>Dmitry Kangin, Plamen Angelov</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14976">https://arxiv.org/abs/2402.14976</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14976">https://arxiv.org/pdf/2402.14976</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14976]] Unsupervised Domain Adaptation within Deep Foundation Latent Spaces(https://arxiv.org/abs/2402.14976)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>The vision transformer-based foundation models, such as ViT or Dino-V2, are aimed at solving problems with little or no finetuning of features. Using a setting of prototypical networks, we analyse to what extent such foundation models can solve unsupervised domain adaptation without finetuning over the source or target domain. Through quantitative analysis, as well as qualitative interpretations of decision making, we demonstrate that the suggested method can improve upon existing baselines, as well as showcase the limitations of such approach yet to be solved.</li>
</ul>

<h3>Title: Mudjacking: Patching Backdoor Vulnerabilities in Foundation Models</h3>
<ul>
<li><strong>Authors: </strong>Hongbin Liu, Michael K. Reiter, Neil Zhenqiang Gong</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14977">https://arxiv.org/abs/2402.14977</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14977">https://arxiv.org/pdf/2402.14977</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14977]] Mudjacking: Patching Backdoor Vulnerabilities in Foundation Models(https://arxiv.org/abs/2402.14977)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Foundation model has become the backbone of the AI ecosystem. In particular, a foundation model can be used as a general-purpose feature extractor to build various downstream classifiers. However, foundation models are vulnerable to backdoor attacks and a backdoored foundation model is a single-point-of-failure of the AI ecosystem, e.g., multiple downstream classifiers inherit the backdoor vulnerabilities simultaneously. In this work, we propose Mudjacking, the first method to patch foundation models to remove backdoors. Specifically, given a misclassified trigger-embedded input detected after a backdoored foundation model is deployed, Mudjacking adjusts the parameters of the foundation model to remove the backdoor. We formulate patching a foundation model as an optimization problem and propose a gradient descent based method to solve it. We evaluate Mudjacking on both vision and language foundation models, eleven benchmark datasets, five existing backdoor attacks, and thirteen adaptive backdoor attacks. Our results show that Mudjacking can remove backdoor from a foundation model while maintaining its utility.</li>
</ul>

<h3>Title: Stable Neural Stochastic Differential Equations in Analyzing Irregular  Time Series Data</h3>
<ul>
<li><strong>Authors: </strong>YongKyung Oh, Dongyoung Lim, Sungil Kim</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.14989">https://arxiv.org/abs/2402.14989</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.14989">https://arxiv.org/pdf/2402.14989</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.14989]] Stable Neural Stochastic Differential Equations in Analyzing Irregular  Time Series Data(https://arxiv.org/abs/2402.14989)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Irregular sampling intervals and missing values in real-world time series data present challenges for conventional methods that assume consistent intervals and complete data. Neural Ordinary Differential Equations (Neural ODEs) offer an alternative approach, utilizing neural networks combined with ODE solvers to learn continuous latent representations through parameterized vector fields. Neural Stochastic Differential Equations (Neural SDEs) extend Neural ODEs by incorporating a diffusion term, although this addition is not trivial, particularly when addressing irregular intervals and missing values. Consequently, careful design of drift and diffusion functions is crucial for maintaining stability and enhancing performance, while incautious choices can result in adverse properties such as the absence of strong solutions, stochastic destabilization, or unstable Euler discretizations, significantly affecting Neural SDEs' performance. In this study, we propose three stable classes of Neural SDEs: Langevin-type SDE, Linear Noise SDE, and Geometric SDE. Then, we rigorously demonstrate their robustness in maintaining excellent performance under distribution shift, while effectively preventing overfitting. To assess the effectiveness of our approach, we conduct extensive experiments on four benchmark datasets for interpolation, forecasting, and classification tasks, and analyze the robustness of our methods with 30 public datasets under different missing rates. Our results demonstrate the efficacy of the proposed method in handling real-world irregular time series data.</li>
</ul>

<h3>Title: Towards Few-Shot Adaptation of Foundation Models via Multitask  Finetuning</h3>
<ul>
<li><strong>Authors: </strong>Zhuoyan Xu, Zhenmei Shi, Junyi Wei, Fangzhou Mu, Yin Li, Yingyu Liang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15017">https://arxiv.org/abs/2402.15017</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15017">https://arxiv.org/pdf/2402.15017</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15017]] Towards Few-Shot Adaptation of Foundation Models via Multitask  Finetuning(https://arxiv.org/abs/2402.15017)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Foundation models have emerged as a powerful tool for many AI problems. Despite the tremendous success of foundation models, effective adaptation to new tasks, particularly those with limited labels, remains an open question and lacks theoretical understanding. An emerging solution with recent success in vision and NLP involves finetuning a foundation model on a selection of relevant tasks, before its adaptation to a target task with limited labeled samples. In this paper, we study the theoretical justification of this multitask finetuning approach. Our theoretical analysis reveals that with a diverse set of related tasks, this multitask finetuning leads to reduced error in the target task, in comparison to directly adapting the same pretrained model. We quantify the relationship between finetuning tasks and target tasks by diversity and consistency metrics, and further propose a practical task selection algorithm. We substantiate our theoretical claims with extensive empirical evidence. Further, we present results affirming our task selection algorithm adeptly chooses related finetuning tasks, providing advantages to the model performance on target tasks. We believe our study shed new light on the effective adaptation of foundation models to new tasks that lack abundant labels. Our code is available at https://github.com/OliverXUZY/Foudation-Model_Multitask.</li>
</ul>

<h3>Title: Fine-tuning Large Language Models for Domain-specific Machine  Translation</h3>
<ul>
<li><strong>Authors: </strong>Jiawei Zheng, Hanghai Hong, Xiaoli Wang, Jingsong Su, Yonggui Liang, Shikai Wu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15061">https://arxiv.org/abs/2402.15061</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15061">https://arxiv.org/pdf/2402.15061</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15061]] Fine-tuning Large Language Models for Domain-specific Machine  Translation(https://arxiv.org/abs/2402.15061)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large language models (LLMs) have made significant progress in machine translation (MT). However, their potential in domain-specific MT remains under-explored. Current LLM-based MT systems still face several challenges. First, for LLMs with in-context learning, their effectiveness is highly sensitive to input translation examples, and processing them can increase inference costs. They often require extra post-processing due to over-generation. Second, LLMs with fine-tuning on domain-specific data often require high training costs for domain adaptation, and may weaken the zero-shot MT capabilities of LLMs due to over-specialization. The aforementioned methods can struggle to translate rare words in domain transfer scenarios. To address these challenges, this paper proposes a prompt-oriented fine-tuning method, denoted as LlamaIT, to effectively and efficiently fine-tune a general-purpose LLM for domain-specific MT tasks. First, we construct a task-specific mix-domain dataset, which is then used to fine-tune the LLM with LoRA. This can eliminate the need for input translation examples, post-processing, or over-specialization. By zero-shot prompting with instructions, we adapt the MT tasks to the target domain at inference time. To further elicit the MT capability for rare words, we construct new prompts by incorporating domain-specific bilingual vocabulary. We also conduct extensive experiments on both publicly available and self-constructed datasets. The results show that our LlamaIT can significantly enhance the domain-specific MT capabilities of the LLM, meanwhile preserving its zero-shot MT capabilities.</li>
</ul>

<h3>Title: AttributionBench: How Hard is Automatic Attribution Evaluation?</h3>
<ul>
<li><strong>Authors: </strong>Yifei Li, Xiang Yue, Zeyi Liao, Huan Sun</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15089">https://arxiv.org/abs/2402.15089</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15089">https://arxiv.org/pdf/2402.15089</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15089]] AttributionBench: How Hard is Automatic Attribution Evaluation?(https://arxiv.org/abs/2402.15089)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Modern generative search engines enhance the reliability of large language model (LLM) responses by providing cited evidence. However, evaluating the answer's attribution, i.e., whether every claim within the generated responses is fully supported by its cited evidence, remains an open problem. This verification, traditionally dependent on costly human evaluation, underscores the urgent need for automatic attribution evaluation methods. To bridge the gap in the absence of standardized benchmarks for these methods, we present AttributionBench, a comprehensive benchmark compiled from various existing attribution datasets. Our extensive experiments on AttributionBench reveal the challenges of automatic attribution evaluation, even for state-of-the-art LLMs. Specifically, our findings show that even a fine-tuned GPT-3.5 only achieves around 80% macro-F1 under a binary classification formulation. A detailed analysis of more than 300 error cases indicates that a majority of failures stem from the model's inability to process nuanced information, and the discrepancy between the information the model has access to and that human annotators do.</li>
</ul>

<h3>Title: Interactive-KBQA: Multi-Turn Interactions for Knowledge Base Question  Answering with Large Language Models</h3>
<ul>
<li><strong>Authors: </strong>Guanming Xiong, Junwei Bao, Wen Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15131">https://arxiv.org/abs/2402.15131</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15131">https://arxiv.org/pdf/2402.15131</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15131]] Interactive-KBQA: Multi-Turn Interactions for Knowledge Base Question  Answering with Large Language Models(https://arxiv.org/abs/2402.15131)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>This study explores the realm of knowledge-base question answering (KBQA). KBQA is considered a challenging task, particularly in parsing intricate questions into executable logical forms. Traditional semantic parsing (SP)-based methods require extensive data annotations, which result in significant costs. Recently, the advent of few-shot in-context learning, powered by large language models (LLMs), has showcased promising capabilities. Yet, fully leveraging LLMs to parse questions into logical forms in low-resource scenarios poses a substantial challenge. To tackle these hurdles, we introduce Interactive-KBQA, a framework designed to generate logical forms through direct interaction with knowledge bases (KBs). Within this framework, we have developed three generic APIs for KB interaction. For each category of complex question, we devised exemplars to guide LLMs through the reasoning processes. Our method achieves competitive results on the WebQuestionsSP, ComplexWebQuestions, KQA Pro, and MetaQA datasets with a minimal number of examples (shots). Importantly, our approach supports manual intervention, allowing for the iterative refinement of LLM outputs. By annotating a dataset with step-wise reasoning processes, we showcase our model's adaptability and highlight its potential for contributing significant enhancements to the field.</li>
</ul>

<h3>Title: Modified CycleGAN for the synthesization of samples for wheat head  segmentation</h3>
<ul>
<li><strong>Authors: </strong>Jaden Myers, Keyhan Najafian, Farhad Maleki, Katie Ovens</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15135">https://arxiv.org/abs/2402.15135</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15135">https://arxiv.org/pdf/2402.15135</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15135]] Modified CycleGAN for the synthesization of samples for wheat head  segmentation(https://arxiv.org/abs/2402.15135)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Deep learning models have been used for a variety of image processing tasks. However, most of these models are developed through supervised learning approaches, which rely heavily on the availability of large-scale annotated datasets. Developing such datasets is tedious and expensive. In the absence of an annotated dataset, synthetic data can be used for model development; however, due to the substantial differences between simulated and real data, a phenomenon referred to as domain gap, the resulting models often underperform when applied to real data. In this research, we aim to address this challenge by first computationally simulating a large-scale annotated dataset and then using a generative adversarial network (GAN) to fill the gap between simulated and real images. This approach results in a synthetic dataset that can be effectively utilized to train a deep-learning model. Using this approach, we developed a realistic annotated synthetic dataset for wheat head segmentation. This dataset was then used to develop a deep-learning model for semantic segmentation. The resulting model achieved a Dice score of 83.4\% on an internal dataset and Dice scores of 79.6% and 83.6% on two external Global Wheat Head Detection datasets. While we proposed this approach in the context of wheat head segmentation, it can be generalized to other crop types or, more broadly, to images with dense, repeated patterns such as those found in cellular imagery.</li>
</ul>

<h3>Title: PUAD: Frustratingly Simple Method for Robust Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Shota Sugawara, Ryuji Imamura</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15143">https://arxiv.org/abs/2402.15143</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15143">https://arxiv.org/pdf/2402.15143</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15143]] PUAD: Frustratingly Simple Method for Robust Anomaly Detection(https://arxiv.org/abs/2402.15143)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Developing an accurate and fast anomaly detection model is an important task in real-time computer vision applications. There has been much research to develop a single model that detects either structural or logical anomalies, which are inherently distinct. The majority of the existing approaches implicitly assume that the anomaly can be represented by identifying the anomalous location. However, we argue that logical anomalies, such as the wrong number of objects, can not be well-represented by the spatial feature maps and require an alternative approach. In addition, we focused on the possibility of detecting logical anomalies by using an out-of-distribution detection approach on the feature space, which aggregates the spatial information of the feature map. As a demonstration, we propose a method that incorporates a simple out-of-distribution detection method on the feature space against state-of-the-art reconstruction-based approaches. Despite the simplicity of our proposal, our method PUAD (Picturable and Unpicturable Anomaly Detection) achieves state-of-the-art performance on the MVTec LOCO AD dataset.</li>
</ul>

<h3>Title: Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and  Context-Aware Visual Speech Processing</h3>
<ul>
<li><strong>Authors: </strong>Jeong Hun Yeo, Seunghee Han, Minsu Kim, Yong Man Ro</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, eess.AS, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15151">https://arxiv.org/abs/2402.15151</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15151">https://arxiv.org/pdf/2402.15151</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15151]] Where Visual Speech Meets Language: VSP-LLM Framework for Efficient and  Context-Aware Visual Speech Processing(https://arxiv.org/abs/2402.15151)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>In visual speech processing, context modeling capability is one of the most important requirements due to the ambiguous nature of lip movements. For example, homophenes, words that share identical lip movements but produce different sounds, can be distinguished by considering the context. In this paper, we propose a novel framework, namely Visual Speech Processing incorporated with LLMs (VSP-LLM), to maximize the context modeling ability by bringing the overwhelming power of LLMs. Specifically, VSP-LLM is designed to perform multi-tasks of visual speech recognition and translation, where the given instructions control the type of task. The input video is mapped to the input latent space of a LLM by employing a self-supervised visual speech model. Focused on the fact that there is redundant information in input frames, we propose a novel deduplication method that reduces the embedded visual features by employing visual speech units. Through the proposed deduplication and Low Rank Adaptors (LoRA), VSP-LLM can be trained in a computationally efficient manner. In the translation dataset, the MuAViC benchmark, we demonstrate that VSP-LLM can more effectively recognize and translate lip movements with just 15 hours of labeled data, compared to the recent translation model trained with 433 hours of labeld data.</li>
</ul>

<h3>Title: The Surprising Effectiveness of Skip-Tuning in Diffusion Sampling</h3>
<ul>
<li><strong>Authors: </strong>Jiajun Ma, Shuchen Xue, Tianyang Hu, Wenjia Wang, Zhaoqiang Liu, Zhenguo Li, Zhi-Ming Ma, Kenji Kawaguchi</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15170">https://arxiv.org/abs/2402.15170</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15170">https://arxiv.org/pdf/2402.15170</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15170]] The Surprising Effectiveness of Skip-Tuning in Diffusion Sampling(https://arxiv.org/abs/2402.15170)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>With the incorporation of the UNet architecture, diffusion probabilistic models have become a dominant force in image generation tasks. One key design in UNet is the skip connections between the encoder and decoder blocks. Although skip connections have been shown to improve training stability and model performance, we reveal that such shortcuts can be a limiting factor for the complexity of the transformation. As the sampling steps decrease, the generation process and the role of the UNet get closer to the push-forward transformations from Gaussian distribution to the target, posing a challenge for the network's complexity. To address this challenge, we propose Skip-Tuning, a simple yet surprisingly effective training-free tuning method on the skip connections. Our method can achieve 100% FID improvement for pretrained EDM on ImageNet 64 with only 19 NFEs (1.75), breaking the limit of ODE samplers regardless of sampling steps. Surprisingly, the improvement persists when we increase the number of sampling steps and can even surpass the best result from EDM-2 (1.58) with only 39 NFEs (1.57). Comprehensive exploratory experiments are conducted to shed light on the surprising effectiveness. We observe that while Skip-Tuning increases the score-matching losses in the pixel space, the losses in the feature space are reduced, particularly at intermediate noise levels, which coincide with the most effective range accounting for image quality improvement.</li>
</ul>

<h3>Title: Fine-Tuning of Continuous-Time Diffusion Models as Entropy-Regularized  Control</h3>
<ul>
<li><strong>Authors: </strong>Masatoshi Uehara, Yulai Zhao, Kevin Black, Ehsan Hajiramezanali, Gabriele Scalia, Nathaniel Lee Diamant, Alex M Tseng, Tommaso Biancalani, Sergey Levine</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15194">https://arxiv.org/abs/2402.15194</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15194">https://arxiv.org/pdf/2402.15194</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15194]] Fine-Tuning of Continuous-Time Diffusion Models as Entropy-Regularized  Control(https://arxiv.org/abs/2402.15194)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models excel at capturing complex data distributions, such as those of natural images and proteins. While diffusion models are trained to represent the distribution in the training dataset, we often are more concerned with other properties, such as the aesthetic quality of the generated images or the functional properties of generated proteins. Diffusion models can be finetuned in a goal-directed way by maximizing the value of some reward function (e.g., the aesthetic quality of an image). However, these approaches may lead to reduced sample diversity, significant deviations from the training data distribution, and even poor sample quality due to the exploitation of an imperfect reward function. The last issue often occurs when the reward function is a learned model meant to approximate a ground-truth "genuine" reward, as is the case in many practical applications. These challenges, collectively termed "reward collapse," pose a substantial obstacle. To address this reward collapse, we frame the finetuning problem as entropy-regularized control against the pretrained diffusion model, i.e., directly optimizing entropy-enhanced rewards with neural SDEs. We present theoretical and empirical evidence that demonstrates our framework is capable of efficiently generating diverse samples with high genuine rewards, mitigating the overoptimization of imperfect reward models.</li>
</ul>

<h3>Title: Label-efficient Multi-organ Segmentation Method with Diffusion Model</h3>
<ul>
<li><strong>Authors: </strong>Yongzhi Huang, Jinxin Zhu, Haseeb Hassan, Liyilei Su, Jingyu Li, Binding Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15216">https://arxiv.org/abs/2402.15216</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15216">https://arxiv.org/pdf/2402.15216</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15216]] Label-efficient Multi-organ Segmentation Method with Diffusion Model(https://arxiv.org/abs/2402.15216)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Accurate segmentation of multiple organs in Computed Tomography (CT) images plays a vital role in computer-aided diagnosis systems. Various supervised-learning approaches have been proposed recently. However, these methods heavily depend on a large amount of high-quality labeled data, which is expensive to obtain in practice. In this study, we present a label-efficient learning approach using a pre-trained diffusion model for multi-organ segmentation tasks in CT images. First, a denoising diffusion model was trained using unlabeled CT data, generating additional two-dimensional (2D) CT images. Then the pre-trained denoising diffusion network was transferred to the downstream multi-organ segmentation task, effectively creating a semi-supervised learning model that requires only a small amount of labeled data. Furthermore, linear classification and fine-tuning decoder strategies were employed to enhance the network's segmentation performance. Our generative model at 256x256 resolution achieves impressive performance in terms of Fr\'echet inception distance, spatial Fr\'echet inception distance, and F1-score, with values of 11.32, 46.93, and 73.1\%, respectively. These results affirm the diffusion model's ability to generate diverse and realistic 2D CT images. Additionally, our method achieves competitive multi-organ segmentation performance compared to state-of-the-art methods on the FLARE 2022 dataset, particularly in limited labeled data scenarios. Remarkably, even with only 1\% and 10\% labeled data, our method achieves Dice similarity coefficients (DSCs) of 71.56\% and 78.51\% after fine-tuning, respectively. The method achieves a DSC score of 51.81\% using just four labeled CT scans. These results demonstrate the efficacy of our approach in overcoming the limitations of supervised learning heavily reliant on large-scale labeled data.</li>
</ul>

<h3>Title: BSPA: Exploring Black-box Stealthy Prompt Attacks against Image  Generators</h3>
<ul>
<li><strong>Authors: </strong>Yu Tian, Xiao Yang, Yinpeng Dong, Heming Yang, Hang Su, Jun Zhu</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.CL, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15218">https://arxiv.org/abs/2402.15218</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15218">https://arxiv.org/pdf/2402.15218</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15218]] BSPA: Exploring Black-box Stealthy Prompt Attacks against Image  Generators(https://arxiv.org/abs/2402.15218)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Extremely large image generators offer significant transformative potential across diverse sectors. It allows users to design specific prompts to generate realistic images through some black-box APIs. However, some studies reveal that image generators are notably susceptible to attacks and generate Not Suitable For Work (NSFW) contents by manually designed toxin texts, especially imperceptible to human observers. We urgently need a multitude of universal and transferable prompts to improve the safety of image generators, especially black-box-released APIs. Nevertheless, they are constrained by labor-intensive design processes and heavily reliant on the quality of the given instructions. To achieve this, we introduce a black-box stealthy prompt attack (BSPA) that adopts a retriever to simulate attacks from API users. It can effectively harness filter scores to tune the retrieval space of sensitive words for matching the input prompts, thereby crafting stealthy prompts tailored for image generators. Significantly, this approach is model-agnostic and requires no internal access to the model's features, ensuring its applicability to a wide range of image generators. Building on BSPA, we have constructed an automated prompt tool and a comprehensive prompt attack dataset (NSFWeval). Extensive experiments demonstrate that BSPA effectively explores the security vulnerabilities in a variety of state-of-the-art available black-box models, including Stable Diffusion XL, Midjourney, and DALL-E 2/3. Furthermore, we develop a resilient text filter and offer targeted recommendations to ensure the security of image generators against prompt attacks in the future.</li>
</ul>

<h3>Title: Let's Rectify Step by Step: Improving Aspect-based Sentiment Analysis  with Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Shunyu Liu, Jie Zhou, Qunxi Zhu, Qin Chen, Qingchun Bai, Jun Xiao, Liang He</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15289">https://arxiv.org/abs/2402.15289</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15289">https://arxiv.org/pdf/2402.15289</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15289]] Let's Rectify Step by Step: Improving Aspect-based Sentiment Analysis  with Diffusion Models(https://arxiv.org/abs/2402.15289)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Aspect-Based Sentiment Analysis (ABSA) stands as a crucial task in predicting the sentiment polarity associated with identified aspects within text. However, a notable challenge in ABSA lies in precisely determining the aspects' boundaries (start and end indices), especially for long ones, due to users' colloquial expressions. We propose DiffusionABSA, a novel diffusion model tailored for ABSA, which extracts the aspects progressively step by step. Particularly, DiffusionABSA gradually adds noise to the aspect terms in the training process, subsequently learning a denoising process that progressively restores these terms in a reverse manner. To estimate the boundaries, we design a denoising neural network enhanced by a syntax-aware temporal attention mechanism to chronologically capture the interplay between aspects and surrounding text. Empirical evaluations conducted on eight benchmark datasets underscore the compelling advantages offered by DiffusionABSA when compared against robust baseline models. Our code is publicly available at https://github.com/Qlb6x/DiffusionABSA.</li>
</ul>

<h3>Title: Semi-supervised Counting via Pixel-by-pixel Density Distribution  Modelling</h3>
<ul>
<li><strong>Authors: </strong>Hui Lin, Zhiheng Ma, Rongrong Ji, Yaowei Wang, Zhou Su, Xiaopeng Hong, Deyu Meng</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15297">https://arxiv.org/abs/2402.15297</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15297">https://arxiv.org/pdf/2402.15297</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15297]] Semi-supervised Counting via Pixel-by-pixel Density Distribution  Modelling(https://arxiv.org/abs/2402.15297)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>This paper focuses on semi-supervised crowd counting, where only a small portion of the training data are labeled. We formulate the pixel-wise density value to regress as a probability distribution, instead of a single deterministic value. On this basis, we propose a semi-supervised crowd-counting model. Firstly, we design a pixel-wise distribution matching loss to measure the differences in the pixel-wise density distributions between the prediction and the ground truth; Secondly, we enhance the transformer decoder by using density tokens to specialize the forwards of decoders w.r.t. different density intervals; Thirdly, we design the interleaving consistency self-supervised learning mechanism to learn from unlabeled data efficiently. Extensive experiments on four datasets are performed to show that our method clearly outperforms the competitors by a large margin under various labeled ratio settings. Code will be released at https://github.com/LoraLinH/Semi-supervised-Counting-via-Pixel-by-pixel-Density-Distribution-Modelling.</li>
</ul>

<h3>Title: Understanding Oversmoothing in Diffusion-Based GNNs From the Perspective  of Operator Semigroup Theory</h3>
<ul>
<li><strong>Authors: </strong>Weichen Zhao, Chenguang Wang, Xinyan Wang, Congying Han, Tiande Guo, Tianshu Yu</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15326">https://arxiv.org/abs/2402.15326</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15326">https://arxiv.org/pdf/2402.15326</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15326]] Understanding Oversmoothing in Diffusion-Based GNNs From the Perspective  of Operator Semigroup Theory(https://arxiv.org/abs/2402.15326)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>This paper presents a novel study of the oversmoothing issue in diffusion-based Graph Neural Networks (GNNs). Diverging from extant approaches grounded in random walk analysis or particle systems, we approach this problem through operator semigroup theory. This theoretical framework allows us to rigorously prove that oversmoothing is intrinsically linked to the ergodicity of the diffusion operator. This finding further poses a general and mild ergodicity-breaking condition, encompassing the various specific solutions previously offered, thereby presenting a more universal and theoretically grounded approach to mitigating oversmoothing in diffusion-based GNNs. Additionally, we offer a probabilistic interpretation of our theory, forging a link with prior works and broadening the theoretical horizon. Our experimental results reveal that this ergodicity-breaking term effectively mitigates oversmoothing measured by Dirichlet energy, and simultaneously enhances performance in node classification tasks.</li>
</ul>

<h3>Title: NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data</h3>
<ul>
<li><strong>Authors: </strong>Sergei Bogdanov, Alexandre Constantin, Timothée Bernard, Benoit Crabbé, Etienne Bernard</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15343">https://arxiv.org/abs/2402.15343</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15343">https://arxiv.org/pdf/2402.15343</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15343]] NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data(https://arxiv.org/abs/2402.15343)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have shown impressive abilities in data annotation, opening the way for new approaches to solve classic NLP problems. In this paper, we show how to use LLMs to create NuNER, a compact language representation model specialized in the Named Entity Recognition (NER) task. NuNER can be fine-tuned to solve downstream NER problems in a data-efficient way, outperforming similar-sized foundation models in the few-shot regime and competing with much larger LLMs. We find that the size and entity-type diversity of the pre-training dataset are key to achieving good performance. We view NuNER as a member of the broader family of task-specific foundation models, recently unlocked by LLMs.</li>
</ul>

<h3>Title: Outlier detection by ensembling uncertainty with negative objectness</h3>
<ul>
<li><strong>Authors: </strong>Anja Delić, Matej Grcić, Siniša Šegvić</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15374">https://arxiv.org/abs/2402.15374</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15374">https://arxiv.org/pdf/2402.15374</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15374]] Outlier detection by ensembling uncertainty with negative objectness(https://arxiv.org/abs/2402.15374)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Outlier detection is an essential capability in safety-critical applications of supervised visual recognition. Most of the existing methods deliver best results by encouraging standard closed-set models to produce low-confidence predictions in negative training data. However, that approach conflates prediction uncertainty with recognition of the negative class. We therefore reconsider direct prediction of K+1 logits that correspond to K groundtruth classes and one outlier class. This setup allows us to formulate a novel anomaly score as an ensemble of in-distribution uncertainty and the posterior of the outlier class which we term negative objectness. Now outliers can be independently detected due to i) high prediction uncertainty or ii) similarity with negative data. We embed our method into a dense prediction architecture with mask-level recognition over K+2 classes. The training procedure encourages the novel K+2-th class to learn negative objectness at pasted negative instances. Our models outperform the current state-of-the art on standard benchmarks for image-wide and pixel-level outlier detection with and without training on real negative data.</li>
</ul>

<h3>Title: Genie: Generative Interactive Environments</h3>
<ul>
<li><strong>Authors: </strong>Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, Yusuf Aytar, Sarah Bechtle, Feryal Behbahani, Stephanie Chan, Nicolas Heess, Lucy Gonzalez, Simon Osindero, Sherjil Ozair, Scott Reed, Jingwei Zhang, Konrad Zolna, Jeff Clune, Nando de Freitas, Satinder Singh, Tim Rocktäschel</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15391">https://arxiv.org/abs/2402.15391</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15391">https://arxiv.org/pdf/2402.15391</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15391]] Genie: Generative Interactive Environments(https://arxiv.org/abs/2402.15391)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We introduce Genie, the first generative interactive environment trained in an unsupervised manner from unlabelled Internet videos. The model can be prompted to generate an endless variety of action-controllable virtual worlds described through text, synthetic images, photographs, and even sketches. At 11B parameters, Genie can be considered a foundation world model. It is comprised of a spatiotemporal video tokenizer, an autoregressive dynamics model, and a simple and scalable latent action model. Genie enables users to act in the generated environments on a frame-by-frame basis despite training without any ground-truth action labels or other domain-specific requirements typically found in the world model literature. Further the resulting learned latent action space facilitates training agents to imitate behaviors from unseen videos, opening the path for training generalist agents of the future.</li>
</ul>

<h3>Title: United We Pretrain, Divided We Fail! Representation Learning for Time  Series by Pretraining on 75 Datasets at Once</h3>
<ul>
<li><strong>Authors: </strong>Maurice Kraus, Felix Divo, David Steinmann, Devendra Singh Dhami, Kristian Kersting</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15404">https://arxiv.org/abs/2402.15404</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15404">https://arxiv.org/pdf/2402.15404</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15404]] United We Pretrain, Divided We Fail! Representation Learning for Time  Series by Pretraining on 75 Datasets at Once(https://arxiv.org/abs/2402.15404)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>In natural language processing and vision, pretraining is utilized to learn effective representations. Unfortunately, the success of pretraining does not easily carry over to time series due to potential mismatch between sources and target. Actually, common belief is that multi-dataset pretraining does not work for time series! Au contraire, we introduce a new self-supervised contrastive pretraining approach to learn one encoding from many unlabeled and diverse time series datasets, so that the single learned representation can then be reused in several target domains for, say, classification. Specifically, we propose the XD-MixUp interpolation method and the Soft Interpolation Contextual Contrasting (SICC) loss. Empirically, this outperforms both supervised training and other self-supervised pretraining methods when finetuning on low-data regimes. This disproves the common belief: We can actually learn from multiple time series datasets, even from 75 at once.</li>
</ul>

<h3>Title: ProTIP: Probabilistic Robustness Verification on Text-to-Image Diffusion  Models against Stochastic Perturbation</h3>
<ul>
<li><strong>Authors: </strong>Yi Zhang, Yun Tang, Wenjie Ruan, Xiaowei Huang, Siddartha Khastgir, Paul Jennings, Xingyu Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15429">https://arxiv.org/abs/2402.15429</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15429">https://arxiv.org/pdf/2402.15429</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15429]] ProTIP: Probabilistic Robustness Verification on Text-to-Image Diffusion  Models against Stochastic Perturbation(https://arxiv.org/abs/2402.15429)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Text-to-Image (T2I) Diffusion Models (DMs) have shown impressive abilities in generating high-quality images based on simple text descriptions. However, as is common with many Deep Learning (DL) models, DMs are subject to a lack of robustness. While there are attempts to evaluate the robustness of T2I DMs as a binary or worst-case problem, they cannot answer how robust in general the model is whenever an adversarial example (AE) can be found. In this study, we first introduce a probabilistic notion of T2I DMs' robustness; and then establish an efficient framework, ProTIP, to evaluate it with statistical guarantees. The main challenges stem from: i) the high computational cost of the generation process; and ii) determining if a perturbed input is an AE involves comparing two output distributions, which is fundamentally harder compared to other DL tasks like classification where an AE is identified upon misprediction of labels. To tackle the challenges, we employ sequential analysis with efficacy and futility early stopping rules in the statistical testing for identifying AEs, and adaptive concentration inequalities to dynamically determine the "just-right" number of stochastic perturbations whenever the verification target is met. Empirical experiments validate the effectiveness and efficiency of ProTIP over common T2I DMs. Finally, we demonstrate an application of ProTIP to rank commonly used defence methods.</li>
</ul>

<h3>Title: A Comprehensive Survey of Convolutions in Deep Learning: Applications,  Challenges, and Future Trends</h3>
<ul>
<li><strong>Authors: </strong>Abolfazl Younesi, Mohsen Ansari, MohammadAmin Fazli, Alireza Ejlali, Muhammad Shafique, Jörg Henkel</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15490">https://arxiv.org/abs/2402.15490</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15490">https://arxiv.org/pdf/2402.15490</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15490]] A Comprehensive Survey of Convolutions in Deep Learning: Applications,  Challenges, and Future Trends(https://arxiv.org/abs/2402.15490)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>In today's digital age, Convolutional Neural Networks (CNNs), a subset of Deep Learning (DL), are widely used for various computer vision tasks such as image classification, object detection, and image segmentation. There are numerous types of CNNs designed to meet specific needs and requirements, including 1D, 2D, and 3D CNNs, as well as dilated, grouped, attention, depthwise convolutions, and NAS, among others. Each type of CNN has its unique structure and characteristics, making it suitable for specific tasks. It's crucial to gain a thorough understanding and perform a comparative analysis of these different CNN types to understand their strengths and weaknesses. Furthermore, studying the performance, limitations, and practical applications of each type of CNN can aid in the development of new and improved architectures in the future. We also dive into the platforms and frameworks that researchers utilize for their research or development from various perspectives. Additionally, we explore the main research fields of CNN like 6D vision, generative models, and meta-learning. This survey paper provides a comprehensive examination and comparison of various CNN architectures, highlighting their architectural differences and emphasizing their respective advantages, disadvantages, applications, challenges, and future trends.</li>
</ul>

<h3>Title: Gen4Gen: Generative Data Pipeline for Generative Multi-Concept  Composition</h3>
<ul>
<li><strong>Authors: </strong>Chun-Hsiao Yeh, Ta-Ying Cheng, He-Yen Hsieh, Chuan-En Lin, Yi Ma, Andrew Markham, Niki Trigoni, H.T. Kung, Yubei Chen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15504">https://arxiv.org/abs/2402.15504</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15504">https://arxiv.org/pdf/2402.15504</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15504]] Gen4Gen: Generative Data Pipeline for Generative Multi-Concept  Composition(https://arxiv.org/abs/2402.15504)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Recent text-to-image diffusion models are able to learn and synthesize images containing novel, personalized concepts (e.g., their own pets or specific items) with just a few examples for training. This paper tackles two interconnected issues within this realm of personalizing text-to-image diffusion models. First, current personalization techniques fail to reliably extend to multiple concepts -- we hypothesize this to be due to the mismatch between complex scenes and simple text descriptions in the pre-training dataset (e.g., LAION). Second, given an image containing multiple personalized concepts, there lacks a holistic metric that evaluates performance on not just the degree of resemblance of personalized concepts, but also whether all concepts are present in the image and whether the image accurately reflects the overall text description. To address these issues, we introduce Gen4Gen, a semi-automated dataset creation pipeline utilizing generative models to combine personalized concepts into complex compositions along with text-descriptions. Using this, we create a dataset called MyCanvas, that can be used to benchmark the task of multi-concept personalization. In addition, we design a comprehensive metric comprising two scores (CP-CLIP and TI-CLIP) for better quantifying the performance of multi-concept, personalized text-to-image diffusion methods. We provide a simple baseline built on top of Custom Diffusion with empirical prompting strategies for future researchers to evaluate on MyCanvas. We show that by improving data quality and prompting strategies, we can significantly increase multi-concept personalized image generation quality, without requiring any modifications to model architecture or training algorithms.</li>
</ul>

<h3>Title: Seamless Human Motion Composition with Blended Positional Encodings</h3>
<ul>
<li><strong>Authors: </strong>German Barquero, Sergio Escalera, Cristina Palmero</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2402.15509">https://arxiv.org/abs/2402.15509</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2402.15509">https://arxiv.org/pdf/2402.15509</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2402.15509]] Seamless Human Motion Composition with Blended Positional Encodings(https://arxiv.org/abs/2402.15509)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Conditional human motion generation is an important topic with many applications in virtual reality, gaming, and robotics. While prior works have focused on generating motion guided by text, music, or scenes, these typically result in isolated motions confined to short durations. Instead, we address the generation of long, continuous sequences guided by a series of varying textual descriptions. In this context, we introduce FlowMDM, the first diffusion-based model that generates seamless Human Motion Compositions (HMC) without any postprocessing or redundant denoising steps. For this, we introduce the Blended Positional Encodings, a technique that leverages both absolute and relative positional encodings in the denoising chain. More specifically, global motion coherence is recovered at the absolute stage, whereas smooth and realistic transitions are built at the relative stage. As a result, we achieve state-of-the-art results in terms of accuracy, realism, and smoothness on the Babel and HumanML3D datasets. FlowMDM excels when trained with only a single description per motion sequence thanks to its Pose-Centric Cross-ATtention, which makes it robust against varying text descriptions at inference time. Finally, to address the limitations of existing HMC metrics, we propose two new metrics: the Peak Jerk and the Area Under the Jerk, to detect abrupt transitions.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
