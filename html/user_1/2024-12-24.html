<link rel="stylesheet" href="../../css/markdown.css" />
<article class="markdown-body">
<h1>2024-12-24</h1>
<h3>Title: Context Clues: Evaluating Long Context Models for Clinical Prediction Tasks on EHRs</h3>
<ul>
<li><strong>Authors: </strong>Michael Wornow, Suhana Bedi, Miguel Angel Fuentes Hernandez, Ethan Steinberg, Jason Alan Fries, Christopher RÃ©, Sanmi Koyejo, Nigam H. Shah</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16178">https://arxiv.org/abs/2412.16178</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16178">https://arxiv.org/pdf/2412.16178</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16178]] Context Clues: Evaluating Long Context Models for Clinical Prediction Tasks on EHRs(https://arxiv.org/abs/2412.16178)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Foundation Models (FMs) trained on Electronic Health Records (EHRs) have achieved state-of-the-art results on numerous clinical prediction tasks. However, most existing EHR FMs have context windows of <1k tokens. This prevents them from modeling full patient EHRs which can exceed 10k's of events. Recent advancements in subquadratic long-context architectures (e.g., Mamba) offer a promising solution. However, their application to EHR data has not been well-studied. We address this gap by presenting the first systematic evaluation of the effect of context length on modeling EHR data. We find that longer context models improve predictive performance -- our Mamba-based model surpasses the prior state-of-the-art on 9/14 tasks on the EHRSHOT prediction benchmark. For clinical applications, however, model performance alone is insufficient -- robustness to the unique properties of EHR is crucial. Thus, we also evaluate models across three previously underexplored properties of EHR data: (1) the prevalence of "copy-forwarded" diagnoses which creates artificial repetition of tokens within EHR sequences; (2) the irregular time intervals between EHR events which can lead to a wide range of timespans within a context window; and (3) the natural increase in disease complexity over time which makes later tokens in the EHR harder to predict than earlier ones. Stratifying our EHRSHOT results, we find that higher levels of each property correlate negatively with model performance, but that longer context models are more robust to more extreme levels of these properties. Our work highlights the potential for using long-context architectures to model EHR data, and offers a case study for identifying new challenges in modeling sequential data motivated by domains outside of natural language. We release our models and code at: this https URL</li>
</ul>

<h3>Title: A Decade of Deep Learning: A Survey on The Magnificent Seven</h3>
<ul>
<li><strong>Authors: </strong>Dilshod Azizov, Muhammad Arslan Manzoor, Velibor Bojkovic, Yingxu Wang, Zixiao Wang, Zangir Iklassov, Kailong Zhao, Liang Li, Siwei Liu, Yu Zhong, Wei Liu, Shangsong Liang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16188">https://arxiv.org/abs/2412.16188</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16188">https://arxiv.org/pdf/2412.16188</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16188]] A Decade of Deep Learning: A Survey on The Magnificent Seven(https://arxiv.org/abs/2412.16188)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Deep learning has fundamentally reshaped the landscape of artificial intelligence over the past decade, enabling remarkable achievements across diverse domains. At the heart of these developments lie multi-layered neural network architectures that excel at automatic feature extraction, leading to significant improvements in machine learning tasks. To demystify these advances and offer accessible guidance, we present a comprehensive overview of the most influential deep learning algorithms selected through a broad-based survey of the field. Our discussion centers on pivotal architectures, including Residual Networks, Transformers, Generative Adversarial Networks, Variational Autoencoders, Graph Neural Networks, Contrastive Language-Image Pre-training, and Diffusion models. We detail their historical context, highlight their mathematical foundations and algorithmic principles, and examine subsequent variants, extensions, and practical considerations such as training methodologies, normalization techniques, and learning rate schedules. Beyond historical and technical insights, we also address their applications, challenges, and potential research directions. This survey aims to serve as a practical manual for both newcomers seeking an entry point into cutting-edge deep learning methods and experienced researchers transitioning into this rapidly evolving domain.</li>
</ul>

<h3>Title: Robust Spectral Anomaly Detection in EELS Spectral Images via Three Dimensional Convolutional Variational Autoencoders</h3>
<ul>
<li><strong>Authors: </strong>Seyfal Sultanov, James P Buban, Robert F Klie</a></li>
<li><strong>Subjects: </strong>cs.CV, cond-mat.mtrl-sci, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16200">https://arxiv.org/abs/2412.16200</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16200">https://arxiv.org/pdf/2412.16200</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16200]] Robust Spectral Anomaly Detection in EELS Spectral Images via Three Dimensional Convolutional Variational Autoencoders(https://arxiv.org/abs/2412.16200)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>We introduce a Three-Dimensional Convolutional Variational Autoencoder (3D-CVAE) for automated anomaly detection in Electron Energy Loss Spectroscopy Spectrum Imaging (EELS-SI) data. Our approach leverages the full three-dimensional structure of EELS-SI data to detect subtle spectral anomalies while preserving both spatial and spectral correlations across the datacube. By employing negative log-likelihood loss and training on bulk spectra, the model learns to reconstruct bulk features characteristic of the defect-free material. In exploring methods for anomaly detection, we evaluated both our 3D-CVAE approach and Principal Component Analysis (PCA), testing their performance using Fe L-edge peak shifts designed to simulate material defects. Our results show that 3D-CVAE achieves superior anomaly detection and maintains consistent performance across various shift magnitudes. The method demonstrates clear bimodal separation between normal and anomalous spectra, enabling reliable classification. Further analysis verifies that lower dimensional representations are robust to anomalies in the data. While performance advantages over PCA diminish with decreasing anomaly concentration, our method maintains high reconstruction quality even in challenging, noise-dominated spectral regions. This approach provides a robust framework for unsupervised automated detection of spectral anomalies in EELS-SI data, particularly valuable for analyzing complex material systems.</li>
</ul>

<h3>Title: Synthetic Time Series Data Generation for Healthcare Applications: A PCG Case Study</h3>
<ul>
<li><strong>Authors: </strong>Ainaz Jamshidi, Muhammad Arif, Sabir Ali Kalhoro, Alexander Gelbukh</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CE, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16207">https://arxiv.org/abs/2412.16207</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16207">https://arxiv.org/pdf/2412.16207</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16207]] Synthetic Time Series Data Generation for Healthcare Applications: A PCG Case Study(https://arxiv.org/abs/2412.16207)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The generation of high-quality medical time series data is essential for advancing healthcare diagnostics and safeguarding patient privacy. Specifically, synthesizing realistic phonocardiogram (PCG) signals offers significant potential as a cost-effective and efficient tool for cardiac disease pre-screening. Despite its potential, the synthesis of PCG signals for this specific application received limited attention in research. In this study, we employ and compare three state-of-the-art generative models from different categories - WaveNet, DoppelGANger, and DiffWave - to generate high-quality PCG data. We use data from the George B. Moody PhysioNet Challenge 2022. Our methods are evaluated using various metrics widely used in the previous literature in the domain of time series data generation, such as mean absolute error and maximum mean discrepancy. Our results demonstrate that the generated PCG data closely resembles the original datasets, indicating the effectiveness of our generative models in producing realistic synthetic PCG data. In our future work, we plan to incorporate this method into a data augmentation pipeline to synthesize abnormal PCG signals with heart murmurs, in order to address the current scarcity of abnormal data. We hope to improve the robustness and accuracy of diagnostic tools in cardiology, enhancing their effectiveness in detecting heart murmurs.</li>
</ul>

<h3>Title: Is Your World Simulator a Good Story Presenter? A Consecutive Events-Based Benchmark for Future Long Video Generation</h3>
<ul>
<li><strong>Authors: </strong>Yiping Wang, Xuehai He, Kuan Wang, Luyao Ma, Jianwei Yang, Shuohang Wang, Simon Shaolei Du, Yelong Shen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.CL, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16211">https://arxiv.org/abs/2412.16211</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16211">https://arxiv.org/pdf/2412.16211</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16211]] Is Your World Simulator a Good Story Presenter? A Consecutive Events-Based Benchmark for Future Long Video Generation(https://arxiv.org/abs/2412.16211)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The current state-of-the-art video generative models can produce commercial-grade videos with highly realistic details. However, they still struggle to coherently present multiple sequential events in the stories specified by the prompts, which is foreseeable an essential capability for future long video generation scenarios. For example, top T2V generative models still fail to generate a video of the short simple story 'how to put an elephant into a refrigerator.' While existing detail-oriented benchmarks primarily focus on fine-grained metrics like aesthetic quality and spatial-temporal consistency, they fall short of evaluating models' abilities to handle event-level story presentation. To address this gap, we introduce StoryEval, a story-oriented benchmark specifically designed to assess text-to-video (T2V) models' story-completion capabilities. StoryEval features 423 prompts spanning 7 classes, each representing short stories composed of 2-4 consecutive events. We employ advanced vision-language models, such as GPT-4V and LLaVA-OV-Chat-72B, to verify the completion of each event in the generated videos, applying a unanimous voting method to enhance reliability. Our methods ensure high alignment with human evaluations, and the evaluation of 11 models reveals its challenge, with none exceeding an average story-completion rate of 50%. StoryEval provides a new benchmark for advancing T2V models and highlights the challenges and opportunities in developing next-generation solutions for coherent story-driven video generation.</li>
</ul>

<h3>Title: AdvIRL: Reinforcement Learning-Based Adversarial Attacks on 3D NeRF Models</h3>
<ul>
<li><strong>Authors: </strong>Tommy Nguyen, Mehmet Ergezer, Christian Green</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CY, cs.GR, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16213">https://arxiv.org/abs/2412.16213</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16213">https://arxiv.org/pdf/2412.16213</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16213]] AdvIRL: Reinforcement Learning-Based Adversarial Attacks on 3D NeRF Models(https://arxiv.org/abs/2412.16213)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The increasing deployment of AI models in critical applications has exposed them to significant risks from adversarial attacks. While adversarial vulnerabilities in 2D vision models have been extensively studied, the threat landscape for 3D generative models, such as Neural Radiance Fields (NeRF), remains underexplored. This work introduces \textit{AdvIRL}, a novel framework for crafting adversarial NeRF models using Instant Neural Graphics Primitives (Instant-NGP) and Reinforcement Learning. Unlike prior methods, \textit{AdvIRL} generates adversarial noise that remains robust under diverse 3D transformations, including rotations and scaling, enabling effective black-box attacks in real-world scenarios. Our approach is validated across a wide range of scenes, from small objects (e.g., bananas) to large environments (e.g., lighthouses). Notably, targeted attacks achieved high-confidence misclassifications, such as labeling a banana as a slug and a truck as a cannon, demonstrating the practical risks posed by adversarial NeRFs. Beyond attacking, \textit{AdvIRL}-generated adversarial models can serve as adversarial training data to enhance the robustness of vision systems. The implementation of \textit{AdvIRL} is publicly available at \url{this https URL}, ensuring reproducibility and facilitating future research.</li>
</ul>

<h3>Title: GALOT: Generative Active Learning via Optimizable Zero-shot Text-to-image Generation</h3>
<ul>
<li><strong>Authors: </strong>Hanbin Hong, Shenao Yan, Shuya Feng, Yan Yan, Yuan Hong</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16227">https://arxiv.org/abs/2412.16227</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16227">https://arxiv.org/pdf/2412.16227</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16227]] GALOT: Generative Active Learning via Optimizable Zero-shot Text-to-image Generation(https://arxiv.org/abs/2412.16227)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Active Learning (AL) represents a crucial methodology within machine learning, emphasizing the identification and utilization of the most informative samples for efficient model training. However, a significant challenge of AL is its dependence on the limited labeled data samples and data distribution, resulting in limited performance. To address this limitation, this paper integrates the zero-shot text-to-image (T2I) synthesis and active learning by designing a novel framework that can efficiently train a machine learning (ML) model sorely using the text description. Specifically, we leverage the AL criteria to optimize the text inputs for generating more informative and diverse data samples, annotated by the pseudo-label crafted from text, then served as a synthetic dataset for active learning. This approach reduces the cost of data collection and annotation while increasing the efficiency of model training by providing informative training samples, enabling a novel end-to-end ML task from text description to vision models. Through comprehensive evaluations, our framework demonstrates consistent and significant improvements over traditional AL methods.</li>
</ul>

<h3>Title: Towards scientific discovery with dictionary learning: Extracting biological concepts from microscopy foundation models</h3>
<ul>
<li><strong>Authors: </strong>Konstantin Donhauser, Kristina Ulicna, Gemma Elyse Moran, Aditya Ravuri, Kian Kenyon-Dean, Cian Eastwood, Jason Hartford</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16247">https://arxiv.org/abs/2412.16247</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16247">https://arxiv.org/pdf/2412.16247</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16247]] Towards scientific discovery with dictionary learning: Extracting biological concepts from microscopy foundation models(https://arxiv.org/abs/2412.16247)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Dictionary learning (DL) has emerged as a powerful interpretability tool for large language models. By extracting known concepts (e.g., Golden-Gate Bridge) from human-interpretable data (e.g., text), sparse DL can elucidate a model's inner workings. In this work, we ask if DL can also be used to discover unknown concepts from less human-interpretable scientific data (e.g., cell images), ultimately enabling modern approaches to scientific discovery. As a first step, we use DL algorithms to study microscopy foundation models trained on multi-cell image data, where little prior knowledge exists regarding which high-level concepts should arise. We show that sparse dictionaries indeed extract biologically-meaningful concepts such as cell type and genetic perturbation type. We also propose a new DL algorithm, Iterative Codebook Feature Learning~(ICFL), and combine it with a pre-processing step that uses PCA whitening from a control dataset. In our experiments, we demonstrate that both ICFL and PCA improve the selectivity of extracted features compared to TopK sparse autoencoders.</li>
</ul>

<h3>Title: Interactive Scene Authoring with Specialized Generative Primitives</h3>
<ul>
<li><strong>Authors: </strong>ClÃ©ment Jambon (1), Changwoon Choi (2), Dongsu Zhang (2), Olga Sorkine-Hornung (1), Young Min Kim (2) ((1) ETH Zurich, (2) Seoul National University)</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16253">https://arxiv.org/abs/2412.16253</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16253">https://arxiv.org/pdf/2412.16253</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16253]] Interactive Scene Authoring with Specialized Generative Primitives(https://arxiv.org/abs/2412.16253)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generating high-quality 3D digital assets often requires expert knowledge of complex design tools. We introduce Specialized Generative Primitives, a generative framework that allows non-expert users to author high-quality 3D scenes in a seamless, lightweight, and controllable manner. Each primitive is an efficient generative model that captures the distribution of a single exemplar from the real world. With our framework, users capture a video of an environment, which we turn into a high-quality and explicit appearance model thanks to 3D Gaussian Splatting. Users then select regions of interest guided by semantically-aware features. To create a generative primitive, we adapt Generative Cellular Automata to single-exemplar training and controllable generation. We decouple the generative task from the appearance model by operating on sparse voxels and we recover a high-quality output with a subsequent sparse patch consistency step. Each primitive can be trained within 10 minutes and used to author new scenes interactively in a fully compositional manner. We showcase interactive sessions where various primitives are extracted from real-world scenes and controlled to create 3D assets and scenes in a few minutes. We also demonstrate additional capabilities of our primitives: handling various 3D representations to control generation, transferring appearances, and editing geometries.</li>
</ul>

<h3>Title: PromptLA: Towards Integrity Verification of Black-box Text-to-Image Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Zhuomeng Zhang, Fangqi Li, Chong Di, Shilin Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16257">https://arxiv.org/abs/2412.16257</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16257">https://arxiv.org/pdf/2412.16257</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16257]] PromptLA: Towards Integrity Verification of Black-box Text-to-Image Diffusion Models(https://arxiv.org/abs/2412.16257)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Current text-to-image (T2I) diffusion models can produce high-quality images, and malicious users who are authorized to use the model only for benign purposes might modify their models to generate images that result in harmful social impacts. Therefore, it is essential to verify the integrity of T2I diffusion models, especially when they are deployed as black-box services. To this end, considering the randomness within the outputs of generative models and the high costs in interacting with them, we capture modifications to the model through the differences in the distributions of the features of generated images. We propose a novel prompt selection algorithm based on learning automaton for efficient and accurate integrity verification of T2I diffusion models. Extensive experiments demonstrate the effectiveness, stability, accuracy and generalization of our algorithm against existing integrity violations compared with baselines. To the best of our knowledge, this paper is the first work addressing the integrity verification of T2I diffusion models, which paves the way to copyright discussions and protections for artificial intelligence applications in practice.</li>
</ul>

<h3>Title: LEARN: A Unified Framework for Multi-Task Domain Adapt Few-Shot Learning</h3>
<ul>
<li><strong>Authors: </strong>Bharadwaj Ravichandran, Alexander Lynch, Sarah Brockman, Brandon RichardWebster, Dawei Du, Anthony Hoogs, Christopher Funk</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16275">https://arxiv.org/abs/2412.16275</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16275">https://arxiv.org/pdf/2412.16275</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16275]] LEARN: A Unified Framework for Multi-Task Domain Adapt Few-Shot Learning(https://arxiv.org/abs/2412.16275)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Both few-shot learning and domain adaptation sub-fields in Computer Vision have seen significant recent progress in terms of the availability of state-of-the-art algorithms and datasets. Frameworks have been developed for each sub-field; however, building a common system or framework that combines both is something that has not been explored. As part of our research, we present the first unified framework that combines domain adaptation for the few-shot learning setting across 3 different tasks - image classification, object detection and video classification. Our framework is highly modular with the capability to support few-shot learning with/without the inclusion of domain adaptation depending on the algorithm. Furthermore, the most important configurable feature of our framework is the on-the-fly setup for incremental $n$-shot tasks with the optional capability to configure the system to scale to a traditional many-shot task. With more focus on Self-Supervised Learning (SSL) for current few-shot learning approaches, our system also supports multiple SSL pre-training configurations. To test our framework's capabilities, we provide benchmarks on a wide range of algorithms and datasets across different task and problem settings. The code is open source has been made publicly available here: this https URL</li>
</ul>

<h3>Title: When Worse is Better: Navigating the compression-generation tradeoff in visual tokenization</h3>
<ul>
<li><strong>Authors: </strong>Vivek Ramanujan, Kushal Tirumala, Armen Aghajanyan, Luke Zettlemoyer, Ali Farhadi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16326">https://arxiv.org/abs/2412.16326</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16326">https://arxiv.org/pdf/2412.16326</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16326]] When Worse is Better: Navigating the compression-generation tradeoff in visual tokenization(https://arxiv.org/abs/2412.16326)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Current image generation methods, such as latent diffusion and discrete token-based generation, depend on a two-stage training approach. In stage 1, an auto-encoder is trained to compress an image into a latent space; in stage 2, a generative model is trained to learn a distribution over that latent space. Most work focuses on maximizing stage 1 performance independent of stage 2, assuming better reconstruction always leads to better generation. However, we show this is not strictly true. Smaller stage 2 models can benefit from more compressed stage 1 latents even if reconstruction performance worsens, showing a fundamental trade-off between compression and generation modeling capacity. To better optimize this trade-off, we introduce Causally Regularized Tokenization (CRT), which uses knowledge of the stage 2 generation modeling procedure to embed useful inductive biases in stage 1 latents. This regularization makes stage 1 reconstruction performance worse, but makes stage 2 generation performance better by making the tokens easier to model: we are able to improve compute efficiency 2-3$\times$ over baseline and match state-of-the-art discrete autoregressive ImageNet generation (2.18 FID) with less than half the tokens per image (256 vs. 576) and a fourth the total model parameters (775M vs. 3.1B) as the previous SOTA (LlamaGen).</li>
</ul>

<h3>Title: DINOv2 Meets Text: A Unified Framework for Image- and Pixel-Level Vision-Language Alignment</h3>
<ul>
<li><strong>Authors: </strong>Cijo Jose, ThÃ©o Moutakanni, Dahyun Kang, Federico Baldassarre, TimothÃ©e Darcet, Hu Xu, Daniel Li, Marc Szafraniec, MichaÃ«l Ramamonjisoa, Maxime Oquab, Oriane SimÃ©oni, Huy V. Vo, Patrick Labatut, Piotr Bojanowski</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16334">https://arxiv.org/abs/2412.16334</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16334">https://arxiv.org/pdf/2412.16334</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16334]] DINOv2 Meets Text: A Unified Framework for Image- and Pixel-Level Vision-Language Alignment(https://arxiv.org/abs/2412.16334)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised, foundation model</a></li>
<li><strong>Abstract: </strong>Self-supervised visual foundation models produce powerful embeddings that achieve remarkable performance on a wide range of downstream tasks. However, unlike vision-language models such as CLIP, self-supervised visual features are not readily aligned with language, hindering their adoption in open-vocabulary tasks. Our method, named this http URL, unlocks this new ability for DINOv2, a widely used self-supervised visual encoder. We build upon the LiT training strategy, which trains a text encoder to align with a frozen vision model but leads to unsatisfactory results on dense tasks. We propose several key ingredients to improve performance on both global and dense tasks, such as concatenating the [CLS] token with the patch average to train the alignment and curating data using both text and image modalities. With these, we successfully train a CLIP-like model with only a fraction of the computational cost compared to CLIP while achieving state-of-the-art results in zero-shot classification and open-vocabulary semantic segmentation.</li>
</ul>

<h3>Title: Iterative Encoding-Decoding VAEs Anomaly Detection in NOAA's DART Time Series: A Machine Learning Approach for Enhancing Data Integrity for NASA's GRACE-FO Verification and Validation</h3>
<ul>
<li><strong>Authors: </strong>Kevin Lee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.geo-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16375">https://arxiv.org/abs/2412.16375</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16375">https://arxiv.org/pdf/2412.16375</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16375]] Iterative Encoding-Decoding VAEs Anomaly Detection in NOAA's DART Time Series: A Machine Learning Approach for Enhancing Data Integrity for NASA's GRACE-FO Verification and Validation(https://arxiv.org/abs/2412.16375)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>NOAA's Deep-ocean Assessment and Reporting of Tsunamis (DART) data are critical for NASA-JPL's tsunami detection, real-time operations, and oceanographic research. However, these time-series data often contain spikes, steps, and drifts that degrade data quality and obscure essential oceanographic features. To address these anomalies, the work introduces an Iterative Encoding-Decoding Variational Autoencoders (Iterative Encoding-Decoding VAEs) model to improve the quality of DART time series. Unlike traditional filtering and thresholding methods that risk distorting inherent signal characteristics, Iterative Encoding-Decoding VAEs progressively remove anomalies while preserving the data's latent structure. A hybrid thresholding approach further retains genuine oceanographic features near boundaries. Applied to complex DART datasets, this approach yields reconstructions that better maintain key oceanic properties compared to classical statistical techniques, offering improved robustness against spike removal and subtle step changes. The resulting high-quality data supports critical verification and validation efforts for the GRACE-FO mission at NASA-JPL, where accurate surface measurements are essential to modeling Earth's gravitational field and global water dynamics. Ultimately, this data processing method enhances tsunami detection and underpins future climate modeling with improved interpretability and reliability.</li>
</ul>

<h3>Title: Learning Cross-Task Generalities Across Graphs via Task-trees</h3>
<ul>
<li><strong>Authors: </strong>Zehong Wang, Zheyuan Zhang, Tianyi Ma, Nitesh V Chawla, Chuxu Zhang, Yanfang Ye</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.SI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16441">https://arxiv.org/abs/2412.16441</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16441">https://arxiv.org/pdf/2412.16441</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16441]] Learning Cross-Task Generalities Across Graphs via Task-trees(https://arxiv.org/abs/2412.16441)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model, in-context</a></li>
<li><strong>Abstract: </strong>Foundation models aim to create general, cross-task, and cross-domain machine learning models by pretraining on large-scale datasets to capture shared patterns or concepts (generalities), such as contours, colors, textures, and edges in images, or tokens, words, and sentences in text. However, discovering generalities across graphs remains challenging, which has hindered the development of graph foundation models. To tackle this challenge, in this paper, we propose a novel approach to learn generalities across graphs via task-trees. Specifically, we first define the basic learning instances in graphs as task-trees and assume that the generalities shared across graphs are, at least partially, preserved in the task-trees of the given graphs. To validate the assumption, we first perform a theoretical analysis of task-trees in terms of stability, transferability, and generalization. We find that if a graph neural network (GNN) model is pretrained on diverse task-trees through a reconstruction task, it can learn sufficient transferable knowledge for downstream tasks using an appropriate set of fine-tuning samples. To empirically validate the assumption, we further instantiate the theorems by developing a cross-task, cross-domain graph foundation model named Graph generality Identifier on task-Trees (GIT). The extensive experiments over 30 graphs from five domains demonstrate the effectiveness of GIT in fine-tuning, in-context learning, and zero-shot learning scenarios. Particularly, the general GIT model pretrained on large-scale datasets can be quickly adapted to specific domains, matching or even surpassing expert models designed for those domains. Our data and code are available at this https URL.</li>
</ul>

<h3>Title: A Generalizable Anomaly Detection Method in Dynamic Graphs</h3>
<ul>
<li><strong>Authors: </strong>Xiao Yang, Xuejiao Zhao, Zhiqi Shen</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16447">https://arxiv.org/abs/2412.16447</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16447">https://arxiv.org/pdf/2412.16447</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16447]] A Generalizable Anomaly Detection Method in Dynamic Graphs(https://arxiv.org/abs/2412.16447)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Anomaly detection aims to identify deviations from normal patterns within data. This task is particularly crucial in dynamic graphs, which are common in applications like social networks and cybersecurity, due to their evolving structures and complex relationships. Although recent deep learning-based methods have shown promising results in anomaly detection on dynamic graphs, they often lack of generalizability. In this study, we propose GeneralDyG, a method that samples temporal ego-graphs and sequentially extracts structural and temporal features to address the three key challenges in achieving generalizability: Data Diversity, Dynamic Feature Capture, and Computational Cost. Extensive experimental results demonstrate that our proposed GeneralDyG significantly outperforms state-of-the-art methods on four real-world datasets.</li>
</ul>

<h3>Title: Positive2Negative: Breaking the Information-Lossy Barrier in Self-Supervised Single Image Denoising</h3>
<ul>
<li><strong>Authors: </strong>Tong Li, Lizhi Wang, Zhiyuan Xu, Lin Zhu, Wanxuan Lu, Hua Huang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16460">https://arxiv.org/abs/2412.16460</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16460">https://arxiv.org/pdf/2412.16460</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16460]] Positive2Negative: Breaking the Information-Lossy Barrier in Self-Supervised Single Image Denoising(https://arxiv.org/abs/2412.16460)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Image denoising enhances image quality, serving as a foundational technique across various computational photography applications. The obstacle to clean image acquisition in real scenarios necessitates the development of self-supervised image denoising methods only depending on noisy images, especially a single noisy image. Existing self-supervised image denoising paradigms (Noise2Noise and Noise2Void) rely heavily on information-lossy operations, such as downsampling and masking, culminating in low quality denoising performance. In this paper, we propose a novel self-supervised single image denoising paradigm, Positive2Negative, to break the information-lossy barrier. Our paradigm involves two key steps: Renoised Data Construction (RDC) and Denoised Consistency Supervision (DCS). RDC renoises the predicted denoised image by the predicted noise to construct multiple noisy images, preserving all the information of the original image. DCS ensures consistency across the multiple denoised images, supervising the network to learn robust denoising. Our Positive2Negative paradigm achieves state-of-the-art performance in self-supervised single image denoising with significant speed improvements. The code will be released to the public.</li>
</ul>

<h3>Title: Enhancing Nighttime Vehicle Detection with Day-to-Night Style Transfer and Labeling-Free Augmentation</h3>
<ul>
<li><strong>Authors: </strong>Yunxiang Yang, Hao Zhen, Yongcan Huang, Jidong J. Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16478">https://arxiv.org/abs/2412.16478</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16478">https://arxiv.org/pdf/2412.16478</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16478]] Enhancing Nighttime Vehicle Detection with Day-to-Night Style Transfer and Labeling-Free Augmentation(https://arxiv.org/abs/2412.16478)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Existing deep learning-based object detection models perform well under daytime conditions but face significant challenges at night, primarily because they are predominantly trained on daytime images. Additionally, training with nighttime images presents another challenge: even human annotators struggle to accurately label objects in low-light conditions. This issue is particularly pronounced in transportation applications, such as detecting vehicles and other objects of interest on rural roads at night, where street lighting is often absent, and headlights may introduce undesirable glare. This study addresses these challenges by introducing a novel framework for labeling-free data augmentation, leveraging CARLA-generated synthetic data for day-to-night image style transfer. Specifically, the framework incorporates the Efficient Attention Generative Adversarial Network for realistic day-to-night style transfer and uses CARLA-generated synthetic nighttime images to help the model learn vehicle headlight effects. To evaluate the efficacy of the proposed framework, we fine-tuned the YOLO11 model with an augmented dataset specifically curated for rural nighttime environments, achieving significant improvements in nighttime vehicle detection. This novel approach is simple yet effective, offering a scalable solution to enhance AI-based detection systems in low-visibility environments and extend the applicability of object detection models to broader real-world contexts.</li>
</ul>

<h3>Title: MOL-Mamba: Enhancing Molecular Representation with Structural & Electronic Insights</h3>
<ul>
<li><strong>Authors: </strong>Jingjing Hu, Dan Guo, Zhan Si, Deguang Liu, Yunfeng Diao, Jing Zhang, Jinxing Zhou, Meng Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, physics.chem-ph, q-bio.BM</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16483">https://arxiv.org/abs/2412.16483</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16483">https://arxiv.org/pdf/2412.16483</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16483]] MOL-Mamba: Enhancing Molecular Representation with Structural & Electronic Insights(https://arxiv.org/abs/2412.16483)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Molecular representation learning plays a crucial role in various downstream tasks, such as molecular property prediction and drug design. To accurately represent molecules, Graph Neural Networks (GNNs) and Graph Transformers (GTs) have shown potential in the realm of self-supervised pretraining. However, existing approaches often overlook the relationship between molecular structure and electronic information, as well as the internal semantic reasoning within molecules. This omission of fundamental chemical knowledge in graph semantics leads to incomplete molecular representations, missing the integration of structural and electronic data. To address these issues, we introduce MOL-Mamba, a framework that enhances molecular representation by combining structural and electronic insights. MOL-Mamba consists of an Atom & Fragment Mamba-Graph (MG) for hierarchical structural reasoning and a Mamba-Transformer (MT) fuser for integrating molecular structure and electronic correlation learning. Additionally, we propose a Structural Distribution Collaborative Training and E-semantic Fusion Training framework to further enhance molecular representation learning. Extensive experiments demonstrate that MOL-Mamba outperforms state-of-the-art baselines across eleven chemical-biological molecular datasets. Code is available at this https URL.</li>
</ul>

<h3>Title: TrojFlow: Flow Models are Natural Targets for Trojan Attacks</h3>
<ul>
<li><strong>Authors: </strong>Zhengyang Qi, Xiaohua Xu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16512">https://arxiv.org/abs/2412.16512</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16512">https://arxiv.org/pdf/2412.16512</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16512]] TrojFlow: Flow Models are Natural Targets for Trojan Attacks(https://arxiv.org/abs/2412.16512)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Flow-based generative models (FMs) have rapidly advanced as a method for mapping noise to data, its efficient training and sampling process makes it widely applicable in various fields. FMs can be viewed as a variant of diffusion models (DMs). At the same time, previous studies have shown that DMs are vulnerable to Trojan/Backdoor attacks, a type of output manipulation attack triggered by a maliciously embedded pattern at model input. We found that Trojan attacks on generative models are essentially equivalent to image transfer tasks from the backdoor distribution to the target distribution, the unique ability of FMs to fit any two arbitrary distributions significantly simplifies the training and sampling setups for attacking FMs, making them inherently natural targets for backdoor attacks. In this paper, we propose TrojFlow, exploring the vulnerabilities of FMs through Trojan attacks. In particular, we consider various attack settings and their combinations and thoroughly explore whether existing defense methods for DMs can effectively defend against our proposed attack scenarios. We evaluate TrojFlow on CIFAR-10 and CelebA datasets, our experiments show that our method can compromise FMs with high utility and specificity, and can easily break through existing defense mechanisms.</li>
</ul>

<h3>Title: Enhancing Contrastive Learning Inspired by the Philosophy of "The Blind Men and the Elephant"</h3>
<ul>
<li><strong>Authors: </strong>Yudong Zhang, Ruobing Xie, Jiansheng Chen, Xingwu Sun, Zhanhui Kang, Yu Wang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16522">https://arxiv.org/abs/2412.16522</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16522">https://arxiv.org/pdf/2412.16522</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16522]] Enhancing Contrastive Learning Inspired by the Philosophy of "The Blind Men and the Elephant"(https://arxiv.org/abs/2412.16522)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Contrastive learning is a prevalent technique in self-supervised vision representation learning, typically generating positive pairs by applying two data augmentations to the same image. Designing effective data augmentation strategies is crucial for the success of contrastive learning. Inspired by the story of the blind men and the elephant, we introduce JointCrop and JointBlur. These methods generate more challenging positive pairs by leveraging the joint distribution of the two augmentation parameters, thereby enabling contrastive learning to acquire more effective feature representations. To the best of our knowledge, this is the first effort to explicitly incorporate the joint distribution of two data augmentation parameters into contrastive learning. As a plug-and-play framework without additional computational overhead, JointCrop and JointBlur enhance the performance of SimCLR, BYOL, MoCo v1, MoCo v2, MoCo v3, SimSiam, and Dino baselines with notable improvements.</li>
</ul>

<h3>Title: Diffusion Prior Interpolation for Flexibility Real-World Face Super-Resolution</h3>
<ul>
<li><strong>Authors: </strong>Jiarui Yang, Tao Dai, Yufei Zhu, Naiqi Li, Jinmin Li, Shutao Xia</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16552">https://arxiv.org/abs/2412.16552</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16552">https://arxiv.org/pdf/2412.16552</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16552]] Diffusion Prior Interpolation for Flexibility Real-World Face Super-Resolution(https://arxiv.org/abs/2412.16552)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion models represent the state-of-the-art in generative modeling. Due to their high training costs, many works leverage pre-trained diffusion models' powerful representations for downstream tasks, such as face super-resolution (FSR), through fine-tuning or prior-based methods. However, relying solely on priors without supervised training makes it challenging to meet the pixel-level accuracy requirements of discrimination task. Although prior-based methods can achieve high fidelity and high-quality results, ensuring consistency remains a significant challenge. In this paper, we propose a masking strategy with strong and weak constraints and iterative refinement for real-world FSR, termed Diffusion Prior Interpolation (DPI). We introduce conditions and constraints on consistency by masking different sampling stages based on the structural characteristics of the face. Furthermore, we propose a condition Corrector (CRT) to establish a reciprocal posterior sampling process, enhancing FSR performance by mutual refinement of conditions and samples. DPI can balance consistency and diversity and can be seamlessly integrated into pre-trained models. In extensive experiments conducted on synthetic and real datasets, along with consistency validation in face recognition, DPI demonstrates superiority over SOTA FSR methods. The code is available at \url{this https URL}.</li>
</ul>

<h3>Title: Learning for Cross-Layer Resource Allocation in MEC-Aided Cell-Free Networks</h3>
<ul>
<li><strong>Authors: </strong>Chong Zheng, Shiwen He, Yongming Huang, Tony Q. S. Quek</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16565">https://arxiv.org/abs/2412.16565</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16565">https://arxiv.org/pdf/2412.16565</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16565]] Learning for Cross-Layer Resource Allocation in MEC-Aided Cell-Free Networks(https://arxiv.org/abs/2412.16565)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Cross-layer resource allocation over mobile edge computing (MEC)-aided cell-free networks can sufficiently exploit the transmitting and computing resources to promote the data rate. However, the technical bottlenecks of traditional methods pose significant challenges to cross-layer optimization. In this paper, joint subcarrier allocation and beamforming optimization are investigated for the MEC-aided cell-free network from the perspective of deep learning to maximize the weighted sum rate. Specifically, we convert the underlying problem into a joint multi-task optimization problem and then propose a centralized multi-task self-supervised learning algorithm to solve the problem so as to avoid costly manual labeling. Therein, two novel and general loss functions, i.e., negative fraction linear loss and exponential linear loss whose advantages in robustness and target domain have been proved and discussed, are designed to enable self-supervised learning. Moreover, we further design a MEC-enabled distributed multi-task self-supervised learning (DMTSSL) algorithm, with low complexity and high scalability to address the challenge of dimensional disaster. Finally, we develop the distance-aware transfer learning algorithm based on the DMTSSL algorithm to handle the dynamic scenario with negligible computation cost. Simulation results under $3$rd generation partnership project 38.901 urban-macrocell scenario demonstrate the superiority of the proposed algorithms over the baseline algorithms.</li>
</ul>

<h3>Title: REO-VLM: Transforming VLM to Meet Regression Challenges in Earth Observation</h3>
<ul>
<li><strong>Authors: </strong>Xizhe Xue, Guoting Wei, Hao Chen, Haokui Zhang, Feng Lin, Chunhua Shen, Xiao Xiang Zhu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16583">https://arxiv.org/abs/2412.16583</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16583">https://arxiv.org/pdf/2412.16583</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16583]] REO-VLM: Transforming VLM to Meet Regression Challenges in Earth Observation(https://arxiv.org/abs/2412.16583)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The rapid evolution of Vision Language Models (VLMs) has catalyzed significant advancements in artificial intelligence, expanding research across various disciplines, including Earth Observation (EO). While VLMs have enhanced image understanding and data processing within EO, their applications have predominantly focused on image content description. This limited focus overlooks their potential in geographic and scientific regression tasks, which are essential for diverse EO applications. To bridge this gap, this paper introduces a novel benchmark dataset, called \textbf{REO-Instruct} to unify regression and generation tasks specifically for the EO domain. Comprising 1.6 million multimodal EO imagery and language pairs, this dataset is designed to support both biomass regression and image content interpretation tasks. Leveraging this dataset, we develop \textbf{REO-VLM}, a groundbreaking model that seamlessly integrates regression capabilities with traditional generative functions. By utilizing language-driven reasoning to incorporate scientific domain knowledge, REO-VLM goes beyond solely relying on EO imagery, enabling comprehensive interpretation of complex scientific attributes from EO data. This approach establishes new performance benchmarks and significantly enhances the capabilities of environmental monitoring and resource management.</li>
</ul>

<h3>Title: IV-tuning: Parameter-Efficient Transfer Learning for Infrared-Visible Tasks</h3>
<ul>
<li><strong>Authors: </strong>Yaming Zhang, Chenqiang Gao, Fangcen Liu, Junjie Guo, Lan Wang, Xinggan Peng, Deyu Meng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16654">https://arxiv.org/abs/2412.16654</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16654">https://arxiv.org/pdf/2412.16654</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16654]] IV-tuning: Parameter-Efficient Transfer Learning for Infrared-Visible Tasks(https://arxiv.org/abs/2412.16654)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Infrared-visible (IR-VIS) tasks, such as semantic segmentation and object detection, greatly benefit from the advantage of combining infrared and visible modalities. To inherit the general representations of the Vision Foundation Models (VFMs), task-specific dual-branch networks are designed and fully fine-tuned on downstream datasets. Although effective, this manner lacks generality and is sub-optimal due to the scarcity of downstream infrared-visible datasets and limited transferability. In this paper, we propose a novel and general fine-tuning approach, namely "IV-tuning", to parameter-efficiently harness VFMs for various infrared-visible downstream tasks. At its core, IV-tuning freezes pre-trained visible-based VFMs and integrates modal-specific prompts with adapters within the backbone, bridging the gap between VFMs and downstream infrared-visible tasks while simultaneously learning the complementarity between different modalities. By fine-tuning approximately 3% of the backbone parameters, IV-tuning outperforms full fine-tuning across various baselines in infrared-visible semantic segmentation and object detection, as well as previous state-of-the-art methods. Extensive experiments across various settings demonstrate that IV-tuning achieves superior performance with fewer training parameters, providing a good alternative to full fine-tuning and a novel method of extending visible-based models for infrared-visible tasks. The code is available at this https URL.</li>
</ul>

<h3>Title: Generalizable Articulated Object Perception with Superpoints</h3>
<ul>
<li><strong>Authors: </strong>Qiaojun Yu, Ce Hao, Xibin Yuan, Li Zhang, Liu Liu, Yukang Huo, Rohit Agarwal, Cewu Lu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16656">https://arxiv.org/abs/2412.16656</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16656">https://arxiv.org/pdf/2412.16656</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16656]] Generalizable Articulated Object Perception with Superpoints(https://arxiv.org/abs/2412.16656)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Manipulating articulated objects with robotic arms is challenging due to the complex kinematic structure, which requires precise part segmentation for efficient manipulation. In this work, we introduce a novel superpoint-based perception method designed to improve part segmentation in 3D point clouds of articulated objects. We propose a learnable, part-aware superpoint generation technique that efficiently groups points based on their geometric and semantic similarities, resulting in clearer part boundaries. Furthermore, by leveraging the segmentation capabilities of the 2D foundation model SAM, we identify the centers of pixel regions and select corresponding superpoints as candidate query points. Integrating a query-based transformer decoder further enhances our method's ability to achieve precise part segmentation. Experimental results on the GAPartNet dataset show that our method outperforms existing state-of-the-art approaches in cross-category part segmentation, achieving AP50 scores of 77.9% for seen categories (4.4% improvement) and $39.3\%$ for unseen categories (11.6% improvement), with superior results in 5 out of 9 part categories for seen objects and outperforming all previous methods across all part categories for unseen objects.</li>
</ul>

<h3>Title: Adversarial Attack Against Images Classification based on Generative Adversarial Networks</h3>
<ul>
<li><strong>Authors: </strong>Yahe Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16662">https://arxiv.org/abs/2412.16662</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16662">https://arxiv.org/pdf/2412.16662</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16662]] Adversarial Attack Against Images Classification based on Generative Adversarial Networks(https://arxiv.org/abs/2412.16662)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Adversarial attacks on image classification systems have always been an important problem in the field of machine learning, and generative adversarial networks (GANs), as popular models in the field of image generation, have been widely used in various novel scenarios due to their powerful generative capabilities. However, with the popularity of generative adversarial networks, the misuse of fake image technology has raised a series of security problems, such as malicious tampering with other people's photos and videos, and invasion of personal privacy. Inspired by the generative adversarial networks, this work proposes a novel adversarial attack method, aiming to gain insight into the weaknesses of the image classification system and improve its anti-attack ability. Specifically, the generative adversarial networks are used to generate adversarial samples with small perturbations but enough to affect the decision-making of the classifier, and the adversarial samples are generated through the adversarial learning of the training generator and the classifier. From extensive experiment analysis, we evaluate the effectiveness of the method on a classical image classification dataset, and the results show that our model successfully deceives a variety of advanced classifiers while maintaining the naturalness of adversarial samples.</li>
</ul>

<h3>Title: Two-in-One: Unified Multi-Person Interactive Motion Generation by Latent Diffusion Transformer</h3>
<ul>
<li><strong>Authors: </strong>Boyuan Li, Xihua Wang, Ruihua Song, Wenbing Huang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16670">https://arxiv.org/abs/2412.16670</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16670">https://arxiv.org/pdf/2412.16670</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16670]] Two-in-One: Unified Multi-Person Interactive Motion Generation by Latent Diffusion Transformer(https://arxiv.org/abs/2412.16670)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Multi-person interactive motion generation, a critical yet under-explored domain in computer character animation, poses significant challenges such as intricate modeling of inter-human interactions beyond individual motions and generating two motions with huge differences from one text condition. Current research often employs separate module branches for individual motions, leading to a loss of interaction information and increased computational demands. To address these challenges, we propose a novel, unified approach that models multi-person motions and their interactions within a single latent space. Our approach streamlines the process by treating interactive motions as an integrated data point, utilizing a Variational AutoEncoder (VAE) for compression into a unified latent space, and performing a diffusion process within this space, guided by the natural language conditions. Experimental results demonstrate our method's superiority over existing approaches in generation quality, performing text condition in particular when motions have significant asymmetry, and accelerating the generation efficiency while preserving high quality.</li>
</ul>

<h3>Title: CyberSentinel: Efficient Anomaly Detection in Programmable Switch using Knowledge Distillation</h3>
<ul>
<li><strong>Authors: </strong>Sankalp Mittal</a></li>
<li><strong>Subjects: </strong>cs.CR, cs.NI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16693">https://arxiv.org/abs/2412.16693</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16693">https://arxiv.org/pdf/2412.16693</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16693]] CyberSentinel: Efficient Anomaly Detection in Programmable Switch using Knowledge Distillation(https://arxiv.org/abs/2412.16693)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>The increasing volume of traffic (especially from IoT devices) is posing a challenge to the current anomaly detection systems. Existing systems are forced to take the support of the control plane for a more thorough and accurate detection of malicious traffic (anomalies). This introduces latency in making decisions regarding fast incoming traffic and therefore, existing systems are unable to scale to such growing rates of traffic. In this paper, we propose CyberSentinel, a high throughput and accurate anomaly detection system deployed entirely in the programmable switch data plane; making it the first work to accurately detect anomalies at line speed. To detect unseen network attacks, CyberSentinel uses a novel knowledge distillation scheme that incorporates "learned" knowledge of deep unsupervised ML models (\textit{e.g.}, autoencoders) to develop an iForest model that is then installed in the data plane in the form of whitelist rules. We implement a prototype of CyberSentinel on a testbed with an Intel Tofino switch and evaluate it on various real-world use cases. CyberSentinel yields similar detection performance compared to the state-of-the-art control plane solutions but with an increase in packet-processing throughput by $66.47\%$ on a $40$ Gbps link, and a reduction in average per-packet latency by $50\%$.</li>
</ul>

<h3>Title: TCAQ-DM: Timestep-Channel Adaptive Quantization for Diffusion Models</h3>
<ul>
<li><strong>Authors: </strong>Haocheng Huang, Jiaxin Chen, Jinyang Guo, Ruiyi Zhan, Yunhong Wang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16700">https://arxiv.org/abs/2412.16700</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16700">https://arxiv.org/pdf/2412.16700</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16700]] TCAQ-DM: Timestep-Channel Adaptive Quantization for Diffusion Models(https://arxiv.org/abs/2412.16700)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models have achieved remarkable success in the image and video generation tasks. Nevertheless, they often require a large amount of memory and time overhead during inference, due to the complex network architecture and considerable number of timesteps for iterative diffusion. Recently, the post-training quantization (PTQ) technique has proved a promising way to reduce the inference cost by quantizing the float-point operations to low-bit ones. However, most of them fail to tackle with the large variations in the distribution of activations across distinct channels and timesteps, as well as the inconsistent of input between quantization and inference on diffusion models, thus leaving much room for improvement. To address the above issues, we propose a novel method dubbed Timestep-Channel Adaptive Quantization for Diffusion Models (TCAQ-DM). Specifically, we develop a timestep-channel joint reparameterization (TCR) module to balance the activation range along both the timesteps and channels, facilitating the successive reconstruction procedure. Subsequently, we employ a dynamically adaptive quantization (DAQ) module that mitigate the quantization error by selecting an optimal quantizer for each post-Softmax layers according to their specific types of distributions. Moreover, we present a progressively aligned reconstruction (PAR) strategy to mitigate the bias caused by the input mismatch. Extensive experiments on various benchmarks and distinct diffusion models demonstrate that the proposed method substantially outperforms the state-of-the-art approaches in most cases, especially yielding comparable FID metrics to the full precision model on CIFAR-10 in the W6A6 setting, while enabling generating available images in the W4A4 settings.</li>
</ul>

<h3>Title: From Pixels to Gigapixels: Bridging Local Inductive Bias and Long-Range Dependencies with Pixel-Mamba</h3>
<ul>
<li><strong>Authors: </strong>Zhongwei Qiu, Hanqing Chao, Tiancheng Lin, Wanxing Chang, Zijiang Yang, Wenpei Jiao, Yixuan Shen, Yunshuo Zhang, Yelin Yang, Wenbin Liu, Hui Jiang, Yun Bian, Ke Yan, Dakai Jin, Le Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16711">https://arxiv.org/abs/2412.16711</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16711">https://arxiv.org/pdf/2412.16711</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16711]] From Pixels to Gigapixels: Bridging Local Inductive Bias and Long-Range Dependencies with Pixel-Mamba(https://arxiv.org/abs/2412.16711)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Histopathology plays a critical role in medical diagnostics, with whole slide images (WSIs) offering valuable insights that directly influence clinical decision-making. However, the large size and complexity of WSIs may pose significant challenges for deep learning models, in both computational efficiency and effective representation learning. In this work, we introduce Pixel-Mamba, a novel deep learning architecture designed to efficiently handle gigapixel WSIs. Pixel-Mamba leverages the Mamba module, a state-space model (SSM) with linear memory complexity, and incorporates local inductive biases through progressively expanding tokens, akin to convolutional neural networks. This enables Pixel-Mamba to hierarchically combine both local and global information while efficiently addressing computational challenges. Remarkably, Pixel-Mamba achieves or even surpasses the quantitative performance of state-of-the-art (SOTA) foundation models that were pretrained on millions of WSIs or WSI-text pairs, in a range of tumor staging and survival analysis tasks, {\bf even without requiring any pathology-specific pretraining}. Extensive experiments demonstrate the efficacy of Pixel-Mamba as a powerful and efficient framework for end-to-end WSI analysis.</li>
</ul>

<h3>Title: GANFusion: Feed-Forward Text-to-3D with Diffusion in GAN Space</h3>
<ul>
<li><strong>Authors: </strong>Souhaib Attaiki, Paul Guerrero, Duygu Ceylan, Niloy J. Mitra, Maks Ovsjanikov</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16717">https://arxiv.org/abs/2412.16717</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16717">https://arxiv.org/pdf/2412.16717</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16717]] GANFusion: Feed-Forward Text-to-3D with Diffusion in GAN Space(https://arxiv.org/abs/2412.16717)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>We train a feed-forward text-to-3D diffusion generator for human characters using only single-view 2D data for supervision. Existing 3D generative models cannot yet match the fidelity of image or video generative models. State-of-the-art 3D generators are either trained with explicit 3D supervision and are thus limited by the volume and diversity of existing 3D data. Meanwhile, generators that can be trained with only 2D data as supervision typically produce coarser results, cannot be text-conditioned, or must revert to test-time optimization. We observe that GAN- and diffusion-based generators have complementary qualities: GANs can be trained efficiently with 2D supervision to produce high-quality 3D objects but are hard to condition on text. In contrast, denoising diffusion models can be conditioned efficiently but tend to be hard to train with only 2D supervision. We introduce GANFusion, which starts by generating unconditional triplane features for 3D data using a GAN architecture trained with only single-view 2D data. We then generate random samples from the GAN, caption them, and train a text-conditioned diffusion model that directly learns to sample from the space of good triplane features that can be decoded into 3D objects.</li>
</ul>

<h3>Title: KKANs: Kurkova-Kolmogorov-Arnold Networks and Their Learning Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Juan Diego Toscano, Li-Lian Wang, George Em Karniadakis</a></li>
<li><strong>Subjects: </strong>cs.LG, math.NA, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16738">https://arxiv.org/abs/2412.16738</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16738">https://arxiv.org/pdf/2412.16738</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16738]] KKANs: Kurkova-Kolmogorov-Arnold Networks and Their Learning Dynamics(https://arxiv.org/abs/2412.16738)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Inspired by the Kolmogorov-Arnold representation theorem and Kurkova's principle of using approximate representations, we propose the Kurkova-Kolmogorov-Arnold Network (KKAN), a new two-block architecture that combines robust multi-layer perceptron (MLP) based inner functions with flexible linear combinations of basis functions as outer functions. We first prove that KKAN is a universal approximator, and then we demonstrate its versatility across scientific machine-learning applications, including function regression, physics-informed machine learning (PIML), and operator-learning frameworks. The benchmark results show that KKANs outperform MLPs and the original Kolmogorov-Arnold Networks (KANs) in function approximation and operator learning tasks and achieve performance comparable to fully optimized MLPs for PIML. To better understand the behavior of the new representation models, we analyze their geometric complexity and learning dynamics using information bottleneck theory, identifying three universal learning stages, fitting, transition, and diffusion, across all types of architectures. We find a strong correlation between geometric complexity and signal-to-noise ratio (SNR), with optimal generalization achieved during the diffusion stage. Additionally, we propose self-scaled residual-based attention weights to maintain high SNR dynamically, ensuring uniform convergence and prolonged learning.</li>
</ul>

<h3>Title: Solving Inverse Problems via Diffusion Optimal Control</h3>
<ul>
<li><strong>Authors: </strong>Henry Li, Marcus Pereira</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16748">https://arxiv.org/abs/2412.16748</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16748">https://arxiv.org/pdf/2412.16748</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16748]] Solving Inverse Problems via Diffusion Optimal Control(https://arxiv.org/abs/2412.16748)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Existing approaches to diffusion-based inverse problem solvers frame the signal recovery task as a probabilistic sampling episode, where the solution is drawn from the desired posterior distribution. This framework suffers from several critical drawbacks, including the intractability of the conditional likelihood function, strict dependence on the score network approximation, and poor $\mathbf{x}_0$ prediction quality. We demonstrate that these limitations can be sidestepped by reframing the generative process as a discrete optimal control episode. We derive a diffusion-based optimal controller inspired by the iterative Linear Quadratic Regulator (iLQR) algorithm. This framework is fully general and able to handle any differentiable forward measurement operator, including super-resolution, inpainting, Gaussian deblurring, nonlinear deblurring, and even highly nonlinear neural classifiers. Furthermore, we show that the idealized posterior sampling equation can be recovered as a special case of our algorithm. We then evaluate our method against a selection of neural inverse problem solvers, and establish a new baseline in image reconstruction with inverse problems.</li>
</ul>

<h3>Title: HyperCLIP: Adapting Vision-Language models with Hypernetworks</h3>
<ul>
<li><strong>Authors: </strong>Victor Akinwande, Mohammad Sadegh Norouzzadeh, Devin Willmott, Anna Bair, Madan Ravi Ganesh, J. Zico Kolter</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16777">https://arxiv.org/abs/2412.16777</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16777">https://arxiv.org/pdf/2412.16777</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16777]] HyperCLIP: Adapting Vision-Language models with Hypernetworks(https://arxiv.org/abs/2412.16777)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-supervised vision-language models trained with contrastive objectives form the basis of current state-of-the-art methods in AI vision tasks. The success of these models is a direct consequence of the huge web-scale datasets used to train them, but they require correspondingly large vision components to properly learn powerful and general representations from such a broad data domain. This poses a challenge for deploying large vision-language models, especially in resource-constrained environments. To address this, we propose an alternate vision-language architecture, called HyperCLIP, that uses a small image encoder along with a hypernetwork that dynamically adapts image encoder weights to each new set of text inputs. All three components of the model (hypernetwork, image encoder, and text encoder) are pre-trained jointly end-to-end, and with a trained HyperCLIP model, we can generate new zero-shot deployment-friendly image classifiers for any task with a single forward pass through the text encoder and hypernetwork. HyperCLIP increases the zero-shot accuracy of SigLIP trained models with small image encoders by up to 3% on ImageNet and 5% on CIFAR-100 with minimal training throughput overhead.</li>
</ul>

<h3>Title: RoomPainter: View-Integrated Diffusion for Consistent Indoor Scene Texturing</h3>
<ul>
<li><strong>Authors: </strong>Zhipeng Huang, Wangbo Yu, Xinhua Cheng, ChengShu Zhao, Yunyang Ge, Mingyi Guo, Li Yuan, Yonghong Tian</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16778">https://arxiv.org/abs/2412.16778</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16778">https://arxiv.org/pdf/2412.16778</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16778]] RoomPainter: View-Integrated Diffusion for Consistent Indoor Scene Texturing(https://arxiv.org/abs/2412.16778)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Indoor scene texture synthesis has garnered significant interest due to its important potential applications in virtual reality, digital media, and creative arts. Existing diffusion model-based researches either rely on per-view inpainting techniques, which are plagued by severe cross-view inconsistencies and conspicuous seams, or they resort to optimization-based approaches that entail substantial computational overhead. In this work, we present RoomPainter, a framework that seamlessly integrates efficiency and consistency to achieve high-fidelity texturing of indoor scenes. The core of RoomPainter features a zero-shot technique that effectively adapts a 2D diffusion model for 3D-consistent texture synthesis, along with a two-stage generation strategy that ensures both global and local consistency. Specifically, we introduce Attention-Guided Multi-View Integrated Sampling (MVIS) combined with a neighbor-integrated attention mechanism for zero-shot texture map generation. Using the MVIS, we firstly generate texture map for the entire room to ensure global consistency, then adopt its variant, namely an attention-guided multi-view integrated repaint sampling (MVRS) to repaint individual instances within the room, thereby further enhancing local consistency. Experiments demonstrate that RoomPainter achieves superior performance for indoor scene texture synthesis in visual quality, global consistency, and generation efficiency.</li>
</ul>

<h3>Title: Layer- and Timestep-Adaptive Differentiable Token Compression Ratios for Efficient Diffusion Transformers</h3>
<ul>
<li><strong>Authors: </strong>Haoran You, Connelly Barnes, Yuqian Zhou, Yan Kang, Zhenbang Du, Wei Zhou, Lingzhi Zhang, Yotam Nitzan, Xiaoyang Liu, Zhe Lin, Eli Shechtman, Sohrab Amirghodsi, Yingyan Celine Lin</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16822">https://arxiv.org/abs/2412.16822</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16822">https://arxiv.org/pdf/2412.16822</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16822]] Layer- and Timestep-Adaptive Differentiable Token Compression Ratios for Efficient Diffusion Transformers(https://arxiv.org/abs/2412.16822)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion Transformers (DiTs) have achieved state-of-the-art (SOTA) image generation quality but suffer from high latency and memory inefficiency, making them difficult to deploy on resource-constrained devices. One key efficiency bottleneck is that existing DiTs apply equal computation across all regions of an image. However, not all image tokens are equally important, and certain localized areas require more computation, such as objects. To address this, we propose DiffRatio-MoD, a dynamic DiT inference framework with differentiable compression ratios, which automatically learns to dynamically route computation across layers and timesteps for each image token, resulting in Mixture-of-Depths (MoD) efficient DiT models. Specifically, DiffRatio-MoD integrates three features: (1) A token-level routing scheme where each DiT layer includes a router that is jointly fine-tuned with model weights to predict token importance scores. In this way, unimportant tokens bypass the entire layer's computation; (2) A layer-wise differentiable ratio mechanism where different DiT layers automatically learn varying compression ratios from a zero initialization, resulting in large compression ratios in redundant layers while others remain less compressed or even uncompressed; (3) A timestep-wise differentiable ratio mechanism where each denoising timestep learns its own compression ratio. The resulting pattern shows higher ratios for noisier timesteps and lower ratios as the image becomes clearer. Extensive experiments on both text-to-image and inpainting tasks show that DiffRatio-MoD effectively captures dynamism across token, layer, and timestep axes, achieving superior trade-offs between generation quality and efficiency compared to prior works.</li>
</ul>

<h3>Title: Human-Guided Image Generation for Expanding Small-Scale Training Image Datasets</h3>
<ul>
<li><strong>Authors: </strong>Changjian Chen, Fei Lv, Yalong Guan, Pengcheng Wang, Shengjie Yu, Yifan Zhang, Zhuo Tang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16839">https://arxiv.org/abs/2412.16839</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16839">https://arxiv.org/pdf/2412.16839</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16839]] Human-Guided Image Generation for Expanding Small-Scale Training Image Datasets(https://arxiv.org/abs/2412.16839)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The performance of computer vision models in certain real-world applications (e.g., rare wildlife observation) is limited by the small number of available this http URL datasets using pre-trained generative models is an effective way to address this limitation. However, since the automatic generation process is uncontrollable, the generated images are usually limited in diversity, and some of them are undesired. In this paper, we propose a human-guided image generation method for more controllable dataset expansion. We develop a multi-modal projection method with theoretical guarantees to facilitate the exploration of both the original and generated images. Based on the exploration, users refine the prompts and re-generate images for better performance. Since directly refining the prompts is challenging for novice users, we develop a sample-level prompt refinement method to make it easier. With this method, users only need to provide sample-level feedback (e.g., which samples are undesired) to obtain better prompts. The effectiveness of our method is demonstrated through the quantitative evaluation of the multi-modal projection method, improved model performance in the case study for both classification and object detection tasks, and positive feedback from the experts.</li>
</ul>

<h3>Title: Adversarial Diffusion Model for Unsupervised Domain-Adaptive Semantic Segmentation</h3>
<ul>
<li><strong>Authors: </strong>Jongmin Yu, Zhongtian Sun, Shan Luo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16859">https://arxiv.org/abs/2412.16859</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16859">https://arxiv.org/pdf/2412.16859</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16859]] Adversarial Diffusion Model for Unsupervised Domain-Adaptive Semantic Segmentation(https://arxiv.org/abs/2412.16859)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Semantic segmentation requires labour-intensive labelling tasks to obtain the supervision signals, and because of this issue, it is encouraged that using domain adaptation, which transfers information from the existing labelled source domains to unlabelled or weakly labelled target domains, is essential. However, it is intractable to find a well-generalised representation which can describe two domains due to probabilistic or geometric difference between the two domains. This paper presents a novel method, the Conditional and Inter-coder Connected Latent Diffusion (CICLD) based Semantic Segmentation Model, to advance unsupervised domain adaptation (UDA) for semantic segmentation tasks. Leveraging the strengths of latent diffusion models and adversarial learning, our method effectively bridges the gap between synthetic and real-world imagery. CICLD incorporates a conditioning mechanism to improve contextual understanding during segmentation and an inter-coder connection to preserve fine-grained details and spatial hierarchies. Additionally, adversarial learning aligns latent feature distributions across source, mixed, and target domains, further enhancing generalisation. Extensive experiments are conducted across three benchmark datasets-GTA5, Synthia, and Cityscape-shows that CICLD outperforms state-of-the-art UDA methods. Notably, the proposed method achieves a mean Intersection over Union (mIoU) of 74.4 for the GTA5 to Cityscape UDA setting and 67.2 mIoU for the Synthia to Cityscape UDA setting. This project is publicly available on 'this https URL.</li>
</ul>

<h3>Title: Learning to Generate Gradients for Test-Time Adaptation via Test-Time Training Layers</h3>
<ul>
<li><strong>Authors: </strong>Qi Deng, Shuaicheng Niu, Ronghao Zhang, Yaofo Chen, Runhao Zeng, Jian Chen, Xiping Hu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16901">https://arxiv.org/abs/2412.16901</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16901">https://arxiv.org/pdf/2412.16901</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16901]] Learning to Generate Gradients for Test-Time Adaptation via Test-Time Training Layers(https://arxiv.org/abs/2412.16901)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Test-time adaptation (TTA) aims to fine-tune a trained model online using unlabeled testing data to adapt to new environments or out-of-distribution data, demonstrating broad application potential in real-world scenarios. However, in this optimization process, unsupervised learning objectives like entropy minimization frequently encounter noisy learning signals. These signals produce unreliable gradients, which hinder the model ability to converge to an optimal solution quickly and introduce significant instability into the optimization process. In this paper, we seek to resolve these issues from the perspective of optimizer design. Unlike prior TTA using manually designed optimizers like SGD, we employ a learning-to-optimize approach to automatically learn an optimizer, called Meta Gradient Generator (MGG). Specifically, we aim for MGG to effectively utilize historical gradient information during the online optimization process to optimize the current model. To this end, in MGG, we design a lightweight and efficient sequence modeling layer -- gradient memory layer. It exploits a self-supervised reconstruction loss to compress historical gradient information into network parameters, thereby enabling better memorization ability over a long-term adaptation process. We only need a small number of unlabeled samples to pre-train MGG, and then the trained MGG can be deployed to process unseen samples. Promising results on ImageNet-C, R, Sketch, and A indicate that our method surpasses current state-of-the-art methods with fewer updates, less data, and significantly shorter adaptation iterations. Compared with a previous SOTA method SAR, we achieve 7.4% accuracy improvement and 4.2 times faster adaptation speed on ImageNet-C.</li>
</ul>

<h3>Title: Self-Corrected Flow Distillation for Consistent One-Step and Few-Step Text-to-Image Generation</h3>
<ul>
<li><strong>Authors: </strong>Quan Dao, Hao Phung, Trung Dao, Dimitris Metaxas, Anh Tran</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16906">https://arxiv.org/abs/2412.16906</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16906">https://arxiv.org/pdf/2412.16906</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16906]] Self-Corrected Flow Distillation for Consistent One-Step and Few-Step Text-to-Image Generation(https://arxiv.org/abs/2412.16906)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Flow matching has emerged as a promising framework for training generative models, demonstrating impressive empirical performance while offering relative ease of training compared to diffusion-based models. However, this method still requires numerous function evaluations in the sampling process. To address these limitations, we introduce a self-corrected flow distillation method that effectively integrates consistency models and adversarial training within the flow-matching framework. This work is a pioneer in achieving consistent generation quality in both few-step and one-step sampling. Our extensive experiments validate the effectiveness of our method, yielding superior results both quantitatively and qualitatively on CelebA-HQ and zero-shot benchmarks on the COCO dataset. Our implementation is released at this https URL</li>
</ul>

<h3>Title: FADA: Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG Distillation</h3>
<ul>
<li><strong>Authors: </strong>Tianyun Zhong, Chao Liang, Jianwen Jiang, Gaojie Lin, Jiaqi Yang, Zhou Zhao</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.GR, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16915">https://arxiv.org/abs/2412.16915</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16915">https://arxiv.org/pdf/2412.16915</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16915]] FADA: Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG Distillation(https://arxiv.org/abs/2412.16915)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion-based audio-driven talking avatar methods have recently gained attention for their high-fidelity, vivid, and expressive results. However, their slow inference speed limits practical applications. Despite the development of various distillation techniques for diffusion models, we found that naive diffusion distillation methods do not yield satisfactory results. Distilled models exhibit reduced robustness with open-set input images and a decreased correlation between audio and video compared to teacher models, undermining the advantages of diffusion models. To address this, we propose FADA (Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG Distillation). We first designed a mixed-supervised loss to leverage data of varying quality and enhance the overall model capability as well as robustness. Additionally, we propose a multi-CFG distillation with learnable tokens to utilize the correlation between audio and reference image conditions, reducing the threefold inference runs caused by multi-CFG with acceptable quality degradation. Extensive experiments across multiple datasets show that FADA generates vivid videos comparable to recent diffusion model-based methods while achieving an NFE speedup of 4.17-12.5 times. Demos are available at our webpage this http URL.</li>
</ul>

<h3>Title: Detect Changes like Humans: Incorporating Semantic Priors for Improved Change Detection</h3>
<ul>
<li><strong>Authors: </strong>Yuhang Gan, Wenjie Xuan, Zhiming Luo, Lei Fang, Zengmao Wang, Juhua Liu, Bo Du</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16918">https://arxiv.org/abs/2412.16918</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16918">https://arxiv.org/pdf/2412.16918</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16918]] Detect Changes like Humans: Incorporating Semantic Priors for Improved Change Detection(https://arxiv.org/abs/2412.16918)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>When given two similar images, humans identify their differences by comparing the appearance ({\it e.g., color, texture}) with the help of semantics ({\it e.g., objects, relations}). However, mainstream change detection models adopt a supervised training paradigm, where the annotated binary change map is the main constraint. Thus, these methods primarily emphasize the difference-aware features between bi-temporal images and neglect the semantic understanding of the changed landscapes, which undermines the accuracy in the presence of noise and illumination variations. To this end, this paper explores incorporating semantic priors to improve the ability to detect changes. Firstly, we propose a Semantic-Aware Change Detection network, namely SA-CDNet, which transfers the common knowledge of the visual foundation models ({\it i.e., FastSAM}) to change detection. Inspired by the human visual paradigm, a novel dual-stream feature decoder is derived to distinguish changes by combining semantic-aware features and difference-aware features. Secondly, we design a single-temporal semantic pre-training strategy to enhance the semantic understanding of landscapes, which brings further increments. Specifically, we construct pseudo-change detection data from public single-temporal remote sensing segmentation datasets for large-scale pre-training, where an extra branch is also introduced for the proxy semantic segmentation task. Experimental results on five challenging benchmarks demonstrate the superiority of our method over the existing state-of-the-art methods. The code is available at \href{this https URL}{SA-CD}.</li>
</ul>

<h3>Title: TAR3D: Creating High-Quality 3D Assets via Next-Part Prediction</h3>
<ul>
<li><strong>Authors: </strong>Xuying Zhang, Yutong Liu, Yangguang Li, Renrui Zhang, Yufei Liu, Kai Wang, Wanli Ouyang, Zhiwei Xiong, Peng Gao, Qibin Hou, Ming-Ming Cheng</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16919">https://arxiv.org/abs/2412.16919</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16919">https://arxiv.org/pdf/2412.16919</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16919]] TAR3D: Creating High-Quality 3D Assets via Next-Part Prediction(https://arxiv.org/abs/2412.16919)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>We present TAR3D, a novel framework that consists of a 3D-aware Vector Quantized-Variational AutoEncoder (VQ-VAE) and a Generative Pre-trained Transformer (GPT) to generate high-quality 3D assets. The core insight of this work is to migrate the multimodal unification and promising learning capabilities of the next-token prediction paradigm to conditional 3D object generation. To achieve this, the 3D VQ-VAE first encodes a wide range of 3D shapes into a compact triplane latent space and utilizes a set of discrete representations from a trainable codebook to reconstruct fine-grained geometries under the supervision of query point occupancy. Then, the 3D GPT, equipped with a custom triplane position embedding called TriPE, predicts the codebook index sequence with prefilling prompt tokens in an autoregressive manner so that the composition of 3D geometries can be modeled part by part. Extensive experiments on ShapeNet and Objaverse demonstrate that TAR3D can achieve superior generation quality over existing methods in text-to-3D and image-to-3D tasks</li>
</ul>

<h3>Title: Revisiting In-Context Learning with Long Context Language Models</h3>
<ul>
<li><strong>Authors: </strong>Jinheon Baek, Sun Jae Lee, Prakhar Gupta, Geunseob (GS)Oh, Siddharth Dalmia, Prateek Kolhar</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16926">https://arxiv.org/abs/2412.16926</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16926">https://arxiv.org/pdf/2412.16926</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16926]] Revisiting In-Context Learning with Long Context Language Models(https://arxiv.org/abs/2412.16926)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>In-Context Learning (ICL) is a technique by which language models make predictions based on examples provided in their input context. Previously, their context window size imposed a limit on the number of examples that can be shown, making example selection techniques crucial for identifying the maximally effective set of examples. However, the recent advent of Long Context Language Models (LCLMs) has significantly increased the number of examples that can be included in context, raising an important question of whether ICL performance in a many-shot regime is still sensitive to the method of sample selection. To answer this, we revisit these approaches in the context of LCLMs through extensive experiments on 18 datasets spanning 4 tasks. Surprisingly, we observe that sophisticated example selection techniques do not yield significant improvements over a simple random sample selection method. Instead, we find that the advent of LCLMs has fundamentally shifted the challenge of ICL from that of selecting the most effective examples to that of collecting sufficient examples to fill the context window. Specifically, in certain datasets, including all available examples does not fully utilize the context window; however, by augmenting the examples in context with a simple data augmentation approach, we substantially improve ICL performance by 5%.</li>
</ul>

<h3>Title: BloomCoreset: Fast Coreset Sampling using Bloom Filters for Fine-Grained Self-Supervised Learning</h3>
<ul>
<li><strong>Authors: </strong>Prajwal Singh, Gautam Vashishtha, Indra Deep Mastan, Shanmuganathan Raman</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16942">https://arxiv.org/abs/2412.16942</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16942">https://arxiv.org/pdf/2412.16942</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16942]] BloomCoreset: Fast Coreset Sampling using Bloom Filters for Fine-Grained Self-Supervised Learning(https://arxiv.org/abs/2412.16942)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>The success of deep learning in supervised fine-grained recognition for domain-specific tasks relies heavily on expert annotations. The Open-Set for fine-grained Self-Supervised Learning (SSL) problem aims to enhance performance on downstream tasks by strategically sampling a subset of images (the Core-Set) from a large pool of unlabeled data (the Open-Set). In this paper, we propose a novel method, BloomCoreset, that significantly reduces sampling time from Open-Set while preserving the quality of samples in the coreset. To achieve this, we utilize Bloom filters as an innovative hashing mechanism to store both low- and high-level features of the fine-grained dataset, as captured by Open-CLIP, in a space-efficient manner that enables rapid retrieval of the coreset from the Open-Set. To show the effectiveness of the sampled coreset, we integrate the proposed method into the state-of-the-art fine-grained SSL framework, SimCore [1]. The proposed algorithm drastically outperforms the sampling strategy of the baseline in SimCore [1] with a $98.5\%$ reduction in sampling time with a mere $0.83\%$ average trade-off in accuracy calculated across $11$ downstream datasets.</li>
</ul>

<h3>Title: DTSGAN: Learning Dynamic Textures via Spatiotemporal Generative Adversarial Network</h3>
<ul>
<li><strong>Authors: </strong>Xiangtian Li, Xiaobo Wang, Zhen Qi, Han Cao, Zhaoyang Zhang, Ao Xiang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16948">https://arxiv.org/abs/2412.16948</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16948">https://arxiv.org/pdf/2412.16948</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16948]] DTSGAN: Learning Dynamic Textures via Spatiotemporal Generative Adversarial Network(https://arxiv.org/abs/2412.16948)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Dynamic texture synthesis aims to generate sequences that are visually similar to a reference video texture and exhibit specific stationary properties in time. In this paper, we introduce a spatiotemporal generative adversarial network (DTSGAN) that can learn from a single dynamic texture by capturing its motion and content distribution. With the pipeline of DTSGAN, a new video sequence is generated from the coarsest scale to the finest one. To avoid mode collapse, we propose a novel strategy for data updates that helps improve the diversity of generated results. Qualitative and quantitative experiments show that our model is able to generate high quality dynamic textures and natural motion.</li>
</ul>

<h3>Title: PromptDresser: Improving the Quality and Controllability of Virtual Try-On via Generative Textual Prompt and Prompt-aware Mask</h3>
<ul>
<li><strong>Authors: </strong>Jeongho Kim, Hoiyeong Jin, Sunghyun Park, Jaegul Choo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16978">https://arxiv.org/abs/2412.16978</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16978">https://arxiv.org/pdf/2412.16978</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16978]] PromptDresser: Improving the Quality and Controllability of Virtual Try-On via Generative Textual Prompt and Prompt-aware Mask(https://arxiv.org/abs/2412.16978)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative, in-context</a></li>
<li><strong>Abstract: </strong>Recent virtual try-on approaches have advanced by fine-tuning the pre-trained text-to-image diffusion models to leverage their powerful generative ability. However, the use of text prompts in virtual try-on is still underexplored. This paper tackles a text-editable virtual try-on task that changes the clothing item based on the provided clothing image while editing the wearing style (e.g., tucking style, fit) according to the text descriptions. In the text-editable virtual try-on, three key aspects exist: (i) designing rich text descriptions for paired person-clothing data to train the model, (ii) addressing the conflicts where textual information of the existing person's clothing interferes the generation of the new clothing, and (iii) adaptively adjust the inpainting mask aligned with the text descriptions, ensuring proper editing areas while preserving the original person's appearance irrelevant to the new clothing. To address these aspects, we propose PromptDresser, a text-editable virtual try-on model that leverages large multimodal model (LMM) assistance to enable high-quality and versatile manipulation based on generative text prompts. Our approach utilizes LMMs via in-context learning to generate detailed text descriptions for person and clothing images independently, including pose details and editing attributes using minimal human cost. Moreover, to ensure the editing areas, we adjust the inpainting mask depending on the text prompts adaptively. We found that our approach, utilizing detailed text prompts, not only enhances text editability but also effectively conveys clothing details that are difficult to capture through images alone, thereby enhancing image quality. Our code is available at this https URL.</li>
</ul>

<h3>Title: A Conditional Diffusion Model for Electrical Impedance Tomography Image Reconstruction</h3>
<ul>
<li><strong>Authors: </strong>Shuaikai Shi, Ruiyuan Kang, Panos Liatsis</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16979">https://arxiv.org/abs/2412.16979</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16979">https://arxiv.org/pdf/2412.16979</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16979]] A Conditional Diffusion Model for Electrical Impedance Tomography Image Reconstruction(https://arxiv.org/abs/2412.16979)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Electrical impedance tomography (EIT) is a non-invasive imaging technique, capable of reconstructing images of the electrical conductivity of tissues and materials. It is popular in diverse application areas, from medical imaging to industrial process monitoring and tactile sensing, due to its low cost, real-time capabilities and non-ionizing nature. EIT visualizes the conductivity distribution within a body by measuring the boundary voltages, given a current injection. However, EIT image reconstruction is ill-posed due to the mismatch between the under-sampled voltage data and the high-resolution conductivity image. A variety of approaches, both conventional and deep learning-based, have been proposed, capitalizing on the use of spatial regularizers, and the paradigm of image regression. In this research, a novel method based on the conditional diffusion model for EIT reconstruction is proposed, termed CDEIT. Specifically, CDEIT consists of the forward diffusion process, which first gradually adds Gaussian noise to the clean conductivity images, and a reverse denoising process, which learns to predict the original conductivity image from its noisy version, conditioned on the boundary voltages. Following model training, CDEIT applies the conditional reverse process on test voltage data to generate the desired conductivities. Moreover, we provide the details of a normalization procedure, which demonstrates how EIT image reconstruction models trained on simulated datasets can be applied on real datasets with varying sizes, excitation currents and background conductivities. Experiments conducted on a synthetic dataset and two real datasets demonstrate that the proposed model outperforms state-of-the-art methods. The CDEIT software is available as open-source (this https URL) for reproducibility purposes.</li>
</ul>

<h3>Title: InterDance:Reactive 3D Dance Generation with Realistic Duet Interactions</h3>
<ul>
<li><strong>Authors: </strong>Ronghui Li, Youliang Zhang, Yachao Zhang, Yuxiang Zhang, Mingyang Su, Jie Guo, Ziwei Liu, Yebin Liu, Xiu Li</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR, cs.MM, cs.SD, eess.AS</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.16982">https://arxiv.org/abs/2412.16982</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.16982">https://arxiv.org/pdf/2412.16982</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.16982]] InterDance:Reactive 3D Dance Generation with Realistic Duet Interactions(https://arxiv.org/abs/2412.16982)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Humans perform a variety of interactive motions, among which duet dance is one of the most challenging interactions. However, in terms of human motion generative models, existing works are still unable to generate high-quality interactive motions, especially in the field of duet dance. On the one hand, it is due to the lack of large-scale high-quality datasets. On the other hand, it arises from the incomplete representation of interactive motion and the lack of fine-grained optimization of interactions. To address these challenges, we propose, InterDance, a large-scale duet dance dataset that significantly enhances motion quality, data scale, and the variety of dance genres. Built upon this dataset, we propose a new motion representation that can accurately and comprehensively describe interactive motion. We further introduce a diffusion-based framework with an interaction refinement guidance strategy to optimize the realism of interactions progressively. Extensive experiments demonstrate the effectiveness of our dataset and algorithm.</li>
</ul>

<h3>Title: HyperNet Fields: Efficiently Training Hypernetworks without Ground Truth by Learning Weight Trajectories</h3>
<ul>
<li><strong>Authors: </strong>Eric Hedlin, Munawar Hayat, Fatih Porikli, Kwang Moo Yi, Shweta Mahajan</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17040">https://arxiv.org/abs/2412.17040</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17040">https://arxiv.org/pdf/2412.17040</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17040]] HyperNet Fields: Efficiently Training Hypernetworks without Ground Truth by Learning Weight Trajectories(https://arxiv.org/abs/2412.17040)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>To efficiently adapt large models or to train generative models of neural representations, Hypernetworks have drawn interest. While hypernetworks work well, training them is cumbersome, and often requires ground truth optimized weights for each sample. However, obtaining each of these weights is a training problem of its own-one needs to train, e.g., adaptation weights or even an entire neural field for hypernetworks to regress to. In this work, we propose a method to train hypernetworks, without the need for any per-sample ground truth. Our key idea is to learn a Hypernetwork `Field` and estimate the entire trajectory of network weight training instead of simply its converged state. In other words, we introduce an additional input to the Hypernetwork, the convergence state, which then makes it act as a neural field that models the entire convergence pathway of a task network. A critical benefit in doing so is that the gradient of the estimated weights at any convergence state must then match the gradients of the original task -- this constraint alone is sufficient to train the Hypernetwork Field. We demonstrate the effectiveness of our method through the task of personalized image generation and 3D shape reconstruction from images and point clouds, demonstrating competitive results without any per-sample ground truth.</li>
</ul>

<h3>Title: An OpenMind for 3D medical vision self-supervised learning</h3>
<ul>
<li><strong>Authors: </strong>Tassilo Wald, Constantin Ulrich, Jonathan Suprijadi, Michal Nohel, Robin Peretzke, Klaus H. Maier-Hein</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17041">https://arxiv.org/abs/2412.17041</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17041">https://arxiv.org/pdf/2412.17041</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17041]] An OpenMind for 3D medical vision self-supervised learning(https://arxiv.org/abs/2412.17041)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>The field of 3D medical vision self-supervised learning lacks consistency and standardization. While many methods have been developed it is impossible to identify the current state-of-the-art, due to i) varying and small pre-training datasets, ii) varying architectures, and iii) being evaluated on differing downstream datasets. In this paper we bring clarity to this field and lay the foundation for further method advancements: We a) publish the largest publicly available pre-training dataset comprising 114k 3D brain MRI volumes and b) benchmark existing SSL methods under common architectures and c) provide the code of our framework publicly to facilitate rapid adoption and reproduction. This pre-print \textit{only describes} the dataset contribution (a); Data, benchmark, and codebase will be made available shortly.</li>
</ul>

<h3>Title: Adapting Image-to-Video Diffusion Models for Large-Motion Frame Interpolation</h3>
<ul>
<li><strong>Authors: </strong>Luoxu Jin, Hiroshi Watanabe</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17042">https://arxiv.org/abs/2412.17042</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17042">https://arxiv.org/pdf/2412.17042</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17042]] Adapting Image-to-Video Diffusion Models for Large-Motion Frame Interpolation(https://arxiv.org/abs/2412.17042)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>The development of video generation models has advanced significantly in recent years. For video frame interpolation, we adopt a pre-trained large-scale image-to-video diffusion model. To enable this adaptation, we propose a conditional encoder, which serves as a simple yet effective trainable module. By leveraging the first and last frames, we extract spatial and temporal features and input them into the conditional encoder. The computed features of the conditional encoder guide the video diffusion model in generating keyframe-guided video sequences. Our method demonstrates superior performance on the FrÃ©chet Video Distance (FVD) metric compared to previous deterministic approaches in handling large-motion cases, highlighting advancements in generative-based methodologies.</li>
</ul>

<h3>Title: DR-Encoder: Encode Low-rank Gradients with Random Prior for Large Language Models Differentially Privately</h3>
<ul>
<li><strong>Authors: </strong>Huiwen Wu, Deyi Zhang, Xiaohan Li, Xiaogang Xu, Jiafei Wu, Zhe Liu</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17053">https://arxiv.org/abs/2412.17053</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17053">https://arxiv.org/pdf/2412.17053</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17053]] DR-Encoder: Encode Low-rank Gradients with Random Prior for Large Language Models Differentially Privately(https://arxiv.org/abs/2412.17053)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>The emergence of the Large Language Model (LLM) has shown their superiority in a wide range of disciplines, including language understanding and translation, relational logic reasoning, and even partial differential equations solving. The transformer is the pervasive backbone architecture for the foundation model construction. It is vital to research how to adjust the Transformer architecture to achieve an end-to-end privacy guarantee in LLM fine-tuning. In this paper, we investigate three potential information leakage during a federated fine-tuning procedure for LLM (FedLLM). Based on the potential information leakage, we provide an end-to-end privacy guarantee solution for FedLLM by inserting two-stage randomness. The first stage is to train a gradient auto-encoder with a Gaussian random prior based on the statistical information of the gradients generated by local clients. The second stage is to fine-tune the overall LLM with a differential privacy guarantee by adopting appropriate Gaussian noises. We show the efficiency and accuracy gains of our proposed method with several foundation models and two popular evaluation benchmarks. Furthermore, we present a comprehensive privacy analysis with Gaussian Differential Privacy (GDP) and Renyi Differential Privacy (RDP).</li>
</ul>

<h3>Title: SAIL: Sample-Centric In-Context Learning for Document Information Extraction</h3>
<ul>
<li><strong>Authors: </strong>Jinyu Zhang, Zhiyuan You, Jize Wang, Xinyi Le</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17092">https://arxiv.org/abs/2412.17092</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17092">https://arxiv.org/pdf/2412.17092</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17092]] SAIL: Sample-Centric In-Context Learning for Document Information Extraction(https://arxiv.org/abs/2412.17092)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Document Information Extraction (DIE) aims to extract structured information from Visually Rich Documents (VRDs). Previous full-training approaches have demonstrated strong performance but may struggle with generalization to unseen data. In contrast, training-free methods leverage powerful pre-trained models like Large Language Models (LLMs) to address various downstream tasks with only a few examples. Nonetheless, training-free methods for DIE encounter two primary challenges: (1) understanding the complex relationship between layout and textual elements in VRDs, and (2) providing accurate guidance to pre-trained models. To address these challenges, we propose Sample-centric In-context Learning (SAIL) for DIE. SAIL introduces a fine-grained entity-level textual similarity to facilitate in-depth text analysis by LLMs and incorporates layout similarity to enhance the analysis of layouts in VRDs. Additionally, SAIL formulates a unified In-Context Learning (ICL) prompt template for various sample-centric examples, enabling tailored prompts that deliver precise guidance to pre-trained models for each sample. Extensive experiments on FUNSD, CORD, and SROIE benchmarks with various base models (e.g., LLMs) indicate that our method outperforms training-free baselines, even closer to the full-training methods. The results show the superiority and generalization of our method.</li>
</ul>

<h3>Title: Similarity Trajectories: Linking Sampling Process to Artifacts in Diffusion-Generated Images</h3>
<ul>
<li><strong>Authors: </strong>Dennis Menn, Feng Liang, Hung-Yueh Chiang, Diana Marculescu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17109">https://arxiv.org/abs/2412.17109</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17109">https://arxiv.org/pdf/2412.17109</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17109]] Similarity Trajectories: Linking Sampling Process to Artifacts in Diffusion-Generated Images(https://arxiv.org/abs/2412.17109)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Artifact detection algorithms are crucial to correcting the output generated by diffusion models. However, because of the variety of artifact forms, existing methods require substantial annotated data for training. This requirement limits their scalability and efficiency, which restricts their wide application. This paper shows that the similarity of denoised images between consecutive time steps during the sampling process is related to the severity of artifacts in images generated by diffusion models. Building on this observation, we introduce the concept of Similarity Trajectory to characterize the sampling process and its correlation with the image artifacts presented. Using an annotated data set of 680 images, which is only 0.1% of the amount of data used in the prior work, we trained a classifier on these trajectories to predict the presence of artifacts in images. By performing 10-fold validation testing on the balanced annotated data set, the classifier can achieve an accuracy of 72.35%, highlighting the connection between the Similarity Trajectory and the occurrence of artifacts. This approach enables differentiation between artifact-exhibiting and natural-looking images using limited training data.</li>
</ul>

<h3>Title: Generative Diffusion Modeling: A Practical Handbook</h3>
<ul>
<li><strong>Authors: </strong>Zihan Ding, Chi Jin</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17162">https://arxiv.org/abs/2412.17162</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17162">https://arxiv.org/pdf/2412.17162</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17162]] Generative Diffusion Modeling: A Practical Handbook(https://arxiv.org/abs/2412.17162)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>This handbook offers a unified perspective on diffusion models, encompassing diffusion probabilistic models, score-based generative models, consistency models, rectified flow, and related methods. By standardizing notations and aligning them with code implementations, it aims to bridge the "paper-to-code" gap and facilitate robust implementations and fair comparisons. The content encompasses the fundamentals of diffusion models, the pre-training process, and various post-training methods. Post-training techniques include model distillation and reward-based fine-tuning. Designed as a practical guide, it emphasizes clarity and usability over theoretical depth, focusing on widely adopted approaches in generative modeling with diffusion models.</li>
</ul>

<h3>Title: Where Did Your Model Learn That? Label-free Influence for Self-supervised Learning</h3>
<ul>
<li><strong>Authors: </strong>Nidhin Harilal, Amit Kiran Rege, Reza Akbarian Bafghi, Maziar Raissi, Claire Monteleoni</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17170">https://arxiv.org/abs/2412.17170</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17170">https://arxiv.org/pdf/2412.17170</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17170]] Where Did Your Model Learn That? Label-free Influence for Self-supervised Learning(https://arxiv.org/abs/2412.17170)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Self-supervised learning (SSL) has revolutionized learning from large-scale unlabeled datasets, yet the intrinsic relationship between pretraining data and the learned representations remains poorly understood. Traditional supervised learning benefits from gradient-based data attribution tools like influence functions that measure the contribution of an individual data point to model predictions. However, existing definitions of influence rely on labels, making them unsuitable for SSL settings. We address this gap by introducing Influence-SSL, a novel and label-free approach for defining influence functions tailored to SSL. Our method harnesses the stability of learned representations against data augmentations to identify training examples that help explain model predictions. We provide both theoretical foundations and empirical evidence to show the utility of Influence-SSL in analyzing pre-trained SSL models. Our analysis reveals notable differences in how SSL models respond to influential data compared to supervised models. Finally, we validate the effectiveness of Influence-SSL through applications in duplicate detection, outlier identification and fairness analysis. Code is available at: \url{this https URL}.</li>
</ul>

<h3>Title: Enhancing Item Tokenization for Generative Recommendation through Self-Improvement</h3>
<ul>
<li><strong>Authors: </strong>Runjin Chen, Mingxuan Ju, Ngoc Bui, Dimosthenis Antypas, Stanley Cai, Xiaopeng Wu, Leonardo Neves, Zhangyang Wang, Neil Shah, Tong Zhao</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.IR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17171">https://arxiv.org/abs/2412.17171</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17171">https://arxiv.org/pdf/2412.17171</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17171]] Enhancing Item Tokenization for Generative Recommendation through Self-Improvement(https://arxiv.org/abs/2412.17171)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Generative recommendation systems, driven by large language models (LLMs), present an innovative approach to predicting user preferences by modeling items as token sequences and generating recommendations in a generative manner. A critical challenge in this approach is the effective tokenization of items, ensuring that they are represented in a form compatible with LLMs. Current item tokenization methods include using text descriptions, numerical strings, or sequences of discrete tokens. While text-based representations integrate seamlessly with LLM tokenization, they are often too lengthy, leading to inefficiencies and complicating accurate generation. Numerical strings, while concise, lack semantic depth and fail to capture meaningful item relationships. Tokenizing items as sequences of newly defined tokens has gained traction, but it often requires external models or algorithms for token assignment. These external processes may not align with the LLM's internal pretrained tokenization schema, leading to inconsistencies and reduced model performance. To address these limitations, we propose a self-improving item tokenization method that allows the LLM to refine its own item tokenizations during training process. Our approach starts with item tokenizations generated by any external model and periodically adjusts these tokenizations based on the LLM's learned patterns. Such alignment process ensures consistency between the tokenization and the LLM's internal understanding of the items, leading to more accurate recommendations. Furthermore, our method is simple to implement and can be integrated as a plug-and-play enhancement into existing generative recommendation systems. Experimental results on multiple datasets and using various initial tokenization strategies demonstrate the effectiveness of our method, with an average improvement of 8\% in recommendation performance.</li>
</ul>

<h3>Title: Foundation Model for Lossy Compression of Spatiotemporal Scientific Data</h3>
<ul>
<li><strong>Authors: </strong>Xiao Li, Jaemoon Lee, Anand Rangarajan, Sanjay Ranka</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17184">https://arxiv.org/abs/2412.17184</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17184">https://arxiv.org/pdf/2412.17184</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17184]] Foundation Model for Lossy Compression of Spatiotemporal Scientific Data(https://arxiv.org/abs/2412.17184)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>We present a foundation model (FM) for lossy scientific data compression, combining a variational autoencoder (VAE) with a hyper-prior structure and a super-resolution (SR) module. The VAE framework uses hyper-priors to model latent space dependencies, enhancing compression efficiency. The SR module refines low-resolution representations into high-resolution outputs, improving reconstruction quality. By alternating between 2D and 3D convolutions, the model efficiently captures spatiotemporal correlations in scientific data while maintaining low computational cost. Experimental results demonstrate that the FM generalizes well to unseen domains and varying data shapes, achieving up to 4 times higher compression ratios than state-of-the-art methods after domain-specific fine-tuning. The SR module improves compression ratio by 30 percent compared to simple upsampling techniques. This approach significantly reduces storage and transmission costs for large-scale scientific simulations while preserving data integrity and fidelity.</li>
</ul>

<h3>Title: Dual Conditioned Motion Diffusion for Pose-Based Video Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Andi Xu, Hongsong Wang, Pinle Ding, Jie Gui</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17210">https://arxiv.org/abs/2412.17210</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17210">https://arxiv.org/pdf/2412.17210</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17210]] Dual Conditioned Motion Diffusion for Pose-Based Video Anomaly Detection(https://arxiv.org/abs/2412.17210)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, anomaly</a></li>
<li><strong>Abstract: </strong>Video Anomaly Detection (VAD) is essential for computer vision research. Existing VAD methods utilize either reconstruction-based or prediction-based frameworks. The former excels at detecting irregular patterns or structures, whereas the latter is capable of spotting abnormal deviations or trends. We address pose-based video anomaly detection and introduce a novel framework called Dual Conditioned Motion Diffusion (DCMD), which enjoys the advantages of both approaches. The DCMD integrates conditioned motion and conditioned embedding to comprehensively utilize the pose characteristics and latent semantics of observed movements, respectively. In the reverse diffusion process, a motion transformer is proposed to capture potential correlations from multi-layered characteristics within the spectrum space of human motion. To enhance the discriminability between normal and abnormal instances, we design a novel United Association Discrepancy (UAD) regularization that primarily relies on a Gaussian kernel-based time association and a self-attention-based global association. Finally, a mask completion strategy is introduced during the inference stage of the reverse diffusion process to enhance the utilization of conditioned motion for the prediction branch of anomaly detection. Extensive experiments on four datasets demonstrate that our method dramatically outperforms state-of-the-art methods and exhibits superior generalization performance.</li>
</ul>

<h3>Title: Discriminative Image Generation with Diffusion Models for Zero-Shot Learning</h3>
<ul>
<li><strong>Authors: </strong>Dingjie Fu, Wenjin Hou, Shiming Chen, Shuhuang Chen, Xinge You, Salman Khan, Fahad Shahbaz Khan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17219">https://arxiv.org/abs/2412.17219</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17219">https://arxiv.org/pdf/2412.17219</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17219]] Discriminative Image Generation with Diffusion Models for Zero-Shot Learning(https://arxiv.org/abs/2412.17219)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Generative Zero-Shot Learning (ZSL) methods synthesize class-related features based on predefined class semantic prototypes, showcasing superior performance. However, this feature generation paradigm falls short of providing interpretable insights. In addition, existing approaches rely on semantic prototypes annotated by human experts, which exhibit a significant limitation in their scalability to generalized scenes. To overcome these deficiencies, a natural solution is to generate images for unseen classes using text prompts. To this end, We present DIG-ZSL, a novel Discriminative Image Generation framework for Zero-Shot Learning. Specifically, to ensure the generation of discriminative images for training an effective ZSL classifier, we learn a discriminative class token (DCT) for each unseen class under the guidance of a pre-trained category discrimination model (CDM). Harnessing DCTs, we can generate diverse and high-quality images, which serve as informative unseen samples for ZSL tasks. In this paper, the extensive experiments and visualizations on four datasets show that our DIG-ZSL: (1) generates diverse and high-quality images, (2) outperforms previous state-of-the-art nonhuman-annotated semantic prototype-based methods by a large margin, and (3) achieves comparable or better performance than baselines that leverage human-annotated semantic prototypes. The codes will be made available upon acceptance of the paper.</li>
</ul>

<h3>Title: CharGen: High Accurate Character-Level Visual Text Generation Model with MultiModal Encoder</h3>
<ul>
<li><strong>Authors: </strong>Lichen Ma, Tiezhu Yue, Pei Fu, Yujie Zhong, Kai Zhou, Xiaoming Wei, Jie Hu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17225">https://arxiv.org/abs/2412.17225</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17225">https://arxiv.org/pdf/2412.17225</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17225]] CharGen: High Accurate Character-Level Visual Text Generation Model with MultiModal Encoder(https://arxiv.org/abs/2412.17225)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recently, significant advancements have been made in diffusion-based visual text generation models. Although the effectiveness of these methods in visual text rendering is rapidly improving, they still encounter challenges such as inaccurate characters and strokes when rendering complex visual text. In this paper, we propose CharGen, a highly accurate character-level visual text generation and editing model. Specifically, CharGen employs a character-level multimodal encoder that not only extracts character-level text embeddings but also encodes glyph images character by character. This enables it to capture fine-grained cross-modality features more effectively. Additionally, we introduce a new perceptual loss in CharGen to enhance character shape supervision and address the issue of inaccurate strokes in generated text. It is worth mentioning that CharGen can be integrated into existing diffusion models to generate visual text with high accuracy. CharGen significantly improves text rendering accuracy, outperforming recent methods in public benchmarks such as AnyText-benchmark and MARIO-Eval, with improvements of more than 8% and 6%, respectively. Notably, CharGen achieved a 5.5% increase in accuracy on Chinese test sets.</li>
</ul>

<h3>Title: OLiDM: Object-aware LiDAR Diffusion Models for Autonomous Driving</h3>
<ul>
<li><strong>Authors: </strong>Tianyi Yan, Junbo Yin, Xianpeng Lang, Ruigang Yang, Cheng-Zhong Xu, Jianbing Shen</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.RO</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17226">https://arxiv.org/abs/2412.17226</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17226">https://arxiv.org/pdf/2412.17226</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17226]] OLiDM: Object-aware LiDAR Diffusion Models for Autonomous Driving(https://arxiv.org/abs/2412.17226)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>To enhance autonomous driving safety in complex scenarios, various methods have been proposed to simulate LiDAR point cloud data. Nevertheless, these methods often face challenges in producing high-quality, diverse, and controllable foreground objects. To address the needs of object-aware tasks in 3D perception, we introduce OLiDM, a novel framework capable of generating high-fidelity LiDAR data at both the object and the scene levels. OLiDM consists of two pivotal components: the Object-Scene Progressive Generation (OPG) module and the Object Semantic Alignment (OSA) module. OPG adapts to user-specific prompts to generate desired foreground objects, which are subsequently employed as conditions in scene generation, ensuring controllable outputs at both the object and scene levels. This also facilitates the association of user-defined object-level annotations with the generated LiDAR scenes. Moreover, OSA aims to rectify the misalignment between foreground objects and background scenes, enhancing the overall quality of the generated objects. The broad effectiveness of OLiDM is demonstrated across various LiDAR generation tasks, as well as in 3D perception tasks. Specifically, on the KITTI-360 dataset, OLiDM surpasses prior state-of-the-art methods such as UltraLiDAR by 17.5 in FPD. Additionally, in sparse-to-dense LiDAR completion, OLiDM achieves a significant improvement over LiDARGen, with a 57.47\% increase in semantic IoU. Moreover, OLiDM enhances the performance of mainstream 3D detectors by 2.4\% in mAP and 1.9\% in NDS, underscoring its potential in advancing object-aware 3D tasks. Code is available at: this https URL.</li>
</ul>

<h3>Title: Enhancing Multi-Text Long Video Generation Consistency without Tuning: Time-Frequency Analysis, Prompt Alignment, and Theory</h3>
<ul>
<li><strong>Authors: </strong>Xingyao Li, Fengzhuo Zhang, Jiachun Pan, Yunlong Hou, Vincent Y. F. Tan, Zhuoran Yang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17254">https://arxiv.org/abs/2412.17254</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17254">https://arxiv.org/pdf/2412.17254</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17254]] Enhancing Multi-Text Long Video Generation Consistency without Tuning: Time-Frequency Analysis, Prompt Alignment, and Theory(https://arxiv.org/abs/2412.17254)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Despite the considerable progress achieved in the long video generation problem, there is still significant room to improve the consistency of the videos, particularly in terms of smoothness and transitions between scenes. We address these issues to enhance the consistency and coherence of videos generated with either single or multiple prompts. We propose the Time-frequency based temporal Attention Reweighting Algorithm (TiARA), which meticulously edits the attention score matrix based on the Discrete Short-Time Fourier Transform. Our method is supported by a theoretical guarantee, the first-of-its-kind for frequency-based methods in diffusion models. For videos generated by multiple prompts, we further investigate key factors affecting prompt interpolation quality and propose PromptBlend, an advanced prompt interpolation pipeline. The efficacy of our proposed method is validated via extensive experimental results, exhibiting consistent and impressive improvements over baseline methods. The code will be released upon acceptance.</li>
</ul>

<h3>Title: Unlocking Cross-Lingual Sentiment Analysis through Emoji Interpretation: A Multimodal Generative AI Approach</h3>
<ul>
<li><strong>Authors: </strong>Rafid Ishrak Jahan, Heng Fan, Haihua Chen, Yunhe Feng</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17255">https://arxiv.org/abs/2412.17255</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17255">https://arxiv.org/pdf/2412.17255</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17255]] Unlocking Cross-Lingual Sentiment Analysis through Emoji Interpretation: A Multimodal Generative AI Approach(https://arxiv.org/abs/2412.17255)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Emojis have become ubiquitous in online communication, serving as a universal medium to convey emotions and decorative elements. Their widespread use transcends language and cultural barriers, enhancing understanding and fostering more inclusive interactions. While existing work gained valuable insight into emojis understanding, exploring emojis' capability to serve as a universal sentiment indicator leveraging large language models (LLMs) has not been thoroughly examined. Our study aims to investigate the capacity of emojis to serve as reliable sentiment markers through LLMs across languages and cultures. We leveraged the multimodal capabilities of ChatGPT to explore the sentiments of various representations of emojis and evaluated how well emoji-conveyed sentiment aligned with text sentiment on a multi-lingual dataset collected from 32 countries. Our analysis reveals that the accuracy of LLM-based emoji-conveyed sentiment is 81.43%, underscoring emojis' significant potential to serve as a universal sentiment marker. We also found a consistent trend that the accuracy of sentiment conveyed by emojis increased as the number of emojis grew in text. The results reinforce the potential of emojis to serve as global sentiment indicators, offering insight into fields such as cross-lingual and cross-cultural sentiment analysis on social media platforms. Code: this https URL.</li>
</ul>

<h3>Title: VarAD: Lightweight High-Resolution Image Anomaly Detection via Visual Autoregressive Modeling</h3>
<ul>
<li><strong>Authors: </strong>Yunkang Cao, Haiming Yao, Wei Luo, Weiming Shen</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17263">https://arxiv.org/abs/2412.17263</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17263">https://arxiv.org/pdf/2412.17263</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17263]] VarAD: Lightweight High-Resolution Image Anomaly Detection via Visual Autoregressive Modeling(https://arxiv.org/abs/2412.17263)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>This paper addresses a practical task: High-Resolution Image Anomaly Detection (HRIAD). In comparison to conventional image anomaly detection for low-resolution images, HRIAD imposes a heavier computational burden and necessitates superior global information capture capacity. To tackle HRIAD, this paper translates image anomaly detection into visual token prediction and proposes VarAD based on visual autoregressive modeling for token prediction. Specifically, VarAD first extracts multi-hierarchy and multi-directional visual token sequences, and then employs an advanced model, Mamba, for visual autoregressive modeling and token prediction. During the prediction process, VarAD effectively exploits information from all preceding tokens to predict the target token. Finally, the discrepancies between predicted tokens and original tokens are utilized to score anomalies. Comprehensive experiments on four publicly available datasets and a real-world button inspection dataset demonstrate that the proposed VarAD achieves superior high-resolution image anomaly detection performance while maintaining lightweight, rendering VarAD a viable solution for HRIAD. Code is available at \href{this https URL}{\url{this https URL}}.</li>
</ul>

<h3>Title: Enabling Time-series Foundation Model for Building Energy Forecasting via Contrastive Curriculum Learning</h3>
<ul>
<li><strong>Authors: </strong>Rui Liang, Yang Deng, Donghua Xie, Fang He, Dan Wang</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17285">https://arxiv.org/abs/2412.17285</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17285">https://arxiv.org/pdf/2412.17285</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17285]] Enabling Time-series Foundation Model for Building Energy Forecasting via Contrastive Curriculum Learning(https://arxiv.org/abs/2412.17285)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>Advances in time-series forecasting are driving a shift from conventional machine learning models to foundation models (FMs) that are trained with generalized knowledge. However, existing FMs still perform poorly in the energy fields, such as building energy forecasting (BEF). This paper studies the adaptation of FM to BEF tasks. We demonstrate the shortcomings of fine-tuning FM straightforwardly from both the perspectives of FM and the data. To overcome these limitations, we propose a new \textit{contrastive curriculum learning}-based training method. Our method optimizes the ordering of training data in the context of TSFM adaptation. Experiments show that our method can improve the zero/few-shot performance by 14.6\% compared to the existing FMs. Our code and new TSFM will be available at <Anonymous Github Repo>.</li>
</ul>

<h3>Title: Free-viewpoint Human Animation with Pose-correlated Reference Selection</h3>
<ul>
<li><strong>Authors: </strong>Fa-Ting Hong, Zhan Xu, Haiyang Liu, Qinjie Lin, Luchuan Song, Zhixin Shu, Yang Zhou, Duygu Ceylan, Dan Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17290">https://arxiv.org/abs/2412.17290</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17290">https://arxiv.org/pdf/2412.17290</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17290]] Free-viewpoint Human Animation with Pose-correlated Reference Selection(https://arxiv.org/abs/2412.17290)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Diffusion-based human animation aims to animate a human character based on a source human image as well as driving signals such as a sequence of poses. Leveraging the generative capacity of diffusion model, existing approaches are able to generate high-fidelity poses, but struggle with significant viewpoint changes, especially in zoom-in/zoom-out scenarios where camera-character distance varies. This limits the applications such as cinematic shot type plan or camera control. We propose a pose-correlated reference selection diffusion network, supporting substantial viewpoint variations in human animation. Our key idea is to enable the network to utilize multiple reference images as input, since significant viewpoint changes often lead to missing appearance details on the human body. To eliminate the computational cost, we first introduce a novel pose correlation module to compute similarities between non-aligned target and source poses, and then propose an adaptive reference selection strategy, utilizing the attention map to identify key regions for animation generation. To train our model, we curated a large dataset from public TED talks featuring varied shots of the same character, helping the model learn synthesis for different perspectives. Our experimental results show that with the same number of reference images, our model performs favorably compared to the current SOTA methods under large viewpoint change. We further show that the adaptive reference selection is able to choose the most relevant reference regions to generate humans under free viewpoints.</li>
</ul>

<h3>Title: Friends-MMC: A Dataset for Multi-modal Multi-party Conversation Understanding</h3>
<ul>
<li><strong>Authors: </strong>Yueqian Wang, Xiaojun Meng, Yuxuan Wang, Jianxin Liang, Qun Liu, Dongyan Zhao</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17295">https://arxiv.org/abs/2412.17295</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17295">https://arxiv.org/pdf/2412.17295</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17295]] Friends-MMC: A Dataset for Multi-modal Multi-party Conversation Understanding(https://arxiv.org/abs/2412.17295)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Multi-modal multi-party conversation (MMC) is a less studied yet important topic of research due to that it well fits real-world scenarios and thus potentially has more widely-used applications. Compared with the traditional multi-modal conversations, MMC requires stronger character-centered understanding abilities as there are many interlocutors appearing in both the visual and textual context. To facilitate the study of this problem, we present Friends-MMC in this paper, an MMC dataset that contains 24,000+ unique utterances paired with video context. To explore the character-centered understanding of the dialogue, we also annotate the speaker of each utterance, the names and bounding bboxes of faces that appear in the video. Based on this Friends-MMC dataset, we further study two fundamental MMC tasks: conversation speaker identification and conversation response prediction, both of which have the multi-party nature with the video or image as visual context. For conversation speaker identification, we demonstrate the inefficiencies of existing methods such as pre-trained models, and propose a simple yet effective baseline method that leverages an optimization solver to utilize the context of two modalities to achieve better performance. For conversation response prediction, we fine-tune generative dialogue models on Friend-MMC, and analyze the benefits of speaker information. The code and dataset is publicly available at this https URL and thus we call for more attention on modeling speaker information when understanding conversations.</li>
</ul>

<h3>Title: Revisiting Multimodal Fusion for 3D Anomaly Detection from an Architectural Perspective</h3>
<ul>
<li><strong>Authors: </strong>Kaifang Long, Guoyang Xie, Lianbo Ma, Jiaqi Liu, Zhichao Lu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17297">https://arxiv.org/abs/2412.17297</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17297">https://arxiv.org/pdf/2412.17297</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17297]] Revisiting Multimodal Fusion for 3D Anomaly Detection from an Architectural Perspective(https://arxiv.org/abs/2412.17297)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Existing efforts to boost multimodal fusion of 3D anomaly detection (3D-AD) primarily concentrate on devising more effective multimodal fusion strategies. However, little attention was devoted to analyzing the role of multimodal fusion architecture (topology) design in contributing to 3D-AD. In this paper, we aim to bridge this gap and present a systematic study on the impact of multimodal fusion architecture design on 3D-AD. This work considers the multimodal fusion architecture design at the intra-module fusion level, i.e., independent modality-specific modules, involving early, middle or late multimodal features with specific fusion operations, and also at the inter-module fusion level, i.e., the strategies to fuse those modules. In both cases, we first derive insights through theoretically and experimentally exploring how architectural designs influence 3D-AD. Then, we extend SOTA neural architecture search (NAS) paradigm and propose 3D-ADNAS to simultaneously search across multimodal fusion strategies and modality-specific modules for the first this http URL experiments show that 3D-ADNAS obtains consistent improvements in 3D-AD across various model capacities in terms of accuracy, frame rate, and memory usage, and it exhibits great potential in dealing with few-shot 3D-AD tasks.</li>
</ul>

<h3>Title: Broadband Ground Motion Synthesis by Diffusion Model with Minimal Condition</h3>
<ul>
<li><strong>Authors: </strong>Jaeheun Jung, Jaehyuk Lee, Chang-Hae Jung, Hanyoung Kim, Bosung Jung, Donghun Lee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, physics.geo-ph</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17333">https://arxiv.org/abs/2412.17333</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17333">https://arxiv.org/pdf/2412.17333</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17333]] Broadband Ground Motion Synthesis by Diffusion Model with Minimal Condition(https://arxiv.org/abs/2412.17333)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Earthquakes are rare. Hence there is a fundamental call for reliable methods to generate realistic ground motion data for data-driven approaches in seismology. Recent GAN-based methods fall short of the call, as the methods either require special information such as geological traits or generate subpar waveforms that fail to satisfy seismological constraints such as phase arrival times. We propose a specialized Latent Diffusion Model (LDM) that reliably generates realistic waveforms after learning from real earthquake data with minimal conditions: location and magnitude. We also design a domain-specific training method that exploits the traits of earthquake dataset: multiple observed waveforms time-aligned and paired to each earthquake source that are tagged with seismological metadata comprised of earthquake magnitude, depth of focus, and the locations of epicenter and seismometers. We construct the time-aligned earthquake dataset using Southern California Earthquake Data Center (SCEDC) API, and train our model with the dataset and our proposed training method for performance evaluation. Our model surpasses all comparable data-driven methods in various test criteria not only from waveform generation domain but also from seismology such as phase arrival time, GMPE analysis, and spectrum analysis. Our result opens new future research directions for deep learning applications in seismology.</li>
</ul>

<h3>Title: FFA Sora, video generation as fundus fluorescein angiography simulator</h3>
<ul>
<li><strong>Authors: </strong>Xinyuan Wu, Lili Wang, Ruoyu Chen, Bowen Liu, Weiyi Zhang, Xi Yang, Yifan Feng, Mingguang He, Danli Shi</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17346">https://arxiv.org/abs/2412.17346</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17346">https://arxiv.org/pdf/2412.17346</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17346]] FFA Sora, video generation as fundus fluorescein angiography simulator(https://arxiv.org/abs/2412.17346)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Fundus fluorescein angiography (FFA) is critical for diagnosing retinal vascular diseases, but beginners often struggle with image interpretation. This study develops FFA Sora, a text-to-video model that converts FFA reports into dynamic videos via a Wavelet-Flow Variational Autoencoder (WF-VAE) and a diffusion transformer (DiT). Trained on an anonymized dataset, FFA Sora accurately simulates disease features from the input text, as confirmed by objective metrics: Frechet Video Distance (FVD) = 329.78, Learned Perceptual Image Patch Similarity (LPIPS) = 0.48, and Visual-question-answering Score (VQAScore) = 0.61. Specific evaluations showed acceptable alignment between the generated videos and textual prompts, with BERTScore of 0.35. Additionally, the model demonstrated strong privacy-preserving performance in retrieval evaluations, achieving an average Recall@K of 0.073. Human assessments indicated satisfactory visual quality, with an average score of 1.570(scale: 1 = best, 5 = worst). This model addresses privacy concerns associated with sharing large-scale FFA data and enhances medical education.</li>
</ul>

<h3>Title: ORIGAMI: A generative transformer architecture for predictions from semi-structured data</h3>
<ul>
<li><strong>Authors: </strong>Thomas RÃ¼ckstieÃ, Alana Huang, Robin Vujanic</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17348">https://arxiv.org/abs/2412.17348</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17348">https://arxiv.org/pdf/2412.17348</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17348]] ORIGAMI: A generative transformer architecture for predictions from semi-structured data(https://arxiv.org/abs/2412.17348)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Despite the popularity and widespread use of semi-structured data formats such as JSON, end-to-end supervised learning applied directly to such data remains underexplored. We present ORIGAMI (Object RepresentatIon via Generative Autoregressive ModellIng), a transformer-based architecture that directly processes nested key/value pairs while preserving their hierarchical semantics. Our key technical contributions include: (1) a structure-preserving tokenizer, (2) a novel key/value position encoding scheme, and (3) a grammar-constrained training and inference framework that ensures valid outputs and accelerates training convergence. These enhancements enable efficient end-to-end modeling of semi-structured data. By reformulating classification as next-token prediction, ORIGAMI naturally handles both single-label and multi-label tasks without architectural modifications. Empirical evaluation across diverse domains demonstrates ORIGAMI's effectiveness: On standard tabular benchmarks converted to JSON, ORIGAMI remains competitive with classical and state-of-the-art approaches. On native JSON datasets, we outperform baselines on multi-label classification and specialized models such as convolutional and graph neural networks on a code classification task. Through extensive ablation studies, we validate the impact of each architectural component and establish ORIGAMI as a robust framework for end-to-end learning on semi-structured data.</li>
</ul>

<h3>Title: Singular Value Scaling: Efficient Generative Model Compression via Pruned Weights Refinement</h3>
<ul>
<li><strong>Authors: </strong>Hyeonjin Kim, Jaejun Yoo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17387">https://arxiv.org/abs/2412.17387</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17387">https://arxiv.org/pdf/2412.17387</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17387]] Singular Value Scaling: Efficient Generative Model Compression via Pruned Weights Refinement(https://arxiv.org/abs/2412.17387)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>While pruning methods effectively maintain model performance without extra training costs, they often focus solely on preserving crucial connections, overlooking the impact of pruned weights on subsequent fine-tuning or distillation, leading to inefficiencies. Moreover, most compression techniques for generative models have been developed primarily for GANs, tailored to specific architectures like StyleGAN, and research into compressing Diffusion models has just begun. Even more, these methods are often applicable only to GANs or Diffusion models, highlighting the need for approaches that work across both model types. In this paper, we introduce Singular Value Scaling (SVS), a versatile technique for refining pruned weights, applicable to both model types. Our analysis reveals that pruned weights often exhibit dominant singular vectors, hindering fine-tuning efficiency and leading to suboptimal performance compared to random initialization. Our method enhances weight initialization by minimizing the disparities between singular values of pruned weights, thereby improving the fine-tuning process. This approach not only guides the compressed model toward superior solutions but also significantly speeds up fine-tuning. Extensive experiments on StyleGAN2, StyleGAN3 and DDPM demonstrate that SVS improves compression performance across model types without additional training costs. Our code is available at: this https URL.</li>
</ul>

<h3>Title: Multimodal Preference Data Synthetic Alignment with Reward Model</h3>
<ul>
<li><strong>Authors: </strong>Robert Wijaya, Ngoc-Bao Nguyen, Ngai-Man Cheung</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17417">https://arxiv.org/abs/2412.17417</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17417">https://arxiv.org/pdf/2412.17417</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17417]] Multimodal Preference Data Synthetic Alignment with Reward Model(https://arxiv.org/abs/2412.17417)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Multimodal large language models (MLLMs) have significantly advanced tasks like caption generation and visual question answering by integrating visual and textual data. However, they sometimes produce misleading or hallucinate content due to discrepancies between their pre-training data and real user prompts. Existing approaches using Direct Preference Optimization (DPO) in vision-language tasks often rely on strong models like GPT-4 or CLIP to determine positive and negative responses. Here, we propose a new framework in generating synthetic data using a reward model as a proxy of human preference for effective multimodal alignment with DPO training. The resulting DPO dataset ranges from 2K to 9K image-text pairs, was evaluated on LLaVA-v1.5-7B, where our approach demonstrated substantial improvements in both the trustworthiness and reasoning capabilities of the base model across multiple hallucination and vision-language benchmark. The experiment results indicate that integrating selected synthetic data, such as from generative and rewards models can effectively reduce reliance on human-annotated data while enhancing MLLMs' alignment capability, offering a scalable solution for safer deployment.</li>
</ul>

<h3>Title: Progressive Boundary Guided Anomaly Synthesis for Industrial Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Qiyu Chen, Huiyuan Luo, Han Gao, Chengkan Lv, Zhengtao Zhang</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17458">https://arxiv.org/abs/2412.17458</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17458">https://arxiv.org/pdf/2412.17458</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17458]] Progressive Boundary Guided Anomaly Synthesis for Industrial Anomaly Detection(https://arxiv.org/abs/2412.17458)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Unsupervised anomaly detection methods can identify surface defects in industrial images by leveraging only normal samples for training. Due to the risk of overfitting when learning from a single class, anomaly synthesis strategies are introduced to enhance detection capability by generating artificial anomalies. However, existing strategies heavily rely on anomalous textures from auxiliary datasets. Moreover, their limitations in the coverage and directionality of anomaly synthesis may result in a failure to capture useful information and lead to significant redundancy. To address these issues, we propose a novel Progressive Boundary-guided Anomaly Synthesis (PBAS) strategy, which can directionally synthesize crucial feature-level anomalies without auxiliary textures. It consists of three core components: Approximate Boundary Learning (ABL), Anomaly Feature Synthesis (AFS), and Refined Boundary Optimization (RBO). To make the distribution of normal samples more compact, ABL first learns an approximate decision boundary by center constraint, which improves the center initialization through feature alignment. AFS then directionally synthesizes anomalies with more flexible scales guided by the hypersphere distribution of normal features. Since the boundary is so loose that it may contain real anomalies, RBO refines the decision boundary through the binary classification of artificial anomalies and normal features. Experimental results show that our method achieves state-of-the-art performance and the fastest detection speed on three widely used industrial datasets, including MVTec AD, VisA, and MPDD. The code will be available at: this https URL.</li>
</ul>

<h3>Title: CALLIC: Content Adaptive Learning for Lossless Image Compression</h3>
<ul>
<li><strong>Authors: </strong>Daxin Li, Yuanchao Bai, Kai Wang, Junjun Jiang, Xianming Liu, Wen Gao</a></li>
<li><strong>Subjects: </strong>cs.CV, eess.IV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17464">https://arxiv.org/abs/2412.17464</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17464">https://arxiv.org/pdf/2412.17464</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17464]] CALLIC: Content Adaptive Learning for Lossless Image Compression(https://arxiv.org/abs/2412.17464)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Learned lossless image compression has achieved significant advancements in recent years. However, existing methods often rely on training amortized generative models on massive datasets, resulting in sub-optimal probability distribution estimation for specific testing images during encoding process. To address this challenge, we explore the connection between the Minimum Description Length (MDL) principle and Parameter-Efficient Transfer Learning (PETL), leading to the development of a novel content-adaptive approach for learned lossless image compression, dubbed CALLIC. Specifically, we first propose a content-aware autoregressive self-attention mechanism by leveraging convolutional gating operations, termed Masked Gated ConvFormer (MGCF), and pretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed to accelerate the coding process. During encoding, we decompose pre-trained layers, including depth-wise convolutions, using low-rank matrices and then adapt the incremental weights on testing image by Rate-guided Progressive Fine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are sorted in descending order by estimated entropy, optimizing learning process and reducing adaptation time. Extensive experiments across diverse datasets demonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless image compression.</li>
</ul>

<h3>Title: A Survey on Multi-Generative Agent System: Recent Advances and New Frontiers</h3>
<ul>
<li><strong>Authors: </strong>Shuaihang Chen, Yuanxing Liu, Wei Han, Weinan Zhang, Ting Liu</a></li>
<li><strong>Subjects: </strong>cs.CL, cs.MA</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17481">https://arxiv.org/abs/2412.17481</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17481">https://arxiv.org/pdf/2412.17481</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17481]] A Survey on Multi-Generative Agent System: Recent Advances and New Frontiers(https://arxiv.org/abs/2412.17481)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Multi-generative agent systems (MGASs) have become a research hotspot since the rise of large language models (LLMs). However, with the continuous influx of new related works, the existing reviews struggle to capture them comprehensively. This paper presents a comprehensive survey of these studies. We first discuss the definition of MGAS, a framework encompassing much of previous work. We provide an overview of the various applications of MGAS in (i) solving complex tasks, (ii) simulating specific scenarios, and (iii) evaluating generative agents. Building on previous studies, we also highlight several challenges and propose future directions for research in this field.</li>
</ul>

<h3>Title: Improving the Noise Estimation of Latent Neural Stochastic Differential Equations</h3>
<ul>
<li><strong>Authors: </strong>Linus Heck, Maximilian Gelbrecht, Michael T. Schaub, Niklas Boers</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17499">https://arxiv.org/abs/2412.17499</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17499">https://arxiv.org/pdf/2412.17499</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17499]] Improving the Noise Estimation of Latent Neural Stochastic Differential Equations(https://arxiv.org/abs/2412.17499)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Latent neural stochastic differential equations (SDEs) have recently emerged as a promising approach for learning generative models from stochastic time series data. However, they systematically underestimate the noise level inherent in such data, limiting their ability to capture stochastic dynamics accurately. We investigate this underestimation in detail and propose a straightforward solution: by including an explicit additional noise regularization in the loss function, we are able to learn a model that accurately captures the diffusion component of the data. We demonstrate our results on a conceptual model system that highlights the improved latent neural SDE's capability to model stochastic bistable dynamics.</li>
</ul>

<h3>Title: DiffusionAttacker: Diffusion-Driven Prompt Manipulation for LLM Jailbreak</h3>
<ul>
<li><strong>Authors: </strong>Hao Wang, Hao Li, Junda Zhu, Xinyuan Wang, Chengwei Pan, MinLie Huang, Lei Sha</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17522">https://arxiv.org/abs/2412.17522</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17522">https://arxiv.org/pdf/2412.17522</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17522]] DiffusionAttacker: Diffusion-Driven Prompt Manipulation for LLM Jailbreak(https://arxiv.org/abs/2412.17522)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) are susceptible to generating harmful content when prompted with carefully crafted inputs, a vulnerability known as LLM jailbreaking. As LLMs become more powerful, studying jailbreak methods is critical to enhancing security and aligning models with human values. Traditionally, jailbreak techniques have relied on suffix addition or prompt templates, but these methods suffer from limited attack diversity. This paper introduces DiffusionAttacker, an end-to-end generative approach for jailbreak rewriting inspired by diffusion models. Our method employs a sequence-to-sequence (seq2seq) text diffusion model as a generator, conditioning on the original prompt and guiding the denoising process with a novel attack loss. Unlike previous approaches that use autoregressive LLMs to generate jailbreak prompts, which limit the modification of already generated tokens and restrict the rewriting space, DiffusionAttacker utilizes a seq2seq diffusion model, allowing more flexible token modifications. This approach preserves the semantic content of the original prompt while producing harmful content. Additionally, we leverage the Gumbel-Softmax technique to make the sampling process from the diffusion model's output distribution differentiable, eliminating the need for iterative token search. Extensive experiments on Advbench and Harmbench demonstrate that DiffusionAttacker outperforms previous methods across various evaluation metrics, including attack success rate (ASR), fluency, and diversity.</li>
</ul>

<h3>Title: Constructing Fair Latent Space for Intersection of Fairness and Explainability</h3>
<ul>
<li><strong>Authors: </strong>Hyungjun Joo, Hyeonggeun Han, Sehwan Kim, Sangwoo Hong, Jungwoo Lee</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.AI, cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17523">https://arxiv.org/abs/2412.17523</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17523">https://arxiv.org/pdf/2412.17523</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17523]] Constructing Fair Latent Space for Intersection of Fairness and Explainability(https://arxiv.org/abs/2412.17523)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>As the use of machine learning models has increased, numerous studies have aimed to enhance fairness. However, research on the intersection of fairness and explainability remains insufficient, leading to potential issues in gaining the trust of actual users. Here, we propose a novel module that constructs a fair latent space, enabling faithful explanation while ensuring fairness. The fair latent space is constructed by disentangling and redistributing labels and sensitive attributes, allowing the generation of counterfactual explanations for each type of information. Our module is attached to a pretrained generative model, transforming its biased latent space into a fair latent space. Additionally, since only the module needs to be trained, there are advantages in terms of time and cost savings, without the need to train the entire generative model. We validate the fair latent space with various fairness metrics and demonstrate that our approach can effectively provide explanations for biased decisions and assurances of fairness.</li>
</ul>

<h3>Title: S-INF: Towards Realistic Indoor Scene Synthesis via Scene Implicit Neural Field</h3>
<ul>
<li><strong>Authors: </strong>Zixi Liang, Guowei Xu, Haifeng Wu, Ye Huang, Wen Li, Lixin Duan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17561">https://arxiv.org/abs/2412.17561</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17561">https://arxiv.org/pdf/2412.17561</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17561]] S-INF: Towards Realistic Indoor Scene Synthesis via Scene Implicit Neural Field(https://arxiv.org/abs/2412.17561)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Learning-based methods have become increasingly popular in 3D indoor scene synthesis (ISS), showing superior performance over traditional optimization-based approaches. These learning-based methods typically model distributions on simple yet explicit scene representations using generative models. However, due to the oversimplified explicit representations that overlook detailed information and the lack of guidance from multimodal relationships within the scene, most learning-based methods struggle to generate indoor scenes with realistic object arrangements and styles. In this paper, we introduce a new method, Scene Implicit Neural Field (S-INF), for indoor scene synthesis, aiming to learn meaningful representations of multimodal relationships, to enhance the realism of indoor scene synthesis. S-INF assumes that the scene layout is often related to the object-detailed information. It disentangles the multimodal relationships into scene layout relationships and detailed object relationships, fusing them later through implicit neural fields (INFs). By learning specialized scene layout relationships and projecting them into S-INF, we achieve a realistic generation of scene layout. Additionally, S-INF captures dense and detailed object relationships through differentiable rendering, ensuring stylistic consistency across objects. Through extensive experiments on the benchmark 3D-FRONT dataset, we demonstrate that our method consistently achieves state-of-the-art performance under different types of ISS.</li>
</ul>

<h3>Title: The Dynamic Duo of Collaborative Masking and Target for Advanced Masked Autoencoder Learning</h3>
<ul>
<li><strong>Authors: </strong>Shentong Mo</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG, eess.IV, eess.SP</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17566">https://arxiv.org/abs/2412.17566</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17566">https://arxiv.org/pdf/2412.17566</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17566]] The Dynamic Duo of Collaborative Masking and Target for Advanced Masked Autoencoder Learning(https://arxiv.org/abs/2412.17566)</code><input type="text"></li>
<li><strong>Keywords: </strong>self-supervised</a></li>
<li><strong>Abstract: </strong>Masked autoencoders (MAE) have recently succeeded in self-supervised vision representation learning. Previous work mainly applied custom-designed (e.g., random, block-wise) masking or teacher (e.g., CLIP)-guided masking and targets. However, they ignore the potential role of the self-training (student) model in giving feedback to the teacher for masking and targets. In this work, we present to integrate Collaborative Masking and Targets for boosting Masked AutoEncoders, namely CMT-MAE. Specifically, CMT-MAE leverages a simple collaborative masking mechanism through linear aggregation across attentions from both teacher and student models. We further propose using the output features from those two models as the collaborative target of the decoder. Our simple and effective framework pre-trained on ImageNet-1K achieves state-of-the-art linear probing and fine-tuning performance. In particular, using ViT-base, we improve the fine-tuning results of the vanilla MAE from 83.6% to 85.7%.</li>
</ul>

<h3>Title: Towards Foundation Models on Graphs: An Analysis on Cross-Dataset Transfer of Pretrained GNNs</h3>
<ul>
<li><strong>Authors: </strong>Fabrizio Frasca, Fabian Jogl, Moshe Eliasof, Matan Ostrovsky, Carola-Bibiane SchÃ¶nlieb, Thomas GÃ¤rtner, Haggai Maron</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.NE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17609">https://arxiv.org/abs/2412.17609</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17609">https://arxiv.org/pdf/2412.17609</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17609]] Towards Foundation Models on Graphs: An Analysis on Cross-Dataset Transfer of Pretrained GNNs(https://arxiv.org/abs/2412.17609)</code><input type="text"></li>
<li><strong>Keywords: </strong>foundation model</a></li>
<li><strong>Abstract: </strong>To develop a preliminary understanding towards Graph Foundation Models, we study the extent to which pretrained Graph Neural Networks can be applied across datasets, an effort requiring to be agnostic to dataset-specific features and their encodings. We build upon a purely structural pretraining approach and propose an extension to capture feature information while still being feature-agnostic. We evaluate pretrained models on downstream tasks for varying amounts of training samples and choices of pretraining datasets. Our preliminary results indicate that embeddings from pretrained models improve generalization only with enough downstream data points and in a degree which depends on the quantity and properties of pretraining data. Feature information can lead to improvements, but currently requires some similarities between pretraining and downstream feature spaces.</li>
</ul>

<h3>Title: Kernel-Aware Graph Prompt Learning for Few-Shot Anomaly Detection</h3>
<ul>
<li><strong>Authors: </strong>Fenfang Tao, Guo-Sen Xie, Fang Zhao, Xiangbo Shu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17619">https://arxiv.org/abs/2412.17619</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17619">https://arxiv.org/pdf/2412.17619</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17619]] Kernel-Aware Graph Prompt Learning for Few-Shot Anomaly Detection(https://arxiv.org/abs/2412.17619)</code><input type="text"></li>
<li><strong>Keywords: </strong>anomaly</a></li>
<li><strong>Abstract: </strong>Few-shot anomaly detection (FSAD) aims to detect unseen anomaly regions with the guidance of very few normal support images from the same class. Existing FSAD methods usually find anomalies by directly designing complex text prompts to align them with visual features under the prevailing large vision-language model paradigm. However, these methods, almost always, neglect intrinsic contextual information in visual features, e.g., the interaction relationships between different vision layers, which is an important clue for detecting anomalies comprehensively. To this end, we propose a kernel-aware graph prompt learning framework, termed as KAG-prompt, by reasoning the cross-layer relations among visual features for FSAD. Specifically, a kernel-aware hierarchical graph is built by taking the different layer features focusing on anomalous regions of different sizes as nodes, meanwhile, the relationships between arbitrary pairs of nodes stand for the edges of the graph. By message passing over this graph, KAG-prompt can capture cross-layer contextual information, thus leading to more accurate anomaly prediction. Moreover, to integrate the information of multiple important anomaly signals in the prediction map, we propose a novel image-level scoring method based on multi-level information fusion. Extensive experiments on MVTecAD and VisA datasets show that KAG-prompt achieves state-of-the-art FSAD results for image-level/pixel-level anomaly detection. Code is available at this https URL.</li>
</ul>

<h3>Title: Be More Diverse than the Most Diverse: Online Selection of Diverse Mixtures of Generative Models</h3>
<ul>
<li><strong>Authors: </strong>Parham Rezaei, Farzan Farnia, Cheuk Ting Li</a></li>
<li><strong>Subjects: </strong>cs.LG, stat.ML</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17622">https://arxiv.org/abs/2412.17622</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17622">https://arxiv.org/pdf/2412.17622</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17622]] Be More Diverse than the Most Diverse: Online Selection of Diverse Mixtures of Generative Models(https://arxiv.org/abs/2412.17622)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>The availability of multiple training algorithms and architectures for generative models requires a selection mechanism to form a single model over a group of well-trained generation models. The selection task is commonly addressed by identifying the model that maximizes an evaluation score based on the diversity and quality of the generated data. However, such a best-model identification approach overlooks the possibility that a mixture of available models can outperform each individual model. In this work, we explore the selection of a mixture of multiple generative models and formulate a quadratic optimization problem to find an optimal mixture model achieving the maximum of kernel-based evaluation scores including kernel inception distance (KID) and RÃ©nyi kernel entropy (RKE). To identify the optimal mixture of the models using the fewest possible sample queries, we propose an online learning approach called Mixture Upper Confidence Bound (Mixture-UCB). Specifically, our proposed online learning method can be extended to every convex quadratic function of the mixture weights, for which we prove a concentration bound to enable the application of the UCB approach. We prove a regret bound for the proposed Mixture-UCB algorithm and perform several numerical experiments to show the success of the proposed Mixture-UCB method in finding the optimal mixture of text-based and image-based generative models. The codebase is available at this https URL .</li>
</ul>

<h3>Title: Detail-Preserving Latent Diffusion for Stable Shadow Removal</h3>
<ul>
<li><strong>Authors: </strong>Jiamin Xu, Yuxin Zheng, Zelong Li, Chi Wang, Renshu Gu, Weiwei Xu, Gang Xu</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17630">https://arxiv.org/abs/2412.17630</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17630">https://arxiv.org/pdf/2412.17630</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17630]] Detail-Preserving Latent Diffusion for Stable Shadow Removal(https://arxiv.org/abs/2412.17630)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Achieving high-quality shadow removal with strong generalizability is challenging in scenes with complex global illumination. Due to the limited diversity in shadow removal datasets, current methods are prone to overfitting training data, often leading to reduced performance on unseen cases. To address this, we leverage the rich visual priors of a pre-trained Stable Diffusion (SD) model and propose a two-stage fine-tuning pipeline to adapt the SD model for stable and efficient shadow removal. In the first stage, we fix the VAE and fine-tune the denoiser in latent space, which yields substantial shadow removal but may lose some high-frequency details. To resolve this, we introduce a second stage, called the detail injection stage. This stage selectively extracts features from the VAE encoder to modulate the decoder, injecting fine details into the final results. Experimental results show that our method outperforms state-of-the-art shadow removal techniques. The cross-dataset evaluation further demonstrates that our method generalizes effectively to unseen data, enhancing the applicability of shadow removal methods.</li>
</ul>

<h3>Title: DreamFit: Garment-Centric Human Generation via a Lightweight Anything-Dressing Encoder</h3>
<ul>
<li><strong>Authors: </strong>Ente Lin, Xujie Zhang, Fuwei Zhao, Yuxuan Luo, Xin Dong, Long Zeng, Xiaodan Liang</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17644">https://arxiv.org/abs/2412.17644</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17644">https://arxiv.org/pdf/2412.17644</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17644]] DreamFit: Garment-Centric Human Generation via a Lightweight Anything-Dressing Encoder(https://arxiv.org/abs/2412.17644)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Diffusion models for garment-centric human generation from text or image prompts have garnered emerging attention for their great application potential. However, existing methods often face a dilemma: lightweight approaches, such as adapters, are prone to generate inconsistent textures; while finetune-based methods involve high training costs and struggle to maintain the generalization capabilities of pretrained diffusion models, limiting their performance across diverse scenarios. To address these challenges, we propose DreamFit, which incorporates a lightweight Anything-Dressing Encoder specifically tailored for the garment-centric human generation. DreamFit has three key advantages: (1) \textbf{Lightweight training}: with the proposed adaptive attention and LoRA modules, DreamFit significantly minimizes the model complexity to 83.4M trainable parameters. (2)\textbf{Anything-Dressing}: Our model generalizes surprisingly well to a wide range of (non-)garments, creative styles, and prompt instructions, consistently delivering high-quality results across diverse scenarios. (3) \textbf{Plug-and-play}: DreamFit is engineered for smooth integration with any community control plugins for diffusion models, ensuring easy compatibility and minimizing adoption barriers. To further enhance generation quality, DreamFit leverages pretrained large multi-modal models (LMMs) to enrich the prompt with fine-grained garment descriptions, thereby reducing the prompt gap between training and inference. We conduct comprehensive experiments on both $768 \times 512$ high-resolution benchmarks and in-the-wild images. DreamFit surpasses all existing methods, highlighting its state-of-the-art capabilities of garment-centric human generation.</li>
</ul>

<h3>Title: Benchmarking Generative AI Models for Deep Learning Test Input Generation</h3>
<ul>
<li><strong>Authors: </strong>Maryam, Matteo Biagiola, Andrea Stocco, Vincenzo Riccio</a></li>
<li><strong>Subjects: </strong>cs.LG, cs.SE</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17652">https://arxiv.org/abs/2412.17652</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17652">https://arxiv.org/pdf/2412.17652</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17652]] Benchmarking Generative AI Models for Deep Learning Test Input Generation(https://arxiv.org/abs/2412.17652)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Test Input Generators (TIGs) are crucial to assess the ability of Deep Learning (DL) image classifiers to provide correct predictions for inputs beyond their training and test sets. Recent advancements in Generative AI (GenAI) models have made them a powerful tool for creating and manipulating synthetic images, although these advancements also imply increased complexity and resource demands for training. In this work, we benchmark and combine different GenAI models with TIGs, assessing their effectiveness, efficiency, and quality of the generated test images, in terms of domain validity and label preservation. We conduct an empirical study involving three different GenAI architectures (VAEs, GANs, Diffusion Models), five classification tasks of increasing complexity, and 364 human evaluations. Our results show that simpler architectures, such as VAEs, are sufficient for less complex datasets like MNIST. However, when dealing with feature-rich datasets, such as ImageNet, more sophisticated architectures like Diffusion Models achieve superior performance by generating a higher number of valid, misclassification-inducing inputs.</li>
</ul>

<h3>Title: A Bias-Free Training Paradigm for More General AI-generated Image Detection</h3>
<ul>
<li><strong>Authors: </strong>Fabrizio Guillaro, Giada Zingarini, Ben Usman, Avneesh Sud, Davide Cozzolino, Luisa Verdoliva</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17671">https://arxiv.org/abs/2412.17671</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17671">https://arxiv.org/pdf/2412.17671</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17671]] A Bias-Free Training Paradigm for More General AI-generated Image Detection(https://arxiv.org/abs/2412.17671)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion, generative</a></li>
<li><strong>Abstract: </strong>Successful forensic detectors can produce excellent results in supervised learning benchmarks but struggle to transfer to real-world applications. We believe this limitation is largely due to inadequate training data quality. While most research focuses on developing new algorithms, less attention is given to training data selection, despite evidence that performance can be strongly impacted by spurious correlations such as content, format, or resolution. A well-designed forensic detector should detect generator specific artifacts rather than reflect data biases. To this end, we propose B-Free, a bias-free training paradigm, where fake images are generated from real ones using the conditioning procedure of stable diffusion models. This ensures semantic alignment between real and fake images, allowing any differences to stem solely from the subtle artifacts introduced by AI generation. Through content-based augmentation, we show significant improvements in both generalization and robustness over state-of-the-art detectors and more calibrated results across 27 different generative models, including recent releases, like FLUX and Stable Diffusion 3.5. Our findings emphasize the importance of a careful dataset curation, highlighting the need for further research in dataset design. Code and data will be publicly available at this https URL</li>
</ul>

<h3>Title: VidTwin: Video VAE with Decoupled Structure and Dynamics</h3>
<ul>
<li><strong>Authors: </strong>Yuchi Wang, Junliang Guo, Xinyi Xie, Tianyu He, Xu Sun, Jiang Bian</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.AI, cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17726">https://arxiv.org/abs/2412.17726</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17726">https://arxiv.org/pdf/2412.17726</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17726]] VidTwin: Video VAE with Decoupled Structure and Dynamics(https://arxiv.org/abs/2412.17726)</code><input type="text"></li>
<li><strong>Keywords: </strong>generative</a></li>
<li><strong>Abstract: </strong>Recent advancements in video autoencoders (Video AEs) have significantly improved the quality and efficiency of video generation. In this paper, we propose a novel and compact video autoencoder, VidTwin, that decouples video into two distinct latent spaces: Structure latent vectors, which capture overall content and global movement, and Dynamics latent vectors, which represent fine-grained details and rapid movements. Specifically, our approach leverages an Encoder-Decoder backbone, augmented with two submodules for extracting these latent spaces, respectively. The first submodule employs a Q-Former to extract low-frequency motion trends, followed by downsampling blocks to remove redundant content details. The second averages the latent vectors along the spatial dimension to capture rapid motion. Extensive experiments show that VidTwin achieves a high compression rate of 0.20% with high reconstruction quality (PSNR of 28.14 on the MCL-JCV dataset), and performs efficiently and effectively in downstream generative tasks. Moreover, our model demonstrates explainability and scalability, paving the way for future research in video latent representation and generation. Our code has been released at this https URL.</li>
</ul>

<h3>Title: Knowledge Editing through Chain-of-Thought</h3>
<ul>
<li><strong>Authors: </strong>Changyue Wang, Weihang Su, Qingyao Ai, Yiqun Liu</a></li>
<li><strong>Subjects: </strong>cs.CL</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17727">https://arxiv.org/abs/2412.17727</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17727">https://arxiv.org/pdf/2412.17727</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17727]] Knowledge Editing through Chain-of-Thought(https://arxiv.org/abs/2412.17727)</code><input type="text"></li>
<li><strong>Keywords: </strong>in-context</a></li>
<li><strong>Abstract: </strong>Large Language Models (LLMs) have demonstrated exceptional capabilities across a wide range of natural language processing (NLP) tasks. However, keeping these models up-to-date with evolving world knowledge remains a significant challenge due to the high costs of frequent retraining. To address this challenge, knowledge editing techniques have emerged to update LLMs with new information without rebuilding the model from scratch. Among these, the in-context editing paradigm stands out for its effectiveness in integrating new knowledge while preserving the model's original capabilities. Despite its potential, existing in-context knowledge editing methods are often task-specific, focusing primarily on multi-hop QA tasks using structured knowledge triples. Moreover, their reliance on few-shot prompting for task decomposition makes them unstable and less effective in generalizing across diverse tasks. In response to these limitations, we propose EditCoT, a novel knowledge editing framework that flexibly and efficiently updates LLMs across various tasks without retraining. EditCoT works by generating a chain-of-thought (CoT) for a given input and then iteratively refining this CoT process using a CoT editor based on updated knowledge. We evaluate EditCoT across a diverse range of benchmarks, covering multiple languages and tasks. The results demonstrate that our approach achieves state-of-the-art performance while offering superior generalization, effectiveness, and stability compared to existing methods, marking a significant advancement in the field of knowledge updating. Code and data are available at: this https URL.</li>
</ul>

<h3>Title: The Superposition of Diffusion Models Using the It\^o Density Estimator</h3>
<ul>
<li><strong>Authors: </strong>Marta Skreta, Lazar Atanackovic, Avishek Joey Bose, Alexander Tong, Kirill Neklyudov</a></li>
<li><strong>Subjects: </strong>cs.LG</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17762">https://arxiv.org/abs/2412.17762</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17762">https://arxiv.org/pdf/2412.17762</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17762]] The Superposition of Diffusion Models Using the It\^o Density Estimator(https://arxiv.org/abs/2412.17762)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>The Cambrian explosion of easily accessible pre-trained diffusion models suggests a demand for methods that combine multiple different pre-trained diffusion models without incurring the significant computational burden of re-training a larger combined model. In this paper, we cast the problem of combining multiple pre-trained diffusion models at the generation stage under a novel proposed framework termed superposition. Theoretically, we derive superposition from rigorous first principles stemming from the celebrated continuity equation and design two novel algorithms tailor-made for combining diffusion models in SuperDiff. SuperDiff leverages a new scalable ItÃ´ density estimator for the log likelihood of the diffusion SDE which incurs no additional overhead compared to the well-known Hutchinson's estimator needed for divergence calculations. We demonstrate that SuperDiff is scalable to large pre-trained diffusion models as superposition is performed solely through composition during inference, and also enjoys painless implementation as it combines different pre-trained vector fields through an automated re-weighting scheme. Notably, we show that SuperDiff is efficient during inference time, and mimics traditional composition operators such as the logical OR and the logical AND. We empirically demonstrate the utility of using SuperDiff for generating more diverse images on CIFAR-10, more faithful prompt conditioned image editing using Stable Diffusion, and improved unconditional de novo structure design of proteins. this https URL</li>
</ul>

<h3>Title: Dora: Sampling and Benchmarking for 3D Shape Variational Auto-Encoders</h3>
<ul>
<li><strong>Authors: </strong>Rui Chen, Jianfeng Zhang, Yixun Liang, Guan Luo, Weiyu Li, Jiarui Liu, Xiu Li, Xiaoxiao Long, Jiashi Feng, Ping Tan</a></li>
<li><strong>Subjects: </strong>cs.CV</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17808">https://arxiv.org/abs/2412.17808</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17808">https://arxiv.org/pdf/2412.17808</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17808]] Dora: Sampling and Benchmarking for 3D Shape Variational Auto-Encoders(https://arxiv.org/abs/2412.17808)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>Recent 3D content generation pipelines commonly employ Variational Autoencoders (VAEs) to encode shapes into compact latent representations for diffusion-based generation. However, the widely adopted uniform point sampling strategy in Shape VAE training often leads to a significant loss of geometric details, limiting the quality of shape reconstruction and downstream generation tasks. We present Dora-VAE, a novel approach that enhances VAE reconstruction through our proposed sharp edge sampling strategy and a dual cross-attention mechanism. By identifying and prioritizing regions with high geometric complexity during training, our method significantly improves the preservation of fine-grained shape features. Such sampling strategy and the dual attention mechanism enable the VAE to focus on crucial geometric details that are typically missed by uniform sampling approaches. To systematically evaluate VAE reconstruction quality, we additionally propose Dora-bench, a benchmark that quantifies shape complexity through the density of sharp edges, introducing a new metric focused on reconstruction accuracy at these salient geometric features. Extensive experiments on the Dora-bench demonstrate that Dora-VAE achieves comparable reconstruction quality to the state-of-the-art dense XCube-VAE while requiring a latent space at least 8$\times$ smaller (1,280 vs. > 10,000 codes). We will release our code and benchmark dataset to facilitate future research in 3D shape modeling.</li>
</ul>

<h3>Title: FaceLift: Single Image to 3D Head with View Generation and GS-LRM</h3>
<ul>
<li><strong>Authors: </strong>Weijie Lyu, Yi Zhou, Ming-Hsuan Yang, Zhixin Shu</a></li>
<li><strong>Subjects: </strong>cs.CV, cs.GR</a></li>
<li><strong>Abstract URL: </strong><a href="https://arxiv.org/abs/2412.17812">https://arxiv.org/abs/2412.17812</a></li>
<li><strong>Pdf URL: </strong><a href="https://arxiv.org/pdf/2412.17812">https://arxiv.org/pdf/2412.17812</a></li>
<li><strong>Copy Paste: </strong><code><input type="checkbox">[[2412.17812]] FaceLift: Single Image to 3D Head with View Generation and GS-LRM(https://arxiv.org/abs/2412.17812)</code><input type="text"></li>
<li><strong>Keywords: </strong>diffusion</a></li>
<li><strong>Abstract: </strong>We present FaceLift, a feed-forward approach for rapid, high-quality, 360-degree head reconstruction from a single image. Our pipeline begins by employing a multi-view latent diffusion model that generates consistent side and back views of the head from a single facial input. These generated views then serve as input to a GS-LRM reconstructor, which produces a comprehensive 3D representation using Gaussian splats. To train our system, we develop a dataset of multi-view renderings using synthetic 3D human head as-sets. The diffusion-based multi-view generator is trained exclusively on synthetic head images, while the GS-LRM reconstructor undergoes initial training on Objaverse followed by fine-tuning on synthetic head data. FaceLift excels at preserving identity and maintaining view consistency across views. Despite being trained solely on synthetic data, FaceLift demonstrates remarkable generalization to real-world images. Through extensive qualitative and quantitative evaluations, we show that FaceLift outperforms state-of-the-art methods in 3D head reconstruction, highlighting its practical applicability and robust performance on real-world images. In addition to single image reconstruction, FaceLift supports video inputs for 4D novel view synthesis and seamlessly integrates with 2D reanimation techniques to enable 3D facial animation. Project page: this https URL.</li>
</ul>

<button id="copy">Copy All</button>
</article>
<script src="../../javascript/clipboard.min.js"></script>
<script src="../../javascript/clipboard.js"></script>
